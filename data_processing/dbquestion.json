{"_id":{"$oid":"5837a572a05283111e4d29f5"},"View_count":57,"Display_name":"Daniele","Question_score":1,"Question_content":"I want to generate sound figures of vocalizations of anurans with Seewave package on R platform.Someone would have a sript to direct me? Thank's a lot!","Creater_id":129632,"Start_date":"2016-08-31 07:30:46","Question_id":232684,"Tags":["r","data-visualization"],"Answer_count":0,"Last_activity":"2016-08-31 07:30:46","Link":"http://stats.stackexchange.com/questions/232684/how-to-make-spectrogram-and-oscillogram-on-r-using-seewave-package","Creator_reputation":6}
{"_id":{"$oid":"5837a572a05283111e4d29f7"},"View_count":34,"Display_name":"bwfilm","Question_score":3,"Question_content":"I am measuring the number of cells with a mutation in a series of 106 subjects. For each position of the genome, the method will output the total number of cells analysed and the number of cells with a specific nucleotide at that position (i.e. how many with A, with T, with G or with C). For each subject, at each genome position I find that most cells have a single nucleotide, so that its frequency is close to 100%. I am interested in understanding if the low frequency of the other nucleotides is real (thus constituting a potential pathogenic mutation) or due to errors.The method has a certain degree of error (unknown, likely influenced by many factors and thus not controllable) which may result, for example, in calling a cell with A rather than the correct value of C. As an example, If I plot the frequency of each nucleotide (y-axis, in %) seen for a specific position across all subjects (x-axis is the series of subjects) I obtain the following graph.Red line indicates the median of the frequency for that nucleotide at that position, the green line represents the median + 3*MAD.If we look at the panel for A, is there a way to understand whether the subjects with a frequency above the green line are outliers, and to assign a p-value to them? My idea is that the smaller peaks are indeed noise due to technical errors, which could be modeled and used to identify outliers (true mutations).Focusing on the frequencies of A, if we do an histogram we obtain the following:It is not a normal distribution, it has been suggested to follow a Poisson distribution. Is there a way to fit a Poisson distribution to this specific dataset and then calculate the probability to see such outliers (sort of a p-value)? In this case, the outliers could be the subjects in bins at 0.09 and 0.14. I know a little bit of R, so it would be great to know if is there a way to use R to analyse the data set to look for outliers.Additional questions:1) Would you approach this analysis differently? How?2) Barplots may not be the best way to represent these data (see first figure), in this case, which graph would you recommend?Thanks a lot for your help and insight!","Creater_id":129594,"Start_date":"2016-08-31 03:15:18","Question_id":232632,"Tags":["distributions","outliers","noise"],"Answer_count":0,"Last_activity":"2016-08-31 07:15:52","Link":"http://stats.stackexchange.com/questions/232632/detection-of-noise-and-outliers","Creator_reputation":16}
{"_id":{"$oid":"5837a572a05283111e4d29f9"},"View_count":32,"Display_name":"demonic toaster","Question_score":1,"Question_content":"I have a balanced panel dataset in which I have observations for N countries across T quarters. I use a fixed effect model to explore the relationship between my variables. Since all countries have the same number of observations, they are all given the same \"weight\" in the regression. I would like to apply a weigthing by GDP such that observations of big countries (in terms of wealth) have more impact on my estimation. My first idea was to compute weights for all countries and rescale the dependent variable. However, I think this method will only affect the (country-specific) intercepts and won't have much impact on the coefficients. I also don't think that controlling for GDP by including it in the regression is the solution. I am working with the R plm package to estimate my models. I know that the lm function has a weight parameter but couldn't find an equivalent in the plm package. Any idea how I should proceed?","Creater_id":127701,"Start_date":"2016-08-31 07:04:36","Question_id":232682,"Tags":["r","panel-data","fixed-effects-model","plm"],"Answer_count":0,"Last_activity":"2016-08-31 07:04:36","Link":"http://stats.stackexchange.com/questions/232682/how-can-i-apply-weights-to-the-cross-sectional-dimension-in-a-fixed-effect-panel","Creator_reputation":6}
{"_id":{"$oid":"5837a572a05283111e4d29fb"},"View_count":25,"Display_name":"PScho","Question_score":3,"Question_content":"I have the following problem that I am not sure of whether I treat it correctly:I run repeated simulations, where the output of a single simulation replicate is a single correlation value. The simulation is replicated (let's say) 1,000 times and finally the mean correlation is calculated for a certain parameter setting of the simulation. The goal is now to compare for different parameter settings not only the mean correlation, but also to which degree the correlation varies across simulation replicates in comparison with other parameter settings. My first approach was to compare the standard deviation of the correlation across the different parameter settings, but I am uncertain about how this is influenced by the actual mean value of the correlation. Would it be appropriate to use the coefficient of variation, i.e., SD(x)/Mean(x), x being the vector of the 1,000 correlation values of one parameter setting, and to compare this value to those of other parameter settings?Thanks in advance for any help! ","Creater_id":129613,"Start_date":"2016-08-31 05:32:58","Question_id":232661,"Tags":["correlation","standard-deviation","coefficient-of-variation"],"Answer_count":1,"Last_activity":"2016-08-31 07:03:31","Link":"http://stats.stackexchange.com/questions/232661/coefficient-of-variation-of-correlation-values","Creator_reputation":16}
{"_id":{"$oid":"5837a572a05283111e4d2a08"},"View_count":16,"Display_name":"dumbscientist","Question_score":1,"Question_content":"sorry for what is probably a rudimentary question.I have a set of polling data.  Each day, the result of a 7-day rolling sample of polling data is reported.  In this particular case, the respondents come from the same group of people (it's technically a panel, rather than a poll).  So for each new day, in theory, I know which responses are falling out of the rolling window as well as who could possibly participate in the current day's polling.  Therefore, I can simulate a range of possible samples in part by using respondent's past behavior.Based on this, I can model a distribution of expected outcomes.  So my model might say, for instance, that on a given day option A will poll higher than option B by 3 points 30% of the time, by 2 points 25% of the time, and so forth.My question is how can I measure the accuracy of this model, given that I only ever see one observation from the real distribution?Both my model's distribution and the real distribution change from day to day, and I have many days of data with which to work.  In my uneducated head, it seems like it should be possible to measure whether things my model say should happen 30% of the time actually happen 30% of time or not.What is the formal way to measure accuracy in this situation?","Creater_id":129624,"Start_date":"2016-08-31 07:00:33","Question_id":232680,"Tags":["probability","distributions","model","accuracy"],"Answer_count":0,"Last_activity":"2016-08-31 07:00:33","Link":"http://stats.stackexchange.com/questions/232680/possbible-to-measure-accuracy-of-modeled-distribution-with-only-one-observation","Creator_reputation":6}
{"_id":{"$oid":"5837a572a05283111e4d2a0a"},"View_count":20,"Display_name":"atomsmasher","Question_score":0,"Question_content":"So in OLS you estimate , one way, for LASSO you estimate  another way, for LTS, yet another way, so on and so forth for each flavor of linear regression. But, given the different solutions for estimating  do all variations compute the confidence intervals, p-values, RSE, SE the same way or is some adjust required per method ?   ","Creater_id":117380,"Start_date":"2016-08-31 06:56:05","Question_id":232678,"Tags":["regression"],"Answer_count":0,"Last_activity":"2016-08-31 06:56:05","Link":"http://stats.stackexchange.com/questions/232678/do-different-versions-of-linear-regression-all-follow-the-same-computation-of-p","Creator_reputation":35}
{"_id":{"$oid":"5837a572a05283111e4d2a0c"},"View_count":24,"Display_name":"alberto","Question_score":2,"Question_content":"In the context of Expectation-Maximization, I would like to compute te entropy factor in order to get the value of the lower bound when the algorithm converged.This lower bound can be expressed as:\\begin{equation}\\mathcal{L}(q, \\theta) = \\underbrace{\\sum_{\\mathbf{Z}} p(\\mathbf{Z} | \\mathbf{X}, \\boldsymbol{\\theta}^{old})\\ln p(\\mathbf{X,Z|\\boldsymbol{\\theta}})}_{\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{old})}-\\underbrace{\\sum_{\\mathbf{Z}} p(\\mathbf{Z}|\\mathbf{X}, \\boldsymbol{\\theta}^{old})\\ln p(\\mathbf{Z} | \\mathbf{X}, \\boldsymbol{\\theta}^{old})}_\\mathcal{H}\\end{equation}(notation from Bishop's Pattern Recognition and Machine Learning, page 452)In the E-M we ignore the entropy and focus on  because the entropy does not depend on . But once we finish, if we want to compare different models, we need the total  and thus we need to compute the entropy  (if my understanding is correct).I'm seeing some strange behaviors in my computed , and I'm starting to think that I'm not computing entropy correctly.So, I have  points and their probabilities . Each probability distribution represents the probability of belonging to each of the components (responsibilities) . If we have two components:\\begin{equation}p(\\mathbf{z}_i | \\cdot ) = (z_{i1}, z_{i2})\\end{equation}How should I compute the entropy?I'm currently computing this:\\begin{align}p(z_{11}) \\ln p(z_{11}) \u0026amp;+ p(z_{12}) \\ln p(z_{12}) ~+\\\\p(z_{21}) \\ln p(z_{21}) \u0026amp;+ p(z_{22}) \\ln p(z_{22}) ~+\\\\...\\\\p(z_{n1}) \\ln p(z_{n1}) \u0026amp;+ p(z_{n2}) \\ln p(z_{n2})\\end{align}","Creater_id":36312,"Start_date":"2016-08-31 06:36:48","Question_id":232675,"Tags":["machine-learning","expectation-maximization","entropy","gaussian-mixture"],"Answer_count":0,"Last_activity":"2016-08-31 06:47:03","Link":"http://stats.stackexchange.com/questions/232675/entropy-of-a-set-of-categorical-variables","Creator_reputation":1272}
{"_id":{"$oid":"5837a572a05283111e4d2a0e"},"View_count":33,"Display_name":"Erik","Question_score":0,"Question_content":"When calculating the limits of agreement (LOA) in Bland-Altman Analysis (BAA), one step is to calculate the differences  between pairs of observations. According to Bland and Altman in their original article the differences are normally distributed. Therefore, the LOA are calculated as , where  for 95% LOA and  the standard deviation of . Note that Bland and Altman themselves use , not , which insinuates they know  is just a sample (because  is used instead of  by convention when talking about samples). When calculating the confidence interval (CI) of a sample, we use  from Student's -distribution with  degrees of freedom (DF) instead of  from the normal distribution,  being the number of observation pairs.Therefore my question: I wonder why Bland and Altman do not use a -distribution with  DF, since the differences are a sample of the true ‘population’ of differences, being all possible observations. Why is the normal distribution used in BAA when calculating the LOA, instead of the -distribution like with CI?Side note 1:Further down their article, Bland and Altman show how to calculate the CI for the LOA themselves. In that section they do use the -distribution with  DF, because the LOA are a result of the  samples in .Side note 2:Since  is the sum of observations from two methods with an unknown distribution, i.e. , does (some variant of) the Central Limit Theorem hold? Because we compare just two measurements, the differences will not truly be normally distributed (although Bland and Altman write  is approximately normally distributed)? However, under the assumption the two measurements perform similarly, their distribution will not be very different and thus their differences might be assumed normal. Is this why  is used instead of ? Even so, I still think  is a sample and thus -distributed.","Creater_id":80486,"Start_date":"2016-08-31 03:57:32","Question_id":232643,"Tags":["normal-distribution","confidence-interval","paired-comparisons","t-distribution","bland-altman-plot"],"Answer_count":0,"Last_activity":"2016-08-31 06:38:45","Link":"http://stats.stackexchange.com/questions/232643/why-are-the-differences-in-bland-altman-analysis-normally-distributed-instead-of","Creator_reputation":125}
{"_id":{"$oid":"5837a572a05283111e4d2a10"},"View_count":15,"Display_name":"Ernest A","Question_score":2,"Question_content":"Consider the following log-likelihood function\\ell (\\vec{\\mu}, \\rho \\mid \\vec{y}) =\\sum_{i=1}^I \\log f(\\mu_{i1}, \\dotsc, \\mu_{iJ}, \\rho \\mid y_{i1}, \\dotsc, y_{iJ})where  () is the response,  () are the corresponding expected values and  is some shape parameter.How do we define the full model?  In the context of GLMs, the full model is defined as a model with  parameters, one for each observation.  But here if we set  for all  there is still one parameter left  which has to be assigned some value.","Creater_id":10028,"Start_date":"2016-08-31 06:38:12","Question_id":232677,"Tags":["generalized-linear-model","likelihood","deviance"],"Answer_count":0,"Last_activity":"2016-08-31 06:38:12","Link":"http://stats.stackexchange.com/questions/232677/deviance-in-a-distribution-that-has-a-shape-parameter","Creator_reputation":585}
{"_id":{"$oid":"5837a572a05283111e4d2a12"},"View_count":31,"Display_name":"Xiphias","Question_score":1,"Question_content":"IntroductionWhen testing a sample against a population, one has one of the following sets of hypotheses:Two-tailed test: , i.e. \"no difference\", and Right-tailed test: , and Left-tailed test: , and I know that some textbooks write , and  for one-tailed tests, i.e. leave out the other side. However, I always considered that bad practice.Now suppose testing the difference between two sample means. Here, the formula states:Then, it is assumed that . Thus, we get:ProblemThis is all fine to me when considering two-tailed tests. However, I encountered an exercise that asks whether there is a significant increase between two samples (sales of first and second year).Now I face two questions:Isn't it correct to state , and  for a left-tailed test comparing two sample means? But then the s would not sum to zero. I assume that this does not matter since if  increases, in the formula, we would get a smaller  which would lead us to rejecting  with a higher probability anyway. Thus, every case is covered with  already.Since the two samples have \"equal rights\", referring to the symmetry between the samples, can we formulate the hypotheses either way, i.e. measure an increase of one or, alternatively, swap  and  and switch the equality signs? This is, of course, a no-go for \"normal\" tests, but facing two sample means, it does not matter which way to look at them, or does it?","Creater_id":35495,"Start_date":"2016-08-31 05:47:10","Question_id":232665,"Tags":["hypothesis-testing","paired-comparisons","z-test"],"Answer_count":2,"Last_activity":"2016-08-31 06:35:09","Link":"http://stats.stackexchange.com/questions/232665/understanding-formulation-of-hypotheses-in-difference-between-two-sample-means","Creator_reputation":127}
{"_id":{"$oid":"5837a572a05283111e4d2a20"},"View_count":45,"Display_name":"user3275222","Question_score":1,"Question_content":"I am looking for an elementary statistics book, that contains topics such as descriptive statistics, sampling distributions of the mean and variance, etc. However, despite my interest in basic topics, I am looking for a book that teaches it in a high level, for example: when discussing the why the variance includes a squared difference from the mean and not absolute value, I am looking for a mathematical explanation (the problem with the derivative), or, a loss function explanation. In other words, I am looking for an elementary statistics book for mathematicians or statisticians, and not just a textbook. Can you please recommend me of such books, if exist? Thank you.","Creater_id":81477,"Start_date":"2016-08-31 05:34:42","Question_id":232662,"Tags":["references","descriptive-statistics"],"Answer_count":1,"Last_activity":"2016-08-31 06:26:14","Link":"http://stats.stackexchange.com/questions/232662/high-level-elementary-statistics-textbook","Creator_reputation":159}
{"_id":{"$oid":"5837a572a05283111e4d2a2d"},"View_count":52,"Display_name":"Andy_3000","Question_score":4,"Question_content":"I have a simple linear regression model and am trying to locate structural breaks in the relationship between the two variables. The data are cross-sectional, not time-series.I've been using Wald tests on pairs of dummies (1 if X is above some threshold, zero otherwise) \u0026amp; interaction terms (of the dummy and X) and found quite a few \"candidate\" points. But after reading up on the topic, it seems this method is frowned upon. That is, testing multiple points in hopes of finding something rather than testing a \"known\" point.The estat sbsingle command in Stata shows some promise, but requires a gapless time series. Is it advisable/reasonable/defensible to assign ranks to the observations based on each's X value, and then treat the rank as a time-series variable in order to meet the estat command's requirements?--Edit in response to GeoMatt22--","Creater_id":129551,"Start_date":"2016-08-30 17:20:36","Question_id":232576,"Tags":["stata","structural-change"],"Answer_count":0,"Last_activity":"2016-08-31 06:23:13","Link":"http://stats.stackexchange.com/questions/232576/must-data-be-time-series-to-contain-structural-breaks","Creator_reputation":21}
{"_id":{"$oid":"5837a572a05283111e4d2a2f"},"View_count":14,"Display_name":"skress","Question_score":1,"Question_content":"I do have difficulty to decide whether or not my classification approach makes sense.My data set contains sensor data (voltages, state of charge, temperatures, etc.) from different machines. Every machine writes a record after every run. Runs are of different lengths and range from a few seconds up to hours.Every record also contains a flag that indicates if a malfunction occured. My goal is to predict this malfunction.How do I choose my target variable? It doesn't make sense to choose the flag because every future record will already contain this flag. Once a record is written it is already too late to predict a malfunction.I had the idea to calculate some kind of offset to the malfunction event. So I can try to predict if a malfunction occurs in the following run of the machine.Time, Var1, Var2, Var3, Malfunction, *Malfunction Next Run*12:00, X, X, X, 0, 012:01, X, X, X, 0, 012:05, X, X, X, 0, 112:25, X, X, X, 1, 0Does this make any sense? If the third record is a perfectly fine run like the first two my classification model won't be able to predict the malfunction accurately.How would you address this problem?","Creater_id":129614,"Start_date":"2016-08-31 06:19:31","Question_id":232672,"Tags":["classification"],"Answer_count":0,"Last_activity":"2016-08-31 06:19:31","Link":"http://stats.stackexchange.com/questions/232672/can-i-predict-a-target-value-event-that-happens-at-the-same-time-when-a-data-r","Creator_reputation":6}
{"_id":{"$oid":"5837a572a05283111e4d2a31"},"View_count":21,"Display_name":"tool.ish","Question_score":2,"Question_content":"I'm tackling the problem of Anomaly Detection in a dataset that's comprised of call counts to a call-centre. The data exhibits daily and weekly seasonality and is known to be over-dispersed.Methodology: The first step is to define \"normal\" behaviour, which is assumed to be the median from the previous n-weeks (median because it is robust to the presence of outliers in the data). For example, say current (Monday 09:00AM) calls are 100, and the last n=5 Monday 09:00AM calls were [110,90,122,105,96], so the baseline \"normal\" calls for this Monday 09:00AM is the median=105. This gives a smoothed series which we define as \"normal\" (this is assumed sacrosanct).Then I fit a negative-binomial model for previous n-weeks of this normal behaviour using MLE. Finally, I compute the likelihood of observed calls or more given the fitted model, and declare anomalies based off a threshold on the upper tail P(X\u003e=observed) i.e. we are interested in unusually high # calls, ignoring # calls less than baseline.Here is a plot of the algorithm in action:Now, i'm trying to evaluate this methodology by injecting artificial anomalies on the baseline series, which requires some knowledge of the residuals:residual = observed - baseline (median of last n-weeks)If I can simulate the residual generating process to inject known anomalies, knowing the correct p-values (labels), it would serve as an objective evaluation of this methodology in terms of accuracy of anomaly detection and quantification.Questions:What form of the residuals should be simulated? in other words, is the choice of considering a raw residual (as above) correct?What should be the distribution of residuals? How do I simulate them?Any thoughts/comments/suggestions on the methodology itself?Really appreciate any help. Thanks!","Creater_id":41367,"Start_date":"2016-08-31 03:40:40","Question_id":232638,"Tags":["binomial","simulation","residuals"],"Answer_count":0,"Last_activity":"2016-08-31 06:16:55","Link":"http://stats.stackexchange.com/questions/232638/residuals-in-negative-binomial-data","Creator_reputation":67}
{"_id":{"$oid":"5837a572a05283111e4d2a33"},"View_count":67,"Display_name":"zpsimpso","Question_score":2,"Question_content":"I have datasets that can form several different curvy patterns between the dependent and independent variables. The 'true' relationship likely depends on a large number of factors that aren't easily measured and so it remains unknown. One method of describing this relationship has been through LOESS.However, I am trying to optimize the LOESS fit by changing the tuning parameter (span or f-value) using k-fold cross-validation (with the loess.wrapper function in the bisoreg package in R). I mostly understand how the optimal span is chosen by minimizing the estimated predictive error via the CV. I've been studying the bias-variance tradeoff topic in sources such as Elements of Statistical Learning and I'm still left with a difficult question. Is it possible to calculate (or estimate) the bias and variance of a LOESS fit for different span values? I'd like to be able to compare different LOESS fits on the same data with these statistics but I think the issue lies in the fact that I don't know the 'true' relationship of the data.Any guidance or recommended readings are appreciated.","Creater_id":99897,"Start_date":"2016-01-06 15:54:29","Question_id":189625,"Tags":["bias","smoothing","loess"],"Answer_count":1,"Last_activity":"2016-08-31 05:46:29","Link":"http://stats.stackexchange.com/questions/189625/calculating-bias-and-variance-in-a-loess-fit","Creator_reputation":11}
{"_id":{"$oid":"5837a572a05283111e4d2a40"},"View_count":36,"Display_name":"asdir","Question_score":0,"Question_content":"I am trying to use Bayesian Model averaging (BMA) for a Poisson model to select     relevant variables from a large set of variables. In the selection process I force some variables in the model since they are necessary from an econometric point of view (fixed effects).The problem is that the BMA seems to go through only 1 possible model, or at least reports only 1, and (therefore?) includes all variables in this model. However, I of course would like the BMA to select only a subset of variables.My R-script looks as follows (I skipped about 80 variables for this example to make things easier here).### Preprm(list = ls())setwd(\"H:\\\\DAF_ENV Climate Change\\\\2015\\\\CPF bid on clean energy\\\\Models\\\\3rd model\\\\R-scripts and tables\")library(BMA)## Loading dataD.all \u0026lt;- read.csv(\"H:/Data/full sets/3rd model/poissonbmadata.csv\")### Poisson family BMA runfes\u0026lt;-c(2:15,16:20) #fixed effectsindepvar\u0026lt;-c(65,66,67,68,72,74,75,76,77,78,79,80,81,82,83,84:91) #other independent variablesx\u0026lt;- D.all[,c(fes,indepvar)] #all independent varsy\u0026lt;-D.all[,c(1)] #dependent variableforce\u0026lt;- 1:dim(x)[2] #creates the force indicator variableforce[1:dim(x)[2]] \u0026lt;- 0.5 #sets all variables in the set to \"not forced\"force[1:length(fes)] \u0026lt;- 1 #sets the FEs as forcedglm.out.invbma \u0026lt;- bic.glm( x, y, glm.family = poisson(),prior.param = force, factor.type=FALSE)summary(glm.out.invbma)The output is:1  models were selectedBest  1  models (cumulative posterior probability =  1 ):                p!=0   EV         SD         model 1   Intercept      100    1.020e+01  1.304e-02   1.020e+01yeardummy2001  100   -1.499e+00  6.912e-03  -1.499e+00yeardummy2002  100   -1.369e+00  6.187e-03  -1.369e+00yeardummy2003  100   -1.496e+00  6.575e-03  -1.496e+00yeardummy2004  100   -1.250e+00  6.032e-03  -1.250e+00yeardummy2005  100   -4.798e-01  5.219e-03  -4.798e-01yeardummy2006  100    3.142e-01  3.956e-03   3.142e-01yeardummy2007  100    5.242e-01  3.723e-03   5.242e-01yeardummy2008  100    5.977e-01  3.697e-03   5.977e-01yeardummy2009  100    5.813e-01  3.787e-03   5.813e-01yeardummy2011  100    8.794e-01  3.603e-03   8.794e-01yeardummy2012  100    8.238e-01  3.709e-03   8.238e-01yeardummy2013  100    6.407e-01  3.771e-03   6.407e-01yeardummy2014  100   -2.690e+00  7.676e-02  -2.690e+00secdummy5      100    1.830e-01  2.335e-03   1.830e-01nrecomp        100   -7.020e-06  8.813e-08  -7.020e-06fit            100    2.453e+00  5.869e-03   2.453e+00tender         100    1.188e-03  2.087e-06   1.188e-03pmr            100   -3.908e-02  2.645e-03  -3.908e-02etspart        100   -5.910e-01  2.354e-03  -5.910e-01terteduc       100   -1.288e-02  7.119e-05  -1.288e-02wdi532         100    8.007e-03  5.450e-05   8.007e-03wdi533         100   -1.473e-02  1.651e-04  -1.473e-02wdi534         100   -7.677e-02  4.526e-04  -7.677e-02wdi535         100   -4.996e-03  2.699e-05  -4.996e-03wdi537         100   -3.188e-03  1.241e-05  -3.188e-03wdi538         100   -1.778e-03  1.679e-05  -1.778e-03wdi5310          0    0.000e+00  0.000e+00       .    dobus          100   -2.662e-02  1.208e-04  -2.662e-02transloss      100   -9.329e-02  3.339e-04  -9.329e-02nVar                                           28     BIC                                          5.970e+06post prob                                    1        I already tested removing the FEs, putting more or less variables and forcing different or no variables. In almost all cases all variables are at 100% inclusion probability. In some cases two models are selected, but still all variables are at \u003e50% inclusion probability.The context of the model is the influence of factors on renewable electricity investment. And, no, I don't think all of these factors have an influence warranting the 100% inclusion probability. :)Edit: The version above also gave warnings after the bic.glm regarding NAs. Once I removed the NAs via D.all\u0026lt;-D.all.original[complete.cases(D.all.original),], and excluded the year-dummies (one or two of them might have become collinear with the constant due to the NA-removal), the results still looked similar, except for the now missing year-FEs.","Creater_id":27427,"Start_date":"2016-08-31 03:41:00","Question_id":232639,"Tags":["r","regression","bayesian","poisson","model-averaging"],"Answer_count":0,"Last_activity":"2016-08-31 05:44:41","Link":"http://stats.stackexchange.com/questions/232639/bma-poisson-selects-only-1-model-with-all-variables-at-inclusion-probability-at","Creator_reputation":50}
{"_id":{"$oid":"5837a572a05283111e4d2a42"},"View_count":185,"Display_name":"Revan Erraboina","Question_score":2,"Question_content":"I have just learnt standard deviation and I am wondering can we replace  with 5 in order to calcualte the standard deviation of .","Creater_id":129601,"Start_date":"2016-08-31 03:42:38","Question_id":232640,"Tags":["statistical-significance","mathematical-statistics","standard-deviation","descriptive-statistics"],"Answer_count":4,"Last_activity":"2016-08-31 05:29:00","Link":"http://stats.stackexchange.com/questions/232640/if-standard-deviation-of-x-is-5-standard-deviation-of-y-2x-3-is-7","Creator_reputation":9}
{"_id":{"$oid":"5837a572a05283111e4d2a52"},"View_count":294,"Display_name":"scttl","Question_score":4,"Question_content":"I'm curious to know what others tend to see as a suitable naming convention for model features or variables, particularly as they relate to their use and reference in software applications.For instance, given two inputs: age and income, we could construct features around various transformations and discretizations of their individual raw values, as well as capture their interaction in a number of ways.Realizing that we want to make these names concise yet descriptive, would the following seem reasonable? Are they too verbose?gt_100k_incomeis_missing_incomelg10_incomege_20_lt_25_agezscale_ageratio_ln_income_ln_age...Is it worth trying to (explicitly or implicitly) denote the return type of a feature value?  How about naming a feature that is derived from say 5 or more other features?","Creater_id":8701,"Start_date":"2012-02-13 16:55:58","Question_id":22766,"Tags":["notation","feature-construction","software"],"Answer_count":2,"Last_activity":"2016-08-31 05:27:21","Link":"http://stats.stackexchange.com/questions/22766/feature-naming-conventions","Creator_reputation":143}
{"_id":{"$oid":"5837a572a05283111e4d2a60"},"View_count":14,"Display_name":"StubbornAtom","Question_score":0,"Question_content":"  If larger values have larger weights then show that for  observations  with respective weights ,  , where  and The result is somewhat clear intuitively but I am not being able to show it analytically. I was thinking of using Chebyshev's sum inequality to solve this.Let , then by the problem it follows that .Then does it follow from Chebyshev's inequality that  ?","Creater_id":119261,"Start_date":"2016-08-31 04:40:34","Question_id":232656,"Tags":["descriptive-statistics","weighted-mean"],"Answer_count":1,"Last_activity":"2016-08-31 04:58:18","Link":"http://stats.stackexchange.com/questions/232656/if-larger-values-have-larger-weights-then-weighted-amsimple-am","Creator_reputation":140}
{"_id":{"$oid":"5837a572a05283111e4d2a6d"},"View_count":25,"Display_name":"Lennart","Question_score":0,"Question_content":"I've constructed a Linear Regression model (OLS) and I'm wondering what the statistical certainty is for the response variable to be true.Say I'm collecting data from a biological system and I want to predict the weight of the cells, e.g resulting in y=50mg. I want to figure out how certain I am that y=50mg is true. Also, if I assume intervall values 1-10mg, 11-20, 21-30.. 91-100mg - can I assume linear distribution i.e that if y=50mg then it's a 10% chance that the values is between 41-50mg (and nothing more) or am I equipped to assume trapezoidal probability function i.e that it's a 50% chance that the value is 41-50mg or less.Any thoughts on this?","Creater_id":85590,"Start_date":"2016-08-31 04:35:31","Question_id":232654,"Tags":["regression","least-squares","uncertainty"],"Answer_count":1,"Last_activity":"2016-08-31 04:43:21","Link":"http://stats.stackexchange.com/questions/232654/probability-distributions-from-a-linear-regression-model","Creator_reputation":150}
{"_id":{"$oid":"5837a572a05283111e4d2a7a"},"View_count":53,"Display_name":"LaNeu","Question_score":2,"Question_content":"I am seeking statistical advice on the random-effects structure of a mixed-model. I am using R's lme4 package.Based on recent papers showing the importance of the random-effects structure  (such as http://www.sciencedirect.com/science/article/pii/S0749596X12001180), I would like to make sure that my random-structure is correct.More specifically, I have predictors A and B and dependent variable Y. Predictor A constitutes the experimental manipulation (every subject comes twice to the lab, undergoing treatment 1 or 2), and I have mulitple observations of the dependent variable Y (100 per participant, \"ID\"). B in contrast, is a nuisance variable (i.e., Hunger), which is assummed to be constant over the short time of the experiment.Now I am not sure how to correctly specify the random effects if a) I am interested (a priori) in the interaction between A and B. Would summary(a\u0026lt;-glmer( Y ~ A * B + (1 + A*B|ID), data= x, family=\"binomial\"), REML=FALSE)or summary(a\u0026lt;-glmer( Y ~ A * B + (1 + A|ID), data= x, family=\"binomial\"), REML=FALSE)be the correct model? b) Assuming I am only interested in the main effect of A. B (e.g., hunger, or something that is constant over all experimental seesions, such as age) is considered a nuisance variable. Wouldsummary(a\u0026lt;-glmer( Y ~ A + B + (1 + A + B|ID), data= x, family=\"binomial\"), REML=FALSE)or  summary(a\u0026lt;-glmer( Y ~ A + B + (1 + A|ID), data= x, family=\"binomial\"), REML=FALSE)be the correct model? Based on the postMixed Model Analyses with Interactions in the Random Effects StructureI would suggest the second, but please let me know if I am incorrect. Thank you, Laura","Creater_id":129602,"Start_date":"2016-08-31 04:05:51","Question_id":232648,"Tags":["r","mixed-model","random-effects-model","lme4"],"Answer_count":1,"Last_activity":"2016-08-31 04:38:24","Link":"http://stats.stackexchange.com/questions/232648/random-effects-structure-lme","Creator_reputation":13}
{"_id":{"$oid":"5837a572a05283111e4d2a87"},"View_count":31,"Display_name":"user259047","Question_score":2,"Question_content":"Which term describes a \"stepwise\" variable? I.e. a variable that may only take sudden values (e.g. 0, 2/3 and 2), and has several observations at each of these values. The variable may indicate eg points in time.The answer is not a discrete variable since these do not allow for decimal values. Nor is it a binned or grouped variable since there is no censoring. Thank you.","Creater_id":127605,"Start_date":"2016-08-31 04:11:01","Question_id":232649,"Tags":["distributions"],"Answer_count":1,"Last_activity":"2016-08-31 04:38:20","Link":"http://stats.stackexchange.com/questions/232649/which-term-describes-a-stepwise-variable","Creator_reputation":25}
{"_id":{"$oid":"5837a572a05283111e4d2a94"},"View_count":77,"Display_name":"Stephan","Question_score":1,"Question_content":"Currently I am trying to implement a mortality rate model (Lee-Li), but I am having some trouble with the maximum likelihood procedure of a time series model given by:\\begin{align}K_{t+1} \u0026amp;= K_t + \\theta + \\epsilon_{t+1}\\\\ \\kappa_{t+1} \u0026amp;= a \\kappa_{t} + \\delta_{t+1}\\end{align}Where  are assumed to be independent and come from a bi variate normal distribution with mean  and co-variance matrix . I have data available for both  and , and the idea is to maximize for ,  and  using maximum likelihood. However I have no clue as how to approach this problem. I tried obtaining the MLE for  using the MLE for the covariance matrix of a bivariate normal distribution, but this seems to be wrong. Furthermore I have no idea on how to approach  and a for this matter either, since I don't know how to set up a (log)-likelihood for this expression.Any help would be appreciated.Edit: The co-variance matrix Where the correlation coefficient  is non-zero.I have tried to do the following steps to derive the maximum likelihood for  :1: Condition the distribution of  on past data  and the parameters, which has a bi variate normal distribution 2: compute the conditional log-likelihood of this equation:\\begin{align}\\sum_{t}\\log(L \\left(\\mathbf{\\Theta} \\right)) \u0026amp;= \\log\\left(\\frac{1}{2 \\pi \\sigma_{\\epsilon} \\sigma_{\\delta}\\sqrt{ 1 - \\rho^{2}}}\\right) -  \\frac{z}{2(1 - \\rho^{2}}\\end{align}where \\begin{align}z = \\frac{(K_{t+1} - (K_{t} + \\theta))^{2}}{\\sigma_\\epsilon^2} + \\frac{(\\kappa_{t+1} - a\\kappa_{t}) ^{2}}{\\sigma_\\delta^2} - \\frac{2\\rho(K_{t+1} - (K_{t} + \\theta))(\\kappa_{t+1} - a\\kappa_{t})}{\\sigma_\\epsilon \\sigma_\\delta}\\end{align}3: take the derivative with respect to . Since  is the only expression involving  we can ignore the rest: \\begin{align}\\frac{\\partial \\log \\left( L \\right)}{\\partial \\theta} \u0026amp;= 0 \\Leftrightarrow \\\\\\sum_{t}\\frac{-2(K_{t+1} - (K_t+\\theta))}{\\sigma_{\\epsilon}^2} + \\sum_{t}\\frac{2\\rho(\\kappa_{t+1} - a\\kappa_t)}{\\sigma_{\\epsilon}\\sigma_{\\delta}}\u0026amp;=0 \\Leftrightarrow \\\\\\sum_{t}\\frac{K_{t+1} - (K_t+\\theta)}{\\sigma_{\\epsilon}^2} \u0026amp;= \\sum_{t}\\frac{\\rho(\\kappa_{t+1} - a\\kappa_t)}{\\sigma_{\\epsilon}\\sigma_{\\delta}} \\Leftrightarrow \\\\\\sum_{t}K_{t+1} - K_t - n\\theta \u0026amp;= \\sigma_{\\epsilon}^2\\sum_{t}\\frac{\\rho(\\kappa_{t+1} - a\\kappa_t)}{\\sigma_{\\epsilon}\\sigma_{\\delta}} \\Leftrightarrow \\\\ \\hat{\\theta_{MLE}} =\\sum_{t}\\frac{(K_{t+1} - K_t)}{n} \u0026amp;-\\sigma_{\\epsilon}^2\\sum_{t}\\frac{\\rho(\\kappa_{t+1} - a\\kappa_t)}{n\\sigma_{\\epsilon}\\sigma_{\\delta}} \\end{align}Similar calculations for  yield \\begin{align}\\hat{a_{MLE}} \u0026amp;= \\frac{\\sum_{t} \\kappa_{t+1} \\kappa_{t} - \\frac{\\sigma_{\\delta}}{\\sigma_{\\epsilon}} \\rho \\sum_{t} \\kappa_{t}(K_{t+1} - K_{t} - \\theta)}{ \\sum_{t} \\kappa_{t}^{2}} \\\\\\hat{\\sigma_{\\epsilon}^{MLE}} \u0026amp;= \\frac{-\\frac{\\rho*(K_{t+1} - K_{t} - \\theta)(\\kappa_{t+1} - a*\\kappa_{t})}{ \\sigma_{\\delta}} + \\sqrt D_{\\epsilon} )}{ 2n(1-\\rho^2)} \\\\\\hat{\\sigma_{\\delta}^{MLE}} \u0026amp;= \\frac{-\\frac{\\rho*(K_{t+1} - K_{t} - \\theta)(\\kappa_{t+1} - a*\\kappa_{t})}{ \\sigma_{\\epsilon}} + \\sqrt D_{\\delta} )}{ 2n(1-\\rho^2)}\\end{align}Where \\begin{align}D_{\\epsilon} \u0026amp;=  \\left(\\frac{\\rho \\sum_{t} (K_{t+1} - K_{t} - \\theta)(\\kappa_{t+1} - a*\\kappa_{t})}{\\sigma_{\\epsilon}}\\right)^{2} + 4 n(1-\\rho^2) \\sum_{t} (\\kappa_{t+1} - a\\kappa_{t})^2 \\\\D_{\\delta} \u0026amp;=\\left(\\frac{\\rho \\sum_{t} (K_{t+1} - K_{t} - \\theta)(\\kappa_{t+1} - a*\\kappa_{t})}{\\sigma_{\\epsilon}}\\right)^{2}  + 4  n(1-\\rho^2) \\sum_{t} (K_{t+1} - K_{t} - \\theta)^2 \\\\\\end{align}Finally, since the expression is too complicated analytically, one can use numerical techniques to solve the ML equations. I have used the quasi Newton-Raphson algorithm proposed by Goodman to determine the parameters.","Creater_id":129128,"Start_date":"2016-08-26 16:17:41","Question_id":231983,"Tags":["time-series","mathematical-statistics","maximum-likelihood"],"Answer_count":1,"Last_activity":"2016-08-31 04:35:09","Link":"http://stats.stackexchange.com/questions/231983/maximum-likelihood-in-a-time-series-multi-population-model","Creator_reputation":78}
{"_id":{"$oid":"5837a572a05283111e4d2aa1"},"View_count":36,"Display_name":"Yohanes Eki Apriliawan","Question_score":0,"Question_content":"We know that AIC (Akaike Information Criterion) for Logistic Regression can be obtained by this formula :where k is number of estimated parameter and L is likelihood function.Now, I'm a bit confused about AIC for Geographically Weighted Logistic Regression since each data points have its own estimated parameter so it has own AIC. How I can get overall(for all data points) AIC for Geographically Weighted Logistic Regression?Thx","Creater_id":110482,"Start_date":"2016-07-03 02:04:34","Question_id":221888,"Tags":["logistic","aic","information"],"Answer_count":0,"Last_activity":"2016-08-31 04:23:22","Link":"http://stats.stackexchange.com/questions/221888/aic-for-geographically-weighted-logistic-regression","Creator_reputation":1}
{"_id":{"$oid":"5837a572a05283111e4d2aa3"},"View_count":23,"Display_name":"Indunil Ruhunuhewa","Question_score":1,"Question_content":"We have performed analyses by dividing the participants into 6 groups. The attribution of the groups was based on two variables: the Body Mass Index (BMI) and Metabolic syndrome (MetS):if BMI\u0026lt;25 and MetS=0 then group=1 if BMI\u0026lt;25 and MetS=1 then group=2if BMI\u003e30 and MetS=0 then group=3if BMI\u003e30 and MetS=1 then group=4etc... Next, the variable 'group' was predicted by baseline variables. Can we call this a stratified analysis? or should it be named as 'a combined analysis', though I'm not very familiar with the latter!     ","Creater_id":70548,"Start_date":"2016-08-31 01:40:45","Question_id":232619,"Tags":["regression","self-study","stratification"],"Answer_count":1,"Last_activity":"2016-08-31 04:18:09","Link":"http://stats.stackexchange.com/questions/232619/combined-analysis","Creator_reputation":106}
{"_id":{"$oid":"5837a572a05283111e4d2ab0"},"View_count":18,"Display_name":"Oliver","Question_score":3,"Question_content":"I work for an organisation that provides free activities that benefit people's health. We have a well maintained database that records participation and I am using it in order to research the active population, and their participation levels and trends.I am trying to show how people use the service over time. I have been able to show this for a small number of people, by using an XY scatter graph, giving each person a unique number that is plotted on the Y axis, then their activity day is plotted on the X axis (number of days since sign up). What this shows is that activity tends to be clumped (i.e. people do the activity for a short period of time then give up again).This is interesting, but I would like to be able to show it for a much larger group of people. However, visualising it in this way quickly gets messy for a much larger dataset. I was wondering if you would be able to suggest some alternative ways I could show it.I am already using a few graphs to show average activity level over time, but what is really interesting here is the 'clumping' of activity which I can't show by aggregating data. I have shown a few images below that should help with my above explanation. I will only be able to use free tools or MS Excel to visualise unfortunately. ","Creater_id":129604,"Start_date":"2016-08-31 03:57:52","Question_id":232644,"Tags":["data-visualization","dataset"],"Answer_count":0,"Last_activity":"2016-08-31 04:06:39","Link":"http://stats.stackexchange.com/questions/232644/visualising-activity","Creator_reputation":16}
{"_id":{"$oid":"5837a572a05283111e4d2ab2"},"View_count":14,"Display_name":"user129599","Question_score":1,"Question_content":"In a logistic regression I have beta, e^beta and the confidence intervals for both for a given linear predictor x1.For x1 = n, the odds ratio (compared to x1 = 0) equals e^(n*beta).But is there also an easy way to calculate the confidence intervals for odds ratios of x1s \u003e 1?If not, what not-so-easy ways are there?Thank you very much!","Creater_id":null,"Start_date":"2016-08-31 03:17:59","Question_id":232633,"Tags":["regression","logistic","confidence-interval","odds-ratio"],"Answer_count":0,"Last_activity":"2016-08-31 04:03:47","Link":"http://stats.stackexchange.com/questions/232633/how-to-calculate-confidence-intervals-of-odds-ratios-for-all-values-of-a-linear","Creator_reputation":null}
{"_id":{"$oid":"5837a572a05283111e4d2ab4"},"View_count":49,"Display_name":"Vladimir","Question_score":1,"Question_content":"If for example, I have 2 correlation values (for the same two variables in two different time periods) 0.5925 and 0.6018, with a difference 0.0093, is this enough to defend a stance saying that the relationship for both periods is \"the same\", and that the difference is small enough to ignore? A large amount of correlation values I see are in the order of magnitude of 0.x, which leads me to think I am able to make this assumption.","Creater_id":129574,"Start_date":"2016-08-31 00:30:04","Question_id":232608,"Tags":["hypothesis-testing","correlation","pearson"],"Answer_count":2,"Last_activity":"2016-08-31 04:02:38","Link":"http://stats.stackexchange.com/questions/232608/at-what-point-can-a-difference-between-correlations-be-ignored","Creator_reputation":6}
{"_id":{"$oid":"5837a572a05283111e4d2ac2"},"View_count":23070,"Display_name":"Idr","Question_score":6,"Question_content":"Is there a way to simplify this equation? \\dbinom{8}{1} + \\dbinom{8}{2} + \\dbinom{8}{3} + \\dbinom{8}{4} + \\dbinom{8}{5} + \\dbinom{8}{6} + \\dbinom{8}{7} + \\dbinom{8}{8}Or more generally,\\sum_{k=1}^{n}\\dbinom{n}{k}","Creater_id":7282,"Start_date":"2012-04-27 10:48:41","Question_id":27266,"Tags":["combinatorics"],"Answer_count":3,"Last_activity":"2016-08-31 03:51:25","Link":"http://stats.stackexchange.com/questions/27266/simplify-sum-of-combinations-with-same-n-all-possible-values-of-k","Creator_reputation":302}
{"_id":{"$oid":"5837a572a05283111e4d2ad1"},"View_count":708,"Display_name":"user12125","Question_score":3,"Question_content":"I have five additives that can be mixed into a chemical.  Each one can be mixed in at a discrete percentage from 0.02% to 0.22% of the chemical (i.e. 0.02%, 0.03%, 0.04%,...0.22%).  Each additive must be present, so mixed in at least 0.02%.  The restriction is that the sum of all percentages cannot exceed 0.3%, but the sum can be less than that, so long as each additive is present.  Without the restriction the answer is easy.  But I can't figure out how to eliminate the number of combinations that would exceed the maximum allowed.  If one additive is mixed at 0.22%, then the other four must be 0.02% each and only that.Thanks!","Creater_id":12125,"Start_date":"2012-06-21 06:35:35","Question_id":30875,"Tags":["multinomial","combinatorics"],"Answer_count":2,"Last_activity":"2016-08-31 03:50:08","Link":"http://stats.stackexchange.com/questions/30875/calculating-possible-combinations-with-restrictions","Creator_reputation":21}
{"_id":{"$oid":"5837a572a05283111e4d2adf"},"View_count":35,"Display_name":"gunjin","Question_score":1,"Question_content":"I found the description that Brownian random walk has the power spectrum on the dependency of 1/f^2 where f is its time frequency.I wonder why it is but couldn't find the proof there and also in some pages I googled.The question is why Brownian random walk has the power spectrum that depends on  the inverse of the square of its frequency.","Creater_id":129582,"Start_date":"2016-08-31 02:02:08","Question_id":232622,"Tags":["mathematical-statistics","spectral-analysis","brownian"],"Answer_count":1,"Last_activity":"2016-08-31 03:28:54","Link":"http://stats.stackexchange.com/questions/232622/intuitive-description-of-spectrum-of-brownian-random-walk-motion","Creator_reputation":8}
{"_id":{"$oid":"5837a572a05283111e4d2aec"},"View_count":11,"Display_name":"gareth","Question_score":1,"Question_content":"Hi  Is it possible to run a repeated measures one way ANCOVA?My data is from a cross-over design study, each subject followed 3 different diets.I can see from a repeated measures ANOVA that there is an effect of the diets on sweat rate (continuous variable). But i think this is influenced by resting core temperature. So i have this measured also across the 3 different conditions in each individual. Can i run a one-way repeated measures ANCOVA to account for the differences in resting core temperature?I'm running the analysis in SPSS as the 3 diets as the within-subject variable, and sweat rate as the measure, with the 3 core temperatures in as  Covariates. If the test of within-subject effects is p\u0026lt;0.05 for diet (sphericity assumed) (as it was with the one way repeated measures ANOVA).The 3 different diet*resting temperature within subject effects are all p\u003e0.05. Therefore the resting temperature is not impacting sweat rate...?Or i actually think this should really be a 2 way repeated measures anova? sorry for the basic questions, I'm new to this.","Creater_id":129600,"Start_date":"2016-08-31 03:20:38","Question_id":232635,"Tags":["ancova"],"Answer_count":0,"Last_activity":"2016-08-31 03:20:38","Link":"http://stats.stackexchange.com/questions/232635/one-way-repeated-measures-ancova","Creator_reputation":6}
{"_id":{"$oid":"5837a572a05283111e4d2aee"},"View_count":23,"Display_name":"Eka","Question_score":0,"Question_content":"I was searching for appliction of unsupervised learning in trading and came across this site. I have understood most part of the code but some I have no idea whats happening.This code is used for optimizing cluster number#optimal number of clusterswss = (nrow(nasa)-1)*sum(apply(nasa,2,var))for (i in 2:15) wss[i] = sum(kmeans(nasa, centers=i)cluster,lag(as.xts(kmeanObjectclusterand obtained last cluster values as 2 so what is this means?","Creater_id":15973,"Start_date":"2016-08-31 03:13:33","Question_id":232631,"Tags":["r","clustering","k-means","unsupervised-learning"],"Answer_count":0,"Last_activity":"2016-08-31 03:13:33","Link":"http://stats.stackexchange.com/questions/232631/k-means-clustering-optimization-and-autocorrelation","Creator_reputation":224}
{"_id":{"$oid":"5837a572a05283111e4d2af0"},"View_count":151,"Display_name":"Sangwoong Yoon","Question_score":6,"Question_content":"When learning a model , what is bad about maximize  instead of maximizing ?In a typical learning setting, a model is trained to maximize likelihood. Likelihood is probability of data given a model, and with IID assumption, it can be written as . This is what we usually do in statistics and machine learning.Here is an instructive question: Why shouldn't it be a summation of probabilities? I think chaining questions can be raised and it's worth contemplating upon. How bad if we perform learning with such a criterion?Is there any pathological or illustrative examples? Is ML always superior than the not-well-formulated sum-of-probabilities?","Creater_id":93401,"Start_date":"2016-08-30 23:29:18","Question_id":232604,"Tags":["probability","maximum-likelihood","likelihood"],"Answer_count":3,"Last_activity":"2016-08-31 02:52:46","Link":"http://stats.stackexchange.com/questions/232604/what-is-bad-about-maximizing-sum-ipx-i-theta-instead-of-maximizing","Creator_reputation":87}
{"_id":{"$oid":"5837a572a05283111e4d2aff"},"View_count":10578,"Display_name":"Richard Herron","Question_score":16,"Question_content":"I am trying to understand standard error \"clustering\" and how to execute in R (it is trivial in Stata). In R I have been unsuccessful using either plm or writing my own function. I'll use the diamonds data from the ggplot2 package.I can do fixed effects with either dummy variables\u0026gt; library(plyr)\u0026gt; library(ggplot2)\u0026gt; library(lmtest)\u0026gt; library(sandwich)\u0026gt; # with dummies to create fixed effects\u0026gt; fe.lsdv \u0026lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)\u0026gt; ct.lsdv \u0026lt;- coeftest(fe.lsdv, vcov. = vcovHC)\u0026gt; ct.lsdvt test of coefficients:                      Estimate Std. Error  t value  Pr(\u0026gt;|t|)    carat                 7871.082     24.892  316.207 \u0026lt; 2.2e-16 ***factor(cut)Fair      -3875.470     51.190  -75.707 \u0026lt; 2.2e-16 ***factor(cut)Good      -2755.138     26.570 -103.692 \u0026lt; 2.2e-16 ***factor(cut)Very Good -2365.334     20.548 -115.111 \u0026lt; 2.2e-16 ***factor(cut)Premium   -2436.393     21.172 -115.075 \u0026lt; 2.2e-16 ***factor(cut)Ideal     -2074.546     16.092 -128.920 \u0026lt; 2.2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 or by de-meaning both left- and right-hand sides (no time invariant regressors here) and correcting degrees of freedom.\u0026gt; # by demeaning with degrees of freedom correction\u0026gt; diamonds \u0026lt;- ddply(diamonds, .(cut), transform, price.dm = price - mean(price), carat.dm = carat  .... [TRUNCATED] \u0026gt; fe.dm \u0026lt;- lm(price.dm ~ carat.dm + 0, data = diamonds)\u0026gt; ct.dm \u0026lt;- coeftest(fe.dm, vcov. = vcovHC, df = nrow(diamonds) - 1 - 5)\u0026gt; ct.dmt test of coefficients:         Estimate Std. Error t value  Pr(\u0026gt;|t|)    carat.dm 7871.082     24.888  316.26 \u0026lt; 2.2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 I can't replicate these results with plm, because I don't have a \"time\" index (i.e., this isn't really a panel, just clusters that could have a common bias in their error terms).\u0026gt; plm.temp \u0026lt;- plm(price ~ carat, data = diamonds, index = \"cut\")duplicate couples (time-id)Error in pdim.default(index[[1]], index[[2]]) : I also tried to code my own covariance matrix with clustered standard error using Stata's explanation of their cluster option (explained here), which is to solve \\hat V_{cluster} = (X\u0026#39;X)^{-1} \\left( \\sum_{j=1}^{n_c} u_j\u0026#39;u_j \\right) (X\u0026#39;X)^{-1} where ,  si the number of clusters,  is the residual for the  observation and  is the row vector of predictors, including the constant (this also appears as equation (7.22) in Wooldridge's Cross Section and Panel Data). But the following code gives very large covariance matrices. Are these very large values given the small number of clusters I have? Given that I can't get plm to do clusters on one factor, I'm not sure how to benchmark my code.\u0026gt; # with cluster robust se\u0026gt; lm.temp \u0026lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)\u0026gt; \u0026gt; # using the model that Stata uses\u0026gt; stata.clustering \u0026lt;- function(x, clu, res) {+     x \u0026lt;- as.matrix(x)+     clu \u0026lt;- as.vector(clu)+     res \u0026lt;- as.vector(res)+     fac \u0026lt;- unique(clu)+     num.fac \u0026lt;- length(fac)+     num.reg \u0026lt;- ncol(x)+     u \u0026lt;- matrix(NA, nrow = num.fac, ncol = num.reg)+     meat \u0026lt;- matrix(NA, nrow = num.reg, ncol = num.reg)+     +     # outer terms (X'X)^-1+     outer \u0026lt;- solve(t(x) %*% x)+ +     # inner term sum_j u_j'u_j where u_j = sum_i e_i * x_i+     for (i in seq(num.fac)) {+         index.loop \u0026lt;- clu == fac[i]+         res.loop \u0026lt;- res[index.loop]+         x.loop \u0026lt;- x[clu == fac[i], ]+         u[i, ] \u0026lt;- as.vector(colSums(res.loop * x.loop))+     }+     inner \u0026lt;- t(u) %*% u+ +     # +     V \u0026lt;- outer %*% inner %*% outer+     return(V)+ }\u0026gt; x.temp \u0026lt;- data.frame(const = 1, diamonds[, \"carat\"])\u0026gt; summary(lm.temp)Call:lm(formula = price ~ carat + factor(cut) + 0, data = diamonds)Residuals:     Min       1Q   Median       3Q      Max -17540.7   -791.6    -37.6    522.1  12721.4 Coefficients:                     Estimate Std. Error t value Pr(\u0026gt;|t|)    carat                 7871.08      13.98   563.0   \u0026lt;2e-16 ***factor(cut)Fair      -3875.47      40.41   -95.9   \u0026lt;2e-16 ***factor(cut)Good      -2755.14      24.63  -111.9   \u0026lt;2e-16 ***factor(cut)Very Good -2365.33      17.78  -133.0   \u0026lt;2e-16 ***factor(cut)Premium   -2436.39      17.92  -136.0   \u0026lt;2e-16 ***factor(cut)Ideal     -2074.55      14.23  -145.8   \u0026lt;2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 1511 on 53934 degrees of freedomMultiple R-squared: 0.9272, Adjusted R-squared: 0.9272 F-statistic: 1.145e+05 on 6 and 53934 DF,  p-value: \u0026lt; 2.2e-16 \u0026gt; stata.clustering(x = x.temp, clu = diamondsresiduals)                        const diamonds....carat..const                11352.64           -14227.44diamonds....carat.. -14227.44            17830.22Can this be done in R? It is a fairly common technique in econometrics (there's a brief tutorial in this lecture), but I can't figure it out in R. Thanks!","Creater_id":1445,"Start_date":"2011-04-26 19:34:11","Question_id":10017,"Tags":["r","panel-data","standard-error","fixed-effects-model","clustered-standard-errors"],"Answer_count":3,"Last_activity":"2016-08-31 02:50:46","Link":"http://stats.stackexchange.com/questions/10017/standard-error-clustering-in-r-either-manually-or-in-plm","Creator_reputation":453}
{"_id":{"$oid":"5837a572a05283111e4d2b0e"},"View_count":21,"Display_name":"user964689","Question_score":2,"Question_content":"I want to calculate if the percentage of sharing of strings (genes in this case) between lists is significantly more than expected by chance for multiple pairwise comparisons between lists of strings. I know there are hypergeometric tests one can do online for this but they ask you to input the number of genes in the genome as a backgroun. My lists each come from slightly different subsets of the genome, so using a single overall genome size for this test is innappropriate. I think I will have more accurate results if I subsample from the specific groups of genes that I have used to create each list. I have multiple lists of strings of different size. I have manually calculated the percentage pairwise sharing of strings between each list. For example for list A and list B I count how many strings occur in both lists then calculate what percentage this is of the combined size of list A and list B.Toy example:List A:[1,] \"EDA\"[2,] \"MGN2\"  [3,] \"5RSK\"      [4,] \"TOLL4\"[5,] \"BBOX\"List B:[1,] \"EDA\"[2,] \"TOLL4\"  [3,] \"KAR2\"      [4,] \"HARL3\"Here two strings (EDA, TOLL4) are shared between the two lists. There are 9 strings in total (list A + list B), so the percentage sharing between lists is 22.2% (is this an appropriate way to calculate percentage sharing between two lists?)I have made the same calculation of percentage pairwise sharing (maybe overlap is a better term) between several other lists. I now want to test whether this overlap is significant using a resampling approach. I would appreciate any help.Each of these lists is itself a subsample from a larger list. For example list A is a sample of 5 strings from a larger group A (size : 100 strings). List B is a subsample of 4 strings from group B (size : 200 strings) and so on for each list. Therefore to calculate if the sharing I observe between list A and list B is significantly more than expected by chance I think I need to randomly sample 5 strings from group A and 4 strings from group B (without replacement), then calculate the percentage overlap between these two randomly sampled lists. Then if I repeat this process 100 times I can create a distribution of percentage overlap scores and compare this to my observed overlap of 22.2%.I have some previous code that I was using to do overlaps from multiple combined lists but I don't think it will scale well to doing this process for multiple pairwise comparisons, eg List A - List B, List A - List C, List B - List C, etc. This can be seen at How to create a loop to repeat random sampling procedure in RI would appreciate any help on calculating these pairwise percentage sharing values, or advice on how to make my question clearer.","Creater_id":91434,"Start_date":"2016-08-31 02:35:54","Question_id":232627,"Tags":["r","sampling","resampling","percentage","hypergeometric"],"Answer_count":0,"Last_activity":"2016-08-31 02:35:54","Link":"http://stats.stackexchange.com/questions/232627/want-to-calculate-significance-of-pairwise-sharing-between-lists-standard-hyper","Creator_reputation":33}
{"_id":{"$oid":"5837a572a05283111e4d2b10"},"View_count":29,"Display_name":"Brianknell","Question_score":0,"Question_content":"I have decomposed a multimodal distribution into the constituent single distributions for for further analysis. I have spent some time researching various approaches and I have not found one that that is mature and available on CRAN. Currently I have assumed each identifiable mode is the centroid of a Gaussian distribution for which I calculate the SD . I then removed this  distribution from the data  and repeated the process with the remaining data, this process can be iterative. There are issues relating to estimating the actual SD in the case of significant distribution overlap,and judging distribution symmetry. I attach the data with an example. Whilst the separated single mode distributions make sense within the context of my problem I am very uneasy with the accuracy of the method.Any suggestions would be welcome. I have looked at Stack exchange and not found anything specific.]2]3","Creater_id":114244,"Start_date":"2016-08-30 13:25:53","Question_id":232555,"Tags":["gaussian-mixture"],"Answer_count":0,"Last_activity":"2016-08-31 02:29:22","Link":"http://stats.stackexchange.com/questions/232555/decomposition-of-multimodal-distributions","Creator_reputation":11}
{"_id":{"$oid":"5837a572a05283111e4d2b12"},"View_count":57,"Display_name":"coldcola","Question_score":1,"Question_content":"As written in the title, I am looking for the stability condition of a vector error correction model (VECM). I have found this phrase:   the companion matrix of a VECM with 𝐾 endogenous variables and 𝑟 cointegrating equations has 𝐾 − 𝑟  unit eigenvalues. If the process is stable, the moduli of the remaining 𝑟 eigenvalues are strictly less than one. Source: http://www.stata.com/manuals13/tsvecintro.pdfIs this the correct condition? If so, could anyone tell me how we define \"the companion matrix of a VECM\"? Is it the coefficient matrix of X_(t-1) ? In the end, how could I test this condition in R? Is there a package to do this?","Creater_id":129586,"Start_date":"2016-08-29 08:37:40","Question_id":232413,"Tags":["r","time-series","forecasting","vecm"],"Answer_count":1,"Last_activity":"2016-08-31 02:07:01","Link":"http://stats.stackexchange.com/questions/232413/what-is-the-stability-condition-of-a-vector-error-correction-model-vecm","Creator_reputation":6}
{"_id":{"$oid":"5837a572a05283111e4d2b1f"},"View_count":47,"Display_name":"Michal","Question_score":3,"Question_content":"I have series of logarithmic prices which seems to be mean reverting (adf.test from the \"tseries\" package in R gives -3.6 value).Based on that I create an AR(1) process y_t=y_{t-1} + \\mu + \\lambda y_{t-1} + \\sigma e_{t-1}where ,  and  are derived from the historical prices (from regression coefficients;  is about -0.02).Then I run simulation and create  paths which I test for mean reversion and most of them do not get the  rejected (no stationarity, no mean reversion).I am confused with the results.Shouldn't the paths generated on the parameters of the mean reverting series be also mean reverting series at specified confidence level? Are the critical levels for the ADF Test the same in case of log prices? (My sample size was 500 and I used -2.57 for 10% confidence level)?There are the parameters I get for the historical setCall:lm(formula = dy ~ y)Residuals:      Min        1Q    Median        3Q       Max -0.091832 -0.009408  0.000525  0.009208  0.071752 Coefficients:             Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)  0.074370   0.020870   3.563 0.000380y           -0.020714   0.005811  -3.564 0.000379---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.01599 on 1214 degrees of freedomMultiple R-squared:  0.01036,   Adjusted R-squared:  0.009542 F-statistic:  12.7 on 1 and 1214 DF,  p-value: 0.0003789R tseries package: Augmented Dickey-Fuller Testdata:  log(data$Adj.Close)Dickey-Fuller = -3.6157, Lag order = 1, p-value = 0.03117alternative hypothesis: stationary","Creater_id":116230,"Start_date":"2016-08-30 20:31:43","Question_id":232591,"Tags":["time-series","augmented-dickey-fuller"],"Answer_count":0,"Last_activity":"2016-08-31 01:58:09","Link":"http://stats.stackexchange.com/questions/232591/adf-test-on-simulated-stationary-ar1-processes-fails-to-reject","Creator_reputation":153}
{"_id":{"$oid":"5837a572a05283111e4d2b21"},"View_count":22,"Display_name":"Kuti","Question_score":1,"Question_content":"I do encounter the following terms in a number of academic journal papers:  \"For a softening process,  the third order Hermite functional transformation is ...\"Efficient stationary multivariate non-Gaussian simulation based on a Hermite PDF modelSimilarly, for instance, the following:  \"This study presents a moment-based  translation model for hardening non-Gaussian processes with kurtosis less than three.\"Moment-Based Translation Model for Hardening Non-Gaussian Response ProcessesWhat do softening/hardening a process mean?","Creater_id":116458,"Start_date":"2016-08-30 10:36:15","Question_id":232530,"Tags":["gaussian-process"],"Answer_count":1,"Last_activity":"2016-08-31 01:55:55","Link":"http://stats.stackexchange.com/questions/232530/what-is-the-meaning-of-softening-process","Creator_reputation":23}
{"_id":{"$oid":"5837a572a05283111e4d2b2e"},"View_count":29,"Display_name":"user8170","Question_score":0,"Question_content":"I am trying to understand some code somebody has written.I have a y vector which is a factor say book to price for 500 companies. Then I also have a x vector of the same shape which is just ones.A OLS regression is performed. The beta of the x variable is saved and called coeff.The residual is then calculate as   residual = y - (x * coeff)The residuals are then sorted and then ranked be being passed into a normal inverse function which gives us our final factor.I do not follow what is going on here? Why is a factor being regressed against a vector of ones?","Creater_id":123757,"Start_date":"2016-08-31 01:21:28","Question_id":232615,"Tags":["regression","regression-coefficients","deming-regression"],"Answer_count":1,"Last_activity":"2016-08-31 01:24:08","Link":"http://stats.stackexchange.com/questions/232615/why-regress-a-y-variable-on-a-vector-of-just-ones","Creator_reputation":13}
{"_id":{"$oid":"5837a572a05283111e4d2b3b"},"View_count":6107,"Display_name":"tronbabylove","Question_score":20,"Question_content":"A player is given a fair, six-sided die. To win, she must roll a number greater than 4 (i.e., a 5 or a 6). If she rolls a 4, she must roll again. What are her odds of winning?I think the probability of winning , can be expressed recursively as:P(W) = P(r = 5 \\cup r = 6) + P(r = 4) \\cdot P(W) I've approximated  as  by running 1 million trials in Java, like this:import java.util.Random;public class Dice {    public static void main(String[] args) {        int runs = 1000000000;        int wins = 0;        for (int i = 0; i \u0026lt; runs; i++) {            wins += playGame();        }        System.out.println(wins / (double)runs);    }    static Random r = new Random();    private static int playGame() {        int roll;        while ((roll = r.nextInt(6) + 1) == 4);        return (roll == 5 || roll == 6) ? 1 : 0;    }}And I see that one could expand  like this:P(W) = \\frac{1}{3} + \\frac{1}{6} \\left(\\frac{1}{3} + \\frac{1}{6}\\left(\\frac{1}{3} + \\frac{1}{6}\\right)\\right)...But I don't know how to solve this type of recurrence relation without resorting to this sort of approximation. Is it possible?","Creater_id":55745,"Start_date":"2016-08-27 17:57:32","Question_id":232106,"Tags":["probability"],"Answer_count":5,"Last_activity":"2016-08-31 00:39:33","Link":"http://stats.stackexchange.com/questions/232106/roll-a-die-until-it-lands-on-any-number-other-than-4-what-is-the-probability-th","Creator_reputation":203}
{"_id":{"$oid":"5837a572a05283111e4d2b4c"},"View_count":13,"Display_name":"isabelle","Question_score":1,"Question_content":"i need your help with logistic regression analysis. My dependent variable is a dummy variable: 0 fails 1 successful.Method: ML - Binary Logit  (Newton-Raphson / Marquardt steps)variables and coefficient arec=             -1.380208comment =      -6.36E-05updates=        0.291811category=      -0.017782obs with Dep : 0=    109 557               1=     76 658My category is 13 that i also code in dummy variables and consider as control variables.others element are good, p-value , z statistic , McFadden R-squared....Now is how to interpret and know which variables improve success and which one reduce success.","Creater_id":129575,"Start_date":"2016-08-31 00:37:08","Question_id":232610,"Tags":["z-test"],"Answer_count":0,"Last_activity":"2016-08-31 00:37:08","Link":"http://stats.stackexchange.com/questions/232610/how-to-interprete-logistic-regresion-using-eviews-having-dummy-variables","Creator_reputation":6}
{"_id":{"$oid":"5837a572a05283111e4d2b4e"},"View_count":35,"Display_name":"Fraz","Question_score":0,"Question_content":"I am trying to learn the fields of bayesian non-parametric approaches. I am going thru this manuscript: http://mayagupta.org/publications/FrigyikKapilaGuptaIntroToDirichlet.pdfI am bit stuck with: Proof of Neutrality for the DirichletBasically (page 11), the context is why the stick breaking process generates random vectors from dirichlet distribution.So, the first challenge for me is to understand the proof? What is the connection of introducing these weird forms of Y and then how does proving that dirichlet is neutral distribution ties to the proof that stick breaking process generates rv from dirichlet distribution?Thanks","Creater_id":7891,"Start_date":"2016-08-30 00:50:35","Question_id":232435,"Tags":["distributions","dirichlet-distribution","nonparametric-bayes"],"Answer_count":1,"Last_activity":"2016-08-31 00:36:32","Link":"http://stats.stackexchange.com/questions/232435/proof-of-neutrality-for-dirichlet-distribution","Creator_reputation":147}
{"_id":{"$oid":"5837a572a05283111e4d2b5b"},"View_count":163,"Display_name":"ssdecontrol","Question_score":1,"Question_content":"I ran a Multiple Factor Analysis on a data set with 3,924 rows and 96 columns, of which six are (unordered) categorical, with 12-14 categories in each, and the rest are numeric, mean-centered and scaled by one-standard-deviation. My goal is dimension reduction, in order to visualize the results of PAM clustering by plotting the first two or three dimensions and coloring the points by assigned partition, as well as highlighting each medoid.I found that no one dimension of PCA space explains more than a small fraction of variance in the data:       eigenvalue percentage of variance cumulative percentage of variancecomp 1  1.0350075               2.466873                          2.466873comp 2  0.8243004               1.964666                          4.431539comp 3  0.8093599               1.929057                          6.360596comp 4  0.7587070               1.808329                          8.168924comp 5  0.6495978               1.548274                          9.717198comp 6  0.6328384               1.508329                         11.225527What should I make of this situation? Can I still use the first two PCA dimensions as a quick 2D approximation of the data set, or will they just fail to represent the data accurately?Is there an alternative dimension reduction technique I could/should use? All of the reviews of nonlinear dimension reduction I've read were somewhat equivocal on their usefulness compared to PCA, except on fabricated data like the swiss roll data set, so I've been hesitant to use them.Edit: here are the PCA results from just the numerical variables:        eigenvalue percentage of variance cumulative percentage of variancecomp 1   5.1704992              5.7449991                          5.744999comp 2   4.0469449              4.4966055                         10.241605comp 3   3.8800122              4.3111247                         14.552729comp 4   3.0606430              3.4007144                         17.953444comp 5   2.7176048              3.0195609                         20.973005comp 6   2.4725503              2.7472781                         23.720283","Creater_id":36229,"Start_date":"2015-04-08 19:37:55","Question_id":145441,"Tags":["variance","pca","dimensionality-reduction"],"Answer_count":1,"Last_activity":"2016-08-31 00:22:22","Link":"http://stats.stackexchange.com/questions/145441/pca-mfa-for-graphical-dimension-reduction-what-to-do-with-very-small-explaine","Creator_reputation":5522}
{"_id":{"$oid":"5837a572a05283111e4d2b68"},"View_count":110,"Display_name":"William","Question_score":3,"Question_content":"Question: How can this be the case? Why are Kalman filters so much more complicated than any other Bayesian network? Are there any Bayesian networks which are intermediate in complexity?Perhaps one way to answer this question would be to state or give a reference to the simplest possible model of a Kalman filter possible.Background:I am only familiar with Kalman filters with regards to literature about stationary stochastic processes in continuous time, in which they appear to be immensely complicated objects which I frankly don't understand. From the description given in Loeve and Shiryaev, where it is referred to as the Kalman-Bucy filter and is more closely associated with signal processing in continuous time and topics like orthogonal Karhunen-Loeve expansion (which again I don't understand yet).However, all of the other Bayesian networks I am familiar are discrete-time, discrete state-space objects that one probably could learn about during the first or second semester of a first introduction to probability theory. This is obviously not true of the Kalman-Bucy filter in Shiryaev.Yet Wikipedia states that Kalman filters are a special case of dynamic Bayesian networks.","Creater_id":113090,"Start_date":"2016-08-30 21:19:46","Question_id":232594,"Tags":["machine-learning","bayesian-network","kalman-filter"],"Answer_count":1,"Last_activity":"2016-08-30 23:22:13","Link":"http://stats.stackexchange.com/questions/232594/why-is-the-kalman-filter-a-specific-case-of-a-dynamic-bayesian-network","Creator_reputation":1826}
{"_id":{"$oid":"5837a572a05283111e4d2b75"},"View_count":45,"Display_name":"Lin Ma","Question_score":0,"Question_content":"I am using Python 2.7, tried many times, but never produce result 50 during 100 times test.import random# fake a Bernoulli distribution with P(1) = 0.3def Bernoulli():    if random.random() \u0026lt;= 0.3:        return 1    else:        return 0def TossCoin():    x1 = Bernoulli()    y1 = x1    while y1 == x1:        y1 = Bernoulli()    return x1 * (1-y1)if __name__ == \"__main__\":    count = 100    result = 0    while count \u0026gt; 0:        count -= 1        result += TossCoin()    print resultregards,Lin","Creater_id":18254,"Start_date":"2016-08-30 22:34:50","Question_id":232596,"Tags":["probability","python","bernoulli-distribution"],"Answer_count":2,"Last_activity":"2016-08-30 23:21:31","Link":"http://stats.stackexchange.com/questions/232596/did-my-solution-produce-0-and-1-with-equal-probability","Creator_reputation":108}
{"_id":{"$oid":"5837a572a05283111e4d2b83"},"View_count":30,"Display_name":"J.r. Paladines","Question_score":4,"Question_content":"I have read some paper that expresses that \"recent works\" show we can use a VAR model with raw data I(1) but there has to be cointegration. This means that there is no reason to difference the data for VAR modelling. Any paper reference about this?","Creater_id":129545,"Start_date":"2016-08-30 16:22:47","Question_id":232573,"Tags":["references","var","cointegration"],"Answer_count":0,"Last_activity":"2016-08-30 22:41:21","Link":"http://stats.stackexchange.com/questions/232573/var-in-levels-for-cointegrated-data","Creator_reputation":21}
{"_id":{"$oid":"5837a572a05283111e4d2b85"},"View_count":12489,"Display_name":"levesque","Question_score":8,"Question_content":"My problem with understanding this expression might come from the fact that English is not my first language, but I don't understand why it's used in this way.The marginal mean is typically the mean of a group or subgroup's measures of a variable in an experiment, but why not just use the word mean? What's the marginal here for?See the definition of marginal from wiktionary. ","Creater_id":1320,"Start_date":"2010-11-05 07:54:16","Question_id":4245,"Tags":["terminology","marginal"],"Answer_count":3,"Last_activity":"2016-08-30 22:24:35","Link":"http://stats.stackexchange.com/questions/4245/what-is-the-meaning-of-marginal-mean","Creator_reputation":380}
{"_id":{"$oid":"5837a572a05283111e4d2b94"},"View_count":21,"Display_name":"SmonJunior","Question_score":3,"Question_content":"I am just beginning to learn about Mixture of Experts Models, so I apologize if parts of my question are elementary.My understanding is that a MEM is a mixture model where the components are linear regressions. So, in very informal terms, we iteratively calculate the probability (pi) of picking a given model i to predict y=f(X) (where X is a vector of features) AND the value of the parameters (held in vector theta)that minimize a cost function of the form:C=E(pi*(y-theta*X)^2)... or one that accomplishes the same goal.The idea is to make certain regression functions specialize on certain regions of the input space. EM method will bring us to local minima eventually and we're on our way to understanding the distribution of our dataset!1) But, what happens if our feature space (vector X) is high dimension? That is, what do we do in situations where regularization would normally be used to bring down the number of features in our regression? Better yet, how do we select the feature space at all?2) Are MEMs used for large p datasets in practice?Thanks for any help you can provide (I would be very grateful for directions to articles that might help as well).","Creater_id":129231,"Start_date":"2016-08-30 18:59:23","Question_id":232583,"Tags":["feature-selection","latent-class","finite-mixture-model"],"Answer_count":0,"Last_activity":"2016-08-30 21:27:06","Link":"http://stats.stackexchange.com/questions/232583/feature-selection-in-mixture-of-experts-model","Creator_reputation":16}
{"_id":{"$oid":"5837a572a05283111e4d2b96"},"View_count":27,"Display_name":"borjasanz","Question_score":2,"Question_content":"I have a number of groups with monthly data from 2010 to 2016. It's over 80 groups. I succesfully ran an ARIMA model with the montly data but with the sales data summed up (without groups). Now I'd like to compare the performance with a per group model that runs an ARIMA model for each group and maybe later consider another type of grouping (geographical location, clustering, etc.)I ran my original model with the following code:        Datos \u0026lt;- read.csv(\"C:/Users/borja.sanz/Desktop/Borja/Forecasting/V`enter code here`entas/Datos para Forecast.csv\")        options(scipen=999)        library(lubridate)        DatosFecha)        #Declare time series        tsDatos\u0026lt;-ts(Datosmean,                             abajo=f_aaupper[,2],                             date=last_date + seq(1/12, 3, by=1/12))    forecast_dfThis is how my data looks like:       Group    Year    Month   Date    Sales1   2010    1   1/01/2010   134536.6251   2010    2   1/02/2010   117506.6251   2010    3   1/03/2010   132153.751   2010    4   1/04/2010   129723.1251   2010    5   1/05/2010   135834.51   2010    6   1/06/2010   130115.3751   2010    7   1/07/2010   1447161   2010    8   1/08/2010   1371951   2010    9   1/09/2010   137522.8751   2010    10  1/10/2010   1870631   2010    11  1/11/2010   162002.751   2010    12  1/12/2010   262297.3751   2011    1   1/01/2011   177291.251   2011    2   1/02/2011   1548161   2011    3   1/03/2011   171231.1251   2011    4   1/04/2011   2177171   2011    5   1/05/2011   178767.751   2011    6   1/06/2011   180817.751   2011    7   1/07/2011   216927.1251   2011    8   1/08/2011   204509.1251   2011    9   1/09/2011   199449.51   2011    10  1/10/2011   243812.1251   2011    11  1/11/2011   232135.8751   2011    12  1/12/2011   330854.751   2012    1   1/01/2012   217123.8751   2012    2   1/02/2012   2005581   2012    3   1/03/2012   215689.51   2012    4   1/04/2012   245500.251   2012    5   1/05/2012   219687.251   2012    6   1/06/2012   243345.6251   2012    7   1/07/2012   2490421   2012    8   1/08/2012   198443.751   2012    9   1/09/2012   209157.3751   2012    10  1/10/2012   2340891   2012    11  1/11/2012   2375311   2012    12  1/12/2012   365301.251   2013    1   1/01/2013   211129.3751   2013    2   1/02/2013   185249.6251   2013    3   1/03/2013   256565.6251   2013    4   1/04/2013   183549.51   2013    5   1/05/2013   189698.251   2013    6   1/06/2013   207955.6251   2013    7   1/07/2013   230764.1251   2013    8   1/08/2013   212551.6251   2013    9   1/09/2013   201329.51   2013    10  1/10/2013   242745.1251   2013    11  1/11/2013   261893.3751   2013    12  1/12/2013   418313.251   2014    1   1/01/2014   205532.751   2014    2   1/02/2014   170487.751   2014    3   1/03/2014   1960771   2014    4   1/04/2014   221760.8751   2014    5   1/05/2014   1981851   2014    6   1/06/2014   204919.251   2014    7   1/07/2014   218972.751   2014    8   1/08/2014   221439.8751   2014    9   1/09/2014   195888.3751   2014    10  1/10/2014   234595.751   2014    11  1/11/2014   259712.8751   2014    12  1/12/2014   355691.8751   2015    1   1/01/2015   205156.251   2015    2   1/02/2015   185358.8751   2015    3   1/03/2015   218555.751   2015    4   1/04/2015   204233.6251   2015    5   1/05/2015   212160.6251   2015    6   1/06/2015   207217.251   2015    7   1/07/2015   225723.751   2015    8   1/08/2015   205902.6251   2015    9   1/09/2015   196940.6251   2015    10  1/10/2015   2509161   2015    11  1/11/2015   236835.1251   2015    12  1/12/2015   358327.6252   2010    1   1/01/2010   227175.8752   2010    2   1/02/2010   2050422   2010    3   1/03/2010   239206.3752   2010    4   1/04/2010   212059.8752   2010    5   1/05/2010   2327892   2010    6   1/06/2010   247876.1252   2010    7   1/07/2010   2785572   2010    8   1/08/2010   270410.1252   2010    9   1/09/2010   251060.3752   2010    10  1/10/2010   302738.6252   2010    11  1/11/2010   266869.752   2010    12  1/12/2010   272978.752   2011    1   1/01/2011   238614.52   2011    2   1/02/2011   224240.3752   2011    3   1/03/2011   245457.3752   2011    4   1/04/2011   238583.52   2011    5   1/05/2011   252392.752   2011    6   1/06/2011   256749.52   2011    7   1/07/2011   264736.1252   2011    8   1/08/2011   2564142   2011    9   1/09/2011   242335.1252   2011    10  1/10/2011   305224.752   2011    11  1/11/2011   289199.8752   2011    12  1/12/2011   281807.752   2012    1   1/01/2012   244886.1252   2012    2   1/02/2012   232062.3752   2012    3   1/03/2012   264991.752   2012    4   1/04/2012   232750.52   2012    5   1/05/2012   248498.3752   2012    6   1/06/2012   264290.8752   2012    7   1/07/2012   272689.752   2012    8   1/08/2012   260441.252   2012    9   1/09/2012   251852.3752   2012    10  1/10/2012   305929.6252   2012    11  1/11/2012   276711.6252   2012    12  1/12/2012   278672.8752   2013    1   1/01/2013   242613.8752   2013    2   1/02/2013   227575.752   2013    3   1/03/2013   250318.8752   2013    4   1/04/2013   250150.3752   2013    5   1/05/2013   258467.252   2013    6   1/06/2013   261359.252   2013    7   1/07/2013   279113.752   2013    8   1/08/2013   2586992   2013    9   1/09/2013   244841.3752   2013    10  1/10/2013   308197.252   2013    11  1/11/2013   284195.52   2013    12  1/12/2013   287718.752   2014    1   1/01/2014   239510.3752   2014    2   1/02/2014   216338.1252   2014    3   1/03/2014   245626.752   2014    4   1/04/2014   230619.8752   2014    5   1/05/2014   251758.8752   2014    6   1/06/2014   254946.752   2014    7   1/07/2014   276268.752   2014    8   1/08/2014   266151.752   2014    9   1/09/2014   245859.3752   2014    10  1/10/2014   317797.52   2014    11  1/11/2014   283786.6252   2014    12  1/12/2014   289767.8752   2015    1   1/01/2015   2440082   2015    2   1/02/2015   2286382   2015    3   1/03/2015   2600562   2015    4   1/04/2015   232560.8752   2015    5   1/05/2015   252642.1252   2015    6   1/06/2015   249018.52   2015    7   1/07/2015   278113.1252   2015    8   1/08/2015   2558512   2015    9   1/09/2015   263046.6252   2015    10  1/10/2015   344240.752   2015    11  1/11/2015   295486.1252   2015    12  1/12/2015   293499.375I only included two groups in the sample. I would like to use a function like one of the apply (tapply, lapply, sapply, etc.) that can run an AUTO.ARIMA model per group. Then I would like to obtain the forecast for each group for x number of months and also if I could visualize the model coefficients.","Creater_id":116784,"Start_date":"2016-08-30 20:16:29","Question_id":232590,"Tags":["r","arima"],"Answer_count":0,"Last_activity":"2016-08-30 20:16:29","Link":"http://stats.stackexchange.com/questions/232590/run-an-arima-model-by-group","Creator_reputation":16}
{"_id":{"$oid":"5837a572a05283111e4d2b98"},"View_count":702,"Display_name":"Constantin","Question_score":9,"Question_content":"While these two ubiquitous terms are often used synonymously, there sometimes seems to be a distinction. Is there indeed a difference, or are they exactly synonymous?","Creater_id":60577,"Start_date":"2015-01-14 08:27:21","Question_id":133389,"Tags":["residuals","error","terminology"],"Answer_count":2,"Last_activity":"2016-08-30 19:59:48","Link":"http://stats.stackexchange.com/questions/133389/what-is-the-difference-between-errors-and-residuals","Creator_reputation":259}
{"_id":{"$oid":"5837a572a05283111e4d2ba6"},"View_count":116,"Display_name":"Munichong","Question_score":2,"Question_content":"I am using Keras to do a machine learning task:Let's say I want to predict the time that a user spends on a product page. Each training case is a partial user visit session. One single user may have multiple sessions. The lengths of sessions are variable.The figure shows a user session. Xn is the feature vector of the nth product page, while yn is the user's reading time on the product page. Given the previous time steps, I want to predict the time that the user will spend on the current page.For example, in the test dataset, given X1, X2, and y1, I want to predict y2. Then after observing actual y2, given X1, X2, X3, and y1, y2, I then predict y3. The key here is that this is like an online prediction: At t=3, the actual  y1 and y2 is already observed. So the observed values, not the predicted ones, are used to predict y3 My question:Given this tutorial, I am not sure whether my case should be 'many to one' or 'many to many'. If 'many to one', there is nowhere to put y1 and y2. I think y1 and y2 should be able to partially determine y3. It is possible to add y1 and y2 as one input feature in X1 and X2. But there is no way to build X3 because y3 is the prediction.Onthe other hand, for 'many to many', I do not need the prediction of y1 and y2 because they should be known when predicting y3.","Creater_id":35802,"Start_date":"2016-08-30 09:38:18","Question_id":232519,"Tags":["machine-learning","neural-networks","rnn"],"Answer_count":0,"Last_activity":"2016-08-30 19:46:47","Link":"http://stats.stackexchange.com/questions/232519/how-should-i-use-recurrent-neural-network-to-model-this-problem","Creator_reputation":241}
{"_id":{"$oid":"5837a572a05283111e4d2ba8"},"View_count":19312,"Display_name":"guestoeijreor","Question_score":88,"Question_content":"Suppose I want to see whether my data is exponential based on a histogram (i.e. skewed to the right). Depending on how I group or bin the data, I can get wildly different histograms. One set of histograms will make is seem that the data is exponential. Another set will make it seem that data are not exponential. How do I make determining distributions from histograms well defined?","Creater_id":21738,"Start_date":"2013-03-08 10:58:55","Question_id":51718,"Tags":["distributions","data-visualization","histogram","binning"],"Answer_count":4,"Last_activity":"2016-08-30 19:44:18","Link":"http://stats.stackexchange.com/questions/51718/assessing-approximate-distribution-of-data-based-on-a-histogram","Creator_reputation":441}
{"_id":{"$oid":"5837a572a05283111e4d2bb8"},"View_count":415,"Display_name":"mathieu","Question_score":7,"Question_content":"I believe that the title of this question says it all. ","Creater_id":129490,"Start_date":"2016-08-30 07:24:12","Question_id":232500,"Tags":["clustering","k-means","high-dimensional"],"Answer_count":2,"Last_activity":"2016-08-30 19:29:07","Link":"http://stats.stackexchange.com/questions/232500/how-do-i-know-my-k-means-clustering-algorithm-is-suffering-from-the-curse-of-dim","Creator_reputation":136}
{"_id":{"$oid":"5837a572a05283111e4d2bc6"},"View_count":32,"Display_name":"docyang100","Question_score":2,"Question_content":"I have a raster stack of 19 layers called \"raster_bio\". The code to do PCA analysis is:  vv     \u0026lt;- getValues(raster_bio)my.prc \u0026lt;- prcomp(na.omit(vv), center=TRUE, scale=TRUE)# Then I selected first 5 PCs and did varimax rotation.varima \u0026lt;- varimax(my.prcloadingsLoadings:      PC1    PC2    PC3    PC4    PC5   bio1          0.350 -0.275              bio2                 0.194         0.658bio3         -0.134 -0.297         0.573bio4          0.240  0.426              bio5          0.505                     bio6          0.141 -0.418              bio7          0.221  0.430              bio8  -0.436  0.394 -0.149        -0.187bio9   0.369  0.163 -0.189         0.275bio10         0.502                     bio11         0.145 -0.415              bio12  0.180                0.361       bio13                       0.537       bio14  0.405                            bio15 -0.291         0.114         0.322bio16                       0.520       bio17  0.369                            bio18 -0.199                0.523       bio19  0.443                                             PC1   PC2   PC3   PC4   PC5SS loadings    1.000 1.000 1.000 1.000 1.000Proportion Var 0.053 0.053 0.053 0.053 0.053Cumulative Var 0.053 0.105 0.158 0.211 0.263$rotmat           [,1]         [,2]        [,3]        [,4]       [,5][1,]  0.6108976 -0.003008026 -0.62556569  0.45759088 -0.1614720[2,]  0.2354179  0.818121480 -0.01836594 -0.43651780 -0.2904661[3,]  0.6128850 -0.321653016  0.63329819 -0.07390564 -0.3382051[4,] -0.2685337  0.369912143  0.36408301  0.74586160 -0.3196696[5,]  0.3516307  0.300620258  0.27332622  0.19568143  0.8203565 \u0026gt; pca_rotatedPrincipal Components AnalysisCall: psych::principal(r = na.omit(vv), nfactors = 5, rotate = \"varimax\", scores = TRUE)Standardized loadings (pattern matrix) based upon correlation matrix    RC1   RC4   RC2   RC3   RC5   h2     u2 combio1   0.47  0.01  0.80  0.37 -0.02 1.00 0.0034 2.1bio2  -0.80 -0.22 -0.22 -0.13  0.46 0.97 0.0327 2.0bio3  -0.30  0.35 -0.28  0.77  0.29 0.97 0.0332 2.4bio4  -0.31 -0.45  0.19 -0.80  0.10 1.00 0.0024 2.1bio5   0.16 -0.35  0.88 -0.26  0.07 1.00 0.0045 1.6bio6   0.49  0.23  0.50  0.66 -0.08 1.00 0.0025 3.1bio7  -0.35 -0.45  0.15 -0.80  0.12 1.00 0.0013 2.2bio8  -0.46 -0.12  0.82 -0.02 -0.21 0.95 0.0531 1.8bio9   0.77  0.03  0.35  0.43  0.21 0.94 0.0571 2.3bio10  0.25 -0.29  0.90 -0.17  0.05 0.99 0.0050 1.5bio11  0.49  0.23  0.50  0.67 -0.06 1.00 0.0022 3.1bio12  0.63  0.71 -0.12  0.27 -0.01 0.99 0.0111 2.3bio13  0.37  0.88 -0.14  0.21  0.06 0.98 0.0172 1.5bio14  0.95  0.21  0.12  0.08  0.05 0.96 0.0355 1.2bio15 -0.93 -0.09 -0.04 -0.14  0.18 0.93 0.0695 1.1bio16  0.33  0.89 -0.16  0.26  0.02 0.99 0.0073 1.5bio17  0.94  0.27  0.08  0.13  0.01 0.97 0.0268 1.2bio18 -0.04  0.91 -0.20  0.33 -0.11 0.99 0.0127 1.4bio19  0.97  0.18  0.04  0.08  0.07 0.99 0.0140 1.1                   RC1  RC4  RC2  RC3  RC5SS loadings           6.77 3.96 3.85 3.54 0.48Proportion Var        0.36 0.21 0.20 0.19 0.03Cumulative Var        0.36 0.57 0.77 0.95 0.98Proportion Explained  0.36 0.21 0.21 0.19 0.03Cumulative Proportion 0.36 0.58 0.78 0.97 1.00Mean item complexity =  1.9Test of the hypothesis that 5 components are sufficient.The root mean square of the residuals (RMSR) is  0.01  with the empirical chi square  32.94  with prob \u0026lt;  1 Fit based upon off diagonal values = 1\u0026gt; The objective for me to do the rotation is to see simplified relationship of PCs and bioclimatic variables. In the first part of my code, I did the varimax rotation on eigenvalues, and the correlation matrix seem to do what I want. But by using \"psych::principal\", the Standardized loadings (pattern matrix) based upon correlation matrix does not show simplified relationships. I'm confused about the difference to these. And based on the youtube video:https://www.youtube.com/watch?v=oZ2nfIPdvjYThe varimax rotation was done without scaling the loadings. So my questions is:1. how can I get simplified relationship of PCs and bioclimatic variables using rotation correctly?2. how to get a raster surface of PC other than matrix?I used to do that in Arcgis which does not give me much control over the process. I appreciate any comment. Thanks a lot!The question is edited. Since the differences is that I got rows of data entry from the raster. But in the end, I want to get a raster surface of the PCs.","Creater_id":129391,"Start_date":"2016-08-30 19:22:38","Question_id":232586,"Tags":["pca","rotation","factor-rotation"],"Answer_count":0,"Last_activity":"2016-08-30 19:22:38","Link":"http://stats.stackexchange.com/questions/232586/how-to-use-varimax-rotated-pca-to-produce-raster-layers","Creator_reputation":11}
{"_id":{"$oid":"5837a573a05283111e4d2bc8"},"View_count":30,"Display_name":"Bishal Gautam","Question_score":2,"Question_content":"I have this algorithm written that can perform the XOR operation. How can i modify it so that it can process the large dataset?import java.util.Arrays;import java.util.Random;public class MLP {public static class MLPLayer {float[] output,input,weights,dweights;boolean isSigmoid = true;public MLPLayer(int inputSize, int outputSize, Random r) {output = new float[outputSize];input = new float[inputSize + 1];weights = new float[(1 + inputSize) * outputSize];dweights = new float[weights.length];initWeights(r);}public void setIsSigmoid(boolean isSigmoid) {this.isSigmoid = isSigmoid;}public void initWeights(Random r) {for (int i = 0; i \u0026lt; weights.length; i++) {weights[i] = (r.nextFloat() - 0.5f) * 4f;}}public float[] run(float[] in) {System.arraycopy(in, 0, input, 0, in.length);input[input.length - 1] = 1;int offs = 0;Arrays.fill(output, 0);for (int i = 0; i \u0026lt; output.length; i++) {for (int j = 0; j \u0026lt; input.length; j++) { output[i] += weights[offs + j] * input[j];}if (isSigmoid) { output[i] = (float) (1 / (1 + Math.exp(-output[i])));}offs += input.length;}return Arrays.copyOf(output, output.length);}public float[] train(float[] error, float learningRate, float momentum) {int offs = 0;float[] nextError = new float[input.length];for (int i = 0; i \u0026lt; output.length; i++) {float d = error[i];if (isSigmoid) {d *= output[i] * (1 - output[i]);}for (int j = 0; j \u0026lt; input.length; j++) { int idx = offs + j; nextError[j] += weights[idx] * d; float dw = input[j] * d * learningRate; weights[idx] += dweights[idx] * momentum + dw; dweights[idx] = dw;}offs += input.length;}return nextError;}}MLPLayer[] layers;public MLP(int inputSize, int[] layersSize) {layers = new MLPLayer[layersSize.length];Random r = new Random(1234);for (int i = 0; i \u0026lt; layersSize.length; i++) {int inSize = i == 0 ? inputSize : layersSize[i - 1];layers[i] = new MLPLayer(inSize, layersSize[i], r);}}public MLPLayer getLayer(int idx) {return layers[idx];}public float[] run(float[] input) {float[] actIn = input;for (int i = 0; i \u0026lt; layers.length; i++) {actIn = layers[i].run(actIn);}return actIn;}public void train(float[] input, float[] targetOutput, float            learningRate, float momentum) {float[] calcOut = run(input);float[] error = new float[calcOut.length];for (int i = 0; i \u0026lt; error.length; i++) {  error[i] = targetOutput[i] - calcOut[i]; // negative error}for (int i = layers.length - 1; i \u0026gt;= 0; i--) {error = layers[i].train(error, learningRate, momentum);}} public static void main(String[] args) throws Exception { float[][] train = new float[][]{new float[]{0, 0}, new float[]{0, 1},     new float[]{1, 0}, new float[]{1, 1}};  float[][] res = new float[][]{new float[]{0}, new float[]{1}, new       float[]{1}, new float[]{0}};  MLP mlp = new MLP(2, new int[]{2, 1});  mlp.getLayer(1).setIsSigmoid(false);  Random r = new Random();  int en = 500;  for (int e = 0; e \u0026lt; en; e++) {  for (int i = 0; i \u0026lt; res.length; i++) {  int idx = r.nextInt(res.length);  mlp.train(train[idx], res[idx], 0.3f, 0.6f);  }  if ((e + 1) % 100 == 0) {  System.out.println();  for (int i = 0; i \u0026lt; res.length; i++) {  float[] t = train[i];  System.out.printf(\"%d epoch\\n\", e + 1);  System.out.printf(\"%.1f, %.1f --\u0026gt; %.3f\\n\", t[0], t[1], mlp.run(t)[0]);  }  }  }  }  }How can i modify it for large number of inputs for the dataset? How can i know what output the dataset gives after processing of dataset? Would be grateful if someone helps me here.","Creater_id":119273,"Start_date":"2016-08-30 19:17:19","Question_id":232585,"Tags":["backpropagation","data-preprocessing"],"Answer_count":0,"Last_activity":"2016-08-30 19:17:19","Link":"http://stats.stackexchange.com/questions/232585/implementing-backpropagation-for-dataset-preprocessing","Creator_reputation":11}
{"_id":{"$oid":"5837a573a05283111e4d2bca"},"View_count":64,"Display_name":"Mike Flynn","Question_score":3,"Question_content":"I often hear of a debate between Bayesian and Frequentist approaches to statistical inference, with both sides (although I've heard much more from the Bayesian side) giving detailed arguments as to why their side is better. What I haven't been convinced of is why this argument is important at all. If I approach a problem from a Bayesian perspective, would this contradict results if I had first approached it from a Frequentist perspective? I haven't really heard of any concrete distinctions. ","Creater_id":12031,"Start_date":"2016-08-30 14:38:20","Question_id":232565,"Tags":["probability","bayesian","frequentist"],"Answer_count":1,"Last_activity":"2016-08-30 19:08:50","Link":"http://stats.stackexchange.com/questions/232565/are-bayesian-and-frequentists-approaches-mutually-exclusive-contradictory","Creator_reputation":542}
{"_id":{"$oid":"5837a573a05283111e4d2bd7"},"View_count":7,"Display_name":"Amir","Question_score":3,"Question_content":"I am going to use the output of princals as the IVs in a regression with my DV.What is the relationship between objscores and pcscores outcome values from  princals {Gifi}?Which one of objscores and pcscores is better to be used in regression with DV?Thank you for your kind help.Yours,Amir","Creater_id":128742,"Start_date":"2016-08-30 18:55:20","Question_id":232582,"Tags":["categorical-data","pca","scores"],"Answer_count":0,"Last_activity":"2016-08-30 18:55:20","Link":"http://stats.stackexchange.com/questions/232582/what-is-the-relationship-between-objscores-and-pcscores-outcome-values-from-prin","Creator_reputation":71}
{"_id":{"$oid":"5837a573a05283111e4d2bd9"},"View_count":74,"Display_name":"RdeWolf","Question_score":1,"Question_content":"I'm new to the field of machine learning and I don't have a strong background in statistics and the likes, making it difficult to get a good grasp of what the important topics in ML are right now. The fact that it develops quickly doesn't help either. If someone could tell me, I'd like to know what some of the most important (research) subjects in ML are nowadays. ","Creater_id":129271,"Start_date":"2016-08-30 15:02:55","Question_id":232566,"Tags":["machine-learning"],"Answer_count":1,"Last_activity":"2016-08-30 18:17:32","Link":"http://stats.stackexchange.com/questions/232566/important-topics-in-machine-learning-right-now","Creator_reputation":22}
{"_id":{"$oid":"5837a573a05283111e4d2be6"},"View_count":31,"Display_name":"user112638","Question_score":2,"Question_content":"I have a balanced panel data set with 5 waves.My DV is an ordinal outcome (physical activity), with 3 values. I am looking at the treatment effect of a random event (diagnosis of chronic disease) on the outcome over time. I believe adjusting for baseline will allow me to get more accurate coefficients,as those who were more active before diagnosis will continue to be so.However I cannot find much information on using baseline variable as a covariate for ordinal data, and wanted to ask about its validity. The coefficients I have obtained for baseline are highly significant (p-value=0.000)Also, how would I go about incorporating baseline variable and time together, possibly as an interaction term?","Creater_id":112638,"Start_date":"2016-08-30 18:05:33","Question_id":232578,"Tags":["panel-data"],"Answer_count":1,"Last_activity":"2016-08-30 18:14:32","Link":"http://stats.stackexchange.com/questions/232578/using-baseline-value-as-covariate-for-ordinal-outcome","Creator_reputation":31}
{"_id":{"$oid":"5837a573a05283111e4d2bf3"},"View_count":29,"Display_name":"Bryan","Question_score":4,"Question_content":"I'm currently analyzing items from a bank for K-12 education. I have a separate data set for each grade/subject combination (for example, 1st grade English is a set, 2nd grade English is a set, etc). Each data set contains 400-7000(!) items.I would love to able to run various DIF analyses, but missing data is a huge problem. Each student in the data set has answered only a fraction of the items. For example, in the 1st grade English data set, I have an N of 4000, but no one student has answered more than 50 percent of the 400 items, and the percentages of missing answers is above 90 percent. This is actually one of the more complete data sets I have. I expect the set with 7000 items to have a percentage missing in the 95 or above range.My questions: there's absolutely no way to perform Mantel-Haenszel, IRT-based, or any other form of DIF on data with so many missings, right? Even if my program of choice (STATA) returned results (which it doesn't), that this information would be unreliable, invalid, and otherwise useless? Also, assuming the above is the case, any suggestions for things I might try to measure different function by groups in lieu of conventional DIF, even if it's not particularly sophisticated?Thanks in advance,Bryan","Creater_id":128683,"Start_date":"2016-08-23 07:48:12","Question_id":231297,"Tags":["missing-data","psychometrics"],"Answer_count":1,"Last_activity":"2016-08-30 17:59:13","Link":"http://stats.stackexchange.com/questions/231297/missing-data-in-differential-item-functioning-analysis","Creator_reputation":21}
{"_id":{"$oid":"5837a573a05283111e4d2bff"},"View_count":64,"Display_name":"St\u0026#233;phane","Question_score":3,"Question_content":"I have some questions relating to the multivariate cumulative distribution function.Let  denotes the multivariate cumulative distribution function of Gaussian random variables, ie for , , ...,  standard normal random variables  with  the correlation matrix of , then for all .For , I read  is given by\\mathcal{N}_n(t_1,.., t_n; \\rho_1,.., \\rho_{n-1}) = \\int_{-\\infty}^{t_1} ... \\int_{-\\infty}^{t_n} \\frac{1}{(2 \\pi)^{\\frac{n}{2}} \\prod_{i=1}^{n-1} \\sqrt{1-\\rho_i^2} } \\exp \\left( \\frac{-x_1^2}{2} - \\sum_{i=1}^{n-1} \\frac{(x_{i+1}-\\rho_i x_i)^2}{2(1-\\rho_i)} \\right) d x_1 ... d x_nHowever, I do not know where it comes from and what really means these . I would like to know if there exists formulae to compute the derivatives with respect to parameters of , ie\\frac{\\partial \\mathcal{N}_n(t_1,.., t_n; \\rho_1,.., \\rho_{n-1})}{\\partial c_1}where . I would start by writting \\frac{\\partial \\mathcal{N}_n(t_1,.., t_n; \\rho_1,.., \\rho_{n-1})}{\\partial c_1} = \\frac{\\partial t_1}{\\partial c_1} \\times \\int_{-\\infty}^{t_2} ... \\int_{-\\infty}^{t_n} \\frac{1}{(2 \\pi)^{\\frac{n}{2}} \\prod_{i=1}^{n-1} \\sqrt{1-\\rho_i^2} }  \\exp \\left( \\frac{-t_1^2}{2} - \\frac{(x_{2}-\\rho_1 t_1)^2}{2(1-\\rho_1)} - \\sum_{i=2}^{n-1} \\frac{(x_{i+1}-\\rho_i x_i)^2}{2(1-\\rho_i)} \\right) d x_2 ... d x_n","Creater_id":129554,"Start_date":"2016-08-30 17:11:23","Question_id":232575,"Tags":["probability","correlation","normal-distribution","multivariate-analysis","integral"],"Answer_count":0,"Last_activity":"2016-08-30 17:11:23","Link":"http://stats.stackexchange.com/questions/232575/differentiate-the-multivariate-standard-normal-distribution","Creator_reputation":16}
{"_id":{"$oid":"5837a573a05283111e4d2c01"},"View_count":84,"Display_name":"user1205901","Question_score":3,"Question_content":"From this thread I learned why the median is a nonlinear function. In the context in which I'm working I need to use a linear function. Googling \"approximate the median\"+\"linear\" didn't reveal anything usable, but somehow it seems intuitive to me that some reasonable approximation should be possible with a linear function.EDIT: I was asked in one of the comments to provide a fuller explanation of the context behind the question. The context is that I'm taking in a bunch of estimates from people about some quantity, and then generating an aggregated prediction them. One of the most obvious things to do is to linearly combine the estimates according to some predetermined weights.The framework is provided by Davis-Stober et al (2014), who write:  Consider a set of N-many decision makers (DMs), where each DM makes a  judgment about the unknown value of a criterion. [...] We take this  criterion value to be a random variable, , with mean  and  variance . [...] A crowd prediction, denoted , is  defined as the random variable formed by linearly combining the DMs  according to predetermined weights , , with the restriction that all  are non-negative  and, to ensure uniqueness, . The weights   are not random variables, but rather fixed choices of how to  combine crowd member judgments. [...]    Let  by the  vector of the DM's mean predictions.  Let  be the covariance matrix of the ,  random variables. Let  denote the  vector of covariances of  with each , . It is straightforward to show that  is  equal to the following    ,    where  is the  vector of weights, , defining .So I want to 'break down'  in the way described in the aforementioned paper, but in the context in which I'm working it's more common to use the median instead of the mean. It's been asserted to me that I could get around this problem by using a linear aggregate to approximate the median.Davis-Stober, C. P., Budescu, D. V., Dana, J., \u0026amp; Broomell, S. B. (2014). When is a crowd wise?. Decision, 1(2), 79.Chicago ","Creater_id":9162,"Start_date":"2016-08-29 18:27:08","Question_id":232408,"Tags":["median"],"Answer_count":0,"Last_activity":"2016-08-30 16:46:30","Link":"http://stats.stackexchange.com/questions/232408/how-can-i-approximate-the-median-with-a-linear-function","Creator_reputation":1998}
{"_id":{"$oid":"5837a573a05283111e4d2c03"},"View_count":34,"Display_name":"Ryan Kim","Question_score":3,"Question_content":"I've been searching for a long time for this issue and couldn't find the proper answer so I post a question here.Here I put the data set example first.**ID  Time      Treated**1      1           01      2           11      3           11      4           01      5           02      1           12      2           12      3           12      4           03      1           03      2           13      3           13      4           0 3      5           13      6           1Based on the data set example, the treatment occurs multiple times at different times for each individual ID. My goal is to divide each individual into treatment groups and control groups and then conduct score matching based on the demographic variables.What I've found were all about the situation that the treatment occurs at different times through each individual but the treatment lasts through the whole time. (I've searched dynamic treatment effects, time varying treatment effects, inverse probability weighted score etc)So in this case, the data set example looks like this:**ID    Time    Treated**1      1           01      2           01      3           02      1           02      2           12      3           12      4           13      1           03      2           13      3           1 It would be appreciated if you give me any idea about my situation.Any intuitive explanation would be more welcome! ","Creater_id":129552,"Start_date":"2016-08-30 16:22:31","Question_id":232572,"Tags":["r","stata","matching","treatment-effect"],"Answer_count":0,"Last_activity":"2016-08-30 16:22:31","Link":"http://stats.stackexchange.com/questions/232572/how-to-deal-with-treatment-effects-that-occurs-multiple-times-at-different-time","Creator_reputation":16}
{"_id":{"$oid":"5837a573a05283111e4d2c05"},"View_count":77,"Display_name":"ilia","Question_score":3,"Question_content":"I previously posted here, however realised this is a better place for it.I am trying to create a k-max pooling layer for mxnet, which offers some documentation about a new operation, a bit like this:class KMaxPooling(mx.operator.CustomOp):    def forward(self, is_train, req, in_data, out_data, aux):        # Desired (k=3):        # in_data = np.array([1, 2, 4, 10, 5, 3])        # out_data = [4, 10, 5]        x = in_data[0].asnumpy()        idx = x.argsort()[-k:]        idx.sort(axis=0)        y = x[idx]However, I'm not sure:How to check if this works the way I want it (is there a github of test material for symbols e.g. test if softmax is working, test if convolution is working, etc.In mxnet you have to include a backward() function not just forward() for gradient back-propagation. However, I'm not sure what that means in terms of a pooling layer.I found this example someone made for keras:import numpy as npimport theano.tensor as Tfrom keras.layers.core import MaskedLayerclass KMaxPooling(MaskedLayer):    def __init__(self, pooling_size):        super(MaskedLayer, self).__init__()        self.pooling_size = pooling_size        self.input = T.tensor3()    def get_output_mask(self, train=False):        return None    def get_output(self, train=False):        data = self.get_input(train)        mask = self.get_input_mask(train)        if mask is None:            mask = T.sum(T.ones_like(data), axis=-1)        mask = mask.dimshuffle(0, 1, \"x\")        masked_data = T.switch(T.eq(mask, 0), -np.inf, data)        result = masked_data[T.arange(masked_data.shape[0]).dimshuffle(0, \"x\", \"x\"),                             T.sort(T.argsort(masked_data, axis=1)[:, -self.pooling_size:, :], axis=1),                             T.arange(masked_data.shape[2]).dimshuffle(\"x\", \"x\", 0)]","Creater_id":129548,"Start_date":"2016-08-30 15:59:08","Question_id":232570,"Tags":["neural-networks","python","deep-learning","conv-neural-network","pooling"],"Answer_count":0,"Last_activity":"2016-08-30 15:59:08","Link":"http://stats.stackexchange.com/questions/232570/k-max-pooling-custom-code-in-python","Creator_reputation":16}
{"_id":{"$oid":"5837a573a05283111e4d2c07"},"View_count":19,"Display_name":"thornate","Question_score":3,"Question_content":"I have a population that is sampled such that each item has a different probability of being selected. That probability is separate and independent of the value of any given item.How do I determine the appropriate number of samples needed such that the mean falls within an appropriate error interval?","Creater_id":129534,"Start_date":"2016-08-30 14:10:14","Question_id":232560,"Tags":["sampling","sample-size","weighted-sampling","weighted-data"],"Answer_count":0,"Last_activity":"2016-08-30 14:10:14","Link":"http://stats.stackexchange.com/questions/232560/appropriate-sample-size-for-weighted-sample","Creator_reputation":116}
{"_id":{"$oid":"5837a573a05283111e4d2c09"},"View_count":19,"Display_name":"Srilekha","Question_score":2,"Question_content":"I want to see if there is an increase in my dependent variable 'Y' within a session for a group A (10 participants).My research question:IS there a significant increase in 'Y' within-session for group A?For every second, dependent variable 'Y' and sensor noise 'X' were sampled. 'Y' will be eliminated if the 'X' = YES. The session time varies for each participant (between 20 to 50 seconds). There are 10 participants. Every participant have around 20 sessions.My data looks like this for every session:Q1: What statistical method should I use to show if the variable 'Y' significantly increased or decreased or no pattern observed within session for the group? I am aware of basic methods such as ANOVA. Q2: I was thinking about the following method:(1) treat each second as repeated measures and perform ANOVA with time as IV, and Y as DV for all 10 participants? In this case, the varying session time becomes a problem. How to handle this?Q3: is there any time-series statistical method that I can use to analyse the change in 'Y' within a session for the sessions numbers 1, 10 and 20 (individually) for all 10 participants.Thank you","Creater_id":129532,"Start_date":"2016-08-30 13:47:41","Question_id":232558,"Tags":["regression","time-series","statistical-significance","anova","repeated-measures"],"Answer_count":0,"Last_activity":"2016-08-30 13:47:41","Link":"http://stats.stackexchange.com/questions/232558/statistical-analysis-of-change-in-variable-y-over-time-within-a-session-for-a","Creator_reputation":16}
{"_id":{"$oid":"5837a573a05283111e4d2c0b"},"View_count":24522,"Display_name":"Stephen Turner","Question_score":19,"Question_content":"I saw this plot in the supplement of a recent paper and I'd love to be able to reproduce it using R. It's a scatterplot, but to fix the overplotting there are contour lines that are \"heat\" colored blue to red corresponding to the overplotting density. How would I do this?","Creater_id":36,"Start_date":"2012-07-05 13:30:30","Question_id":31726,"Tags":["r","data-visualization","scatterplot"],"Answer_count":2,"Last_activity":"2016-08-30 13:36:45","Link":"http://stats.stackexchange.com/questions/31726/scatterplot-with-contour-heat-overlay","Creator_reputation":2065}
{"_id":{"$oid":"5837a573a05283111e4d2c18"},"View_count":4495,"Display_name":"ML_Pro","Question_score":6,"Question_content":"I need to use binary variables (values 0 \u0026amp; 1) in k-means. But k-means only works with continuous variables. I know some people still use these binary variables in k-means ignoring the fact that k-means is only designed for continuous variables. This is unacceptable to me.Questions:  So what is the statistically / mathematically correct way of using binary variables in k-means / hierarchical clustering?How to implement the solution in SAS / R?","Creater_id":54214,"Start_date":"2015-01-02 07:55:24","Question_id":130974,"Tags":["r","clustering","binary-data","k-means"],"Answer_count":2,"Last_activity":"2016-08-30 13:05:42","Link":"http://stats.stackexchange.com/questions/130974/how-to-use-both-binary-and-continuous-variables-together-in-clustering","Creator_reputation":598}
{"_id":{"$oid":"5837a573a05283111e4d2c26"},"View_count":86,"Display_name":"FredrikAa","Question_score":3,"Question_content":"Background:In Statisical Methods by Pfaffenberger and Patterson, they say that \"A parameter is a numerical measure of a population characteristic.\" (p. 306)\"A statistic is a numerical measure calculated from a set of sample observations.\"\"Sampling error is the difference between the value of the parameter in the population and the value of the statistic in the sample, excluding other erros, such as \" (p. 311)They also define the population mean and the sample mean to be respectively:\\mu=\\frac{\\sum_{i=1}^N x_i}{N} (p. 39)and\\bar{x}=\\frac{\\sum_{i=1}^n x_i}{n} (p. 39), where N is the number of individuals in the population and  are the measurements of each of the individuals in the population and  is a sample of size  of the  measurements. The population standard deviation and sample variance are defined respectively as\\sigma^2=\\frac{\\sum_{i=1}^{N}(x_i-\\mu)^2}{N}(p.47)and s^2=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}{n-1} (p. 47)where the elements  are defined as for the mean. They also make some examples where the definitions are used. In example 8.1 on page 309, they calculate . Then they list the different subsamples possible with three elements. One possible sample is . Then . The sampling error is then 10-5.9=4.10.    Definitions:Random variable: mapping from the sample space to the real line. If the RV  is discrete, then , where  is the sample space.Realization: a particular value of the random variable . The random variable takes the different values  according to a probability distribution. These probabilities are the long run frequencies of the different outcomes in the experiment, i.e  (assuming  is discrete)Experiment:  you are considering the result of some action in the future, ie. the outcome of an action that have not occured yet. This action can result in different events. One of these events must occur. Performing this action is called performing an \"experiment\" in statistics.  Distribution: the long run relative frequency of different outcomes for a discrete RV. I am not sure of the interpretation in the continuous case. Here the distribution is a density, so that  Population: all of the individuals that you consider modelling with the random variable. So if you model the wage of the employees at a firm with a random variable, then all the employees are the population.   Questions re-edited:1) What are the authors assuming when they say that the mean is equal to the mean of all the individuals in the population, that ? 2) Normally we would assume that  distributed as  and say that  is an estimator of . Then some say we assume an infinite population. Why is this so? Where is the proof of this? Original questions:1) According to Weak law of large numbers in finite populations, the set consisting of all the realisations of the random variables corresponding to the individuals in the population is still a sample. Why then do Pfaffenger and Patterson say that the population mean is exactly the mean of the realisations of the RVs in the sample? 2) If , then the distribution of the realisations of the RVs obtained in the experiment must be the true distribution of the underlyding random variable? 3) Is the distribution of the RVs in the sample the true distribution of the population characteristic? Or do we say that the values obtained are only realisations of a RV, where the mean of the random variable may not coincide with the mean of the sample. 4) How can you tell someone that the mean of all the employees is not the true mean (i.e sample mean of all the employees may not be equal to the expectation of the population RV)? This is fine when you only look at a sample, but I look at all employees.  5) We may choose to model the population RV with a parametric distribution, then it makes sense that . However, is it also a model assumption that I can view the different salaries as realisations of a random variable, whatever be the distribution? Is this the reason the mean of the salary RV does not necessarily equal the mean of all the observations in the population, even if the true distribution of the random variable was available?      ","Creater_id":101150,"Start_date":"2016-08-29 06:58:16","Question_id":232300,"Tags":["variance","mean","random-variable","sample","population"],"Answer_count":1,"Last_activity":"2016-08-30 12:57:01","Link":"http://stats.stackexchange.com/questions/232300/population-sample-and-model","Creator_reputation":72}
{"_id":{"$oid":"5837a573a05283111e4d2c33"},"View_count":77,"Display_name":"Eoin","Question_score":0,"Question_content":"I'm doing research into understand the influential factors within a logistic regression model I've built in R using the glm() function.From my research, it seems that using the summary() function to summarize the model is a popular method to identify which variables are significant. What I can't seem to find however is a description of what the summary function is doing to determine the significance codes (eg. the *) for each variable. This answer states that the significance codes are simply categorizations of the p-value, but I don't really understand that. Is there anyone out there that could maybe help me understand how R computes this?","Creater_id":101431,"Start_date":"2016-08-30 12:36:48","Question_id":232548,"Tags":["r","logistic","statistical-significance","p-value"],"Answer_count":1,"Last_activity":"2016-08-30 12:54:56","Link":"http://stats.stackexchange.com/questions/232548/r-how-are-the-significance-codes-determined-when-summarizing-a-logistic-regres","Creator_reputation":40}
{"_id":{"$oid":"5837a573a05283111e4d2c40"},"View_count":25,"Display_name":"Eli","Question_score":1,"Question_content":"If I have hypothesized that a variable  will have a positive effect on , but the analysis shows that the effect is negative, but not significant, how should I interpret these results? Should I say that the results do not support my hypothesis, yet that it is not conclusive whether this is true only for the sample or also for the entire population? ","Creater_id":89451,"Start_date":"2016-08-30 11:39:02","Question_id":232537,"Tags":["statistical-significance","interpretation"],"Answer_count":1,"Last_activity":"2016-08-30 12:34:04","Link":"http://stats.stackexchange.com/questions/232537/how-to-interpret-coefficients-which-do-not-have-the-hypothesised-sign-but-are-al","Creator_reputation":20}
{"_id":{"$oid":"5837a573a05283111e4d2c4d"},"View_count":9,"Display_name":"GRS","Question_score":0,"Question_content":"The aim is to allocate a vector to one of the groups.My book says that given  we allocate  to a group which minimises the square Mahalanobis distance  between  and :.Need help with an Example:Let . I assumed one vector to be drawn from  and another from .Now the aim is to find the region for allocation of vectors to group .R_1=\\{\\textrm{x}:(\\textrm{x}-\\mu_1)'\\Sigma^{-1}(\\textrm{x}-\\mu_1)\\leq (\\textrm{x}-\\mu_2)'\\Sigma^{-1}(\\textrm{x}-\\mu_2)\\}My book says the solution should be I tried simplifying my expression, but couldn't get it to work (I'll post it at the bottom).Then I was told to find the probability of incorrectly allocating  a vector into the other group. I need to find:I tried to integrate: where  is the density function for being in , but I fail to visualise it in more than 1 dimension.  My Simplification Attempt:=\\textrm{x}'\\Sigma^{-1}\\textrm{x}-\\mu_1'\\Sigma^{-1}\\textrm{x}-\\mu_1'\\Sigma^{-1}\\textrm{x}+\\mu_1'\\Sigma^{-1}\\mu_1-\\textrm{x}'\\Sigma^{-1}\\textrm{x}+\\textrm{x}'\\Sigma^{-1}\\mu_2+\\mu_2'\\Sigma^{-1}\\textrm{x}-\\mu_2'\\Sigma^{-1}\\mu_2=(\\mu_2'-\\mu_1')\\Sigma^{-1} \\textrm{x}+\\mu_2'\\Sigma^{-1} \\textrm{x}-\\mu_1'\\Sigma^{-1} \\textrm{x}+\\mu_1'\\Sigma^{-1}\\mu_1-\\mu_2'\\Sigma^{-1}\\mu_2Then I tried to take transposes, but I got nowhere.","Creater_id":109646,"Start_date":"2016-08-30 12:32:22","Question_id":232546,"Tags":["discriminant-analysis","discriminant"],"Answer_count":0,"Last_activity":"2016-08-30 12:32:22","Link":"http://stats.stackexchange.com/questions/232546/allocations-classifications-of-p-dimensional-distributions","Creator_reputation":113}
{"_id":{"$oid":"5837a573a05283111e4d2c4f"},"View_count":138,"Display_name":"RUser","Question_score":1,"Question_content":"I'm doing a liner regression fit using R. I used lm() to do the regression. Then I standardize my data using scale() and again do the regression on standardize data using lm().Surprisingly the regression coefficient of one variable was positive before standardization and after standardization I found it is showing negative coefficient. I checked the correlation between that variable and my predictor. It has positive significant correlation.  data_bd2=data_2[,c(1:3,5:7)] str(data_bd2) fit_bd=lm(data_bd2) vif(fit_bd) summary(fit_bd) scale_data_bd2=data.frame(scale(data_bd2))colnames(scale_data_bd2)=colnames(data_bd2) fit_bd_std=lm(scale_data_bd2) summary(fit_bd_std) Can you please help me understand why sign of regression coefficient differ before and after standardization?","Creater_id":39764,"Start_date":"2016-08-30 00:01:28","Question_id":232433,"Tags":["r","regression","least-squares"],"Answer_count":1,"Last_activity":"2016-08-30 12:07:50","Link":"http://stats.stackexchange.com/questions/232433/why-does-the-sign-of-regression-coefficients-for-standardized-unstandardize-da","Creator_reputation":23}
{"_id":{"$oid":"5837a573a05283111e4d2c5c"},"View_count":21,"Display_name":"ecologist1234","Question_score":0,"Question_content":"In R, I have completed a simple regression of the form lm(Y~x+a). The dataset original dataset included several non-independent data points, which I selected among using a set of rules. I want to test the robustness of the regression results by randomly resampling one from each set of the \"duplicate\" (non-independent) data points and completing something like a bootstrap. As an example, a subset of the data look like this:Index x       a      Y1   0.33    61.04   0.952   0.41    8.67    1.163   0.00    6.19    0.914   0.00    13.60   0.955   0.10    3.72    0.946   0.00    5.14    1.047   0.00    23.16   0.008   0.03    7.59    0.989   0.00    14.44   1.1210  0.10    11.17   1.0610  0.18    10.64   1.1210  0.05    10.82   1.1010  0.10    10.13   0.0010  0.00    11.61   1.0210  0.00    11.61   1.0310  0.18    8.53    0.0010  0.00    11.61   1.1110  0.18    10.64   1.06I would like to randomly include one of the Index = 10 records each time the regression is run, then calculate confidence intervals for the regression intercept and slopes. Thanks.","Creater_id":59714,"Start_date":"2016-08-30 08:39:31","Question_id":232511,"Tags":["r","regression","bootstrap","resampling","sensitivity-analysis"],"Answer_count":1,"Last_activity":"2016-08-30 11:51:49","Link":"http://stats.stackexchange.com/questions/232511/regression-sensitivity-analysis-by-re-sampling-duplicates","Creator_reputation":35}
{"_id":{"$oid":"5837a573a05283111e4d2c69"},"View_count":34,"Display_name":"user1586400","Question_score":1,"Question_content":"I have to show there is no significant overlap between two gene lists. The way I did was to show high p value of the overlap (from hypergeometric test) and show that I can't reject the null hypothesis of no significant overlap. But the critique says being unable to reject the null hypothesis doesn't necessarily prove the inverse that no significant overlap exists. In theory, I understand the comment, but I can't think of a test that directly estimates p value of no significant overlap (or significance of absence of overlap). ","Creater_id":76615,"Start_date":"2016-08-29 09:14:42","Question_id":232330,"Tags":["hypothesis-testing","statistical-significance"],"Answer_count":0,"Last_activity":"2016-08-30 11:41:42","Link":"http://stats.stackexchange.com/questions/232330/statistical-tests-for-no-overlap-between-lists-not-for-overlap","Creator_reputation":23}
{"_id":{"$oid":"5837a573a05283111e4d2c6b"},"View_count":39,"Display_name":"mguzmann","Question_score":0,"Question_content":"I am trying to represent graphically the results of a classifier with multiple groups. I am not so much interested in the accuracy of the classifier, as the degree to which the classifier confuses certain categories. So, for example, from a confusion matrix like the following:  a  b  ca 10 0  6b 1  8  1c 7  0  15The plot show should that a and c are more similar than a and b or than b and c. What I am doing is applying PCA directly to the confusion matrix and then plotting the first and second components. Also creating a dendogram with -cor(m) as the distance. Both seem to produce the 'right' result but I am not sure this is justified. Also, nonmetric MDS seems to work very much like the PCA approach.Are these approaches valid? are there better alternatives?","Creater_id":22265,"Start_date":"2016-08-30 10:37:46","Question_id":232531,"Tags":["pca","confusion-matrix"],"Answer_count":0,"Last_activity":"2016-08-30 10:42:36","Link":"http://stats.stackexchange.com/questions/232531/plot-confusion-matrix-using-pca","Creator_reputation":172}
{"_id":{"$oid":"5837a573a05283111e4d2c6d"},"View_count":90,"Display_name":"RarelySee","Question_score":1,"Question_content":"I'm a little bit confused about this, so any help would be appreciated!Let's say I have a repeated-measures design in which participants take part in a task where they have to rate the attractiveness of 50 different faces in 4 different conditions of facial expression type. Condition A is a baseline with neutral expression, condition B is happy, condition C sad and condition D angry.I've run individual linear regressions with conditions B, C and D as the dependent variable and condition A as a predictor, generating 3 different regression equations. I would like to find a way to statistically compare these equations in terms of their slope and intercept.Specifically, the theory I am trying to test predicts there will be no change in slope between them, but condition B should raise the intercept, condition D should lower it and there should be no change in C. In other words, the different conditions should result in a uniform shift up or down in terms of attractiveness.Is there any way to test this statistically?","Creater_id":129177,"Start_date":"2016-08-27 05:53:36","Question_id":232040,"Tags":["regression","spss","intercept"],"Answer_count":1,"Last_activity":"2016-08-30 10:15:52","Link":"http://stats.stackexchange.com/questions/232040/ways-of-comparing-linear-regression-interepts-and-slopes","Creator_reputation":8}
{"_id":{"$oid":"5837a573a05283111e4d2c7a"},"View_count":3407,"Display_name":"Learner","Question_score":9,"Question_content":"I came across term perplexity which refers to the log-averaged inverse probability on unseen data. Wikipedia article on perplexity doesnt give a intuitive meaning for the same. This perplexity measure was used in pLSA paper.Can anyone explain the need and intuitive meaning of perplexity measure? ","Creater_id":4290,"Start_date":"2011-05-03 23:04:26","Question_id":10302,"Tags":["measurement"],"Answer_count":2,"Last_activity":"2016-08-30 09:22:21","Link":"http://stats.stackexchange.com/questions/10302/what-is-perplexity","Creator_reputation":692}
{"_id":{"$oid":"5837a573a05283111e4d2c88"},"View_count":42,"Display_name":"PaulC","Question_score":0,"Question_content":"Consider a AR(1) model with states given by, and the observations given by , for . The parameters of this model are . I would like to find the posterior density of the parameters to conduct Gibbs sampling. I derived the posterior over  to be inverse Gamma as  where  and . Could anyone please help me derive the posterior of the other two parameters? I guess  is inverse Gamma and  is normal(?) TIA.","Creater_id":109981,"Start_date":"2016-08-10 01:06:13","Question_id":229134,"Tags":["bayesian","autoregressive","posterior","gibbs"],"Answer_count":1,"Last_activity":"2016-08-30 09:21:07","Link":"http://stats.stackexchange.com/questions/229134/posterior-distributions-of-parameters-in-a-ar1-model","Creator_reputation":18}
{"_id":{"$oid":"5837a573a05283111e4d2c95"},"View_count":26,"Display_name":"monade","Question_score":0,"Question_content":"I just came across a paper [1] (p.105), which uses the Wald-test to test whether the performance of two classifiers is significantly different. I haven't seen that before, but now found lecture notes that confirm this application [2]. The idea seems to be to apply both classifiers to the same data set and then make a paired statistical comparison of the k obtained predictions.Questions:(1) In [2] The Wald statistic is given as  - isn't that exactly the same as the t statistic?(2) What do you think of this approach? The comparison between classifiers is discussed in many related threads on this platform, but I have never seen the Wald test been suggested as a potential approach.Update 08/30/16I have now found the code used in [1] applying the Wald-test (Matlab file exchange). I have adapted my question to reflect this new information. In the code, they do a paired statistical comparison of the variables \\(classifier 2 correct predictions).% Tests the null hypothesis that two classifiers that were tested on% the same dataset (paired sample) have the same prediction accuracy on% unseen data.% % This function is to be used to compare two classifiers tested on the same% dataset. In order to compare two classifiers that were tested on% independent datasets, use ACC_CMP2 instead.% % Usage:%     p = acc_cmp1(targs,preds1,preds2)%% Arguments:%     targs - a vector of target labels%     preds1 - a vector of predictions by classifier 1%     preds2 - a vector of predictions by classifier 2% % Make sure that length(targs) == length(preds1) == length(preds2).% % See also:%     acc_cmp2% Kay H. Brodersen, ETH Zurich, Switzerland% http://people.inf.ethz.ch/bkay/% % -------------------------------------------------------------------------function p = acc_cmp1(targs,preds1,preds2)    warning('Bug - order of pred1, pred2 matters! Is this a test of whether classifier 1 better than classifier 2?');    % Check input    assert(~any(isnan(targs)));    assert(~any(isnan(preds1)));    assert(~any(isnan(preds2)));    assert(all(size(targs)==size(preds1)));    assert(all(size(preds1)==size(preds2)));    targs = targs(:);    preds1 = preds1(:);    preds2 = preds2(:);    % Prepare test statistic    n = length(targs);    X = double(preds1==targs);    Y = double(preds2==targs);    D = X-Y;    delta = mean(D);    S_2 = mean((D-mean(D)).^2);    se = sqrt(S_2)/sqrt(n);    W = delta / se;    % Return p value of Wald test    p = 1-normcdf(W,0,1);end[1] http://people.inf.ethz.ch/bkay/publications/Brodersen_2014_NeuroImageClinical.pdf[2] https://www.ismll.uni-hildesheim.de/lehre/ml-07w/skript/ml-1up-07-evaluation-testing.pdf[3] http://www.its.caltech.edu/~zuev/teaching/2016Winter/lecture13a.pdf","Creater_id":62140,"Start_date":"2016-08-29 14:40:00","Question_id":232382,"Tags":["machine-learning","classification","statistical-significance"],"Answer_count":0,"Last_activity":"2016-08-30 09:11:14","Link":"http://stats.stackexchange.com/questions/232382/comparison-of-classifiers-using-the-wald-test-statistic","Creator_reputation":16}
{"_id":{"$oid":"5837a573a05283111e4d2c97"},"View_count":16,"Display_name":"luchonacho","Question_score":2,"Question_content":"I am using data from a longitudinal survey, where the sampling unit is the household. Once households are selected, data is collected from each eligible individual in the household. My model is defined at the individual level. Since several members from a households are in my model, does my model suffer from cross-sectional dependence (CSD)?At first I thought this was a trivial matter, and that it was obvious that my model suffers from CSD. But then looking at survey papers, CSD seems to arise due to omitted common effects, spatial effects, common shocks, but not a single mention on sampling. Exploring further, I can see that common effects refers to some factors affecting all individuals (e.g. oil shock). In my case, my concern is with common effects within households (e.g. same neighborhood). Is then CSD not a problem because the size of the household is insignificant in a large dataset? Thus, ? I am surprised how little the literature in my field worries about CSD. Is this because it is an ubiquitous issue with every (longitudinal) survey dataset? (a bit like measurement errors... hardly anyone account for them). The following is a quote from Wooldridge (2010), p.6:  For better or worse, spatial correlation is often ignored in applied work because correcting the problem can be difficult.","Creater_id":100369,"Start_date":"2016-08-29 10:15:50","Question_id":232340,"Tags":["panel-data","survey","non-independent"],"Answer_count":1,"Last_activity":"2016-08-30 08:27:57","Link":"http://stats.stackexchange.com/questions/232340/cross-sectional-dependence-in-longitudinal-survey-data","Creator_reputation":584}
{"_id":{"$oid":"5837a573a05283111e4d2ca4"},"View_count":3139,"Display_name":"Tae-Sung Shin","Question_score":14,"Question_content":"Maybe this is a dumb question as my experience has been more on statistical computing algorithms than on data analysis. But I would like to know, or have references on, analysis process most of statistical data analysts go through for each data analysis project.  If I make a \"list\", to complete data analysis project, an analyst has to: first collect requirements for the project, plan/design his data analysis based on those requirements before actually pre-processing data, executing the data analysis and writing a report based on his analysis results.For this question, I am interested in more details of Step 2. But I understand this is not practically clear cut as the analyst might have to change his plan or design according to data analysis output. Is there any reference on this subject?","Creater_id":5597,"Start_date":"2012-02-12 14:06:17","Question_id":22697,"Tags":["project-management"],"Answer_count":3,"Last_activity":"2016-08-30 08:27:14","Link":"http://stats.stackexchange.com/questions/22697/what-is-a-practically-good-data-analysis-process","Creator_reputation":315}
{"_id":{"$oid":"5837a573a05283111e4d2cb1"},"View_count":14570,"Display_name":"John","Question_score":20,"Question_content":"I am confused. I don't understand the difference a ARMA and a GARCH process.. to me there are the same no ?Here is the (G)ARCH(p, q) processAnd here is the ARMA(p, q):Is the ARMA simply an extension of the GARCH, GARCH being used only for returns and with the assumption  where  follows a strong white process?","Creater_id":16387,"Start_date":"2012-10-30 07:20:03","Question_id":41509,"Tags":["finance","garch","arma"],"Answer_count":4,"Last_activity":"2016-08-30 08:25:30","Link":"http://stats.stackexchange.com/questions/41509/what-is-the-difference-between-garch-and-arma","Creator_reputation":240}
{"_id":{"$oid":"5837a573a05283111e4d2cc1"},"View_count":55,"Display_name":"Demenyi Norbert","Question_score":2,"Question_content":"I would like to know if Gradient Boosting Machines and Gradient Boosted Regression Trees are the same algorithm or different. If they are different algorithms what is the main difference between them?","Creater_id":127424,"Start_date":"2016-08-26 02:54:55","Question_id":231867,"Tags":["machine-learning","data-mining","cart","ensemble"],"Answer_count":0,"Last_activity":"2016-08-30 08:20:29","Link":"http://stats.stackexchange.com/questions/231867/what-is-the-difference-between-gradient-boosting-machines-and-gradient-boosted-r","Creator_reputation":16}
{"_id":{"$oid":"5837a573a05283111e4d2cc3"},"View_count":81,"Display_name":"Jcotignola","Question_score":0,"Question_content":"I am new in bioinformatics and machine learning. I am trying to predict a disease outcome using cv.glmnet to choose the best lambda for the prediction. The problem I have is that outcome groups are uneven (30 samples for outcome 0 and 14 samples for outcome 1). Therefore, in a 10-fold CV (even in a 5-fold), there will be a high probability of having groups with only one outcome.Does cv.glmnet take into account this difference in numbers (since the outcome vector is specified) and it always randomly pick samples from both groups. If not, what is the best way to perform CV for uneven groups?Thank you all.","Creater_id":129303,"Start_date":"2016-08-28 19:01:21","Question_id":232228,"Tags":["machine-learning","cross-validation"],"Answer_count":1,"Last_activity":"2016-08-30 08:18:41","Link":"http://stats.stackexchange.com/questions/232228/cross-validation-for-uneven-groups-using-cv-glmnet","Creator_reputation":1}
{"_id":{"$oid":"5837a573a05283111e4d2cd0"},"View_count":191,"Display_name":"Mr Validation","Question_score":0,"Question_content":"LASSO and adaptive LASSO are two different things, right? (To me, the penalties look different, but I'm just checking whether I miss something.)When you generally speak about elastic net, is the special case LASSO or adaptive LASSO?Which one does the glmnet package do, provided you choose alpha=1?Adaptive LASSO works on milder conditions, right? Both have the oracle property in suitable data, right?","Creater_id":128910,"Start_date":"2016-08-25 03:24:40","Question_id":231643,"Tags":["lasso","glmnet","elastic-net","oracle"],"Answer_count":2,"Last_activity":"2016-08-30 08:15:41","Link":"http://stats.stackexchange.com/questions/231643/lasso-vs-adaptive-lasso","Creator_reputation":31}
{"_id":{"$oid":"5837a573a05283111e4d2cde"},"View_count":48,"Display_name":"GGA","Question_score":0,"Question_content":"I selected for a univariate cox model an optimal cut-point based on the minimum p value of a log-rank test.Are there any procedures to perform a sort of cross-validation or performance study for this cutpoint?I'm using R so coding is not a problem","Creater_id":97342,"Start_date":"2016-08-30 05:54:27","Question_id":232485,"Tags":["regression","predictive-models","survival"],"Answer_count":1,"Last_activity":"2016-08-30 08:15:02","Link":"http://stats.stackexchange.com/questions/232485/cross-validation-for-optimal-cut-point","Creator_reputation":171}
{"_id":{"$oid":"5837a573a05283111e4d2ceb"},"View_count":33,"Display_name":"lacerbi","Question_score":1,"Question_content":"I started using Radial Basis Function Networks for regression (see here for an overview of RBFNs). The specifics are: input points, , where  is the input space dimensionality (usually ). normalized squared exponential units, centered on the inputs (plus possibly a bias unit);a single scale factor  for all units.The number of inputs is low enough that I can find the weight matrix  exactly by solving the linear system (e.g., backslash operator in MATLAB). However, I need to define a good scale factor , and RBFNs are notoriously prone to overfitting.My current approach consists of performing leave-one-out cross-validation. The cross-validated loss is a jagged/noisy function of  due to cross-validation variability. So I am optimizing it via robust Bayesian Optimization (see here). This seems to work fine, but I wonder if I can do better -- there is a plentitude of methods for choosing hyperparameters and I am new to this specific field (used to work with Gaussian Processes).What would you recommend as methods for picking hyperparameter(s) for this specific case?","Creater_id":80479,"Start_date":"2016-08-29 12:48:12","Question_id":232364,"Tags":["regression","machine-learning","hyperparameter","radial-basis"],"Answer_count":0,"Last_activity":"2016-08-30 07:56:31","Link":"http://stats.stackexchange.com/questions/232364/tuning-hyperparameters-of-radial-basis-function-network-for-regression","Creator_reputation":1737}
{"_id":{"$oid":"5837a573a05283111e4d2ced"},"View_count":91,"Display_name":"Zarya","Question_score":0,"Question_content":"I’m working with sports performance data and I have a sample of 10 observations () regarding the total number of ... per match. The according values are 15, 18, 24, 12, 13, 16, 28, 20, 14, 25.How do I compute the probability that the value of an additional observation (e.g.  = 7) comes from the same population as the given sample?Is this even possible without fitting a distribution and estimating the parameters (which is difficult in this case due to the low sample size)?Edit: The 10 observations are from one player only. There are more observations from other players available and I modeled them using a Negative Binomial distribution which describes the data very well (overdispersed count data, therefore not Poisson).","Creater_id":110947,"Start_date":"2016-08-29 06:06:19","Question_id":232290,"Tags":["probability","distributions","sample"],"Answer_count":1,"Last_activity":"2016-08-30 07:40:11","Link":"http://stats.stackexchange.com/questions/232290/probability-that-one-observation-comes-from-the-same-population-as-a-given-sampl","Creator_reputation":8}
{"_id":{"$oid":"5837a573a05283111e4d2cfa"},"View_count":76,"Display_name":"Jack Pierce-Brown","Question_score":0,"Question_content":"Consider the integral of a stationary random process  with mean , variance  and stationary correlation function :I = \\int_0^L X(t)\\,\\mathrm{dt}In my previous post the moments of  was found to be:E[I] = \\mu_I = L\\mu\\text{Var}[I] = \\sigma_I^2 =\\sigma^2 \\int_0^L \\int_0^L \\rho(t_1-t_2)\\,\\mathrm{dt_1\\,dt_2} = 2\\sigma^2\\int_{0}^{L} \\rho(\\tau)(L-|\\tau|)\\,\\mathrm d\\tau Where . I would now like the calculate the skewness of :\\text{Skew}[I] = E\\left[\\left(\\frac{I-\\mu_I}{\\sigma_I}\\right)^3\\right] = \\frac{E[I^3] - 3\\mu_I\\sigma_I^2 - \\mu_I^3}{\\sigma_I^3}I know  and  so the problem is to evaluate :E[I^3] = E\\left[\\int_0^L X(t)\\,\\mathrm{dt}\\int_0^L X(u)\\,\\mathrm{du} \\int_0^L X(v)\\,\\mathrm{dv}\\right] = E\\left[\\int_0^L \\int_0^L \\int_0^L X(t) X(u)X(v)\\,\\mathrm{dt}\\,\\mathrm{du}\\,\\mathrm{dv}\\right] = \\int_0^L \\int_0^L \\int_0^L E\\left[X(t) X(u)X(v)\\right]\\,\\mathrm{dt}\\,\\mathrm{du}\\,\\mathrm{dv}In the previous post the expected value was then written in terms of the covariance function however I don't know how to proceed here.Can you show me how to evaluate the 3rd moment (if possible)?","Creater_id":112088,"Start_date":"2016-08-08 09:11:41","Question_id":228818,"Tags":["stochastic-processes","skewness","integration"],"Answer_count":2,"Last_activity":"2016-08-30 07:25:09","Link":"http://stats.stackexchange.com/questions/228818/skewness-of-the-integral-of-a-stochastic-process","Creator_reputation":91}
{"_id":{"$oid":"5837a573a05283111e4d2d08"},"View_count":59,"Display_name":"Yehoshaphat Schellekens","Question_score":1,"Question_content":"I'm currently using bayesian logistic regression for binary clasification (arm package - R) , one of its many wonderful features that I love to exploit is the possibility the update my coefficient every time a new batch of data arrives (What I understand as bayesian updating), avoiding the need to upload all my data at once.I'm running a sanity check to make sure my code works properly.To my understanding, running two blocks of bayesian logistic regressions, where the first block's coefficients will be used as priors to the second block, will result  in a similar output as if I would have run all my data at once (or am I wrong !?)Sanity check code:library(data.table)library(arm)#some sample data:n \u0026lt;- 100x1 \u0026lt;- rnorm (n)x2 \u0026lt;- rbinom (n, 1, .5)b0 \u0026lt;- 1; b1 \u0026lt;- 1.5; b2 \u0026lt;- 2y \u0026lt;- rbinom (n, 1, invlogit(b0+b1*x1+b2*x2))z1 \u0026lt;- trunc(runif(n, 4, 9))dt\u0026lt;-data.table(y=y,x1=x1,z1=z1)head(dt)#i like to work in agregated format :)first_batch\u0026lt;-dt[,.(count=.N),.(y,z1)]#full data - without beysian updating full_dt\u0026lt;-rbind(first_batch,first_batch)full_dt\u0026lt;-full_dt[,.(count=sum(count)),.(y,z1)]full_model\u0026lt;- bayesglm(y ~ factor(z1),weights=count,family=binomial(link=\"logit\"),data=full_dt)#The following is what i really want to compute:#using first batchprior_estimates \u0026lt;-bayesglm(y ~ factor(z1),weights=count,family=binomial(link=\"logit\"),data=first_batch)prior_estimates_dt\u0026lt;-data.table(term=attr(summary(prior_estimates)coefficients)coefficients)[,2,with = FALSE])postirior_model \u0026lt;-bayesglm(y ~ factor(z1),                   prior.mean=prior_estimates_dt[term!=\"(Intercept)\"]std.error,                   weights=count,family=binomial(link=\"logit\"),data=first_batch)#compate resutlsdisplay (full_model,detail=FALSE)display(prior_estimates,detail=FALSE)display(postirior_model,detail=FALSE)\u0026gt; display (full_model,detail=FALSE)bayesglm(formula = y ~ factor(z1), family = binomial(link = \"logit\"),     data = full_dt, weights = count)            coef.est coef.se(Intercept)  0.72     0.31  factor(z1)5  1.67     0.71  factor(z1)6 -0.30     0.42  factor(z1)7  0.30     0.47  factor(z1)8 -0.42     0.43  ---n = 10, k = 5residual deviance = 236.4, null deviance = 250.7 (difference = 14.3)\u0026gt; display(prior_estimates,detail=FALSE)bayesglm(formula = y ~ factor(z1), family = binomial(link = \"logit\"),     data = first_batch, weights = count)            coef.est coef.se(Intercept)  0.73     0.42  factor(z1)5  1.52     0.91  factor(z1)6 -0.31     0.57  factor(z1)7  0.27     0.63  factor(z1)8 -0.42     0.59  ---n = 10, k = 5residual deviance = 118.3, null deviance = 125.4 (difference = 7.1)\u0026gt; display(postirior_model,detail=FALSE)bayesglm(formula = y ~ factor(z1), family = binomial(link = \"logit\"),     data = first_batch, weights = count, prior.mean = prior_estimates_dt[term !=         \"(Intercept)\"]std.error)            coef.est coef.se(Intercept)  0.73     0.29  factor(z1)5  1.64     0.62  factor(z1)6 -0.32     0.35  factor(z1)7  0.29     0.40  factor(z1)8 -0.43     0.36  ---n = 10, k = 5residual deviance = 118.2, null deviance = 125.4 (difference = 7.2)Notice the results of full_model VS. postirior_model , the bayesian update seemed to get the posterior results closer to the full models results, but not exactly as I anticipated, can anyone explain the source of this mismatch? Thanks in advance!","Creater_id":45372,"Start_date":"2016-08-29 22:16:40","Question_id":232425,"Tags":["bayesian"],"Answer_count":1,"Last_activity":"2016-08-30 07:19:14","Link":"http://stats.stackexchange.com/questions/232425/bayesian-updating-in-logistic-regression-prior-posterior-wont-match-full-dat","Creator_reputation":214}
{"_id":{"$oid":"5837a573a05283111e4d2d15"},"View_count":73,"Display_name":"Ali Turab Lotia","Question_score":0,"Question_content":"I understand the answer to this question is that it entirely depends on the data set.However, this does not help people understand if their model is suitable or whether they should explore other variables.So I am attempting to getting this question answered by people with experience in different fields where statistics is applied. Assuming the model was not overfit and no other problems existed with the model, what I would like to know from someone experienced is the vastly different projects they worked on from very different industries and R^2 they obtained from their models so that people may gain a 'feel' whether they are heading in the right direction.This question is of a highly atypical format so let me know if this is inappropriate for this website and I will remove it. Any suggestions on where to post this will also be appreciated.","Creater_id":124010,"Start_date":"2016-08-30 05:27:52","Question_id":232478,"Tags":["r-squared"],"Answer_count":1,"Last_activity":"2016-08-30 07:10:02","Link":"http://stats.stackexchange.com/questions/232478/what-is-a-good-r2-value","Creator_reputation":69}
{"_id":{"$oid":"5837a573a05283111e4d2d22"},"View_count":43,"Display_name":"Rilcon42","Question_score":1,"Question_content":"I am just starting to learn time-series analysis and I was got great results with an ARIMA(3,0,2) model when I tested against the last 5000 hours. I know this doesnt apply to all of the data when you consider it since the 1980's, butAm I justified in restricting the range to only current (within the last 10 years or so) of data because the market has changed so much?Can I use the fact that an ARIMA(3,0,2) model works in the most recent 5000 hours to improve an overall model?","Creater_id":62559,"Start_date":"2016-08-30 06:59:08","Question_id":232497,"Tags":["time-series","arima"],"Answer_count":1,"Last_activity":"2016-08-30 07:08:05","Link":"http://stats.stackexchange.com/questions/232497/is-it-a-bad-idea-to-fit-a-time-series-in-pieces","Creator_reputation":281}
{"_id":{"$oid":"5837a573a05283111e4d2d2f"},"View_count":36,"Display_name":"user2457324","Question_score":1,"Question_content":"I have two random variables :  is the product of a normal distribution and an independent lognormal distribution  so a lognormal mixture, \u0026amp; In view of the above, how can I simplify the following expression: \\mathbb{E} [\\Delta(z,y).y] where ? And  where .I would like to ideally get, for some constant  defined in terms of the covariance of : :Is there a possibility to get the above upper-bound using results from the sub-gaussianity literature, e.g., http://arxiv.org/pdf/1011.3027v7.pdf? ","Creater_id":129212,"Start_date":"2016-08-29 15:39:36","Question_id":232390,"Tags":["probability","distributions","probability-inequalities"],"Answer_count":1,"Last_activity":"2016-08-30 06:56:04","Link":"http://stats.stackexchange.com/questions/232390/bounds-on-the-following-mixture-model","Creator_reputation":6}
{"_id":{"$oid":"5837a573a05283111e4d2d3c"},"View_count":3725,"Display_name":"blunders","Question_score":6,"Question_content":"Assuming \"complete randomness\" and given a string with a length of 20 characters where each character may be one of 62 possible characters: What are the total number of combinations possible? (Guessing 20 to the power of 62.) Also, if new strings are randomly selected one after another and added to a list of strings selected so far, how many strings must be selected before the chance of selecting a string that has already been selected is below 1-in-100000 ()?Note: 62 comes from: numeric digits (0-9), uppercase letters (A-Z), and lowercase letters (a-z).","Creater_id":10098,"Start_date":"2012-03-24 18:49:14","Question_id":25211,"Tags":["probability","combinatorics"],"Answer_count":1,"Last_activity":"2016-08-30 06:55:16","Link":"http://stats.stackexchange.com/questions/25211/simple-combination-probability-question-based-on-string-length-and-possible-char","Creator_reputation":187}
{"_id":{"$oid":"5837a573a05283111e4d2d49"},"View_count":500,"Display_name":"Pedro.Alonso","Question_score":0,"Question_content":"Hi I have a problem undertanding this:Example: In a deck of 52 cards, 5 cards are chosen.What is the probability that all 5 cards have diﬀerent face values?total number of outcomes = 52 C 5 total number of face value combinations = 13 C 5 total number of suit possibilities, with replacement = 4^5Now I get the 52 C 5, the total number of possibilities, and I get the 13 C 5 the number of face cards in the deck, so: P( 5 diff face values) = (13 C 5)(4^5)/(52 C 5)Why? What exactly does the 4^5 is doing there? Help.","Creater_id":31843,"Start_date":"2013-11-09 10:39:07","Question_id":75060,"Tags":["probability","self-study","combinatorics"],"Answer_count":1,"Last_activity":"2016-08-30 06:53:29","Link":"http://stats.stackexchange.com/questions/75060/probability-understanding-52-c-5","Creator_reputation":197}
{"_id":{"$oid":"5837a573a05283111e4d2d56"},"View_count":70,"Display_name":"Ali Turab Lotia","Question_score":1,"Question_content":"We have two models that use the same method to calculate log likelihood and the AIC for one is lower than the other. However, the one with the lower AIC is far more difficult to interpret.We are having trouble deciding if it is worth introducing the difficulty and we judged this using a percentage difference in AIC. We found that the difference between the the two AICs was only 0.7%, with the more complicated model having a 0.7% lower AIC.Is the low percentage difference between the two a good reason to avoid using the model with the lower AIC?Does the percentage of difference explain that 0.7% more information is lost in the less complicated model?Can the two models have very different results?","Creater_id":124010,"Start_date":"2016-08-30 03:42:06","Question_id":232465,"Tags":["model-selection","aic"],"Answer_count":1,"Last_activity":"2016-08-30 06:36:28","Link":"http://stats.stackexchange.com/questions/232465/how-to-compare-models-on-the-basis-of-aic","Creator_reputation":69}
{"_id":{"$oid":"5837a573a05283111e4d2d63"},"View_count":30,"Display_name":"skleene","Question_score":1,"Question_content":"I often work with multivariate data and therefore use facet_wrap to get a perspective on the relationships that are present in the data. This often means that I will have many (\u003e 20) panels. In RStudio when I have figures with many panels and use the default graphics device I get panels that are shrunk down and compressed at a scale that cannot be used.ggplot(diamonds, aes(x = carat, y = price)) + geom_point() + facet_wrap(~cut*color*clarity, ncol = 4)A good solution is to use ggsave and set the size of the plot region to have sufficient space to present all the panels.Is there a good solution that involves using the default RStudio graphics device? For example, getting the call to ggplot to draw a high quality facet plot within RStudio.","Creater_id":44995,"Start_date":"2016-08-18 10:29:49","Question_id":230543,"Tags":["ggplot2"],"Answer_count":0,"Last_activity":"2016-08-30 06:32:59","Link":"http://stats.stackexchange.com/questions/230543/how-to-use-ggplot2-to-generate-a-figure-with-many-facets-and-avoid-each-facet-be","Creator_reputation":110}
{"_id":{"$oid":"5837a573a05283111e4d2d65"},"View_count":1608,"Display_name":"Andreas","Question_score":7,"Question_content":"I'm doing some experiments with neural networks and I wanted to ask for some support to verify my methodology and my results.My setup: I have separated my data into time slots of 5 seconds, i.e. all timestamps are within 5 seconds. For each slot I get approx. 1000 samples. Furthermore, I have 7 features for each sample which are mean normalized. 4 are numerical and 3 are boolean (0,1). My network topology is 7-15-1, training algorithm is resilient backpropagation (Jordan recurrent network) with a sigmoid activation function and atan error function. I use 5-fold cross validation to check the network. The target feature of each sample is a boolean value. My goal is to train the network with the data of one time slot (time slot n) and use it to rank the samples from the next (n+1) time slot (on a scale from 0-1, thus the single output node). I'm using neural networks as I want to compare its performance with other models like SVM and logistic regression.Here are my questions regarding the setup and the results:I'm able to collect all samples with all features within the time slot. Do I need to check for the probability distribution of the features that I'm using or is the mean normalization sufficient?Is it reasonable to use the results of a single output node as a ranking function?The training error quickly drops below 0.1 (after 20-50 iterations). With more iterations (up to 10.000) it doesn't get better, often it gets worse (error up to 0.4). Is it ok that the error rates drops so quickly or should I be sceptical? Are neural networks in general only really usable after, e.g. 50.000 iterations?Is it reasonable to accept an error rate of 0.1? Of course, the less the better, but I'm not able to further minimize the error.The cross validation error is always approx. twice the training error. Is this too much? I read \"when the cv error is way larger than the training error, you're suffering from high variance\" Is 0.2 already large or is 0.4 large?As I have a target value for each sample, I also do a f-measure evaluation for each classifier. It's always approx. 0.75. I chose  0.3 as classification error threshold, i.e. when the difference between ideal value (0 or 1) and predicted value are within 0.3 then the classification is ok. Is this value too large or would you say that one should always go with an classification threshold \u003e0.95?","Creater_id":8116,"Start_date":"2012-01-08 06:08:39","Question_id":20756,"Tags":["machine-learning","neural-networks","validation","performance"],"Answer_count":1,"Last_activity":"2016-08-26 13:34:09","Link":"http://stats.stackexchange.com/questions/20756/verifying-neural-network-model-performance","Creator_reputation":229}
{"_id":{"$oid":"5837a573a05283111e4d2d72"},"View_count":66,"Display_name":"Rita V","Question_score":2,"Question_content":"Let  be a variable which has density  and mean Define , a Bernoulli variable, which has a probability Consider the following expression:\\begin{equation}\\frac{\\sum_{i=1}^{N}b_{i}x_{i}}{\\sum_{i=1}^{N}b_{i}}\\end{equation}This can be interpreted as a sampling procedure where the probability of being included in the sample is a Bernoulli draw. The above expression is therefore simply an average.Is the following derivation correct:\\begin{equation}\\lim_{N \\rightarrow \\infty}\\frac{\\frac{1}{N}\\sum_{i=1}^{N}b_{i}x_{i}}{\\frac{1}{N}\\sum_{i=1}^{N}b_{i}}=\\frac{E[BX]}{E[B]}=\\frac{E[E[BX|X]]}{E[B]}=\\frac{E[\\frac{1}{N^{\\gamma}}X]}{E[\\frac{1}{N^{\\gamma}}]}=E[X]\\end{equation}I am unsure whether this is correct because of the way the Bernouilli probability is specified:  tends to zero as . But I do not see where this would invalidate the above derivation - although I have a feeling it might.Any help is much appreciated.","Creater_id":87030,"Start_date":"2015-08-28 02:33:25","Question_id":169157,"Tags":["expected-value","convergence","asymptotics","bernoulli-distribution","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 13:13:02","Link":"http://stats.stackexchange.com/questions/169157/convergence-of-bernoulli-sampling-procedure","Creator_reputation":13}
{"_id":{"$oid":"5837a573a05283111e4d2d7f"},"View_count":86,"Display_name":"mavavilj","Question_score":2,"Question_content":"Does squeeze theorem apply in convergence in probability?My statistics reference (where it talks about convergence in probability and its condition) does not cite it (but does seem to apply it), but it seems like it could apply.What does the formulation look like for probabilities?","Creater_id":78575,"Start_date":"2015-11-30 18:05:12","Question_id":184370,"Tags":["convergence","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 13:12:51","Link":"http://stats.stackexchange.com/questions/184370/convergence-in-probability-does-squeeze-theorem-apply","Creator_reputation":682}
{"_id":{"$oid":"5837a573a05283111e4d2d8c"},"View_count":77,"Display_name":"EconStats","Question_score":2,"Question_content":"Am I right in thinking that it is the average of the sum of  different populations means? Here it is used in the context that confused me. It's the Chebychev WLLN, apparently. \"If  is a sample of  observations such that  and Var[ such that  as  then ) = 0.\"Is this saying that each sample of , corresponds to it's own population of  (from above, ) and as the sample get bigger we have to average over populations?So if I were to draw the random variable 1 from a population of {1,2,3} and the random variable 4 from the population {4,5,6} then the population of my sample 1,4 is {1,2,3,4,5,6}?","Creater_id":31188,"Start_date":"2014-07-26 11:55:16","Question_id":109489,"Tags":["mean","sample-mean","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 13:12:38","Link":"http://stats.stackexchange.com/questions/109489/what-is-bar-mu-n","Creator_reputation":390}
{"_id":{"$oid":"5837a573a05283111e4d2d99"},"View_count":35,"Display_name":"kurtkim","Question_score":2,"Question_content":"Recently I saw the new representation of LLN like belowDoes anyone verify this?In the paper,  is a kernel function","Creater_id":70877,"Start_date":"2016-03-07 07:02:06","Question_id":200354,"Tags":["asymptotics","law-of-large-numbers"],"Answer_count":2,"Last_activity":"2016-08-26 13:12:30","Link":"http://stats.stackexchange.com/questions/200354/another-representation-of-the-law-of-large-numbers","Creator_reputation":78}
{"_id":{"$oid":"5837a573a05283111e4d2da7"},"View_count":54,"Display_name":"Daniel","Question_score":3,"Question_content":"Suppose  are iid random variables.  .Also define:U_n = \\frac{1}{n-2}\\sum_{i=1}^n{|X_i|}V_n = \\sqrt{\\frac{1}{n-1}   \\sum_{i=1}^n{X_i^2} }Given that  shows convergence in probability, we want to find ,  such that: U_n \\rightarrow_p a, \\quad V_n \\rightarrow_p bHere are my thoughts:  could considered as an exponential distribution variable. With that,  (weak law of large numbers). However this result does not seem to be very useful. Also we know that  has Gamma/Erlang distribution.","Creater_id":17812,"Start_date":"2014-12-01 03:33:13","Question_id":126139,"Tags":["self-study","estimation","convergence","asymptotics","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 13:12:21","Link":"http://stats.stackexchange.com/questions/126139/convergence-in-probability-for-two-statistics-of-laplace-random-variables","Creator_reputation":687}
{"_id":{"$oid":"5837a573a05283111e4d2db4"},"View_count":75,"Display_name":"Mur1lo","Question_score":3,"Question_content":"Let  be a sequence of i.i.d random variables with a continuous density function. If  is the true median and  is the sample median, is it true that:\\frac{\\sum_{i=1}^{n}\\big(1\\{X_i\\leq m\\} - 1\\{X_i\\leq\\hat m _n\\}\\big)}{\\sqrt{n}}goes to 0 in probability under suitable conditions?","Creater_id":120428,"Start_date":"2016-06-18 09:58:56","Question_id":219526,"Tags":["probability","convergence","median","indicator-variables","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 13:12:02","Link":"http://stats.stackexchange.com/questions/219526/convergence-to-zero-of-a-sum-involving-the-median","Creator_reputation":527}
{"_id":{"$oid":"5837a573a05283111e4d2dc1"},"View_count":1216,"Display_name":"bdoering","Question_score":3,"Question_content":"The goal of both methods seems to be to derive an estimate of a posterior/target distribution. If a process model exists which links some input parameters (which are themselves uncertain and can be described by a PDF) to an output parameter through a model equation or other computations, why would one choose one method over the other? Would both be applicable? Can one make a statement on the benefit of one method over the other with respect to the number of required draws/simulation runs in order to reach a sufficiently good approximation of the target PDF?","Creater_id":25045,"Start_date":"2013-07-09 05:46:23","Question_id":63767,"Tags":["simulation","mcmc","monte-carlo","uncertainty","law-of-large-numbers"],"Answer_count":2,"Last_activity":"2016-08-26 13:11:53","Link":"http://stats.stackexchange.com/questions/63767/what-is-the-difference-between-the-monte-carlo-mc-and-monte-carlo-markov-chain","Creator_reputation":18}
{"_id":{"$oid":"5837a573a05283111e4d2dcf"},"View_count":293,"Display_name":"Kolibris","Question_score":4,"Question_content":"Weak law of large numbers: Let  be an  sequence of iid random variables with mean  that exists and is finite. Then  in probability. I don't understand why we have exists and is finite. Could you give me an example when the expectation exists but is infinite?","Creater_id":20727,"Start_date":"2014-09-30 12:31:50","Question_id":117376,"Tags":["expected-value","law-of-large-numbers"],"Answer_count":2,"Last_activity":"2016-08-26 13:10:55","Link":"http://stats.stackexchange.com/questions/117376/wlln-can-expectation-exist-but-be-infinite","Creator_reputation":175}
{"_id":{"$oid":"5837a573a05283111e4d2ddd"},"View_count":130,"Display_name":"Ruth","Question_score":4,"Question_content":"This theorem is on Econometric Analysis (7th edition) by Greene (2012), Page 1071. It states that \"If ,  is a sample of observations such that  and  such that  as . Then , where  and  are the average of  and , respectively\".I guess  can be written as \"\" if  exists (i.e., is a constant). Am I correct? Or under what condition, can it be written like that, since in the book, it mentions that \"The Chebychev theorem does not state that  converges to , or even that it converges to a constant at all. That would require a precise statement about the behavior of \".Edit1: I understand this theorem. My question is more on: under what conditions can we  converges to  in probability since the book mentions \"That would require a precise statement about the behavior of \" and I am not sure what can the statement be. As you pointed out,  is not a random variable, neither does . But in the practical setting I apply this, s are  samples from a distribution  with finite mean m and variance v.It is like each time I draw a  from  and then generate  from . So in this setting,  is a random variable, so does . And now . Can we say  in this specific setting? We can assume independence here. Thanks.","Creater_id":97999,"Start_date":"2015-12-17 09:14:16","Question_id":187285,"Tags":["probability","mathematical-statistics","convergence","asymptotics","law-of-large-numbers"],"Answer_count":2,"Last_activity":"2016-08-26 13:10:33","Link":"http://stats.stackexchange.com/questions/187285/chebychev-s-weak-law-of-large-numbers","Creator_reputation":69}
{"_id":{"$oid":"5837a573a05283111e4d2deb"},"View_count":85,"Display_name":"user3933614","Question_score":0,"Question_content":"I'm afraid I can't provide a reproducible example because I'm not sure what the problem is - I'm just hoping someone else can eyeball it or has encountered it before.I am using XGboost to predict future outcomes. As a result, I'll use data up to and including July 2016 to predict August, September, etc. This means for my training data I provide labels to xgb.DMatrixdtrain \u0026lt;- xgb.DMatrix(     data = data.matrix( train[ , -1  ] ),     label = train[ , 1 ],     missing = NA )This is all well and good. But I (obviously) have no labels for my test data:dtest \u0026lt;- xgb.DMatrix(    data = data.matrix( test ),    missing = NA )If I use a validation set and provide labels for dtest, everything works great. If I move up the time window and no longer provide labels, suddenly the predictions are only NaN. I've looked and looked, and all examples and questions I've seen use a validation set with labels provided, and none make predictions for unseen outcomes.What is going wrong? Once again, I'm sorry I can't be more specific. I've been trying for hours to figure this out...I must be missing something simple, but I don't know what it is. ","Creater_id":129115,"Start_date":"2016-08-26 12:01:08","Question_id":231954,"Tags":["r","prediction","xgboost"],"Answer_count":1,"Last_activity":"2016-08-26 13:01:41","Link":"http://stats.stackexchange.com/questions/231954/how-to-use-xgboost-to-predict-without-labels","Creator_reputation":1}
{"_id":{"$oid":"5837a573a05283111e4d2df8"},"View_count":21,"Display_name":"overwhelmed","Question_score":1,"Question_content":"In this study pregnant women were measured repeatedly (say 10 times) for their behavioral practices (food portion, exercise, blood pressure, etc) and followed until the delivery of the baby to measure the weight of the baby. I want to fit a model with the dependent variable of birth weight and independent variables of behavioral practices.     I know repeated measures ANOVA and mixed effect models, but all those models deal with repeated measures of dependent variable. How to fit a single measured dependent variable to repeated measured independent variables? By the way Q\u0026amp;A posted here  does not help!Thanks in advance.  ","Creater_id":42238,"Start_date":"2016-08-26 13:00:26","Question_id":231964,"Tags":["regression","repeated-measures"],"Answer_count":0,"Last_activity":"2016-08-26 13:00:26","Link":"http://stats.stackexchange.com/questions/231964/how-to-fit-a-regression-model-with-repeatedly-measured-independent-variable-but","Creator_reputation":141}
{"_id":{"$oid":"5837a573a05283111e4d2dfa"},"View_count":30,"Display_name":"user4951834","Question_score":0,"Question_content":"I have a standard OLS as follows: Y = intercept + var1 + var2 + var3 + eI want to run an additional OLS as follows: var3 = intercept + var1 + var2 + evar1 is the variable of interest; the remaining variables are controls. My theory predicts var1 affects Y and it could also possibly affect var3, hence the second regression.The question is, would this cause any issues like multicollinearity in the first regression? Any interpretation problems?Thanks!","Creater_id":115579,"Start_date":"2016-08-26 11:39:17","Question_id":231950,"Tags":["regression","least-squares"],"Answer_count":1,"Last_activity":"2016-08-26 12:58:35","Link":"http://stats.stackexchange.com/questions/231950/can-independent-variable-be-used-as-dependent-variable","Creator_reputation":6}
{"_id":{"$oid":"5837a573a05283111e4d2e07"},"View_count":33,"Display_name":"Jeannie","Question_score":1,"Question_content":"I have got hourly data, which may have daily and annually seasonality. The ACF plot of the data are shown as follows, which is plotted up to 44 lags (upper left), 100 lags (upper right), and plot of diff(data, 24) (bottom left), and diff(data,24*365) (bottom right). I also tried to take the difference of 24, and then 24*365, but still does not work. I am wondering that how to remove the seasonality in this time series. Thanks.","Creater_id":114130,"Start_date":"2016-08-26 12:55:40","Question_id":231962,"Tags":["r","time-series","multiple-seasonalities"],"Answer_count":0,"Last_activity":"2016-08-26 12:55:40","Link":"http://stats.stackexchange.com/questions/231962/how-to-remove-the-seasonality-of-a-time-series","Creator_reputation":59}
{"_id":{"$oid":"5837a573a05283111e4d2e09"},"View_count":96,"Display_name":"user6507246","Question_score":1,"Question_content":"First of all I'm studying machine learning with Bishop's pattern recognition I'm stuck with chapter two Gaussian parts. It requires a lot of linear  Algebra and Statistics. which books you  recommending to form the base for Gaussian part? and this is my second question about chapter 2(Gaussian) this equation is from gaussian conditional distribution for quadratic equation -\\dfrac{1}{2}({\\bf x}-{\\pmb \\mu})^T\\Sigma^{-1}({\\bf x}-{\\pmb \\mu})=-\\dfrac{1}{2}{\\bf x}^T\\Sigma^{-1}{\\bf x} + {\\bf x}^T\\Sigma^{-1}{\\pmb \\mu}+const \\qquad{(2.71)}This is called completing the square How left equation derived to the right equation?what does equation represent? (do not understand the format of the euations and why we use this?) what does it mean to find the mean and the variance from this equation? and this is conditional probability for gaussian and this equation -\\dfrac{1}{2}{\\bf x}_a^T\\Lambda_{aa}{\\bf x}_a \\qquad{(2.72)}it said take derivative two times and this equations derived. .....why....\\Sigma\\_{a|b} = \\Lambda_{aa}^{-1} \\qquad{(2.73)}why some covariance represent the conditional probability? if its too hard to explain can you list the \"things\" I should know for this equations? ","Creater_id":128678,"Start_date":"2016-08-23 07:33:27","Question_id":231293,"Tags":["machine-learning","normal-distribution","conditional-probability","gaussian-process"],"Answer_count":1,"Last_activity":"2016-08-26 12:53:11","Link":"http://stats.stackexchange.com/questions/231293/conditional-probability-for-gaussian","Creator_reputation":58}
{"_id":{"$oid":"5837a573a05283111e4d2e16"},"View_count":31,"Display_name":"Ehsan Abd","Question_score":0,"Question_content":"I came across an article that calculated model fits for two rival models: one with four first-order factors and another with two second-order and four first-order factors. The fit indices were almost equivalent and the chi-squareds were not different either; but the authors favored the second-order model because of \"theoretical considerations\". Their argument about these theoretical considerations is disputable. However, I want to know that from a statistical point of view, are there any reasons to favor the second-order model? I appreciate citing any good reference book/article for this. I attached the two models below:The first-order modelThe second-order model","Creater_id":28637,"Start_date":"2016-08-26 02:41:58","Question_id":231865,"Tags":["modeling","model","sem","order"],"Answer_count":0,"Last_activity":"2016-08-26 12:50:37","Link":"http://stats.stackexchange.com/questions/231865/is-a-second-order-model-better-than-a-first-order-one-sem","Creator_reputation":148}
{"_id":{"$oid":"5837a573a05283111e4d2e18"},"View_count":71,"Display_name":"ollama","Question_score":1,"Question_content":"I am trying to find out how to determine / display the mode of the decision trees (used by Random Forest to make further predictions / assignments). I am using library(randomForest) in R:r \u0026lt;- randomForest(Species ~., data=train.set, importance=TRUE, do.trace=100, ntree=100)I can extract a given tree using t \u0026lt;- getTree(r, k=2) where k is the tree number. However, in my understanding, Random Forest is using the mode of these trees to classify new items.How can I access this, and perhaps display a final decision tree?","Creater_id":129116,"Start_date":"2016-08-26 12:08:19","Question_id":231955,"Tags":["r","random-forest"],"Answer_count":1,"Last_activity":"2016-08-26 12:46:11","Link":"http://stats.stackexchange.com/questions/231955/get-mode-of-decision-trees-from-random-forest","Creator_reputation":11}
{"_id":{"$oid":"5837a573a05283111e4d2e25"},"View_count":60,"Display_name":"www3","Question_score":0,"Question_content":"Is there a way to do cross validation with an LSTM in Keras?  The main problem is the time series calculation: if I remove one term or more in the middle of the time series, I don't mind if the x values are entered into the LSTM to iterate the state cell one step, but I don't want the LSTM to backprop the gradient because I don't want it to learn from validation entries.  Is there any way to do this in Keras or would I have to go back to Theano and build it from there? ","Creater_id":93111,"Start_date":"2016-08-26 12:42:54","Question_id":231959,"Tags":["backpropagation","lstm","rnn","theano"],"Answer_count":0,"Last_activity":"2016-08-26 12:42:54","Link":"http://stats.stackexchange.com/questions/231959/cross-validation-in-keras-with-an-lstm","Creator_reputation":67}
{"_id":{"$oid":"5837a573a05283111e4d2e27"},"View_count":159,"Display_name":"pidamarthy prashanth","Question_score":1,"Question_content":"\u0026gt; trees   Girth Height Volume1    8.3     70   10.3                   2    8.6     65   10.33    8.8     63   10.24   10.5     72   16.45   10.7     81   18.86   10.8     83   19.77   11.0     66   15.68   11.0     75   18.29   11.1     80   22.610  11.2     75   19.911  11.3     79   24.212  11.4     76   21.013  11.4     76   21.414  11.7     69   21.315  12.0     75   19.116  12.9     74   22.217  12.9     85   33.818  13.3     86   27.419  13.7     71   25.7I would like to know how to tell if each of my variables is discrete or continuous.","Creater_id":129112,"Start_date":"2016-08-26 11:01:05","Question_id":231943,"Tags":["continuous-data","discrete-data"],"Answer_count":1,"Last_activity":"2016-08-26 12:39:30","Link":"http://stats.stackexchange.com/questions/231943/how-can-i-tell-if-my-variable-is-discrete-or-continuous","Creator_reputation":6}
{"_id":{"$oid":"5837a573a05283111e4d2e34"},"View_count":125,"Display_name":"FredrikAa","Question_score":3,"Question_content":"Background:The weak law of large numbers states that for a sequence  of iid RVs, with expectation  and variance , the sample mean converges to :\\hat{X}=\\frac{\\sum_{i=1}^nX_i}{n}\\stackrel{p}{\\rightarrow}\\muThat is the sample mean converges in probability to the population mean as the number of RVs approaches .Question:How can you obtain infinitely many iid random variables from a finite population? How do you in practise check that your random variables are independent?Edit:By finite population I mean that you consider a population of individuals. This population is finite. You consider a characteristic in the population. You model the characteristic with a random variable. I do not mean that the range of the random variable is finite.Edit 2 We know that  is a population characteristic. Let us assume the population is of size . Denote by  the random variable that describes the population characteristic. Then . Let  denote the random variables of respectively individual 1 to . By definition . We then make a sample from the population. How can we obtain a sample of size  or  that is iid from a population that is finite? Here some say that we can sample from  WITH replacement. Edit 3If we consider a sample of size  with , where the sampling is done without replacement, can we can obtain a iid sample if the original RVs are independent? If  are iid and we let  and consider any subset of , then this subset will consist of iid RVs, right? Am I missing some point here?   ","Creater_id":101150,"Start_date":"2016-08-25 05:22:44","Question_id":231666,"Tags":["finite-population","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 12:06:16","Link":"http://stats.stackexchange.com/questions/231666/weak-law-of-large-numbers-in-finite-populations","Creator_reputation":72}
{"_id":{"$oid":"5837a573a05283111e4d2e41"},"View_count":23,"Display_name":"luchonacho","Question_score":0,"Question_content":"There are quite a number of questions about missing data, but I can't find one related to its consequences on validity in the context of regression. Please correct me if this is not the case. The article on Wikipedia is quite vague on the issue too, with phrases like \"with different impacts on the validity of conclusions from research\", without really specifying what it means by \"conclusions\". Question: it is clear to me that under missing completely at random (MCAR), results from a regression are both internally (i.e. consistency) and externally (inference to wider population) valid (minding no other problems of course). What about MAR and NMAR? ","Creater_id":100369,"Start_date":"2016-08-26 10:21:37","Question_id":231939,"Tags":["regression","missing-data","validity"],"Answer_count":1,"Last_activity":"2016-08-26 11:52:26","Link":"http://stats.stackexchange.com/questions/231939/missing-data-and-internal-and-external-validity","Creator_reputation":584}
{"_id":{"$oid":"5837a573a05283111e4d2e4e"},"View_count":29,"Display_name":"Eka","Question_score":1,"Question_content":"Disclaimer: Not from math backgroundI want to use neural networks for forecasting a time series data. I am reading/watching basics about ACF and PACF. While reading about those functions I came to understand that PACF is an important function to find different time series models for example AR(1), AR(2) etc which tells us which lagged time series is important for forecasting. In the above PACF image time series lagged at 1,2,12,24 are more prominent and above confidence level. Can we use these time series lagged data Y(t-1),Y(t-2),Y(t-12), and Y(t-24) as the feature vectors for neural network for training to forecast Y(t+n)?","Creater_id":15973,"Start_date":"2016-08-24 23:06:16","Question_id":231611,"Tags":["neural-networks","feature-selection","autocorrelation"],"Answer_count":1,"Last_activity":"2016-08-26 11:25:36","Link":"http://stats.stackexchange.com/questions/231611/can-we-use-pacf-plots-for-feature-selection","Creator_reputation":224}
{"_id":{"$oid":"5837a573a05283111e4d2e5b"},"View_count":32,"Display_name":"Clark","Question_score":0,"Question_content":"I'm running the following normal / poisson simulation, and fitting the model with rjags. I'm having trouble getting rjags to actually use the initial values I specify. In this simulation, I am specifying that theta1 should start with an initial value of 150, however, you can see clearly from the trace plot that it is starting at a value near 0.Any suggestions?set.seed(101)library(data.table)service \u0026lt;- data.table(id=1:1000,sjobs=c(rpois(250,0.1),rpois(250,0.15),rpois(250,0.25),rpois(250,0.25)),system=rep(1:4,each=250))cost \u0026lt;- data.table(id=rep(servicesjobs))setkey(service,id)setkey(cost,id)cost[service,system:=system]cost.sim2 \u0026lt;- function(x){  if(x==1)    return(rnorm(1,150,10))  else if(x==2)    return(rnorm(1,150,10))  else if (x==3)    return(rnorm(1,150,10))  else if (x==4)    return(rnorm(1,150,10))}cost[,cost:=sapply(system,cost.sim2)]service[cost[,.(sumcost=sum(cost)),by=id],sum.cost:=sumcost]service[is.na(sum.cost),sum.cost:=0.0] library(rjags)model.strings \u0026lt;- list(model1=\"model {  for (i in 1:N){    y[i] ~ dnorm(eta[i]*theta[system[i]], 0.01/(eta[i]+0.000001))    eta[i] ~ dpois(lambda[system[i]])  }  for(z in 1:4){    theta[z] ~ dnorm(50,0.00000001)  }}\")[![enter image description here][1]][1]model.spec \u0026lt;- textConnection(model.stringssjobs,                                'lambda'=c(0.1,0.15,0.25,0.25),                               'y' = servicesystem,                               'N' = nrow(service)),                   inits=jags.inits,                   n.chains = 1,                   n.adapt = 0)js.coda \u0026lt;- coda.samples(jags,c('theta','eta'),n.iter=100)plot(js.coda[[1]][,c('eta[5]','theta[1]')])Thanks!","Creater_id":28225,"Start_date":"2016-08-25 15:04:18","Question_id":231787,"Tags":["r","jags"],"Answer_count":1,"Last_activity":"2016-08-26 11:18:06","Link":"http://stats.stackexchange.com/questions/231787/rjags-does-not-seem-to-use-initial-values-specified","Creator_reputation":30}
{"_id":{"$oid":"5837a573a05283111e4d2e68"},"View_count":137,"Display_name":"Y Zhang","Question_score":1,"Question_content":"Hi I'm doing some logistic regression, currently using glmnet package in R.glmnet provides a few measure metrics for cross validation. For classification, we can use type.measure = 'auc' (area under ROC curve) or type.measure = 'class' (misclassification rate).I'm working with some insurance data with a low rate of positive examples (y = 0 for 95% data, and = 1 for 5% of them. OR 95% people did't buy insurance). library(ISLR)data(Caravan)y = dta$Purchasex = as.matrix( dta[ , -which(colnames(dta)=='Purchase') ] )And I want to predict who are likely to buy insurance.I think I need to use F1 score or some custom metrics. For example, if I can make 200 by selling an insurance, and the cost to contact one person is 20, then I want to maximize metric = 200 * N(true positive) - 20 * N(predicted positive)Is there a way I can do this with glmnet or something else more suitable? Or is AUC a similar metric as F1 score? Any help or discussion's appreciated. Thanks","Creater_id":83136,"Start_date":"2015-09-27 07:39:31","Question_id":174396,"Tags":["r","cross-validation"],"Answer_count":1,"Last_activity":"2016-08-26 11:10:15","Link":"http://stats.stackexchange.com/questions/174396/how-to-use-costum-measure-metrics-in-glmnet-r","Creator_reputation":108}
{"_id":{"$oid":"5837a573a05283111e4d2e75"},"View_count":136,"Display_name":"The learning R Kid","Question_score":0,"Question_content":"So my dataset has columns Male, Female, Month and Region (East or West).And as this is a count data (number of recordings of either male or female cannot be less than 0 for any entry) I am supposed to use poisson.First with gaussian distribution I get significant p-values about these birds being spotted in certain months.eb1Male + eb1DATE \u0026gt;= \"2005-01-01\",]))Call:glm(formula = bird_cnt ~ Region + Month, data = eb1[eb1DATE \u0026gt;= \"2005-01-01\", ])Deviance Residuals:  Min        1Q    Median        3Q       Max  -0.36585  -0.22290  -0.15306  -0.01284   1.33480  Coefficients:           Estimate Std. Error z value Pr(\u0026gt;|z|)(Intercept)     0.01281    0.20472   0.063    0.950RegionWEST     -0.10741    0.13711  -0.783    0.433MonthAugust     0.15643    0.28058   0.558    0.577MonthDecember   0.12780    0.25589   0.499    0.617MonthFebruary   0.22705    0.27229   0.834    0.404MonthJanuary    0.20210    0.25377   0.796    0.426MonthJuly       0.03459    0.36665   0.094    0.925MonthJune       0.14555    0.36954   0.394    0.694MonthMarch      0.13644    0.25944   0.526    0.599MonthMay        0.01008    0.39106   0.026    0.979MonthNovember   0.24614    0.24109   1.021    0.307MonthOctober    0.30528    0.24541   1.244    0.214MonthSeptember  0.33203    0.26834   1.237    0.216(Dispersion parameter for poisson family taken to be 1)Null deviance: 41.040  on 321  degrees of freedomResidual deviance: 37.071  on 309  degrees of freedomAIC: 746.26Number of Fisher Scoring iterations: 4And again I try it with quasipoisson and get significant p-values for certain months but underdispersed.Call:glm(formula = bird_cnt ~ Region + Month, family = \"quasipoisson\", data = eb1[eb1DATE \u0026gt;= \"2005-01-01\",]))summary(model2)png(\"myplot.png\"); par(mfrow=c(2,2)); plot(model2); dev.off()","Creater_id":129079,"Start_date":"2016-08-26 04:46:28","Question_id":231883,"Tags":["r","bioinformatics"],"Answer_count":2,"Last_activity":"2016-08-26 10:58:48","Link":"http://stats.stackexchange.com/questions/231883/count-data-gaussian-poisson-or-quasipoisson","Creator_reputation":8}
{"_id":{"$oid":"5837a573a05283111e4d2e83"},"View_count":131,"Display_name":"ahala","Question_score":2,"Question_content":"For an inverted correlation matrix , I read that its diagonal elements  are related to the multiple correlation between measure i as a criterion predicted from all other measures in the set, as follows:  R_{i,12...p} = \\sqrt{1 - \\frac{1}{C^{-1}_{ii}}} Why is that?There is a related question Why does inversion of a covariance matrix yield partial correlations between random variables?. But that is about the off-diagonal elements. ","Creater_id":129089,"Start_date":"2016-08-26 07:00:56","Question_id":231905,"Tags":["regression","correlation","multiple-regression","multivariate-analysis","partial-correlation"],"Answer_count":1,"Last_activity":"2016-08-26 10:34:59","Link":"http://stats.stackexchange.com/questions/231905/why-the-diagonal-elements-of-the-inverted-correlation-matrix-is-related-to-corre","Creator_reputation":123}
{"_id":{"$oid":"5837a573a05283111e4d2e90"},"View_count":18785,"Display_name":"Cesare Camestre","Question_score":5,"Question_content":"I'm having a look at the intraclass correlation coefficient in SPSS.Data: 17 participants rated two lists of 9 \u0026amp; 7 items from 0 to 5 (0 being unimportant and 5 being very important).All participants rated all the items and the participants are a sample of a large population.The following output has been produced in SPSS.I am struggling to find anything online which deals with interpreting this, nor does any book interpret this in the level of detail I need.There is surprisingly little information/examples of the interpretation online on this, the literature is about choosing an intraclass correlation coefficient and not interpreting it.One problem I foresee here is the F test. The data is only 17 responses, which do not follow the normality assumption.","Creater_id":26267,"Start_date":"2013-07-18 09:00:53","Question_id":64725,"Tags":["spss","small-sample","inter-rater"],"Answer_count":4,"Last_activity":"2016-08-26 09:59:58","Link":"http://stats.stackexchange.com/questions/64725/intraclass-correlation-coefficient-interpretation","Creator_reputation":134}
{"_id":{"$oid":"5837a573a05283111e4d2ea0"},"View_count":47,"Display_name":"Peat","Question_score":1,"Question_content":"I am doing some mixed model analyses using the lmer function in R's lme4 package. I am using the lmerTest package to obtain p-values for fixed effects with the Kenward-Roger method of estimating denominator degrees of freedom. My understanding is that to test fixed effects in a mixed model, maximum likelihood should be used instead of REML. However, when using the Kenward-Roger method in lmerTest, REML is required. Why is this the case, and is this appropriate, given that REML is not typically recommended for testing fixed effects in mixed models?","Creater_id":129084,"Start_date":"2016-08-26 06:35:00","Question_id":231900,"Tags":["mixed-model","model","lme4","reml"],"Answer_count":1,"Last_activity":"2016-08-26 09:47:09","Link":"http://stats.stackexchange.com/questions/231900/kenward-rogers-and-reml","Creator_reputation":6}
{"_id":{"$oid":"5837a573a05283111e4d2ead"},"View_count":20,"Display_name":"whitman","Question_score":1,"Question_content":"Is it more accurate to say that trends are interpretations of data, or rather that they are properties of data, that is, characteristics that the data exhibits? In other words, are trends inherent in the data or are they external to it? Suggestions of further reading would be appreciated.","Creater_id":119214,"Start_date":"2016-08-26 09:14:07","Question_id":231928,"Tags":["dataset","trend"],"Answer_count":0,"Last_activity":"2016-08-26 09:14:07","Link":"http://stats.stackexchange.com/questions/231928/how-to-think-of-trends","Creator_reputation":106}
{"_id":{"$oid":"5837a573a05283111e4d2eaf"},"View_count":217,"Display_name":"Paul","Question_score":9,"Question_content":"I'm reading Doug Bates' theory paper on R's lme4 package to better understand the nitty-gritty of mixed models, and came across an intriguing result that I'd like to understand better, about using restricted maximum likelihood (REML) to estimate variance.In section 3.3 on the REML criterion, he states that the use of REML in variance estimation is closely related to the use of a degrees of freedom correction when estimating variance from residual deviations in a fitted linear model. In particular, \"although not usually derived this way\", the degrees of freedom correction can be derived by estimating the variance through optimization of a \"REML criterion\" (Eq. (28)). The REML criterion is essentially just the likelihood, but the linear fit parameters have been eliminated by marginalizing (instead of setting them equal to the fit estimate, which would give the biased sample variance).I did the math and verified the claimed result for a simple linear model with only fixed effects. What I'm struggling with is the interpretation. Is there some perspective from which it is natural to derive a variance estimate by optimizing a likelihood where the fit parameters have been marginalized out? It feels sort of Bayesian, as though I'm thinking of the likelihood as a posterior and marginalizing out the fit parameters as though they are random variables.Or is the justification primarily just mathematical - it works in the linear case but is also generalizable?","Creater_id":11646,"Start_date":"2015-05-10 07:44:47","Question_id":151654,"Tags":["maximum-likelihood","mixed-model","unbiased-estimator","hierarchical-bayesian","reml"],"Answer_count":2,"Last_activity":"2016-08-26 08:41:14","Link":"http://stats.stackexchange.com/questions/151654/why-does-restricted-maximum-likelihood-yield-a-better-unbiased-estimate-of-the","Creator_reputation":3385}
{"_id":{"$oid":"5837a573a05283111e4d2ebd"},"View_count":12,"Display_name":"not_a_robot","Question_score":0,"Question_content":"Suppose I have a dataframe with different rows of text, and I want to cluster those rows to find out underlying themes in the data:import pandas as pddf = pd.DataFrame({\"id_num\": np.random.randint(low = 0, high = 50, size = 10), \"text\": [\"hello these are words i would like to cluster\", \"hello i would like to go home\", \"home i would like to go please thank you\", \"thank you please apple banana\", \"orange banana apple fruit corn\", \"orange orange orange banana banana banana banana\", \"can you take me home i have had enough of this place\", \"i am bored can we go home\", \"i would like to leave now to go home\", \"apple apple banana\"])I will first separate this dataframe into train and test:\u0026gt;\u0026gt;\u0026gt; from sklearn.cross_validation import train_test_split\u0026gt;\u0026gt;\u0026gt; train, test = train_test_split(df, test_size = 0.40)\u0026gt;\u0026gt;\u0026gt; train, test = train[\"text\"], test[\"text\"]Then start the clustering process:\u0026gt;\u0026gt;\u0026gt; from sklearn.feature_extraction.text import TfidfVectorizer\u0026gt;\u0026gt;\u0026gt; from sklearn.cluster import KMeans\u0026gt;\u0026gt;\u0026gt; vectorizer = TfidfVectorizer()\u0026gt;\u0026gt;\u0026gt; train_X = vectorizer.fit_transform(train)\u0026gt;\u0026gt;\u0026gt; test_X = vectorizer.fit_transform(test)\u0026gt;\u0026gt;\u0026gt; model = KMeans(n_clusters = 2)\u0026gt;\u0026gt;\u0026gt; model.fit(train_X)\u0026gt;\u0026gt;\u0026gt; model.predict(test_X)ValueError: Incorrect number of features. Got 22 features, expected 18.Of course, if you run this code on your own machine, you might get different results. Perhapsthe number of features might even be aligned. But in most cases, the dimensions of train_X and test_X will not match up.Has anyone else dealt with this issue? I suppose one approach to make the dimensions equal would be to employ some sort of dimensionality reduction by taking only the features (read: words) that are present in both train and test.  The other solution, which would make larger matrices, would be to fill in zeros in both matrices where a given document doesn't have the word from the other corpus.Is there another way I should be approaching this?","Creater_id":93702,"Start_date":"2016-08-26 08:39:00","Question_id":231923,"Tags":["machine-learning","text-mining"],"Answer_count":0,"Last_activity":"2016-08-26 08:39:00","Link":"http://stats.stackexchange.com/questions/231923/train-and-test-matrices-for-text-analysis","Creator_reputation":118}
{"_id":{"$oid":"5837a573a05283111e4d2ebf"},"View_count":41,"Display_name":"cdeterman","Question_score":1,"Question_content":"Generally speaking, the pervasive idea (AFAIK) in predictive modeling is to use the simplest model that performs the best.  This is relatively easy with some algorithms such as random forest (less variables at each split = less complex).This is also easy at first with simple neural networks.  A neural network with 5 nodes is clearly more complex than a 3 node network.  But what about when we get in to multiple layers?  For example, is a 100 node, single layer network considered less complex than a two layer network with 10 nodes in each layer?  Or are the layers multiplicative (10*10 = 100) and we would say they are the same complexity? Naturally there are many different combinations which increase as more layers are added.  I am curious about this as I have seen discussions regarding very wide versus very deep networks and want to know if there is any consensus on what defines a 'simpler' network.","Creater_id":37428,"Start_date":"2016-08-26 06:34:16","Question_id":231899,"Tags":["machine-learning","neural-networks"],"Answer_count":1,"Last_activity":"2016-08-26 08:31:19","Link":"http://stats.stackexchange.com/questions/231899/neural-network-model-complexity-metric","Creator_reputation":1548}
{"_id":{"$oid":"5837a573a05283111e4d2ecc"},"View_count":18,"Display_name":"mackbox","Question_score":1,"Question_content":"Suppose I know the true distribution,  and I have approximated the true distribution with , which is the predictive posterior density. Does the relative entropy of  with respect to  become:D_{KL}(P^{*}||\\tilde{P})=\\int_{-\\infty}^{\\infty}P^{*}(x)\\,\\log\\frac{P^*(x)}{\\tilde{P}(x|D)} dxAlso, does a smaller relative entropy in this case also imply a smaller distance between my hypothesis and the true distribution?  ","Creater_id":73733,"Start_date":"2016-08-26 07:52:57","Question_id":231917,"Tags":["bayesian","posterior","kullback-leibler"],"Answer_count":1,"Last_activity":"2016-08-26 08:26:23","Link":"http://stats.stackexchange.com/questions/231917/is-this-the-right-interpretation-of-relative-entropy-for-the-bayesian-approach","Creator_reputation":128}
{"_id":{"$oid":"5837a573a05283111e4d2ed9"},"View_count":56,"Display_name":"EngrStudent","Question_score":1,"Question_content":"Is there a clear analytic link from Kernel smoothing, particularly the Nadaraya–Watson estimator, (S-G, H-P, or W-H) smoothing filter?The \"filter\" is called by different names in different fields including analytic chemists \u0026amp; engineers, economists, actuaries, and  the statisticians (who tend to know several variants).When I say \"kernel method\"for smoothing I am referring to an approach like this (link), or possibly this (link).  I give an example in code below as well.  A Gaussian kernel is a variation on the Normal probability density, which by definition is non-negative.  The Epanechnikov and Uniform estimators are defined as piece-wise but share this constraint.The \"filter\", like a kernel based smoother, makes a weighted sum of a window of points around the point of interest.  Unlike the smoother, the \"filter\" has negative valued coefficients.How do I relate the negative weights in the \"filter\" with a kernel approach?  Is there a connection between the two?  Are they fundamentally incompatible?  How would I make a more robust Gaussian kernel smoother that has negative weights.What I mean is this:The coefficients for the 5-element quadratic smoothing filter are \"-3, 12,17,12,and -3\" with a normalization constant of 35.  The absolute values sum to 41.  This means ~15% of weight is negative. Is there a derivation from good solid first principles that gives a Normal Kernel smooth with similarly valued coefficients?    For example, let's say that I want estimate y_true from y_meas as given by the following.set.seed(231529)#create dataN \u0026lt;- 200t \u0026lt;- seq(from=1,to=7*pi,length.out = N)y_true \u0026lt;-  0.23*sin(1.5*t) - 0.5*cos(0.2*t+0.9)#corrupt with noiseepsilon \u0026lt;- +runif(n = N,min=-0.1,max = 0.1)y_meas \u0026lt;- y_true + epsilon#plot itplot(t,y_meas, xlab = \"t\", ylab=\"y\")lines(t,y_true,col=\"Green\",lwd=2)I can do this with Savitzky-Golay as follows:#Savitzy-Golaylibrary(pracma)y_sg \u0026lt;- savgol(y_meas,   #my data               fl = 17,               dorder=0, # snoothing               forder=3)  # quadraticlines(t,y_sg,col=\"Red\")While the Kernel methods are a way to approximate a probability density function there is this: \\mu = \\frac{\\int x\\cdot p\\left(x\\right) } {\\int p \\left(x\\right) } or in a discrete case \\mu = \\frac{\\Sigma x\\cdot p\\left(x\\right) } {\\Sigma p\\left(x\\right) } and so it can make a clean weighted windowed mean for non-uniformly distributed data.#Kernely_k \u0026lt;- numeric(length = length(t) )for (i in 9:(N-8)){     xi \u0026lt;- t[(i-8):(i+8)]-t[i]; #domain     yi \u0026lt;- y_meas[(i-8):(i+8)]; #range     temp \u0026lt;- density(xi, #the window                     bw = \"sj\",                     kernel = \"gaussian\")     #evaluate at our points -      w \u0026lt;- interp1(x=tempy,xi=xi)     #normalize     w \u0026lt;- w/sum(w)     #find mean     y_k[i] \u0026lt;- sum(w*yi)}lines(t,y_k,col=\"Blue\",lwd=3) For the same input data, the SG, using \"eyeball norm\", seems to do better.Here is my plot.The difference between the two is that SG (or such) is robust (negative coefficients) in a way that the classic Gaussian Kernel is not, unless there is a good reason to adjust the weights. Because it has a window, the N-W estimator is tending to regress toward the mean and so it overshoots the valleys and undershoots the peaks.I appreciate the responses so far, and will go dig through them.  Thanks.","Creater_id":22452,"Start_date":"2016-08-24 12:02:10","Question_id":231529,"Tags":["kernel-smoothing","smoothing","filter"],"Answer_count":1,"Last_activity":"2016-08-26 08:00:30","Link":"http://stats.stackexchange.com/questions/231529/savitzky-golay-aka-hodrick-prescot-or-whittaker-henderson-vs-kernel","Creator_reputation":4186}
{"_id":{"$oid":"5837a573a05283111e4d2ee6"},"View_count":20,"Display_name":"user42924","Question_score":1,"Question_content":"The problem is quite simple but I somehow got stuck, so any help would be highly appreciated. In R I fitted a DCC MGARCH in 2 (M=2) dimensions as follows:`uspec \u0026lt;- ugarchspec(variance.model = list(model = \"eGARCH\", garchOrder = c(1, 1), submodel = NULL,                                          external.regressors = NULL, variance.targeting = FALSE),                   mean.model     = list(armaOrder = c(0, 0), external.regressors = NULL),                   distribution.model = \"std\", start.pars = list(), fixed.pars = list())dcc.spec = dccspec(uspec = multispec( replicate(M, uspec)), dccOrder = c(1,1), distribution = \"mvt\")dcc.fit = dccfit(dcc.spec, data = X, out.sample = N.OOS, fit.control = list(eval.se=T))X is my dataset of N + N.OOS rows and M (=2) columns and I want to keep N.OOS observations for out of sample testing. The innovations are multivariate t distributed. Now I want to recalculate the log likelihood. dcc.H \u0026lt;- dcc.fit@mfitcoef[1], dcc.fit@mfitlog.likelihoods)  and my manual recalculation (values) I get small (but large enough such that it is not a numerical issue) discrepancies. Shouldn't the likelihoods that I calculated manually be the same as the one that were calculated by the dccfit routine? Can anybody hint me at where my thinking is wrong?I can provide the data too, if necessary. EDIT:Of course one needs to appropriately scale the dispersion matrix with the degrees of freedom. With the following change the log-likelihoods are much closer but still not exactly the same: dcc.df \u0026lt;- coef(dcc.fit)[15] values = rep(0, 503) for (i in 1:503){ values[i] \u0026lt;- dmvt(x = X[i,], delta = dcc.mean,               sigma = dcc.H[1:2, 1:2, i] * (dcc.df - 2)/dcc.df                        , df = dcc.df)}Still, I am a bit puzzled as to why I am not getting the exact same values. Any ideas?","Creater_id":42924,"Start_date":"2016-08-26 00:40:12","Question_id":231844,"Tags":["time-series","maximum-likelihood","garch"],"Answer_count":0,"Last_activity":"2016-08-26 07:59:13","Link":"http://stats.stackexchange.com/questions/231844/dcc-mgarch-likelihood","Creator_reputation":11}
{"_id":{"$oid":"5837a573a05283111e4d2ee8"},"View_count":7015,"Display_name":"Victor","Question_score":10,"Question_content":"Except for the fact that returns can be negative while prices must be positive, is there any other reason behind modelling stock prices as a log normal distribution but modelling stock returns as a normal distribution?","Creater_id":61158,"Start_date":"2014-11-27 12:55:55","Question_id":125761,"Tags":["normal-distribution","lognormal","finance"],"Answer_count":1,"Last_activity":"2016-08-26 07:38:50","Link":"http://stats.stackexchange.com/questions/125761/why-stock-prices-are-lognormal-but-stock-returns-are-normal","Creator_reputation":1168}
{"_id":{"$oid":"5837a573a05283111e4d2ef5"},"View_count":121,"Display_name":"flobrr","Question_score":4,"Question_content":"For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):   UV1 UV2 AV1    1   1  12    1   1  13    1   1  14    1   1  15    1   1  16    1   1  17    1   1  18    0   0  19    0   0  110   0   0  111   1   1  012   1   1  013   1   0  014   1   0  015   1   0  016   1   0  017   1   0  018   0   0  019   0   0  020   0   0  0AV = dependent variable/criterionUV1 / UV2 = both independant variables/predictorsFor measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code\u0026gt; lrmodel \u0026lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = \"binomial\")including \"family = \"binomial\"\". Is this correct ( I think so :-))?Regarding my test-data, I was wondering about the whole model, especiallythe estimators and sigificance:\u0026gt; summary(lrmodel)Call:glm(formula = AV ~ UV1 + UV2, family = \"binomial\", data = lrdata)Deviance Residuals:     Min       1Q   Median       3Q      Max  -1.7344  -0.2944   0.3544   0.7090   1.1774  Coefficients:              Estimate Std. Error z value Pr(\u0026gt;|z|)(Intercept) -4.065e-15  8.165e-01   0.000    1.000UV1         -1.857e+01  2.917e+03  -0.006    0.995UV2          1.982e+01  2.917e+03   0.007    0.995(Dispersion parameter for binomial family taken to be 1)    Null deviance: 27.726  on 19  degrees of freedomResidual deviance: 17.852  on 17  degrees of freedomAIC: 23.852Number of Fisher Scoring iterations: 17Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. I was expecting that UV2 is a significant discriminator.Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.The whole \"picture\" I gained from the logistic is confusing me...What was consuming me more:When I run a \"NOT-logistic\" regression (by omitting \"family = \"binomial\")\u0026gt; lrmodel \u0026lt;- glm(AV ~ UV1 + UV2, data = lrdata,)I get the expected resultsCall:glm(formula = AV ~ UV1 + UV2, data = lrdata)Deviance Residuals:     Min       1Q   Median       3Q      Max  -0.7778  -0.1250   0.1111   0.2222   0.5000  Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)   (Intercept)   0.5000     0.1731   2.889  0.01020 * UV1          -0.5000     0.2567  -1.948  0.06816 . UV2           0.7778     0.2365   3.289  0.00433 **---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for gaussian family taken to be 0.1797386)    Null deviance: 5.0000  on 19  degrees of freedomResidual deviance: 3.0556  on 17  degrees of freedomAIC: 27.182Number of Fisher Scoring iterations: 2UV1 is not significant! :-)UV2 has an positive effect on AV = 1! :-)The intercept is 0.5! :-)My overall question: Why isn't logistic regression (including \"family = \"binomial\") producing results as expected, but a \"NOT-logistic\" regression (not including \"family = \"binomial\") does?Update:are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56After manipulating the UV2's data AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0UV2: 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation \u0026lt; 0.1 between UV1 and UV2) hence:UV1 UV2 AV1    1   0  12    1   0  13    1   0  14    1   1  15    1   1  16    1   1  17    1   1  18    0   1  19    0   1  110   0   1  111   1   1  012   1   1  013   1   0  014   1   0  015   1   0  016   1   0  017   1   0  018   0   0  019   0   0  020   0   0  0to avoid correlation, my results come closer to my expectations:Call:glm(formula = AV ~ UV1 + UV2, family = \"binomial\", data = lrdata)Deviance Residuals:      Min        1Q    Median        3Q       Max  -1.76465  -0.81583  -0.03095   0.74994   1.58873  Coefficients:            Estimate Std. Error z value Pr(\u0026gt;|z|)  (Intercept)  -1.1248     1.0862  -1.036   0.3004  UV1           0.1955     1.1393   0.172   0.8637  UV2           2.2495     1.0566   2.129   0.0333 *---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 27.726  on 19  degrees of freedomResidual deviance: 22.396  on 17  degrees of freedomAIC: 28.396Number of Fisher Scoring iterations: 4But why does the correlation influence the results of the logistic regression and not the results of the \"not-logistic\" regression? ","Creater_id":129062,"Start_date":"2016-08-26 03:39:43","Question_id":231872,"Tags":["r","regression","logistic","logit"],"Answer_count":0,"Last_activity":"2016-08-26 07:37:06","Link":"http://stats.stackexchange.com/questions/231872/logistic-regression-with-r","Creator_reputation":21}
{"_id":{"$oid":"5837a573a05283111e4d2ef7"},"View_count":55,"Display_name":"Gaston","Question_score":0,"Question_content":"I'm new in the forum and also I'm a beginner on a mixed effect model using lme4 package in R. After reading lot of questions and answer in different forums and also several tutorials and book chapters, I still have some doubts about my formulation of my model.I've done an experiment with two fixed effects and two levels each (Pollution: Yes/No, and Temperature: 17°/22°) and one random effect with 5 levels (Population: 1, 2, 3, 4, 5) that is nested on Pollution effect. For each combination I have 6 replicates. I'm interested on the effect of Pollution and Temperature on the fertilisation success. So I was considering the following model: y = Pollution(Population) + Temperature + Pollution(Population)*Temperature + errHoping that my model includes Population as a random effect nested in Pollution and accounting for interaction with temperature I've been able to piece together the following:   model1\u0026lt;-lmer(rto_Embryos~ Pollution + (1|Pollution:Population) + Pollution*Temp + (1|Replicate), data=mydata, REML=F) Is this accurately expressed? Any suggestions for how to improve my code if it does not correctly depicted?Thank you very much in advance...","Creater_id":128446,"Start_date":"2016-08-21 08:25:07","Question_id":230978,"Tags":["mixed-model","lme4","nested"],"Answer_count":1,"Last_activity":"2016-08-26 07:36:43","Link":"http://stats.stackexchange.com/questions/230978/syntax-doubts-on-lme4","Creator_reputation":1}
{"_id":{"$oid":"5837a573a05283111e4d2f04"},"View_count":59,"Display_name":"luchonacho","Question_score":4,"Question_content":"This is probably a trivial question, but I can't find it online or in the textbooks I have access to. For a simple model:y_{it} = x_{it}\\beta + c_{i} + \\epsilon_{it} After a random-effects estimation, what is the method used to predict ?I know how to obtain them using several software, but I am interest in the methodology behind.","Creater_id":100369,"Start_date":"2016-08-24 02:23:55","Question_id":231437,"Tags":["panel-data","random-effects-model"],"Answer_count":2,"Last_activity":"2016-08-26 07:19:12","Link":"http://stats.stackexchange.com/questions/231437/how-are-unobserved-components-predicted-in-random-effect-models","Creator_reputation":584}
{"_id":{"$oid":"5837a573a05283111e4d2f12"},"View_count":7625,"Display_name":"Mike Spivey","Question_score":5,"Question_content":"I have a large data set consisting of the values of several hundred financial variables that could be used in a multiple regression to predict the behavior of an index fund over time.  I would like to reduce the number of variables to ten or so while still retaining as much predictive power as possible.  Added: The reduced set of variables needs to be a subset of the original variable set in order to preserve the economic meaning of the original variables.  Thus, for example, I shouldn't end up with linear combinations or aggregates of the original variables.Some (probably naive) thoughts on how to do this:Perform a simple linear regression with each variable and choose the ten with the largest  values.  Of course, there's no guarantee that the ten best individual variables combined would be the best group of ten.Perform a principal components analysis and try to find the ten original variables with the largest associations with the first few principal axes.  I don't think I can perform a hierarchical regression because the variables aren't really nested.  Trying all possible combinations of ten variables is computationally infeasible because there are too many combinations.  Is there a standard approach to tackle this problem of reducing the number of variables in a multiple regression?  It seems like this would be a sufficiently common problem that there would be a standard approach.A very helpful answer would be one that not only mentions a standard method but also gives an overview of how and why it works.  Alternatively, if there is no one standard approach but rather multiple ones with different strengths and weaknesses, a very helpful answer would be one that discusses their pros and cons.whuber's comment below indicates that the request in the last paragraph is too broad.  Instead, I would accept as a good answer a list of the major approaches, perhaps with a very brief description of each.  Once I have the terms I can dig up the details on each myself.","Creater_id":2195,"Start_date":"2012-02-07 09:37:27","Question_id":22393,"Tags":["regression","multivariate-analysis","model-selection","multiple-regression"],"Answer_count":4,"Last_activity":"2016-08-26 07:08:53","Link":"http://stats.stackexchange.com/questions/22393/reducing-the-number-of-variables-in-a-multiple-regression","Creator_reputation":163}
{"_id":{"$oid":"5837a573a05283111e4d2f22"},"View_count":40,"Display_name":"e.mal","Question_score":1,"Question_content":"I have the following question that I haven't managed to find a satisfying answer. In an Error Correction Model (assuming that all its assumptions hold):\\Delta y_{t} = a + b(y_{t-1}-\\hat c-\\hat kx_{t-1}) + c\\Delta y_{t-1} + d\\Delta x_{t-1}+ e_{t}what is the accurate interpretation of  as the time that is required to correct the deviation from disequilibrium? Of course it has to be .Until now what I have found is:1) That  measures the half life ie the number of periods that are required to correct half the deviation from equilibrium. Can we infer something about the time that is required to correct fully the deviation?2) That  measures the number of periods that are required to correct fully the deviation ignoring any other short-term disruptions.Complete answers and especially references that discusses exactly this are much appreciated. Thank you in advance!","Creater_id":126928,"Start_date":"2016-08-07 13:35:00","Question_id":228683,"Tags":["time-series","cointegration","ecm"],"Answer_count":1,"Last_activity":"2016-08-26 07:05:31","Link":"http://stats.stackexchange.com/questions/228683/interpretation-of-the-error-correction-term-as-time-to-correct","Creator_reputation":8}
{"_id":{"$oid":"5837a573a05283111e4d2f2e"},"View_count":50,"Display_name":"cgo","Question_score":0,"Question_content":"I am currently learning many aspects of Reinforcement Learning (RL), as is evident in many of my previous posts: Reinforcement learning with subgoalsModel free reinforcement learning with subgoals: how to reinforce learning with only one reward?Formulation of states for this RL problem and other questions.This question is more 'soft', in it I am not really expecting a mathematically rigorous answer. It may bent on some frustrations I have while studying RL and some related topics. So, I am using RL to model an interesting behavior found in some animals. (Sorry, this is as specific as I could go.) In this kind of species, and in RL, there is a task to be done. This is simple, because there REALLY needs to be an objective, or a task to be performed. Otherwise, there wouldn't be a problem! Now, here is the interesting part (plot twist): Something happens while this species perform the task. It exhibits a certain behavior BEFORE achieving the task. You can think of it as some preliminary task it should perform before the main task/goal can be achieved. This is the reason why all those questions on subgoals are present above.CurrentlyI made a RL model of the task. I defined the state representation to be the relative position between the current position and the subgoal position. Rewards are given when the subgoal position is approached. Rewards are also given when the main task/objective is achieved. Is it any good? It gets the job done. You see it perform and whatever needs to be done has been achieved. However, I feel that the 'subgoal' part steals the limelight from the main task. Perhaps at this point, all I have done is show that whatever I have accomplished 'mimics' the interesting animal behavior that is supposed to be modeled.What should be done, ideally?I envision that the main goal should be in the state space representation and not just the subgoal. The main goal should play a more important role, I reckon. (And by 'more important', I suspect it is difficult to quantify this very subjective word.) My main questionHow do you include such a subgoal? Or ... How do you model the task using RL taking into account the interesting behavior I mentioned above? ","Creater_id":67413,"Start_date":"2016-08-25 00:31:38","Question_id":231618,"Tags":["machine-learning","reinforcement-learning"],"Answer_count":0,"Last_activity":"2016-08-26 06:48:15","Link":"http://stats.stackexchange.com/questions/231618/choosing-state-representations-rewards-functions-into-rl","Creator_reputation":591}
{"_id":{"$oid":"5837a573a05283111e4d2f30"},"View_count":60,"Display_name":"Alex","Question_score":1,"Question_content":"I'd like to compute p-values for simple Pearson correlations when using bootstrapping.Is there any opportunity to do this (I'm using SPSS and the only thing you can do within this software is to compute the CI's)? Can I just take the p-values that a simple linear regression will give me (SPSS can do this) or is this incorrect?Thanks for your answer.I'd like to have the p-value as a bootstrapped estimate (just like in the linear regression). The normal p-value gets not bootstrapped as i know, even if the bootstrapped icon is selected.I need bootstrapping because the assumptions aren't that clealry fulfilled in my data.So it is not an Option to use the bootstrapped p-value that SPSS gives me for a simple linear Regression?Thanks!","Creater_id":128980,"Start_date":"2016-08-25 11:08:41","Question_id":231741,"Tags":["correlation","spss","p-value","bootstrap"],"Answer_count":0,"Last_activity":"2016-08-26 06:30:58","Link":"http://stats.stackexchange.com/questions/231741/bootstrapped-p-value-for-pearson-correlation","Creator_reputation":6}
{"_id":{"$oid":"5837a573a05283111e4d2f32"},"View_count":13,"Display_name":"Jonathan Myers","Question_score":1,"Question_content":"I have two contingency tables of frequency data examining the same set of variables but at two different time points.I can make two separate before and after correspondence analysis plots but would anyone know if there's a method to combine these into one graph - say with some small line, arc or graphic to show any movement of the attributes (or just plot the attribute before and after points in different colours?","Creater_id":129076,"Start_date":"2016-08-26 05:42:36","Question_id":231891,"Tags":["factor-analysis","correspondence-analysis"],"Answer_count":0,"Last_activity":"2016-08-26 06:23:39","Link":"http://stats.stackexchange.com/questions/231891/visualizing-a-correspondence-analysis-over-time","Creator_reputation":6}
{"_id":{"$oid":"5837a573a05283111e4d2f34"},"View_count":634,"Display_name":"Rod","Question_score":1,"Question_content":"I did a simple mediation analysis using PROCESS (attached). The result is all significant however the path \"b\" or M -\u003e Y is negative. I've read somewhere, Zhao et al., that this is called \"competitive mediation?\", BUT i do not know how to explain or interpret the result. Can anybody explain this?Thank you.","Creater_id":34886,"Start_date":"2014-04-05 22:17:19","Question_id":92693,"Tags":["mediation"],"Answer_count":2,"Last_activity":"2016-08-26 06:22:34","Link":"http://stats.stackexchange.com/questions/92693/competitive-mediation-explanation","Creator_reputation":16}
{"_id":{"$oid":"5837a573a05283111e4d2f42"},"View_count":49,"Display_name":"EconStats","Question_score":4,"Question_content":"I'm comparing the Chebychev (Weak) Law of Large Numbers (LLN) to the Kolmogorov (Strong) LLN in an econometric textbook and both definitions start off differently. The Chebychev LLN begins withIf  is a sample of of observations such that . . . while the Kolmogorov LLN begins withIf  is a sequence of independently distributed random variable such that . . . I know they refer to convergence in probability and convergence almost surely respectively but what is the significance of using a sample in one and a sequence in the other? Is it harder to prove convergence in a sequence as opposed to a sample?","Creater_id":31188,"Start_date":"2014-05-26 15:46:57","Question_id":100119,"Tags":["sample","convergence","law-of-large-numbers"],"Answer_count":0,"Last_activity":"2016-08-26 06:12:24","Link":"http://stats.stackexchange.com/questions/100119/difference-between-sample-and-sequence-in-law-of-large-numbers","Creator_reputation":390}
{"_id":{"$oid":"5837a573a05283111e4d2f44"},"View_count":149,"Display_name":"sjunmm","Question_score":5,"Question_content":"I have come across a bit of a dilemma in my exam revision, this is not a topic I am particularly strong with so help is most appreciated:Assume that  is an i.i.d sequence, where  and ,Using central limit theorem, find the distribution of \\frac{\\sum_{i=1}^nX_i}{\\sqrt{5n}}Here I have let  and we know that , then we have: Yn=\\frac{\\sum_{i=1}^nX_i - nE(x))}{σ\\sqrt{n}} = \\frac{\\sum_{i=1}^nX_i}{\\sqrt{5n}} -\u0026gt; N(0,1) Next I am asked to apply the strong law of large numbers to  \\frac{\\sum_{i=1}^nX_i^2}{n} to show that it converges almost surely and compute its finite limitI have had a go at this by letting  and  and plugging this into to the strong law of large numbers formula (  a.s.) to find  -\u003e  a.s.However I do not feel as though I am showing that  is converging almost surely using this method?Any ideas would be appreciated Also lastly I have to find, by using central limit theorem, the distribution of \\frac{\\sum_{i=1}^nXi}{\\sqrt{\\sum_{i=1}^nX_i^2}}I am lost on this one completely, although I am making the assumption as we are using central limit theorem with it them it will be N(0,1) but as I say that is just a guess,Help/hints/explanations would be really appreciated","Creater_id":35879,"Start_date":"2013-12-08 14:49:01","Question_id":79007,"Tags":["convergence","central-limit-theorem","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 06:12:10","Link":"http://stats.stackexchange.com/questions/79007/law-of-large-numbers-and-central-limit-theorem","Creator_reputation":30}
{"_id":{"$oid":"5837a573a05283111e4d2f51"},"View_count":115,"Display_name":"Ruth","Question_score":3,"Question_content":"Suppose  and ,  and  are i.i.d samples from X and Y, respectively. Consider the estimator . What does r converge to when sample size  is large? Intuitively, this should be . But what are steps to reach this conclusion and the basis for each step? any help is highly appreciated. In particular, is the following true? On the right hand side, I treat  and  as random variables.\\begin{equation*}  lim_{n\\rightarrow{\\infty}}\\frac{\\sum_{i=1}^{n}(x_i-\\overline{x})^2}{\\sum_{i=1}^{n}(y_i-\\overline{y})^2}=\\frac{E[\\sum_{i=1}^{n}(X_i-\\mu_1)^2]}{E[\\sum_{i=1}^{n}(Y_i-\\mu_2)^2]}\\end{equation*} ","Creater_id":97999,"Start_date":"2015-12-13 17:00:20","Question_id":186600,"Tags":["mathematical-statistics","inference","convergence","asymptotics","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 06:11:53","Link":"http://stats.stackexchange.com/questions/186600/law-of-large-numbers-and-convergence","Creator_reputation":69}
{"_id":{"$oid":"5837a573a05283111e4d2f5e"},"View_count":124,"Display_name":"user3698581","Question_score":4,"Question_content":"This rather looks quite basic, but when referring to weak and strong law of large numbers this is the definition I look at (Casella and Berger)Can you please give an 'intuition' in understanding the difference between them. Also, what does the limits inside the probability signify for the strong law? Can you give me a simulation in R to signify the difference between them? ","Creater_id":97351,"Start_date":"2016-04-04 13:43:02","Question_id":205496,"Tags":["r","intuition","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 06:11:42","Link":"http://stats.stackexchange.com/questions/205496/intuition-behind-strong-vs-weak-laws-of-large-numbers-with-an-r-simulation","Creator_reputation":26}
{"_id":{"$oid":"5837a573a05283111e4d2f6b"},"View_count":196,"Display_name":"user18496","Question_score":5,"Question_content":"I might be missing something basic - but it appears that the strong law of large numbers covers the weak law. If that case, why is the weak law needed?","Creater_id":18496,"Start_date":"2013-01-09 06:16:35","Question_id":47310,"Tags":["probability","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 06:10:45","Link":"http://stats.stackexchange.com/questions/47310/weak-law-of-large-numbers-redundant","Creator_reputation":26}
{"_id":{"$oid":"5837a573a05283111e4d2f78"},"View_count":7687,"Display_name":"user9097","Question_score":9,"Question_content":"The central limit theorem states that the mean of i.i.d. variables, as  goes to infinity, becomes normally distributed. This raises two questions:Can we deduce from this the law of large numbers? If the law of large numbers says that the mean of a sample of a random variable's values equals the true mean  as  goes to infinity, then it seems even stronger to say that (as the central limit says) that the value becomes  where  is the standard deviation. Is it fair then to say that central limit implies the law of large numbers?Does the central limit theorem apply to linear combination of variables?Thanks.","Creater_id":9097,"Start_date":"2012-02-09 18:10:35","Question_id":22557,"Tags":["probability","central-limit-theorem","law-of-large-numbers"],"Answer_count":3,"Last_activity":"2016-08-26 06:10:37","Link":"http://stats.stackexchange.com/questions/22557/central-limit-theorem-versus-law-of-large-numbers","Creator_reputation":583}
{"_id":{"$oid":"5837a573a05283111e4d2f87"},"View_count":1182,"Display_name":"Tim","Question_score":7,"Question_content":"By (weak/strong) law of large numbers, given some iid sample points  of a distribution, their sample mean  converges to the distribution mean both in probability and a.s., as sample size   goes to infinity.When the sample size  is fixed, I wonder if the LLN estimator  is an estimator best in some sense?For example, its expectation is the distribution mean, so it  is an unbiased estimator. Its variance is  where  is the distribution variance. But is it UMVU?is there some function  such that  solvethe minimization problem:  f^*(\\{x_i, i=1,\\ldots,N\\}) = \\operatorname{argmin}_{u\r    \\in \\mathbb{R}^n} \\quad \\sum_{i=1}^N l_0(x_i, u)? In other words,  is the best wrt some contrast function  in the minimum contrast framework (c.f. Section 2.1 \"Basic Heuristics of Estimation\" in \"Mathematical statistics:basic ideas and selected topics, Volume 1\" by Bickle and Doksum).For example, if the distribution is known/restricted to be from the family of Gaussian distributions, then sample mean will be the MLE estimator of distribution mean, and MLE belongs to the minimum contrast framework, and its contrast function  is minus the log likelihood function.is there some function  such that  solve the minimizationproblem:  f^* = \\operatorname{argmin}_{f} \\quad \\operatorname{E}_{\\text{iid }\\{x_i, i=1,\\ldots,N\\} \\text{ each with distribution }P } \\quad l(f(\\{x_i, i=1,\\ldots,N\\}), P)?\r    for any distribution  of  within some family  of distributions?In other words,  is the best wrt some lost function  and some family  of distributions in the decision theoretic framework (c.f. Section 1.3 \"The Decision Theoretic Framework\" in \"Mathematical statistics:basic ideas and selected topics, Volume 1\" by Bickle and Doksum).Note that the above are three different interpretations for a \"best\" estimation that I have known so far. If you know about other possible interpretations that may apply to the LLN estimator, please don't hesitate to mention that as well.Thanks and regards!","Creater_id":1005,"Start_date":"2011-10-04 16:32:25","Question_id":16507,"Tags":["estimation","expected-value","law-of-large-numbers"],"Answer_count":2,"Last_activity":"2016-08-26 06:09:49","Link":"http://stats.stackexchange.com/questions/16507/is-sample-mean-the-best-estimation-of-distribution-mean-in-some-sense","Creator_reputation":5507}
{"_id":{"$oid":"5837a573a05283111e4d2f95"},"View_count":1011,"Display_name":"emanuele","Question_score":10,"Question_content":"The question is simply what is stated in the title:  When does the law of large numbers fail?  What I mean is, in what cases will the frequency of an event not tend to the theoretical probability?","Creater_id":10017,"Start_date":"2012-06-06 03:13:37","Question_id":29882,"Tags":["probability","mathematical-statistics","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 06:09:38","Link":"http://stats.stackexchange.com/questions/29882/when-does-the-law-of-large-numbers-fail","Creator_reputation":671}
{"_id":{"$oid":"5837a573a05283111e4d2fa2"},"View_count":732,"Display_name":"Pegah","Question_score":12,"Question_content":"I have a very beginner's question regarding the Central Limit Theorem (CLT):I am aware that the CLT states that a mean of i.i.d. random variables is approximately normal distributed (for , where  is the index of the summands) or the standardized random variable would have a standard normal distribution.Now the Law of Large Number states roughly speaking that the mean of i.i.d random variables converges (in probability or almost surely) to their expected value.What I don't understand is: If, as the CLT states, the mean is approximately normally distributed, how then can it also converge to the expected value at the same time? Convergence would imply for me that with time the probability that the mean takes a value which is not the expected value is almost zero, hence the distribution would not really be a normal but almost zero everywhere except at the expected value.Any explanation is welcome.","Creater_id":20395,"Start_date":"2013-02-02 10:49:10","Question_id":49123,"Tags":["probability","normal-distribution","convergence","central-limit-theorem","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-26 06:09:31","Link":"http://stats.stackexchange.com/questions/49123/central-limit-theorem-and-the-law-of-large-numbers","Creator_reputation":77}
{"_id":{"$oid":"5837a573a05283111e4d2faf"},"View_count":2117,"Display_name":"Cassandra Gelvin","Question_score":15,"Question_content":"I'm trying to make a video about loaded dice, and at one point in the video we roll about 200 dice, take all the sixes, roll those again, and take all the sixes and roll those a third time. We had one die that came up 6 three times in a row, which is obviously not unusual because there should be a 1/216 chance of that happening and we had about 200 dice. So how do I explain that it's not unusual? It doesn't quite seem like the Law of Large Numbers. I want to say something like \"If you do enough tests, even unlikely things are bound to happen\" but my partner said people might take issue with the \"bound to\" terminology.Is there a standard way to state this concept?","Creater_id":29282,"Start_date":"2013-08-19 00:18:32","Question_id":67721,"Tags":["probability","dice","law-of-large-numbers"],"Answer_count":4,"Last_activity":"2016-08-26 06:09:23","Link":"http://stats.stackexchange.com/questions/67721/is-there-a-law-that-says-if-you-do-enough-trials-rare-things-happen","Creator_reputation":105}
{"_id":{"$oid":"5837a573a05283111e4d2fbf"},"View_count":4783,"Display_name":"bnjmn","Question_score":27,"Question_content":"I'm thinking of this from a very basic, minimal requirements perspective. What are the key theories an industry (not academic) statistician should know, understand and utilize on a regular basis? A big one that comes to mind is Law of large numbers. What are the most essential for applying statistical theory to data analysis? ","Creater_id":7471,"Start_date":"2012-02-14 09:57:28","Question_id":22804,"Tags":["theory","careers","law-of-large-numbers"],"Answer_count":7,"Last_activity":"2016-08-26 06:09:04","Link":"http://stats.stackexchange.com/questions/22804/what-theories-should-every-statistician-know","Creator_reputation":113}
{"_id":{"$oid":"5837a573a05283111e4d2fd2"},"View_count":2563,"Display_name":"mpiktas","Question_score":33,"Question_content":"Here is the article in NY times called \"Apple confronts the law of large numbers\". It tries to explain Apple share price rise using law of large numbers. What statistical (or mathematical) errors does this article make? ","Creater_id":2116,"Start_date":"2012-03-13 00:12:16","Question_id":24562,"Tags":["probability","central-limit-theorem","law-of-large-numbers","statistics-in-media"],"Answer_count":3,"Last_activity":"2016-08-26 06:08:05","Link":"http://stats.stackexchange.com/questions/24562/why-law-of-large-numbers-does-not-apply-in-the-case-of-apple-share-price","Creator_reputation":24961}
{"_id":{"$oid":"5837a573a05283111e4d2fe1"},"View_count":24,"Display_name":"Brian","Question_score":0,"Question_content":"I want to understand if the coefficient of variation () is the right calculation to use for measuring how far off I am when shipping goods to customers.  The date I want to ship to customers is the target date and the date I do ship to customers is the ship date.For my orders, if I took  of target date - ship date, would that be a normalized way to measure how far off I am to my target ship dates?  Some additional details:Some goods take longer than others to produce and get ready to shipI want to penalize myself both for shipping early and late.  Late is obviously bad.  Early is bad too, as I am probably late on something else.This results in negative numbers in the aggregation (i.e., if I generally ship late), which Wikipedia says was not good for coefficient of variation.","Creater_id":7770,"Start_date":"2016-08-26 04:36:08","Question_id":231879,"Tags":["coefficient-of-variation"],"Answer_count":1,"Last_activity":"2016-08-26 06:05:40","Link":"http://stats.stackexchange.com/questions/231879/coefficient-of-variation-for-shipping","Creator_reputation":33}
{"_id":{"$oid":"5837a573a05283111e4d2fee"},"View_count":28,"Display_name":"User9523","Question_score":0,"Question_content":"We're given the random variables  ,  \u0026amp;  defined as follows :  : The claim amount received. : The amount paid by the insurer. : The amount paid by the re-insurer.Let's say the retention limit is . Thus the function  and  are defined as follows :  Y = \\begin{cases}       X, \u0026amp; X\u0026lt;M \\\\      M, \u0026amp; X  \\geq M \\\\ \\end{cases}  Z = \\begin{cases}       0, \u0026amp; X\u0026lt;M \\\\      X-M, \u0026amp; X  \\geq M \\\\ \\end{cases} Now we want to find the amount paid by the re-insurer : For this , what I do is , I work out  , which is given by : =\u003e My question is , what's the difference between  and .The function  too gives the expected amount paid by the re-insurer , whereas in the texts I read , it's given by .Can anyone explain ?","Creater_id":128502,"Start_date":"2016-08-26 03:39:54","Question_id":231873,"Tags":["distributions","expected-value"],"Answer_count":1,"Last_activity":"2016-08-26 06:02:22","Link":"http://stats.stackexchange.com/questions/231873/difference-in-formulas-for-expected-payout-by-reinsurer","Creator_reputation":149}
{"_id":{"$oid":"5837a573a05283111e4d2ffb"},"View_count":13,"Display_name":"FJC","Question_score":1,"Question_content":"I have a set of 6 statistics which I have measured on many simulations, and I have combined them using PCA to use just the two first principle components. But looking at the relative importance of each of the statistics in my PCs, some of them seem to have surprisingly low weights given that I know they separate my simulations very well.To give an example, statistic A varies very nicely across my simulated parameter space with small standard deviations, but has weights of less than 0.1 in most of the principle components! I have noticed that statistic A has a small dynamic range (varies between 0.01 and 0.2) while others have much bigger ranges (varying form 2 to 8, for example). Would this cause it to have a lower weighing in my analysis?","Creater_id":112092,"Start_date":"2016-08-26 04:52:39","Question_id":231882,"Tags":["pca"],"Answer_count":0,"Last_activity":"2016-08-26 05:50:07","Link":"http://stats.stackexchange.com/questions/231882/does-the-range-of-a-variable-make-a-difference-to-its-weight-in-principle-compon","Creator_reputation":113}
{"_id":{"$oid":"5837a573a05283111e4d2ffd"},"View_count":25,"Display_name":"Daniel","Question_score":0,"Question_content":"I am afraid that is a very beginner's question, but search didn't help me (or I didn't know the terms to use).I have a corpus of sentences which are coded for their word order (like “subject first” or “object first”). The following is an (imagined) illustration:Sentence WordOrder1        Subject-Object2        Subject-IndirectObject-Object3        Object-Subject4        Subject-Object-IndirectObject…        …Frequencies:  Subject-Object: 2.300  Subject-IndirectObject-Object: 30  Object-Subject: 280  Subject-Object-IndirectObject: 560I would like to test whether the differences in the frequency the various word order are significant. For instance, is Subject-Object significantly more common than Object-Subject?However, I am not sure what to use, since I do not have different groups, so that a chi^2 test doesn't work (if I understand it correctly).Is there a test for this or do I have to “construct” different groups (e.g. according to when the sentence was uttered or by whom)?","Creater_id":128571,"Start_date":"2016-08-25 12:00:48","Question_id":231746,"Tags":["statistical-significance","frequency"],"Answer_count":1,"Last_activity":"2016-08-26 05:36:04","Link":"http://stats.stackexchange.com/questions/231746/test-for-differences-in-frequencies-in-one-group","Creator_reputation":60}
{"_id":{"$oid":"5837a573a05283111e4d300a"},"View_count":43,"Display_name":"dixi","Question_score":3,"Question_content":"For example: I want to survey a group of people to get the average time the call will ring before answered. To calculate a sample size, I used the formula n = \\frac{z^2 \\times \\text{Variance}}{(\\text{Margin of error})^2} eg. for a 95% CI and 5% margin of error and variance is 5 seconds n = \\frac{1.96^2 \\times 5}{0.05^2} = 7683.2How do you interpret the  here? Is it the total seconds we need to sample or will it be the number of people we need to sample?","Creater_id":3379,"Start_date":"2016-08-26 04:55:42","Question_id":231884,"Tags":["sample-size"],"Answer_count":0,"Last_activity":"2016-08-26 05:23:57","Link":"http://stats.stackexchange.com/questions/231884/how-to-compute-sample-size","Creator_reputation":36}
{"_id":{"$oid":"5837a573a05283111e4d300c"},"View_count":147,"Display_name":"user1205901","Question_score":5,"Question_content":"I have data in which about 150 subjects each separately estimate 6 different quantities. The quantities are the answers to general knowledge questions like \"How many far apart in kilometres are Milan and Minsk?\", for which it's very unlikely anyone will know the exact value.I have the true value for all 6 quantities. I can see how confident (on a scale of 1 to 5) each subject was in each estimate that they made.I'm interested in the accuracy of aggregations of estimates, and with that in mind I can calculate the mean or median estimate across everyone. This produces rather accurate results, with the median doing particularly well (the mean gets affected by wild outliers, which in this context are possible). This is often known as the wisdom of the crowd.However, I wonder if it might work better to weight the estimates by confidence, with a confident estimate being weighted more highly than an unconfident estimate. How can I decide what the optimal weighting scheme is? As per the comments below, it may be best to assume that minimizing the MSE of the crowd's estimate is the goal, although minimising median error, or any other appropriate measure of error, would also be fine.I investigated the data a bit further, and have made it accessible here.I looked at the correlation between confidence and MSE on the questions, and it's -0.06. This is a modest correlation, but it's in the right direction, since that indicates that higher confidence was associated with lower MSE.","Creater_id":9162,"Start_date":"2015-11-12 05:50:33","Question_id":181440,"Tags":["error","aggregation","mse","weighted-data"],"Answer_count":1,"Last_activity":"2016-08-26 05:01:14","Link":"http://stats.stackexchange.com/questions/181440/how-can-i-find-the-optimal-weighting-scheme-to-aggregate-these-estimates","Creator_reputation":1998}
{"_id":{"$oid":"5837a574a05283111e4d3019"},"View_count":359,"Display_name":"Trees4theForest","Question_score":9,"Question_content":"I am looking at whether abundance is related to size. Size is (of course) continuous, however, abundance is recorded on a scale such that A = 0-10B = 11-25C = 26-50D = 51-100E = 101-250F = 251-500G = 501-1000H = 1001-2500I = 2501-5000J = 5001-10,000etc... A through Q... 17 levels. I was thinking one possible approach would be to assign each letter a number: either the minimum, maximum, or median (ie A=5, B=18, C=38, D=75.5...). What are the potential pitfalls -- and as such, would it better to treat this data as categorical?I have read through this question which provides some thoughts -- but one of the keys of this data set is that the categories are not even -- so treating it as categorical would assume the difference between A and B is the same as the difference between B and C... (which can be rectified by using logarithm - thanks Anonymouse)Ultimately, I would like to see whether size can be used as a predictor for abundance after taking other environmental factors into consideration. The prediction will also be in a range: Given size X and factors A, B, and C we predict that Abundance Y will fall between Min and Max (which I suppose could span one or more scale points: More than Min D and less than Max F... though the more precise the better).","Creater_id":7534,"Start_date":"2011-12-08 04:50:52","Question_id":19534,"Tags":["categorical-data","variance","model","continuous-data"],"Answer_count":2,"Last_activity":"2016-08-26 04:38:50","Link":"http://stats.stackexchange.com/questions/19534/best-practices-when-treating-range-data-as-continuous","Creator_reputation":180}
{"_id":{"$oid":"5837a574a05283111e4d3027"},"View_count":868,"Display_name":"sgh","Question_score":6,"Question_content":"I'm stuck with a regression modeling problem. I have panel data where the dependent variable is a probability. Below is an excerpt from my data. The complete panel covers more countries and years, however it is unbalanced. What I can observe is the number of events and the number of trials. The event probability was derived from those values (estimation of this probability should be quite good, given the large number of trials). All independent variables are county-year specific.     country  year  event_prob  events trials    x    x_lag2 ... more variables  1   Cyprus  2008  0.03902140  11342  290661   4.60   4.13  ...  2   Cyprus  2009  0.04586650  13482  293940   4.60   4.48  ...  3   Cyprus  2010  0.05188398  15206  293077   4.60   4.60  ...  4   Cyprus  2011  0.06433411  18505  287639   5.79   4.60  ...  5  Estonia  2008  0.07872978  21686  275449   6.02   4.11  ...  6  Estonia  2009  0.09516270  33599  353069  13.18   4.91  ...  7  Estonia  2010  0.08645905  36180  418464   7.95   6.03  ...  8  Estonia  2011  0.07731997  31590  408562   5.53  13.18  ...  ...165  USA  2011  0.06100000  9192822  150702000   2.73  3.27  ...My goal is to use regression analysis to find out which variables are significant for the event probability. In R-terminology, I'm looking for a model of the form event_prob ~ x + x_lag2 + ... .The problem is as follows: event_prob has to be between 0 and 1, hence using event_prob ~ x + x_lag2 + ... might not be the best idea. So I was thinking of using the logit transform of event_prob such that logit(event_prob) ranges from  to . The first idea was to use the R's plm package, i.e. plm(logit(event_prob)~x+x_lag2,data,index=c(\"country\",\"year\"),model=\"random\") or model=\"within\" (see below). Is that a reasonable approach or am I violating some essential assumptions?I was also thinking of using panel generalized linear models from the package pglm (with the logit link function), however since I don't know the outcome of the binary events (only the total number of events and trials) is known, I got stuck there. Maybe someone can help me how to proceed here.Since I have panel data, I'd like to compute both fixed-effects models and random-effects model and then apply the Hausman (1978) test to decide which model is more appropriate.Do my first attempts at modeling make sense? I'm really not sure how to correctly address this problem. I hope the description of my problem is detailed enough. If not, I'm happy to provide more detailsIn terms of software, I'd prefer R. SAS and SPSS are also ok since my university has licences for them. I just don't have much experience with them.","Creater_id":25368,"Start_date":"2013-05-08 02:00:54","Question_id":58448,"Tags":["r","generalized-linear-model","panel-data","random-effects-model","fixed-effects-model"],"Answer_count":1,"Last_activity":"2016-08-26 04:31:24","Link":"http://stats.stackexchange.com/questions/58448/which-model-for-panel-data-with-dependent-variables-from-0-1","Creator_reputation":31}
{"_id":{"$oid":"5837a574a05283111e4d3034"},"View_count":99,"Display_name":"DeltaIV","Question_score":0,"Question_content":"After answering to question Compare the statistical significance of the difference between two polynomial regressions in R, I realized that I have always assumed that ggplot2 plots simultaneous confidence bands, not pointwise confidence bands, without actually knowing that for sure. I asked on SO: http://stackoverflow.com/q/39110516/1711271. I got an interesting answer, which I tried to apply. Results however can be weird:library(dplyr)# sample datasetssetosa \u0026lt;- iris %\u0026gt;% filter(Species == \"setosa\") %\u0026gt;% select(Sepal.Length, Sepal.Width, Species)virginica \u0026lt;- iris %\u0026gt;% filter(Species == \"virginica\") %\u0026gt;% select(Sepal.Length, Sepal.Width, Species)# compute simultaneous confidence bandssetosa \u0026lt;- setosa %\u0026gt;% arrange(Sepal.Length)virginica \u0026lt;- virginica %\u0026gt;% arrange(Sepal.Length)# 1. compute linear modelsModel \u0026lt;- as.formula(Sepal.Width ~ poly(Sepal.Length,2))fit1  \u0026lt;- lm(Model, data = setosa)fit2  \u0026lt;- lm(Model, data = virginica)# 2. compute design matricesX1  \u0026lt;- model.matrix(fit1)X2  \u0026lt;- model.matrix(fit2)# 3. general linear hypothesescht1 \u0026lt;- multcomp::glht(fit1, linfct = X1) cht2 \u0026lt;- multcomp::glht(fit2, linfct = X2) # 4. simultaneous confidence bands (finally!)cc1 \u0026lt;- confint(cht1); cc1 \u0026lt;- as.data.frame(cc1confint)setosalwrsetosauprvirginicalwrvirginicaupr# combine datasetsmydata \u0026lt;- rbind(setosa, virginica)# plot both simultaneous confidence bands and pointwise confidence# bands, to show the differencelibrary(ggplot2)# prepare a plot using dataframe mydata, mapping sepal Length to x,# sepal width to y, and grouping the data by speciesggplot(data = mydata, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + # add data pointsgeom_point() +# add quadratic regression with orthogonal polynomials and 95% pointwise# confidence intervalsgeom_smooth(method =\"lm\", formula = y ~ poly(x,2)) +# # add 95% simultaneous confidence bandsgeom_ribbon(aes(ymin = LowerBound, ymax = UpperBound),alpha = 0.5, fill = \"grey70\")Questions:How would you plot the simultaneous confidence bands? Using ribbons with some transparency sort of does the job, but I'd rather not have the colored contours around the ribbons.Why both the upper and lower boundary of the setosa ribbon are so smooth, while the upper bound of the virginca ribbon is so jagged? I would expect simultaneous confidence bands to be \"hyperbolic\" bands around the regression curve, thus very smooth. Am I computing the right thing here?PS just for the sake for clarity, I'm not interested in prediction bands. Here the focus is on simultaneous confidence bands and pointwise confidence bands.","Creater_id":58675,"Start_date":"2016-08-25 02:20:22","Question_id":231632,"Tags":["r","regression","confidence-interval","ggplot2"],"Answer_count":1,"Last_activity":"2016-08-26 04:19:40","Link":"http://stats.stackexchange.com/questions/231632/how-to-plot-simultaneous-and-pointwise-confidence-bands-for-linear-regression-wi","Creator_reputation":1259}
{"_id":{"$oid":"5837a574a05283111e4d3041"},"View_count":47,"Display_name":"mjktfw","Question_score":0,"Question_content":"I'm using the nlme package's lme function in R to fit a random-intercept, random-slope linear mixed model for some generated test data.  Although the fixed effect coefficients are estimated as expected, the variance parameter estimation yields results I do not fully understand.Specifically, the rho parameter of the  compound symmetry correlation structure specified in the model statement and correlation of the random effects are not estimated, as I would expect from the data. When not fixing the correlation in corCompSymm(), the estimated rho is almost equal to default 0, and correlation of random effects is then obviously wrong (0.64 instead of -0.5). I'm aware, that the variance-covariance parameters are estimated marginally, but is there a way to improve the estimates, given I correctly specify the correlation matrix type (CS, AR, etc.) but do not know it's parameters? And secondly (somehow unrelated), does R allow for fitting a linear model with no random effects specified, but assumed structure for within-subject correlation matrix (analogous to SAS's PROC MIXED with no RANDOM statement)The fitted model is:lme(data = dat, fixed = y ~ as.factor(f) * x, random = ~ 1 + x | id, correlation = corCompSymm(fixed = FALSE))and the outcome variable is simulated as:y \u0026lt;- ifelse(f == 0, beta0[1], beta0[2]) +      ifelse(f == 0, beta1[1] * x, beta1[2] * x + b1 * x) +      b0 + b1 * x + e, where: f is a subject-specific grouping factor with 2 levelsx simulates ordinal variable over which the repeated measures have been taken (design is balanced)beta0 and beta1 are vectors of group-specific intercept and slope, consecutivelye is an error-term, following multivariate normal with means 0 and compound symmetry variance-covariance matrix, having 1 diagonal and .8 off-diagonalb0 and b1 are subject-specific errors for intercept, and slope consecutively, simulated to follow bivariate standard normal with covariance -0.5: \\begin{bmatrix}1 \u0026amp; - 0.5\\\\ - 0.5 \u0026amp; 1\\end{bmatrix}The estimated parameters:  Correlation Structure: Compound symmetry  Formula: ~1 | id   Parameter estimate(s):           Rho   4.260007e-06    \u0026gt; VarCorr(f3)  id = pdLogChol(1 + x)               Variance  StdDev    Corr  (Intercept) 0.5682353 0.7538139 (Intr)  x           2.5251728 1.5890792 0.64  Residual    5.0764053 2.2530879   Reproducible code:require(reshape2)require(nlme)require(MASS)set.seed(1)n \u0026lt;- 1000 # number of subjectsm \u0026lt;- 4 # rep. measurments per subjectbeta0 \u0026lt;- c(2, 5) # intercepts for each subject-specific factorbeta1 \u0026lt;- c(2, 3) # slopes for each subject-specific factor# correlation matrix for random effectsd \u0026lt;- - .50D \u0026lt;- matrix(c(1, d,              d, 1), nrow = 2, byrow = T)# correlation matrix for error termsr \u0026lt;- .8R \u0026lt;- c(1, r, r, r,       r, 1, r, r,       r, r, 1, r,       r, r, r, 1) * 5R \u0026lt;- matrix(R, nrow = sqrt(length(R)))R \u0026lt;- R[1:m, 1:m]dat \u0026lt;- data.frame(id = 1:n)datx \u0026lt;- rep(1:m, n)# error termdaty \u0026lt;- with(dat,               ifelse(f == 0, beta0[1], beta0[2]) +                ifelse(f == 0, beta1[1] * x, beta1[2] * x + b1 * x) +                b0 + b1 * x + e)# lmm with assumed compound symmetry and correlation not fixedsummary(f3 \u0026lt;- lme(data = dat, fixed = y ~ as.factor(f) * x, random = ~ 1 + x | id,                   correlation = corCompSymm(fixed = FALSE)))VarCorr(f3)# lmm with with fixed correlationsummary(f4 \u0026lt;- lme(data = dat, fixed = y ~ as.factor(f) * x, random = ~ 1 + x | id,                   correlation = corCompSymm(r, fixed = TRUE)))VarCorr(f4)    ","Creater_id":25757,"Start_date":"2016-08-25 16:02:31","Question_id":231795,"Tags":["mixed-model","multivariate-analysis","lme","nlme"],"Answer_count":2,"Last_activity":"2016-08-26 04:18:14","Link":"http://stats.stackexchange.com/questions/231795/how-to-correctly-specify-the-within-group-correlation-matrix-for-linear-mixed-mo","Creator_reputation":30}
{"_id":{"$oid":"5837a574a05283111e4d304f"},"View_count":27,"Display_name":"new2matlab","Question_score":0,"Question_content":"This question is a follow on from a question I asked here :http://stackoverflow.com/questions/39127208/how-do-i-correctly-plot-the-clusters-produced-from-a-cluster-analysis-in-matlab/39142286?noredirect=1#comment65632215_39142286I have carried out hierarchical clustering in Matlab to create five clusters from my data with 200 observations and 10 variables:What I want to know is how to statistically identify the variables in a cluster analysis that are the most different between clusters: ie. the variables that contribute most to creating the partition between two clusters? What I have done so far is to create boxplots of each of the variables across all the clusters in one graph...which allows me to graphically ascertain if a particular variable varies alot across clusters. Is there a better way to do this?","Creater_id":127835,"Start_date":"2016-08-26 03:43:42","Question_id":231875,"Tags":["hierarchical-clustering","boxplot"],"Answer_count":1,"Last_activity":"2016-08-26 03:54:57","Link":"http://stats.stackexchange.com/questions/231875/how-can-i-identify-the-variables-that-contribute-most-to-creating-the-partition","Creator_reputation":21}
{"_id":{"$oid":"5837a574a05283111e4d305c"},"View_count":79,"Display_name":"F.Uhl","Question_score":-1,"Question_content":"I have a dataset of behavioural observations in an experiment on free ranging breeding birds. I'd like to calculate GLMMs. The dataset has the following parameters:Behaviour (count data)Behaviour during Baseline (count data)NestIndividualTreatment ConditionSex of IndividualAmount of time an individual was seen during experimentI would like to use the time an individual was seen as a model weight, as I want cases in which the individual in question is seen more to have higher influence in the model than cases in which they are only seen very rarely.When checking my response variable (the counts of the behaviour) it most likely fits a negative binomial distibution:When looking at the data the it has a very high count of zeros (133 out of 240 to be exact)quantile(Datapar, ctrl = control$checkConv,  :  Model failed to converge with max|grad| = 0.017808 (tol = 0.001, component 1)I have tried to simplify the model, yet I did not manage to get any models calculated. I also tried both optimisers included in lme4.My question is as follows: Is there a way to model my data in a way that accounts for the amount of time an individual was seen during the experiment?I'm running lme4 version 1.1-12 in R 3.2.1 under Windows 7 (x86_64-w64-mingw32).","Creater_id":128902,"Start_date":"2016-08-25 06:41:25","Question_id":231680,"Tags":["r","lme4","convergence","negative-binomial","nested"],"Answer_count":1,"Last_activity":"2016-08-26 03:52:07","Link":"http://stats.stackexchange.com/questions/231680/glmer-nb-with-weights-and-nested-random-effects-convergence-error","Creator_reputation":1}
{"_id":{"$oid":"5837a574a05283111e4d3069"},"View_count":2670,"Display_name":"Sildona Ristani","Question_score":2,"Question_content":"I have generated a nonparametric percentiles bootstrap confidence interval (), and a BCA confidence interval () with an initial sample size of .  Evidently, the BCA interval is larger than the percentiles interval. Shouldn't the BCA interval be better than the percentiles interval (and by \"better\" I mean \"smaller\")? I noticed also that when the sample size is larger, for example , BCa confidence interval decreases. ","Creater_id":16873,"Start_date":"2012-11-15 01:36:01","Question_id":43635,"Tags":["r","nonparametric","bootstrap"],"Answer_count":1,"Last_activity":"2016-08-26 03:43:25","Link":"http://stats.stackexchange.com/questions/43635/comparison-between-bootstrap-percentiles-confidence-interval-and-bca-confidence","Creator_reputation":13}
{"_id":{"$oid":"5837a574a05283111e4d3076"},"View_count":49,"Display_name":"Marieke","Question_score":1,"Question_content":"I am trying to compare two forecasts using the Mariano Diebold test in R. Both  forecasts are for 150 days ahead; that is, on day  I forecast . I deduced from this post that my forecast horizon . Using that, the Diebold-Mariano test (implemented using function dm.test in \"forecast\" package in R) gives a p-value of 1 no matter what forecasts I compare. I looked into the code of this function, and I figured that this is caused by the following 3 lines of code (d is the vector of loss-differential series):n \u0026lt;- length(d)k \u0026lt;- ((n + 1 - 2 * h + (h/n) * (h - 1))/n)^(1/2)   STATISTIC \u0026lt;- STATISTIC * kSince in my case n = h, the variable k will always be 0 and therefore the test statistic is always zero. Questions:Does this mean that we cannot use the Diebold-Mariano test when we are forecasting an entire period at once? I could not find any evidence of this in their paper. How should I proceed to find a model-based way to compare my forecasts, rather than for example simply taking the MSE?","Creater_id":129054,"Start_date":"2016-08-26 02:17:52","Question_id":231860,"Tags":["r","forecasting","accuracy"],"Answer_count":1,"Last_activity":"2016-08-26 03:38:05","Link":"http://stats.stackexchange.com/questions/231860/diebold-mariano-test-for-multiple-prediction-horizons","Creator_reputation":8}
{"_id":{"$oid":"5837a574a05283111e4d3083"},"View_count":59,"Display_name":"Asma","Question_score":0,"Question_content":"I have a data frame with 1200 observations and 30 variables and I'am trying to do a multinomial logistic regression to explain the intentions of vote of Tunisian citizens using multinom(). my dependent variable has 10 levels.When I executed the command multinom () i got this warning Warning messages: 1: In sqrt(diag(vc)) : NaNs produced so i reduced the number of the predictor variables to 13 , the levels of my dependent variable to only 3 and the warning message no longer appears , but once I calculate the p.value the mojority of my predictor variables are non significant.      \u0026gt; str(k)    'data.frame':   1081 obs. of  19 variables:      REGION    : Factor w/ 24 levels \"Ariana\",\"Beja\",..: 23 23 23 23 23 23 23 23 23 23 ...      Q3A       : Factor w/ 5 levels \"Fairly bad\",\"Fairly good\",..: 2 1 1 4 4 4 2 4 1 3 ...        Q7        : Factor w/ 2 levels \"Going in the right direction\",..: 1 2 2 2 2 2 2 2 2 1 ...        Q27       : Factor w/ 9 levels \"Did not vote for some other reason\",..: 6 6 6 6 6 3 6 6 6 1 ...        Q63PT1    : Factor w/ 8 levels \" Services gouvernementaux\",..: 5 5 4 4 4 4 5 4 4 5 ...        Q96       : Factor w/ 3 levels \"No (looking)\",..: 3 2 2 2 1 2 2 3 2 1 ...        Q97       : Factor w/ 4 levels \"Aucune éducation formelle \",..: 1 3 1 4 4 3 4 3 1 4 ...       out=relevel(kcoefficients/summary(fit)out)    Ne pas voter       Nahdha Nidaa Tounes      307          292          266 ","Creater_id":121657,"Start_date":"2016-08-25 14:18:02","Question_id":231777,"Tags":["r","regression","logistic","p-value","multinomial"],"Answer_count":1,"Last_activity":"2016-08-26 03:29:34","Link":"http://stats.stackexchange.com/questions/231777/multinomial-logistic-regression-nans-produced-in-r-and-no-significant-variables","Creator_reputation":16}
{"_id":{"$oid":"5837a574a05283111e4d3090"},"View_count":740,"Display_name":"Anton Andreev","Question_score":7,"Question_content":"What conclusions can we draw if ? Does not rejecting the  mean anything?","Creater_id":74734,"Start_date":"2015-07-30 07:54:00","Question_id":163957,"Tags":["hypothesis-testing","t-test","z-test"],"Answer_count":3,"Last_activity":"2016-08-26 03:20:47","Link":"http://stats.stackexchange.com/questions/163957/what-follows-if-we-fail-to-reject-the-null-hypothesis","Creator_reputation":139}
{"_id":{"$oid":"5837a574a05283111e4d309f"},"View_count":77,"Display_name":"juod","Question_score":4,"Question_content":"Given two multidimensional datasets,  and , some people perform multivariable analysis by building a surrogate dependent variable using principal component analysis (PCA). That is, run PCA on  set, take scores along the first component , and run a multiple regression of those scores on : . (I am basing my question on this article).  It looks like some adulterated form of canonical correlation analysis (CCA) between the two datasets to me. But having no background in this area I can't put my finger on it. So my question is, what could be the pros/cons of the PCA+regression analysis, compared to CCA?Intuition says that CCA should be more reasonable here, as (I believe) it builds the canonical variates not to blindly maximize explained variance, but already with the final purpose of maximizing correlation with  in mind. Am I right?  Reference: Mei et al., 2010, Principal-component-based multivariate regression for genetic association studies of metabolic syndrome components","Creater_id":82898,"Start_date":"2016-08-25 04:06:43","Question_id":231653,"Tags":["regression","multiple-regression","pca","canonical-correlation"],"Answer_count":1,"Last_activity":"2016-08-26 03:20:44","Link":"http://stats.stackexchange.com/questions/231653/doing-cca-vs-building-a-dependent-variable-with-pca-and-then-doing-regression","Creator_reputation":116}
{"_id":{"$oid":"5837a574a05283111e4d30ac"},"View_count":17,"Display_name":"mic","Question_score":2,"Question_content":"Let's say I apply a multidimensional scaling(MDS) to a dynamic dataset of  points (eg, time series). At each step I will obtain a projection (in 2/3D) of the  points. If nothing meaningful happen it should be similar, if there are some changes, the projection should look different.However, since, to the best of my knowledge, MDS techniques would return positions with arbitrary translation and reflection, it can be hard to monitor from a step to another if the projections are the same.For example, in the picture below, points red and violet should be at the same place than the 'corresponding' blue,orange,green.My question is thus: Are there specific techniques for applying MDS in a temporal setting where one wants to link the different projections?I have cooked some naive solution by estimation a rotation o reflexion between two successive steps such that the projections look similar. But that is not a 'proper' solution as I mixed the method rotation o reflexion with some of 'real' changes in the projection.Any suggestions?","Creater_id":67168,"Start_date":"2016-08-26 03:20:34","Question_id":231869,"Tags":["time-series","multidimensional-scaling","geometry","projection"],"Answer_count":0,"Last_activity":"2016-08-26 03:20:34","Link":"http://stats.stackexchange.com/questions/231869/temporal-multi-dimensional-scaling","Creator_reputation":1035}
{"_id":{"$oid":"5837a574a05283111e4d30ae"},"View_count":25,"Display_name":"user462455","Question_score":0,"Question_content":"Pretty new to stats (and health related experiments).I bought a blood pressure monitor and want to measure my blood pressure daily ( few times at different times of day). Also, every month, I want to do a experiment. Example: First month, I want to workout daily. Second month, I won't work out. And I want to compare how blood pressure readings compare with readings in second month. What is the statistical way to do this, i.e. how do i make sure readings in one month are more statistically significant.Workout is just an example. I am planning run to multiple experiments on myself (eating spicy foods, not eating spicy foods, taking shower with cold water...)Any help on how to start would be highly appreciated.","Creater_id":119738,"Start_date":"2016-08-25 23:42:34","Question_id":231836,"Tags":["hypothesis-testing","statistical-significance","experiment-design","measurement"],"Answer_count":1,"Last_activity":"2016-08-26 02:55:48","Link":"http://stats.stackexchange.com/questions/231836/tracking-self-measurements","Creator_reputation":106}
{"_id":{"$oid":"5837a574a05283111e4d30bb"},"View_count":49,"Display_name":"Jamie","Question_score":0,"Question_content":"I was analyzing behavioral data, comparing two groups across 5 blocks. Performance is a continuous variable, while Group \u0026amp; Block are both factors in my data frame (Group 0/1, Block=1-5). I completed a repeated measures ANOVA using the following syntax:mod1\u0026lt;-aov(Performance~Group*Block + Error(SUB_ID / Block), data=TEMP)(which I believe is correct?). There is a significant main effect for group and block, as well as a group-by-block interaction. I was next interested in two things:Extracting the effect size of the interactionConducting post-hoc testsFor the first question, I have used etasq, from heplots, in previous work, but I get an error when trying to examine things here. Specifically, the result is:\u0026gt; etasq(mod1, anova=TRUE)Error in UseMethod(\"etasq\", x) :   no applicable method for 'etasq' applied to an object of class \"c('aovlist', 'listof')\"Thoughts on what I am doing wrong? Any R packages specifically for effect size calculations of repeated measures interactions? For a repeated measures ANOVA, could I just use the sum of squares output via the within-subject effects (Block, Group:Block) to calculate an eta-squared?For the second point, I have re-run things with lmeLme.mod\u0026lt;-lme(Performance~Group*Block, data=TEMP, random=~1|SUB_ID/Block)and the outputs looks similar at with aov (and that has post-hoc tests). Is that the most efficient/appropriate way to do things?Any help is greatly appreciated! Thanks much!","Creater_id":null,"Start_date":"2016-08-25 13:55:34","Question_id":231828,"Tags":["r","anova","lme"],"Answer_count":0,"Last_activity":"2016-08-26 02:55:02","Link":"http://stats.stackexchange.com/questions/231828/r-effect-size-post-hoc-tests-in-repeated-measures-anova-with-an-interaction","Creator_reputation":null}
{"_id":{"$oid":"5837a574a05283111e4d30bd"},"View_count":22,"Display_name":"Ozeuss","Question_score":0,"Question_content":"When modelling and deciding on correct transformation/accounting for non-linearity, how can I decide whether or not for it to be performed?For instance, say I'm using a restricted cubic spline to model age against mortality (as binary outcome, in log reg). Age should be definitely non-linear, but in my data, non-linearity is insignificant when tested in anova (in R, the rms package's version which is very convenient) although it does add some 2. I can further try to adjust number of the spline knots or position - but in general - including or excluding a transformation or a version of it based on significance testing feels a bit of overfitting or fishing.To tl;dr-For a term I assume to be non-linear, should I always use a non-linear transformation such as rcs(), even if non-significant (and \"wastes\" some df)","Creater_id":44082,"Start_date":"2016-08-25 13:27:22","Question_id":231770,"Tags":["r","regression","nonlinear","splines"],"Answer_count":0,"Last_activity":"2016-08-26 02:52:18","Link":"http://stats.stackexchange.com/questions/231770/including-a-non-linear-term-or-its-linear-version","Creator_reputation":43}
{"_id":{"$oid":"5837a574a05283111e4d30bf"},"View_count":49,"Display_name":"Qwertford","Question_score":0,"Question_content":"Let me state a question to use as an example:Suppose I have 3 blue marbles and 4 red marbles. Suppose we take 5 marbles from this without replacement. What would be the pdf of the random variable Y if Y is the number of blue marbles among the 5 selected marbles.We assign each marble a number 1,2,3,...,7. Now after this, we start to find the number of combinations. So Y=2 has associated sample points: 12456,12457, etcand the number of these is  =  and we would calculate the number of sample points as = My question would be, why do we calculate the combinations rather than the permutations of these events? I would think that 12456 would be a different event to 14256 and need to be counted twice in the same way that flipping a coin 3 times and getting Head Head Tails is a seperate event from Tails Head Head. So if we count the coin example as 2 sample points, because of difference in order, why don't we do the same for the marble questions?","Creater_id":86977,"Start_date":"2016-03-07 13:27:15","Question_id":200427,"Tags":["probability","combinatorics","hypergeometric"],"Answer_count":0,"Last_activity":"2016-08-26 02:29:08","Link":"http://stats.stackexchange.com/questions/200427/why-do-we-use-combinations-rather-than-permutations-when-calculating-sample-poin","Creator_reputation":38}
{"_id":{"$oid":"5837a574a05283111e4d30c1"},"View_count":43,"Display_name":"Jeannie","Question_score":0,"Question_content":"I have got Demand, Temperature, and Price data. I would like to plot Demand against other two variables separately to see the linear relationship between them, but the I that I got is very ugly. I am wondering is there any other plot that I can use? contour plot? Thanks.par(mfrow=c(1,2))plot(dataDemand,ylab=\"Demand (MW)\", xlab=\"Temperature (Celsius)\",main = \"Plot of Demand against Temperature\")abline(lm(dataTemp), col=\"red\") plot(dataDemand,ylab=\"Demand (MW)\", xlab=\"Price (GBP)\",main = \"Plot of Demand against Price\")abline(lm(dataPrice), col=\"red\") ","Creater_id":114130,"Start_date":"2016-08-26 02:13:59","Question_id":231858,"Tags":["r","data-visualization"],"Answer_count":0,"Last_activity":"2016-08-26 02:13:59","Link":"http://stats.stackexchange.com/questions/231858/better-way-for-linear-relationship-plot-between-two-variables","Creator_reputation":59}
{"_id":{"$oid":"5837a574a05283111e4d30c3"},"View_count":20,"Display_name":"Nathan Geldner","Question_score":1,"Question_content":"I'm working with data where I can expect both multiplicative error and additive error sources in a linear model. In an ideal world I'd like to be able to say that I'm looking an additive error which is normal and has variance  and multiplicative error which is normal and has variance  so my variance at a given predictor level  is .I've done some looking and I can't come up with a good way of dealing with this. Going back to the basics and optimizing over , , , and  using z-score as a proxy for likelihood would fail to converge as it would push  and  infinitely large, and I'm concerned that if most of my data has   that optimizing over the ratio of  that similar issues would push me to zero and not for the reasons I want. Is there a better way to compute likelihood that would help me? Is what I'm trying to do just not tractable?","Creater_id":129002,"Start_date":"2016-08-25 14:01:24","Question_id":231775,"Tags":["regression","modeling","error"],"Answer_count":1,"Last_activity":"2016-08-26 02:12:52","Link":"http://stats.stackexchange.com/questions/231775/is-there-a-good-way-to-quantify-both-additive-and-multiplicative-error-in-the-sa","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d30d0"},"View_count":9,"Display_name":"Laurent S.","Question_score":1,"Question_content":"I am trying to perform Mauchly's test of sphericity on a set of Data for which I have done a repeated-measures ANOVA using aov. I wanted to test the \"Participant\" as an independant variable, as in my experiment, participants who are presented with the same system will not perceive it the same way. My anova therefore looks likeres.aov \u0026lt;- aov(Attribute~System+Participant+Repetition+System:Participant+System:Repetition+Participant:Repetition,data=sample1Norm)which gives me the following summary:                        Df Sum Sq Mean Sq F value Pr(\u0026gt;F)  System                   6   6721  1120.2   2.723 0.0168 *Participant              9      0     0.0   0.000 1.0000  Repetition               2      0     0.0   0.000 1.0000  System:Participant      54  32531   602.4   1.464 0.0475 *System:Repetition       12   4877   406.4   0.988 0.4653  Participant:Repetition  18      0     0.0   0.000 1.0000  Residuals              108  44432   411.4                 ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1However, if I try to do the same with ezANOVA, I am not sure how to affect \"Participant\" as an independant variable:ezANOVA(data=sample1Norm, dv=.(Attribute), wid=.(Participant), within=.(Repetition,System), detailed=TRUE, type=1)gives me the following result:$ANOVA             Effect DFn DFd          SSn          SSd         F         p p\u0026lt;.05          ges1        Repetition   2  18 5.446667e-07 2.187057e-05 0.2241368 0.8014030       7.076965e-122            System   6  54 6.720992e+03 3.253121e+04 1.8594123 0.1048659       8.031364e-023 Repetition:System  12 108 4.876884e+03 4.443211e+04 0.9878433 0.4652740       5.959033e-02Can anybody tell me what I am doing wrong here?(and here's the data)Participant,Repetition,System,Attribute1,1,1,441,1,2,671,1,3,671,1,4,161,1,5,541,1,6,841,1,7,181,2,1,50.2861,2,2,52.2861,2,3,74.2861,2,4,48.2861,2,5,72.2861,2,6,31.2861,2,7,21.2861,3,1,43.1431,3,2,49.1431,3,3,60.1431,3,4,24.1431,3,5,71.1431,3,6,64.1431,3,7,38.1432,1,1,502,1,2,502,1,3,502,1,4,502,1,5,502,1,6,502,1,7,502,2,1,47.1432,2,2,52.1432,2,3,47.1432,2,4,47.1432,2,5,47.1432,2,6,60.1432,2,7,49.1432,3,1,502,3,2,502,3,3,502,3,4,502,3,5,502,3,6,502,3,7,503,1,1,70.2863,1,2,47.2863,1,3,28.2863,1,4,69.2863,1,5,38.2863,1,6,52.2863,1,7,44.2863,2,1,27.8573,2,2,36.8573,2,3,72.8573,2,4,55.8573,2,5,48.8573,2,6,46.8573,2,7,60.8573,3,1,84.7143,3,2,49.7143,3,3,85.7143,3,4,44.7143,3,5,18.7143,3,6,17.7143,3,7,48.7144,1,1,65.2864,1,2,51.2864,1,3,40.2864,1,4,16.2864,1,5,78.2864,1,6,50.2864,1,7,48.2864,2,1,34.2864,2,2,74.2864,2,3,75.2864,2,4,48.2864,2,5,61.2864,2,6,22.2864,2,7,34.2864,3,1,85.1434,3,2,55.1434,3,3,47.1434,3,4,39.1434,3,5,66.1434,3,6,49.1434,3,7,8.14295,1,1,39.2865,1,2,49.2865,1,3,44.2865,1,4,47.2865,1,5,47.2865,1,6,61.2865,1,7,61.2865,2,1,45.4295,2,2,55.4295,2,3,47.4295,2,4,47.4295,2,5,53.4295,2,6,54.4295,2,7,46.4295,3,1,48.8575,3,2,47.8575,3,3,52.8575,3,4,48.8575,3,5,47.8575,3,6,51.8575,3,7,51.8576,1,1,83.5716,1,2,19.5716,1,3,60.5716,1,4,53.5716,1,5,14.5716,1,6,68.5716,1,7,49.5716,2,1,130.866,2,2,24.8576,2,3,4.85716,2,4,13.8576,2,5,130.866,2,6,32.8576,2,7,11.8576,3,1,24.2866,3,2,61.2866,3,3,100.296,3,4,5.28576,3,5,100.296,3,6,41.2866,3,7,17.2867,1,1,65.4297,1,2,14.4297,1,3,84.4297,1,4,25.4297,1,5,77.4297,1,6,12.4297,1,7,70.4297,2,1,42.4297,2,2,74.4297,2,3,80.4297,2,4,68.4297,2,5,35.4297,2,6,-8.57147,2,7,57.4297,3,1,91.2867,3,2,23.2867,3,3,77.2867,3,4,-12.7147,3,5,59.2867,3,6,67.2867,3,7,44.2868,1,1,51.1438,1,2,58.1438,1,3,58.1438,1,4,18.1438,1,5,58.1438,1,6,50.1438,1,7,56.1438,2,1,44.4298,2,2,48.4298,2,3,52.4298,2,4,50.4298,2,5,52.4298,2,6,52.4298,2,7,49.4298,3,1,60.4298,3,2,53.4298,3,3,60.4298,3,4,-3.57148,3,5,60.4298,3,6,57.4298,3,7,61.4299,1,1,55.4299,1,2,30.4299,1,3,62.4299,1,4,52.4299,1,5,40.4299,1,6,52.4299,1,7,56.4299,2,1,50.2869,2,2,52.2869,2,3,48.2869,2,4,45.2869,2,5,51.2869,2,6,47.2869,2,7,55.2869,3,1,42.8579,3,2,59.8579,3,3,60.8579,3,4,32.8579,3,5,46.8579,3,6,45.8579,3,7,60.85710,1,1,4510,1,2,4410,1,3,4510,1,4,8310,1,5,5010,1,6,5310,1,7,3010,2,1,48.57110,2,2,41.57110,2,3,36.57110,2,4,83.57110,2,5,45.57110,2,6,40.57110,2,7,53.57110,3,1,55.28610,3,2,28.28610,3,3,52.28610,3,4,105.2910,3,5,30.28610,3,6,53.28610,3,7,25.286Bonus question: why am I not getting the details of the results (including Mauchly's test of sphericity)?Thanks!Laurent","Creater_id":129050,"Start_date":"2016-08-26 01:55:45","Question_id":231854,"Tags":["r","anova","repeated-measures"],"Answer_count":0,"Last_activity":"2016-08-26 01:55:45","Link":"http://stats.stackexchange.com/questions/231854/i-cant-get-the-same-anova-with-aov-and-ezanova-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d30d2"},"View_count":50,"Display_name":"TMOTTM","Question_score":1,"Question_content":"I wrote a small script that generates a distribution with mean m1 and a second distribution with mean m2, where m2 is in the vicinity diff of m1.m1   \u0026lt;- 2.50diff \u0026lt;- 0.1m2   \u0026lt;- seq(m1 - diff, m1 + diff, by=diff/15)nsamples    \u0026lt;- 250d \u0026lt;- data.frame(\"m1\"=rep(m1, length(m2)), \"m2\"=m2, \"pv\"=rep(0, length(m1)))for(i in 1:nrow(d)) {    dpv[i] + t.test(     rnorm(n=nsamples, mean=dm2[i], sd=0.1)           )$p.value}I'm interested now in how the p-value behaves depending on diff and nsamples, so I plotted it for the given parameters:Am I interpreting correctly (i.e. does my code do the right thing?) that from diff around 2.455 and 2.555 the p-value reaches significance?If its not too unrelated, is the way I'm setting up the d data.frame to accumulate the p-values done in a good way? Do I really have to prepare the m1 column with all the same value 2.5 in advance?","Creater_id":52669,"Start_date":"2016-08-16 12:01:10","Question_id":230155,"Tags":["r","p-value","programming"],"Answer_count":1,"Last_activity":"2016-08-26 01:27:10","Link":"http://stats.stackexchange.com/questions/230155/how-to-determine-required-difference-for-significance-in-r","Creator_reputation":256}
{"_id":{"$oid":"5837a574a05283111e4d30df"},"View_count":54,"Display_name":"Andrew","Question_score":0,"Question_content":"I have a dataset of about 100,000 individuals, each of which is assigned a score between 1 and 5. Let's assume half are male and half are female, so 50,000 each.I find that the average score for males is 3.9 whereas the average score for women is 3.6. How do I determine if gender has a statistically significant role? For reference, the scores do not follow a normal distribution.Would a chi-squared test be appropriate by comparing this to the expected case of the results being identical for males and females?","Creater_id":129024,"Start_date":"2016-08-25 19:57:21","Question_id":231820,"Tags":["probability","hypothesis-testing","statistical-significance"],"Answer_count":3,"Last_activity":"2016-08-26 01:27:10","Link":"http://stats.stackexchange.com/questions/231820/statistical-significance-for-a-dataset-of-males-vs-females","Creator_reputation":1}
{"_id":{"$oid":"5837a574a05283111e4d30ee"},"View_count":17,"Display_name":"Alex","Question_score":1,"Question_content":"I need to validate a couple of subscales in a larger questionnaire. Items in the scales range from 5 to 13 composed of dichotomous and ordinal values.I currently use Cronbach's alpha to measure reliability of the scales. I read Sijtsma (2009) on the limitations of the Cronbach's alpha and want to include glb values as well. Can anyone tell me how to calculate glb values in Stata?Thank you in advance! :)","Creater_id":2652,"Start_date":"2016-08-26 01:26:17","Question_id":231849,"Tags":["stata","reliability","cronbachs-alpha"],"Answer_count":0,"Last_activity":"2016-08-26 01:26:17","Link":"http://stats.stackexchange.com/questions/231849/calculating-greater-lower-bound-glb-reliability-in-stata","Creator_reputation":209}
{"_id":{"$oid":"5837a574a05283111e4d30f0"},"View_count":40,"Display_name":"user39531","Question_score":0,"Question_content":"I have the following scales:1 (Strongly Disagree);2 (Disagree);3 (Not Applicable);4 (Agree);5 (Strongly Agree);Questionnaire responses:Question Item1: 2;Question Item2: 3;Question Item3: 1;Question Item4: 1;Question Item5: 4;If I need to average the above responses into the Excel Sheet, while treating Question Item2: zero (since it is considered missing data), do I need to divide by 5 or 4 items to obtain the average?\\text{Average} = \\frac{2 + \\mathbf 0 + 1 + 1+ 4}{5}\\quad\\text{or}\\quad\\text{Average} = \\frac{2 + \\mathbf{\\_} + 1 + 1+ 4}{4}Kindly advise.","Creater_id":39531,"Start_date":"2016-08-25 19:28:15","Question_id":231817,"Tags":["statistical-significance","anova","data-transformation","descriptive-statistics"],"Answer_count":3,"Last_activity":"2016-08-26 01:25:33","Link":"http://stats.stackexchange.com/questions/231817/how-is-missing-data-calculated-on-likert-scale","Creator_reputation":60}
{"_id":{"$oid":"5837a574a05283111e4d30ff"},"View_count":27,"Display_name":"sbsbsb945","Question_score":2,"Question_content":"In Blei's original paper about LDA(Latent Dirichlet Allocation), he used VBEM method to train the model. In E step, update the posterior distribution, in M step, update the hyperparameter. But in other tutorials about VBEM, I didn't see them use such method. VBEM in other tutorials just compute  in E step and compute  in M step. I'm confused what is a standrad variational bayes EM. Is standard variational bayes EM need to update hyperparameters? Or we can say Blei's method is a combination of variational bayes EM and empirical bayes?","Creater_id":128792,"Start_date":"2016-08-25 21:57:53","Question_id":231826,"Tags":["machine-learning","variational-bayes"],"Answer_count":0,"Last_activity":"2016-08-26 01:21:16","Link":"http://stats.stackexchange.com/questions/231826/is-standard-variational-bayes-em-need-to-update-hyperparameters-as-empirical-bay","Creator_reputation":11}
{"_id":{"$oid":"5837a574a05283111e4d3101"},"View_count":37,"Display_name":"N F N","Question_score":0,"Question_content":"I have somewhat of an odd question. I'm currently working on a dataset and after training my model, I'm trying to get the correct AUC and I'm using the RocR package to get that value. Im using these commands     pred \u0026lt;- predict(model, testData,type='prob')    pr=prediction(pred[,2],testData$var)    prf \u0026lt;- performance(pr, measure = \"tpr\", x.measure = \"fpr\")    auc \u0026lt;- performance(pr, measure = \"auc\")    auc \u0026lt;- auc@y.values[[1]]  I've noticed that whether I add the type='prob' changes the value of my auc, meaning that if i predict just the binary output instead of the probabilities, the AUC changes. So my question is which value should I use? I'm a bit lost.  ","Creater_id":79536,"Start_date":"2016-08-25 19:31:51","Question_id":231818,"Tags":["r","auc"],"Answer_count":1,"Last_activity":"2016-08-26 01:07:50","Link":"http://stats.stackexchange.com/questions/231818/choosing-the-correct-auc-value-with-rocr-package","Creator_reputation":48}
{"_id":{"$oid":"5837a574a05283111e4d310e"},"View_count":29,"Display_name":"Luis","Question_score":1,"Question_content":"I am trying to fit the Bivariate Poisson distribution to a set of sports results to serve as a comparison model to a new model I am developing/developed with my masters thesis.My model is as follow:model{  C \u0026lt;- 0    for( i in 1:nGames ){    # Min score    minscore[i] \u0026lt;- min( Score1[i],Score2[i]) + 1    # Generate minscore latent variable    u[i] ~ dunif(0,minscore[i])    z3[i] \u0026lt;- trunc( u[i] )    # Calculate z1 and z1 latent variables    z1[i] \u0026lt;- Score1[i] - z3[i]    z2[i] \u0026lt;- Score2[i] - z3[i]    Zeros[i] ~ dpois( zeros.mean[i] )    zeros.mean[i] \u0026lt;- -l[i] + C    l[i] \u0026lt;- -lambda[i,1] + z1[i]*log( lambda[i,1] ) - loggam( z1[i]+1 )            -lambda[i,2] + z2[i]*log( lambda[i,2] ) - loggam( z2[i]+1 )            -lambda[i,3] + z3[i]*log( lambda[i,3] ) - loggam( z3[i]+1 )    log( lambda[i,1] ) \u0026lt;- mu + offense[Team1[i]] + defense[Team2[i]]    log( lambda[i,2] ) \u0026lt;- mu + offense[Team2[i]] + defense[Team1[i]]    log( lambda[i,3] ) \u0026lt;- beta  }  for( j in 2:nTeams ){    offense[j] ~ dnorm(0,.0001)    defense[j] ~ dnorm(0,.0001)  }  offense[1] \u0026lt;- -sum( offense[2:nTeams] )  defense[1] \u0026lt;- -sum( defense[2:nTeams] )  mu~dnorm(0,0.001)  beta~dnorm(0,.001)}Now, I am trying to fix a diagonal inflated Bivariate Poisson. My question is, what shoud I change from the above code to implement a diagonal inflated Bivariate Poisson?My data set:Team1/Team2 - the home and away teamsScore1/Score2 - the observed scoresnGames - the number of games in the datasetnTeams - the number of teams in the datasetAny help would be greatly appreciated.","Creater_id":129045,"Start_date":"2016-08-26 00:45:39","Question_id":231845,"Tags":["r","jags"],"Answer_count":0,"Last_activity":"2016-08-26 00:45:39","Link":"http://stats.stackexchange.com/questions/231845/diagonal-inflated-bivariate-poisson-model","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d3110"},"View_count":15,"Display_name":"Rahul Singh","Question_score":0,"Question_content":"I'm using Random Forest for classification which gives the following confusion matrix.0        1    class.error0   839   24     0.0271   60     86    0.410You can notice that the classification error is a lot higher for incorrect classification of 1's (false negatives).I've noticed - even when applying RF to other problems - that the classification error tends be higher on the basis of the proportions of the dependent variable. For eg:- in the example above, there are 146 cases with DV = 1  as opposed to 863 cases with DV = 0 so the error for classifying a case as 1 is much higherMy question is this: What is the reason that the RF algorithm behaves this way and how can I improve the results to remove what seems to me like a bias.","Creater_id":92193,"Start_date":"2016-08-19 05:35:58","Question_id":230687,"Tags":["classification","random-forest"],"Answer_count":1,"Last_activity":"2016-08-26 00:41:26","Link":"http://stats.stackexchange.com/questions/230687/random-forest-classification-appears-dependent-on-dependent-variable-proportions","Creator_reputation":13}
{"_id":{"$oid":"5837a574a05283111e4d311d"},"View_count":86,"Display_name":"Dominique","Question_score":0,"Question_content":"I'm trying to see if there is a correlation between the height of grass and the height under branches available for grass to grow. I have 227 paired observations:GrassHeight HeightUnderDebris0            00            00            08            160            00            00            02            26            60            00            01            10            00            00            08            150            07            715           15My data is not normally distributed and it fails at the assumption of bivariate normality:result\u0026lt;-hzTest(data,cov = TRUE,qqplot = FALSE)result\u0026lt;-mardiaTest(data,cov = TRUE,qqplot = FALSE)result\u0026lt;-roystonTest(data,qqplot = FALSE)Therefore, I need to use a Spearman's rho or Kendall's tau. Firstly, Spearman's rho results in a warning message:cor.test(GrassHeight, HeightUnderDebris, method=\"spearman\")Spearman's rank correlation rhodata:  GrassHeight and HeightUnderDebrisS = 123090, p-value \u0026lt; 2.2e-16alternative hypothesis: true rho is not equal to 0sample estimates:rho 0.9368622 Warning message:In cor.test.default(GrassHeight, HeightUnderDebris, method = \"spearman\") :Cannot compute exact p-value with tiesSo I then decided to use Kendall's tau as it can deal with ties:cor.test(GrassHeight, HeightUnderDebris, method=\"kendall\")Kendall's rank correlation taudata:  GrassHeight and HeightUnderDebrisz = 17.202, p-value \u0026lt; 2.2e-16alternative hypothesis: true tau is not equal to 0sample estimates:tau 0.858494Firstly, should I be concerned that my data has many zeros? They are important as they reflect that if there is no space under branches, then there is no space for grass growth hence why the grass height is 0.Secondly, how would you interpret Kendall's results? Is it right that the two variables are uncorrelated at 0.05 significance level if their correlation coefficient is zero? In this case, tau is 0.858. That is not zero and will be rounded up to 1. Can I say that the two variables are correlated based on this?Should I rather look at rpudplus and the function rpucor, which now uses Kendall’s tau-b to compute the correlation coefficient?What post-hoc test can I do to find out the nature of the correlation, i.e: as the height between the ground and branch increases, grass height increases?","Creater_id":125334,"Start_date":"2016-08-25 22:12:49","Question_id":231829,"Tags":["r","correlation"],"Answer_count":2,"Last_activity":"2016-08-26 00:03:46","Link":"http://stats.stackexchange.com/questions/231829/which-correlation-should-be-used-for-non-normal-data-spearmans-rho-versus-kend","Creator_reputation":30}
{"_id":{"$oid":"5837a574a05283111e4d312b"},"View_count":74,"Display_name":"Marine","Question_score":0,"Question_content":"I used the function mix (package mixdist) to fit Gamma mixture distributions. The function gives mu and sigma parameters (output below). How can I find the scale and shape parameters ? Parameters:       pi    mu sigma 1 0.2089 185.7 285.4 2 0.7911 530.1 423.5 ","Creater_id":96381,"Start_date":"2016-08-25 18:58:27","Question_id":231814,"Tags":["gamma-distribution","mixture"],"Answer_count":1,"Last_activity":"2016-08-25 23:55:13","Link":"http://stats.stackexchange.com/questions/231814/scale-and-shape-parameters-of-gamma-mixture-distributions","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d3138"},"View_count":62,"Display_name":"Chris","Question_score":3,"Question_content":"While logistic regression has a set of easier to meet assumptions than linear regression, logistic regression can still suffer from outliers in the independent input variables. This can manifest itself when one variable can drown out another in certain cases.It has been suggested that those could be handled by clipping (i.e. Winsoring) or trimming the outlier values.  Transformations such as log or inverse can limit the risk from outliers. However what are the thoughts of using a sigmoid transform, along the lines of a z-score to p-value transformation (cumulative distribution function).  In R this can be done via pnorm((x-mean(x))/sd(x)) or approximated by 1.0/(1.0+exp(-1.69897*(x-mean(x))/sd(x))).In the example code below, the skewness of the predictor variables were dramatically reduced and the KS scores in the fictitious example increased from 60.8 to 61.6. The McFadden pseudo R squared improved from 0.17 to 0.24.  While not a huge improvement in the synthetic example, it shows some promise.Are there any papers that discuss this technique?ReferencesChris Busch. 2015. “Need a quick \u0026amp; dirty pnorm, normdist formula in a pinch for #SQL” https://twitter.com/cbuschnotes/status/667505348962418688DS. 2009. Guide to Credit Scoring in R – CRAN. https://cran.r-project.org/doc/contrib/Sharma-CreditScoring.pdfAppendix ks.scoreAP=function(act,prd,yvar='',show=T){    require(ROCR)  pred\u0026lt;-prediction(prd,act)  perf \u0026lt;- performance(pred,\"tpr\",\"fpr\")  #this code builds on ROCR library by taking the max delt  #between cumulative bad and good rates being plotted by  #ROCR  ks.score=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])  if(show){    plot(perf,main=paste0(yvar,' KS=',round(ks.score*100,1),'%'))    lines(x = c(0,1),y=c(0,1))  }  ks.score;}require(e1071)require(pscl)set.seed(11)d=data.frame(x1=rlnorm(1000),x2=rlnorm(1000))summary(d)dx1\u0026lt;1 \u0026amp; dy[1:200]=round(runif(200)) #noiseskewness(dx2)m=glm(y ~. ,family=binomial(link='logit'),data=d)ks.scoreAP(dx1=pnorm((dx1))/sd(dx2=pnorm((dx2))/sd(dx1)skewness(dy,predict(m,type='response'))pR2(m)","Creater_id":70282,"Start_date":"2016-08-23 18:09:00","Question_id":231390,"Tags":["regression","logistic","outliers"],"Answer_count":1,"Last_activity":"2016-08-25 21:46:47","Link":"http://stats.stackexchange.com/questions/231390/softmax-normalization-using-sigmoid-transformation-on-the-independent-variables","Creator_reputation":410}
{"_id":{"$oid":"5837a574a05283111e4d3145"},"View_count":103,"Display_name":"Atte Juvonen","Question_score":0,"Question_content":"Let's say we're building a spam classifier. When we feed it an email, it accurately classifies it as spam/not-spam 98% of the time. Then we discover that 99% of the email we receive is actually spam. The 98% accuracy doesn't look good anymore.  On skewed datasets (e.g., when there are more positive examples than negative examples, accuracy is not a good measure of performance and you should instead use F score, which is based on precision and recall. -Week 6 of Andrew Ng's Machine Learning on CourseraI can see the value of looking at precision and recall separately, but I don't see the value of combining them with a non intuitive formula to replace accuracy as our primary overall measurement of how our learning algorithm is performing. If 99% is the accuracy that a simple \"always guess yes\" -algorithm can achieve, can't we just set that as a comparison and say that our algorithm is doing good when it's achieving considerably better accuracy (like 99,76% for instance).Accuracy has the obvious benefit that it is simple and intuitive. What advantages does the F-score hold over accuracy in our \"spam classifier\" -example?","Creater_id":101702,"Start_date":"2016-08-25 15:03:25","Question_id":231786,"Tags":["machine-learning","model","skewness","supervised-learning"],"Answer_count":1,"Last_activity":"2016-08-25 20:42:44","Link":"http://stats.stackexchange.com/questions/231786/skewed-dataset-performance-measurement-in-machine-learning","Creator_reputation":68}
{"_id":{"$oid":"5837a574a05283111e4d3152"},"View_count":49,"Display_name":"s.san","Question_score":1,"Question_content":"Given mean and standard deviation, is there any way to find minimum and maximum in a collection of 50 numbers?","Creater_id":129009,"Start_date":"2016-08-25 15:29:34","Question_id":231793,"Tags":["standard-deviation","maximum","minimum"],"Answer_count":2,"Last_activity":"2016-08-25 19:21:47","Link":"http://stats.stackexchange.com/questions/231793/finding-min-and-max-using-sd","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d3160"},"View_count":26,"Display_name":"user6507246","Question_score":0,"Question_content":"I saw the equation for the Before step Gaussian MEL \\ln p({\\bf X}|{\\pmb \\mu}, \\Sigma) = -\\frac{ND}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln|\\Sigma|-\\frac{1}{2}\\sum_{n=1}^{N}({\\bf x}_n-{\\pmb \\mu})^T\\Sigma^{-1}({\\bf x}_n-{\\pmb \\mu}) \\qquad{(2.118)}what does N,D represent in the equation?is D represent from here?\\left(\\begin{array}{cc}A \u0026amp; B \\\\ C \u0026amp; D \\end{array}\\right)^{-1} = \\left(\\begin{array}{cc} M \u0026amp; -MBD^{-1} \\\\ -D^{-1}CM \u0026amp; D^{-1}CMBD^{-1} \\end{array}\\right) \\qquad{(2.76)}and the second question is how they get the 2.76(right side)?which part of linear algebra should I look to understand this equation?(or derive step?) ","Creater_id":128678,"Start_date":"2016-08-25 18:17:26","Question_id":231811,"Tags":["machine-learning","normal-distribution","linear-algebra"],"Answer_count":0,"Last_activity":"2016-08-25 18:17:26","Link":"http://stats.stackexchange.com/questions/231811/mle-for-gaussian-distribution-log-step","Creator_reputation":58}
{"_id":{"$oid":"5837a574a05283111e4d3162"},"View_count":51,"Display_name":"Jo\u0026#227;o_testeSW","Question_score":0,"Question_content":"Imagine this example. I've a field with following values:10000134559012347901111235679How can I identify that 90 are a outlier? Using average or standard deviation?Thanks!","Creater_id":129015,"Start_date":"2016-08-25 16:42:02","Question_id":231801,"Tags":["outliers","descriptive-statistics"],"Answer_count":1,"Last_activity":"2016-08-25 18:09:04","Link":"http://stats.stackexchange.com/questions/231801/identify-outliers-using-statistics-methods","Creator_reputation":1}
{"_id":{"$oid":"5837a574a05283111e4d316f"},"View_count":65,"Display_name":"ShanZhengYang","Question_score":0,"Question_content":"When using a gaussian mixture model, you usually need to specify the number the number of clusters in the data. However, are there methods whereby you could infer the number of clusters to use, given the data? ","Creater_id":80118,"Start_date":"2016-08-04 13:55:22","Question_id":228330,"Tags":["machine-learning","inference","computational-statistics","gaussian-mixture","unsupervised-learning"],"Answer_count":2,"Last_activity":"2016-08-25 18:06:13","Link":"http://stats.stackexchange.com/questions/228330/when-using-a-gaussian-mixture-model-gmm-is-it-possible-at-all-to-infer-the-numb","Creator_reputation":207}
{"_id":{"$oid":"5837a574a05283111e4d317d"},"View_count":43,"Display_name":"Wintermute","Question_score":3,"Question_content":"Suppose that we want to compute \\int_{\\mathbb{R}} f(x) dx using monte carlo integration so that we can normalize  and make it a pdf. The examples you typically see involve integrals over a finite domain such as . In such cases we can sample from the uniform distribution on . In the case we integrate over  we need to sample from a pdf whose support is , say the standard normal pdf which we represent as . We can then compute \\frac{1}{n} \\sum_{i=1}^{n} \\frac{f(x_i)}{p(x_i)} as our unbiased estimate of \\int_{\\mathbb{R}}f(x)dx=\\int_{\\mathbb{R}}\\frac{f(x)}{p(x)}p(x)dx=\\mathbb{E}\\left[\\frac{f(x)}{p(x)}\\right] so long as (1)\\;\\;\\;\\;\\int_{\\mathbb{R}}\\frac{f(x)^2}{p(x)}dx \u0026lt; \\infty We can also rely on estimates of the variance of the estimator as well as employ the central limit theorem to construct confidence intervals so long as (2)\\;\\;\\;\\;\\int_{\\mathbb{R}}\\frac{f(x)^4}{p(x)^3}dx \u0026lt; \\infty My problem is that I struggle to construct and example where (1) and (2) hold. Can someone give examples of monte carlo integration techniques used to normalize a function over  which satisfy the necessary assumptions so that the estimate is reliable?","Creater_id":36049,"Start_date":"2016-08-25 09:19:23","Question_id":231715,"Tags":["variance","monte-carlo","central-limit-theorem","numerical-integration"],"Answer_count":1,"Last_activity":"2016-08-25 18:05:48","Link":"http://stats.stackexchange.com/questions/231715/monte-carlo-integration-on-the-real-line","Creator_reputation":311}
{"_id":{"$oid":"5837a574a05283111e4d318a"},"View_count":25,"Display_name":"7yl4r","Question_score":0,"Question_content":"I am confident that the below methodology creates a bias in that it indirectly uses the test set as part of the training procedure, however, I am having trouble explaining this to my colleagues. I would greatly appreciate any relevant terminology or reading material to help me articulate the problem.Here is an example scenario:I have a dataset split into three chunks A, B, and C.I use two chunks to train my model, and one to test.I try all permutations of test/train splits (ie using each AB, AC, BCas training), and get different results each time.I report the performance from the best of the splits as the performance of my model on this data.In reality, this value is an over-optimistic evaluation of my model's performance because I have essentially looked at all the data and chosen the best way to split it. This is cheating because I have looked at my test data before evaluating. The actual performance of my model would be the average performance of all split permutations.What is the bias I have introduced called?What is this (common) mistake called?Where/how can I find a more complete description of this problem?","Creater_id":38394,"Start_date":"2016-08-25 16:46:13","Question_id":231802,"Tags":["machine-learning"],"Answer_count":1,"Last_activity":"2016-08-25 17:34:42","Link":"http://stats.stackexchange.com/questions/231802/what-is-the-bias-from-choosing-best-train-test-set-from-many-options-called","Creator_reputation":113}
{"_id":{"$oid":"5837a574a05283111e4d3198"},"View_count":411,"Display_name":"Nicolas Bourbaki","Question_score":13,"Question_content":"Given two random variables  and  we can compute their \"correlation coefficient\" , and form the line of best fit between these two random variables. My question is why? 1) There are random variables,  and  which are dependent in the worst possible way, i.e.  and despite this . If one only thinks along linear regression, then one would be totally blinded to this. 2) Why linear specifically? There are other kinds of relationships that can exist between random variables. Why single that one out of all others? ","Creater_id":68480,"Start_date":"2016-08-21 19:20:08","Question_id":231024,"Tags":["regression"],"Answer_count":5,"Last_activity":"2016-08-25 17:08:11","Link":"http://stats.stackexchange.com/questions/231024/why-study-linear-regression","Creator_reputation":530}
{"_id":{"$oid":"5837a574a05283111e4d31a9"},"View_count":23,"Display_name":"Vincent Laufer","Question_score":0,"Question_content":"This question is very similar, but I believe different, to this prior post. The question in this link asks, if C beats B 80% of the time, and B beats A 80% of the time, how often would C beat A?The answer is that there is insufficient data to determine this, because winning is not necessarily transitive and because we do not have all the required probabilities we need.Very well, let's modify the question a bit. Instead of 2 direct matchups, imagine a scenario where people join teams constantly, and rarely or never join the same team twice due to a huge number of available players, and that teams are paired randomly. If you knew the players in advance, the probablity that team A or B wins might vary, but if you consider a random team without foreknowledge of the players or other conditions, then the probability any (random) team wins against any other (randomly selected) should approach 0.5 over the long haul.Now imagine that we follow the teams on which one star player, we will call him Michael Jordan plays on, and the teams that a rookie plays on.After some time it is discovered that, on average, teams having Jordan on them win 55% of the time. On the other hand, after a large number of games, it is found that teams having the rookie win only 44% of games (compared to 50% of the time for a random team without the rookie).So, here are my questions:1) Assuming there is no special interaction effect between Jordan and the rookie (illustrative example, the rookie looks like Jordan's son, and Jordan doesn't want to try his hardest against him), what is the probability that Jordan's team wins head-to-head vs the rookie's team over a huge number of games (remember, the other players on both teams change every time they play)? Does this question differ from the previous post in that an answer can be given? (see below clarification)2) Suppose the outcomes of a huge number of games between Jordan and the rookie are known. The difference between the empirically observed win rate for Jordan's team over the rookie's team and the expected win rate calculated in 1) could be a useful summary statistic. How might one evaluate this difference for significance (i.e. an interaction does occur; the rookie has a relatively higher win rate than would be suggested by chance). Clarification: First, I think positing an arbitrary number of players with arbitrary skill levels that are re-assorted randomly (and could even end up on either team day-to-day) eliminates concerns about nontransitivity, but perhaps I am wrong. Second, I think question may provide enough information because it does not rely on three discrete pairings, and background probabilities independent of everything except the two players are known, but please let me know if this is not accurate.","Creater_id":86388,"Start_date":"2016-08-24 17:14:15","Question_id":231575,"Tags":["probability"],"Answer_count":0,"Last_activity":"2016-08-25 15:40:51","Link":"http://stats.stackexchange.com/questions/231575/probability-of-a-team-winning-and-another-losing-head-to-head-if-their-win-rates","Creator_reputation":125}
{"_id":{"$oid":"5837a574a05283111e4d31ab"},"View_count":2093,"Display_name":"Reen","Question_score":22,"Question_content":"A is positively related to B.C is the outcome of A and B, but the effect of A on C is negative and the effect of B on C is positive.Can this happen?","Creater_id":127169,"Start_date":"2016-08-09 12:07:40","Question_id":229052,"Tags":["regression","correlation"],"Answer_count":5,"Last_activity":"2016-08-25 15:35:04","Link":"http://stats.stackexchange.com/questions/229052/when-a-and-b-are-positively-related-variables-can-they-have-opposite-effect-on","Creator_reputation":111}
{"_id":{"$oid":"5837a574a05283111e4d31bc"},"View_count":44,"Display_name":"half-pass","Question_score":2,"Question_content":"There are 5 widely-invoked modes of convergence for a random variable, listed below from strongest to weakest:   1. Complete convergence        2. Almost sure convergence        3. Convergence in  mean        4. Convergence in probability        5. Convergence in distribution     at all continuity pointsI'm seeking (to whatever extent possible) intuitive motivations for, and connections between, these modes of convergence. I know how to prove their interrelationships formally, but I find these concepts rather abstract and would like to develop a more well-rounded understanding. I often find visual and/or simulation-based intuitions especially helpful, but use your creative pedagogy.","Creater_id":11511,"Start_date":"2016-08-25 13:48:16","Question_id":231773,"Tags":["convergence"],"Answer_count":1,"Last_activity":"2016-08-25 15:18:45","Link":"http://stats.stackexchange.com/questions/231773/intuition-behind-various-modes-of-convergence-of-random-variables","Creator_reputation":1065}
{"_id":{"$oid":"5837a574a05283111e4d31c9"},"View_count":18,"Display_name":"ShannonC","Question_score":0,"Question_content":"If I want to see if the proportion of people with a certain condition is statistically significantly different between different cities, fractional regression would be a good way to go, yes?And if so, does it matter if the cities are greatly different sizes? (We may be talking 50% of roughly 100 people in one city with the condition, versus 30% of roughly 1,000 in another city.)Any better ways to check out significant? Paired t-test not an option, because there are more than two cities. ANOVA?","Creater_id":54736,"Start_date":"2016-08-25 15:07:08","Question_id":231789,"Tags":["regression","anova","stata"],"Answer_count":0,"Last_activity":"2016-08-25 15:07:08","Link":"http://stats.stackexchange.com/questions/231789/fractional-regression-with-different-population-sizes","Creator_reputation":337}
{"_id":{"$oid":"5837a574a05283111e4d31cb"},"View_count":47,"Display_name":"Rievturge","Question_score":0,"Question_content":"I'm working at Score test realization and I need to calculate Fisher information in basic logistic model: And I have stuck at the calculation of this expectation:  (where )Maybe someone has faced to this problem?P.S. Calculations:Hypothesis: ,  - likelihood function,Statistics:  Calculating U statistic:Taking the derivative by :At least: V statistic is expectation of derivative U by For every  : So Now I should take expectation of this, and I don't know, how to do it.","Creater_id":116498,"Start_date":"2016-08-23 10:58:09","Question_id":231329,"Tags":["regression","logistic","mathematical-statistics","fisher-information"],"Answer_count":1,"Last_activity":"2016-08-25 15:06:43","Link":"http://stats.stackexchange.com/questions/231329/fisher-information-in-logit-model","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d31d8"},"View_count":31,"Display_name":"elleciel","Question_score":0,"Question_content":"Suppose a robber is predicting how much gold is stored in the a bank's gold vault over time and that they release the figures weekly so I have actual observations of my label. He decides to rob the bank every time the gold in the vault is predicted to exceed 100 lbs.Most software packages just solve for MSE on the unconditional set. However, it's preferable to have a model that has a large number of predictions that exceed 100 lbs and low error conditional on these cases, which translates to greater wealth for the robber. Hence, I'm thinking to have a custom loss function to minimize the sum rather than the root mean square and also condition on the 100 lb threshold.I'm hesitant to train my model directly against this custom loss function as I know the MSE has nice properties: Suppose I want to choose from a variety of linear models. Should I directly estimate the parameters for the generalized linear model that minimize the sum of squared error conditional on \u003e100 lb prediction via SGD, or should I take the OLS estimates against the unconditional set of labels then select between models in cross validation based on my custom loss function?For OLS, I know the properties to look out for (homoskedasticity, independent errors, additivity/linearity etc.), but I'm worried that a different set of assumptions hold for my custom loss function that I unknowingly induce on the model.What should I do?","Creater_id":49620,"Start_date":"2016-08-25 14:50:44","Question_id":231784,"Tags":["regression","machine-learning","generalized-linear-model","least-squares","loss-functions"],"Answer_count":0,"Last_activity":"2016-08-25 15:06:12","Link":"http://stats.stackexchange.com/questions/231784/using-custom-loss-functions-and-theoretical-consequences","Creator_reputation":113}
{"_id":{"$oid":"5837a574a05283111e4d31da"},"View_count":93,"Display_name":"Ralph M","Question_score":0,"Question_content":"I'm trying to understand the difference between XTREG and PLM.  First, I have looked at this answered question:Difference between fixed effects models in R (plm) and Stata (xtreg)But when I try the code provided by the answerer, I get different answers from R and Stata.  The STATA results match those of the answerer, but the R results do not.I have an inkling for why.  When I execute that code in R, R doesn't create lags within the grouping variable, it creates lags overall.  For example, if there are 50 states and 17 years, when including a lag in the regression, I will lose 50 observations: the first year for each state.  In STATA, the sample size reduces accordingly.  In R, the sample size reduces by 1. This is because its not identifying the \"state\" grouping variable.  So, does anyone have an idea of what is going on here?","Creater_id":128998,"Start_date":"2016-08-25 13:49:23","Question_id":231774,"Tags":["r","panel-data","stata","plm"],"Answer_count":1,"Last_activity":"2016-08-25 14:42:15","Link":"http://stats.stackexchange.com/questions/231774/how-exactly-does-the-plm-package-in-r-create-lags","Creator_reputation":1}
{"_id":{"$oid":"5837a574a05283111e4d31e7"},"View_count":85,"Display_name":"OpiesDad","Question_score":3,"Question_content":"I am having a hard time understanding how to do model diagnostics, and in particular how to understand the residuals of a fit ARI(1,1) model.  I analyzed some data (n=96) and found that it was non-stationary as the autocorrelations were significant up until lag 22.  I did first differencing to correct for this issue, and obtained the following new autocorrelation chart:The yellow lines are an estimate of two standard errors, calculated as .  In looking at this, it appears that this could be an AR(1) or ARMA(1) process, so I first modeled this as an AR(1) process, obtaining the autoregressive parameter of -0.479.  I then tried to calculate the standardized residual to analyze whether this fit the data.  To do this, I calculated the residual as actual minus predicted value at each time.  The predicted value was calculated by taking  where  is the prediction at time t and  is the actual value at time t-1.I then calculated the standard error using the formula  and then took the standardized residual by taking the residual divided by s.This created the following chart:So far, I think this analysis is correct, but if there is anything amiss, any direction would be helpful.Then, I wanted to determine if these residuals could be from a white noise process, so I calculated the autocorrelation of the residuals.  The autocorrelation of the residuals shows many values outside of the confidence interval, which I again set to a rough estimate of The autocorrelation is calculated as Then, I calculated the Box-Pierce Q statistic using  which was equal to 7306. This value seems excessively large (considering the Chi-Square table for degree of freedom 50 (I think I should be using 49, but it's close enough) at a confidence interval of 1% is 29.7.  Am I doing something wrong or is this model just vastly incorrect?  Should I be doing the final autocorrelation of the residuals on the standardized residuals or the non-standardized residuals?  Which should I use for the Box-Pierce statistic? Sorry if this post is too long.  I am a regular StackExchange User and not too knowledgeable about Statistics, so am not sure what the most relevant details would be.  Thanks for any help!","Creater_id":128698,"Start_date":"2016-08-23 12:20:07","Question_id":231345,"Tags":["time-series","arima","autocorrelation","residuals"],"Answer_count":1,"Last_activity":"2016-08-25 14:21:03","Link":"http://stats.stackexchange.com/questions/231345/understanding-autocorrelation-of-the-residuals-ari1-1-model","Creator_reputation":118}
{"_id":{"$oid":"5837a574a05283111e4d31f4"},"View_count":81,"Display_name":"MattCrow","Question_score":3,"Question_content":"Has anyone developed a \"cheat sheet\" of sorts that describes the appropriate use of distribution types for different types of data? For example, beta for coin-type data (e.g. Therapy versus control), poisson for counts...","Creater_id":125265,"Start_date":"2016-08-25 09:31:59","Question_id":231722,"Tags":["distributions","bayesian","references"],"Answer_count":2,"Last_activity":"2016-08-25 14:13:22","Link":"http://stats.stackexchange.com/questions/231722/distribution-cheat-sheet-for-bayes-data-analysis","Creator_reputation":26}
{"_id":{"$oid":"5837a574a05283111e4d3202"},"View_count":576,"Display_name":"Deep North","Question_score":3,"Question_content":"The question is from a problem I am trying to solve in Robert Hogg's introduction to Mathematical Statistics 6th version problem 7.2.9 in page 380.The problem is:  We consider a random sample  from a distribution  with pdf )exp(),  .  Possibly, in a life testing situation, however, we only observe the  first r order statistics .    (a) Record the joint pdf of these order statistics and denote it by      (b) Under these conditions, find the mle, , by  maximizing .    (c)Find the mgf and pdf of  .    (d) With a slight extension of the definition of sufficiency, is   a sufficient statistic?I can solve (a) and (b) but I am completely stuck by (c) therefore cannot forward to (d)Solve (a):We know joint pdf for  is  we just integrate out the (r+1) to n terms we will get joint pfd for .(b)This part is not difficult. It just a normal way to calculate mle.Take derivative of the log likelihood function we get:Set the derivative to zeroWe get: (c)To solve (c) I think we need at least to know the distribution of .I search the internet, there is a paper talk about this distribution, https://www.ocf.berkeley.edu/~wwu/articles/orderStatSum.pdfBut I think the method might not be correct since for order statistic  are different, we cannot use binomial distribution there.There is another paper here http://www.jstor.org/stable/4615746?seq=1#page_scan_tab_contentsBut I am totally lost at formula (2.2) if someone would like to explain the paper with more detailed calculations, it will be highly appreciated.(d) only after solve (c)","Creater_id":61705,"Start_date":"2015-07-13 00:05:35","Question_id":161145,"Tags":["self-study","mathematical-statistics","maximum-likelihood","order-statistics","sufficient-statistics"],"Answer_count":1,"Last_activity":"2016-08-25 14:12:46","Link":"http://stats.stackexchange.com/questions/161145/distribution-of-sum-of-order-statistics","Creator_reputation":1538}
{"_id":{"$oid":"5837a574a05283111e4d320f"},"View_count":555,"Display_name":"philosodad","Question_score":4,"Question_content":"We have n actors. Each actor chooses from n*2 actions. How can I calculate the probability that at least one actor will choose a unique outcome?For example, say we have 5 pickup artists in a town with 10 bars. Each PUA chooses a bar at random. What are the odds that at least one PUA will have a bar to himself/herself? This is relevant to some work I'm doing on scheduling in a distributed system. I know that the relevant equation should probably start with , but then what?","Creater_id":2995,"Start_date":"2011-01-31 15:49:27","Question_id":6762,"Tags":["probability","combinatorics"],"Answer_count":2,"Last_activity":"2016-08-25 13:44:44","Link":"http://stats.stackexchange.com/questions/6762/probability-of-at-least-one-unique-outcome","Creator_reputation":123}
{"_id":{"$oid":"5837a574a05283111e4d321d"},"View_count":23,"Display_name":"RDizzl3","Question_score":1,"Question_content":"I have a question, I apologize if it seems trivial but I just want to make sure this is correct or be corrected if not. I have a function f(r,x) = \\dfrac{r}{x}Where  - is total revenue of purchases and  is the number of customers who made a purchase on a website. Here is my question with an example:Example: Say, I have 300f(300, 8)f(300, 10)$If this is incorrect what is the correct way of comparing? This may seem trivial to some but I believe I'm being thrown off by the change in the denominator.Thank you for any help or comments!","Creater_id":53098,"Start_date":"2016-08-25 12:36:28","Question_id":231756,"Tags":["statistical-significance","mathematical-statistics","percentage"],"Answer_count":1,"Last_activity":"2016-08-25 13:44:21","Link":"http://stats.stackexchange.com/questions/231756/comparing-the-increase-of-two-values","Creator_reputation":275}
{"_id":{"$oid":"5837a574a05283111e4d322a"},"View_count":6488,"Display_name":"rpierce","Question_score":7,"Question_content":"Sometimes I want to do an exact test by examining all possible combinations of the data to build an empirical distribution against which I can test my observed differences between means.  To find the possible combinations I'd typically use the combn function.  The choose function can show me how many possible combinations there are.  It is very easy for the number of combinations to get so large that it is not possible to store the result of the combn function, e.g. combn(28,14) requires a 2.1 Gb vector.  So I tried writing an object that steped through the same logic as the combn function in order to provide the values off an imaginary \"stack\" one at a time.  However, this method (as I instantiated it) is easily 50 times slower than combn at reasonable combination sizes, leading me to think it will also be painfully slow for larger combination sizes.  Is there a better algorithm for doing this sort of thing than the algorithm used in combn?Specifically is there a way to generate and pull the Nth possible combination without calculating through all previous combinations?","Creater_id":196,"Start_date":"2010-08-04 21:54:46","Question_id":1286,"Tags":["r","nonparametric","combinatorics"],"Answer_count":2,"Last_activity":"2016-08-25 13:43:28","Link":"http://stats.stackexchange.com/questions/1286/how-can-i-obtain-some-of-all-possible-combinations-in-r","Creator_reputation":8010}
{"_id":{"$oid":"5837a574a05283111e4d3238"},"View_count":186,"Display_name":"baer","Question_score":0,"Question_content":"This should be an easy one. I'm a novice when it comes to statistics and English isn't my first language so bear with me. I have one population that numbers about 700. Of these 700, 25 are of special interest. I want to compare the age, height, weight and BMI of the 25 and 675 groups. The problem is I'm not certain which test I should use. Do I need to check the normality of both the groups I'm comparing (25 vs 675) or do I just run one check for the whole group (700)? According to my interpretation of Kolmogorov-Smirnov and Shapiro-Wilk(and visual interpretation of QQ plots etc.) the bigger samples (675 and 700) are not normally distributed in any of the variables. The smaller sample of 25, however, is normally distributed in every variable except age. What kind of test should I use to compare these two groups?","Creater_id":101048,"Start_date":"2016-01-18 05:16:41","Question_id":191202,"Tags":["normal-distribution","normality","population"],"Answer_count":1,"Last_activity":"2016-08-25 13:27:51","Link":"http://stats.stackexchange.com/questions/191202/do-i-have-to-test-normality-for-both-groups-when-comparing-from-a-single-populat","Creator_reputation":1}
{"_id":{"$oid":"5837a574a05283111e4d3245"},"View_count":1169,"Display_name":"awhitesong","Question_score":2,"Question_content":"I am having five types of objects (flower, building, face, pair of shoes and a car) in my object recognition and i need to classify these. Identifying through edges in this type of data set seems to be a valid and distinguishable approach. So i have used canny edge detector(read it somewhere as a good approach) to find the edges in the image. Now i want to use this edged image to classify my objects. But since i am new to image recognition, i really couldn't figure how should i be selecting the features from this edged image for classification.Will it be a good approach to use all the pixels in the image(after canny edge detector) as features to any ANN classifier, but i think that would give a lot of redundant features (as most of the image is black except the edges) and there might be a possibility to reduce these. Is there any algorithm to select appropriate features from the image formed after canny detector?Second possibility could be to use any feature matching algorithm that could calculate the distance of the pixels between training and test data set(both edged) and predict the result with minimum distance. This is my approach but not sure about any existing algorithm. So needed some help on this.Also i tried considering CNN(since they intrinsically use the edged approach) but these seem to be really computationally expensive.","Creater_id":72792,"Start_date":"2015-06-25 12:59:09","Question_id":158696,"Tags":["machine-learning","neural-networks","image-processing"],"Answer_count":3,"Last_activity":"2016-08-25 13:19:17","Link":"http://stats.stackexchange.com/questions/158696/use-edge-detection-in-image-classification","Creator_reputation":11}
{"_id":{"$oid":"5837a574a05283111e4d3254"},"View_count":17,"Display_name":"Min gi Oh","Question_score":0,"Question_content":"I'm studying transfer entropy these days and have some questions about it.From the wiki, Transfer entropy is defined asTE(X→Y) = H(Yt∣Yt−1:t−L) − H(Yt∣Yt−1:t−L,Xt−1:t−L).As we can see in the formula, since past components of X of 2nd term in right hand side work as condition of entropy of Yt, we can tell whether X components influence uncertainty(entropy) of Y or not. If TE \u003e 0, then past X components give some effect on entropy of Y and we may say that information flows from process X to process Y.(I feel like I'm speaking roughly)In this point of view, is it okay to remove the condition 'Yt−1:t−L' in entropy calculation? As a beginner of this field, I think that 'H(Yt) − H(Yt∣Xt−1:t−L)' is enough to find out if information of past X flows to Y.Help me please!!!! ","Creater_id":128954,"Start_date":"2016-08-25 08:31:07","Question_id":231702,"Tags":["entropy","information-theory","mathematics"],"Answer_count":1,"Last_activity":"2016-08-25 13:17:36","Link":"http://stats.stackexchange.com/questions/231702/is-it-really-necessary-for-formula-of-transfer-entropy-to-contain-its-past","Creator_reputation":3}
{"_id":{"$oid":"5837a574a05283111e4d3261"},"View_count":22,"Display_name":"sambajetson","Question_score":0,"Question_content":"Say you are interested in finding the mean or median group size of a population, where individuals in the population belong to groups of size 1 to N, and you take a sample of individuals from the population. [And for simplicity, assume there is no measurement error / recording error].For example, you ask 100 people to write down their household size. They would write 1 if they live alone, 2 if they live with 1 other person, etc.The problem is that especially for a large ratio of sample size / population size, you may possibly sample individuals who come from the same group. Therefore, you will have a biased estimate because you duplicate counts of individuals from larger groups, since there are more likely repeats.If you exactly knew the population, you do the following:The number of households of size 4 is the total number of individuals who wrote 4, divided by 4. More generally, the actual number of unique households of size K is N_k / K, i.e. the number of individuals who wrote household size K, divided by the household size K. Then you could go on and calculate the actual mean/median household size.How do we do something similar to the above, but in the situation where we sample individuals from the population without knowing the population distribution? Basically, I want an unbiased estimator of the mean or median group size (household size in above example).","Creater_id":83546,"Start_date":"2016-08-25 13:10:48","Question_id":231764,"Tags":["estimation","sample","small-sample","unbiased-estimator"],"Answer_count":0,"Last_activity":"2016-08-25 13:10:48","Link":"http://stats.stackexchange.com/questions/231764/estimator-of-group-size-by-sampling-individuals-in-groups","Creator_reputation":136}
{"_id":{"$oid":"5837a574a05283111e4d3263"},"View_count":26,"Display_name":"rbm","Question_score":0,"Question_content":"I have a set of  models that have generated simulated values based on theoretical assumptions (not regressions) for times . I want to check which of these models has produced the best fit over this period (i.e. backward looking not forward looking) compared with the series of actuals. Important to note is that I am interested in which series mimics the \"shape\" (i.e. upward and downward movements etc.) of the actuals series the best, however, I am not interested in which series is actually closest to the actuals in terms of absolute distance (like Mean Squared Error would give me). I.e., a series could be very far away from the actuals at times, but as long as it moves in approximately the same way it is considered \"good\".Of course I can visually inspect this, but I want to be able to base it on numbers. Does anyone have any ideas on how I could do this in R?Thanks in advance.","Creater_id":31563,"Start_date":"2016-08-25 13:10:16","Question_id":231763,"Tags":["r","regression","goodness-of-fit","fitting","curve-fitting"],"Answer_count":0,"Last_activity":"2016-08-25 13:10:16","Link":"http://stats.stackexchange.com/questions/231763/which-curve-is-the-best-fit-in-terms-of-having-the-same-shape-as-the-actuals-se","Creator_reputation":193}
{"_id":{"$oid":"5837a574a05283111e4d3265"},"View_count":10,"Display_name":"Deepend","Question_score":0,"Question_content":"I have user ratings in the from continuous interval type data ranging from -100 to +100 e.g.P1D1 -47P4D1 2P6D3 19P7D1 37P4D7 -60P1D9 89I have seen some studies where the researchers converted all of their data to positive values for ease of computing.I am considering a Two Way Repeated Measures ANOVA but I am unsure whether I should/can use negative values or not. Should I convert these to positive values?The simplest method would be to simply add 100 to every value so that the range is 0 to 200 e.g.P1D1 53P4D1 102P6D3 119P7D1 137P4D7 40P1D9 189Is there any issue I should watch out for if I do this?P.S. is there an accepted term for this sort of conversion? ","Creater_id":39684,"Start_date":"2016-08-25 13:07:58","Question_id":231762,"Tags":["anova"],"Answer_count":0,"Last_activity":"2016-08-25 13:07:58","Link":"http://stats.stackexchange.com/questions/231762/should-i-convert-negative-values-to-positive-before-undertaking-two-way-repeated","Creator_reputation":53}
{"_id":{"$oid":"5837a574a05283111e4d3267"},"View_count":101,"Display_name":"cozos","Question_score":3,"Question_content":"Based on what I've read, the best model-free reinforcement learning algorithm to this date is Q-Learning, where each state, action pair in the agent's world is given a Q-value, and at each state the action with the highest Q-value is chosen. The Q-value is then updated as follows:Q(s,a) = (1-\\alpha)Q(s,a) +\\alpha(R(s,a,s') + (\\max_{a'}  Q(s',a'))) where  is the learning rate.Apparently, for problems with high dimensionality, the number of states become astronomically large making q-value table storage infeasible. So the practical implementation of Q-Learning requires using Q-value approximation via generalization of states aka features. For example if the agent was Pacman then the features would be:Distance to closest dotDistance to closest ghostIs Pacman in a tunnel?And then instead of q-values for every single state you would only need to only have q-values for every single feature.So my question is, is it possible for a reinforcement learning agent to create or generate additional features?Geramifard's iFDD method is a way of \"discovering feature dependencies\", but I'm not sure if that is feature generation, as the paper assumes that you start off with a set of binary features.Another paper that I found was apropos is Playing Atari with Deep Reinforcement Learning, which \"extracts high level features using a range of neural network architectures\".I've read over the paper but still need to flesh out/fully understand their algorithm. Is this what I'm looking for?","Creater_id":62403,"Start_date":"2014-12-08 19:32:17","Question_id":127252,"Tags":["machine-learning","deep-learning","reinforcement-learning"],"Answer_count":1,"Last_activity":"2016-08-25 13:05:54","Link":"http://stats.stackexchange.com/questions/127252/is-it-possible-for-a-reinforcement-learning-agent-to-create-or-generate-addition","Creator_reputation":16}
{"_id":{"$oid":"5837a574a05283111e4d3274"},"View_count":83,"Display_name":"Donbeo","Question_score":2,"Question_content":"This question is a followup of my previous question Forecasting with ARIMA and GARCH: does my plan look alright?I have a times series  and I am trying to estimate its volatility with a GARCH model as in the referred question.You can for example consider the following time series: [1] -0.008230499 -0.025105921 -0.025752496  0.025752496 [5] -0.008510690  0.041847110 -0.033336420  0.041499731 [9] -0.008163311  0.024292693 -0.032523192 -0.008298803[13]  0.080042708  0.000000000  0.088292607  0.041385216[17] -0.013605652  0.046831300  0.006514681 -0.019672766[21]  0.013158085 -0.006557401  0.019544596  0.092373320[25]  0.116474991  0.020726131  0.169418152  0.167054085[29] -0.128832872  0.056695344 -0.032002731 -0.016393810[33] -0.151399646  0.104879631  0.159701110 -0.029964789[37] -0.003809528 -0.034955015 -0.011928571 -0.016129382[41]  0.012121361 -0.004024150 -0.004040410 -0.008130126[45] -0.033198069 -0.065382759  0.017857617  0.034786116[49] -0.017241806 -0.026433257  0.013303966  0.000000000[53] -0.045052664  0.013730193 -0.009132484 -0.013857035[57] -0.023530497 -0.019231362 -0.070380797 -0.005221944[61]  0.015584731 -0.010362787 -0.048009219 -0.005479466\u0026gt; the elements of which are not autocorrelated. The volatility is then estimated using a GARCH(1,1) model and predicted from it as follows:G.A = garchFit(formula = A~garch(1, 1), data = diff_log_close_price, trace = F)G.A.est = predict(G.A, 30, plot=T, nx=nrow(closing_price))volatility.A = c(volatility(G.A, type = \"sigma\"), G.A.est$standardDeviation)Questions:How can I estimate the confidence interval for the volatility from a theoretical point of view? Is there an R function that does it?I need the intervals for both predicted and historical volatility.","Creater_id":25392,"Start_date":"2016-08-21 12:42:53","Question_id":230995,"Tags":["time-series","confidence-interval","garch","prediction-interval","volatility-forecasting"],"Answer_count":0,"Last_activity":"2016-08-25 13:04:35","Link":"http://stats.stackexchange.com/questions/230995/estimating-the-confidence-interval-for-the-volatility-of-a-garch-model","Creator_reputation":968}
{"_id":{"$oid":"5837a574a05283111e4d3276"},"View_count":64,"Display_name":"Ashlyn","Question_score":1,"Question_content":"I am currently trying to perform PCA on time series data and I'm having some confusion. I need to make my data stationary first before I can perform PCA on it and here is the confusion.I have 128 observations for each time series but some time series require second differencing while some others require just a single one. So after differencing, the resulted time series will be of different length (126 and 127). Then how do I get  matrix since some observations are missing from the differencing?","Creater_id":115356,"Start_date":"2016-08-23 09:12:14","Question_id":231313,"Tags":["time-series","pca"],"Answer_count":2,"Last_activity":"2016-08-25 13:00:43","Link":"http://stats.stackexchange.com/questions/231313/applying-pca-on-time-series-when-some-time-series-have-to-be-differenced-and-som","Creator_reputation":11}
{"_id":{"$oid":"5837a574a05283111e4d3284"},"View_count":8,"Display_name":"roboreb","Question_score":0,"Question_content":"I'm studying a phenomenon that has no directly observed prevalence estimate. Instead, it has been calculated in the following manner:The question is: How many Quags are there in the general population? (X)There is an estimated 2% of Blorks in the general population (A)Among the sub-population of Blorks, 40% are estimated to be Quags (B)Among the sub-population of Quags, 40% are estimated to be Blorks (C)X = A * B * 1/CX = 0.02 * 0.4 * 1/0.4X = 0.02I'm trying to critique this estimate. Am I correct in thinking that the error term for this estimate would be equal to multiplying the error terms for A, B, and C?","Creater_id":28438,"Start_date":"2016-08-25 12:54:56","Question_id":231759,"Tags":["error","error-propagation"],"Answer_count":0,"Last_activity":"2016-08-25 12:54:56","Link":"http://stats.stackexchange.com/questions/231759/prevalence-by-calculation-error-terms-get-multiplied","Creator_reputation":33}
{"_id":{"$oid":"5837a574a05283111e4d3286"},"View_count":46,"Display_name":"Paul Reiners","Question_score":0,"Question_content":"Suppose I have a data set containing many rows and one column like this:##      value ## 1 4.315975         ## 2 4.416492         ## 3 4.305562         ## 4 4.339515         ## 5 4.313470        ## 6 4.412710         I can summarize how anomalous each row is by adding another column for standard score normalized values:##      value normalized_value## 1 4.315975       -0.6813408## 2 4.416492        1.2954239## 3 4.305562       -0.8861225## 4 4.339515       -0.2184037## 5 4.313470       -0.7306040## 6 4.412710        1.2210472 I can then consider a row to be anomalous if the normalized_value of that row is less than, say, -3.0 or greater than 3.0.Now suppose I have two or more columns of values and also their normalized values like this:##    value_1 norm_value_1  value_2 norm_value_2## 1 4.315975   -0.6813408 2.782256   -1.7706630## 2 4.416492    1.2954239 2.900394   -0.1166862## 3 4.305562   -0.8861225 2.886051   -0.3174936## 4 4.339515   -0.2184037 2.940737    0.4481311## 5 4.313470   -0.7306040 2.967396    0.8213673## 6 4.412710    1.2210472 2.975537    0.9353443Are there any standard techniques for expressing the 'anomalousness' of the two or more columns with a single number for each row?   We can't make a combined column by averaging the two normalized value columns.  We're concerned about anomalies and a non-anomalous value in one column could cancel out an anomalous value in the other column.If any of the values are anomalous for a particular row, we want to be certain that this is preservedin the combined column.  So we could set the values for the new combined value column for a given rowto be the maximum of the absolute value of each of the original normalized value columns for that row.abs_max_abs_func \u0026lt;- function(x, y) max(abs(x), abs(y))combined_value \u0026lt;- sapply(norm_value_1, abs_max_abs_func, y = norm_value_2)##    value_1 norm_value_1  value_2 norm_value_2 combined_value## 1 4.315975   -0.6813408 2.782256   -1.7706630      1.7706630## 2 4.416492    1.2954239 2.900394   -0.1166862      1.2954239## 3 4.305562   -0.8861225 2.886051   -0.3174936      0.8861225## 4 4.339515   -0.2184037 2.940737    0.4481311      0.4481311## 5 4.313470   -0.7306040 2.967396    0.8213673      0.8213673## 6 4.412710    1.2210472 2.975537    0.9353443      1.2210472Note that another reasonable approach would be to set the value of the combined valuecolumn to be the value of that normalized value column that has the maximum absolute value (that is,we don't take the absolute value of those values).  max_abs_func \u0026lt;- function(x, y) if (abs(x) \u0026gt;= abs(y)) x else ycombined_value \u0026lt;- sapply(norm_value_1, max_abs_func, y = norm_value_2)##    value_1 norm_value_1  value_2 norm_value_2 combined_value## 1 4.315975   -0.6813408 2.782256   -1.7706630     -1.7706630## 2 4.416492    1.2954239 2.900394   -0.1166862      1.2954239## 3 4.305562   -0.8861225 2.886051   -0.3174936     -0.8861225## 4 4.339515   -0.2184037 2.940737    0.4481311      0.4481311## 5 4.313470   -0.7306040 2.967396    0.8213673      0.8213673## 6 4.412710    1.2210472 2.975537    0.9353443      1.2210472The problemwith this approach (if, indeed, it is a problem) is that there are more discontinuities.We can jump from a large negative value to a large positive value or vice-versa.  But we do lose less information doing it this way.Another possible answer is to find the probability of each value.  Since these are real values and are probably all unique, probability doesn't make sense.  However we could bin the values and find the probability of a value being in the bin that it is in (by dividing the number of values in that bin by the number of all values).  Then, for a row, multiply the probabilities together.  This is assuming the columns are independent.  But, even if they aren't all completely independent, this might give reasonable values.Any suggestions?  I'm sure someone must have tried to do this sort of thing.","Creater_id":1618,"Start_date":"2016-08-25 10:36:47","Question_id":231737,"Tags":["anomaly-detection"],"Answer_count":1,"Last_activity":"2016-08-25 12:46:27","Link":"http://stats.stackexchange.com/questions/231737/summarizing-the-anomalousness-of-several-columns","Creator_reputation":285}
{"_id":{"$oid":"5837a574a05283111e4d328b"},"View_count":73,"Display_name":"Wuchen","Question_score":1,"Question_content":"Suppose I have two random variable  and , I know that for every , . Can I get the conclusion that ?Another similar question is that suppose I have two sequence of random variables  and . if I know that for every ,  converges in distribution to. Can I get the conclusion that  itself converges in distribution to ?","Creater_id":101690,"Start_date":"2016-08-24 18:21:49","Question_id":231581,"Tags":["normal-distribution","conditional-probability","convergence"],"Answer_count":1,"Last_activity":"2016-08-25 12:36:11","Link":"http://stats.stackexchange.com/questions/231581/does-conditional-normality-imply-normality","Creator_reputation":23}
{"_id":{"$oid":"5837a574a05283111e4d3298"},"View_count":60,"Display_name":"The Lazy Log","Question_score":1,"Question_content":"I designed my own neural network for solving the problem of text summarization. The number of documents in my training dataset is big (more than 100,000 documents) so it is hard to check it on the whole data. In order to verify that my model is good, I train it on a very small dataset (100 documents) in about 100 epochs to see how it behaves. I split this small dataset into 3 sets (6/2/2): training, validation and test. Here is the chart of the losses (red line is training loss, blue line is validation loss and green line is test loss)Is my evaluation on this small dataset good enough to tell whether my model is performing well? Does the above chart shows that my model is getting overfitting easily? Do you have any recommendation for quickly evaluating a new proposed model and avoid overfitting?Update: I trained my network again on a bigger data set (4000 documents) and I got the following chart. It does seem my model is not well-designed.","Creater_id":128516,"Start_date":"2016-08-23 23:36:36","Question_id":231416,"Tags":["neural-networks","deep-learning"],"Answer_count":1,"Last_activity":"2016-08-25 12:34:31","Link":"http://stats.stackexchange.com/questions/231416/is-there-something-wrong-in-my-neural-network-model","Creator_reputation":123}
{"_id":{"$oid":"5837a574a05283111e4d32a5"},"View_count":50,"Display_name":"user39531","Question_score":0,"Question_content":"May someone advise how a significant value of 0.000 be interpreted in a one way within subject ANOVA test, with one independent var, comprising of 2 levels.Thank you","Creater_id":39531,"Start_date":"2016-08-25 12:14:40","Question_id":231748,"Tags":["anova","descriptive-statistics"],"Answer_count":1,"Last_activity":"2016-08-25 12:28:59","Link":"http://stats.stackexchange.com/questions/231748/what-does-sig-value-0-000-mean","Creator_reputation":60}
{"_id":{"$oid":"5837a574a05283111e4d32b2"},"View_count":65,"Display_name":"Glen","Question_score":3,"Question_content":"Based on section 26.3 of the stan user guide, I'm trying to specify a model in which observed values are rounded and the true values are known to fall in a certain range (between observed and observed -1).  The STAN code is below.  Looking at the traceplot for xtrue the sampled values are not constrained between (xobs-1,xobs).  Any help is appreciated.library(rstan)rstan_options(auto_write = FALSE)options(mc.cores = parallel::detectCores())nobs=10xtrue=runif(nobs,0,5)xobs=ceiling(xtrue+rnorm(nobs,0,1))dat=list(N=length(xobs),x=xobs)init_fun \u0026lt;- function() {list(xtrue=xobs-.5) }m=\"data { int\u0026lt;lower = 1\u0026gt; N; real x[N];}parameters { real xtrue[N]; }model{    for(i in 1:N){            increment_log_prob(normal_log(xtrue[i], x[i], 1));            increment_log_prob(-log_diff_exp(normal_cdf_log(x[i],0,1),                normal_cdf_log(x[i]-1,0,1)));    }}\"fit=stan(model_code=m, data = dat,iter = 2000, chains = 1,thin=3,init=init_fun)parms=extract(fit,c('xtrue'))xtrue \u0026lt;- colMeans(parms[['xtrue']])head(xobs)head(xtrue)traceplot(fit)","Creater_id":2310,"Start_date":"2016-03-19 14:44:14","Question_id":202563,"Tags":["stan"],"Answer_count":1,"Last_activity":"2016-08-25 12:26:23","Link":"http://stats.stackexchange.com/questions/202563/sampling-from-truncated-distribution-in-stan","Creator_reputation":3572}
{"_id":{"$oid":"5837a574a05283111e4d32bf"},"View_count":65,"Display_name":"Student001","Question_score":2,"Question_content":"This question is about fitting a multivariate linear regression by maximum likelihood, under a specific parameterization of the covariance matrix, when the number of observations is smaller than the number of responses. It arises in an applied project that I'm part of.Let  be independent multivariate normal with (non-stochastic) mean  and covariance matrix .  is a diagonal matrix and the  are correlation matrices of sizes  and , respectively, where . The number of predictors, , is small enough that the MLE of  may be computed as , where  and  are  and  matrices with the  and  as rows.Now, after profiling out  the profile log-likelihood is:\\ell(\\Lambda, R_1, R_2) = -\\frac{nr}{2}\\log(2\\pi) - n\\log\\vert \\Lambda\\vert - \\frac{nr_2}{2}\\log\\vert R_1\\vert - \\frac{nr_1}{2}\\log\\vert R_2\\vert - \\frac{n}{2}\\mathrm{tr}\\left[\\Lambda^{-1}(R_1^{-1}\\otimes R_2^{-1})\\Lambda^{-1}S\\right],where .Question: Can this be optimized over  subject to the constraint that  is diagonal and  are correlation matrices? I do have (unconstrained) gradients for all parameters.","Creater_id":37483,"Start_date":"2016-08-04 07:40:17","Question_id":228264,"Tags":["regression","maximum-likelihood","linear-model","optimization","multivariate-normal"],"Answer_count":1,"Last_activity":"2016-08-25 12:20:46","Link":"http://stats.stackexchange.com/questions/228264/maximum-likelihood-in-multivariate-linear-regression","Creator_reputation":2852}
{"_id":{"$oid":"5837a574a05283111e4d32cc"},"View_count":85,"Display_name":"Guilherme Salom\u0026#233;","Question_score":4,"Question_content":"The definition of a compound Poisson process and its characteristic function I have are the following:  Let  and . Also,  are i.i.d. and independent of . And  are i.i.d., , and independent from . Define:  Y_t\\equiv\\sum_{i=1}^N\\mathbb{1}_{\\{U_i\\leq t\\}}X_i, 0\\leq t\\leq T  Then  is a compound Poisson process with intensity parameter  and jump pdf .    The characteristic function of  is:  \\mathbb{E}(e^{iuY_1})=e^{\\lambda\\int(e^{ix}-1)f(x)dx}Note that the characteristic function I quoted above is for , not . I am trying to show the equality above. I currently have:\\begin{align}\\mathbb{E}(e^{iuY_1})\u0026amp;=\\sum_nP(N=n)\\mathbb{E}(e^{iuY_1}\\mid N=n)\\\\\u0026amp;=\\sum_nP(N=n)\\prod_{j=1}^n\\mathbb{E}(e^{iu\\mathbb{1}_{\\{U_j\\leq 1\\}}X_j})\\quad\\text{(by independence)}\\\\\u0026amp;=\\sum_n P(N=n)\\prod_{j=1}\\int e^{iux}f(x)dx\\quad\\text{(by uniform)}\\end{align}I am not sure how to proceed. Any tips?Thanks for helping! :D","Creater_id":25824,"Start_date":"2016-08-22 17:29:59","Question_id":231194,"Tags":["probability","poisson","poisson-process","characteristic-function"],"Answer_count":2,"Last_activity":"2016-08-25 12:20:36","Link":"http://stats.stackexchange.com/questions/231194/characteristic-function-of-a-compound-poisson-process","Creator_reputation":380}
{"_id":{"$oid":"5837a574a05283111e4d32da"},"View_count":599,"Display_name":"user34790","Question_score":6,"Question_content":"I have this confusion related to the benefits of Gaussian processes. I mean comparing it to simple linear regression, where we have defined that the linear function models the data.However, in Gaussian processes we define the distribution of the functions means we don't specifically define that the function should be linear. We can define a prior over the function which is the Gaussian prior which defines features like how much smooth the function should be and all.So we don't have to explicitly define what the model should be. However, I have questions. We do have marginal likelihood and using it we can tune the covariance function parameters of the gaussian prior. So this is similar to defining the type of function that it should be isn't it.It boils down to the same thing defining the parameters even though in GP they are hyperparameters. For eg in this paper. They have defined that the mean function of the GP is something like m(x) = ax ^2 + bx + c \\quad \\text{i.e. a second order polynomial.}So definitely the model/function is defined isn't it. So what's the difference in defining the function to be linear like in the LR.I just didn't get what the benefit is of using GP","Creater_id":12329,"Start_date":"2012-12-30 17:24:45","Question_id":46738,"Tags":["gaussian-process"],"Answer_count":1,"Last_activity":"2016-08-25 11:30:11","Link":"http://stats.stackexchange.com/questions/46738/gaussian-processes-benefits","Creator_reputation":1463}
{"_id":{"$oid":"5837a574a05283111e4d32e7"},"View_count":80,"Display_name":"Z. Zhang","Question_score":0,"Question_content":"I am studying the randomForest package to deal with an extremely imbalanced dataset (target: 98% vs 2%). I realize I can utilize parameters sampsize and strata to do Balanced Random Forest (downsizing). However, assuming I am taking 70% of the minor group and the same size of the major group as my sample size (e.g. sampsize = c(minorSize*0.7,minorSize*0.7)), should I do replacement = TRUE or FALSE? ","Creater_id":100150,"Start_date":"2016-08-24 00:48:18","Question_id":231422,"Tags":["r","random-forest"],"Answer_count":1,"Last_activity":"2016-08-25 11:26:36","Link":"http://stats.stackexchange.com/questions/231422/r-randomforest-resample-replacement-or-not","Creator_reputation":15}
{"_id":{"$oid":"5837a574a05283111e4d32f4"},"View_count":11,"Display_name":"Karolis Juodelė","Question_score":0,"Question_content":"Suppose there are objects (of a single type, like movies, antiques, papers or etc) and experts, who may evaluate (the quality, price, correctness or etc, of) any number of these objects. How can I, at the same time, average these evaluations for each object, based on how much I trust each expert, and compute how trustworthy each expert is, based on how well he agrees with other experts?I imagine this is a general enough problem that someone would have thought about it, but I have no idea what to call it.Initially this may need some labeled training data. I'm not sure what. Probably a few experts should start off with higher confidence than others. On one hand, I wouldn't want to introduce my bias into the system, so these initial confidences should not be permanent. On the other hand, I want the system to be somewhat stable, so that even a flood of malicious experts couldn't easily ruin the scores. I can't tell if those goals are contradictory. And more generally, I'm not sure if this whole idea is circular.","Creater_id":68877,"Start_date":"2016-08-25 11:07:03","Question_id":231740,"Tags":["machine-learning","estimation"],"Answer_count":0,"Last_activity":"2016-08-25 11:07:03","Link":"http://stats.stackexchange.com/questions/231740/simultaneously-learning-values-of-objects-and-trustworthiness-of-experts","Creator_reputation":123}
{"_id":{"$oid":"5837a574a05283111e4d32f6"},"View_count":1150,"Display_name":"Eitan Schechtman","Question_score":5,"Question_content":"I am using Fisher's combined test to fuse several different independent tests. I have a problem understanding the results in some cases.Example:Let's say I run two different tests, both with the hypothesis that mu is smaller than 0. Let's say that n is identical and the two samples have the same calculated variance. However, let's assume that one test yielded an average that is  and the other is . I will get two complementing p-vals (e.g.,  \u0026amp; ). Interestingly, combining the two brings about a significant -value in the Fisher test: .This is weird because I could have chosen the exact opposite test  and sampled results - and still get . It's almost as if the Fisher test does not take the direction of the hypothesis into account.Can anyone explain this?Thanks","Creater_id":12194,"Start_date":"2012-06-25 05:58:29","Question_id":31070,"Tags":["chi-squared","fisher","combining-p-values"],"Answer_count":2,"Last_activity":"2016-08-25 10:21:02","Link":"http://stats.stackexchange.com/questions/31070/understanding-fishers-combined-test","Creator_reputation":26}
{"_id":{"$oid":"5837a574a05283111e4d3304"},"View_count":39,"Display_name":"qiang","Question_score":2,"Question_content":"I wish to examine the correlation between two variables ( \u0026amp; ) of the eye. Each participant (e.g.  participants) contributes two eyes ( samples). I don't wish to look at the relationship of  \u0026amp;  for right eye, and for left eye individually, but just for the relationship of  \u0026amp;  for all eyes. Do I disregard left and right, and that they are from the same participant and take ?Do I randomly select a single eye from each participant ()?Or is there a way of using both eyes taking into consideration that there are  pairs of highly correlated data? ","Creater_id":128951,"Start_date":"2016-08-25 07:50:47","Question_id":231694,"Tags":["paired-comparisons","paired-data"],"Answer_count":0,"Last_activity":"2016-08-25 10:14:30","Link":"http://stats.stackexchange.com/questions/231694/statistics-on-paired-eyes","Creator_reputation":11}
{"_id":{"$oid":"5837a574a05283111e4d3306"},"View_count":28,"Display_name":"user254769","Question_score":0,"Question_content":"I know some probability theory but I am still not very familiar with more advanced topics in this area. I was wondering whether anyone can help me with the following question.Is there any way possible to show/prove that \\textrm{Pr}(X+Y\\leq L) \u0026lt; \\textrm{Pr}(X+Y+Z\\leq L)\\quad? where , ,  are discrete independent random variables and the probabilities express essentially the CDF of their sums.  is a constant known value.If anyone has any thoughts/feedback, they would be of tremendous help!","Creater_id":128970,"Start_date":"2016-08-25 09:54:01","Question_id":231728,"Tags":["probability"],"Answer_count":1,"Last_activity":"2016-08-25 10:14:29","Link":"http://stats.stackexchange.com/questions/231728/cdf-of-sum-of-independent-discrete-random-variables","Creator_reputation":1}
{"_id":{"$oid":"5837a574a05283111e4d3313"},"View_count":29,"Display_name":"have fun","Question_score":2,"Question_content":"I am studying animal behavior and I want to know whether the frequency of a specific behavior changes with time.I thought it was a common and simple problem but I cannot find papers with examples that suits my case.So far I proceeded as follows:I binned my time points, and I tested whether the events for each animal and each series followed the poisson distribution with the k-s test. I took as lambda the mean of each vector of value.I am not sure that this approach is the best. What if the bins with highest numbers are always at the start?Looking at other possibilities I came across time series analysis but I am totally new to them. I tried to self teach myself but with scarce success.Basically I would like to show that there is not any time pattern: the frequency of events do not decrease or increase with time. Moreover, I would like to show it considering several individuals. (Maybe one individual shows acclimatization but the population doesn't, especially this last point is not common in time series examples)How to do this in R? what tests to use?Example of the original dataset, (actually for each ID I have hundreds of occurrences):time\u0026lt;-c(7.11447,19.13773, 42.38522, 49.91215, 57.75048, 62.06984, 83.17565,    87.91016, 88.26145, 98.34730, 5.81488,  6.12617, 19.92766, 20.33673, 22.51982, 27.85156, 32.95741, 33.07515, 35.65510, 37.02395,102.6407,  103.6427,  506.1014,  569.6760,  578.3639,  623.6512,  637.4765,  992.3210, 1003.3756, 1016.9787)ID\u0026lt;-rep(c(1,2,3), each=10)dat\u0026lt;- data.frame(cbind(time, ID))Thanks for any solution/direction","Creater_id":70361,"Start_date":"2016-08-22 06:37:53","Question_id":231088,"Tags":["r","time-series","poisson-process"],"Answer_count":1,"Last_activity":"2016-08-25 09:56:53","Link":"http://stats.stackexchange.com/questions/231088/how-do-i-check-whether-a-series-of-time-events-does-not-show-acclimatization-an","Creator_reputation":41}
{"_id":{"$oid":"5837a574a05283111e4d3320"},"View_count":362,"Display_name":"tams","Question_score":1,"Question_content":"I use two criterion ( and ) for predicting the outcome of football matches. Analysis of historical records has provided me with two best fit linear equations. The probability that the team playing at their home ground will win based on criteria w, is . A different probability based on  is . What method should be used to combine these two equations to produce a single equation to obtain , or should an entirely different approach be used to arrive at a single equation? ","Creater_id":12185,"Start_date":"2012-06-25 11:41:03","Question_id":31092,"Tags":["regression","probability"],"Answer_count":0,"Last_activity":"2016-08-25 09:50:28","Link":"http://stats.stackexchange.com/questions/31092/combining-probabilities-for-sports-prediction","Creator_reputation":16}
{"_id":{"$oid":"5837a574a05283111e4d3322"},"View_count":112,"Display_name":"Peter","Question_score":2,"Question_content":"I am trying to estimate a causal effect using a difference in difference estimator. I suspect there will be a different effect for small and large firms. I am interested in this different effect. I want to know if it is statistically significantly different.  y_it  =d_t* \\beta1+Treated_i* \\beta2+Treated_i *d_t *\\beta3+big_i*d_t*\\beta4+big_i *Treated_i*\\beta5+Treated_i*d_t*big_i*\\beta6+\\epsilonWhere big is a dummy indicating whether the firm is small or big. 'd' is an indicator for the moment of treatment and treated is an dummy for being in the treatment group.The function will include some control variables, to ensure the trend in the residuals is similar.This was the formula I was thinking of using. Or do I only need to interact the actual treatment effect with the interaction term (rather than the treatment moment and treatmentgroup dummy as well)?Furthermore am I correct in thinking that this will satisfy the assumptions underlying the DiD estimator ? The trends in the pre-treatment period will be the same, conditional on being a large or small firm, which is captured by this setup. The treatment is an exogeneous shock. If this is not a valid approach, what is a suitable alternative ?I thought of running the DiD separately for the small and big sample, but this will not tell me anything about which group faces a bigger effect. At least not in a way that I can statistically test which one is stronger. ","Creater_id":96097,"Start_date":"2015-11-24 02:41:38","Question_id":183302,"Tags":["regression","interaction","panel-data","difference-in-difference"],"Answer_count":2,"Last_activity":"2016-08-25 09:45:28","Link":"http://stats.stackexchange.com/questions/183302/difference-in-difference-with-interaction","Creator_reputation":38}
{"_id":{"$oid":"5837a574a05283111e4d3330"},"View_count":14,"Display_name":"Thanos","Question_score":0,"Question_content":"I have a set of user transactions with a service and I suspect that there is a strong correlation between how users rate the service and how likely they are to use the service again.I have chosen to represent user ratings - for each user in the dataset - as a number [1-5] which is the average of all ratings the given user has given in the past. There is more to it in regards to how to represent trends etc, but this is not in context for the problem at hand.My issue is that there is a high number of users that have never left any ratings at all and I am not sure how to deal with these users, in terms of finding the right value for the rating-related features for them.What I have tried so far is to represent ratings in a different way, shown below:UID    rating_high    rating_medium    rating_low  A              2                0             1  B              0                0             0where each of the predictors represent the count of ratings - in each rating category - by each of the users in the dataset.In the above case, user A has left a good rating in 2 occasions, a low rating in one occasion and has never left a 'medium' rating. User B has never left any ratings for the service at all, therefore, he is assigned a count of 0 for all 'rating categories'.on the 1-5 rating scale, I class all ratings below 3 as rating_low, all those equal to 3 as rating_medium and the ones above 3 as rating_high.I have not managed to find any other way to represent this data, but I would very much rather not drop this part of the dataset as I believe it caries valuable semantics for my problem.Any ideas on how to best deal with this issue are most appreciated!","Creater_id":111718,"Start_date":"2016-08-25 09:41:36","Question_id":231724,"Tags":["machine-learning","dataset","missing-data"],"Answer_count":0,"Last_activity":"2016-08-25 09:41:36","Link":"http://stats.stackexchange.com/questions/231724/user-ratings-and-conditional-values-in-dataset","Creator_reputation":101}
{"_id":{"$oid":"5837a574a05283111e4d3332"},"View_count":40,"Display_name":"Paul","Question_score":0,"Question_content":"  I need to find the exact test of level  for null hypothesis  against the alternative hypothesis  based on i.i.d data  that follow the exponential distribution with scale parameter .I know that the likelihood function is  and the relative MLE is . I use the definition of likelihood ratio:\\Lambda=\\frac{L(\\theta)}{\\sup L(\\theta)}=\\frac{\\theta^{-n}e^{-\\frac{\\sum y}{\\theta}}}{\\bar y^{-n}e^{-\\frac{\\sum y}{\\bar y}}}=\\left(\\frac{\\theta}{\\bar y}\\right)^{-n}e^{-\\left(\\frac{\\sum y}{\\theta}+\\frac{\\sum y}{\\bar y}\\right)}But now I'm stuck on how to proceded. Any help or suggest?","Creater_id":105511,"Start_date":"2016-08-25 08:59:23","Question_id":231708,"Tags":["hypothesis-testing","self-study"],"Answer_count":1,"Last_activity":"2016-08-25 09:37:45","Link":"http://stats.stackexchange.com/questions/231708/likelihood-ratio-test-for-exponential-distribution-with-scale-parameter","Creator_reputation":59}
{"_id":{"$oid":"5837a574a05283111e4d333f"},"View_count":799,"Display_name":"John-Annual","Question_score":4,"Question_content":"I am learning RBM (restricted Boltzmann machine) for deep learning. The log-likelihood of RBM is given as : \\ln(L(\\theta|v))=\\ln(p(v|\\theta))=\\ln\\frac{1}{Z}\\sum_h e^{-E(v,h)}=\\ln\\sum_h e^{E(v,h)}-\\ln\\sum_{v,h}e^{-E(v,h)}and its gradient w.r.t. the parameter is: \\frac{\\partial L(\\theta|v)}{\\partial\\theta}=-\\sum_h p(h|v)\\frac{\\partial E(v,h)}{\\partial\\theta}+\\sum_{v,h}p(v,h)\\frac{E(v,h)}{\\partial\\theta}I don't understand how is the gradient derived from the log-likelihood.","Creater_id":54772,"Start_date":"2014-08-27 03:59:24","Question_id":113395,"Tags":["neural-networks","deep-learning","rbm","deep-belief-networks"],"Answer_count":3,"Last_activity":"2016-08-25 09:27:45","Link":"http://stats.stackexchange.com/questions/113395/how-to-derive-the-gradient-formula-for-the-maximum-likelihood-in-rbm","Creator_reputation":21}
{"_id":{"$oid":"5837a574a05283111e4d334e"},"View_count":31,"Display_name":"them","Question_score":0,"Question_content":"Here on page 7, example 2.7. The claim is that sufficient statistics for  dimensional multivariate normal   is \\left(n^{-1}\\sum_{i=1}^n \\mathbf{x}_i, \\hat{\\Sigma} \\right)\\,, where . Where can I find derivation of this result? Thanks!","Creater_id":94074,"Start_date":"2016-08-25 09:15:37","Question_id":231714,"Tags":["mathematical-statistics","multivariate-normal","exponential-family","sufficient-statistics"],"Answer_count":0,"Last_activity":"2016-08-25 09:15:37","Link":"http://stats.stackexchange.com/questions/231714/sufficient-statistic-for-multivariate-normal","Creator_reputation":116}
{"_id":{"$oid":"5837a574a05283111e4d3350"},"View_count":119,"Display_name":"rrrj","Question_score":4,"Question_content":"I want a set of input values to be as similar to the output values as possible.I have an input matrix X (m*n) that has m data points and n dimensions for each data point. I also have an output matrix Y (m*n) that has m data points and n dimensions.I want to solve the equation XA + e = Y where A is a diagonal matrix (nn) and e is the error. I want to find the diagonal matrix A that would minimize e. Diagonal matrix A would essentially give me one value for each dimension that would bring my X closest to Y. I would expect A to minimize the sum of squares between each data point.Additional Constraint is that the diagonal matrix should have values between 0 and 1.Any help is greatly appreciated. Any alternate approach for to find a value for each dimension that brings X closest to Y is appreciated.","Creater_id":124537,"Start_date":"2016-08-25 04:32:17","Question_id":231657,"Tags":["regression","least-squares","optimization","matrix-decomposition","total-least-squares"],"Answer_count":2,"Last_activity":"2016-08-25 09:12:32","Link":"http://stats.stackexchange.com/questions/231657/find-best-fit-diagonal-matrix-for-error-minimization","Creator_reputation":33}
{"_id":{"$oid":"5837a574a05283111e4d335e"},"View_count":38,"Display_name":"Donbeo","Question_score":1,"Question_content":"Suppose I have a forecasted daily volatility for K days. How can I get the forecasted monthly volatility from the daily ?  ","Creater_id":25392,"Start_date":"2016-08-25 08:39:14","Question_id":231704,"Tags":["time-series","variance","garch","volatility-forecasting"],"Answer_count":1,"Last_activity":"2016-08-25 09:11:36","Link":"http://stats.stackexchange.com/questions/231704/how-to-convert-daily-volatility-to-monthly","Creator_reputation":968}
{"_id":{"$oid":"5837a574a05283111e4d336b"},"View_count":184,"Display_name":"dimebucker91","Question_score":5,"Question_content":"I have data about different teams, players etc. I am trying to figure out the best way to model the outcome of a match, which can end in a win for the home team, a loss for the home team, or a draw. I am having trouble modelling this though. For example, I can use a poisson regression to model the number of goals each team scores, and then calculate a grid of their probabilities, but I am not too happy with the independence assumption. I could also do a bivariate poisson, which i dont have much experience with. I am wondering what a suitable approach is for modelling the dependence of the outcome on the two teams, while also preserving the fact that the outcomes are mutually exclusive (the probabilities assigned to win draw loss should sum to unity).","Creater_id":55946,"Start_date":"2016-08-15 02:35:13","Question_id":229881,"Tags":["regression","modeling","poisson"],"Answer_count":2,"Last_activity":"2016-08-25 08:41:24","Link":"http://stats.stackexchange.com/questions/229881/modelling-a-win-draw-loss-outcome-in-sports","Creator_reputation":176}
{"_id":{"$oid":"5837a574a05283111e4d3379"},"View_count":31,"Display_name":"jessyjemy","Question_score":1,"Question_content":"I am trying to understand the meaning of the phi parameter of the AR modeling. A bit of background: I am digging into statistical parametric mapping (SPM) and the prewhitening method, used to get rid of the temporal correlation in the data.SPM in particular models the noise with a AR(1) model, which means that the model takes into account the correlation between samples that are 1 lag apart, right?In the specifications of the AR(1) model, phi is set to 0.2. There is no reference for this, only in a mailing list I found that this is an empirical value that is good for fMRI.The topic of course is also explained on wikipedia: But despite the readings and the research what I still don't get is: what's the exact effect of this parameter on the data?When I change it in SPM, final results change drastically (in terms of statistical significance). How to assess which phi is the right phi?","Creater_id":128938,"Start_date":"2016-08-25 08:31:31","Question_id":231703,"Tags":["generalized-linear-model","autocorrelation","autoregressive","white-noise","neuroimaging"],"Answer_count":0,"Last_activity":"2016-08-25 08:37:52","Link":"http://stats.stackexchange.com/questions/231703/whats-the-meaning-of-the-expansion-coefficient-of-the-ar-model","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d337b"},"View_count":28,"Display_name":"Tisdale","Question_score":1,"Question_content":"In a bag there are m type of items, with each type having n items. I can pick items from the bag from 1 to total m*n items. What is the total possible number of ways to do this? I know in case a single item type in the bag we can do it in 2^n-1 ways, but I'm unable to extend it for the more general case of m type of items. Any guidance in the right direction would be appreciated. Thanks for your help. Edit: For example, let the bag have 4 blue items, 4 green items and 4 red items. This corresponds to the car when m=3,n=4. So the bag has total of 3*4=12 items. One can pick items of the bag without replacement. So I can pick 1 item or 2 items and hence forth till 12 items. Let's demote the ways to pick 1 item from this bag as P(1,3,4). Need to find the sum \\sum P(i, 3,4) as i goes from 1 to 12.","Creater_id":128912,"Start_date":"2016-08-25 04:06:29","Question_id":231652,"Tags":["binomial","combinatorics"],"Answer_count":0,"Last_activity":"2016-08-25 08:36:28","Link":"http://stats.stackexchange.com/questions/231652/total-number-of-ways-to-choose","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d337d"},"View_count":34,"Display_name":"jcod0","Question_score":1,"Question_content":"Consider that I am given a set of  integers , which can take any value in the interval , where  is a positive integer, how can I find the number of combinations so that the sum of these  integers is equal to .As an example, for 4 integers bounded as , the number of unordered (i.e. sequence does not matter) combinations so that  is 3, given as (4,4,4,1), (4,4,3,2), (4,3,3,3).It can be assumed that  is choosen such that there exist an assignment of , such that  is always true.","Creater_id":105572,"Start_date":"2016-02-18 01:24:01","Question_id":197181,"Tags":["combinatorics"],"Answer_count":1,"Last_activity":"2016-08-25 08:20:23","Link":"http://stats.stackexchange.com/questions/197181/given-a-set-of-bounded-integers-find-number-combinations-which-sums-to-s","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d338a"},"View_count":29,"Display_name":"Bavo DC","Question_score":1,"Question_content":"In a study on intra-rater agreement with 8 raters, I have computed Cohen's Kappa for each of the raters and I estimated the standard error of each of the kappa values using the Jackknife method.To get an estimate of the overall intra-rater agreement, I took the mean of all kappa values. So far so good, but what is the correct way to combine the standard errors? I want to combine these standard errors, so I can compute a 95% CI for the mean kappa.I first thought that I could combine the standard errors by using the formula for pooled variance (wikipedia:pooled variance, function in R), with the kappa values as the means, the squared standard errors as the variances and the number of observations as N. However, I am not sure whether this is entirely correct. Can I replace the mean in the formula by the estimated kappa? If this is incorrect, can someone explain it to me from a mathematical point of view and a statistical point of view? Furthermore, I got upper confidence limits \u003e 1 and hence, a value that is impossible for kappa.","Creater_id":108169,"Start_date":"2016-03-10 10:50:32","Question_id":201025,"Tags":["standard-error","reliability","kappa"],"Answer_count":1,"Last_activity":"2016-08-25 08:18:53","Link":"http://stats.stackexchange.com/questions/201025/combining-the-standard-errors-of-cohens-kappa-computed-for-multiple-rater-pairs","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d3397"},"View_count":36,"Display_name":"jim","Question_score":0,"Question_content":"This is probably just a basic CS question, so I appreciate the help. I'm doing Monte Carlo permutation tests in matlab for One-Sample and Paired Two-Sample Tests (where there are x^n unique vectors for n observations of x possible classes) and it is taking a very long time to generate all of the unique vectors.For very large numbers of samples, the time it takes to generate that many unique vectors increases dramatically as the number of samples increases (for two reasons, one is the reduced ratio of unsample:sampled unique permutations and the second is the need to compare each new permutation to the previous permutations to see if it’s unique). I could speed things up immensely if I could pre-generate every possible subset and then just sample from the indices of those vectors, but obviously the size of the matrix of all possible vectors is too large. Is there a way to turn an index into what would be the corresponding vector? It seems like there must be. I.e. in the binary case (x = 2), if each vector has 10 observations, there are 2^10 unique binary vectors. The random number generator delivers 512, so the program calculates the unique binary vector that corresponds to index 512 and returns that. That way I could just pull the desired number of indices from [1,2^10] and calculate the corresponding unique binary vector for each.Eventually I’ll need to take that to the multiclass case (x\u003e3) and for unpaired two-sample tests (where there are n! possible combinations of the observations), but the binary subset case seems like the easiest place to start.Thought on how to do that? Again, thanks for the help.","Creater_id":128879,"Start_date":"2016-08-24 22:44:04","Question_id":231607,"Tags":["t-test","matlab","monte-carlo","combinatorics","permutation-test"],"Answer_count":1,"Last_activity":"2016-08-25 08:06:17","Link":"http://stats.stackexchange.com/questions/231607/how-to-efficiently-sample-a-large-number-of-unique-binary-vectors","Creator_reputation":3}
{"_id":{"$oid":"5837a574a05283111e4d33a4"},"View_count":23,"Display_name":"user3266595","Question_score":1,"Question_content":"I have a dataset consisting \"Name of Power plant\", \"Year\", \"Status of compontent \",  \"Status of compontent \", \"Availability of power plant per year [%]\", \"Number of times the plant had interruptions\" and \"Number of hours with interuptions\".The status of each component is dichotomous, where 0 indicate status \"bad\" and 1 indicate \"good\". To consider the overall status of a Power plant the mean status of the  Components is used. This gives  levels of the overall status, .I am interested to test if it is possible to see a correlation between the status of the plant and each of the three measures \"availability\", \"Number of interruptions\" and \"Hours with interruptions\".The test that I have done now is Kruskal-Wallis test to see if the distribution measure in each level of status is the same. And if they are the same we know that the mean and the variance is the same and then we can conclude that is it not possible to se differences of the measure in each level of status. In the case where the test shows that the distributions are not the same I have done a Spearman rho test to see find the correlation and if it is positive or negative.Now my question is which test to use and if my way of testing with Kruskal-Wallis and Spearman rho is valid?","Creater_id":128946,"Start_date":"2016-08-25 08:05:18","Question_id":231699,"Tags":["correlation","kruskal-wallis","spearman"],"Answer_count":0,"Last_activity":"2016-08-25 08:05:18","Link":"http://stats.stackexchange.com/questions/231699/help-with-choosing-statistical-test","Creator_reputation":6}
{"_id":{"$oid":"5837a574a05283111e4d33a6"},"View_count":53,"Display_name":"Avirudh Theraja","Question_score":0,"Question_content":"Hi so I'm trying to implement binary classification of scientific publications from various journals such as bioinformatics, nature etc. The goal is to classify each publication as either a software tool or a non tool. A software tool is defined as any publication which has some open source code/implementation available anywhere online and hence has a link for that in the publication text.My training set is around 60 tools and 170 non tools consisting of the entire text of the publications (from various journals). What is the best approach to solve this problem and obtain around ~90% accuracy? Here's what I've tried so far using scikit-learn:Used TFIDFVectorizer on entire text and then SVM with grid search to find optimal parameters. Couldn't achieve more than 75-80% accuracy.Only used sentences which contained urls (for both training and testing) and used tf-idf with LSA(truncated svd). Used SVM, SGDC classifiers but again same accuracy as above on average.The main problem I'm facing is that after many different approaches most tools still get misclassified as non tools. Many publications themselves make use of some software tools but are still not software tools themselves. So I decided to focus on the urls rather than the entire text but am still not getting the accuracy I want.Any suggestions/approach on the problem would be greatly appreciated.","Creater_id":128855,"Start_date":"2016-08-24 15:53:00","Question_id":231561,"Tags":["machine-learning","classification","svm","text-mining","scikit-learn"],"Answer_count":1,"Last_activity":"2016-08-25 07:55:26","Link":"http://stats.stackexchange.com/questions/231561/classification-of-scientific-publications-as-software-tools","Creator_reputation":1}
{"_id":{"$oid":"5837a574a05283111e4d33b3"},"View_count":26,"Display_name":"tomka","Question_score":0,"Question_content":"I consider a proportional odds (ordinal regression) model, as described in Agresti (2002) and estimated by R base package function MASS. The software reports t-values but no p-values in summary. I wonder how I should determine the degrees of freedom for the t-test. In simple linear regression  where  the sample size. Is this also the case for ordinal linear regression?","Creater_id":24515,"Start_date":"2016-08-25 07:01:56","Question_id":231685,"Tags":["regression","ordinal","degrees-of-freedom","polr"],"Answer_count":0,"Last_activity":"2016-08-25 07:45:16","Link":"http://stats.stackexchange.com/questions/231685/how-should-i-determine-degrees-of-freedom-for-t-test-in-ordinal-regression","Creator_reputation":1592}
{"_id":{"$oid":"5837a574a05283111e4d33b5"},"View_count":1270,"Display_name":"emeryville","Question_score":2,"Question_content":"I always struggle to get the true essence of the incidental parameter problem. I read in several occasions that the fixed effects estimators of nonlinear panel data models can be severely biased because of the \"well-known\" incidental parameter problem. When I ask for a clear explanation of this problem the typical answer is: Assume that the panel data has N individuals over T time periods. If T is fixed, as N grows the covariate estimates become biased. This occurs because the number of nuisance parameters grow quickly as N increases. I would greatly appreciatea more precise but still simple explanation (if possible)and/or a concrete example that I can work out with R or Stata.","Creater_id":90521,"Start_date":"2015-12-09 18:05:20","Question_id":185998,"Tags":["nonlinear-regression","fixed-effects-model","bias"],"Answer_count":1,"Last_activity":"2016-08-25 07:35:07","Link":"http://stats.stackexchange.com/questions/185998/incidental-parameter-problem","Creator_reputation":199}
{"_id":{"$oid":"5837a574a05283111e4d33c1"},"View_count":69,"Display_name":"Toney Shields","Question_score":4,"Question_content":"Suppose we have  and they are dependent. Does the joint distribution  have a closed form?Edit: let's take as an example a random graph, what's the joint distribution of the degrees of an ER random graph?","Creater_id":109437,"Start_date":"2016-08-25 07:12:16","Question_id":231688,"Tags":["joint-distribution"],"Answer_count":1,"Last_activity":"2016-08-25 07:21:15","Link":"http://stats.stackexchange.com/questions/231688/joint-distribution-of-dependent-binomial-random-variables","Creator_reputation":192}
{"_id":{"$oid":"5837a574a05283111e4d33ce"},"View_count":4072,"Display_name":"jenn wan","Question_score":2,"Question_content":"I am doing my thesis and have absolutely no previous experience in statistics. I have constructed several Likert scales by forming composites scores each based on  4-6 items which tests the level of agreement of my respondents. Specifically, I have created two scales 'purchase behaviour' and 'website appeal'. I want to see whether 'website appeal' is correlated with 'purchase behavior'.Now that I am doing my analysis, I am confused as to whether I should use Spearman rho or multiple regression. The Spearman correlation and multiple regression have different p-values, so much so that one states that I need to reject my hypothesis and another accept. So in this case should I use Spearman's rho or multiple regression?Is there a theoretical rule that I must use, say, multiple regression because I am testing 4-6 items on the likert, although I have grouped them together and intend to 'view' them as two single variables.Thanks, Jeromy. The article by Gelman and Stern (2006) is really interesting! Being the non-statistician me and trying to get on with my MBA thesis, I would be very tempted to find an analysis which gives me a simple method to analyse my data and ultimately, test my hypotheses. I know this shouldn't be the way, but stats aren't exactly fun nor interesting. I was talking to my supervisor and he suggested using regression when I had planned to use Spearman (cos Likert scale items are ordinal and if I want to test ordinal vs ordinal, I use Spearman - according to my research methods text)Yes, I am planning to test only two variables (predictors?) at a time, so, Spearman can technically be used. But these two variables (both dependent and independent) are computed as a new variable from the different items I have on the Likert scale (does this make sense?). I am just concerned that my analysis would be deemed incorrect if I used the 'wrong' statistical analysis - or doesn't this matter?","Creater_id":10918,"Start_date":"2012-04-26 17:57:07","Question_id":27205,"Tags":["regression","likert","spearman"],"Answer_count":1,"Last_activity":"2016-08-25 07:20:20","Link":"http://stats.stackexchange.com/questions/27205/whether-to-use-spearmans-rho-or-multiple-regression-to-examine-relationship-bet","Creator_reputation":11}
{"_id":{"$oid":"5837a574a05283111e4d33db"},"View_count":50,"Display_name":"S. Ming","Question_score":0,"Question_content":"Why does  give exactly the conditional expectation of  for a given  value? I don't understand how we can know the average value of  for a given  value. In my drawing I circled an obervation (x). How do we know now, given only this oberservation, that the average of  given  is on the line? Wouldn't we we need more obervations for this given  value to conclude where the average is? I'm quite confused, I hope you understand what I mean.","Creater_id":128181,"Start_date":"2016-08-25 07:06:17","Question_id":231686,"Tags":["regression"],"Answer_count":1,"Last_activity":"2016-08-25 07:15:59","Link":"http://stats.stackexchange.com/questions/231686/how-do-we-know-the-expected-value-of-y-given-x-population-regression-function","Creator_reputation":46}
{"_id":{"$oid":"5837a574a05283111e4d33e8"},"View_count":82,"Display_name":"dorien","Question_score":0,"Question_content":"I seem to remember from my graduate statistics course that if higher order variables (i.e., X^2, X^3, etc) are significant in a polynomial regression analysis such as our quadratic regression, then the relationship between the DV and IVs is considered to be the highest order variable. In other words, when I do a regression in the format of X + X^2, and both the linear (X) and quadratic (X^2) components of the analysis are significant, we report the relationship as quadratic? Both the X and X^2 predictors are significant in the modelbut X is more significant is it still considered to be a quadratic relationship? Note also that a simple linear regression has a lower R^2 than the quadratic regression.","Creater_id":16912,"Start_date":"2016-08-25 03:53:52","Question_id":231649,"Tags":["regression","linear-model","terminology","polynomial"],"Answer_count":2,"Last_activity":"2016-08-25 06:51:31","Link":"http://stats.stackexchange.com/questions/231649/when-to-report-quadratic-versus-linear-relationships","Creator_reputation":187}
{"_id":{"$oid":"5837a574a05283111e4d33f6"},"View_count":4803,"Display_name":"Fomite","Question_score":19,"Question_content":"So this is an odd fit, though really I think it's an odd fit for any site, so I thought I'd try it here, among my data-crunching brethren.I came to epidemiology and biostatistics from biology, and still definitely have some habits from that field. One of them is keeping a lab notebook. It's useful for documenting thoughts, decisions, musings about the analysis, etc. All in one place, everything committed so I can look back on analysis later and have some clue what I did.But it would be nice to move that into the 21st century. Particularly because even though the lab notebook system is decent enough for one person and documenting decisions, it would be nice to be able to attach plots from EDA, emails from data managers discussing a particular data set, etc.I'm guessing this will involve rigging up my own system from an unholy union of many different bits, but is anyone currently using a system and have any recommendations?","Creater_id":5836,"Start_date":"2011-10-11 23:38:39","Question_id":16889,"Tags":["references","software","eda"],"Answer_count":8,"Last_activity":"2016-08-25 06:37:33","Link":"http://stats.stackexchange.com/questions/16889/ideas-for-lab-notebook-software","Creator_reputation":14344}
{"_id":{"$oid":"5837a574a05283111e4d340a"},"View_count":74,"Display_name":"L_T","Question_score":0,"Question_content":"I wonder which is the best way to plot in r the results from the lme function, in presence of a significant interaction. The model has two interacting fixed effects and a random effect on subjects to manage repeated measures. The analysis is from an experiment where 19 participants had to adjust a parameter of the sound (Centroid), and I want to assess whether there is a relationship with their height and weight. There were 12 stimuli repeated twice for a total of 24 trials.Here is the output of my analysis:\u0026gt; library(nlme)\u0026gt; lme_Centroid \u0026lt;- lme(Centroid ~ Weight*Height, data = scrd, random = ~1 | Subject)\u0026gt; \u0026gt; summary(lme_Centroid)Linear mixed-effects model fit by REML Data: scrd        AIC      BIC    logLik  25809.38 25840.69 -12898.69Random effects: Formula: ~1 | Subject        (Intercept) ResidualStdDev:    398.9658  3027.67Fixed effects: Centroid ~ Weight * Height                    Value Std.Error   DF   t-value p-value(Intercept)   -20232.203  9101.096 1349 -2.223051  0.0264Weight           478.854   152.184   15  3.146536  0.0067Height           140.440    52.194   15  2.690751  0.0168Weight:Height     -2.725     0.838   15 -3.253770  0.0053 Correlation:               (Intr) Weight HeightWeight        -0.927              Height        -0.994  0.886       Weight:Height  0.951 -0.996 -0.919Standardized Within-Group Residuals:       Min         Q1        Med         Q3        Max -1.5059828 -0.8664208 -0.2111113  0.7098706  2.3620633 Number of Observations: 1368Number of Groups: 19 I do not know how to represent in R these results. I tried xyplot(Centroid ~  Weight * Height, type = c(\"p\",\"r\"), data = scrd) but I guess it is wrong.","Creater_id":4701,"Start_date":"2015-11-22 05:07:09","Question_id":182977,"Tags":["r","data-visualization","lme"],"Answer_count":0,"Last_activity":"2016-08-25 06:06:55","Link":"http://stats.stackexchange.com/questions/182977/how-to-plot-in-r-results-from-linear-mixed-effects-model-with-significant-intera","Creator_reputation":528}
{"_id":{"$oid":"5837a574a05283111e4d340c"},"View_count":35,"Display_name":"Paula","Question_score":0,"Question_content":"I'm trying to make a \"triplot\" to illustrate Bayesian inference (so I'd like to have prior, likelihood and posterior in the same picture). For likelihood I'm using \\begin{equation}\\label{eq:lik}f(y|\\tau)\u0009=\u0009\\prod_{i=1}^{n}\\frac{\\tau}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\tau(y_{i}-\\mu)^{2}}{2}\\right)\u0009=\u0009\\frac{\\tau}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\tau\\sum_{i=1}^{n}(y_{i}-\\mu)^{2}}{2}\\right),\\end{equation}i.e., the Gaussian distribution with known mean  and unknown precision , where  is the unknown variance.If we choose the prior on  to be a gamma distribution\\begin{equation}p(\\tau)=\\Gamma(\\alpha,\\beta),\\end{equation}with the shape  and the rate , we can use the conjugacy theory to find the form of the posterior.The posterior distribution in our example is the following gamma distribution\\begin{equation}p(\\tau|y)=\\Gamma\\left(\\alpha+\\frac{n}{2},\\beta+\\frac{\\sum_{i=1}^{n}(y_{i}-\\mu)^{2}}{2}\\right).\\end{equation}I plotted it with , ,   and the random sample  was generated assuming the \"true\" value of .As you can see, the likelihood is not visible. I tried different configurations of parameters but nothing helped.I'd be very grateful for any ideas how to choose  and  so that I get a nice illustration, something like that.","Creater_id":126827,"Start_date":"2016-08-24 02:41:13","Question_id":231438,"Tags":["bayesian","data-visualization","likelihood","parameterization","conjugate-prior"],"Answer_count":1,"Last_activity":"2016-08-25 06:06:45","Link":"http://stats.stackexchange.com/questions/231438/bayesian-inference-with-conjugate-priors-triplot","Creator_reputation":38}
{"_id":{"$oid":"5837a574a05283111e4d3419"},"View_count":14603,"Display_name":"Sara","Question_score":15,"Question_content":"I understand that when sampling from a finite population and our sample size is more than 5% of the population, we need to a correction on the sample's mean and standard error using this formula:\rWhere N is the population size and n is the sample size.I have 3 questions about this formula:Why is the threshold set at 5%?How was the formula derived?Are there other online resources that comprehensively explain this formula besides this paper?Thank you,","Creater_id":1636,"Start_date":"2010-12-05 02:40:51","Question_id":5158,"Tags":["sampling","finite-population"],"Answer_count":1,"Last_activity":"2016-08-25 05:54:26","Link":"http://stats.stackexchange.com/questions/5158/explanation-of-finite-correction-factor","Creator_reputation":502}
{"_id":{"$oid":"5837a574a05283111e4d3426"},"View_count":73,"Display_name":"Tonja","Question_score":0,"Question_content":"Logistic regression can be penalized with L2 or L1 to avoid overfilling and/or select variables. The idea is to maximized the likelihood. Accordingly, the total quality formula is following:total quality = measure of fit (likelihood of the data) - measure of the magnitude of the coefficients (L2 penalty).Why do we substract the measure of the magnitude and do not add it?","Creater_id":82889,"Start_date":"2016-08-25 05:15:16","Question_id":231664,"Tags":["regression"],"Answer_count":2,"Last_activity":"2016-08-25 05:49:36","Link":"http://stats.stackexchange.com/questions/231664/l2-or-l1-penalty-maximizing-likelihood","Creator_reputation":105}
{"_id":{"$oid":"5837a574a05283111e4d3434"},"View_count":43,"Display_name":"Mattia","Question_score":1,"Question_content":"In a typical regression problem there is usually no need for normalizing/rescaling the labels (targets) before performing the optimization.In deep regression networks there would be in principle no need to rescale since the last activation is linear and the cost function is the euclidean distance of the prediction from the targets. However, for numerical stability and performance of the training process, the idea is to keep the values of the input and hidden units in the range [-1,1]. Does it mean that also the target of the regression should be rescaled to the [-1,1] range?","Creater_id":52609,"Start_date":"2016-08-25 03:18:13","Question_id":231642,"Tags":["regression","neural-networks","deep-learning","multilabel"],"Answer_count":0,"Last_activity":"2016-08-25 05:48:55","Link":"http://stats.stackexchange.com/questions/231642/label-normalization-in-deep-regression-networks","Creator_reputation":106}
{"_id":{"$oid":"5837a574a05283111e4d3436"},"View_count":46,"Display_name":"Dataman","Question_score":1,"Question_content":"I am reading an article in Systems Biology where the authors use the term background model which I guess refers to something related to statistics in general. However, my Googlesearch did not result in any significant finding of what the term means!Here is the context (Drake et al., 2016):  The input sets were found to be significantly close (p \u0026lt; 0.012) in a pathway space according to a conservative background model generated by 1,000 permutations of the data (Paull et al., 2013), where each input set (kinase regulators, transcriptional regulators, genomic alterations) was swapped with genes of similar network connectivity while the other two were fixed.Here is another context for the term (Systems Biology: A Textbook, 2016) which I found by searching. However, still I cannot wrap my mind around this notion:  To test if a pattern is significantly abundant in a network, the network is compared with a random graph that serves as background model.It would be great to define this notion in plain English and/or with a help of an example.","Creater_id":49251,"Start_date":"2016-08-23 03:38:30","Question_id":231248,"Tags":["modeling","model"],"Answer_count":1,"Last_activity":"2016-08-25 05:45:30","Link":"http://stats.stackexchange.com/questions/231248/what-does-the-term-background-model-mean-in-statistics","Creator_reputation":127}
{"_id":{"$oid":"5837a574a05283111e4d3443"},"View_count":494,"Display_name":"overwhelmed","Question_score":5,"Question_content":"I am working on zero-inflated count data models using the pscl package. I am just wondering why there is no development of models for one-inflated count data models! Also why there is no development of bimodal, say zero-and-2-inflated, count data models! Once I generated one-inflated Poisson data and found that neither the glm with family=poisson model nor the negative binomial (glm.nb) model was good enough to fit the data well. If any one can shed some light on my thought, eccentric though it might be, it would be very helpful for me.","Creater_id":42238,"Start_date":"2014-03-20 23:55:50","Question_id":90817,"Tags":["r","generalized-linear-model","zero-inflation","poisson-regression"],"Answer_count":2,"Last_activity":"2016-08-25 05:15:34","Link":"http://stats.stackexchange.com/questions/90817/why-are-there-no-one-inflated-count-data-models","Creator_reputation":141}
{"_id":{"$oid":"5837a574a05283111e4d3450"},"View_count":47,"Display_name":"mousecoder","Question_score":1,"Question_content":"I wish to do topic modeling on text corpus some of which are about company earnings which has lots of numbers in it. It has no sentence structure. I think tagging numbers using nltk.pos_tagging can help me find out if the number is CD (numeral/cardinal). Using numeral feature as one of the many features in the BOW can help me identify the topic related to Statistics or Maths. Intuitively if I drop this information about numbers appearing in the text, I will not be able to figure out the topic of unseen texts.I will really appreciate your views regarding this. Also, if anyone knows about a BOW library in python that can do so internally that will be helpful. I came across scikit-learn library's count vectorizer in which  I think if I change token_pattern, it might help me in getting numbers as features but that will not help me in collapsing all numeric features as one feature. I hope I am clear. Really appreciate your time and help.Thanks.","Creater_id":41146,"Start_date":"2014-06-17 12:45:21","Question_id":103747,"Tags":["machine-learning","text-mining","natural-language","bag-of-words"],"Answer_count":0,"Last_activity":"2016-08-25 04:26:57","Link":"http://stats.stackexchange.com/questions/103747/treating-numerals-cardinals-in-bag-of-words-bow-model","Creator_reputation":166}
{"_id":{"$oid":"5837a574a05283111e4d3452"},"View_count":284,"Display_name":"xjackx","Question_score":0,"Question_content":"Hi I am trying to use a stacked LSTM architecture in keras, similar to what is shown here https://keras.io/getting-started/sequential-model-guide/. My problem is formulated as a binary time series classification, my timesteps are 2 and I have 7 attributes. The shape of my data is as follows:-X_train_smote_reshaped (1256L, 2L, 7L)y_train_smote_reshaped (1256L, 2L)X_validation_std_reshaped (168L, 2L, 7L)y_validation_reshaped (168L, 2L)The error I get is:Exception: Error when checking model target: expected dense_1418 to have shape (None, 1) but got array with shape (1256L, 2L)My keras code is listed below:from keras.layers import LSTMX_train_smote_reshaped=np.array([X_train_smote_std[i:i+2] for i in     range(len(X_train_smote_std)-2)])y_train_smote_reshaped=np.array([y_train_smote[i:i+2] for i in range(len(y_train_smote)-2)])X_validation_std_reshaped=np.array([X_validation_std[i:i+2] for i in range(len(X_validation_std)-2)])y_validation_reshaped=np.array([y_validation[i:i+2] for i in range(len(y_validation)-2)])data_dim=7timesteps=2#1.define the networkmodel=Sequential()model.add(LSTM(20,return_sequences=True,input_shape=(timesteps,data_dim)))model.add(LSTM(20))#one neuron in the output layer with a sigmoid activation functionmodel.add(Dense(1,activation='sigmoid'))#2. compile the networkmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])#3. fit the modelmodel.fit(X_train_smote_reshaped,y_train_smote_reshaped,batch_size=64,nb_epoch=5,validation_data=(X_validation_std_reshaped,y_validation_reshaped))","Creater_id":123137,"Start_date":"2016-08-24 16:35:39","Question_id":231569,"Tags":["lstm","rnn","theano"],"Answer_count":1,"Last_activity":"2016-08-25 04:23:04","Link":"http://stats.stackexchange.com/questions/231569/stacked-lstm-for-sequence-classification-keras-error","Creator_reputation":6}
{"_id":{"$oid":"5837a575a05283111e4d345d"},"View_count":46,"Display_name":"FredrikAa","Question_score":0,"Question_content":"I have some questions regarding the statistical notion of \"population'', the mean, standard deviation and the distribution of a population characteristic.  Background:According to Wikipedia, ``A statistical population can be a group of actually existing objects\". Let us consider the distribution of heights of a population consisting of 100 people. Assume the population height has the normal distribution with mean  and standard deviation . Let us generate possible actual heights of the individuals in the population using R:n=100mu=150sigma=30set.seed(5)example_data=rnorm(n,mu,sigma)It is then possible to make a plot of the cdf:plot(ecdf(example_data))which gives the folowing plot:The theoretical cumulative distribution function is given in the following plotWe can calculate the populations mean and variance asandThe population standard deviation is then Questions:1) we see that the plot of the empirical cdf and the plot of the cdf are not exactly the same. They have similar shape but do not coincide at all points along the abscissa. How can we then say that the population height is distributed as the specified normal distribution?2) When we calculate the population mean and standard deviation, we do not get the true values 150 and 30. Why are then the population mean and variance defined as they are? Is it only in an infinite population the expression for the population mean, variance etc. give the true values? 3) According to the weak law of large numbers the sample mean converges to the true mean as the number of observations approaches infinity. How can then the expression for the population mean give the true value of the mean? 4) Can then a population consist of a finite amount of individuals or are finite sets of individuals just samples? ","Creater_id":101150,"Start_date":"2016-08-25 01:47:48","Question_id":231625,"Tags":["variance","mean","sample","population","ecdf"],"Answer_count":1,"Last_activity":"2016-08-25 03:48:27","Link":"http://stats.stackexchange.com/questions/231625/population-and-the-mean-standard-deviation-and-the-distribution-of-a-population","Creator_reputation":72}
{"_id":{"$oid":"5837a575a05283111e4d346a"},"View_count":47,"Display_name":"user93892","Question_score":1,"Question_content":"My question is similar to this question, but the solution provided didn't tell whether increasing the sample size influences the prediction interval, so I would like to ask again.The formulae for confidence interval:\\hat y \\pm t_{\\alpha/2, n-2} \\sqrt{MSE} \\sqrt{1/n + \\frac{(x-\\bar x)^2}{\\sum (x_i - \\bar x)^2}}and prediction interval:\\hat y \\pm t_{\\alpha/2, n-2} \\sqrt{MSE} \\sqrt{1 + 1/n + \\frac{(x-\\bar x)^2}{\\sum (x_i - \\bar x)^2}}If the sample size is increased, the standard error on the mean outcome given a new observation will decrease, then the confidence interval will become narrower. In my mind, at the same time, the prediction interval will also become narrower which is obvious from the fomular. However, my professor told me that the increasing sample size does not influence too much the prediction interval, so I am confused now. Could anybody give me some explanation?","Creater_id":93892,"Start_date":"2016-08-25 01:52:42","Question_id":231626,"Tags":["self-study","sample-size","prediction-interval"],"Answer_count":1,"Last_activity":"2016-08-25 03:29:34","Link":"http://stats.stackexchange.com/questions/231626/whether-increasing-the-sample-size-influences-the-prediction-interval","Creator_reputation":58}
{"_id":{"$oid":"5837a575a05283111e4d3477"},"View_count":58,"Display_name":"drj3122","Question_score":0,"Question_content":"Problem: Estimate coefficient of variation for the mean expenditures for a number of groups. For each group, I have a sample of totals and the counts of people. E.g., for group A, I have totals (people): 1000(200), 1100(205), 1113 (199), and so on. Based on the CLT, the means should be approximately normally distributed for large enough sample sizes. I can then use the estimator given here.My question is how should I handle the estimation for groups where the sample size is small? For example, I have some groups with only 4 data points. Would this be a good case for resampling techniques like bootstrapping?","Creater_id":102277,"Start_date":"2016-08-24 12:31:03","Question_id":231532,"Tags":["estimation","central-limit-theorem","resampling","coefficient-of-variation"],"Answer_count":1,"Last_activity":"2016-08-25 03:27:00","Link":"http://stats.stackexchange.com/questions/231532/estimating-coefficient-of-variation-with-small-sample-size","Creator_reputation":3}
{"_id":{"$oid":"5837a575a05283111e4d3484"},"View_count":86,"Display_name":"numX","Question_score":0,"Question_content":"Which is the best/most reliable representation of a model's predictive accuracy, the accuracy based on the n-fold cross validation or the model's accuracy on an unseen dataset?At a glance, I would definitely have thought that predicting the unseen dataset would be a better measure of predictive accuracy but on second thoughts it seems likely that this will be smaller than the training dataset and hence more prone to anomalies causing mis-representations in the model's predictive power.Any references in the answer would be greatly appreciated.EDIT: Describing current setup -With a dataset of 100k , I randomly selected 10% and removed these building the model on the remaining dataset. I used the 10 % to generate predictions and calculate the accuracy, which of course differs from the accuracy of predictions from cross validation. Which of these realistically represents the model's predictive accuracy more? Thanks","Creater_id":127280,"Start_date":"2016-08-22 14:37:25","Question_id":231171,"Tags":["machine-learning","cross-validation","predictive-models","accuracy"],"Answer_count":2,"Last_activity":"2016-08-25 03:17:53","Link":"http://stats.stackexchange.com/questions/231171/machine-learning-calculating-predictive-accuracy-cross-validation-vs-accuracy","Creator_reputation":101}
{"_id":{"$oid":"5837a575a05283111e4d3492"},"View_count":11,"Display_name":"GRS","Question_score":0,"Question_content":"  I'm fitting the model:  (Orthogonal polynomials)  I want to test if any of the explanatory variables can be omitted from the modelFirst of all I put the model into  where  Sorry for bad notation.I estimated Residual SS to be:  where  and  don't include  and I found the pure error SS to be Hence Lack of Fit error is Now I need to find Regression SS on  to test if I can remove them from the model.I will then conduct and F-test - SS/Lack of fit (subject to df) to find if they can be disallowed.Extra Question:I know that the RSS has  degrees of freedom. But how many does the lack of fit, and how many does the pure error have (I know they should add up to )? It seems that for Pure error, we only need to know what  and  are, so we need them all, 16  df.","Creater_id":109646,"Start_date":"2016-08-25 03:06:23","Question_id":231638,"Tags":["regression","anova","goodness-of-fit","f-test","sums-of-squares"],"Answer_count":0,"Last_activity":"2016-08-25 03:16:31","Link":"http://stats.stackexchange.com/questions/231638/how-to-find-regression-ss-on-x-1-x-2-x-3-conduct-f-test-to-see-if-variables-c","Creator_reputation":113}
{"_id":{"$oid":"5837a575a05283111e4d3494"},"View_count":20,"Display_name":"user3275222","Question_score":0,"Question_content":"I have a time series. I plotted it and saw that it is not stationary. Thus, I have calculated the difference. Then I plotted the autocorrelation and partial autocorrelation on the differences, along with the confidence bands. Non of the lags were out of the confidence bands, and the correlations for all lags are very low. I wanted to ask, does this mean that I cannot make good forecasts, or is there something else I could do. What should be the next stage? I would add and say that my series has no trend or seasonality (there is a trend down which at some point becomes a trend up, like a stock graph).","Creater_id":81477,"Start_date":"2016-08-25 03:07:16","Question_id":231639,"Tags":["time-series"],"Answer_count":0,"Last_activity":"2016-08-25 03:07:16","Link":"http://stats.stackexchange.com/questions/231639/time-series-with-no-autocorrelation","Creator_reputation":159}
{"_id":{"$oid":"5837a575a05283111e4d3496"},"View_count":32,"Display_name":"user6507246","Question_score":0,"Question_content":"For Gaussian (quadratic) form\\Delta^2 = ({\\bf x}-{\\pmb \\mu})^T{\\bf \\Sigma}^{-1}({\\bf x}-{\\pmb \\mu}) \\qquad{}where this equation mean and come from?and what is \\Delta^2 mean in the equation?After this they resolve the equation like this.-\\dfrac{1}{2}({\\bf x}-{\\pmb \\mu})^T\\Sigma^{-1}({\\bf x}-{\\pmb \\mu})=\\\\-\\dfrac{1}{2}({\\bf x}_a-{\\pmb \\mu}_a)^T\\Lambda_{aa}({\\bf x}_a-{\\pmb \\mu}_a) - \\dfrac{1}{2}({\\bf x}_a-{\\pmb \\mu}_a)^T\\Lambda_{ab}({\\bf x}_b-{\\pmb \\mu}_b)\\\\-\\dfrac{1}{2}({\\bf x}_b-{\\pmb \\mu}_b)^T\\Lambda_{ba}({\\bf x}_a-{\\pmb \\mu}_a) - \\dfrac{1}{2}({\\bf x}_b-{\\pmb \\mu}_b)^T\\Lambda_{bb}({\\bf x}_b-{\\pmb \\mu}_b) \\qquad{}can you explain how this equation derived?and Also How covariance form calculated in the equation. ","Creater_id":128678,"Start_date":"2016-08-25 02:08:09","Question_id":231629,"Tags":["machine-learning","normal-distribution","gaussian-process"],"Answer_count":1,"Last_activity":"2016-08-25 02:49:03","Link":"http://stats.stackexchange.com/questions/231629/gaussian-quadratic-equation","Creator_reputation":58}
{"_id":{"$oid":"5837a575a05283111e4d34a3"},"View_count":99,"Display_name":"Gordon Honerkamp-Smith","Question_score":2,"Question_content":"Background on Two-Stage DesignsIn clinical trials, we are often interested in the response rate  for an experimental treatment.  In a typical trial, we might expose  patients to the treatment and observe the total number of responses  in order to test the null hypothesis  against a one-sided alternative , where  could be the response rate for a standard-of-care treatment.  In order to limit patient exposure to an ineffective treatment, two-stage designs are popular: in the first stage,  patients receive treatment and the number of responses  is observed.  If there are  or fewer responses, the trial ends.  If , a second stage is carried out: additional patients are enrolled until a total of  patients have received treatment, and the total number of responses  is observed.  If there are more than  total responses, the null hypothesis is rejected; otherwise, the null is not rejected and the new treatment is considered ineffective (or at least not superior to the standard of care).  The parameters  and  are chosen by the investigator before carrying out the study.The ProblemWhile working on designing such a study, I have come across a phenomenon that I would like to better understand.  Intuitively it seems that increasing the cut-off  should decrease the type-1 error of the test.  One might reason that for larger values of , it is less likely that the study will proceed to the second stage, and thus less likely that one will ultimately reject the null.  However, it appears that this is not always the case.  For example, with the parameters  and , I found the type-1 error to be the same (up to 9 digits) for  as for  (I got ). In fact, I get the same number for all values of .  See my code below.My QuestionSo my question is, does it make sense that changing the value of  does not affect the type-1 error in some cases?  Can you help me refine my intuition to understand why this is true?CodeHere is a quick function I have written to compute the type-1 error (as well as the power, for a given alternative, and the expected sample size) for two-stage designs.##### Function to compute the type-1 error, power, and expected sample size for a##### two-stage design. The parameters p0 and p1 are the response rates under the##### null and alternative, respectively.twoStage = function(n, n1, r1, r, p0, p1){  # Type 1 error = P(reject H0 | p = p0)   #              = P(X1 \u0026gt; r1 \u0026amp;\u0026amp; X \u0026gt; r | p = p0)]  n2 = n - n1  x1 = (r1 + 1):n1  alpha = sum(dbinom(x1, n1, p0)*(1 - pbinom(r-x1, n2, p0)))  # Power = P(reject H0 | p = p1)   #       = P(X1 \u0026gt; r1 \u0026amp;\u0026amp; X \u0026gt; r | p = p1)]  # Same as above, but with p = p1  power = sum(dbinom(x1, n1, p1)*(1 - pbinom(r-x1, n2, p1)))  # Expected number of subjects enrolled under H0  expected.n = n1 + (1 - pbinom(r1, n1, p0))*n2  return(cbind(n = n, n1 = n1, r1 = r1, r = r, p0 = p0, p1 = p1, alpha = alpha,   power =   power, expected.n = expected.n))}##### Example:\u0026gt; twoStage(n=30, n1=20, r1=5, r=25, p0=0.6, p1=0.85)[,\"alpha\"]  alpha 0.001510074 \u0026gt; twoStage(n=30, n1=20, r1=10, r=25, p0=0.6, p1=0.85)[,\"alpha\"]  alpha 0.001510074","Creater_id":8411,"Start_date":"2014-06-20 16:32:06","Question_id":104201,"Tags":["experiment-design","biostatistics","clinical-trials"],"Answer_count":1,"Last_activity":"2016-08-25 02:47:17","Link":"http://stats.stackexchange.com/questions/104201/type-1-error-of-the-two-stage-design","Creator_reputation":13}
{"_id":{"$oid":"5837a575a05283111e4d34b0"},"View_count":56,"Display_name":"Guillaume","Question_score":0,"Question_content":"I want to run a feature selection study to select only the most important features, before running a machine learning classification. My data is 30,000 x 17 (Observed objects x Features). I use the R implementation of Boruta, with default parameters.My results is: all my 17 features are green (confirmed as \"important\"). It is suspicious because it is likely that some are not and should be dropped.When I only use a sub-set of observations (eg 100 randomly chosen observations among 30,000), the Boruta algo then changes drastically: 6 features are red (unimportant) and 11 are green (important).Why do I have such different results, is it overfitting? How should I perform to make sure I correctly identify the less and most relevant features among the initial set of 17?","Creater_id":128900,"Start_date":"2016-08-25 01:34:14","Question_id":231623,"Tags":["machine-learning","feature-selection","overfitting"],"Answer_count":1,"Last_activity":"2016-08-25 02:33:42","Link":"http://stats.stackexchange.com/questions/231623/features-selection-why-does-boruta-confirms-all-my-features-as-important","Creator_reputation":1}
{"_id":{"$oid":"5837a575a05283111e4d34bd"},"View_count":84,"Display_name":"Chirayu Chamoli","Question_score":1,"Question_content":"Background: I have a data set that has lot of missing values so for each observation i would like to give a score on how many field's the person has input, my hypothesis is that higher the no of fields he inputs, higher is his chance of getting converted.I'm trying to build a scoring model. I have around 10 variables in my dataset. My question is: if for an observation/record, let's say I have 4 missing values, so can I use the remaining 6 for calculating the score and input that as a feature in my model. Would this be redundant?","Creater_id":100552,"Start_date":"2016-08-17 05:19:50","Question_id":230260,"Tags":["r","classification","feature-selection"],"Answer_count":1,"Last_activity":"2016-08-25 02:24:36","Link":"http://stats.stackexchange.com/questions/230260/can-i-use-the-variable-given-in-my-dataset-to-create-a-new-variable","Creator_reputation":150}
{"_id":{"$oid":"5837a575a05283111e4d34ca"},"View_count":42,"Display_name":"Keith Hughitt","Question_score":5,"Question_content":"Does anyone know of a good way to visualize differences between alternative clusterings?I know there are some measures which can be used to quantify the similarity between two partitionings, e.g.:Jaccard IndexAdjusted Rand IndexWeighted KappaEach of these can provide me with a single number in the range 0-1 or so (although there may negative values with the later two methods).This is useful, but doesn't help to build an intuition about how the clusterings differ from one another -- for instance, are the two clusterings grouping elements completely differently? Or are they just dividing things up at difference scales?For small hierarchical clustering comparisons, there is an R package dendextend which can be used to compare alternative dendrograms.Are there any other methods that can be used that aren't specific to hierarchical clustering, and would work well for larger datasets? (e.g. 10,000 elements).","Creater_id":18331,"Start_date":"2016-08-19 12:33:37","Question_id":230775,"Tags":["clustering","data-visualization","partitioning","method-comparison"],"Answer_count":1,"Last_activity":"2016-08-25 01:56:11","Link":"http://stats.stackexchange.com/questions/230775/visualizing-differences-between-alternative-clusterings","Creator_reputation":131}
{"_id":{"$oid":"5837a575a05283111e4d34d6"},"View_count":24,"Display_name":"Marcel10","Question_score":1,"Question_content":"I am doing a time to event analysis in R. My problem is as follows:A customer signs a contract at a company for one year and is able to cancel the contract at any time he wants. I am interested in the probability that a customer cancels the contract by himself (so before the automatic cancellation after one year.)The definition of the 'event' in this case is the cancellation due the client himself. I know that after 365 days (366 for a leap year) the observation is censored. This means that the requirement of noninformative censoring is not met (right?). How can I account for censoring with a fixed end-time in a Cox model?J.P. Klein and M.L. Moescherger call this type of censoring progressive Type I Censoring in their book Survival Analysis: Techniques for Censored and Truncated Data, however Google provides mostly information about progressive type II (not I) censoring. ","Creater_id":128538,"Start_date":"2016-08-25 01:40:48","Question_id":231624,"Tags":["survival","cox-model","censoring"],"Answer_count":0,"Last_activity":"2016-08-25 01:40:48","Link":"http://stats.stackexchange.com/questions/231624/how-to-account-for-progressive-right-censoring-in-a-cox-model","Creator_reputation":158}
{"_id":{"$oid":"5837a575a05283111e4d34d8"},"View_count":1109,"Display_name":"DJack","Question_score":3,"Question_content":"I have a serie of objects for whom I know the probability of belonging to 10 classes. This probability can be null (see example below with 4 classes).    A    B    C    D1  0.4  0.0  0.2  0.42  0.1  0.3  0.4  0.23  0.0  0.0  0.0  1.0In order to get for each object an information about the quality of the classification, I wanted to calculate the Shannon's entropy but it does not work when one of the classes has a probability equal to zero (log(0)=-Inf).My question: Is there a parameter similar with Shannon's entropy (or an adaptation) which can manage probability equal to zero ?","Creater_id":24771,"Start_date":"2013-04-24 05:57:24","Question_id":57069,"Tags":["classification","entropy"],"Answer_count":2,"Last_activity":"2016-08-25 00:54:03","Link":"http://stats.stackexchange.com/questions/57069/alternative-to-shannons-entropy-when-probability-equal-to-zero","Creator_reputation":140}
{"_id":{"$oid":"5837a575a05283111e4d34e5"},"View_count":20,"Display_name":"Shubhangi Shresth","Question_score":1,"Question_content":"So, it is said that whatever is in fashion once comes back someday or the other. We have collected data on 10 different garment types, each having 5 different sub-types on what was in fashion in 50 years. (Eg- Jeans-\u003eWaist-\u003eHigh/Low/Mid-rise)We plan on assigning each sub-type a grade/number to help with the analysis.What kind of statistical analysis should we do on this time series to look for cyclic/seasonal/random trends? What software should we use?","Creater_id":128896,"Start_date":"2016-08-25 00:51:40","Question_id":231619,"Tags":["regression","time-series","trend"],"Answer_count":0,"Last_activity":"2016-08-25 00:51:40","Link":"http://stats.stackexchange.com/questions/231619/what-tests-and-analysis-for-50-year-data-collected-on-fashion-trends","Creator_reputation":6}
{"_id":{"$oid":"5837a575a05283111e4d34e7"},"View_count":24,"Display_name":"LoulouChameau","Question_score":0,"Question_content":"Ok so I am building a text classifier with approx. 20000 entries and 20 categories. I have heard several times that using a bigram representation of my input instead of the classic \"Bag of Words\" representation could potentially boost my classifier's accuracy.Unfortunately it was not the case for me, so now I wonder : When is it a good idea to make that change? Any insight on what kind of classification tasks actually benefits from this?Thanks.","Creater_id":127872,"Start_date":"2016-08-24 01:30:37","Question_id":231427,"Tags":["machine-learning","classification","feature-selection","text-mining"],"Answer_count":1,"Last_activity":"2016-08-25 00:43:30","Link":"http://stats.stackexchange.com/questions/231427/changing-classifier-input-features-from-unigrams-to-bigrams","Creator_reputation":90}
{"_id":{"$oid":"5837a575a05283111e4d34f4"},"View_count":245,"Display_name":"Jonas","Question_score":3,"Question_content":"In the engineering context several data sources like different kinds of measurement signals (for example distances, angles and efficiencies) are very common. If it would be possible to observe these data for instance in unsupervised functional data clustering like [1] it would be a great effort for me.If I would observe only one parameter the solution might me very clear for me (like the cited paper). But what if we consider several different signals? How would you include these in functional data clustering?I'm very interested at your ideas.[1] Jacques, J. \u0026amp; Preda, C.: Functional Data Clustering. A Survey. In: Advances in Data Analysis and Classification 8(3), S. 231-255. DOI:10.1007/s11634-013-0158-y","Creater_id":115304,"Start_date":"2016-05-11 07:23:49","Question_id":212037,"Tags":["machine-learning","clustering","unsupervised-learning","functional-data-analysis"],"Answer_count":1,"Last_activity":"2016-08-25 00:27:33","Link":"http://stats.stackexchange.com/questions/212037/how-to-consider-different-samples-in-functional-data-clustering","Creator_reputation":16}
{"_id":{"$oid":"5837a575a05283111e4d3501"},"View_count":50,"Display_name":"achompas","Question_score":6,"Question_content":"Here's a problem I'm currently working on, as well as the empirical Bayesian approach I'm using. I'd like to make sure my approach is grounded in solid statistical theory.I have a set of entities , as well as arrival counts at different time periods  for each entity , denoted by . Here is a histogram of these arrival counts for all entities across all time periods in my data set.The pink line is . Note that there are no entries in the first bin -- this is because my dataset omits all entities whose arrival counts fall below a certain threshold (for simplicity let's say 3000 arrivals). The median for this data also falls around 5000.I am interested in identifying entities in this dataset whose recent arrival counts are accelerating rapidly within a recent time window. Here is an example of an entity whose arrival counts have accelerated, peaked, and subsequently dropped off.For this chart, I'd want to highlight this entity around the second x-tick, where its counts increase from 5000 to about 15000.I believe empirical Bayesian estimation using a Gamma-Poisson model will work well for this problem. It's best if I walk through my algorithm:For each entity , use  as a prior distribution for arrival counts within a set of time periods . Remember that 5000 is the empirical median for all arrival counts.Observe arrival counts  for . I propose that ; that is, arrival counts are generated by a Poisson that is stationary over .By conjugacy we can obtain the posterior of arrival counts for  over . It is p(\\lambda_{e_i, T}~|~y_{e_i, t_a},...y_{e_i, t_b}) \\sim Gamma(k + \\sum_{t \\in T} y_{e_i, t}, \\frac{\\theta}{|T|\\theta + 1}) where  is the number of time periods.I then observe an arrival count . This is the next arrival count for entity  after the time period .Compute a z-score for this arrival count using the posterior distribution. Call this .We can then sort entities according to their z-scores. Entities with the highest z-scores have deviated the most from their estimated posterior; I argue these entities have the fastest-accelerating arrival counts.Here's a list of questions I'd like to answer:First, and most importantly: have I made any glaring mistakes? Should I model arrival counts using a different distribution? Would you use  as a prior?Is there an easier approach that incorporates recent observations and uses prior knowledge about arrival counts?","Creater_id":25660,"Start_date":"2016-08-24 13:21:02","Question_id":231544,"Tags":["bayesian","poisson","gamma-distribution","empirical"],"Answer_count":1,"Last_activity":"2016-08-24 23:53:24","Link":"http://stats.stackexchange.com/questions/231544/using-empirical-bayesian-estimation-gamma-poisson-to-analyze-high-arrival-coun","Creator_reputation":98}
{"_id":{"$oid":"5837a575a05283111e4d350e"},"View_count":76,"Display_name":"S. Cow","Question_score":3,"Question_content":"I have a very simple question for Hypothesis testing. Is there a way to calculate the probability that the null hypothesis is false?Since in the literature they're always talking about the significance level and the power of a test, these values are calculated assuming something about the null hypothesis (being true or false in each case).But, in general terms, can we calculate the probability that a given null hypothesis is false, or true?","Creater_id":111402,"Start_date":"2016-08-24 18:21:11","Question_id":231580,"Tags":["hypothesis-testing"],"Answer_count":1,"Last_activity":"2016-08-24 23:19:54","Link":"http://stats.stackexchange.com/questions/231580/can-we-calculate-the-probability-that-a-null-hypothesis-is-true-in-general","Creator_reputation":43}
{"_id":{"$oid":"5837a575a05283111e4d351b"},"View_count":8584,"Display_name":"Marko","Question_score":55,"Question_content":"I heard many times about curse of dimensionality, but somehow I'm still unable to grasp the idea, it's all foggy.Can anyone explain this in the most intuitive way, as you would explain it to a child, so that I (and the others confused as I am) could understand this ones for good?EDIT:Now, let's say that the child somehow heard about clustering (for example, they know how to cluster their toys :) ). How would the increase of dimensionality make the job of clustering their toys harder?For example, they used to consider only the shape of the toy and the color of the toy (one-color toys), but now need to consider the size and the weight of toys also. Why is it more difficult for the child to find similar toys?EDIT 2For the sake of discussion I need to clarify that by - \"Why is it more difficult for the child to find similar toys\" - I also mean why is the notion of distance lost in high-dimensional spaces?","Creater_id":54659,"Start_date":"2015-08-28 02:11:08","Question_id":169156,"Tags":["machine-learning","dimensionality-reduction","high-dimensional"],"Answer_count":11,"Last_activity":"2016-08-24 22:58:20","Link":"http://stats.stackexchange.com/questions/169156/explain-curse-of-dimensionality-to-a-child","Creator_reputation":600}
{"_id":{"$oid":"5837a575a05283111e4d3532"},"View_count":26,"Display_name":"sovo2014","Question_score":2,"Question_content":"Let's say that I am an active user of the Tinder app and in addition to the standard features, I've got access to the A/B testing of my primary picture (the one that appears first when users stumble on my profile).My objective is maximizing the number of \"swipe right\" events, so I try various pictures, but I test them first to make sure that they wouldn't harm the \"swipe right rate\". I am using Bayesian approach to assess the test pictures' performance as soon as possible. I'm preserving the history of my test results which looks like:So far so good. But I can only run the single test at a time, let's say this is the limitation of this hypothetical Tinder Testing feature. At the same time I have a lot of pictures I would like to try, so I need to somehow prioritize them. Here is the idea :Compare each picture with the basiline one and detect what areas have been changed. For example in one of the new pictures I have a new haircut, so \"hairs\" have changed, another one features my new car, so \"environment\" has changed and so on. For each area select the tests results from the history and build the beta-distribution of the \"swipe right rate\" of those tests.Compare the calculated beta distributions by using one of the available methods, let's say compare the graphs to roughly estimate which area can provide better results.Arrange the \"tests backlog\" accordingly.I do not have the strong math/stat background, so I understand that that idea might be pretty silly, but I would like to know what community thinks about that. In general what methods could be used to prioritize the a/b tests based on the prior data?","Creater_id":128870,"Start_date":"2016-08-24 22:29:00","Question_id":231606,"Tags":["bayesian","estimation","prior","ab-test"],"Answer_count":0,"Last_activity":"2016-08-24 22:29:00","Link":"http://stats.stackexchange.com/questions/231606/prioritizing-a-b-tests-by-using-the-prior-data","Creator_reputation":11}
{"_id":{"$oid":"5837a575a05283111e4d3534"},"View_count":58,"Display_name":"UncleCat","Question_score":0,"Question_content":"I am trying to find parameters of an AR(1) process with error terms  t-distribution. With  given, what I have done so far is to fit the linear regression , and then calculate the residuals . The model assumption is that  follows a student-t distribution, however, the standard deviation of   is smaller than 1 (say 0.3). Therefore it could be a scaled t distribution. I was wondering if there is any method for me to find the scaling parameter and the degrees of freedom of the distribution of these residuals?Also, is it OK to directly use least square method to find  and  first? I know they should still be unbiased but no longer the MLE. I was wondering if I could fit the AR(1) model directly with error terms follow a t distribution?","Creater_id":90574,"Start_date":"2016-08-24 18:22:39","Question_id":231582,"Tags":["time-series","arima","autoregressive","t-distribution"],"Answer_count":1,"Last_activity":"2016-08-24 22:21:59","Link":"http://stats.stackexchange.com/questions/231582/how-to-find-parameters-of-ar1-model-with-student-t-error-terms","Creator_reputation":31}
{"_id":{"$oid":"5837a575a05283111e4d3541"},"View_count":46,"Display_name":"WolfRayette","Question_score":3,"Question_content":"I'm trying to wrap my head around this, but when someone says that they used a Gaussian Process, is this not the same as doing linear regression in a feature space defined by the kernel used?","Creater_id":52974,"Start_date":"2016-08-24 19:11:45","Question_id":231585,"Tags":["regression","generalized-linear-model","kernel-trick","gaussian-process"],"Answer_count":1,"Last_activity":"2016-08-24 22:15:21","Link":"http://stats.stackexchange.com/questions/231585/is-a-gaussian-process-the-same-as-a-kernelized-generalized-linear-model","Creator_reputation":27}
{"_id":{"$oid":"5837a575a05283111e4d354e"},"View_count":335,"Display_name":"Catherine Bulka","Question_score":2,"Question_content":"Is anyone aware of propensity score matching methods for when there are more than 2 treatment groups? I am working on a project with 4 treatment groups: ABA and BNeither A nor BCalculating propensity scores using multinomial logistic regression might work, but then I'd get multiple scores for each observation so I'm not sure how I'd match/analyze the matched data.Thanks!","Creater_id":23945,"Start_date":"2014-03-12 14:24:08","Question_id":89807,"Tags":["logistic","matching","propensity-scores"],"Answer_count":2,"Last_activity":"2016-08-24 21:50:57","Link":"http://stats.stackexchange.com/questions/89807/propensity-score-matching-with-multiple-treatments","Creator_reputation":26}
{"_id":{"$oid":"5837a575a05283111e4d355c"},"View_count":418,"Display_name":"Peter Nash","Question_score":8,"Question_content":"For t-tests, according to most texts there's an assumption that the population data is normally distributed. I don't see why that is. Doesn't a t-test only require that the sampling distribution of sample means is normally distributed, and not the population?If it is the case that t-test only ultimately requires normality in the sampling distribution, the population can look like any distribution, right? So long as there is a reasonable sample size. Is that not what the central limit theorem states?(I'm referring here to one-sample or independent samples t-tests)","Creater_id":70909,"Start_date":"2015-03-11 07:52:25","Question_id":141314,"Tags":["hypothesis-testing","t-test","assumptions","central-limit-theorem"],"Answer_count":1,"Last_activity":"2016-08-24 21:48:41","Link":"http://stats.stackexchange.com/questions/141314/question-about-normality-assumption-of-t-test","Creator_reputation":41}
{"_id":{"$oid":"5837a575a05283111e4d3569"},"View_count":547,"Display_name":"Joel Sinofsky","Question_score":4,"Question_content":"I am trying to build a confidence interval for the Poisson distribution using the pivotal method. I have the theory down but I am struggling to come up with , the probability distribution which does not depend on the parameter. Can anyone help me out?","Creater_id":58436,"Start_date":"2014-11-05 14:14:08","Question_id":122836,"Tags":["confidence-interval","poisson","inference","pivot"],"Answer_count":1,"Last_activity":"2016-08-24 21:44:51","Link":"http://stats.stackexchange.com/questions/122836/poisson-confidence-interval-using-the-pivotal-method","Creator_reputation":184}
{"_id":{"$oid":"5837a575a05283111e4d3576"},"View_count":583,"Display_name":"Sumit","Question_score":3,"Question_content":"There are 7 friends A, B, C, D, E, F, and G that belong to a classroom of 35 students. Three students are chosen from the 35. What the probability that exactly two of the group of friends is chosen? Probability that exactly none of the seven friends are chosen?. . . . . So I know the total number of combinations of 3 students chosen out of 35 is (35 c 3). That is the denominator for these questions. I can't figure out what the numerator should be.Is it (7 c 2)/(35 c 3) and (7 c 0)/(35 c 3) ? This does not seem correct.The number (7 c 2) I chose because I need 2 member from this group of seven, and there are (7 c 2) combinations of getting (AB, BC, AD, FE, etc).  On second thought, I might also want to include the possibility ways of getting a single non-friend member for the last slot, so it the numerator:(7 c 2)*(35 - 7) ?Then for no friends, it would be (7 c 0)*(35)(34)(33) as the numerator.Because there are three slots for non-friends?","Creater_id":74313,"Start_date":"2015-04-22 19:10:24","Question_id":147857,"Tags":["probability","self-study","combinatorics"],"Answer_count":4,"Last_activity":"2016-08-24 21:44:01","Link":"http://stats.stackexchange.com/questions/147857/probability-of-selecting-exactly-2-members-of-a-group-of-7-out-of-35-people-if","Creator_reputation":16}
{"_id":{"$oid":"5837a575a05283111e4d3586"},"View_count":4226,"Display_name":"cgo","Question_score":33,"Question_content":"Ridge regression coefficient estimate  are the values that minimize the  \\text{RSS} + \\lambda \\sum_{j=1}^p\\beta_j^2. My questions are:If , then we see that the expression above reduces to the usual RSS. What if ?  I do not understand the textbook explanation of the behaviour of the coefficients.To aid in understanding the concept behind a particular term, why is the term called RIDGE Regression? (Why ridge?) And what could have been wrong with the usual/common regression that there is a need to introduce a new concept called ridge regression? Your insights would be great.  ","Creater_id":67413,"Start_date":"2015-05-07 11:54:59","Question_id":151304,"Tags":["ridge-regression","statistical-learning","history"],"Answer_count":2,"Last_activity":"2016-08-24 21:34:51","Link":"http://stats.stackexchange.com/questions/151304/why-is-ridge-regression-called-ridge-why-is-it-needed-and-what-happens-when","Creator_reputation":591}
{"_id":{"$oid":"5837a575a05283111e4d3594"},"View_count":52,"Display_name":"kyrenia","Question_score":4,"Question_content":"One of the advantage that i have herd talked about propensity score matching, vs. a regression, is that propensity-score matching doesn't rely on linearity assumptions.This seems incorrect on the surface, given probit [/logit] models used to calculate the propensity scores, will embed linearity assumptions. [i.e assuming that you must estimate propensity scores]What would be references to material which discuss this, or compare how similar the assumptions of matching and regressions are more broadly?","Creater_id":30317,"Start_date":"2015-11-23 11:07:49","Question_id":183184,"Tags":["regression","nonparametric","references","matching","propensity-scores"],"Answer_count":1,"Last_activity":"2016-08-24 21:28:53","Link":"http://stats.stackexchange.com/questions/183184/propensity-score-matching-linearity-assumption","Creator_reputation":255}
{"_id":{"$oid":"5837a575a05283111e4d35a1"},"View_count":946,"Display_name":"Klausos","Question_score":13,"Question_content":"I'm using multiple linear regression to describe relationships between Y and X1,X2.From theory I understood that multiple regression assumes linear relationships between Y and each of X (Y and X1, Y and X2). I'm not using any transformation of X.So, I got the model with R=0.45 and all significant X (P\u0026lt;0.05).Then I plotted Y against X1. I don't understand why red-colored circles that are predictions of the model do not form a line. As I said before, I expected that each pair of Y and X is fitted by a line.The plot is generated in python this way:fig, ax = plt.subplots()plt.plot(x['var1'], ypred, 'o', validation['var1'], validation['y'], 'ro');ax.set_title('blue: true,   red: OLS')ax.set_xlabel('X')ax.set_ylabel('Y')plt.show()","Creater_id":89588,"Start_date":"2015-09-16 04:50:02","Question_id":172729,"Tags":["regression","multiple-regression","python","linear"],"Answer_count":1,"Last_activity":"2016-08-24 21:19:17","Link":"http://stats.stackexchange.com/questions/172729/in-multiple-linear-regression-why-does-a-plot-of-predicted-points-not-lie-in-a","Creator_reputation":119}
{"_id":{"$oid":"5837a576a05283111e4d3618"},"View_count":42,"Display_name":"Ben","Question_score":-1,"Question_content":"I don't understand results of ar() function in R. I made up a very simple case, Fibonacci sequence:x \u0026lt;- c(1,1,2,3,5,8,13,21,34,55)ar(x)result is Coefficients:     1  0.5531 I would expect result of 1,1 - for the model x(n) = 1* x(n-1) + 1 * x(n-2)Can anyone explain me please why I don't get expected result? ","Creater_id":112476,"Start_date":"2016-08-24 13:18:12","Question_id":231594,"Tags":["r"],"Answer_count":2,"Last_activity":"2016-08-24 21:06:08","Link":"http://stats.stackexchange.com/questions/231594/how-does-autoregression-work-in-r","Creator_reputation":101}
{"_id":{"$oid":"5837a576a05283111e4d3626"},"View_count":73,"Display_name":"user6507246","Question_score":5,"Question_content":"We have a  \\boldsymbol\\mu= \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\end{bmatrix}  \\boldsymbol Y= \\begin{bmatrix} Y_1 \\\\ Y_2 \\end{bmatrix} with  \\begin{bmatrix}\\Sigma_{11} \u0026amp; \\Sigma_{12}\\\\\\Sigma_{21} \u0026amp; \\Sigma_{22}\\end{bmatrix}Why they represent covariance with 4 separated matrix? and what does   \\Sigma_{11}  \\Sigma_{12}  \\Sigma_{21}  \\Sigma_{22}  seperately mean in Gaussian? ","Creater_id":128678,"Start_date":"2016-08-23 17:30:49","Question_id":231385,"Tags":["normal-distribution","linear-algebra"],"Answer_count":1,"Last_activity":"2016-08-24 20:52:20","Link":"http://stats.stackexchange.com/questions/231385/gaussian-covariance-matrix-basic-concept","Creator_reputation":58}
{"_id":{"$oid":"5837a576a05283111e4d3632"},"View_count":50,"Display_name":"LetsPlayYahtzee","Question_score":0,"Question_content":"I am preparing a dataset for a binary classification. Some of the columns contain missing values.I am thinking of either handling the missing values manually using pandas and replace each missing value by the mean of the column by class or using the Imputer method by scikit.Two questions, is it better to replace the missing values with the mean of the class rather than the mean of the whole column and if yes why isn't there such an option in scikit's Imputer method?some data related infoFor some features (sentiment analysis features) there a lot of missing values and that comes from the fact that some users (my training samples are pairs of users) don't share common hashtags (it's twitter a analysis task), but most of the features are topological so it doesn't make sense to discard all those lines.On the other hand someone could say that since the users don't share any hashtags they should have a zero value for those features. But this isn't necessarily true. My crawling includes the latest 500 tweets for those users, not all tweets they have ever produced.Also from visualizing the data I can see clear correlations between those features and the class values, from the values that are not gathered around zero due to lack of common hashtags.Here are some visualizationsHere are two sentiment features, not exactly easily linearly separable but the problem can be observed (not exactly seen because of density), the stripe on the crossing of the axes holds close to half the valuesHere is another one, the problem is the same, the small stripe on the crossing of the axes holds close to half the valuesHere is the same visualization but from weka this time","Creater_id":107476,"Start_date":"2016-08-24 19:21:14","Question_id":231586,"Tags":["missing-data","scikit-learn","data-imputation"],"Answer_count":1,"Last_activity":"2016-08-24 20:41:50","Link":"http://stats.stackexchange.com/questions/231586/is-it-better-to-replace-missing-values-by-mean-or-mean-by-class","Creator_reputation":108}
{"_id":{"$oid":"5837a576a05283111e4d363f"},"View_count":26,"Display_name":"user3504466","Question_score":1,"Question_content":"I am running a regression using the gam function.fit \u0026lt;- gam(y~ s(x))The smoothing parameter used is the default one and number of knots are kept to be determined automatically.The plot below shows the scatter plot for variable x and the smoothed curve. I fail to understand as why is there an upward trend in the smoothed curve when there is no such pattern seen in the scatter plot. Please advice where am I going wrong. Thanks.","Creater_id":58474,"Start_date":"2016-08-24 20:21:48","Question_id":231591,"Tags":["r","regression","gam","splines"],"Answer_count":0,"Last_activity":"2016-08-24 20:21:48","Link":"http://stats.stackexchange.com/questions/231591/regression-using-gam-gives-an-unexpected-shape","Creator_reputation":118}
{"_id":{"$oid":"5837a576a05283111e4d3641"},"View_count":21,"Display_name":"noumenal","Question_score":1,"Question_content":"A robust paired t-test is a better choice for skewed distributions than the conventional paired t-test (e.g Fradrette, Keselman, Lix, \u0026amp; Wilcox, 2003). One version of the robust test uses a trimmed mean and a Winsorized variance. Is there a way to find the optimal cutoff point for the Winsorization percentiles that maximises the sensitivity?If I understand correctly, robust statistics perform better for skewed distributions than for symmetric ones and modifying the tails of the distributions using Winsorization or trimming thus changes the skewness. For example, consider if one were to increase the Winsorization percentage iteratively. I imagine that the skewness statistic approaches a certain criterion and the adjusted  difference for the cross-correlation between the original distribution and the current Winsorized distribution approaches a criterion close to zero.If using such an algorithm, would it be possible to reach an unbiased estimates of central tendency and measures of dispersion? Are there any known implementations, for example, in the field of machine learning?Fradrette, K., Keselman, H. J., Lix, L., Algina, J., \u0026amp; Wilcox, R. R. (2003). Conventional and robust paired independent-samples t test: Type I error and power rate. Journal of Modern Applied Statistical Methods, 2(2) 22, 480-496. Retrieved from http://digitalcommons.wayne.edu/jmasm/vol2/iss2/22 [Open Access] 2016-08-25.","Creater_id":10790,"Start_date":"2016-08-24 08:19:32","Question_id":231482,"Tags":["machine-learning","t-test","optimization","robust"],"Answer_count":0,"Last_activity":"2016-08-24 19:30:35","Link":"http://stats.stackexchange.com/questions/231482/optimizing-robust-statistics","Creator_reputation":493}
{"_id":{"$oid":"5837a576a05283111e4d3643"},"View_count":39,"Display_name":"SKM","Question_score":1,"Question_content":"Confusion 1:Let, N=100 be the number of observations generated from a function/process, for example a dynamical system that is of the form for .  Let When I iterate the above function, I get a vector  of observations which is of length  for example: This data is an ordered collection of values.Assuming, Gaussian distribution of mean  and variance  expressed asEach of the value  is called a random variable. So, are there 100 random variables? Please correct me if this statement is wrong.Confusion 2:  Considering another example from signal processing : The information signal  is passed through a communication channel with the channel impulse response modeled as a moving average model of order 2, and the output is corrupted with white Gaussian noise, the model is expressed as for What are the random varaibles and distribution kind in this case - viz. univaraite or multivariate? Do we have one random variable  or are there 100 random variables?Confusion 3: Another example, from image processing. In the case of any matrix, for example X = [0.1, 0.3,0.11;     0.2, 0.4,0.55;      0.1, 0.3,0.21,;            ] which is of size  (N = 3, p = 3). Here,  are not related to the above two questions/ confusion. Each row  is an example say the pixel values of an image and so there are 3 images each of 3 pixel values.Assuming Gaussian distribution, what would be the random variable and its distribution (multivariate or univariate) here? Would each column i.e; feature component R,G,B be called a random variable? Thus, would there be 3 random variables?","Creater_id":21160,"Start_date":"2016-08-24 15:00:29","Question_id":231555,"Tags":["probability","self-study","random-variable"],"Answer_count":0,"Last_activity":"2016-08-24 18:54:13","Link":"http://stats.stackexchange.com/questions/231555/conceptual-question-on-random-variable-and-probability-distribution","Creator_reputation":99}
{"_id":{"$oid":"5837a576a05283111e4d3645"},"View_count":96,"Display_name":"JasoonS","Question_score":2,"Question_content":"In exercise 3.6 of the book, 'An Introduction to Reinforcement Learning' by Sutton, R. and Barto, A. They ask the following question at the very end of the chapter 3.5 (which introduces the Markov Property).  Broken Vision System: Imagine that you are a vision system. When you  are first turned on for the day, an image floods into your camera. You  can see lots of things, but not all things. You can't see objects that  are occluded, and of course you can't see objects that are behind you.    i) After seeing that first scene, do you have access to the Markov state  of the environment?     ii) Suppose your camera was broken that day and you  received no images at all, all day. Would you have access to the  Markov state then?I don't quite understand what they are asking. What is the 'Markov state of the environment'?What I've thought so far:i) It doesn't have the Markov property, as what the state of the environment will be in the next image does not depend entirely on what is in the current image (although it may well be a good approximation). I don't quite know what that says about the Markov state of the environment though? Or is the Markov state of the environment just all the information in the environment at that point in time? In which case it can't see occluded objects and objects not in field of view, so it doesn't have access to all that information (the state).ii) I think its best I wait to get feedback on the first question before any kind of assumptions about answers to this second question.Thanks, I'll be very grateful if you help me patch up my understanding of this :)","Creater_id":128814,"Start_date":"2016-08-24 11:03:49","Question_id":231515,"Tags":["markov-process","reinforcement-learning"],"Answer_count":2,"Last_activity":"2016-08-24 18:45:41","Link":"http://stats.stackexchange.com/questions/231515/what-does-the-markov-state-of-the-environment-mean","Creator_reputation":113}
{"_id":{"$oid":"5837a576a05283111e4d3653"},"View_count":41,"Display_name":"Manoel Ribeiro","Question_score":0,"Question_content":"I've recently come accross the following statement in Johnson's Statistics Principles and Methods 9th Edition  , the sample mean, is normal when sampling from a normal population.My straightforward - and perhaps naïve - question is: why?","Creater_id":89255,"Start_date":"2016-08-24 15:56:47","Question_id":231562,"Tags":["self-study","sample-mean"],"Answer_count":2,"Last_activity":"2016-08-24 18:31:26","Link":"http://stats.stackexchange.com/questions/231562/why-is-the-sample-mean-normal-when-sampling-from-a-normal-population","Creator_reputation":40}
{"_id":{"$oid":"5837a576a05283111e4d3661"},"View_count":1309,"Display_name":"Alison Fairbrass","Question_score":4,"Question_content":"I have four linear mixed effect models of similar structure:model1 \u0026lt;- lmer(index1 ~ biophony + anthrophony + (1|Site), data=df, REML=F) model2 \u0026lt;- lmer(index2 ~ biophony + anthrophony + (1|Site), data=df, REML=F) model3 \u0026lt;- lmer(index3 ~ biophony + anthrophony + (1|Site), data=df, REML=F) model4 \u0026lt;- lmer(index4 ~ biophony + anthrophony + (1|Site), data=df, REML=F) These models are testing the relationship between biophony (sounds generated by biodiversity) and anthrophony (sounds generated by humans) with four different indices for bioacoustic diversity, with Site as a random effect.Index 1 is a sum of positive values. Index 2 is a sum of proportions. Index 3 is the area under the curve on a plot of frequency (Hz) against decibels (dB). Index 4 is a ratio of power in two frequency bins. Therefore indices 1-3 can only be positive. Index 4 is bounded by -1 to +1.Using the following code to generate normal QQ-plots of the residuals:qqnorm(residuals(model1))abline(0,1)I have found that the residuals of the models look very non-normal (see plots below).  The first plot appears to be heavy-tailed. The rest I do not know what distribution the plots indicate (I can't find any examples online of similar plots).  I have already tried log transforming the indices data but this hasn't improved the distribution of the data. I have tried using the boxcox function to find an appropriate power transformation as described here, but again this did not improve the distribution of the residuals.My questions are:Should I transform the data? If so can you recommend appropriate transformations? If a transformation isn't appropriate, should I use generalised linear mixed models to analyse these data? Could you recommend what family and link functions would be appropriate for a glmer analysis of data of these distributions?I have attached histograms of the marginal distributions of the indices data for more information:","Creater_id":61376,"Start_date":"2014-11-24 06:06:17","Question_id":125258,"Tags":["regression","data-transformation","residuals","mixed-model","histogram"],"Answer_count":1,"Last_activity":"2016-08-24 18:26:13","Link":"http://stats.stackexchange.com/questions/125258/linear-mixed-effects-models-what-to-do-when-the-residual-qq-plot-looks-non-norm","Creator_reputation":21}
{"_id":{"$oid":"5837a576a05283111e4d366e"},"View_count":12,"Display_name":"user39531","Question_score":1,"Question_content":"There is 1 independent var (AT) with two levels (PQ_Low and PQ_High).1) My SPSS output reveals sig as 0.000 Can I assume that my p-value is significant as it is less than the alpha level of 0.05?2) Should I be concerned about violation of Mauchly's Test of Sphericity in this case, as there are only 2 levels?Thanks","Creater_id":39531,"Start_date":"2016-08-24 18:24:51","Question_id":231583,"Tags":["anova","variance","descriptive-statistics","manova"],"Answer_count":0,"Last_activity":"2016-08-24 18:24:51","Link":"http://stats.stackexchange.com/questions/231583/1-way-within-subject-anova-sig-value","Creator_reputation":60}
{"_id":{"$oid":"5837a576a05283111e4d3670"},"View_count":4703,"Display_name":"user538603","Question_score":1,"Question_content":"When fitting multiple variables to one outcome via the lm() function in R, summary(lm) gives me the p-values for individual regressors but not for the full model in an easily extractable (as in, just accessing fields) kind of way.According to this question, it is possible to extract the p-value via summary(lm)fstatistic[1],xfstatistic[3],lower.tail=FALSE)However, while in the example linked this provides the same p-value as is printed, I get a different one:\u0026gt; summary(model)# ...Residual standard error: 1.533 on 371 degrees of freedom  (555 observations deleted due to missingness)Multiple R-squared:  0.3364,    Adjusted R-squared:  0.2864 F-statistic: 6.718 on 28 and 371 DF,  p-value: \u0026lt; 2.2e-16and:f = summary(model)$fstatistic\u0026gt; pf(f[1],f[2],f[3],lower.tail=F)       value 5.948007e-20 What are possible reasons for these values to be different, and which one is the \"right\" one for the significance of the whole model?","Creater_id":24765,"Start_date":"2014-04-07 03:54:43","Question_id":92824,"Tags":["r","regression","multiple-regression","p-value"],"Answer_count":1,"Last_activity":"2016-08-24 17:37:52","Link":"http://stats.stackexchange.com/questions/92824/multiple-regression-p-value-differs-in-summary-and-in-pf-output-used-for-extra","Creator_reputation":38}
{"_id":{"$oid":"5837a576a05283111e4d367d"},"View_count":4043,"Display_name":"Bach","Question_score":4,"Question_content":"I have learning data consisting of ~45k samples, each has 21 features. I am trying to train a random forest classifier on this data, which is labelled to 3 classes (-1, 0 and 1). The classes are more or less equal in their sizes.My random forest classifier model is using gini as its split quality criterion, the number of trees is 10, and I have not limited the depth of a tree.Most of the features have shown negligible importance - the mean is about 5%, a third of them is of importance 0, a third of them is of importance above the mean.However, perhaps the most striking fact is the oob (out-of-bag) score: a bit less than 1%. It made me think the model fails, and indeed, testing the model on a new independent set of size ~40k, I got score of 63% (sounds good so far), but a deeper inspection of the confusion matrix have shown me that the model only succeeds for class 0, and fails in about 50% of the cases when it comes to decide between 1 and -1.Python's output attached:array([[ 7732,   185,  6259],       [  390, 11506,   256],       [ 7442,   161,  6378]])This is naturally because the 0 class has special properties which makes it much easier to predict. However, is it true that the oob score I've found is already a sign that the model is not good? What is a good oob score for random forests? Is there some law-of-thumb which helps determining whether a model is \"good\", using the oob score alone, or in combination with some other results of the model?Edit: after removing bad data (about third of the data), the labels were more or less 2% for 0 and 49% for each of -1/+1. The oob score was 0.011 and the score on the test data was 0.49, with confusion matrix hardly biased towards class 1 (about 3/4 of the predictions).","Creater_id":40740,"Start_date":"2014-04-30 06:45:22","Question_id":95818,"Tags":["classification","random-forest","out-of-sample"],"Answer_count":3,"Last_activity":"2016-08-24 16:57:43","Link":"http://stats.stackexchange.com/questions/95818/what-is-a-good-oob-score-for-random-forests-with-sklearn-three-class-classifica","Creator_reputation":136}
{"_id":{"$oid":"5837a576a05283111e4d368c"},"View_count":37,"Display_name":"Vivek Bagaria","Question_score":1,"Question_content":"Problem SettingLet  be identical and marginally  random variables. There is no restriction on the joint distribution of .ObservationThe entropy  is maximized (over all possible joint distributions) when  are independent. This can be proved by expanding the entropy term using chain ruleQuestionIs the entropy of their sum,  also maximized when they are independent? ","Creater_id":99046,"Start_date":"2016-08-05 01:47:42","Question_id":228400,"Tags":["references","conditional-probability","entropy","information-theory"],"Answer_count":1,"Last_activity":"2016-08-24 16:56:17","Link":"http://stats.stackexchange.com/questions/228400/maximizing-entropy-for-sum-of-random-variables","Creator_reputation":266}
{"_id":{"$oid":"5837a576a05283111e4d3698"},"View_count":456,"Display_name":"Joel Graff","Question_score":0,"Question_content":"I'm playing with a little prediction project using a multi-layer perception (MLP) with robust back propagation.  I have a variety of variables which correlate to the single output I'm attempting to predict.On a whim, I wanted to try providing the neural network with a single input of random uniform values (scaled -1 to 1).  The output surprised me.A simple plot of the target values vs. the predicted values shows that the neural network learned the data to a surprising degree.The images below describe what I'm observing.  The scale of the response makes the plot hard to see, but it should seem obvious that the training and test data perform similarly.  I can understand both results having the same distribution and scale, but cannot explain why the test data doesn't show more randomness in it's curve.The neural network had 125 neurons in a single hidden layer and was iterated 800 times.  If anything, I might have expected overfitting on the training data, which should have exacerbated the randomness in the test data's response even more.I had hoped to use this as a baseline metric to gauge the meaningfulness of other variables in the data set, though I'm not sure how to proceed from here.I'm sure I've stumbled upon a well-documented phenomenon / technique, here...  Any thoughts?","Creater_id":62362,"Start_date":"2015-03-30 13:09:39","Question_id":144115,"Tags":["neural-networks"],"Answer_count":2,"Last_activity":"2016-08-24 16:44:58","Link":"http://stats.stackexchange.com/questions/144115/training-a-neural-network-with-uniform-random-inputs","Creator_reputation":97}
{"_id":{"$oid":"5837a576a05283111e4d36a6"},"View_count":23,"Display_name":"Justin","Question_score":2,"Question_content":"I've read in several places that a negative-binomial model is a reasonable alternative to a Poisson regression when the latter shows overdispersion. However, none of the several sources I read said whether it is also an improvement over a Poisson that shows underdispersion.  So is a negative-binomial worth considering for count data where the variance doesn't equal the mean? Or more specifically, only when the variance is larger than the mean?","Creater_id":127487,"Start_date":"2016-08-19 08:46:07","Question_id":230730,"Tags":["poisson","negative-binomial"],"Answer_count":1,"Last_activity":"2016-08-24 16:44:11","Link":"http://stats.stackexchange.com/questions/230730/negative-binomial-for-underdispersed-data","Creator_reputation":86}
{"_id":{"$oid":"5837a576a05283111e4d36b3"},"View_count":2887,"Display_name":"ktdrv","Question_score":11,"Question_content":"I am looking to implement a biplot for principal component analysis (PCA) in JavaScript. My question is, how do I determine the coordinates of the arrows from the  output of the singular vector decomposition (SVD) of the data matrix?Here is an example biplot produced by R:biplot(prcomp(iris[,1:4]))I tried looking it up in the Wikipedia article on biplot but it's not very useful. Or correct. Not sure which.","Creater_id":10433,"Start_date":"2015-03-09 21:52:29","Question_id":141085,"Tags":["pca","svd","biplot"],"Answer_count":1,"Last_activity":"2016-08-24 16:08:49","Link":"http://stats.stackexchange.com/questions/141085/positioning-the-arrows-on-a-pca-biplot","Creator_reputation":208}
{"_id":{"$oid":"5837a576a05283111e4d36c0"},"View_count":25,"Display_name":"generic_user","Question_score":0,"Question_content":"Given a consistent estimator of  and a procedure to consistently estimate the variance of , is the variance of  necessarily equal to the variance of ?This is the case in OLS (see for example feasible generalized least squares).  But I'm not sure whether it holds in general or not.I'm thinking of this in particular in the context of this paper and this one, but it seems like a more general question.  Is there a general answer?  If not, how can one tell for a particular estimator?","Creater_id":17359,"Start_date":"2016-08-24 16:08:01","Question_id":231565,"Tags":["variance","random-forest","heteroscedasticity","estimators"],"Answer_count":0,"Last_activity":"2016-08-24 16:08:01","Link":"http://stats.stackexchange.com/questions/231565/is-the-conditional-variance-of-an-estimator-always-equal-to-the-conditional-vari","Creator_reputation":2771}
{"_id":{"$oid":"5837a576a05283111e4d36c2"},"View_count":844,"Display_name":"N26","Question_score":16,"Question_content":"I have carried out a principal components analysis of six variables , , , ,  and . If I understand correctly, unrotated PC1 tells me what linear combination of these variables describes/explains the most variance in the data and PC2 tells me what linear combination of these variables describes the next most variance in the data and so on. I'm just curious -- is there any way of doing this \"backwards\"? Let's say I choose some linear combination of these variables -- e.g. , could I work out how much variance in the data this describes?","Creater_id":3845,"Start_date":"2011-03-22 07:00:23","Question_id":8630,"Tags":["variance","pca","r-squared","covariance-matrix"],"Answer_count":3,"Last_activity":"2016-08-24 15:58:15","Link":"http://stats.stackexchange.com/questions/8630/principal-component-analysis-backwards-how-much-variance-of-the-data-is-expla","Creator_reputation":81}
{"_id":{"$oid":"5837a576a05283111e4d36d1"},"View_count":41,"Display_name":"Ben","Question_score":0,"Question_content":"I think the title is fairly self-explanatory. I want to compute the cross-correlation between two time series controlled for the values at other lags. I can't find any existing code to do this, either in R or any other language, and I'm not at all confident enough in my knowledge of statistics (or R) to try to write something myself. It would be analogous to the partial autocorrelation function, just for the cross-correlation instead of the autocorrelation.If it helps at all, my larger objective is to look for lagged correlations between different measurements of a physical system (to start with, flux and photon index from gamma ray measurements of blazars), with the goal of building a general linear model to try to predict flaring events.","Creater_id":114017,"Start_date":"2016-08-24 15:44:25","Question_id":231560,"Tags":["r","cross-correlation","partial-correlation"],"Answer_count":0,"Last_activity":"2016-08-24 15:44:25","Link":"http://stats.stackexchange.com/questions/231560/partial-cross-correlation-in-r","Creator_reputation":11}
{"_id":{"$oid":"5837a576a05283111e4d36d3"},"View_count":10,"Display_name":"numX","Question_score":0,"Question_content":"Currently I am doing some experimentation trying out different deep learning models on a real estate prediction scenario  where the goal is to predict the price given factors such as location, type etc.Using 5-fold cross validation, I can compare the R^2 ( co-efficient of determination) between between the cross validation metrics of models and determine which has the highest value. However, I can also run the models against data which was excluded from training ( hold out validation) and from these predictions I can calculate a % error rate on the price using the function (1-(ABS(P-A)/A)) * 100 where P is the predicted price, A is the actual price and ABS represents the Absolute function.Is either of these methods flawed in some way i.e. is misrepresenting the predictive power, or is one preferable over the other? I'm relatively new to statistics so I may have misunderstood some concepts! Thanks","Creater_id":127280,"Start_date":"2016-08-24 15:34:05","Question_id":231559,"Tags":["regression","machine-learning","cross-validation","deep-learning","out-of-sample"],"Answer_count":0,"Last_activity":"2016-08-24 15:34:05","Link":"http://stats.stackexchange.com/questions/231559/choosing-best-model-by-co-efficient-of-determination-vs-error-rate","Creator_reputation":101}
{"_id":{"$oid":"5837a576a05283111e4d36d5"},"View_count":29,"Display_name":"quant","Question_score":4,"Question_content":"I have the model  where  is a k-dimensional vector of explanatory variables and  is a k-dimensional parameter vector, where  where  is an l-dimensional vector of explanatory variables and  is a  parameter matrix. Assuming that  and  and also assuming uninformative priors for the parameters  and  I cannot seem to find their posterior distributions. Any help ? The reason I need their posterior distributions is to use Gibbs sampling later to estimate them.","Creater_id":121519,"Start_date":"2016-08-23 04:48:06","Question_id":231545,"Tags":["bayesian"],"Answer_count":1,"Last_activity":"2016-08-24 15:28:25","Link":"http://stats.stackexchange.com/questions/231545/bayesian-posterior-distributions","Creator_reputation":26}
{"_id":{"$oid":"5837a576a05283111e4d36e2"},"View_count":40,"Display_name":"Anton","Question_score":0,"Question_content":"I have a theoretical question on the correct way to make learning curves to diagnose a classifier. To see a generic example of these curves one can refer to this (min 34 onward) lecture by Andrew Ng or this example from scikit. There is sparse information on the topic elsewhere -- that is my reason to ask experts here.My understanding on the matter can be briefly summarized in the procedure below.Make a train/test split of the data set. The test set is held constant over the next steps.Derive a training set subsample of size N and train your classifier on it.Evaluate your classifier performance on the test set you derived at step #1 to get a point for the test error curve. Evaluate performance on the training subsample from step #2 you just trained the classifier on to get a point on the training error curve.Increase your subsample size N (unless you hit the training set size limit) by some delta and repeat from step #2.Plot the curves to see whether bias or variance prevails.One of the assumptions of the procedure above (as per my understanding) is that you hold the test set fixed. But what is about the classifier itself? Say, we are training classification trees where I can tune a bunch of hyperparameters like node size, depth, obs. per terminal node, splitting criterion, etc. Or kNN where k is a model tuning parameter. In both examples the classifier with some preset parameter values may even be nonsensical for some N-sized subsamples (say, setting obs. per leaf that is too large for small N or vise versa or nearest neighbors that are surely an overfit as my subsample size N gets larger and larger). This leads to the idea of doing an extra CV step between #1 and #2 to choose proper (for the given sample size) hyperparameters of the classifier. But this essentially leads to a \"different\" classifier I employ at each step of the above procedure.So, this left me with the following questions.In general, is doing CV between the lines of the above procedure appropriate? Again, it seems natural that e.g. I don't want to report a classifier overfit (based on the train/test error curves convergence behavior) at full sample size by growing a huge tree just because I set min node size equals 2 in the very beginning and never adjusted it having a larger subsample later on.In case I'm allowed to tune classifiers inside the loop of the procedure, are there any parameters I should/must avoid tuning? Say, what if CV suggests different SVM kernels at various Ns and I definitely end up with a classifier that is different from the one I started with?Lastly, if I'm not allowed to CV, how do I cope with at least nonsensical k\u003eN in kNN?It would be interesting to hear your thoughts as I probably completely miss something.Thanks!","Creater_id":123365,"Start_date":"2016-07-14 16:38:34","Question_id":223854,"Tags":["machine-learning","classification","cross-validation","cart","performance"],"Answer_count":1,"Last_activity":"2016-08-24 15:19:53","Link":"http://stats.stackexchange.com/questions/223854/doing-cross-validation-when-diagnosing-a-classifier-through-learning-curves","Creator_reputation":1}
{"_id":{"$oid":"5837a576a05283111e4d36ef"},"View_count":72,"Display_name":"Mert Ovn","Question_score":2,"Question_content":"What I mean is that if we normalize the beta cdf to unity and treat it as a pdf, could we have an analytical expression for the mean of this distribution in terms of alpha and beta parameters of the Beta distribution function?","Creater_id":128796,"Start_date":"2016-08-24 12:44:08","Question_id":231534,"Tags":["distributions"],"Answer_count":1,"Last_activity":"2016-08-24 15:10:23","Link":"http://stats.stackexchange.com/questions/231534/what-is-the-mean-of-a-cumulative-beta-distribution-function","Creator_reputation":13}
{"_id":{"$oid":"5837a576a05283111e4d36fc"},"View_count":28,"Display_name":"Caspa Letti","Question_score":3,"Question_content":"I apologize in advance for my question that can seem redundant, but I am still struggling to interpret the outcome of my lme model, as the other posts mainly deal with several categorical variables and I get really confused...I have three continuous variables, Delta (my measurement), Date, and Light and two categorical variables, Identity (names) and Treatment applied to the subjects, which has 3 non ordered levels.I understand how to interpret betas when the parameters are all categorical/all continuous - but the mix makes me perplex.I use the lme model:model.lme = lme(Delta ~ Treatment + Light * Date, random =~1|Identity, na.action = na.omit,control = lmeControl(optimum =\"opt\"))and I get :Random effects: Formula: ~1 | ID        (Intercept)  ResidualStdDev:  0.08049669 0.3757045Fixed effects: Delta ~ Treatment + Light * Date                   Value  Std.Error     DF    t-value p-value(Intercept)  0.27590485 0.03358550 350400   8.214999  0.0000Treatment2  -0.06606907 0.04418992 350400  -1.495116  0.1349Treatment3  -0.01027265 0.04861505     16  -0.211306  0.8353Light        0.00088466 0.00009181 350400   9.635306  0.0000Date        -0.00083441 0.00004336 350400 -19.241967  0.0000Light:Date   0.00000134 0.00000050 350400   2.673734  0.0075 Correlation:            (Intr) Trtmn2 Trtmn3 Light  DOY   Treatment2 -0.709                            Treatment3 -0.654  0.490                     Light      -0.109  0.000  0.000              DOY        -0.235  0.004  0.007  0.459       Light:DOY   0.109  0.000  0.000 -0.995 -0.462Standardized Within-Group Residuals:        Min          Q1         Med          Q3         Max -5.28753179 -0.23911026 -0.09499650  0.05150716 43.17908373 Number of Observations: 350422Number of Groups: 18If I am right, Intercept is the base effect, the value of Delta when the others parameters = 0. But somehow in what I have read I understand that I should take intercept as Treatment1 ? Does it mean that Treatment1 is taken as a reference to which other parameters are compared ? It happens that Treatment1 is a control condition, but I am not sure whether or not this is scientifically valid as I would like all treatments to be considered 'equals'.If this is not right, what exactly is Treatment1 here ?Also, should I interpret the value as a linear relation, as the model assumes so ? eg. Date has a significant effect on Delta, and its value is negative. So the further we are in time the lower is Delta ? Is it correct to say that Delta decreases following a slope of 0,0008 ? Or is it more accurate to say that the slope is (0,2759 - 0,0008)?I guess that if I have no proof that there is a linear relation (which is not very likely here I think) the only thing I can say with this model is 'Delta is significantly affected by Light and Date' ?Many, many thanks in advance for any help on this,Caspa","Creater_id":128805,"Start_date":"2016-08-24 09:52:36","Question_id":231504,"Tags":["r","mixed-model","interpretation","intercept"],"Answer_count":1,"Last_activity":"2016-08-24 14:53:40","Link":"http://stats.stackexchange.com/questions/231504/how-to-interpret-lme-outcome-with-a-mix-of-cat-non-cat-variables","Creator_reputation":23}
{"_id":{"$oid":"5837a576a05283111e4d3709"},"View_count":104,"Display_name":"user128712","Question_score":1,"Question_content":"I am using the prop.test function in R to test the differences of proportions against the alternative that the difference of the two proportions are significantly different from zero, however I wish to perform the same test, but instead of testing if they are significantly different from zero I want to test if they are different from some constant (lets say C) where more often then not C is not equal to 0.Does anyone know of a function in R to do this?","Creater_id":128712,"Start_date":"2016-08-23 13:13:56","Question_id":231357,"Tags":["hypothesis-testing","proportion"],"Answer_count":2,"Last_activity":"2016-08-24 14:52:05","Link":"http://stats.stackexchange.com/questions/231357/test-whether-difference-in-proportions-differs-from-a-non-zero-constant","Creator_reputation":6}
{"_id":{"$oid":"5837a576a05283111e4d3717"},"View_count":30,"Display_name":"user1363251","Question_score":0,"Question_content":"Consider 2 models A and B. A (5 free parameters) is nested within B (6 free parameters). These models are drift diffusion models (see Bogacz et al., 2006, Psychological Review)Imagine that we do an experiment and acquire data from N = 21 participants. We fit models A and B to each individual dataset by minimizing Pearson's chi-square statistic (see paragraph \"Calculating the test-statistic\" at https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test).Chi-square values are generally lower for model B than  model A, indicating a better goodness-of-fit for model B. I would like to test whether the improvement in goodness-of-fit for Model B is significant by doing a chi-square test for nested models. 1) I am not sure about how to perform this test. I think I have to compute the difference chi-sq (modelA) - chi-sq (modelB). Remember that the sample size N = 21, so I get 21 chi-square difference values. These values should be generally positive, because the goodness-of-fit is generally worse for model A. The difference should follow a chi-square distribution with a number of degrees of freedom df(diff) = df(model B) - df(model A) = 1. Am I correct?2) How can I implement this test in python? ","Creater_id":14271,"Start_date":"2016-08-24 13:06:52","Question_id":231541,"Tags":["chi-squared","python","model","nested"],"Answer_count":0,"Last_activity":"2016-08-24 14:40:57","Link":"http://stats.stackexchange.com/questions/231541/chi-square-test-for-nested-models-theory-and-python-implementation","Creator_reputation":11}
{"_id":{"$oid":"5837a576a05283111e4d3719"},"View_count":168,"Display_name":"S. Ming","Question_score":0,"Question_content":"I've been trying to figure out why the expected value of the error term equals zero when the intercept is included. I don't understand the formal proof. In my book the following proof is given:Suppose the expectation of  is  instead of , then . If we add  to the constant term and subtract it from the error term, we obtain:Since both equations are equivalent, and since , then the latter equation can be written in a form that has a zero expectation for the error term:where  and I have the following questions:Why do we subtract  from the error term ()? Why can't we just set ? I don't understand this specification.And why is the  added to the intercept?","Creater_id":128181,"Start_date":"2016-08-24 14:00:01","Question_id":231549,"Tags":["regression","error"],"Answer_count":1,"Last_activity":"2016-08-24 14:40:38","Link":"http://stats.stackexchange.com/questions/231549/expected-value-of-error-term-equals-zero-formal-proof","Creator_reputation":46}
{"_id":{"$oid":"5837a576a05283111e4d3726"},"View_count":57,"Display_name":"Teddy","Question_score":1,"Question_content":"Hi I have data (weight in kg) for samples from different heights (e.g. 5.5 ft, 5.8ft, 6ft). I want to normalize their weight with height (to exclude the effect of height). I am not very familiar with statistics. Example data are as follows:e.g. patient A has 5.5ft and 60 kg,     patient B has 5.8ft and 73 kg,     patient C has 6 ft  and 100 kg.In that case, I would like to normalize patient B and C with patient A who has lowest height 5.5 ft. I use this formula: Normalized weight value = (real weight value * minimum height)/real height. So, Patient B (normalized) = (73 * 5.5)/5.8 = 73.017 kgIs my way of calculation correct? If not, please advise the correct ways.Thanks.","Creater_id":128840,"Start_date":"2016-08-24 14:23:06","Question_id":231552,"Tags":["normalization"],"Answer_count":0,"Last_activity":"2016-08-24 14:23:06","Link":"http://stats.stackexchange.com/questions/231552/how-to-normalize-data-between-different-samples","Creator_reputation":6}
{"_id":{"$oid":"5837a576a05283111e4d3728"},"View_count":54,"Display_name":"Ian","Question_score":2,"Question_content":"I've been having difficulty in obtaining the exact solution given by the author. I need help as my solution doesn't factorize completely to the same term given by the author.Obtaining the kth moments entails computing the integral given in the picture. I carried out the integration but I did not get the exact term the author got. I was of the opinion that I may be wrong in my factorization, hence I seek other ideas to obtain the exact solution the author got. Otherwise, I want to be justified that the author is probably wrong.[![enter image description here][3]][3][![enter image description here][4]][4]","Creater_id":56265,"Start_date":"2016-08-22 13:32:18","Question_id":231163,"Tags":["distributions","moments","integration"],"Answer_count":1,"Last_activity":"2016-08-24 14:19:14","Link":"http://stats.stackexchange.com/questions/231163/moments-of-transmuted-lindley-distribution","Creator_reputation":11}
{"_id":{"$oid":"5837a576a05283111e4d3735"},"View_count":71,"Display_name":"quirik","Question_score":0,"Question_content":"Following this post Bivariate probit model with sample selection, what would be an example of negative correlation between errors of selection and outcome equation?I have also found an example where rho is negative: http://personal.rhul.ac.uk/uhte/006/ec5040/selectivity.pdf. Based on this example, unobserved factors or positive errors that make participation more likely (e.g. ability) tend to be associated with lower wages or negative errors of outcome equation. What could unobservables in outcome equation represent?","Creater_id":43204,"Start_date":"2016-08-23 15:11:38","Question_id":231374,"Tags":["self-study","econometrics","heckman"],"Answer_count":1,"Last_activity":"2016-08-24 14:16:03","Link":"http://stats.stackexchange.com/questions/231374/heckman-selection-model-and-negative-rho","Creator_reputation":113}
{"_id":{"$oid":"5837a576a05283111e4d3742"},"View_count":50,"Display_name":"Fomite","Question_score":0,"Question_content":"I've got measurements from two different types of sites, lets call them \"Source\" and \"Sensor\", with a proportion measure for each ranging from 0 to 1, like so:Source, Sensor0.25, 0.340.05, 0.720.38, 0.26There's some interest in finding out if Sensor could be used as a proxy for Source, as Source is much more difficult to collect samples in. The two measurements are paired, so it makes sense to me to just use an ICC agreement statistic.Here's the rub: In addition to multiple samples of each type of site, there's potentially multiple visits to the actual sites. So for example, the data actually looks like:SourceID, Source, SensorA, 0.25, 0.34A, 0.17, 0.28A, 0.19, 0.45B, 0.06, 0.72C, 0.38, 0.26D, 0.89, 0.97D, 0.73, 0.85E, 0.17, 0.19I'd like to leverage those multiple visits, but of course now all the observations aren't independent, and there are some repeated measures. Is there a way to handle ICC with repeated measures?","Creater_id":5836,"Start_date":"2016-08-11 16:55:02","Question_id":229443,"Tags":["repeated-measures","intraclass-correlation","agreement-statistics"],"Answer_count":1,"Last_activity":"2016-08-24 13:51:44","Link":"http://stats.stackexchange.com/questions/229443/icc-agreement-with-repeated-measures","Creator_reputation":14354}
{"_id":{"$oid":"5837a576a05283111e4d374f"},"View_count":150,"Display_name":"HotMilo23","Question_score":4,"Question_content":"This is from my college project management course. Reading through an example question here it says:  When estimating in parts, the total error will be less than the sum  of the part errors.This makes sense to me.  For a 1000-hour job with estimating accuracy of ± 50%, the estimate  range is from 500 to 1500 hours.This also makes sense  If the estimate is independently made in 25 parts, each with 50%  error, the total would be 1000 hours, as before and the estimate range  would be from 900 to 1100 hoursOkay, How did we get that?  To combine independently-made estimates      Add the estimated values.  Combine the variances (squares) of the errors.  Cool, following you...  With 25 estimates for a 1000-hour job      Each estimate averages 40 hours  The standard deviation is 50%, or 20 hours  Here's where it stops making sense. It goes on to take the square root of sum of the variances which makes sense, but where did he get the standard deviation being 50%?I could be missing something here?","Creater_id":25257,"Start_date":"2013-05-04 23:45:51","Question_id":58155,"Tags":["self-study","estimation","error"],"Answer_count":3,"Last_activity":"2016-08-24 13:48:06","Link":"http://stats.stackexchange.com/questions/58155/combined-individual-error","Creator_reputation":24}
{"_id":{"$oid":"5837a576a05283111e4d375e"},"View_count":3267,"Display_name":"Mary","Question_score":0,"Question_content":"In my Masters thesis I do a mediation analysis with Multiple Regression Analysis. So I make use of the Baron \u0026amp; Kenny method for testing mediation. I also want to control for some variables. For example I want to know if gender has an effect on the mediation effect. What I did was to split the data file(women/ men) (I'm using SPSS 21) and then run the mediation analysis again separately for the two groups. However, if I want to control for age, splitting the file makes no sense. Is there any analytic procedure to test the effect age or any other non categorical variable has on the mediation effect? And if this should be the case, how can I run this analysis in SPSS?Thank you very much for your help.","Creater_id":27133,"Start_date":"2013-06-20 23:43:03","Question_id":62247,"Tags":["spss","multiple-regression","mediation"],"Answer_count":1,"Last_activity":"2016-08-24 13:10:23","Link":"http://stats.stackexchange.com/questions/62247/how-can-i-test-control-variables-within-a-mediation-analysis","Creator_reputation":1}
{"_id":{"$oid":"5837a576a05283111e4d376b"},"View_count":246,"Display_name":"dimid","Question_score":3,"Question_content":"My problem: The input data is a corpus of short documents (a few sentences each). In each document some expressions need to be classified to categories. A document must contain some categories (each expression has a single label), and the rest are optional. The task: given such an expression and its surrounding words, classify its category.As a solution I thought to convert my vocabulary words to vectors using word2vec, and then apply some multi-class classifier.Is there any classifier which is a particularly good fit to word2vec's output? I thought using svm, is there a recommended kernel?","Creater_id":80956,"Start_date":"2015-10-21 02:13:35","Question_id":177955,"Tags":["machine-learning","classification","natural-language","word2vec"],"Answer_count":2,"Last_activity":"2016-08-24 13:04:31","Link":"http://stats.stackexchange.com/questions/177955/multi-class-classification-with-word2vec","Creator_reputation":125}
{"_id":{"$oid":"5837a576a05283111e4d3779"},"View_count":53,"Display_name":"Paula","Question_score":0,"Question_content":"In this paper the authors list advantages of SMC. One of them is:  Unlike MCMC, SMC particles are uncorrelated and do not require the  determination of a burn-in period or assessment of convergence.However, I believe that they are not uncorrelated after the resampling. Could someone help me understand why they would be uncorrelated?","Creater_id":126827,"Start_date":"2016-08-24 03:28:58","Question_id":231447,"Tags":["correlation","monte-carlo","resampling"],"Answer_count":1,"Last_activity":"2016-08-24 12:56:44","Link":"http://stats.stackexchange.com/questions/231447/are-smc-samples-uncorrelated","Creator_reputation":38}
{"_id":{"$oid":"5837a576a05283111e4d3786"},"View_count":12752,"Display_name":"Behacad","Question_score":13,"Question_content":"It seems fairly common to describe Cronbach's alpha values as follows:α ≥ 0.9   Excellent0.7 ≤ α \u0026lt; 0.9 Good0.6 ≤ α \u0026lt; 0.7 Acceptable0.5 ≤ α \u0026lt; 0.6 Poorα \u0026lt; 0.5   UnacceptableWhere do these values come from? I cannot find an original research article describing these. Edit: I am 90% sure its merely based on convention and there is no classic research article outlining these.","Creater_id":3262,"Start_date":"2013-09-17 09:34:41","Question_id":70274,"Tags":["reliability","psychometrics","cronbachs-alpha"],"Answer_count":3,"Last_activity":"2016-08-24 12:56:39","Link":"http://stats.stackexchange.com/questions/70274/where-do-the-descriptors-for-cronbachs-alpha-values-come-from-e-g-poor-exce","Creator_reputation":2520}
{"_id":{"$oid":"5837a576a05283111e4d3795"},"View_count":74,"Display_name":"lacerbi","Question_score":4,"Question_content":"What are state-of-the-art alternatives to Gaussian Processes (GP) for nonparametric nonlinear regression with prediction uncertainty, when the size of the training set starts becoming prohibitive for vanilla GPs, but it is still not very large?Details of my problem are:input space is low-dimensional (, with )output is real-valued ()training points are , about a order of magnitude larger than what you could deal with standard GPs (without approximations)the function  to approximate is a black-box; we can assume continuity and a relative degree of smoothness (e.g., I would use a Matérn covariance matrix with  for a GP)for each queried point, the approximation needs to return mean and variance (or analogous measure of uncertainty) of the predictionI need the method to be retrainable relatively fast (of the order of seconds) when one or a few new training points are added to the training setAny suggestion is welcome (a pointer/mention to a method and why you think it'd work is enough). Thank you!","Creater_id":80479,"Start_date":"2016-08-05 07:53:20","Question_id":228452,"Tags":["regression","machine-learning","bayesian","nonparametric","gaussian-process"],"Answer_count":1,"Last_activity":"2016-08-24 12:48:10","Link":"http://stats.stackexchange.com/questions/228452/nonparametric-nonlinear-regression-with-prediction-uncertainty-besides-gaussian","Creator_reputation":1737}
{"_id":{"$oid":"5837a576a05283111e4d37a2"},"View_count":363,"Display_name":"AdamO","Question_score":6,"Question_content":"What does it mean to calibrate survey weights?Also, what are other definitions of calibration in statistics? I have heard it used in several contexts, particularly risk prediction (referring to whether the total number of predicted events in a cohort is statistically consistent with the observed number events).Is there a grand, unifying notion of calibration in statistics?Our wiki on calibration scratches the surface, or perhaps begs the question. ","Creater_id":8013,"Start_date":"2015-07-27 10:25:33","Question_id":163414,"Tags":["survey","terminology","calibration","survey-weights"],"Answer_count":4,"Last_activity":"2016-08-24 12:34:51","Link":"http://stats.stackexchange.com/questions/163414/what-is-calibration","Creator_reputation":17364}
{"_id":{"$oid":"5837a576a05283111e4d37b2"},"View_count":455,"Display_name":"BGreene","Question_score":2,"Question_content":"I am attempting to explain a dichotomous outcome variable using a large set of continuous valued sensor-derived variables. Many of these variables are highly mutually correlated, some are based on solid physical concepts (and therefore easier to interpret) others are more abstract and difficult to explain.I am attempting to examine the association of these sensor variables with the outcome (i.e. which variables or combination of variables can best explain the dichotomous outcome variable?) a secondary objective is to determine if a subset of these variables can be used to explain the dichotomous outcome.I wish to avoid using a stepwise fitting procedure (due to the danger of overfitting and, a feeling that the more easily interpretable variables should be given preference in the model over highly correlated but less easily interpretable variables). In short, I am looking for the true associations rather than noisy surrogates using correlated but less globally informative variables. To avoid multicollinearity in the analyses, I reduced the number of variables using logistic regression by block analysis. Sensor derived variables were grouped by type into blocks. The dichotomized outcome variable was used as the dependent variable in each sub-group. Working with each block, I performed a logistic regression on each independent variable and only those which were significant (α \u0026lt; 0.05) were retained in each block. Through this procedure I excluded all non significant variables from the analyses for the final model.I then generated a final logistic regression model using the results of each sub-group analysis.   I would great appreciate opinions as to whether this is a valid approach? Can the odds ratios from the final logistic regression model can be used to interpret the associations of the included sensor variables with the outcome variable in the manner of a hypothesis test?","Creater_id":11030,"Start_date":"2013-02-27 09:38:49","Question_id":50960,"Tags":["regression","hypothesis-testing","logistic"],"Answer_count":1,"Last_activity":"2016-08-24 12:31:15","Link":"http://stats.stackexchange.com/questions/50960/exploratory-analysis-regression-model-with-mutually-correlated-predictors-to-ex","Creator_reputation":1576}
{"_id":{"$oid":"5837a576a05283111e4d37bf"},"View_count":169,"Display_name":"Fiery Chicken","Question_score":0,"Question_content":"I have 3 numeric variables , , and . Specifically  ,  are tachometer readings. I am given  and  and I need to predict  (the velocity of a vehicle). I have built a model that uses standard methods (random forest) and this achieves a decent () accuracy. It achieves this despite the dependence of my  values.However I have since discovered that there is a function     which has a  accuracy (higher than my model at current).So is there a way I can use this function to improve the accuracy of my classifier. Baring in mind the fact that I never actually gather a value for velocity, I can only ever predict it.I thought that maybe using lagged variable might work, but since  in decidedly non-linear I am not sure if the model would be able to cope, and it seems slightly lacking in elegance.My next thought would be to add a feature of  or  but again since  is non linear this may not work.My last idea would be to use something like the Kalman filter to join both the prediction based on , and the prediction based on  but I feel this may lead to error propagation.So my questions are: Does what I am doing have an actual name (i.e what could I type into google to get either papers or other questions about this topic)?Would any of these approaches work?Does anyone have any better ideas?","Creater_id":125346,"Start_date":"2016-08-20 22:59:03","Question_id":230936,"Tags":["regression","machine-learning","time-series"],"Answer_count":1,"Last_activity":"2016-08-24 12:28:42","Link":"http://stats.stackexchange.com/questions/230936/predicting-time-series-data-with-non-time-data","Creator_reputation":53}
{"_id":{"$oid":"5837a576a05283111e4d37cc"},"View_count":54,"Display_name":"tumbleweed","Question_score":0,"Question_content":"I have a pandas dataframe with several columns, for example let's say:   A   B   C   D(labels)   45  88  44  0   62  34   2  1   85  65  11  1   74  43  42  1   90  38  34  0        ...    0  94  45  1   58  23  23  0How can I detect which columns are useless and which columns are useful to a machine learning model?. I all ready tried several methods like PCA, removing features with low variance, univariate feature selection with a chi criteria, etc. However none of them seems to work since the performance of my classifier still is low. I also tried to create more features (add more columns to the feature matrix) and they decreased the performance of my classifier, thus is there anyway to spot which columns are useless?.","Creater_id":71919,"Start_date":"2016-08-23 23:20:25","Question_id":231415,"Tags":["machine-learning","feature-selection","python","feature-construction","pandas"],"Answer_count":1,"Last_activity":"2016-08-24 12:20:05","Link":"http://stats.stackexchange.com/questions/231415/how-to-spot-and-remove-unimportant-columns-in-a-dataframe","Creator_reputation":36}
{"_id":{"$oid":"5837a576a05283111e4d37d8"},"View_count":19,"Display_name":"SKM","Question_score":0,"Question_content":"I have training data  that are the observations and consists of  rows as examples and  columns of variables(features). Matrix  is and hidden through a transformation matrix .But, the values in  are binary. Since,  is unknown the problem can be formulated as latent variable estimation. If  then what is the conditional distribuion ? is the model and assuming error to be  Gaussian distributed of unknown mean and variance.The objective is to estimate .I can apply the Expectation Maximization algorithm which is an iterative method to solve the Maximum Likelihood problem.The prior pdf   \\begin{align}P_x(x) \u0026amp;= N(0,\\sigma_x^2\\mathbf{I}) \\\\\\end{align}\\begin{align}Posterior, P(X|Y) = P(Y|X)P(X)\\end{align}If the random variables take values in binary, then the distribution is probably  would be Bernoulli but I am not sure. It shall be immensely helpful if the likelihood expression is provided. Thank you ","Creater_id":21160,"Start_date":"2016-08-22 15:30:13","Question_id":231184,"Tags":["probability","conditional-probability"],"Answer_count":0,"Last_activity":"2016-08-24 12:10:13","Link":"http://stats.stackexchange.com/questions/231184/what-is-the-conditional-distribution-for-this-model","Creator_reputation":99}
{"_id":{"$oid":"5837a576a05283111e4d37da"},"View_count":17,"Display_name":"tiantianchen","Question_score":0,"Question_content":"I have two independently conducted surveys measuring positive or negative response from samples (respondents) drawn from the same target population. The two surveys were conducted in two years, samples from both surveys were independently and randomly selected. However, a number of respondents happen to be selected in both surveys. Now I would like to test the proportion of positive response (marginal probability of the contingency table) between the two survey years.My question is:1) Do I need to take into account that some respondents have repeated measurements in both surveys? The overlapping respondents across years should not be an uncommon thing in sampling, however, I can not find any literature about how to or not to deal with it. 2) I could test the survey/or year effect using a mixed model with respondent as random intercept. However, in this case, I am just testing the pure year effect (e.g. a bad or good year that causes the changes in positive rate), rather than the population based marginal probability.3) McNemar's test can test the marginal probability, however, in my case, only part of the samples are overlapped ","Creater_id":13702,"Start_date":"2016-08-24 12:01:28","Question_id":231528,"Tags":["sampling","survey"],"Answer_count":0,"Last_activity":"2016-08-24 12:07:20","Link":"http://stats.stackexchange.com/questions/231528/how-to-test-the-marginal-probability-for-two-independent-survey-years-with-large","Creator_reputation":610}
{"_id":{"$oid":"5837a576a05283111e4d37dc"},"View_count":33,"Display_name":"Great38","Question_score":4,"Question_content":"Normally the null hypothesis for a f-test in ANOVA is for a comparison of  groups.How do I alter the f-statistic so that the null hypothesis is ?When comparing two groups with a t-test this is simple:Can this be done for an f-test?","Creater_id":122545,"Start_date":"2016-08-23 07:49:41","Question_id":231298,"Tags":["hypothesis-testing","anova","f-test"],"Answer_count":0,"Last_activity":"2016-08-24 11:58:42","Link":"http://stats.stackexchange.com/questions/231298/how-to-modify-the-f-test-null-hypothesis-in-anova","Creator_reputation":111}
{"_id":{"$oid":"5837a576a05283111e4d37de"},"View_count":141,"Display_name":"hxd1011","Question_score":2,"Question_content":"The tricky thing of manually implement optimization algorithm is that, even there are some errors, such as wrong gradient, the algorithm still can work in some way, i.e., decrease the objective, and even find the optimal parameters.I am manually implementing gradient boosting algorithm (gradient descent), can I checking the correct implementation for gradient boosting algorithm by looking at if the loss is monotonically decreasing?For example, I am plotting objective function for  iterations in left subplot, and plotting the diff(L_trace)\u0026gt;0 in the right subplot.There are 2 cases in right sub-plot where the objective is not monotonically decreasing, so, can we know something wrong with the algorithm?","Creater_id":113777,"Start_date":"2016-08-23 22:57:08","Question_id":231410,"Tags":["machine-learning","optimization","gradient-descent","boosting","gradient"],"Answer_count":3,"Last_activity":"2016-08-24 11:54:13","Link":"http://stats.stackexchange.com/questions/231410/can-i-checking-the-correct-implementation-for-gradient-descent-algorithm-by-look","Creator_reputation":4423}
{"_id":{"$oid":"5837a576a05283111e4d37ed"},"View_count":20,"Display_name":"Daniel Yefimov","Question_score":0,"Question_content":"Here we see the derivation of square sums between and within groups.The derivation of  I understood(It takes this form because , where  is the differential effect of ith treatment.(from  and But how do we get in  the ? Can you please explain the deravation of this formula?EDITED: i added Lemma A. I suppose, that we got  beacause  doesn't contain  (only ). Am i right?","Creater_id":108018,"Start_date":"2016-08-24 11:35:36","Question_id":231522,"Tags":["anova"],"Answer_count":0,"Last_activity":"2016-08-24 11:45:23","Link":"http://stats.stackexchange.com/questions/231522/expected-values-of-square-sums-between-and-within-groupsanalysis-of-variance","Creator_reputation":267}
{"_id":{"$oid":"5837a576a05283111e4d37ef"},"View_count":33,"Display_name":"Shieryn","Question_score":0,"Question_content":"I was following the ACF and PACF Plot but it didn't fulfill paramater significance, white noise assumption and normality assumption. So I did model identification for several times to get the model which fulfill those assumptions and didn't . and then I pick an ARIMA Seasonal model (2,1,0)(0,0,4)7 and it already fulfilled those assumptions. As You can see, order P = 2. So it has to cut off at lag 2 in PACF plot. but in PACF lag 2 didn't cut off. Is it ok to force the model based on those assumptions? Please complete Your answer with references. Thankyou :) ","Creater_id":92551,"Start_date":"2016-08-23 10:54:19","Question_id":231328,"Tags":["time-series","predictive-models","modeling","arima","diagnostic"],"Answer_count":1,"Last_activity":"2016-08-24 11:41:23","Link":"http://stats.stackexchange.com/questions/231328/define-arima-model-p-d-q-based-on-parameter-significance-whitenoise-and-norma","Creator_reputation":15}
{"_id":{"$oid":"5837a576a05283111e4d37fc"},"View_count":25,"Display_name":"zkurtz","Question_score":2,"Question_content":"I have a simple problem but don't know how to search for its solutions in the literature.  What keywords do you associate with this?:A man walks into a bar.  He stays a while.  He leaves.  Hours later, he enters the bar again.  He stays a while.  He leaves. .... [there is no punchline].  In general, the duration of each visit and the time between visits is only semi-regular. The man is the bar's sole client.The bar owner wants to replace the bar tender with a robot, but he's not very good at robotics.  The best he can do is make a robot that serves a drink every t hours, starting from some time .  If the bar's sole client happens to be there when the robot serves the drink, the bar owner earns C dollars*.  Otherwise, the bar owner loses D dollars. Given historical data on the bar's sole client entering and leaving the bar, how should the bar tender choose  and  in a way that maximizes his expected future earnings?*Caveat:  The client is totally smashed after a single drink, and will refuse any further drinks offered to him in the course of the same visit. Thus, the bar owner loses D dollars whenever the robot serves a second drink within the same visit.In the general case  and  can be different, but even a solution that assumes  (or one that only notionally maximizes profit) could be of interest.","Creater_id":27765,"Start_date":"2016-08-24 10:20:08","Question_id":231509,"Tags":["time-series","interval"],"Answer_count":0,"Last_activity":"2016-08-24 10:42:17","Link":"http://stats.stackexchange.com/questions/231509/on-again-off-again-time-series","Creator_reputation":987}
{"_id":{"$oid":"5837a576a05283111e4d37fe"},"View_count":256,"Display_name":"Alex Brown","Question_score":5,"Question_content":"According to the Rao-Blackwell theorem, if statistic  is a sufficient and complete for , and , then  is a  uniformly minimum-variance unbiased estimator (UMVUE).I am wondering how to justify that an unbiased estimator is a UMVUE: if  is not sufficient, can it be a UMVUE? if  is not complete, can it be a UMVUE? If  is not sufficient or complete, can it be a UMVUE?","Creater_id":86054,"Start_date":"2015-08-16 08:54:19","Question_id":167373,"Tags":["mathematical-statistics","umvue","rao-blackwell"],"Answer_count":2,"Last_activity":"2016-08-24 10:23:59","Link":"http://stats.stackexchange.com/questions/167373/what-is-the-necessary-condition-for-a-unbiased-estimator-to-be-umvue","Creator_reputation":26}
{"_id":{"$oid":"5837a576a05283111e4d380c"},"View_count":133,"Display_name":"shn","Question_score":2,"Question_content":"Is there any interesting problem in the area of \"Document Image Analysis and Retrieval\" which by nature needs an online/incremental clustering process ? The problem may be in the context of \"Logical Structure Analysis\", or \"Document Layout Analysis\" to identify regions of interest in a scanned page, or any other related topics. What matters is that the considered problem naturally needs an online/incremental clustering. Do you have any ideas or suggestions about such problems ?Note: the considered document images are actually a scanned administrative documents","Creater_id":8114,"Start_date":"2012-09-11 01:47:02","Question_id":36082,"Tags":["clustering","image-processing","online"],"Answer_count":1,"Last_activity":"2016-08-24 10:20:56","Link":"http://stats.stackexchange.com/questions/36082/document-image-analysis-and-retrieval-with-online-incremental-clustering","Creator_reputation":547}
{"_id":{"$oid":"5837a576a05283111e4d3818"},"View_count":17316,"Display_name":"Ram Sharma","Question_score":40,"Question_content":"I am trying to understand difference between different resampling methods (Monte Carlo simulation, parametric bootstrapping, non-parametric bootstrapping, jackknifing, cross-validation, randomization tests, and permutation tests) and their implementation in my own context using R.Say I have the following situation – I want to perform ANOVA with a Y variable (Yvar) and X variable (Xvar). Xvar is categorical. I am interested in the following things:(1) Significance of p-values – false discovery rate(2) effect size of Xvar levels  Yvar \u0026lt;- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)Xvar \u0026lt;- c(rep(\"A\", 5),  rep(\"B\", 5),    rep(\"C\", 5))mydf \u0026lt;- data.frame (Yvar, Xvar)Could you gel me to explain the  sampling differences with explicit worked examples how these resampling method work?Edits:Here are my attempts:Bootstrap 10 bootstrap samples, sample number of samples with replacement, means that samples can be repeated   boot.samples \u0026lt;- list()for(i in 1:10) {   t.xvar \u0026lt;- Xvar[ sample(length(Xvar), length(Xvar), replace=TRUE) ]   t.yvar \u0026lt;- Yvar[ sample(length(Yvar), length(Yvar), replace=TRUE) ]   b.df \u0026lt;- data.frame (t.xvar, t.yvar)    boot.samples[[i]] \u0026lt;- b.df }str(boot.samples) boot.samples[1]Permutation:10 permutation samples, sample number of samples without replacement permt.samples \u0026lt;- list()    for(i in 1:10) {       t.xvar \u0026lt;- Xvar[ sample(length(Xvar), length(Xvar), replace=FALSE) ]       t.yvar \u0026lt;- Yvar[ sample(length(Yvar), length(Yvar), replace=FALSE) ]       b.df \u0026lt;- data.frame (t.xvar, t.yvar)        permt.samples[[i]] \u0026lt;- b.df     }    str(permt.samples)    permt.samples[1]Monte Caro Simulation Although the term \"resampling\" is often used to refer to any repeated random or pseudorandom sampling simulation, when the \"resampling\" is done from a known theoretical distribution, the correct term is \"Monte Carlo\" simulation.I am not sure about all above terms and whether my above edits are correct. I did find some information on jacknife but I could not tame it to my situation. ","Creater_id":7397,"Start_date":"2014-06-19 10:59:02","Question_id":104040,"Tags":["r","bootstrap","resampling","jackknife","permutation-test"],"Answer_count":2,"Last_activity":"2016-08-24 10:17:58","Link":"http://stats.stackexchange.com/questions/104040/resampling-simulation-methods-monte-carlo-bootstrapping-jackknifing-cross","Creator_reputation":973}
{"_id":{"$oid":"5837a576a05283111e4d3826"},"View_count":83,"Display_name":"she\u0026#223;","Question_score":3,"Question_content":"I want to do a permutation test for a command where I'm using weights. E.g.: permute treatment _b[treatment]: reg y x treatment [pweight=w]permute doesn't allow for weights, but can be forced to ignore that. The help-file only has the vague statement that: \"permute is not suited for weighted estimation, thus permute should not be used with weights or svy\".Is anyone here able to explain why weights might be problematic for permutation tests?","Creater_id":83571,"Start_date":"2016-08-19 07:34:44","Question_id":230720,"Tags":["stata","weighted-regression","permutation-test"],"Answer_count":1,"Last_activity":"2016-08-24 10:17:24","Link":"http://stats.stackexchange.com/questions/230720/statas-permute-and-weighted-regression","Creator_reputation":391}
{"_id":{"$oid":"5837a576a05283111e4d3833"},"View_count":20237,"Display_name":"Henrik","Question_score":43,"Question_content":"I have data from an experiment that I analyzed using t-tests. The dependent variable is interval scaled and the data are either unpaired (i.e., 2 groups) or paired (i.e., within-subjects). E.g. (within subjects):x1 \u0026lt;- c(99, 99.5, 65, 100, 99, 99.5, 99, 99.5, 99.5, 57, 100, 99.5,         99.5, 99, 99, 99.5, 89.5, 99.5, 100, 99.5)y1 \u0026lt;- c(99, 99.5, 99.5, 0, 50, 100, 99.5, 99.5, 0, 99.5, 99.5, 90,         80, 0, 99, 0, 74.5, 0, 100, 49.5)However, the data are not normal so one reviewer asked us to use something other than the t-test. However, as one can easily see, the data are not only not normally distributed, but the distributions are not equal between conditions:Therefore, the usual nonparametric tests, the Mann-Whitney-U-Test (unpaired) and the Wilcoxon Test (paired), cannot be used as they require equal distributions between conditions. Hence, I decided that some resampling or permutation test would be best.Now, I am looking for an R implementation of a permutation-based equivalent of the t-test, or any other advice on what to do with the data.  I know that there are some R-packages that can do this for me (e.g., coin, perm, exactRankTest, etc.), but I don't know which one to pick. So, if somebody with some experience using these tests could give me a kick-start, that would be ubercool.UPDATE: It would be ideal if you could provide an example of how to report the results from this test.","Creater_id":442,"Start_date":"2011-01-10 05:10:29","Question_id":6127,"Tags":["r","t-test","nonparametric","permutation-test"],"Answer_count":6,"Last_activity":"2016-08-24 10:12:32","Link":"http://stats.stackexchange.com/questions/6127/which-permutation-test-implementation-in-r-to-use-instead-of-t-tests-paired-and","Creator_reputation":7692}
{"_id":{"$oid":"5837a576a05283111e4d3845"},"View_count":4021,"Display_name":"user2886545","Question_score":10,"Question_content":"I have two datasets and I would like to know if they are significantly different or not (this comes from \"Two groups are significantly different? Test to use\").I decided to use a permutation test, doing the following in R:permutation.test \u0026lt;- function(coding, lncrna) {    coding \u0026lt;- coding[,1] # dataset1    lncrna \u0026lt;- lncrna[,1] # dataset2    ### Under null hyphotesis, both datasets would be the same. So:    d \u0026lt;- c(coding, lncrna)    # Observed difference    diff.observed = mean(coding) - mean(lncrna)    number_of_permutations = 5000    diff.random = NULL    for (i in 1:number_of_permutations) {        # Sample from the combined dataset        a.random = sample (d, length(coding), TRUE)        b.random = sample (d, length(lncrna), TRUE)        # Null (permuated) difference        diff.random[i] = mean(b.random) - mean(a.random)    }    # P-value is the fraction of how many times the permuted difference is equal or more extreme than the observed difference    pvalue = sum(abs(diff.random) \u0026gt;= abs(diff.observed)) / number_of_permutations    pvalue}Nevertheless, p-values should not be 0 according to this paper: http://www.statsci.org/smyth/pubs/permp.pdfWhat do you recommend me to do? Is this way to calculate the p-value:pvalue = sum(abs(diff.random) \u0026gt;= abs(diff.observed)) / number_of_permutationsa good way? Or is it better doing the following?pvalue = sum(abs(diff.random) \u0026gt;= abs(diff.observed)) + 1 / number_of_permutations + 1","Creater_id":52649,"Start_date":"2014-07-24 06:53:18","Question_id":109207,"Tags":["p-value","permutation-test"],"Answer_count":2,"Last_activity":"2016-08-24 10:12:24","Link":"http://stats.stackexchange.com/questions/109207/p-values-equal-to-0-in-permutation-test","Creator_reputation":151}
{"_id":{"$oid":"5837a576a05283111e4d3853"},"View_count":24,"Display_name":"L. Blanc","Question_score":2,"Question_content":"We have n datasets, all in the same format, managed by separate organizations. For legal/privacy reasons, the raw data cannot be retrieved from their servers. We run classifier models against the individual datasets. Are there any legitimate approaches to combining the predicted class probabilities to make inferences about the larger population?Since this is a broad question, pointers to relevant literature would be greatly appreciated.","Creater_id":128785,"Start_date":"2016-08-24 07:03:05","Question_id":231474,"Tags":["classification"],"Answer_count":1,"Last_activity":"2016-08-24 10:10:37","Link":"http://stats.stackexchange.com/questions/231474/are-there-any-methodologies-for-combining-the-class-probabilities-from-a-classif","Creator_reputation":113}
{"_id":{"$oid":"5837a576a05283111e4d3860"},"View_count":43,"Display_name":"drRussClay","Question_score":2,"Question_content":"I've read the threads on some similar topics but I'm not positive that they address the issue that I'm having.  I am conducting analysis on a data set investigating the factors that predict whether or not patients have had an exam (binary outcome variable, so I'm using logistic regression).  One of the prediction factors is the implementation of an intervention (dichotomous, between-subjects predictor), and I want to see if this intervention was effective across pre-post measures (dichotomous, within-subjects predictor).  Patients are nested within doctors, and it is actually the doctors that are either part of the intervention or not.  Ultimately, I want to examine the interaction effect of the intervention x time, with the hypothesis that the change in proportion of exams completed (from pre to post) will be higher in the intervention group compared to the control group.  I'm trying to run this analysis in R using the glmer package, but I'm not quite sure how to set it up correctly.  If these are my variables:outcome (dichotomous)intervention (dichotomous, between subjects)time (dichotomous, within-subjects)Doctor (nominal, doctor name)ID (participant ID)I essentially want to do:outcome ~ intervention + time + intervention:timebut I want time to be within ID, and I also want to allow random intercepts and slopes by Doctor.I've tried:  outcome ~ 1|Doctor + intervention + time|ID + intervention:timeBut this gives me errors about a model frame, formula mismatch in R.  I can't share the data because of publicly identifiable information, but I'm stuck and was hoping somebody could help point me in the right direction.Thanks","Creater_id":128793,"Start_date":"2016-08-24 08:51:03","Question_id":231491,"Tags":["regression","logistic","mixed-model"],"Answer_count":1,"Last_activity":"2016-08-24 09:29:26","Link":"http://stats.stackexchange.com/questions/231491/mixed-effects-logistic-regression-with-repeated-measures-predictor","Creator_reputation":13}
{"_id":{"$oid":"5837a576a05283111e4d386d"},"View_count":23,"Display_name":"user136266","Question_score":0,"Question_content":"According to the screenshot above, it says that the prior covariance may not be positive definite. Can someone explain to me why the original formulation ensures that but the approximation doesn't. Thanks.","Creater_id":83975,"Start_date":"2016-08-24 09:19:43","Question_id":231498,"Tags":["gaussian-process","approximation"],"Answer_count":0,"Last_activity":"2016-08-24 09:19:43","Link":"http://stats.stackexchange.com/questions/231498/why-nystr%c3%b6m-approximation-of-prior-covariance-matrix-in-gaussian-process-does-no","Creator_reputation":150}
{"_id":{"$oid":"5837a576a05283111e4d386f"},"View_count":42,"Display_name":"dprofancik","Question_score":2,"Question_content":"I need help calculating the number of possible passwords with a given set of criteria. Here is the set of criteria:Passwords are case insensitive.Must be 6-14 characters.Must contain at least 1 letter and 1 number.Must not be equivalent to your current or 3 previous passwords.May not contain 9 or more numbers.May not be identical to your Username.May not repeat the same number or letter more than 3 times in a row.May not contain more than 3 sequential numbers or letters (such as '1234' or 'abcd') in a row.May contain special characters (such as @, %, \u0026amp;, #).Any help would be greatly appreciated.","Creater_id":113052,"Start_date":"2016-04-21 09:55:59","Question_id":208645,"Tags":["combinatorics"],"Answer_count":1,"Last_activity":"2016-08-24 09:17:53","Link":"http://stats.stackexchange.com/questions/208645/need-help-calculating-number-of-possible-passwords-with-given-criteria","Creator_reputation":13}
{"_id":{"$oid":"5837a576a05283111e4d387b"},"View_count":35,"Display_name":"rert588","Question_score":0,"Question_content":"I have this one question in statistics:There are five women and six men in a group. From this group a committee of 4 is to be chosen. In how many ways can the committee be formed if the committee is to have at least 3 women in it?I am not exactly sure on how to approach this question since I cannot find a way to determine how many object there will and how many times they will be taken so that I can use the combination formula. My worries is with the words at least 3 women, does this mean that there could be 4 women then?","Creater_id":117037,"Start_date":"2016-05-26 11:22:46","Question_id":214844,"Tags":["probability","self-study","combinatorics"],"Answer_count":2,"Last_activity":"2016-08-24 09:13:58","Link":"http://stats.stackexchange.com/questions/214844/pemutations-and-combinations-problem","Creator_reputation":103}
{"_id":{"$oid":"5837a576a05283111e4d3889"},"View_count":25,"Display_name":"Vikram Murthy","Question_score":1,"Question_content":"i am working on a consulting assignment where we are developing a system that generates credit scores for borrowers w.r.t. their eligibility for micro loans of different tenures and amounts (South Asia)Repayment rates for these borrowers differ based on geographies and seasons (given that post monsoons, rural populace have higher disposable incomes and pre monsoons most of the money is spent in procurements). I have a bunch of attributes and a fairly large amount of borrower data stretching over multiple years. A very basic approach (and a decent first pass) is to construct a decision tree (or use tree bagging approaches to avoid overfits) or use neural nets to train and test the data.Queriesa) all the approaches are going to give me a probabilistic classification of whether the borrower deserves a loan or not. Can i use different labels rather than a simple YES / NO ? For instance for a combination of some factors a borrower is eligible for a 4,000 - 8,000 rupee loan, while in other cases she is eligible to get a 8k-10k loan etc. I can simply code them as labels A.B and C or whatever and still be able to do a multi label classification right ? b) given that repayment is going to be heavily dependant on weather patterns (so if monsoons are good, chances of repayment on time are higher), can we combine the output of the ML algo with a monte carlo simulation of probability distribution of monsoon success / failure ? because the model might say a borrower has high odds of repayment but the algo DOESN'T know that the MET has forecasted 10% deficient rainfall","Creater_id":67436,"Start_date":"2016-08-24 08:52:03","Question_id":231493,"Tags":["classification","predictive-models","random-forest","monte-carlo"],"Answer_count":0,"Last_activity":"2016-08-24 08:52:03","Link":"http://stats.stackexchange.com/questions/231493/combine-machine-learning-with-monte-carlo","Creator_reputation":61}
{"_id":{"$oid":"5837a576a05283111e4d388b"},"View_count":15,"Display_name":"Shivi Bhatia","Question_score":0,"Question_content":"I am using Kolmogorov-Smirnov Test to check the logistic model fit performance along with the ROC Curve.   I have built 3 models with varied predictor variables and the accuracy is around 70%- 76% for the models (the higher the better).   Is there any threshold or a limit that we consider as a good fit for the model.  Please suggest.   ","Creater_id":79611,"Start_date":"2016-08-24 08:51:38","Question_id":231492,"Tags":["statistical-significance","predictive-models"],"Answer_count":0,"Last_activity":"2016-08-24 08:51:38","Link":"http://stats.stackexchange.com/questions/231492/kolmogorov-smirnov-test-for-model-performance","Creator_reputation":13}
{"_id":{"$oid":"5837a576a05283111e4d388d"},"View_count":43,"Display_name":"James Abbott","Question_score":0,"Question_content":"I've made box and whisker plots of how fish prices vary in a rural African market (as a way of detecting change in food security). My data are skewed to the left-and so my lower whiskers are right at the bottom of the axis. I tried log-transforming but it looks even worse. Is this the best way to present my data? Thanks.","Creater_id":127578,"Start_date":"2016-08-13 06:05:45","Question_id":229658,"Tags":["skewness","boxplot"],"Answer_count":2,"Last_activity":"2016-08-24 08:50:41","Link":"http://stats.stackexchange.com/questions/229658/boxplot-with-whiskers-close-to-zero","Creator_reputation":36}
{"_id":{"$oid":"5837a576a05283111e4d389b"},"View_count":27,"Display_name":"user_anon","Question_score":0,"Question_content":"Should one be able to find out the degrees of freedom of a given statistic?I liked this answer regarding degrees of freedom and I tried to apply it.Some examples:. As  is fixed,  are constants.It would finally turn into  and if we fixed , then  is fully determined and not free to vary. So, .. As  is fixed,  are constants. Let's say we fix  such that , and let's say . I turn out that again  is fully determined (Consider : calculus. So: ). So, again .First question: Am I right?Second question: Isn't all the time the same? I mean when we have only one statistic the number of degrees may be at most decreased by 1, no? How come in t-test for mean difference ? Is it possible to have just one equation and think of decreasing the freedom by ?","Creater_id":125475,"Start_date":"2016-08-24 08:20:26","Question_id":231483,"Tags":["hypothesis-testing","mathematical-statistics","t-test","degrees-of-freedom","inferential-statistics"],"Answer_count":1,"Last_activity":"2016-08-24 08:46:52","Link":"http://stats.stackexchange.com/questions/231483/given-a-statistic-find-out-its-degrees-of-freedom","Creator_reputation":102}
{"_id":{"$oid":"5837a576a05283111e4d38a8"},"View_count":3770,"Display_name":"JohnRos","Question_score":54,"Question_content":"The world of statistics was divided between frequentists and Bayesians. These days it seems everyone does a bit of both. How can this be? If the different approaches are suitable for different problems, why did the founding fathers of statistics did not see this? Alternatively, has the debate been won by Frequentists and the true subjective Bayesians moved to decision theory? ","Creater_id":6961,"Start_date":"2012-01-03 13:08:38","Question_id":20558,"Tags":["bayesian","frequentist","history","philosophical"],"Answer_count":6,"Last_activity":"2016-08-24 08:36:44","Link":"http://stats.stackexchange.com/questions/20558/where-did-the-frequentist-bayesian-debate-go","Creator_reputation":3506}
{"_id":{"$oid":"5837a576a05283111e4d38ba"},"View_count":349,"Display_name":"nostock","Question_score":7,"Question_content":"I am reading the (german) Applied Statistics and on page 140 as a consequence of the Kolmogorov axioms it is stated that if  one cannot conclude that  . Similarly if  one also cannot conclude that . Why is that?Also, if  this means that event A is almost never possible and if  will almost surely occur.I am having a bit of a trouble intuitively understanding the need for the above statements (almost surely or almost never) and why if  one cannot conclude that . [deleted edit]","Creater_id":9671,"Start_date":"2013-12-01 16:32:24","Question_id":78245,"Tags":["probability","kolmogorov-axioms"],"Answer_count":3,"Last_activity":"2016-08-24 08:35:33","Link":"http://stats.stackexchange.com/questions/78245/question-on-the-consequences-of-the-kolmogorov-axioms","Creator_reputation":537}
{"_id":{"$oid":"5837a576a05283111e4d38c9"},"View_count":18,"Display_name":"mackbox","Question_score":1,"Question_content":"Suppose I have got some models trained using the Bayesian methodology. Is it valid to compare the models with their prediction error of the test set just like in the Frequentist approach? ","Creater_id":73733,"Start_date":"2016-08-24 08:33:03","Question_id":231485,"Tags":["bayesian"],"Answer_count":0,"Last_activity":"2016-08-24 08:33:03","Link":"http://stats.stackexchange.com/questions/231485/can-i-apply-mean-prediction-error-to-assess-the-quality-of-my-model","Creator_reputation":128}
{"_id":{"$oid":"5837a576a05283111e4d38cb"},"View_count":1455,"Display_name":"Handwritten","Question_score":22,"Question_content":"Usually probability theory is taught with Kolgomorov's axioms. Do Bayesians also accept Kolmogorov's axioms?","Creater_id":61786,"Start_date":"2014-11-30 13:08:45","Question_id":126056,"Tags":["probability","bayesian","kolmogorov-axioms"],"Answer_count":2,"Last_activity":"2016-08-24 08:30:03","Link":"http://stats.stackexchange.com/questions/126056/do-bayesians-accept-kolmogorovs-axioms","Creator_reputation":111}
{"_id":{"$oid":"5837a576a05283111e4d38d9"},"View_count":8599,"Display_name":"BYS2","Question_score":21,"Question_content":"Can someone give a good rundown of the differences between the Bayesian and the frequentist approach to probability?From what I understand:The frequentists view is that the data is a repeatable random sample (random variable) with a specific frequency/probability (which is defined as the relative frequency of an event as the number of trials approaches infinity). The underlying parameters and probabilities remain constant during this repeatable process and that the variation is due to variability in  and not the probability distribution (which is fixed for a certain event/process).The bayesian view is that the data is fixed while the frequency/probability for a certain event can change meaning that the parameters of the distribution changes. In effect, the data that you get changes the prior distribution of a parameter which gets updated for each set of data.To me it seems that the frequentist approach is more practical/logical since it seems reasonable that events have a specific probability and that the variation is in our sampling.Furthermore, most data analysis from studies is usually done using the frequentist approach (i.e. confidence intervals, hypothesis testing with p-values etc) since it is easily understandable.I was just wondering whether anyone could give me a quick summary of their interpretation of bayesian vs frequentist approach including bayesian statistical equivalents of the frequentist p-value and confidence interval. In addition, specific examples of where 1 method would be preferable to the other is appreciated.","Creater_id":12347,"Start_date":"2012-07-05 07:41:27","Question_id":31867,"Tags":["probability","bayesian","frequentist"],"Answer_count":5,"Last_activity":"2016-08-24 08:27:23","Link":"http://stats.stackexchange.com/questions/31867/bayesian-vs-frequentist-interpretations-of-probability","Creator_reputation":530}
{"_id":{"$oid":"5837a576a05283111e4d38ea"},"View_count":45,"Display_name":"ertopd123","Question_score":2,"Question_content":"I have 4 variables for a panel of N metropolitan areas over T years:productivitynumber of patent applicationspopulationshare of graduates in the labor forceI wanted to regress log(productivity) over log(patents), log(population) and share of graduates, along with time \u0026amp; metropolitan area fixed effects.Of course, since all these variables are I(1) I thought about doing an ECM. However, when applying cointegration tests for panel data on these 4 variables (xtwest in Stata), the null of no-cointegration could not be rejected.What should be the proper procedure in that case ? Thank you","Creater_id":127120,"Start_date":"2016-08-09 05:36:45","Question_id":228974,"Tags":["panel-data","stata","fixed-effects-model","cointegration"],"Answer_count":1,"Last_activity":"2016-08-24 08:22:05","Link":"http://stats.stackexchange.com/questions/228974/regression-with-non-cointegrated-i1-series","Creator_reputation":11}
{"_id":{"$oid":"5837a576a05283111e4d38f7"},"View_count":42,"Display_name":"Davo","Question_score":1,"Question_content":"I am trying to do a Granger causality test. In the general form Granger causality analysis includes estimating the following equation:y_{t}=a_{1}y_{t-1}+a_{2}y_{t-2}+\\dotsc+a_{p}y_{t-p}+b_{1}x_{t-1}+b_{2}x_{t-2}+\\dotsc+b_{q}x_{t-q}+e_{t}.  It is not necessary that , so we can choose different values for  and  (using information criteria such as AIC or BIC). However, EViews forces , look here. Questions:Is it possible to choose different lag values  in Eviews? Doesn't the lack of the ability to choose a lag contradict the theoretical background of Granger causality?","Creater_id":126573,"Start_date":"2016-08-03 22:57:25","Question_id":228201,"Tags":["granger-causality","eviews"],"Answer_count":1,"Last_activity":"2016-08-24 07:59:48","Link":"http://stats.stackexchange.com/questions/228201/granger-causality-need-for-different-lag-lengths-for-x-and-y","Creator_reputation":6}
{"_id":{"$oid":"5837a576a05283111e4d3904"},"View_count":30,"Display_name":"J. R. C.","Question_score":0,"Question_content":"I have a model with 3 parameters that I would like to fit to my (unidimensional iid) data. The model is a Hidden Markov Model and therefore it is intrinsically probabilistic. It is fairly straightforward to simulate samples from it. However, given the functional form of the transition and emission probabilities, it is very troublesome to obtain the likelihood function.So I was thinking, what if I simulate  iid samples  from the model and then compare it with my sequence of  iid observations  with a two sample K-S test to check if they are drawn from the same distribution? Then I could find the set of parameters that minimize the  statistic coming from the K-S test. How does this compare to maximum likelihood? Would this method have any clear disadvantages? Is there a preferred size  of the simulated samples that would optimize the method according to ?","Creater_id":127493,"Start_date":"2016-08-24 07:15:05","Question_id":231476,"Tags":["maximum-likelihood","fitting","hidden-markov-model","kolmogorov-smirnov"],"Answer_count":1,"Last_activity":"2016-08-24 07:44:58","Link":"http://stats.stackexchange.com/questions/231476/two-sample-k-s-test-as-an-alternative-to-maximum-likelihood","Creator_reputation":21}
{"_id":{"$oid":"5837a576a05283111e4d3911"},"View_count":77,"Display_name":"sponge_knight","Question_score":7,"Question_content":"I hear the term bias being thrown around a lot in statistical literature. For example,   By using mean-wise imputation, we are adding bias to our estimate.Another example,  The bias-variance trade-off is an important subject when picking models.Are these the same \"bias\"?","Creater_id":46925,"Start_date":"2016-03-17 18:31:43","Question_id":202289,"Tags":["terminology","bias"],"Answer_count":2,"Last_activity":"2016-08-24 07:27:16","Link":"http://stats.stackexchange.com/questions/202289/are-there-two-definitions-of-the-word-bias","Creator_reputation":1512}
{"_id":{"$oid":"5837a576a05283111e4d391f"},"View_count":32,"Display_name":"HonzaB","Question_score":0,"Question_content":"I have two datasets - dataset A and B. Both datasets have same attributes. I'd like to take stratified sample out of B based on A.Let's say I want to have sample size 10%. If A has 15.000 rows, the stratified sample will have 1.500. If A's attribute X has average 0.5, I want my stratified sample to have average value of X ~0.5 as well etc.Usually I do stratification like this in Python:from sklearn.cross_validation import StratifiedShuffleSplitX = df[columns]y = targetsss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.2, random_state=0)for train_index, test_index in sss:    X_train, X_test = X.iloc[train_index], X.iloc[test_index]    y_train, y_test = y.iloc[train_index], y.iloc[test_index]But I don't really see how to do it sort of backwards.","Creater_id":88687,"Start_date":"2016-08-24 07:23:28","Question_id":231477,"Tags":["sampling","python","stratification"],"Answer_count":0,"Last_activity":"2016-08-24 07:23:28","Link":"http://stats.stackexchange.com/questions/231477/take-stratified-sample-based-on-another-dataset","Creator_reputation":233}
{"_id":{"$oid":"5837a576a05283111e4d3921"},"View_count":55,"Display_name":"Rebecca","Question_score":2,"Question_content":"I just recently started learning about principal component regression (PCR) and I'm wondering if it's possible to use both principal components and original variables as predictors of a given outcome (the outcome is binary, so I'll need to perform a logistic regression).  I have 34 predictor variables about perceptions of weather conditions, frequency and importance of land use and sea ice use, and land/sea ice travel behaviors and special travel equipment; given the large number of variables and strong correlations between certain variables, I'd like to run a PCA as a dimensionality reduction technique and aid with potential problems arising from multicollinearity.However, I also have a few sociodemographic predictor variables that I would like to keep in their original form (i.e., not include them in the PCA with the other predictors).Are there any problems with running a (logistic) regression analysis using both principal components and original variables as predictors? And, if this approach is alright, does anyone know of any studies/references using this approach?(Also, I am technically running a categorical/non-linear PCA in SPSS (CATPCA), but I'm assuming the answer to my question is the same regardless of whether a linear or non-linear PCA is being performed?)","Creater_id":128690,"Start_date":"2016-08-23 09:49:49","Question_id":231319,"Tags":["regression","categorical-data","pca","spss"],"Answer_count":0,"Last_activity":"2016-08-24 06:52:32","Link":"http://stats.stackexchange.com/questions/231319/principal-component-regression-pcr-with-some-of-the-original-predictors-left-o","Creator_reputation":11}
{"_id":{"$oid":"5837a576a05283111e4d3923"},"View_count":51,"Display_name":"Klapaucius","Question_score":0,"Question_content":"I was reading the paper by Gromping (2007) about variance decomposition.I was refering to page 141 (LMG) but I do not know what is meant in statistics by . Could you clarify this aspect?In addition do you know any good package for variance decomposition that follows the approach of Lindeman, Merenda and Gold?","Creater_id":108790,"Start_date":"2016-08-24 03:07:03","Question_id":231442,"Tags":["variance","variance-decomposition"],"Answer_count":1,"Last_activity":"2016-08-24 06:39:20","Link":"http://stats.stackexchange.com/questions/231442/variance-decomposition-svar","Creator_reputation":11}
{"_id":{"$oid":"5837a576a05283111e4d3930"},"View_count":119,"Display_name":"PolinLnd","Question_score":6,"Question_content":"I’m reading the book Quick Count and Election Observation (chapter 5). I’m interested in understanding the statistics used in Quick Counts. Quick Counts is a methodology for verifying official election results by counting (in parallel to the official counting) the vote counts of some polling places.The Quick Count works as follows: A certain sample size  is decided before hand, and the expected margin error is computed as well.In the election day,  of the polling places are sampled at random.The results of each sampled polling place (number of votes for each candidate) are reported and summed up.In order to check if the elections are flaw or not, the results of the sampling are compared to the official results.  (Note that the sampling unit is not voters but polling places, that is, aggregation of voters).  The proportion of votes is modeled as a binomial distribution with parameter . The objective is to sample that binomial distribution, and estimate    and find the confidence interval such that , were  is the sample mean and  the margin error. Usually,  is a design parameter (given by the user) and one has to find the smallest sample size  (number of polling places) such that the margin error is smaller than .The problem is similar to that of sampling a binomial distribution, but the samples are not independent since they votes are sampled in bulks (one for each polling place sampled).My concerns are regarding how the margin error and sample size are computed. According to the book linked above:First they compute the number of voters that should be sampled., where  is the total population of potential voters, and  is the z-score of a normal distribution at the desired confidence level.   is usually set to  (maximum variance of the binomial distribution).  (see example in page 73, and explanation in page 72)The number of polling places to sample  is determined by .The real margin error is computed (see page 66 and 77), where  is the heterogeneity (i.e., the square root of the variance).Q1: Why do they compute two margin errors  and . I understand that only , the margin error that is computed with the number of polling places, is correct. Do you think that is done this way in order to have an idea of the best-case margin error? Q2: Why not compute the sample size of polling place directly with? ()Polling places have different number of votes. In order to compute the expected proportion of votes for a candidate. There are two options (the book does not specify which one to use)Sum up the votes in all sampled polling places and compute the proportion. This would give to each vote the same weight in the proportion estimation. However, polling places with less votes will have less weight, and I’m afraid that the way to compute the margin error,  is not correct for this case. Q3: how would you compute the margin error in this case?Compute the proportion of votes for each sampled polling place, average those proportions. This would give each polling place the same weight, but different weight to each vote. In this case I think that  is computed correctly, but I think it does not make sense to give more weight to votes in polling places with few votes. Q4: Which option would you use to compute the expected proportion of votes 1) or 2). ","Creater_id":94645,"Start_date":"2016-08-12 05:18:32","Question_id":229521,"Tags":["confidence-interval","sampling","binomial","stratification","elections"],"Answer_count":0,"Last_activity":"2016-08-24 06:33:21","Link":"http://stats.stackexchange.com/questions/229521/election-fraud-detection-the-statistics-of-quick-count","Creator_reputation":176}
{"_id":{"$oid":"5837a576a05283111e4d3932"},"View_count":95,"Display_name":"Erosennin","Question_score":5,"Question_content":"In Poisson regression with an offset, like in this answer, @Hong Ooi writes   Your underlying random variable is still , but by dividing by   we've converted the LHS of the model equation to be a rate of events  per unit exposure. But this division also alters the variance of the  response, so we have to weight by  when fitting the model.Since the exposure  is accounted for when fitting the model, does that mean one can divide a (multiple) observation into multiple observation(s) with new exposure , where , without this affecting the variance of the parameter estimates ?Intuitively I would think that doing this would give me more \"observations\", which would decrease the variance of each  estimate, even though we weigh by  when fitting the model.EDIT: By dividing a observation into multiple observation I mean the following:","Creater_id":30589,"Start_date":"2016-08-22 06:07:16","Question_id":231086,"Tags":["generalized-linear-model","offset"],"Answer_count":0,"Last_activity":"2016-08-24 06:08:03","Link":"http://stats.stackexchange.com/questions/231086/poisson-regression-how-number-of-observations-and-the-offset-affects-variance-o","Creator_reputation":482}
{"_id":{"$oid":"5837a576a05283111e4d3934"},"View_count":28,"Display_name":"Sanyo Mn","Question_score":0,"Question_content":"Is it correct to say that the each value of a random variable correspond to an event (at least in the case of discrete random variables). These corresponding events are mutually exclusive and their probabilities add up to 1. I think these follow from the definition of a random variable as a function assigning a value to every element of the sample space. However, I could not find them in the textbooks, so I just want to ask here.Thanks","Creater_id":29475,"Start_date":"2016-08-24 06:01:16","Question_id":231465,"Tags":["random-variable","events"],"Answer_count":0,"Last_activity":"2016-08-24 06:01:16","Link":"http://stats.stackexchange.com/questions/231465/relationship-between-events-and-random-variables","Creator_reputation":88}
{"_id":{"$oid":"5837a576a05283111e4d3936"},"View_count":63,"Display_name":"Cherryl","Question_score":2,"Question_content":"If  and  using the moment generating function, what kind of distribution has random variable .So far as I know :M_X(t)=e^{-\\lambda(1-e^t)}=e^{-2(1-e^t)}M_Y(t)=e^{-\\lambda(1-e^t)}=e^{-3(1-e^t)}M_{2Y}(t)=e^{-\\lambda(1-e^t)}=e^{-3(1-e^{2t})}M_{X+Y}(t)=e^{-5(1-e^t)}SinceX+Y \\sim P(\\lambda_1+\\lambda_2)so I am guessing that  Z=X+2Y \\sim P(\\lambda_1+2\\lambda_2) but I can find it from the equationM_{X+2Y}(t)=e^{-2(1-e^t)} e^{-3(1-e^{2t})}=e^{(-2(1-e^t)-3(1-e^{2t})}I don't know how to rearrange this equation to get the distribution.How to get rid of \"\".","Creater_id":97622,"Start_date":"2016-08-24 03:35:32","Question_id":231449,"Tags":["self-study","poisson","mgf"],"Answer_count":0,"Last_activity":"2016-08-24 04:51:28","Link":"http://stats.stackexchange.com/questions/231449/mgf-of-poisson-z-x2y","Creator_reputation":33}
{"_id":{"$oid":"5837a576a05283111e4d3938"},"View_count":30,"Display_name":"Cyrine Ezzahra","Question_score":0,"Question_content":"I have a variable which represents the activation energy for machine.df_no_missing['P_ACT_KW'].describe()Out[30]:count     52.000000mean     157.166586std       26.373297min       89.21495325%      145.16840350%      155.86805675%      173.538194max      241.400000Name: P_ACT_KW, dtype: float64I would like to categorize these machine depending their energy activation value : high activation, medium activation or low activation.Have you an idea to help me how can I segment P_ACT_KW values in these 3 categories? ","Creater_id":74552,"Start_date":"2016-08-24 04:37:14","Question_id":231455,"Tags":["distributions"],"Answer_count":0,"Last_activity":"2016-08-24 04:46:23","Link":"http://stats.stackexchange.com/questions/231455/mean-quantile-and-other-way-to-analyze-a-variable-in-python","Creator_reputation":111}
{"_id":{"$oid":"5837a576a05283111e4d393a"},"View_count":14899,"Display_name":"Russell Gallop","Question_score":21,"Question_content":"I want to represent a variable as a number between 0 and 1. The variable is a non-negative integer with no inherent bound. I map 0 to 0 but what can I map to 1 or numbers between 0 and 1?I could use the history of that variable to provide the limits. This would mean I have to restate old statistics if the maximum increases. Do I have to do this or are there other tricks I should know about?","Creater_id":652,"Start_date":"2010-08-02 07:38:55","Question_id":1112,"Tags":["normalization"],"Answer_count":8,"Last_activity":"2016-08-24 03:52:21","Link":"http://stats.stackexchange.com/questions/1112/how-to-represent-an-unbounded-variable-as-number-between-0-and-1","Creator_reputation":208}
{"_id":{"$oid":"5837a576a05283111e4d394e"},"View_count":30,"Display_name":"Damon Williams","Question_score":1,"Question_content":"Suppose that the pdf for  is exponentially distributedf(x; θ) = \\frac{1}{θ}e^{−x/θ}, 0 ≤ x \u0026lt; ∞.Suppose we test n components and record the failure times .(a) Show that is an unbiased estimator of .(b) What is the variance of ?(c) It can be shown that  also has an exponential distributionwith parameter . That isg(z; θ) = \\frac{1}{θ/n}e^{−z/(θ/n)}(Do not show this). Use this to findanother unbiased estimator  of  and determine its variance.Parts (a) and (b) are quite straightforward. But I'm having trouble with part (c). I know how to show something is an unbiased estimator but I'm unsure how to find an unbiased estimator. Would appreciate any help.","Creater_id":128759,"Start_date":"2016-08-24 03:08:06","Question_id":231443,"Tags":["variance","exponential","unbiased-estimator","minimum"],"Answer_count":1,"Last_activity":"2016-08-24 03:39:44","Link":"http://stats.stackexchange.com/questions/231443/need-help-finding-an-unbiased-estimator","Creator_reputation":6}
{"_id":{"$oid":"5837a576a05283111e4d395b"},"View_count":50,"Display_name":"ReadBeard","Question_score":1,"Question_content":"I would like to be able to perform a sample size calculation for an Ordinal Logistic regression with mixed effects. The proposed design would have two different tests each with 5 different items, each participant does both tests and each item. (i.e. wide format data would be: ten columns of data - five for test 1 and five for test 2. Each row is a different participant). The dependent variable is ordered categorical with three levels (0,1,2). Reading information on GLMMs by Ben Bolker, Andrew Gelman, and others, I see that that it might be sensible to include 'test' as a fixed effect rather than random effect (not enough levels?), but with 'item' as a random effect? My question is in two parts:Is the correct model for this type of data either a series of binomial contrasts using logistic GLMM or is an ordinal logistic GLMM possible? I cannot find information on the ordinal GLMM model and an R implementation?Is it possible to perform a power calculation via simulation for an ordinal GLMM or alternatively, should I do a separate power calculation for each binomial contrast?","Creater_id":86393,"Start_date":"2016-08-23 02:47:44","Question_id":231236,"Tags":["logistic","mixed-model","sample-size","ordinal","glmm"],"Answer_count":1,"Last_activity":"2016-08-24 03:38:48","Link":"http://stats.stackexchange.com/questions/231236/sample-size-calculation-for-ordinal-logistic-glmm-in-r","Creator_reputation":21}
{"_id":{"$oid":"5837a576a05283111e4d3967"},"View_count":38,"Display_name":"rmccloskey","Question_score":0,"Question_content":"The papers which develop SMC (eg. [1]) often begin by describing SIS. The two terms, SMC and SIS, don't seem to be synonyms. But neither does SIS seem to be just one \"type\" of SMC method. So how exactly do SIS and SMC fit together? Is SIS the basis or foundation for SMC?[1] Del Moral, Pierre, Arnaud Doucet, and Ajay Jasra. \"Sequential Monte Carlo samplers.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68.3 (2006): 411-436.","Creater_id":68395,"Start_date":"2016-02-10 14:07:06","Question_id":194967,"Tags":["monte-carlo","importance-sampling","particle-filter"],"Answer_count":1,"Last_activity":"2016-08-24 03:20:02","Link":"http://stats.stackexchange.com/questions/194967/what-is-the-relationship-between-sequential-monte-carlo-smc-and-sequential-imp","Creator_reputation":103}
{"_id":{"$oid":"5837a576a05283111e4d3974"},"View_count":33,"Display_name":"S.Ramagokula krishnan","Question_score":0,"Question_content":"In SPPS, how does one interpret the last two symbols/letters 'E4' in an arithmetic mean? For example, mean monthly family income is shown as 3.79E4. What does E4 symbolise? How can we interpret it?","Creater_id":73031,"Start_date":"2016-08-23 22:25:58","Question_id":231407,"Tags":["spss","notation"],"Answer_count":1,"Last_activity":"2016-08-24 02:44:59","Link":"http://stats.stackexchange.com/questions/231407/how-does-one-interpret-the-last-two-symbols-letters-e4-in-an-arithmetic-mean","Creator_reputation":5}
{"_id":{"$oid":"5837a576a05283111e4d3981"},"View_count":26,"Display_name":"com","Question_score":0,"Question_content":"I have a text classification problem. There are 100 articles. Each article contains from 5 up  to 20 sentences (an article cannot be separated into sentences, we cannot train and test on sentence-level). I have two classifiers: classifier1 and classifier2. The macro-average F1 of classifier1 on 10-fold cross validation is higher that the macro-average F1 of classifier2. I want to identify whether this advantage is statistical significant. What is the correct way to calculate statistical significance?By far I found that the wilcoxon test can be compatible, however it requires at least 30 test sets (articles). One solution could be in every fold interchange the train and the test, such that the train is the smallest set, then divide 90 test articles into 30 sets by 3 article, and for current 10 train articles calculate macro-average F1 on each bunch of 3 articles, so as result we have 30 F1 values just for the current fold.But what to do with the other folds? As a final result should I get 300 F1 values and then check the statistical significance or calculate the average of 30 F1 values among all 10 folds and then check the statistical significance.","Creater_id":38347,"Start_date":"2016-08-24 02:42:27","Question_id":231439,"Tags":["machine-learning","classification","statistical-significance","natural-language"],"Answer_count":0,"Last_activity":"2016-08-24 02:42:27","Link":"http://stats.stackexchange.com/questions/231439/calculating-statistical-significance-with-cross-validation","Creator_reputation":150}
{"_id":{"$oid":"5837a576a05283111e4d3983"},"View_count":45,"Display_name":"benjimin","Question_score":1,"Question_content":"How do I calculate a statistic for one mixture component?My samples are drawn from a distribution which is an additive mixture of two overlapping component distributions. Given a probabilistic estimate for the labelling of all the data-points (e.g. sample #13 has a 4% plausibility of having arisen from component-B), how do I estimate e.g. the standard deviation of component A?BackgroundI considered a few potential approaches:Deterministically round the probabilistic labels, to produce a non-fuzzy labelling (e.g. assign sample #13 to component-A because this is its most probable classification). Then apply the ordinary standard deviation formula to the identified subset. The downside is this labelling will create ubrupt boundaries that poorly represent the true shapes of either distribution.A Monte Carlo approach: instantiate a non-fuzzy labelling via a random process (using the plausibilities as biases), calculate the standard deviation as before, repeat many times and take the mean. The downside is iterative computation.Find a formula that weights samples according to their probability of belonging to the sub-population. Wikipedia contrasts “frequency weights” (positive integers representing duplicated datapoints) or “reliability weights” (which seems to quantify uncertainty/imprecision in the measured value, rather than the uncertainty about the measurement itself). Should the probabilistic classification be treated as fractional frequencies?Apply a more robust estimator of dispersion (e.g. median absolute deviation), and if necessary convert this to standard deviation later.This kind of scenario occurs in physics (e.g. proportional counters for radiation particle spectroscopy). Picture a detector for some phenomenon of interest, but the detector lacks specificity (additionally detecting events from an unrelated mechanism), resulting in a spurious background that confounds with the signal. Using prior knowledge, how much new information is it possible to infer about the phenomenon of interest (somehow separating out the background noise)?","Creater_id":128602,"Start_date":"2016-08-23 06:21:59","Question_id":231277,"Tags":["clustering","standard-deviation","expectation-maximization","fuzzy","weights"],"Answer_count":1,"Last_activity":"2016-08-24 02:14:31","Link":"http://stats.stackexchange.com/questions/231277/probabilistically-weighted-variance","Creator_reputation":106}
{"_id":{"$oid":"5837a576a05283111e4d3990"},"View_count":39,"Display_name":"PaoloH","Question_score":0,"Question_content":"So basically I was trying to compare two models in R using the anova function, here is what my data looks like :I wanted to compare tose two plots (know if there was a statistical difference), in order to do so I was advised to compare two regression, one model where regression was applied to all the points I had and one model where in the regression model I took into account the fact variable.log(y) ~ poly(x, 3) * fact  vs log(y) ~ poly(x, 3) (First I tried to use y ~ poly(x,3), but the log(y) seemed graphically like a way better fit, this might be one of the mistakes I made)Using the Anova test would then have told me whether the models where statistically different.But when doing so, I had weird results. When I compared the two models, I had very low p-value (\u0026lt;2.2e-16) even though, they intuitively didn't seem that different.Just to make sure everything was working correctly, I wanted to know if the degree for the polynomial regression that used (3) was a good fit. So I did an ANOVA test on log(y) ~ poly(x, n) * fact  vs log(y) ~ poly(x, n+1) * fact. Until the values were not significant anymore, but it went on (I had p-values\u0026lt;2.2e-16) till n=10. When I made the same algorithm on the regessions without the * fact, it only got through n=3, which graphically seemed way more logical. I do know that I shouldn't only trust the graphical aspect of things, but I'm afraid I may be overfitting my models, chosing the wrong model, using the anova() function the wrong way.Here is the graphical aspect (regressions with fact taken into account) of things that made me wonder whether my results were accurate or not : (polynomial degrees are 6,9,10 in that order)Maybe the anova() function in R isn't as accurate for models with grouping variables which is why when I compare the models with * fact to the ones without it I always get a p-value which is \u0026lt;2.2e-16.I also believe I may have chosen a wrong model by doing a polynomial regression on my functions which is why all my results would be useless.I don't know if adding the actual data would be useful to anyone but if so I'll edit it in.","Creater_id":127240,"Start_date":"2016-08-24 02:05:18","Question_id":231435,"Tags":["r","regression","anova","modeling"],"Answer_count":0,"Last_activity":"2016-08-24 02:05:18","Link":"http://stats.stackexchange.com/questions/231435/are-these-anova-test-p-values-indicating-a-bad-model-choice","Creator_reputation":58}
{"_id":{"$oid":"5837a576a05283111e4d3992"},"View_count":9,"Display_name":"mackbox","Question_score":0,"Question_content":"If so, what will the likelihood in the AIC function be? Will it be the posterior predictive distribution? Thanks.  ","Creater_id":73733,"Start_date":"2016-08-24 02:01:06","Question_id":231434,"Tags":["bayesian","aic"],"Answer_count":0,"Last_activity":"2016-08-24 02:01:06","Link":"http://stats.stackexchange.com/questions/231434/can-i-use-aic-for-model-selection-when-the-models-are-estimated-with-the-bayesia","Creator_reputation":128}
{"_id":{"$oid":"5837a576a05283111e4d3994"},"View_count":66,"Display_name":"user3504466","Question_score":1,"Question_content":"I want to model a variable in the form of fit \u0026lt;- gam(y ~ x + s(z,k=-1,bs=?))To let the function decide the ideal number of knots, I am using k=-1 but I am not able to figure out what type of spline to use. I have added a plot below. Please advise or if possible direct me to sources where I can learn about it. Thanks.","Creater_id":58474,"Start_date":"2016-08-23 19:30:55","Question_id":231399,"Tags":["r","regression","gam","splines"],"Answer_count":1,"Last_activity":"2016-08-24 01:53:32","Link":"http://stats.stackexchange.com/questions/231399/how-to-determine-the-type-of-spline-in-gam","Creator_reputation":118}
{"_id":{"$oid":"5837a576a05283111e4d3997"},"View_count":10,"Display_name":"Greconomist","Question_score":0,"Question_content":"Assuming that I have the following data-set where RV is the Realised Variance, Returns^2 is the squared returns and Garch Norm/SSTD/NIV are is forecasted conditional variance by the GARCH with Normal, Skew Student and Normal Inverse Gaussian distribute error terms.   Index    RV          Returns^2       garch_norm   garch_sstd garch_inv08/10/2015  7.78E-05    4.94E-09    0.045823829 0.044235949 0.04361713909/10/2015  5.97E-05    7.53E-05    0.140594458 0.135747268 0.13454239410/10/2015  7.36E-08    2.64E-06    0.099717722 0.096310528 0.09540451512/10/2015  4.48E-05    6.95E-07    0.038480606 0.037120276 0.03671122613/10/2015  7.91E-05    3.99E-05    0.122535104 0.118033927 0.11620424814/10/2015  9.38E-05    0.000132559 0.147389345 0.142428238 0.141403577The Mincer Zarnowitz regression for e.g the Garch- Normal can be constructed asRV regressed on a constant and on the forecasted conditional variance, and subsequently we the assess the R^2. Equivalently the Patton Sheppard MZ regression is the same except for the fact that we take logarithmic values so that log(RV) ¬ c + log(garch_norm).How can we construct a forecast encompassing regression and a Dieblo Mariano test? Also there is a modified version of the Dieblod Mariano test by Harvey-Leybourne and Diebold.","Creater_id":112822,"Start_date":"2016-08-24 01:34:03","Question_id":231429,"Tags":["volatility-forecasting"],"Answer_count":0,"Last_activity":"2016-08-24 01:34:03","Link":"http://stats.stackexchange.com/questions/231429/construction-of-several-forecast-appraisal-techniques","Creator_reputation":13}
{"_id":{"$oid":"5837a576a05283111e4d3999"},"View_count":23,"Display_name":"rrrrf","Question_score":0,"Question_content":"i have this following situation. I have a dataset of four measurementoccasions. In the first three occasions the same test was introduced.This three timepoints are equidistant. The fourth data aqcuisition wassome years later. The time distance to the fourth measurement occasionis much longer, and another test, though a similiar psychologicalconstruct, was conducted.At first i tried to fit a model on the data of the first threemeasurement occasions. On theese, a linear model fits perfect to thedata.To take this special fourth measure into account, i thought aboutusing the information of the slope and intercept mean and variation(of the first three measurements) as a predictor for the fourthmeasurement occasion in a regression.So i wrote a model: i s | y1@0 y2@.1 y3@.2;4th_var ON i s;But the model fit decreased drastically.So i was wondering, if this is the false solution for this problem?Are there other, elegant solutions for this problem?Thank you very much!","Creater_id":90092,"Start_date":"2015-09-22 02:58:57","Question_id":173617,"Tags":["regression","growth-model","mplus"],"Answer_count":1,"Last_activity":"2016-08-24 01:26:06","Link":"http://stats.stackexchange.com/questions/173617/growth-modeling-of-longitudinal-data-growth-factors-as-predictors-in-regression","Creator_reputation":1}
{"_id":{"$oid":"5837a576a05283111e4d399b"},"View_count":254,"Display_name":"eleonora","Question_score":1,"Question_content":"I need help for understanding how can I interpret this correlogram in order to determine the ,  and  orders for ARIMA model. I use Stata, and I am analysing a time series with really few data.","Creater_id":98791,"Start_date":"2015-12-21 03:02:16","Question_id":187735,"Tags":["model-selection","arima"],"Answer_count":2,"Last_activity":"2016-08-24 01:17:40","Link":"http://stats.stackexchange.com/questions/187735/how-can-i-determine-the-arima-orders-p-d-q-from-this-correlogram","Creator_reputation":6}
{"_id":{"$oid":"5837a576a05283111e4d399d"},"View_count":137,"Display_name":"user36397","Question_score":6,"Question_content":"Does using the bayesian estimator to complete SEM in Mplus mitigate some concerns with a limited sample size (n=120). I.e is this approach preferred over using the traditional ML estimator with associated p values?","Creater_id":36397,"Start_date":"2013-12-18 16:16:21","Question_id":80077,"Tags":["bayesian","sample-size","sem","mplus"],"Answer_count":1,"Last_activity":"2016-08-24 01:13:52","Link":"http://stats.stackexchange.com/questions/80077/is-bayesian-structural-equation-modelling-better-than-maximum-likelihood-with-sm","Creator_reputation":31}
{"_id":{"$oid":"5837a576a05283111e4d399f"},"View_count":62,"Display_name":"Lembik","Question_score":5,"Question_content":"  If you have a graph represented by an adjacency matrix, what  intuitively in terms of the original graph would low rank correspond  to?I am interested in this for both directed and undirected graphs.This is relevant because low rank is very important in common techniques such as non-negative matrix factorization and it would be interesting to understand what the assumptions mean for graphs.","Creater_id":53128,"Start_date":"2016-08-21 03:06:01","Question_id":230954,"Tags":["machine-learning","graph-theory","matrix-decomposition"],"Answer_count":1,"Last_activity":"2016-08-24 00:30:28","Link":"http://stats.stackexchange.com/questions/230954/what-is-low-rank-intuitively-for-an-adjacency-matrix","Creator_reputation":119}
{"_id":{"$oid":"5837a576a05283111e4d39a1"},"View_count":67,"Display_name":"FloK","Question_score":3,"Question_content":"I am analyzing self-reported delinquency (SRD) data in a school sample of juveniles (sample size = 3406). As expected, most of the juveniles in my sample (approximately 66%) have not shown any of the 15 studied behaviors.The 15 indicators are binary, that means the answers only differ between \"not offended\" (code=0) and \"offended\" (code=1) for each offense. Some single SRD behaviors are reported only very rarely. Offending proportions for the 15 SRD items range from 0.8 percent (n = 27) to 15.9 percent (n = 540).Here are my questions:Is there some kind of \"threshold\" for the amount of rare event data (be it a percentage or total number of cases) where it seems not reasonable to analyze this data with the help of IRT?The second question concerns the estimation of mixture IRT models. I found a paper (Wall et al., 2015) that recommends to estimate mixture IRT models with rare event data (zero-inflation). Despite from fitting better to the model (what it should), is there again a problem with the amount of zero-inflation also in these kind of models?Is there a different \"event threshold\" between IRT and mixture IRT models?If it is not reasonable to analyze data below a specific threshold, what are the best options to deal with this issue?Would it be, for example, sensible to delete items below this \"threshold\" to estimate IRT models?","Creater_id":113910,"Start_date":"2016-08-14 02:44:56","Question_id":229749,"Tags":["sample-size","binary-data","irt","rare-events"],"Answer_count":1,"Last_activity":"2016-08-23 23:53:31","Link":"http://stats.stackexchange.com/questions/229749/irt-mixture-modeling-of-rare-event-behavioral-data-what-amount-of-occurring-e","Creator_reputation":33}
{"_id":{"$oid":"5837a576a05283111e4d39a3"},"View_count":31,"Display_name":"JimSD","Question_score":0,"Question_content":"Let's say, X and Y are discrete/continuous random variables from gaussian distribution for simplicity.To get the probability density function of f(X,Y), you need to calculate \"inverse\" of f-function as shown below.http://www.math.uah.edu/stat/dist/Transformations.htmlFor continuous case, you need to calculate Jacobian.I am wondering what if function f is like max operator such as f(x)=max(const,X+Y)?I suppose inverse of max(const,X+Y) operator is not even one-to-one function, since output 0 could have been from input of X+Y=-1 or X+Y=-4 or whatever.. Thank you ","Creater_id":124657,"Start_date":"2016-08-23 22:51:33","Question_id":231409,"Tags":["pdf","maximum"],"Answer_count":1,"Last_activity":"2016-08-23 23:47:45","Link":"http://stats.stackexchange.com/questions/231409/probability-density-of-maxconst-xy-of-random-variable-x-and-y","Creator_reputation":11}
{"_id":{"$oid":"5837a576a05283111e4d39a5"},"View_count":35,"Display_name":"dimebucker91","Question_score":1,"Question_content":"in the case of logistic regression, say our model has  covariates and we have  distinct covariate patterns (distinct combinations of the values of the p covariates) and we have a total of  observations. The number of covariate patterns with pattern  is denoted by . Then, let  denote the number of success responses out of the .We have the statement:The distribution of goodness of fit tests are obtained by letting . They are said to be  asymptotic. If we fix  and let  then each , distribution assumptions based on these are known as m-asymptotics.I am trying to understand why the deviance and pearson chi squared statistics produce incorrect -values in the case that .The text by Hosmer, Lemeshow states that when , the distribution of both the pearson residual and the deviance residual is achieved under n asymptotics, and hence, the number of 'parameters' is increasing at the same rate as the sample size.I don't quite understand why this is problematic, I'm looking for an intuitive explanation of this","Creater_id":55946,"Start_date":"2016-05-29 05:20:28","Question_id":215225,"Tags":["regression","logistic","goodness-of-fit","pearson","deviance"],"Answer_count":1,"Last_activity":"2016-08-23 23:24:24","Link":"http://stats.stackexchange.com/questions/215225/n-m-asymptotics-and-logistic-regression-goodness-of-fit-tests","Creator_reputation":176}
{"_id":{"$oid":"5837a576a05283111e4d39a7"},"View_count":746,"Display_name":"A.M.A","Question_score":6,"Question_content":"How to interpret the coefficients of logistic regression? To be more specific, I have a set of independent variables, and one dependant variable (let it be \"rain\" or \"no rain\" expressed as 1 and 0 respectively)I build my logistic regression model and I want get an insight about the relations between my inputs and outputs, and see what are the most influential variables in the model. To do this I resort to the model coefficients:Variable       Coeff               P-Value x1_0          0.63914           1.27e-11 ***X2_0          0.59451           2e-16 ***X3_0         -0.38567           1.16e-08 ***X4_0         -0.58933           6.23e-05 ***X5_0         -0.01629           0.775    My question now is are these coefficients refer to the \"rain\" or to the \"no rain\" in my output? In the book \"Practical Data Science with R\" in chapter 7, it says: \"Negative coefficients that are statistically significant correspond to variables that are negatively correlated to the odds (and hence to the probability) of a positive outcome (the baby being at risk). Positive coefficients that are statistically significant are positively correlated to the odds of a positive outcome.\"Does the  positive outcome here refer to the \"rain\" in my output variable? ","Creater_id":86766,"Start_date":"2015-08-24 15:06:32","Question_id":168637,"Tags":["logistic"],"Answer_count":1,"Last_activity":"2016-08-23 23:21:53","Link":"http://stats.stackexchange.com/questions/168637/logistic-regression-what-happens-to-the-coefficients-when-we-switch-the-labels","Creator_reputation":33}
{"_id":{"$oid":"5837a576a05283111e4d39a9"},"View_count":2623,"Display_name":"C\u0026#233;line","Question_score":3,"Question_content":"I try to fit data with Holt-Winters function in R. Nevertheless, i am getting the following message:ts1\u0026lt;-ts(data$nb_decl,frequency=53)hw1\u0026lt;-HoltWinters(ts1)Warning message:In HoltWinters(ts1) :  optimization difficulties: ERROR: ABNORMAL_TERMINATION_IN_LNSRCH'data' counts 313 lines and if I just delete or change the last value (or add 314th value), the code works... Am I doing something wrong? or what is the problem with my data?see bellow the data used:data    week year nb_decl1     00 2006       02     01 2006       03     02 2006       04     03 2006       15     04 2006       06     05 2006       17     06 2006       08     07 2006       09     08 2006       010    09 2006       011    10 2006       112    11 2006       013    12 2006       214    13 2006       115    14 2006       116    15 2006       017    16 2006       218    17 2006       019    18 2006       120    19 2006       021    20 2006       022    21 2006       023    22 2006       024    23 2006       125    24 2006       026    25 2006       127    26 2006       128    27 2006       029    28 2006       130    29 2006       031    30 2006       032    31 2006       033    32 2006       034    33 2006       035    34 2006       136    35 2006       037    36 2006       138    37 2006       039    38 2006       040    39 2006       041    40 2006       142    41 2006       143    42 2006       044    43 2006       045    44 2006       046    45 2006       147    46 2006       348    47 2006       249    48 2006       450    49 2006       251    50 2006       152    51 2006       153    52 2006       054    01 2007       055    02 2007       156    03 2007       157    04 2007       158    05 2007       059    06 2007       260    07 2007       061    08 2007       162    09 2007       163    10 2007       164    11 2007       165    12 2007       166    13 2007       167    14 2007       168    15 2007       169    16 2007       170    17 2007       071    18 2007       072    19 2007       173    20 2007       074    21 2007       075    22 2007       376    23 2007       077    24 2007       078    25 2007       179    26 2007       080    27 2007       281    28 2007       082    29 2007       183    30 2007       084    31 2007       085    32 2007       186    33 2007       087    34 2007       288    35 2007       189    36 2007       190    37 2007       191    38 2007       192    39 2007       293    40 2007       094    41 2007       395    42 2007       096    43 2007       097    44 2007       098    45 2007       399    46 2007       0100   47 2007       0101   48 2007       0102   49 2007       0103   50 2007       1104   51 2007       1105   52 2007       0106   53 2007       0107   00 2008       1108   01 2008       9109   02 2008       0110   03 2008       0111   04 2008       1112   05 2008       0113   06 2008       0114   07 2008       2115   08 2008       0116   09 2008       2117   10 2008       2118   11 2008       1119   12 2008       0120   13 2008       0121   14 2008       3122   15 2008       1123   16 2008       0124   17 2008       1125   18 2008       2126   19 2008       1127   20 2008       1128   21 2008       1129   22 2008       1130   23 2008       2131   24 2008       1132   25 2008       1133   26 2008       0134   27 2008       0135   28 2008       3136   29 2008       0137   30 2008       4138   31 2008       1139   32 2008       0140   33 2008       1141   34 2008       1142   35 2008       2143   36 2008       0144   37 2008       1145   38 2008       0146   39 2008       0147   40 2008       1148   41 2008       0149   42 2008       2150   43 2008       1151   44 2008       0152   45 2008       1153   46 2008       2154   47 2008       0155   48 2008       3156   49 2008       3157   50 2008       2158   51 2008       0159   52 2008       0160   00 2009       2161   01 2009       3162   02 2009       3163   03 2009       2164   04 2009       3165   05 2009       1166   06 2009       1167   07 2009       1168   08 2009       1169   09 2009       2170   10 2009       3171   11 2009       1172   12 2009       1173   13 2009       2174   14 2009       1175   15 2009       2176   16 2009       4177   17 2009       0178   18 2009       0179   19 2009       3180   20 2009       0181   21 2009       2182   22 2009       0183   23 2009       0184   24 2009       1185   25 2009       1186   26 2009       1187   27 2009       1188   28 2009       3189   29 2009       0190   30 2009       3191   31 2009       4192   32 2009       0193   33 2009       1194   34 2009       3195   35 2009       0196   36 2009       2197   37 2009       1198   38 2009       1199   39 2009       1200   40 2009       2201   41 2009       0202   42 2009       0203   43 2009       1204   44 2009       3205   45 2009       2206   46 2009       2207   47 2009       1208   48 2009       0209   49 2009       2210   50 2009       0211   51 2009       0212   52 2009       0213   00 2010       1214   01 2010       2215   02 2010       3216   03 2010       2217   04 2010       1218   05 2010       2219   06 2010       0220   07 2010       2221   08 2010       0222   09 2010       1223   10 2010       1224   11 2010       1225   12 2010       1226   13 2010       1227   14 2010       0228   15 2010       1229   16 2010       0230   17 2010       1231   18 2010       1232   19 2010       1233   20 2010       2234   21 2010       0235   22 2010       2236   23 2010       4237   24 2010       3238   25 2010       4239   26 2010       1240   27 2010       1241   28 2010       1242   29 2010       2243   30 2010       2244   31 2010       2245   32 2010       0246   33 2010       0247   34 2010       2248   35 2010       0249   36 2010       1250   37 2010       1251   38 2010       1252   39 2010       1253   40 2010       2254   41 2010       4255   42 2010       2256   43 2010       1257   44 2010       3258   45 2010       1259   46 2010       0260   47 2010       6261   48 2010       1262   49 2010       1263   50 2010       1264   51 2010       2265   52 2010      11266    0 2011       0267    1 2011       3268    2 2011       2269    3 2011       0270    4 2011       3271    5 2011       0272    6 2011       4273    7 2011       2274    8 2011       1275    9 2011       1276   10 2011       1277   11 2011       2278   12 2011       2279   13 2011       3280   14 2011       1281   15 2011       5282   16 2011       3283   17 2011       3284   18 2011       1285   19 2011       2286   20 2011       1287   21 2011       3288   22 2011       0289   23 2011       2290   24 2011       1291   25 2011       2292   26 2011       1293   27 2011       2294   28 2011       2295   29 2011       2296   30 2011       1297   31 2011       0298   32 2011       0299   33 2011       2300   34 2011       0301   35 2011       2302   36 2011       1303   37 2011       1304   38 2011       1305   39 2011       3306   40 2011       3307   41 2011       2308   42 2011       2309   43 2011       1310   44 2011       2311   45 2011       1312   46 2011       1313   47 2011       0thank you for your help!","Creater_id":54422,"Start_date":"2014-08-20 11:33:20","Question_id":112668,"Tags":["r","time-series","optimization","exponential-smoothing"],"Answer_count":2,"Last_activity":"2016-08-23 23:08:34","Link":"http://stats.stackexchange.com/questions/112668/holt-winters-and-abnormal-termination-in-lnsrch","Creator_reputation":18}
{"_id":{"$oid":"5837a576a05283111e4d39ab"},"View_count":102,"Display_name":"andrija","Question_score":0,"Question_content":"I am wondering if there is any examples for application of survival analysis for probability of default estimation dealing with multiple cohort case?Here multiple cohort means following: if we have some firm with rating BB at the beginning of 2009 and defaulted at the end of 2010. This company enters the cohort that was built at the start of 2009. At the start of 2010 the same firm was rated CCC and is included in cohort built at 2010. The firm would be included in the cohort at 2010 even if has the same rating. Going further we have panel data (from t=1 to t=n) set with overlapping lifetimes (in this example overlapping period is 12 months).If we include one covariate (for example Unemployment rate), estimate Cox PH for all cohorts separately we will get n estimated betas for Unemployment rate. Can we then average all those betas and say this is an average effect of Unemployment rate on PD? Clearly betas will be autocorrelated since cohorts lifetime are overlapping and cohorts are dependent.Here is sample data in R.Only aggregated data are available:    #def - num of defaults    #cen - number of censoring cases     def \u0026lt;- structure(list(cohort = structure(c(14610, 14700, 14791, 14883,     14975), class = \"Date\"), `2010-04-01` = c(3, NA, NA, NA, NA),         `2010-07-01` = c(5, 1, NA, NA, NA), `2010-10-01` = c(3, 5,         2, NA, NA), `2011-01-01` = c(1, 3, 4, 5, NA), `2011-04-01` = c(5,         4, 5, 3, 1)), .Names = c(\"cohort\", \"2010-04-01\", \"2010-07-01\",     \"2010-10-01\", \"2011-01-01\", \"2011-04-01\"), row.names = c(NA,     5L), class = \"data.frame\")     cen \u0026lt;- structure(list(cohort = structure(c(14610, 14700, 14791, 14883,     14975), class = \"Date\"), `2010-04-01` = c(19, NA, NA, NA, NA),         `2010-07-01` = c(20, 14, NA, NA, NA), `2010-10-01` = c(18,         19, 20, NA, NA), `2011-01-01` = c(14, 10, 17, 17, NA), `2011-04-01` = c(12,         17, 18, 11, 14)), .Names = c(\"cohort\", \"2010-04-01\", \"2010-07-01\",     \"2010-10-01\", \"2011-01-01\", \"2011-04-01\"), row.names = c(NA,     5L), class = \"data.frame\")Cohort 1 is all active accounts on 2010-01-01, cohort 2 all active accounts on 2010-01-01 including all previous cases from cohort 1 that are still active and so on.So from def and cen, after transformation and using package survival we can estimate survival rate for each cohort for 1Q+, 2Q+ meaning for cohort 1 probability of survival for one quarter forward (1Q+), two quarters forward (2Q+) and so on.        library(survival)def_trans \u0026lt;- data.frame(def[,1, drop=FALSE], t(apply(def[,-1], 1,                         function(x) x=c(x[!is.na(x)], rep(NA, sum(is.na(x))))))   )names(def_trans) \u0026lt;- c(\"cohort\", 1:5)cen_trans \u0026lt;- data.frame(cen[,1, drop=FALSE], t(apply(cen[,-1], 1,                         function(x) x=c(x[!is.na(x)], rep(NA, sum(is.na(x)))))))names(cen_trans) \u0026lt;- c(\"cohort\", 1:5)d_trans \u0026lt;- rep(1:5, def_trans[1,-1])c_trans \u0026lt;-  rep(1:5, cen_trans[1,-1])dc_trans \u0026lt;- c(d_trans, c_trans) status \u0026lt;- rep(c(1,0), c(length(d_trans),length(c_trans))) y_km \u0026lt;- survfit(Surv(dc_trans, status) ~ 1, type=\"kaplan-meier\")summary(y_km)Call: survfit(formula = Surv(dc_trans, status) ~ 1, type = \"kaplan-meier\") time n.risk n.event survival std.err lower 95% CI upper 95% CI    1    100       3    0.970  0.0171        0.937        1.000    2     78       5    0.908  0.0313        0.849        0.971    3     53       3    0.856  0.0412        0.779        0.941    4     32       1    0.830  0.0479        0.741        0.929    5     17       5    0.586  0.0977        0.422        0.812If we repeat this for all cohorts, at the end we can obtain average survival rate for 1Q+, 2Q+... (just simple mean of all obtained survival rates for 1Q forward, 2Q forward)Now, if we introduce one external variable unemployment rate: unemployment_rate \u0026lt;- structure(list(Quarter = structure(c(14700, 14791, 14883, 14975, 15065), class = \"Date\"), rate = c(0.14, 0.18, 0.14, 0.12, 0.1)), .Names = c(\"Quarter\", \"rate\"), row.names = c(NA, -5L), class = \"data.frame\")Can we estimate for each cohort Cox PH model including unemployment rate as only covariate, get for each cohort beta coefficient, then average those betas and on that way estimate the average effect of unemployment on survival rate?","Creater_id":7488,"Start_date":"2016-08-21 09:25:36","Question_id":230983,"Tags":["r","survival"],"Answer_count":0,"Last_activity":"2016-08-23 22:41:48","Link":"http://stats.stackexchange.com/questions/230983/multiple-cohort-survival-analysis","Creator_reputation":16}
{"_id":{"$oid":"5837a576a05283111e4d39ad"},"View_count":148,"Display_name":"rano","Question_score":1,"Question_content":"I am looking for a test to assert the \"equality\" of two multivariate discrete distributions,  and  for which I do observe two samples  and .I know about the Kolmogorov-Smirnov test for univariate distributionsand the same wikipedia page I linked provides some pointers to extensions to the multivariate case. However I'd like to know if an efficient version does exist (and maybe it is already implemented in some software). What do I mean by efficient? something whose complexity could be linear in the number of instances or exploiting statistics that could have very likely been already computed (like  or ).UPDATEI am looking for a distribution agnostic test, meaning that no big assumption can be done on the two multivariate distributions. At the moment I am dealing with binary data, i.e.  and my aim is to be able to determine how to say to samples are drawn from the same distributions in order to apply some analogies from one to the other.","Creater_id":14101,"Start_date":"2015-05-18 00:44:28","Question_id":152795,"Tags":["statistical-significance","multivariate-analysis","descriptive-statistics","kolmogorov-smirnov"],"Answer_count":0,"Last_activity":"2016-08-23 22:16:05","Link":"http://stats.stackexchange.com/questions/152795/efficient-statistical-test-to-compare-two-multivariate-distributions","Creator_reputation":134}
{"_id":{"$oid":"5837a576a05283111e4d39af"},"View_count":37,"Display_name":"Carl","Question_score":1,"Question_content":"What does the symmetry requirement for the Conover rank squared tests entail? In that link it is stated that the Conover test assumes the data is symmetric about a common median. Is this symmetry of the ranked distribution about the median, and whatever it is, how stringent a requirement is it?","Creater_id":99274,"Start_date":"2016-08-23 18:03:15","Question_id":231389,"Tags":["probability","hypothesis-testing","variance","nonparametric"],"Answer_count":1,"Last_activity":"2016-08-23 21:51:03","Link":"http://stats.stackexchange.com/questions/231389/what-does-the-symmetry-requirement-for-the-conover-rank-squared-tests-entail","Creator_reputation":1745}
{"_id":{"$oid":"5837a576a05283111e4d39b1"},"View_count":31,"Display_name":"kon7","Question_score":3,"Question_content":"Its is known that in ordinary Kriging (Gaussian Process) the mean and variance at any new point is given as \\\\\\begin{pmatrix}\\mathbf{y}\\\\y^{*} \\end{pmatrix} \\sim N(\\mathbf{0},\\begin{bmatrix} K \u0026amp; K_{*}^{T} \\\\  K_{*} \u0026amp; K_{**}\\end{bmatrix})\\\\y^{*}|\\mathbf{y} \\sim N(K_{*}K^{-1}\\mathbf{y},K_{**}-K_{*}K^{-1}K_{*}^{T})What is the mean and variance in universal kriging, for example when a linear mean trend is assumed ?","Creater_id":107919,"Start_date":"2016-08-23 18:57:42","Question_id":231393,"Tags":["gaussian-process","kriging"],"Answer_count":1,"Last_activity":"2016-08-23 20:48:31","Link":"http://stats.stackexchange.com/questions/231393/universal-kriging-with-non-zero-mean-vector","Creator_reputation":137}
{"_id":{"$oid":"5837a576a05283111e4d39b3"},"View_count":28,"Display_name":"RegalPlatypus","Question_score":0,"Question_content":"I have seven treatments with multiple replications of each.  My response variable is Bernoulli data, so I'm using a GLM with a binomial family and logit link.  I've calculated the treatment means as a proportion (#success/#trials) via bootstrapping.  Looking at the plotted means with error bars it's apparent there won't be any significant differences, but I'd like a formal test to show that.  Something along the lines of TukeyHSD with grouping letters.  Generally that's exactly what I'd do: use the {multicomp} package.  However, in this case, the model I'm using is not significantly better than the null in a likelihood ratio test.  Can I still use {multicomp} to get my groupings or is there a different test(s) I should use?  It makes sense to me that the LRT wouldn't be significant if there aren't significant differences.","Creater_id":121418,"Start_date":"2016-08-23 19:18:50","Question_id":231398,"Tags":["generalized-linear-model","multiple-comparisons","likelihood-ratio","tukey-hsd"],"Answer_count":0,"Last_activity":"2016-08-23 19:18:50","Link":"http://stats.stackexchange.com/questions/231398/hypothesis-testing-when-glm-likelihood-ratio-test-is-not-significant","Creator_reputation":13}
{"_id":{"$oid":"5837a576a05283111e4d39b5"},"View_count":5200,"Display_name":"ssx6","Question_score":6,"Question_content":"I have a data set with 24 predictor variables, all continuous, but with different scales and potential collinearity. I’m trying to decide whether to use randomForest or cforest in party with conditional importance permutation. I recognize that I should probably use cforest if I want to overcome variable selection bias, but I find the ability to get partial dependence plots and percent variance explained from the randomForest package to be quite appealing. I was wondering if anyone knew if it were possible to get partial dependence plots and percent variance explained from cforest?Also, it appears that ctree uses a significance test to select variables; is this the same for cforest? And how might I get these significance values for each variable in cforest?","Creater_id":14100,"Start_date":"2012-09-14 12:26:55","Question_id":36324,"Tags":["r","random-forest"],"Answer_count":2,"Last_activity":"2016-08-23 19:01:49","Link":"http://stats.stackexchange.com/questions/36324/randomforest-vs-cforest-can-i-get-partial-dependence-plots-and-percent-varianc","Creator_reputation":31}
{"_id":{"$oid":"5837a576a05283111e4d39b7"},"View_count":180,"Display_name":"them","Question_score":4,"Question_content":"I want to use Bayesian approach to test whether a single data point  came from model  or model . I am having difficult time to get my head around this very basic setting.I make a few steps a long the way and then I get stuck \\ confused. So the two models are:M_1: X \\sim N(0, 1)\\,, M_2: X \\sim N(\\mu, 1)~~~ \\text{ with }~~ \\mu \\sim U[1, 2]\\,. Where  stands for Normal distribution with mean  and variance , and  is uniform distribution. My attempt to find the posterior odds. By Bayesian formula \\frac{P(M_1|x)}{P(M_2|x)} = \\frac{P(x|M_1)}{P(x|M_2)}\\frac{P(M_1)}{P(M_2)}\\,.At this point I need to introduce priors for  and  let those be  and , this leads to \\frac{P(M_1|x)}{P(M_2|x)} = \\frac{P(x|M_1)}{P(x|M_2)}\\frac{\\pi_1}{ (1- \\pi_1)}\\,.Here I already get confused - formally probability of  , any way, I do continue  P(X  = x|M_2) = \\int f_{(X|\\mu)}(x)f_\\mu(\\mu)d\\mu \\,,where  is conditional probability density of   given  and   is probability density of . is uniform on . Thus, P(X  = x|M_2) = \\int f_{(X|\\mu)}(x)f_\\mu(\\mu)d\\mu  = \\int_1^2 \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2}}d\\mu = C \\neq 0\\,. Overall \\frac{P(M_1|x)}{P(M_2|x)} = \\frac{P(x|M_1)}{P(x|M_2)}\\frac{\\pi_1}{ (1- \\pi_1)} = \\frac{0}{C}\\frac{\\pi_1}{ (1- \\pi_1)}  \\equiv 0\\,. \\quad(\\textbf{?})So, regardless of  the odds in favor of  are zero.  Am I having not enough coffee? did I get the Bayesian development above all wrong, should I have used likelihood function  instead of probability (that is to look at densities not probabilities)? I would appreciate any help, thanks!  ","Creater_id":94074,"Start_date":"2016-08-23 02:51:49","Question_id":231238,"Tags":["hypothesis-testing","bayesian"],"Answer_count":2,"Last_activity":"2016-08-23 17:55:24","Link":"http://stats.stackexchange.com/questions/231238/confused-by-a-simple-setting-in-bayesian-inference","Creator_reputation":116}
{"_id":{"$oid":"5837a576a05283111e4d39b9"},"View_count":13,"Display_name":"user1938107","Question_score":0,"Question_content":"I can find many resources on item-item or user-user similarity or collaborative filtering, but am having a hard time finding or knowing which terms to search for when combining them.For example, a movie database that has a database of user profiles with features of the user such as location and age.  Then a database of user rankings of items such as items purchased or returned.  Finally a database of item features such as length of movie time and box office open profit.So the matrices are:Users x User FeaturesItems x Item FeaturesUsers x Item PurchasesFirst, I am having a hard time understanding how these can be combined.  If I do a similarity matrix of the users and items, can these be used as a scaling factor to a collaborative filter based on userxitem purchases?The second issue, I am not sure what it is called, so can not find it, but essentially I do not want latent factor modeling because I would like to manually weight user or item features.  In one output, maybe the user feature of location should be weighted 2x a users age, and an item length of time should be weighted 1.5x the box office profits.","Creater_id":52884,"Start_date":"2016-07-05 20:56:41","Question_id":222348,"Tags":["correlation","collaborative"],"Answer_count":1,"Last_activity":"2016-08-23 17:51:55","Link":"http://stats.stackexchange.com/questions/222348/how-to-use-variables-as-factors-in-collaborative-filter","Creator_reputation":108}
{"_id":{"$oid":"5837a576a05283111e4d39bb"},"View_count":84,"Display_name":"maxheld","Question_score":5,"Question_content":"Our example person Azra has assigned (open-ended categories of her own choosing) to a fixed set of 35 items, recorded as logical values (TRUE, FALSE).We have summarised this data matrix into a co-occurence matrix of items x items, where each cell counts the number of categories that are assigned to both items of the item-pair in question.We interpret these counts as a measure of categorical similarity between items, and consequently set the diagonal to the maximum possible number of categories, 18 in Azra's case.We then divide all cells by that maximum to scaled our cells to 1.(We also need to do this because there are other people than Azra who have less then 18 categories in total, so we want to make them comparable).Loosely speaking, we assume that our cells in this (scaled) co-occurence matrix can be interpreted as percentages of similarity, where 1 on the diagonal is – naturally – the maximum: item, say but-how with but-how obviously is completely similar.Here's Azra (sorry):Azra \u0026lt;- structure(c(1, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.222222222222222, 0.333333333333333, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.166666666666667, 0.222222222222222, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.333333333333333, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 1, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 1, 0.111111111111111, 0.277777777777778, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0, 0.111111111111111, 0.0555555555555556, 0, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.111111111111111, 1, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.333333333333333, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.277777777777778, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.222222222222222, 0.277777777777778, 0, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.166666666666667, 1, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0.222222222222222, 0.333333333333333, 0.0555555555555556, 0, 0.222222222222222, 0.333333333333333, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.333333333333333, 0.222222222222222, 0.277777777777778, 0.388888888888889, 0.222222222222222, 0.333333333333333, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0, 0.222222222222222, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 1, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0.0555555555555556, 0, 0, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0, 1, 0, 0, 0, 0, 0.111111111111111, 0.111111111111111, 0, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0, 1, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0, 0.166666666666667, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 1, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0, 0, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0, 0.166666666666667, 0, 0.0555555555555556, 0, 0.166666666666667, 0.0555555555555556, 1, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.166666666666667, 0.111111111111111, 0.166666666666667, 1, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.111111111111111, 0, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 1, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.0555555555555556, 0.111111111111111, 0.333333333333333, 0.222222222222222, 0.166666666666667, 0.333333333333333, 0.333333333333333, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.222222222222222, 1, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.388888888888889, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.277777777777778, 0.444444444444444, 0.277777777777778, 0.388888888888889, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 1, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0, 0.166666666666667, 0, 0.0555555555555556, 0, 0.166666666666667, 0.0555555555555556, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 1, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0, 0.166666666666667, 0.0555555555555556, 0.222222222222222, 0, 0.111111111111111, 0, 0.111111111111111, 0, 0.0555555555555556, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0, 1, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.166666666666667, 0.277777777777778, 0.111111111111111, 0.166666666666667, 0.277777777777778, 0.333333333333333, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.388888888888889, 0.111111111111111, 0.111111111111111, 0.222222222222222, 1, 0.111111111111111, 0.333333333333333, 0.222222222222222, 0.222222222222222, 0.166666666666667, 0.277777777777778, 0.333333333333333, 0.277777777777778, 0.333333333333333, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0, 0.0555555555555556, 0.0555555555555556, 0, 0.166666666666667, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 1, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.333333333333333, 0.111111111111111, 1, 0.166666666666667, 0.166666666666667, 0, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.222222222222222, 0, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 1, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.333333333333333, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0, 0, 0, 0.166666666666667, 0.222222222222222, 0, 0, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 1, 0.166666666666667, 0.222222222222222, 0.222222222222222, 0.166666666666667, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0, 0.166666666666667, 0.0555555555555556, 0, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 1, 0.166666666666667, 0.222222222222222, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.0555555555555556, 0, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.0555555555555556, 0.166666666666667, 0.277777777777778, 0.0555555555555556, 0, 0.222222222222222, 0.277777777777778, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.166666666666667, 1, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0, 0.222222222222222, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.333333333333333, 0.166666666666667, 0.222222222222222, 0.277777777777778, 0.388888888888889, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.444444444444444, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.333333333333333, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 1, 0.277777777777778, 0.388888888888889, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.222222222222222, 0.166666666666667, 0, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.277777777777778, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.277777777777778, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.277777777777778, 1, 0.277777777777778, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.111111111111111, 0.277777777777778, 0.333333333333333, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.388888888888889, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.333333333333333, 0.166666666666667, 0.222222222222222, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.388888888888889, 0.277777777777778, 1, 0.0555555555555556, 0.222222222222222, 0.222222222222222, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 1, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0, 0.111111111111111, 0.222222222222222, 0, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.111111111111111, 0, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 1, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0, 0, 0.0555555555555556, 0.166666666666667, 0, 0, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.0555555555555556, 1, 0, 0.111111111111111, 0, 0, 0.0555555555555556, 0, 0.166666666666667, 0.111111111111111, 0, 0.166666666666667, 0, 0.0555555555555556, 0, 0.166666666666667, 0.0555555555555556, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 1, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.222222222222222, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 1, 0.166666666666667, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 1, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0, 0.111111111111111, 0, 0.0555555555555556, 0, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0.0555555555555556, 0, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 1, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0, 0.0555555555555556, 1, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0, 0, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 1), .Dim = c(35L, 35L), .Dimnames = structure(list(items = c(\"but-how\", \"encyclopedia\", \"alien\", \"language-of-bees\", \"bad-hen\", \"correspondence\", \"bamboozable\", \"inventions\", \"gray-hair\", \"i-we\", \"the-same\", \"the-better\", \"cockatoo\", \"caravan\", \"comma\", \"headturner\", \"crocodile\", \"countries\", \"level\", \"morning\", \"idiom\", \"lullaby\", \"riddle\", \"eating-grandpa\", \"easter-bunny\", \"alphabet-of-swearing\", \"guilt\", \"trouble\", \"eating-animals\", \"random-poetry\", \"comparatively\", \"one-letter\", \"resistance\", \"conjugation\", \"censored\"), items = c(\"but-how\", \"encyclopedia\", \"alien\", \"language-of-bees\", \"bad-hen\", \"correspondence\", \"bamboozable\", \"inventions\", \"gray-hair\", \"i-we\", \"the-same\", \"the-better\", \"cockatoo\", \"caravan\", \"comma\", \"headturner\", \"crocodile\", \"countries\", \"level\", \"morning\", \"idiom\", \"lullaby\", \"riddle\", \"eating-grandpa\", \"easter-bunny\", \"alphabet-of-swearing\", \"guilt\", \"trouble\", \"eating-animals\", \"random-poetry\", \"comparatively\", \"one-letter\", \"resistance\", \"conjugation\", \"censored\")), .Names = c(\"items\", \"items\")))A plot is easily made, but not very informative for us, because it's overwhelming, so we need some kind of summry.library(ggplot2)ggplot(data = melt(Azra, varnames = c(\"x\", \"y\")), mapping = aes(x = x, y = y, fill = value, )) + geom_raster() + scale_fill_continuous(low = \"white\", high = \"steelblue\") + theme(axis.text.x = element_text(angle = 90, hjust = 1))We want to be able to see which items Azra thought were categorically similar, and if so, what her dimensions (or clusters?) of similarity are.What would be an appropriate, and informative method to summarise this kind of data?Specifically, we'd like to be able to use something akin to Horn's Parallel Analysis (from a PCA context) to decide just how many retainable dimensions there are in Azras similarity matrix.Notes:We've looked at Multidimensional Scaling (MDS), but that seems to require the number of dimensions as an input, though to us, the number of dimensions on which Azra sees similarity is an empirical question.We've looked at Hierarchical Clustering Methods and respective dendrogram plots, but that seems to sit awkwardly with the fact that similarity is a multidimensional phenomenon; item censored might share category A with but-how, but category B with language-of-bees etc.We've looked at Principal Components Analysis (PCA) – mostly because we know it well, so everything starts looking like a nail ... – but that doesn't work with a similarity matrix, and even with a (converted) distance matrix only under some conditions about which we're not sure. (We tried anyway, see below).Here's our rough, hacky and probably just plain wrong/dumb way to do this:It doesn't give us any indication of how many dimensions we should be retaining.We first make a distance matrix as per the cosine theorem (?!):distances \u0026lt;- sqrt(1-Azra)We then plug that into a (classical?) MDS and plot the result:md_scaled \u0026lt;- cmdscale(d = distances)md_scaled \u0026lt;- as.data.frame(md_scaled)ggplot(data = md_scaled, mapping = aes(x = V1, y = V2, label = rownames(md_scaled))) + geom_text()","Creater_id":60119,"Start_date":"2016-08-18 04:47:43","Question_id":230479,"Tags":["clustering","pca","dimensionality-reduction","hierarchical-clustering","multidimensional-scaling"],"Answer_count":2,"Last_activity":"2016-08-23 17:36:22","Link":"http://stats.stackexchange.com/questions/230479/how-to-reduce-the-dimensionality-of-a-similarity-matrix-of-categorical-co-occur","Creator_reputation":338}
{"_id":{"$oid":"5837a577a05283111e4d39bd"},"View_count":38,"Display_name":"colin","Question_score":0,"Question_content":"I am looking to get a pseudo  metric from a beta-regression model fit using JAGS in the runjags package for R. To do so I have calculated the deviance of the fitted model, and the deviance of a null model. I plan to calculate McFadden's pseudo  as1-\\frac{\\text{Residual Deviance}}{\\text{Null Deviance}}.Where residual deviance is the deviance of the fitted model, and null deviance is the deviance of the null model. However, both of my deviance values are negative. Residual deviance = -6622.103 ans null deviance = -5939.539. So, 1 - (-6622.103/-5939.539) = -0.1149187. Negative  values don't seem right. ","Creater_id":30451,"Start_date":"2016-08-23 14:00:33","Question_id":231365,"Tags":["regression","r-squared","deviance","beta-regression"],"Answer_count":1,"Last_activity":"2016-08-23 17:00:13","Link":"http://stats.stackexchange.com/questions/231365/calculating-a-pseudo-r2-value-when-deviance-is-negative","Creator_reputation":266}
{"_id":{"$oid":"5837a577a05283111e4d39bf"},"View_count":55,"Display_name":"ayhan","Question_score":0,"Question_content":"I am studying Applied Linear Statistical Models (Kutner et al, 2005). They present a proof for minimum variance unbiased estimator of  as follows:   For regression model , the  estimator          has the minimum variance among all linear unbiased estimators of the  form:        where the  are arbitrary constants.    Since  is required to be unbiased, the following must  hold:        Substituting         For the last equation to hold, the  must follow the restrictions:     and The proof continues but my question is about the last line. I can see that when  and  the equation holds but I am not sure how to show that this is a unique solution. I should be able to reach a conclusion based on the number of variables and number of linearly independent equations but I don't see how. Should I treat  and  as constants? If so, does summing over  make it a single equation? How should I approach this problem?","Creater_id":21227,"Start_date":"2016-08-23 13:11:46","Question_id":231356,"Tags":["regression","self-study","linear-model"],"Answer_count":1,"Last_activity":"2016-08-23 16:54:22","Link":"http://stats.stackexchange.com/questions/231356/minimum-variance-linear-unbiased-estimator-of-beta-1","Creator_reputation":128}
{"_id":{"$oid":"5837a577a05283111e4d39c1"},"View_count":44,"Display_name":"Benj","Question_score":4,"Question_content":"There is a kind of (pretty simple) visualization we use a lot in my office, but none of us knows what it's called:The idea is that the second bar drills down into the indicated data from the first bar.  As pictured here, 69% of the first bar is highlighted.  The second bar shows that 76% of that initial 69% is highlighted, and so on.We usually call these \"engagement funnels\", but searching online for that term pulls up a very different visualization.  I'm asking because I'd like to build a tool to auto-generate them (likely with a library like d3) but want to know if others are already doing that...Is there a standard name for this type of display?(Bonus: do you know of a library or plugin that generates them already?)","Creater_id":128589,"Start_date":"2016-08-22 13:54:10","Question_id":231164,"Tags":["data-visualization","barplot"],"Answer_count":1,"Last_activity":"2016-08-23 16:42:26","Link":"http://stats.stackexchange.com/questions/231164/what-is-the-name-for-this-bar-chart-visualization-where-successive-bars-drill-do","Creator_reputation":121}
{"_id":{"$oid":"5837a577a05283111e4d39c3"},"View_count":29,"Display_name":"Baron Yugovich","Question_score":1,"Question_content":"Input to tfidf is a list of documents, output is a matrix which contains a numerical value for each (word, document) pair. How can I use that matrix to perform feature selection, i.e. reduce the size of the dictionary?","Creater_id":78063,"Start_date":"2016-08-21 11:55:15","Question_id":230992,"Tags":["feature-selection","natural-language","tf-idf"],"Answer_count":1,"Last_activity":"2016-08-23 15:12:57","Link":"http://stats.stackexchange.com/questions/230992/tfidf-for-feature-selection","Creator_reputation":76}
{"_id":{"$oid":"5837a577a05283111e4d39c5"},"View_count":32,"Display_name":"tmilrandom","Question_score":0,"Question_content":"This is probably an easy question, but it's really confusing me.Suppose that, given that the result is NOT a draw, Team A has a probability of beating Team B of 60%. I want to ignore every external factor (players tired, different strategies, etc) and focus only on the math. If the first half was also not a draw, is the probability of Team A defeating Team B in the first half also 60%, or less than 60%?My intuition would, at first, say 60%, because if they play that first half an infinite number of times, then I guess A would win 60%.However, I also think that it would make sense to be less than 60%, because, since it's a smaller time frame, Team B would have a better chance of getting 'lucky', and therefore, it's probability of winning would increase.Which of my reasonings is correct, if any?","Creater_id":128720,"Start_date":"2016-08-23 14:10:39","Question_id":231369,"Tags":["probability","conditional-probability"],"Answer_count":1,"Last_activity":"2016-08-23 15:06:44","Link":"http://stats.stackexchange.com/questions/231369/probability-of-team-a-defeating-team-b-in-the-first-half-given-probability-for","Creator_reputation":11}
{"_id":{"$oid":"5837a577a05283111e4d39c7"},"View_count":119,"Display_name":"quirik","Question_score":1,"Question_content":"Could you please provide an example and explanation why to use the bivariate probit model with sample selection?In this context, to what sample selection bias refers to?","Creater_id":43204,"Start_date":"2016-04-17 08:14:19","Question_id":207804,"Tags":["econometrics","bias","bivariate"],"Answer_count":1,"Last_activity":"2016-08-23 14:23:38","Link":"http://stats.stackexchange.com/questions/207804/bivariate-probit-model-with-sample-selection","Creator_reputation":113}
{"_id":{"$oid":"5837a577a05283111e4d39c9"},"View_count":810,"Display_name":"Miroslav Sabo","Question_score":8,"Question_content":"Background: I asked hundreds of participants in my survey how much they are interested in selected areas (by five point Likert scales with 1 indicating \"not interested\" and 5 indicating \"interested\").Then I tried PCA. The picture below is a projection into first two principal components. Colors are used for genders and PCA arrows are original variables (i.e. interests).I noticed that: Dots (respondents) are quite well separated by the second component.No arrow points left.Some arrows are much shorter than others.Variables tend to make clusters, but not observations.It seems that arrows pointing down (to males) are mainly males' interests and arrows pointing up are mainly females' interests.Some arrows point neither down nor up.Questions: How to correctly interpret relationships between dots (respondents), colors (genders) and arrows (variables)? What other conclusions about respondents and their interests can be mined from this plot? The data can be found here.","Creater_id":14730,"Start_date":"2013-06-08 05:53:16","Question_id":61215,"Tags":["pca","interpretation","survey","likert","biplot"],"Answer_count":1,"Last_activity":"2016-08-23 14:22:57","Link":"http://stats.stackexchange.com/questions/61215/how-to-interpret-this-pca-biplot-coming-from-a-survey-of-what-areas-people-are-i","Creator_reputation":1747}
{"_id":{"$oid":"5837a577a05283111e4d39cb"},"View_count":32,"Display_name":"StatsScared","Question_score":1,"Question_content":"I am familiar with how to interpret the regression coefficients on dummy categorical variables when one chooses a reference category by dropping it. However, how does one interpret the regression coefficients on each categorical variable when all categories are included (i.e. one dummy variable for each category) and the intercept is dropped? ","Creater_id":41267,"Start_date":"2016-08-23 13:40:23","Question_id":231360,"Tags":["regression","multiple-regression","categorical-data","regression-coefficients"],"Answer_count":1,"Last_activity":"2016-08-23 14:05:46","Link":"http://stats.stackexchange.com/questions/231360/interpretation-of-coefficient-on-categorical-variable-when-all-categories-are-in","Creator_reputation":165}
{"_id":{"$oid":"5837a577a05283111e4d39cd"},"View_count":12,"Display_name":"Ulderique Demoitre","Question_score":2,"Question_content":"Given a sample of  iid variables \\{X_i\\}_{i=1}^{n} distributed as a multinomial distribution  wherep = (p_1,...,p_n)I will have that each  is a n-tuple with all zeros except a 1 in a certain (random) position. I have the following doubts:Can I say that the j-th coordinate of each  is a bernoulli with parameter ?If Y = \\sum_i X_iCan I say that the j-th coordinate of each  is a binomial with parameter  and , Y$ are not independent..","Creater_id":106828,"Start_date":"2016-08-23 13:44:17","Question_id":231362,"Tags":["distributions"],"Answer_count":0,"Last_activity":"2016-08-23 13:47:20","Link":"http://stats.stackexchange.com/questions/231362/are-the-coordinates-of-a-multinomial-distributed-as-binomials","Creator_reputation":131}
{"_id":{"$oid":"5837a577a05283111e4d39cf"},"View_count":35732,"Display_name":"dfrankow","Question_score":28,"Question_content":"Christopher Manning's writeup on logistic regression in R shows a logistic regression in R as follows:ced.logr \u0026lt;- glm(ced.del ~ cat + follows + factor(class),   family=binomial)Some output:\u0026gt; summary(ced.logr)Call:glm(formula = ced.del ~ cat + follows + factor(class),    family = binomial(\"logit\"))Deviance Residuals:Min            1Q    Median       3Q      Max-3.24384 -1.34325   0.04954  1.01488  6.40094Coefficients:              Estimate Std. Error z value Pr(\u0026gt;|z|)(Intercept)   -1.31827    0.12221 -10.787 \u0026lt; 2e-16catd          -0.16931    0.10032  -1.688 0.091459catm           0.17858    0.08952   1.995 0.046053catn           0.66672    0.09651   6.908 4.91e-12catv          -0.76754    0.21844  -3.514 0.000442followsP       0.95255    0.07400  12.872 \u0026lt; 2e-16followsV       0.53408    0.05660   9.436 \u0026lt; 2e-16factor(class)2 1.27045    0.10320  12.310 \u0026lt; 2e-16factor(class)3 1.04805    0.10355  10.122 \u0026lt; 2e-16factor(class)4 1.37425    0.10155  13.532 \u0026lt; 2e-16(Dispersion parameter for binomial family taken to be 1)Null deviance: 958.66 on 51 degrees of freedomResidual deviance: 198.63 on 42 degrees of freedomAIC: 446.10Number of Fisher Scoring iterations: 4He then goes into some detail about how to interpret coefficients, compare different models, and so on.  Quite useful.However, how much variance does the model account for?  A Stata page on logistic regression says:  Technically, R2 cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-R2, in logistic regression, is defined as 1 - L1/L0, where L0 represents the log likelihood for the \"constant-only\" model and L1 is the log likelihood for the full model with constant and predictors. I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term).  Log likelihood is a measure of how closely the parameters fit the data.  In fact, Manning sort of hints that the deviance might be -2 log L. Perhaps null deviance is constant-only and residual deviance is -2 log L of the model?  However, I'm not crystal clear on it.Can someone verify how one actually computes the pseudo-R^2 in R using this example?","Creater_id":2849,"Start_date":"2011-03-19 15:44:06","Question_id":8511,"Tags":["r","logistic"],"Answer_count":3,"Last_activity":"2016-08-23 13:32:09","Link":"http://stats.stackexchange.com/questions/8511/how-to-calculate-pseudo-r2-from-rs-logistic-regression","Creator_reputation":798}
{"_id":{"$oid":"5837a577a05283111e4d39d1"},"View_count":13,"Display_name":"Munichong","Question_score":0,"Question_content":"I want to use RNN to build a recommender system which suggests interesting articles to a user based on the ones that the user has read in the current session. In my application, a user can choose to rate an article that he/she has read: 0 means dislike, 1 means like. Such rating is certainly optional. Therefore, in the visible input layer of the RNN at each time step, I want to consider:the one-hot encoding of the current article, e.g. [0,0,1,0...0,0]. This is commonly used in most session based RNN recommender, e.g. https://erikbern.com/2014/06/28/recurrent-neural-networks-for-collaborative-filtering/.the rating history of the user, which represent user preference. The rating history for each user will be very sparse. It is typical in recommendation systems. The rating history in the dataset contains:\"0\", the articles that the user dislikes\"1\", the articles that the user likes\"?\", A lot of missing values, i.e., unseen, because a user only rated a very small subset of articles. My question: How can I specify the input values of the missing values? If using 0, I concern that RNN cannot distinguish \"dislike\" and \"unseen\". ","Creater_id":35802,"Start_date":"2016-08-23 13:28:23","Question_id":231359,"Tags":["machine-learning","neural-networks","deep-learning","rnn"],"Answer_count":0,"Last_activity":"2016-08-23 13:28:23","Link":"http://stats.stackexchange.com/questions/231359/how-can-i-deal-with-the-missing-visible-units-in-recurrent-neural-network","Creator_reputation":241}
{"_id":{"$oid":"5837a577a05283111e4d39d3"},"View_count":75,"Display_name":"wisc88","Question_score":4,"Question_content":"After running a linear mixed model, I want to obtain the 95% confidence interval for the variance estimation of my random effect. The function confint() in R gives me the 95% CI of the standard deviation of the random effect. Can I square the lower and upper limits to get obtain the 95% CI for the variance?","Creater_id":29876,"Start_date":"2016-08-23 06:14:40","Question_id":231275,"Tags":["r","mixed-model","confidence-interval","variance","random-variable"],"Answer_count":1,"Last_activity":"2016-08-23 13:12:51","Link":"http://stats.stackexchange.com/questions/231275/how-can-i-obtain-the-95-confidence-interval-for-the-variance-of-a-random-effect","Creator_reputation":65}
{"_id":{"$oid":"5837a577a05283111e4d39d5"},"View_count":44,"Display_name":"chad39","Question_score":1,"Question_content":"I have been asked to develop a predictive model to optimize call times from a call center. We have historical data on successful calls, date/time, member-specific attributes, etc.Ideally, the output would be an hour of day and day of week to call a member to have the best chance of reaching them based on the members attributes. I would like recommendations on the type of model to apply and an appropriate method for evaluating the model. Also, any suggestions on potential features to employ would be helpful.My current idea is to cluster the data points and find the best time to reach people within the cluster by getting the maximal value of a 2d histogram of day of the week and hour of the day. Basically, I'm trying to maximize P(pick up | member attributes AND hour of day AND day of week)","Creater_id":128701,"Start_date":"2016-08-23 11:57:02","Question_id":231340,"Tags":["optimization","circular-statistics"],"Answer_count":0,"Last_activity":"2016-08-23 13:07:05","Link":"http://stats.stackexchange.com/questions/231340/how-to-optimize-call-center-call-times","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d39d7"},"View_count":44,"Display_name":"SMeznaric","Question_score":2,"Question_content":"Suppose that random variables X and Y are positively correlated, with mean zero and variance 1. Suppose Z is a positive random variable. Can it be said that X/Z and Y/Z are also positively correlated?If no, are there some simple conditions that would make this true?","Creater_id":128705,"Start_date":"2016-08-23 12:20:09","Question_id":231346,"Tags":["regression","correlation"],"Answer_count":2,"Last_activity":"2016-08-23 13:03:33","Link":"http://stats.stackexchange.com/questions/231346/correlation-after-dividing-with-a-random-variable","Creator_reputation":113}
{"_id":{"$oid":"5837a577a05283111e4d39d9"},"View_count":18,"Display_name":"luchonacho","Question_score":0,"Question_content":"I have a linear panel data model: y_{it} = \\alpha + x_{it}\\beta + e_{it} for individuals  and period . Assume conditions for Pooled OLS consistency hold.I also observe the region  where the individual lives. Define as the new cluster the pair individual-region, , where the same individual living in different regions is treated as different units. The new model would be: y_{kt} = \\alpha + x_{kt}\\beta + e_{kt} Is Pooled OLS still consistent?","Creater_id":100369,"Start_date":"2016-08-23 12:51:39","Question_id":231351,"Tags":["regression","mixed-model","panel-data","least-squares","multilevel-analysis"],"Answer_count":0,"Last_activity":"2016-08-23 12:51:39","Link":"http://stats.stackexchange.com/questions/231351/does-a-redefinition-of-the-cluster-level-change-estimation-consisntency","Creator_reputation":584}
{"_id":{"$oid":"5837a577a05283111e4d39db"},"View_count":50,"Display_name":"elise14b","Question_score":1,"Question_content":"In SPSS, I conducted a general linear model with repeated measures to determine changes in various blood markers, BMI, waist circumference, and blood pressure in youth from pre to post intervention comparing an intervention and control group using SPSS. The within-subject factor is time with 2 levels (pre and post), the within-subject variables (dependent variables) include various blood markers, BMI, waist circumference, and blood pressure. The between-subjects factor is treatment group (intervention or control group), and the covariates were age, sex, and area deprivation. Under \"Tests of Within-Subjects Effects,\" the multivariate analysis notes there was no main effect for time nor was there a significant interaction between time and treatment group. Under the univariate tests, there was no main effect of time for any of the variables, but there was a significant interaction between time and treatment group for some but not all of the mentioned variables. Do I interpret the univariate rather than the multivariate analysis and conclude that there were significant improvements in some of the variables over time and between groups? Alternatively, do I interpret the multivariate analysis and concludesince there was not significant main effect or time by treatmentinteraction, the intervention had no effect on any of the variablesover time between groups?","Creater_id":128693,"Start_date":"2016-08-23 10:13:33","Question_id":231324,"Tags":["univariate","mixed-design"],"Answer_count":0,"Last_activity":"2016-08-23 12:37:43","Link":"http://stats.stackexchange.com/questions/231324/in-a-2x2-mixed-anova-do-i-interpret-the-timetreatment-interaction-from-the-mult","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d39dd"},"View_count":27,"Display_name":"Ferdi","Question_score":0,"Question_content":"How can I represent a Multivariate Gaussian distribution as a Bayesian network? How can I represent a Multivariate Gaussian distribution as a Markov network/Markov Process?","Creater_id":128677,"Start_date":"2016-08-23 07:22:40","Question_id":231291,"Tags":["markov-process","bayesian-network","multivariate-distribution"],"Answer_count":1,"Last_activity":"2016-08-23 12:35:26","Link":"http://stats.stackexchange.com/questions/231291/multivariate-gaussian-as-bayesian-or-markov-network","Creator_reputation":697}
{"_id":{"$oid":"5837a577a05283111e4d39df"},"View_count":51,"Display_name":"shawnt00","Question_score":3,"Question_content":"Here are some interesting statistics at the bottom of a news article from today:  \"Up to 6% of children have food allergies, ...\"  \"Nearly nine in 10 schools nationally had one or more students with food allergies.\"I readily found a 15-year-old source that showed that, for public schools at least, the average population of schools was generally high enough that most schools have a very high likelihood of food allergies, being on the order of as few as 160 and probability of allergy around 0.00005. Of course that table doesn't say anything about the distribution other than the mean or anything about non-public schools, which are likely to be smaller.I find it very unlikely that there could be that many schools without allergies if the incidence rate is that high and a typical school has a few hundred students. Is there any relatively simple ways to (in)validate the plausibility of those two figures?","Creater_id":123167,"Start_date":"2016-08-23 11:33:17","Question_id":231332,"Tags":["distributions"],"Answer_count":2,"Last_activity":"2016-08-23 12:12:33","Link":"http://stats.stackexchange.com/questions/231332/contradictory-statistics-in-news-source","Creator_reputation":75}
{"_id":{"$oid":"5837a577a05283111e4d39e1"},"View_count":244,"Display_name":"Gandhi91","Question_score":3,"Question_content":"In order to measure precisely the influence of one variable  over  I wanted to use the mutual information because so far I believed that correlation coefficient (Pearson) was only limited to linear relation.But I realized that even in non-linear cases it gives good results.Example in Matlab: is a uniform random variable and  is the range of  Why the correlation is quite high in all those non-linear cases? I am beginning to think that correlation is only weak when the monotony of  and  is not the same. Otherwise it is not so bad.Do some people have a point of view on this?","Creater_id":42545,"Start_date":"2014-05-22 01:01:53","Question_id":99659,"Tags":["correlation","pearson"],"Answer_count":2,"Last_activity":"2016-08-23 12:08:50","Link":"http://stats.stackexchange.com/questions/99659/is-the-correlation-coefficient-better-than-we-think","Creator_reputation":101}
{"_id":{"$oid":"5837a577a05283111e4d39e3"},"View_count":37,"Display_name":"madsthaks","Question_score":1,"Question_content":"A little background, here is my model:mod1 = glm.nb(Goals~Defense, data = Messi.Liga)summary(mod1)And here is the output:Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 3.05943 1.11817 2.736 0.00622 **Defense -0.03954 0.01498 -2.639 0.00831 **Just so you get an idea of what the data frames look like, goals are the number of goals scored in a single game and Defense is the Defensive Rating of the opposition. Prediction model:BPLpredictions = data.frame(predict(mod1, bpl.df, type = \"response\", se.fit = TRUE))Everything seems to be going smoothly but i've seen blog posts where they will take the exponential of the \"fit\" results and use that for interpretation but when I do that, I get completely unreasonable results. What's odd is my results prior to exponentiating \"fit\" are logical. Also, to create my confidence interval, I've been using the following code:BPLpredictions[\"upr\"] = BPLpredictionsse.fit)BPLpredictions[\"lwr\"] = BPLpredictionsse.fit)If you think I should be doing it differently, please feel free to give recommendations. Thanks. ","Creater_id":125449,"Start_date":"2016-08-14 15:58:13","Question_id":229834,"Tags":["r","prediction","negative-binomial"],"Answer_count":1,"Last_activity":"2016-08-23 11:45:51","Link":"http://stats.stackexchange.com/questions/229834/having-issues-interpreting-my-prediction-results-for-my-negative-binomial-regres","Creator_reputation":45}
{"_id":{"$oid":"5837a577a05283111e4d39e5"},"View_count":51,"Display_name":"John K","Question_score":1,"Question_content":"I am trying to classify short natural language documents, for which I have a small labeled dataset. Using out-of-the-box document classifiers and basic td-idf representation, I am able to get \"reasonable\" performance. But the data is so sparse that it is doing little more than keyword matching. However, I also have a reasonably large corpus of unlabeled documents with high domain overlap. Using unsupervised techniques (LDA, LSA, doc2vec, clustering) I am also able to get reasonable results. I feel like there should be some way to use all the data together, but I don't really know where to start at this point. My intuition is that the unlabeled data carries much information (for example, word synonymy and polysemy), that I should be able use to give my classifier a good head start on the language model.Can anyone suggest any algorithms/techniques/white papers/libraries that fit the bill, or tell me why I'm thinking about this the wrong way? ","Creater_id":128587,"Start_date":"2016-08-23 11:43:04","Question_id":231336,"Tags":["classification","semi-supervised"],"Answer_count":0,"Last_activity":"2016-08-23 11:43:04","Link":"http://stats.stackexchange.com/questions/231336/combining-supervised-and-unsupervised-learning","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d39e7"},"View_count":20250,"Display_name":"bgbg","Question_score":15,"Question_content":"I am trying to predict the outcome of a complex system using neural networks (ANN's). The outcome (dependent) values range between 0 and 10,000. The different input variables have different ranges. All the variables have roughly normal distributions. I consider different options to scale the data before training. One option is to scale the input (independent) and output (dependent) variables to [0, 1] by computing cumulative distribution function using the mean and standard deviation values of each variable, independently. The problem with this method is that if I use the sigmoid activation function at the output, I will very likely miss extreme data, especially those not seen in the training setAnother option is to use a z-score. In that case I don't have the extreme data problem; however, I'm limited to a linear activation function at the output. What are other accepted normalization techniques that are in use with ANN's? I tried to look for reviews on this topic, but failed to find anything useful.","Creater_id":1496,"Start_date":"2011-03-01 11:53:04","Question_id":7757,"Tags":["machine-learning","neural-networks","multidimensional-scaling"],"Answer_count":4,"Last_activity":"2016-08-23 11:08:25","Link":"http://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks","Creator_reputation":924}
{"_id":{"$oid":"5837a577a05283111e4d39e9"},"View_count":21,"Display_name":"Benn","Question_score":0,"Question_content":"I am using an algorithm ( as a rule-based classifier) to create classification rules but I am so confused about the following :1. It will generate too many rules which lead to overfitting and should I post-prune the rules in the same training set?Or2. post-pruning is usually done by cross validation ( 10 fcv) 3. Should I also have a validation set to validate final ( post-pruned) rules?Appreciate your help","Creater_id":107584,"Start_date":"2016-08-23 10:26:45","Question_id":231326,"Tags":["classification"],"Answer_count":0,"Last_activity":"2016-08-23 10:26:45","Link":"http://stats.stackexchange.com/questions/231326/rule-based-classifier","Creator_reputation":24}
{"_id":{"$oid":"5837a577a05283111e4d39eb"},"View_count":45,"Display_name":"Tom","Question_score":3,"Question_content":"This is my first post here, so please bear with me! I'm comparing several biomarkers with Kaplan-Meier curves and calculating hazard ratios for different risk groups (defined by a certain, well established cut-off value of the biomarker) by using Cox regression in R. We have 3 tiers with a low, intermediate and high risk group, however the low risk group for one biomarker contains no events.This, so my understanding, leads to quasi-separation in the Cox regression and hence infinite values and large coefficients and SEs. I understand the Likelihood ratio is still valid, but what I'm obviously interested in is a calculation of the HR from exp(coef). A sample of the data is displayed below:   \u0026gt; head(riskgroups)   ID FU_3y_death FU_3y_death_days biomarker bm.riskcat   1           0             1095           58.2    group.3   2           1               79           11.5    group.2   22           0             1095           11.7    group.2   27           0              929            9.0    group.2   44           0              949            7.0    group.2   46           0             1095            7.5    group.2Now I have found that using Firth's method might allow a workaround, hence I've tried to run the analysis using coxphf with the following code:cox.groups \u0026lt;- coxphf(riskgroups, formula=Surv(FU_3y_death_days,FU_3y_death) ~ bm.riskcat, pl=T, firth = T)Rather bizarrely, this results in the following error:  Error in coxphf(riskgroups, formula = Surv(FU_3y_death_days, FU_3y_death) ~  :     NA/NaN/Inf in foreign function call (arg 3)I would have assumed that this is exactly what coxphf is trying to avoid? When setting pl to FALSE (to base the tests on the Wald method instead of profile penalised LL) I get results with all NaN. Of course the fact that there is no event in the lowest risk group is in itself an important message, but I do require hazard ratios for the second and third tier of risk categories to compare the different biomarkers. Any bright thoughts on this, my research into this has hit a wall after 3 days of reading...","Creater_id":128645,"Start_date":"2016-08-23 05:04:58","Question_id":231260,"Tags":["r","cox-model","proportional-hazards"],"Answer_count":1,"Last_activity":"2016-08-23 10:11:25","Link":"http://stats.stackexchange.com/questions/231260/quasi-separation-when-running-cox-regression-comparing-3-risk-groups","Creator_reputation":16}
{"_id":{"$oid":"5837a577a05283111e4d39ed"},"View_count":34,"Display_name":"Mayank","Question_score":2,"Question_content":"I have noticed different authors using different forms of IMR i.e. f/F or f/(1-F) depending on whether they are modelling selection or non-selection in stage 1 model. I did some simulations in my case where a lot of observations were censored at 0. In different runs, I tried f/F, or f/(1-F), or xbeta, or probit_score but to my surprise coefficient on my variable of interest hardly changed even though magnitude and signs of IMR changed drastically.My questions goes like this:Is it true to say irrespective of the formula applied, beta on variable of interest adjust accordingly? I mean as long as we are factoring selection bias, beta on variable of interest is unbiased and consistentHow do I interpret IMR? I have observed that f/F is directly proportional to xbeta (and hence probit score) while f/(1-F) is inversely proportional to xbeta. So, a positive sign on IMR should suggest that an increase in IMR (or decrease in xbeta if we are using f/F) leads to higher Y value. In other words, lesser the likelihood of stage 1 model, more is the Y value (for +ve IMR f/F). Is that correct?","Creater_id":115257,"Start_date":"2016-05-13 06:08:11","Question_id":212385,"Tags":["hazard","heckman"],"Answer_count":1,"Last_activity":"2016-08-23 10:07:29","Link":"http://stats.stackexchange.com/questions/212385/different-forms-of-inverse-mills-ratio-their-interpretation","Creator_reputation":11}
{"_id":{"$oid":"5837a577a05283111e4d39ef"},"View_count":26,"Display_name":"research111","Question_score":0,"Question_content":"I have been stuck on the following selection/endogeneity problem for a while:I have a panel dataset with 1.000 products becoming fair trade products, all at very different times between 2010 and 2014. For each product, I have weekly sales data for 6 months before they become fair trade up to 1 year after they become fair trade. In step 1, I estimated the effect of becoming fair trade using a (step) dummy variable that becomes 1 from the week onwards they become fair trade using OLS for each product individually. In step 2, I regress the coefficients from the step dummy on about 12 variables (moderaters). I use a 2 step procedure, because 12 interaction effects are not possible to estimate in the same equation due to multicollinearity.The problem, however, is that brands self-select into joining fair trade or not. I do have data on all products, including those who did not become fair trade.In short, my question is: Can I estimate 1 (panel?) selection equation across time across products, and include the same single inverse Mills ratio in each product equation seperately?Or are there other, better approaches to my problem?","Creater_id":98964,"Start_date":"2016-08-08 14:17:08","Question_id":228868,"Tags":["endogeneity","heckman"],"Answer_count":1,"Last_activity":"2016-08-23 10:01:10","Link":"http://stats.stackexchange.com/questions/228868/inverse-mills-ratio-in-system-of-equations","Creator_reputation":11}
{"_id":{"$oid":"5837a577a05283111e4d39f1"},"View_count":242,"Display_name":"Piet van den Berg","Question_score":6,"Question_content":"I would like to obtain 95% confidence intervals on the predictions of a non-linear mixed nlme model. As nothing standard is provided to do this within nlme, I was wondering if it is correct to use the method of \"population prediction intervals\", as outlined in Ben Bolker's book chapter in the context of models fit with maximum likelihood, based on the idea of resampling fixed effect parameters based on the variance-covariance matrix of the fitted model, simulating predictions based on this, and then taking the 95% percentiles of these predictions to get the 95% confidence intervals? The code to do this looks as follows :(I here use the 'Loblolly' data from the nlme help file) library(effects)library(nlme)library(MASS)fm1 \u0026lt;- nlme(height ~ SSasymp(age, Asym, R0, lrc),    data = Loblolly,    fixed = Asym + R0 + lrc ~ 1,    random = Asym ~ 1,    start = c(Asym = 103, R0 = -8.5, lrc = -3.3))xvals=seq(min(Loblollyage),length.out=100)nresamp=1000pars.picked = mvrnorm(nresamp, mu = fixef(fm1), Sigma = vcov(fm1)) # pick new parameter values by sampling from multivariate normal distribution based on fityvals = matrix(0, nrow = nresamp, ncol = length(xvals))for (i in 1:nresamp) {    yvals[i,] = sapply(xvals,function (x) SSasymp(x,pars.picked[i,1], pars.picked[i,2], pars.picked[i,3]))} quant = function(col) quantile(col, c(0.025,0.975)) # 95% percentilesconflims = apply(yvals,2,quant) # 95% confidence intervalsNow that I have my confidence limits I create a graph:meany = sapply(xvals,function (x) SSasymp(x,fixef(fm1)[[1]], fixef(fm1)[[2]], fixef(fm1)[[3]]))par(cex.axis = 2.0, cex.lab=2.0)plot(0, type='n', xlim=c(3,25), ylim=c(0,65), axes=F, xlab=\"age\", ylab=\"height\");axis(1, at=c(3,1:5 * 5), labels=c(3,1:5 * 5)) axis(2, at=0:6 * 10, labels=0:6 * 10)   for(i in 1:14){    data = subset(Loblolly, LoblollySeed)[i])       lines(dataheight, col = \"red\", lty=3)}lines(xvals,meany, lwd=3)lines(xvals,conflims[1,])lines(xvals,conflims[2,])Here's the plot with the 95% confidence intervals obtained this way:Is this approach valid, or are there any other or better approaches to calculate 95% confidence intervals on the predictions of a nonlinear mixed model? I am not entirely sure of how to deal with the random effect stucture of model... Should one average perhaps over random effect levels? Or would it be OK to have confidence intervals for an average subject, which would seem to be closer to what I have now?","Creater_id":128527,"Start_date":"2016-08-22 04:43:27","Question_id":231074,"Tags":["r","mixed-model","confidence-interval","nlme","nlmer"],"Answer_count":1,"Last_activity":"2016-08-23 09:58:13","Link":"http://stats.stackexchange.com/questions/231074/confidence-intervals-on-predictions-for-a-non-linear-mixed-model-nlme","Creator_reputation":38}
{"_id":{"$oid":"5837a577a05283111e4d39f3"},"View_count":24,"Display_name":"a.powell","Question_score":1,"Question_content":"I have experience with both regression and R but am a bit puzzled with how exactly to set up this regression. I am looking at sporting events where two teams play head to head. From each of these games I have the statistics for those games for the winning team and the losing team. I want to predict the MOV based on a number of those statistics. Here is how I currently have the data set up:Year    Winning_TM     Winning_TM_Pts    Losing_TM    Losing_TM_Pts  MOV ...  2011    A              45                B            32             132012    C              27                D            25             2...There is much more data but I think you can see how the data is arranged currently. My concern is what effects this will have on the model when in the future I want to predict the margin of victory for a given team? Is there a better way to organize this data so that the model meets its purpose and doesn't?Note: The model set up, in my mind, would look something like this (possibly with other statistics depending on their utility):MOV ~ Yds_Rush + Yds_Pass + SOS","Creater_id":119338,"Start_date":"2016-08-23 09:38:42","Question_id":231318,"Tags":["r","regression","multiple-regression"],"Answer_count":0,"Last_activity":"2016-08-23 09:38:42","Link":"http://stats.stackexchange.com/questions/231318/organizing-data-to-run-regression","Creator_reputation":538}
{"_id":{"$oid":"5837a577a05283111e4d39f5"},"View_count":36,"Display_name":"them","Question_score":0,"Question_content":"In lecture notes on introductory graduate course on Bayesian statistics, there is a short discussion of how Multiple linear regression may be treated in the paradigm of \"borrowing strength\" aka \"hierarchical models\". The outline of the treatment is given in the lecture note, and the rest is left as an easy exercise for the reader, which I am not able to complete. We are given the following multiple regression: Y_{[1 \\times n]} = \\beta_{[1 \\times p]} X_{[p \\times n]}+ \\varepsilon_{[1 \\times n]}\\,, where the subscript indicates the dimensionality of the quantities,  is known (response)  is known (predictors) (both are centered so we are not concerned with the intersect),  we want to estimate  vector. The errors \\{\\varepsilon_i\\}_{i=1}^n\\,~\\text{are  i.i.d drawn from}~~N \\left(0, \\frac{1}{\\tau}\\right)\\,, with  being the normal distribution (that is the variance of  is ).The system is assumed to be under-determined, meaning . Using \"hierarchical approach\" in this situation is claimed to be reasonable. Specifically, we assume that the elements of vector  are distributed as\\beta_i \\sim N\\left(0, \\frac{\\sigma^2}{\\tau}\\right)\\quad \\left(\\text{i.e. the variance is } \\frac{\\sigma^2}{\\tau}\\right)\\,.Here things start to become a bit vague. The claim is that one can use \"empirical Bayes approach\", that is to estimate \"hyper parameters\"  and  using MLE, which is claimed to be easy, that would give estimates . And then the posterior distribution of  is available in closed form. I was not able to carry this scheme out, my (long) attempts are detailed below. Any help or reference to a solution would be appreciated.  What I try to do is the following write likelihood function maximize with respect to , to find MLEs , . (I get stuck here)plug in the MLE estimates from the above and write posterior distribution. (Painful) Details:So I want to write the likelihood function , for this I need the probability density . The given model implies thatY_i = \\sum_{j=1}^p\\beta_j x_{i,j} + \\varepsilon_i\\,,thus, Y_i| \\tau, \\sigma \\sim N\\left(0, \\text{var}\\left[{\\sum_{j=1}^p\\beta_j x_{i,j}}\\right] + \\text{var}[\\varepsilon_i]\\right) = N\\left(0, \\frac{\\sigma^2}{\\tau}\\left(\\sum_{i=1}^p x_{i,j}^2\\right) + \\frac{1}{\\tau}\\right)= N\\left(0, \\frac{\\sigma^2}{\\tau}\\sigma^2_{i} + \\frac{1}{\\tau}\\right)\\,,where I used a shorthand . The probability density of  is f(y_i | \\tau, \\sigma, \\{x_{i,j}\\}) = \\frac{\\sqrt{\\tau}}{\\sqrt{2 \\pi \\left(\\sigma^2\\sigma^2_{i} + 1\\right)}}\\exp\\left[\\frac{\\tau y_i^2}{2 \\left(\\sigma^2\\sigma^2_{i} + 1\\right)}\\right]\\,.This yields the log likelihood function L(\\tau, \\sigma^2: \\{x\\}, Y) = \\sum_{i=1}^n \\log\\left(\\frac{\\sqrt{\\tau}}{\\sqrt{2 \\pi \\left(\\sigma^2\\sigma^2_{i} + 1\\right)}}\\exp\\left[-\\frac{\\tau y_i^2}{2 \\left(\\sigma^2\\sigma^2_{i} + 1\\right)}\\right]\\right)= 0.5 n \\cdot \\log(\\tau) - 0.5 \\sum_{i=1}^n \\log\\left(\\sigma^2\\sigma^2_{i} + 1\\right) - 0.5 n \\cdot \\log(\\pi)- \\sum_{i=1}^n\\frac{\\tau y_i^2}{2 \\left(\\sigma^2\\sigma^2_{i} + 1\\right)}\\,.\\tag{1}The MLE for  is easy to get (derivative  and equating to zero).  0.5 n  \\cdot \\frac{1}{\\tau} - \\sum_{i=1}^n\\frac{y_i^2}{2 \\left(\\sigma^2\\sigma^2_{i} + 1\\right)} = 0\\,,\\boxed{\\hat{\\tau} = \\frac{2}{n}\\left(\\sum_{i=1}^n\\frac{y_i^2}{2 \\left(\\sigma^2\\sigma^2_{i} + 1\\right)}\\right)^{-1}}\\,.For the MLE of , derivative of ,  yields - \\sum_{i=1}^n \\frac{\\sigma^2_{i}}{\\left(\\sigma^2\\sigma^2_{i} + 1\\right)}  + \\sum_{i=1}^n\\frac{\\tau y_i^2  \\sigma^2_{i} }{\\left(\\sigma^2\\sigma^2_{i} + 1\\right)^2\\,.}Rearranging  and equating to zero \\left(\\sum_{i=1}^n \\frac{\\sigma^2_{i}}{\\left(\\sigma^2\\sigma^2_{i} + 1\\right)}  - \\sum_{i=1}^n\\frac{\\tau y_i^2  \\sigma^2_{i} }{\\left(\\sigma^2\\sigma^2_{i} + 1\\right)^2}\\right) = 0\\,.Here I get stuck I don't see how I can solve for  (in the denominators). All the development seems to be too cumbersome, contrary to what the claim in the note is - that it is easy. ","Creater_id":94074,"Start_date":"2016-08-23 09:12:39","Question_id":231314,"Tags":["regression","bayesian","multiple-regression","maximum-likelihood","hierarchical-bayesian"],"Answer_count":0,"Last_activity":"2016-08-23 09:24:11","Link":"http://stats.stackexchange.com/questions/231314/multiple-linear-regression-as-a-hierarchical-model-in-bayesian-framework-cant-s","Creator_reputation":116}
{"_id":{"$oid":"5837a577a05283111e4d39f7"},"View_count":13,"Display_name":"Ben","Question_score":2,"Question_content":"Forgive me for the crummy title.  Not sure how to describe what I want without giving an explicit example...Suppose you go to various geographic regions and measure the population of some species.  Your resulting data looks something like this:train \u0026lt;- data.frame(  Region=c(1,2,3,4,5),  BlackBears=c(NA, 45, 109, 63, 20),  Deer=c(213, NA, 97, 80, NA),  Alligators=c(41, 57, NA, 22, NA),  Pelicans=c(116, 183, NA, 0, 52))  Region BlackBears Deer Alligators Pelicans1      1         NA  213         41      1162      2         45   NA         57      1833      3        109   97         NA       NA4      4         63   80         22        05      5         20   NA         NA       52What statistical model can you use to estimate the population of, say, Alligators in region 3?","Creater_id":31542,"Start_date":"2016-08-22 21:26:11","Question_id":231215,"Tags":["regression","missing-data"],"Answer_count":0,"Last_activity":"2016-08-23 09:07:34","Link":"http://stats.stackexchange.com/questions/231215/how-to-estimate-the-value-of-a-random-variable-in-this-multivariate-setting","Creator_reputation":335}
{"_id":{"$oid":"5837a577a05283111e4d39f9"},"View_count":27,"Display_name":"Munichong","Question_score":1,"Question_content":"I am build a neural network (NN). Its input include the parameters of a distribution, e.g., mean, median, standard deviation.For example, I am using NN to predict the height of a person. The input features include some attributes of the person. A subset of the attributes are the parameters of the height distribution of all people from the same country. The parameters are like the mean, median, and std of the distribution.Does this introduce collinearity problem to the NN? Does this impact the accuracy of the prediction?","Creater_id":35802,"Start_date":"2016-08-23 08:54:50","Question_id":231311,"Tags":["machine-learning","neural-networks","multicollinearity"],"Answer_count":0,"Last_activity":"2016-08-23 08:54:50","Link":"http://stats.stackexchange.com/questions/231311/does-collinearity-largely-impact-the-performance-of-neural-network","Creator_reputation":241}
{"_id":{"$oid":"5837a577a05283111e4d39fb"},"View_count":44,"Display_name":"Henok Fasil","Question_score":1,"Question_content":"If there were no ARCH effects in the residuals, the ACF \u0026amp; PACF should be zero at all lags. However, here in the ACF the first two lags are out of the band; and in the PACF lags 1, 2 and 18 are out of band. What can I conclude? Can I say volatility clustering or ARCH is present?","Creater_id":128659,"Start_date":"2016-08-23 04:49:27","Question_id":231257,"Tags":["arch"],"Answer_count":1,"Last_activity":"2016-08-23 08:53:31","Link":"http://stats.stackexchange.com/questions/231257/can-i-say-the-residuals-have-an-arch-effect-in-this-plot","Creator_reputation":8}
{"_id":{"$oid":"5837a577a05283111e4d39fd"},"View_count":20,"Display_name":"S. Cow","Question_score":0,"Question_content":"A statistician tests the null hypothesis that the proportion ofmen favoring a tax reform proposal is the same as the proportionof women. Based on sample data, the null hypothesisis rejected at the 5% significance level. Does this implythat the probability is at least 0.95 that the null hypothesisis false? If not, provide a valid probability statement.Please, can someone give me some intuition here?","Creater_id":111402,"Start_date":"2016-08-23 08:19:31","Question_id":231306,"Tags":["statistical-significance"],"Answer_count":1,"Last_activity":"2016-08-23 08:45:29","Link":"http://stats.stackexchange.com/questions/231306/significance-level","Creator_reputation":43}
{"_id":{"$oid":"5837a577a05283111e4d39ff"},"View_count":59,"Display_name":"RUser","Question_score":0,"Question_content":"I'm doing a regression using R.Initially I used the fit=lm(data).Got all of my variables are significant including intercept.I checked the VIF using vif(fit) \u0026amp; got maximum VIF as 2.5. But my customer wants model without intercept and I don't have any option other than removing intercept. So I used following line of codefit=lm(A ~ B+C+D+E+F-1,data=data) , I'm just coding the variables as it is client data \u0026amp; I can't share that.The data set I used in first model is same as the data set used in second model with same set of variables.Only in second model I removed intercept forcefully.But after running the model I'm seeing my maximum vif is coming 2079.30. I'm not able to understand the reason for such high vif as I used to think that VIF determines how much the variance of a coefficient is “inflated” because of linear dependence with other predictors \u0026amp; it does not depend on intercept.Can you expert please help me understand why VIF is drastically changed after removing intercept in R","Creater_id":39764,"Start_date":"2016-08-23 04:22:03","Question_id":231252,"Tags":["r","regression"],"Answer_count":1,"Last_activity":"2016-08-23 08:30:20","Link":"http://stats.stackexchange.com/questions/231252/high-vif-after-removing-intercept-in-r","Creator_reputation":23}
{"_id":{"$oid":"5837a577a05283111e4d3a01"},"View_count":36,"Display_name":"Greconomist","Question_score":0,"Question_content":"My code is very simperequire(rugarch)require(quantmod)    #Daily GARCH(1,1)    date_from = c(\"1996-01-01\", \"2000-01-02\", \"2004-01-03\", \"2008-01-04\", \"2012-01-05\")    date_to = c(\"2000-01-01\", \"2004-01-02\", \"2008-01-03\", \"2012-01-04\", \"2016-08-20\")    forex = vector(mode = 'list', length = 5)    for (i in 1:5) {      getSymbols(\"EUR/AUD\", src=\"oanda\", from = date_from[i], to = date_to[i])      forex[[i]] = EURAUD    }    EURAUD = Reduce(rbind,forex)    EURAUDEUR.AUD))    spec7 = ugarchspec(variance.model = list(model = \"sGARCH\", garchOrder = c(1,1)),                       mean.model = list(armaOrder = c(0,0), include.mean = TRUE))    roll_7 = ugarchroll(spec7, EURAUD[,2], forecast.length = 1500, refit.every = 50, windows.size = 1500, refit.window = 'moving', solver = 'hybrid', calculate.VaR = FALSE, keep.coef = FALSE)Unfortunately it fails to converge. What are my alternatives? The standard GARCH(1,1) is crucial as it is proven by Hanen and Lunde (2005) that provides the most accurate forecasts and I nee as a benchmark model for my thesis.","Creater_id":112822,"Start_date":"2016-08-23 06:33:00","Question_id":231280,"Tags":["r","garch"],"Answer_count":1,"Last_activity":"2016-08-23 08:27:50","Link":"http://stats.stackexchange.com/questions/231280/garch1-1-fails-to-converge-in-rugarch-in-r","Creator_reputation":13}
{"_id":{"$oid":"5837a577a05283111e4d3a03"},"View_count":23,"Display_name":"StatsPlease","Question_score":2,"Question_content":"I'm aware of testing the proportional hazards assumption in the context of Cox PH models, but I haven't encountered anything relating to parametric models? Is there a feasible way to test the PH assumption of certain parametric models?It seems like there should be given that parametric models are only slightly different from the semi-parametric Cox models?","Creater_id":102399,"Start_date":"2016-08-23 08:13:31","Question_id":231303,"Tags":["survival","assumptions","proportional-hazards"],"Answer_count":0,"Last_activity":"2016-08-23 08:13:31","Link":"http://stats.stackexchange.com/questions/231303/testing-proportional-hazards-assumption-in-parametric-models","Creator_reputation":445}
{"_id":{"$oid":"5837a577a05283111e4d3a05"},"View_count":116,"Display_name":"cgo","Question_score":5,"Question_content":"Consider a gridworld 100  100 with a starting position, ,  on the lower left corner and a goal position, , somewhere at the center. Both  and  are fixed and does not move. Furthermore, subgoals exist one at a time. For example, '1' appears in a random position. If '1' is reached, then '2' appears and so on. The succeeding numbers appear randomly but will have a distance closer and closer to the final goal . The transition probability is not available and will not be learned, so model free learning will be used.Agent can move in 4 cardinal directions. I have many serious questions:What could be a nice state space representation for this? The fact that it is 100  100 means that I am avoiding the usual  'coordinate' position or the cardinality of the state space will be so big.My idea would be to choose the 'relative' position between the agent and the goal. Something like: , .  if the goal is to the left of the agent, 1 if the goal is to the right, and 0 if the goal is at the same  coordinate as the agent. ,  if the goal is below,  if above the agent. 0 if they the same coordinate.So maybe the state space formulation can follow this form found here:Model free reinforcement learning with subgoals: how to reinforce learning with only one reward? where  is the number subgoal attained? Is this a tractable problem? I mean, I understand that this may have been poorly written, but I want to share my intuition. The number of subgoals that I have here might not be known apriori. But they are definitely going nearer and nearer to the goal position. Is there a way to write the states to include the subgoal positions without giving multiple rewards? I thought about giving multiple rewards to the agent after getting each subgoal, but that would be 'cheating'. I believe it is standard in RL that the reward is given at the end of the task. I would some your valued insights into this problem. Feel free to send comments or more questions if there are any. ","Creater_id":67413,"Start_date":"2016-08-13 08:35:53","Question_id":229676,"Tags":["machine-learning","algorithms","reinforcement-learning"],"Answer_count":1,"Last_activity":"2016-08-23 08:06:20","Link":"http://stats.stackexchange.com/questions/229676/formulation-of-states-for-this-rl-problem-and-other-questions","Creator_reputation":591}
{"_id":{"$oid":"5837a577a05283111e4d3a07"},"View_count":22,"Display_name":"user2705000","Question_score":1,"Question_content":"I have 4 conditions, (Control, Stim-1, Stim-2, Stim-3) and 9 different proteins expression levels in each conditions.  The sample number of each protein expression level in each conditions are about 40 ~ 130 individuals. I would like to set significant level as 0.05. In order to control for the overall significance level of 0.05 considering nine simultaneous tests(9 protein expression tests), the significant level is modified 0.05/9 = 0.00555556Then, there is pair-wise comparisons adjustment. So I divide 0.0055556 with 6 pairs numbers (con-stim1, con-stim2, con-stim3, stim1-stim2, stim1-stim3, stim2-stim3)0.05/9/ ((4*3)/2)  = 0.000925925Is it right ?Any comments would be appreciated. ","Creater_id":128682,"Start_date":"2016-08-23 08:05:07","Question_id":231302,"Tags":["p-value","multiple-comparisons","kruskal-wallis","adjustment"],"Answer_count":0,"Last_activity":"2016-08-23 08:05:07","Link":"http://stats.stackexchange.com/questions/231302/what-should-be-proper-p-value-cutoff-point-for-multiple-comparison-case","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3a09"},"View_count":164,"Display_name":"Gerenuk","Question_score":3,"Question_content":"Sometimes I encode categorical features as binary values - one feature per possible category value indicating whether that feature name matches the original category value (i.e. one-of-K scheme).Now these values are linearly dependent, since obviously their total sum is 1.Does this linear dependence matter for linear SVM, kernel SVM, logistic regression, etc.?Where does it matter so that I need to remove one of the features? Does it cause problems for normal linear regression?For which methods does it not make a difference?","Creater_id":8694,"Start_date":"2015-01-18 14:30:10","Question_id":133967,"Tags":["logistic","svm","data-preprocessing"],"Answer_count":2,"Last_activity":"2016-08-23 07:57:50","Link":"http://stats.stackexchange.com/questions/133967/does-collinearity-of-one-hot-encoded-features-matter-for-svm-and-logreg","Creator_reputation":541}
{"_id":{"$oid":"5837a577a05283111e4d3a0b"},"View_count":38,"Display_name":"Lilly","Question_score":0,"Question_content":"I’m planning a regression analysis. The outcome is the score of an achievement test. Assuming there are 120 items and a person has answered 90 items correctly, the person gets a raw value of 90. This raw value can be converted in an age related t score. Do I use the age related t score or the raw score in conjunction with controlling for age? What is better?","Creater_id":66160,"Start_date":"2016-08-23 07:07:33","Question_id":231289,"Tags":["regression","analysis","age"],"Answer_count":1,"Last_activity":"2016-08-23 07:53:32","Link":"http://stats.stackexchange.com/questions/231289/regression-analysis-with-raw-value-or-age-related-t-score","Creator_reputation":3}
{"_id":{"$oid":"5837a577a05283111e4d3a0d"},"View_count":700,"Display_name":"Matek","Question_score":12,"Question_content":"While learning about Gradient Boosting, I haven't heard about any constraints regarding the properties of a \"weak classifier\" that the method uses to build and ensemble model. However, I could not imagine an application of a GB that uses linear regression, and in fact when I've performed some tests - it doesn't work. I was testing the most standard approach with a gradient of sum of squared residuals and adding the subsequent models together.The obvious problem is that the residuals from the first model are populated in such manner that there is really no regression line to fit anymore. My another observation is that a sum of subsequent linear regression models can be represented as a single regression model as well (adding all intercepts and corresponding coefficients) so I cannot imagine how that could ever improve the model. The last observation is that a linear regression (the most typical approach) is using sum of squared residuals as a loss function - the same one that GB is using.I also thought about lowering the learning rate or using only a subset of predictors for each iteration, but that could still be summed up to a single model representation eventually, so I guess it would bring no improvement.What am I missing here? Is linear regression somehow inappropriate to use with Gradient Boosting? Is it because the linear regression uses the sum of squared residuals as a loss function? Are there any particular constraints on the weak predictors so they can be applied to Gradient Boosting?","Creater_id":67868,"Start_date":"2015-12-15 17:41:15","Question_id":186966,"Tags":["regression","machine-learning","boosting","ensemble","gradient"],"Answer_count":1,"Last_activity":"2016-08-23 07:29:29","Link":"http://stats.stackexchange.com/questions/186966/gradient-boosting-for-linear-regression-why-does-it-not-work","Creator_reputation":100}
{"_id":{"$oid":"5837a577a05283111e4d3a0f"},"View_count":25,"Display_name":"user_anon","Question_score":0,"Question_content":"Here is my way of viewing inferential statistics so far:Sampling distribution - basic conceptWhat's the point of inferential statistics here? Oh, well, estimating parameters! OK so here comes:Estimation:a. point estimation;b. interval estimation = confidence interval.Pretty well. But now comes:Hypothesis testing. Is there any link between confidence intervals and hypothesis testing or are they separate concepts? Is 'estimation' the link? How come? I can't see the link between estimation and hypothesis testing...","Creater_id":125475,"Start_date":"2016-08-23 07:05:44","Question_id":231288,"Tags":["hypothesis-testing","inferential-statistics"],"Answer_count":0,"Last_activity":"2016-08-23 07:20:02","Link":"http://stats.stackexchange.com/questions/231288/introductory-statistics-overview","Creator_reputation":102}
{"_id":{"$oid":"5837a577a05283111e4d3a11"},"View_count":40,"Display_name":"Giorgia Sala","Question_score":1,"Question_content":"I have two samples with a mean and SE for each. I want to combine them, so how do I calculate a combined standard error when combining two samples means? I can only find information about combining means and SD's at the moment. ","Creater_id":128495,"Start_date":"2016-08-21 20:15:32","Question_id":231027,"Tags":["mean","standard-error"],"Answer_count":1,"Last_activity":"2016-08-23 07:04:41","Link":"http://stats.stackexchange.com/questions/231027/combining-samples-based-off-mean-and-standard-error","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3a13"},"View_count":52,"Display_name":"hxd1011","Question_score":3,"Question_content":"I am confused when I read the boosting papers. In boosting our model is a sum of base learners:f(x)=\\sum_{m=1}^M b_m(x)where  is number of iterations in boosting,  is the model for  iteration.Now, here is my question: can the base learner be linear? If the base learner is linear, does the whole model reduced into a simpler linear model?For example, suppose we just run  iterations, and   and , then f(x)=\\sum_{m=1}^2 b_m(x)=\\beta_0+ \\beta_1x+\\theta_0+ \\theta_1x=(\\beta_0+\\theta_0)+ (\\beta_1+ \\theta_1)xwhich is a simple linear model ! In other words, the ensemble model have the \"same power\" with the base learner!What's wrong there? Or nothing wrong, we just do not want to select linear model as a base learner?","Creater_id":113777,"Start_date":"2016-08-23 07:03:02","Question_id":231286,"Tags":["regression","machine-learning","boosting"],"Answer_count":0,"Last_activity":"2016-08-23 07:03:02","Link":"http://stats.stackexchange.com/questions/231286/in-boosting-if-the-base-learner-is-a-linear-model-does-the-final-model-is-just","Creator_reputation":4423}
{"_id":{"$oid":"5837a577a05283111e4d3a15"},"View_count":60,"Display_name":"dasboth","Question_score":0,"Question_content":"My understanding is that in machine learning it can be a problem if your dataset has highly correlated features, as they effectively encode the same information.Recently someone pointed out that when you do one-hot encoding on a categorical variable you end up with correlated features, so you should drop one of them as a \"reference\".For example, encoding gender as two variables, is_male and is_female, produces two features which are perfectly negatively correlated, so they suggested just using one of them, effectively setting the baseline to say male, and then seeing if the is_female column is important in the predictive algorithm.That made sense to me but I haven't found anything online to suggest this may be the case, so is this wrong or am I missing something?Possible (unanswered) duplicate: Does collinearity of one-hot encoded features matter for SVM and LogReg?","Creater_id":94687,"Start_date":"2016-08-23 06:51:01","Question_id":231285,"Tags":["machine-learning","categorical-data"],"Answer_count":0,"Last_activity":"2016-08-23 06:51:01","Link":"http://stats.stackexchange.com/questions/231285/dropping-one-of-the-columns-when-using-one-hot-encoding","Creator_reputation":28}
{"_id":{"$oid":"5837a577a05283111e4d3a17"},"View_count":43,"Display_name":"ddf","Question_score":1,"Question_content":"I'm conducting a meta-analysis with a data set that is somewhat similar in structure to this post, where there are multiple effect sizes per study. Specifically, most studies I'm analyzing contain multiple experiments, and the effect sizes from these experiments are dependent (i.e., are derived from the same participants). Here's a depiction of what a small part the data looks like:esid  studyid  sampleid  testtype   1        1         1         a   2        2         2         b   3        2         3         b   4        3         4         a   5        3         4         a   6        3         4         b        esid = effect size identifierstudyid = study identifier (same number = effect size from same study)sampleid = sample (correlated effect) identifier (same number = effect size from same participants)I'm planning on using metafor to fit a three-level model (similar to Konstantopoulos, 2011), including a random effect at the studyid level (i.e., paper level) since effect sizes are nested within studies:ml.mod \u0026lt;- rma.mv(yi, vi, random = ~1 | studyid/esid, data = dat)Then, using the robust function to account for the dependency between some effect sizes (clustering at the sampleid level):robust(ml.mod, cluster = datsampleid, adjust = TRUE)Or would it be more appropriate to conduct separate meta-analyses looking at the effect when using testtype a and then separately using testtype b (and not directly compare them):ml.mod.a \u0026lt;- rma.mv(yi, vi, random = ~1 | studyid/esid, data = dat, subset=(testtype==\"a\")robust(ml.mod.a, cluster = datsampleid, adjust = TRUE)Any suggestions would be extremely appreciated by this newbie! Thank you!","Creater_id":128619,"Start_date":"2016-08-23 05:53:11","Question_id":231267,"Tags":["interaction","multilevel-analysis","meta-analysis","robust"],"Answer_count":1,"Last_activity":"2016-08-23 06:50:50","Link":"http://stats.stackexchange.com/questions/231267/moderator-analysis-in-multilevel-meta-analysis-of-dependent-effect-sizes","Creator_reputation":8}
{"_id":{"$oid":"5837a577a05283111e4d3a19"},"View_count":53,"Display_name":"Shivi Bhatia","Question_score":0,"Question_content":"After using weight of evidence \u0026amp; Information value mechanism, of the 40 odd   variables I am left with 8 variables which are highly or moderately significant.One of the independent variable which is categorical has 60+ categories. This is a very highly predictable variable hence please suggest as to how should Iuse this variable in the model.When I add this variable in the model my null deviance and AIC decreases   and makes other predictors loose their predictive power.Then another model without this variable my null deviance and AIC improves.What could be the reason. Is this variable collinear with some other predictor.   Please see the syntax: \u0026lt; Without that Categorical Var\u003e  m1.logit\u0026lt;- glm(survey ~ region+ know + repS+ und+ case_status, family = binomial(logit), data = a1 )m1.logit  summary(m1.logit)Call:  glm(formula = survey ~ region + know + repS + und + case_status,     family = binomial(logit), data = a1)  Deviance Residuals:        ` Min       1Q   Median    3Q     Max`    -2.579    0.271   0.290   0.336   2.895    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 2553.5  on 2540  degrees of freedomResidual deviance: 1287.7  on 2526  degrees of freedomAIC: 1318    Number of Fisher Scoring iterations: 13Also ran an anova test to analyze the table of deviance  anova(m1.logit, test=\"Chisq\")   Analysis of Deviance Table  Model: binomial, link: logit  Response: survey  Terms added sequentially (first to last)               Df Deviance Resid. Df Resid. Dev             Pr(\u0026gt;Chi)      NULL                         2540       2554                            region       5       13      2535       2540                0.022 *     know         1      507      2534       2033 \u0026lt; 0.0000000000000002 ***   repS         1      715      2533       1319 \u0026lt; 0.0000000000000002 ***   und          1        3      2532       1316                0.109         case_status  6       28      2526       1288             0.000078 ***     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  Please suggest as to how to deal with this predictor variable with 50+ categories  ","Creater_id":79611,"Start_date":"2016-08-23 04:59:23","Question_id":231258,"Tags":["r","regression","logistic","statistical-significance"],"Answer_count":1,"Last_activity":"2016-08-23 06:47:23","Link":"http://stats.stackexchange.com/questions/231258/logistic-regression-in-r-with-multi-level-categorical-variable","Creator_reputation":13}
{"_id":{"$oid":"5837a577a05283111e4d3a1b"},"View_count":1413,"Display_name":"Jeromy Anglim","Question_score":18,"Question_content":"Background:  Previously on Cross Validated, we have had questions on:What is best practice when preparing plots?What are good tips available online for plotting two numeric variables?It was suggested by @david in the comments to this question that we should have a community wiki question with one visualization rule per answer that the community could vote on.Question  What are the essential rules on designing and producing graphical representations of data?Rules  One rule per answerIdeally, include a brief explanation of why you think it is a good ideaAnswers with examples (code and image) of good and bad practice preferred.","Creater_id":183,"Start_date":"2011-10-06 14:40:29","Question_id":16631,"Tags":["data-visualization"],"Answer_count":8,"Last_activity":"2016-08-23 06:42:53","Link":"http://stats.stackexchange.com/questions/16631/what-are-essential-rules-for-designing-and-producing-plots","Creator_reputation":27863}
{"_id":{"$oid":"5837a577a05283111e4d3a1d"},"View_count":89,"Display_name":"olveh","Question_score":2,"Question_content":"What would be a good test to check if two sample variances are significantly different when data are non normal (leptokurtic, slight negative skew) and heteroskedastic?The samples are of equal size, 500.My impression are that most tests assume normality and homoskedasticity.Edit: I see that my original post was a bit unclear.I have two samples, where each sample is created by giving different weights to 10 random variables (e.g. one sample has equal weighting of the random variables, 10%, and the other sample have different weights, say 5% for 9 of the variables and 55% for the 10th). All these 10 random variables exhibit heteroskedasticity, and are non normal. The exact weighting of each random variable changes in both samples under a testing period of 500 observations.I want to test whether or not the sample variance of these two samples are significantly different or not, is there a way to do this?Could I in some way use Levene's test (or Brown–Forsythe)?","Creater_id":127755,"Start_date":"2016-08-22 14:10:48","Question_id":231165,"Tags":["hypothesis-testing","statistical-significance","anova","heteroscedasticity"],"Answer_count":2,"Last_activity":"2016-08-23 06:30:09","Link":"http://stats.stackexchange.com/questions/231165/what-test-to-see-if-variances-are-different-when-data-are-non-normal","Creator_reputation":13}
{"_id":{"$oid":"5837a577a05283111e4d3a1f"},"View_count":84,"Display_name":"KevinKim","Question_score":2,"Question_content":"When I first time learn multiple linear regression, I remember the interpretation of the regression coefficient is that: the marginal contribution of a specific predictor.Now I am rethinking this interpretation and create an example to ask myself a question: is this interpretation always correct?Suppose I have 2 dummy variables in my data set  and , to make it concrete, they are defined as the following\\begin{equation*}X_i=\\begin{cases}\u0026amp;1,~\\text{if people  go to school before}\\\\\u0026amp;0,~\\text{otherwise}\\end{cases},~~~Z_i=\\begin{cases}\u0026amp;1,~\\text{if people  drink milk}\\\\\u0026amp;0,~\\text{otherwise}\\end{cases}\\end{equation*}Let  be the response variable, say the wage of people . There are 4 combinations of the value of . If we can FORCE all people in the population have a specific value of , then we will be able to get a wage distribution, denoted by random variable . So we have , ,  and . Note that, without loss of generality, we can always assume that \\begin{equation*}Y^1_{1i}=\\mu^1_1+\\epsilon^1_{1i},~Y^1_{0i}=\\mu^1_0+\\epsilon^1_{0i},~Y^0_{1i}=\\mu^0_1+\\epsilon^0_{1i},~Y^0_{0i}=\\mu^0_0+\\epsilon^0_{0i}\\\\\\end{equation*}where  is the mean of the wage if the people in the population ALL have value  and . After rearrangement, it is easily to write , which is the observed wage in the real world, as\\begin{align*}Y_i\u0026amp;=\\mu^0_0+X_i(\\mu^0_1-\\mu^0_0)+Z_i(\\mu^1_0-\\mu^0_0)+X_iZ_i(\\mu^1_1-\\mu^0_1-\\mu^1_0+\\mu^0_0)\\\\\u0026amp;+\\underset{\\epsilon_i}{\\underbrace{\\epsilon^0_{0i}+X_i(\\epsilon^0_{1i}-\\epsilon^0_{0i})+Z_i(\\epsilon^1_{0i}-\\epsilon^0_{0i})+X_iZ_i(\\epsilon^1_{1i}-\\epsilon^0_{1i}-\\epsilon^1_{0i}+\\epsilon^0_{0i})}}\\end{align*}To write it clearly, we have\\begin{equation}Y_i=a+bX_i+cZ_i+dX_iZ_i+\\epsilon_i,\\label{eq:2D}\\end{equation}where Note that this equation decribes the TRUTH of the world (without making ANY assumption) and all of these coefficients have real meaning, e.g.,  is the increment of mean wage if the population have no milk but change from no school to all go to school, i.e., the effect of schooling when  (no milk). Note that whether we can consistently estimate these coefficient is another story.Now, what if when you see the data set , you build up the following model\\begin{equation}Y_i=a'+b'X_i+c'Z_i+\\epsilon'_i,\\end{equation}i.e., you didn't incorporate the term  in your model, which means that you dump this term into the error term, then here comes my questions:(1) if you don't include the term , is it always true that the error term will be correlated with the predictors? Intuitively, it looks yes, since how could the random variable  and  being uncorrelated? Then this could implies that OLS cannot consistently estimate . If for some reason that I don't know,  is uncorrelated with  and , then we know that the omitted variable  does not affect the OLS, so in this case, , but then what is the correct interpretation of these coefficients?  is still: \"the increment of mean wage if the population have no milk but change from no school to all go to school\"? (2) somewhat related to (1), if initially I only have data , then I build the model , where , i.e., the mean wage of population if nobody go to school regardless of whether they drink milk and . Now somehow I observe the information about whether each individual drink milk or not. Then I project the error term  onto , i.e., . Then I run , what is the meaning of  and  in the new model? Are they the same as in the old model (both in the theoretical value and the interpretation)?(3) For any dataset we have (with a response variable  and a predictors vector ), we can always write , where  is the residual, which is defined as . By definition,  is uncorrelated with . In this post,  are all dummies, , then  is linear, i.e., you could write  as . Note that those  have nothing to do with the aforementioned . Here those Greek letters have completely different interpretation, e.g., here  means the impact of unit change in  on  when , which is a pure description about the relationship between the predictors and the conditional expectation, that has nothing to do with the counterfactual effect like the interpretation of . So in this case, if we omit the variable , and model the linear functional  as , then  just means the impact of unit change in  on  (no \"when \"). In empirical study, which interpretation of the parameters should we adopt? the  one or the  one? Note that as  is the residual (NOT the  aforementioned), which is defined as , then by definition,  is uncorrelated with . Hence, in this case, the OLS is always consistent, in the sense that, OLS estimator will converge to those 's (the Greek letter), which does not necessarily equal to  (the English letter). I recall that when I learned econometrics, the consistency of OLS estimator is a big chunk of lectures, so I assume the correct interpretation of the coefficients in the linear regression model should be the  one, rather than the  one, since otherwise, the OLS estimator is always consistent. Did I miss something important?","Creater_id":66461,"Start_date":"2016-08-22 11:08:35","Question_id":231140,"Tags":["multiple-regression","estimation","linear-model","least-squares","regression-coefficients"],"Answer_count":1,"Last_activity":"2016-08-23 06:15:29","Link":"http://stats.stackexchange.com/questions/231140/understanding-the-meaning-of-the-parameters-in-the-linear-regression-model","Creator_reputation":1625}
{"_id":{"$oid":"5837a577a05283111e4d3a21"},"View_count":88,"Display_name":"Dave","Question_score":2,"Question_content":"I would like to understand how the gradient and hessian of the logloss function are computed in an xgboost sample script.I've simplified the function to take numpy arrays, and generated y_hat and y_true which are a sample of the values used in the script. Here is the simplified example:import numpy as npdef loglikelihoodloss(y_hat, y_true):    prob = 1.0 / (1.0 + np.exp(-y_hat))    grad = prob - y_true    hess = prob * (1.0 - prob)    return grad, hessy_hat = np.array([1.80087972, -1.82414818, -1.82414818,  1.80087972, -2.08465433,                  -1.82414818, -1.82414818,  1.80087972, -1.82414818, -1.82414818])y_true = np.array([1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.])loglikelihoodloss(y_hat, y_true)The log loss function is the sum of  where .The gradient (with respect to p) is then  however in the code its . Likewise the second derivative (with respect to p) is however in the code it is .How are the equations equal?","Creater_id":116759,"Start_date":"2016-08-22 23:15:46","Question_id":231220,"Tags":["entropy","loss-functions","derivative","gradient","xgboost"],"Answer_count":1,"Last_activity":"2016-08-23 05:57:40","Link":"http://stats.stackexchange.com/questions/231220/how-to-compute-the-gradient-and-hessian-of-logarithmic-loss-question-is-based","Creator_reputation":135}
{"_id":{"$oid":"5837a577a05283111e4d3a23"},"View_count":348,"Display_name":"Guess Gucci","Question_score":0,"Question_content":"The R code \"intervals()\" gives confidence intervals for fixed effects only in a mixed model.*Is there a reason why only fixed effects' confidence intervals are provided?*Is there any way to get confidence intervals for random effects as well?","Creater_id":21071,"Start_date":"2013-04-23 10:59:43","Question_id":56991,"Tags":["r","mixed-model","confidence-interval","mathematical-statistics"],"Answer_count":1,"Last_activity":"2016-08-23 05:55:17","Link":"http://stats.stackexchange.com/questions/56991/how-to-calculate-95-ci-for-a-random-effect","Creator_reputation":140}
{"_id":{"$oid":"5837a577a05283111e4d3a25"},"View_count":45,"Display_name":"vivek","Question_score":2,"Question_content":"It is a general question on linear regression with polynomial terms. I am using x and x^2 as my independent variables and my regression equation is as follows y=a0+a1*x+a2*x^2+a3*z+e. I am doing regression to find the coefficients. I have read through this article, which explains that I need to de-mean the variable x before I run the regression to obtain my coefficients a1 and a2. Once we de-mean our new variables (de-meaned and its square) are not correlated.I was wondering is it necessary to always de-mean the variables when we use the polynomial term as well?EDIT: Clarifying further - when i am using the linear and quadratic terms both. Do I need to de-mean the linear term (call it demean_x) and take the square of it (demean-x^2) and use them for regression instead of x and x^2.","Creater_id":128657,"Start_date":"2016-08-23 04:32:39","Question_id":231255,"Tags":["regression","nonlinear-regression"],"Answer_count":0,"Last_activity":"2016-08-23 05:35:45","Link":"http://stats.stackexchange.com/questions/231255/linear-regression-with-non-linear-terms","Creator_reputation":11}
{"_id":{"$oid":"5837a577a05283111e4d3a27"},"View_count":20,"Display_name":"Stephen Carman","Question_score":0,"Question_content":"I have a 10 year time series of weekly data that comes with weekly additional variables with the idea that 0 or more of these variables would be useful predictors in determining the value of the time series into the future. An obvious look at the data would imply that there is some seasonality in the data that should be removed in order to work on the underlying trend, but I am debating whether a seasonal decomposition is necessary or not. The data is segmented into specific geographic areas and has a categorical label for that area, so I ran plotted a smoothed periodogram of the data by geographic location and it appears that the valid period for the data is a week, which seems to imply to me that there is no seasonality in the data or at least it does not display enough seasonality for it to matter. Currently stl() is used to do a seasonal decomp, but the proper frequency of 1 wouldn't work on an stl(), so I'm wondering again if the stl is even necessary.I'm looking for suggestions regarding perhaps other ways to determine the seasonality window in the data and perhaps models or methods that play well into that window in order to be able to find the variables that most effect the underlying time series. Currently it's done via a bunch of regressions and they take the best performing one, but it feels like obviously there is a better way than this. I was thinking mutual information or some kind of information gain metric might be a good way to feature select for a model, but I'm not sure. Thus any insight would be awesome, thanks!","Creater_id":128662,"Start_date":"2016-08-23 05:01:41","Question_id":231259,"Tags":["regression","time-series","forecasting","seasonality","spectral-analysis"],"Answer_count":0,"Last_activity":"2016-08-23 05:01:41","Link":"http://stats.stackexchange.com/questions/231259/should-i-decompose-time-series-data","Creator_reputation":101}
{"_id":{"$oid":"5837a577a05283111e4d3a29"},"View_count":62,"Display_name":"user_anon","Question_score":1,"Question_content":"I've read somewhere that we often choose an estimator for the parameter in H0, H1 as the test statistic.I've also noticed that we use the sampling distribution as the chosen distribution very often.Do we always do this?","Creater_id":125475,"Start_date":"2016-08-22 23:27:05","Question_id":231224,"Tags":["hypothesis-testing","inferential-statistics"],"Answer_count":1,"Last_activity":"2016-08-23 04:32:15","Link":"http://stats.stackexchange.com/questions/231224/how-to-choose-the-test-statistic-in-a-hypothesis-test","Creator_reputation":102}
{"_id":{"$oid":"5837a577a05283111e4d3a2b"},"View_count":24,"Display_name":"Valerie","Question_score":0,"Question_content":"Is it reasonable to use Cohen's d guidelines in the interpretation of aCox logit d'?","Creater_id":117882,"Start_date":"2016-06-04 03:37:55","Question_id":216253,"Tags":["cox-model"],"Answer_count":0,"Last_activity":"2016-08-23 04:28:31","Link":"http://stats.stackexchange.com/questions/216253/effect-size-interpretation-from-cox-logit-d","Creator_reputation":1}
{"_id":{"$oid":"5837a577a05283111e4d3a2d"},"View_count":20,"Display_name":"Dave","Question_score":1,"Question_content":"I have a 2d data set where one dimension is circular (direction and speed). I would like to create a kernel density estimate but am unsure how to create a kernel. One idea I had was to use a von mises distribution in one direction, a gaussian in the other and then take the outer product of the results to give a 2d kernel. Would this work or has anyone any other/better ideas?","Creater_id":69486,"Start_date":"2016-08-23 04:23:40","Question_id":231253,"Tags":["kernel-smoothing","circular-statistics"],"Answer_count":0,"Last_activity":"2016-08-23 04:23:40","Link":"http://stats.stackexchange.com/questions/231253/2d-kde-with-one-circular-dimension","Creator_reputation":106}
{"_id":{"$oid":"5837a577a05283111e4d3a2f"},"View_count":98,"Display_name":"Alex","Question_score":0,"Question_content":"I am running a Logistic Regression and checking for the assumption of Multivariate Outliers. My Mahalanobis and Cook's distances are all within the acceptable values, however some of the Leverage values exceed the number of 3 times the mean.So does that still mean that I have multivariate outliers or can I ignore the Leverage values since the Mahalanobis and Cook's distance show otherwise? Additionally, I have spotted no participants that are outliers in all the predictor variables. I am not sure whether the assumption has been met or if I should be looking further. And if it is not met, can I still run the regression?Thank you so much in advance","Creater_id":95646,"Start_date":"2015-11-19 09:56:54","Question_id":182609,"Tags":["regression","logistic","multivariate-analysis","outliers"],"Answer_count":0,"Last_activity":"2016-08-23 03:27:42","Link":"http://stats.stackexchange.com/questions/182609/logistic-regression-multivariate-outliers","Creator_reputation":1}
{"_id":{"$oid":"5837a577a05283111e4d3a31"},"View_count":71,"Display_name":"JacKeown","Question_score":2,"Question_content":"The wikipedia page for Bayesian networks lists belief networks as a synonym so basically my question is how are bayesian networks related to deep belief networks? From here I gathered that deep belief networks are like deep neural networks where the weights and biases are set by iterations of an unsupervised algorithm. (I hope I'm understanding that right anyway.)  ","Creater_id":122137,"Start_date":"2016-08-22 20:04:35","Question_id":231204,"Tags":["machine-learning","neural-networks","bayesian-network","networks"],"Answer_count":1,"Last_activity":"2016-08-23 03:19:49","Link":"http://stats.stackexchange.com/questions/231204/belief-networks-and-bayesian-networks","Creator_reputation":27}
{"_id":{"$oid":"5837a577a05283111e4d3a33"},"View_count":35,"Display_name":"Harry UNL","Question_score":1,"Question_content":"I have one dataset of two variables (x,y). When the data is plotted in a 2D diagram, I see some data points create a good cluster (i.e., green points), while the other data points are scattered randomly (i.e., red points). I am interested in running a clustering algorithm to identify the smaller cluster (i.e., green points) buried inside of other objects or other clusters.Would you please help me which clustering algorithm typically helps me better in this case? kNN?Thank you!","Creater_id":126672,"Start_date":"2016-08-22 09:10:29","Question_id":231116,"Tags":["clustering","k-nearest-neighbour"],"Answer_count":3,"Last_activity":"2016-08-23 03:09:32","Link":"http://stats.stackexchange.com/questions/231116/identify-a-smaller-cluster-of-objects-buried-inside-of-other-objects-in-other-cl","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3a35"},"View_count":143,"Display_name":"houneida","Question_score":2,"Question_content":"I chose the sampling method based on probability proportional to size (PPS) to conduct a survey among school children. The total number of schools (clusters) in my population is 140. My concern is how to set the number total of clusters to select? There are conditions to be respected?I would be grateful for any advice or guidance.","Creater_id":55247,"Start_date":"2014-11-01 04:35:25","Question_id":122274,"Tags":["sampling"],"Answer_count":1,"Last_activity":"2016-08-23 03:08:08","Link":"http://stats.stackexchange.com/questions/122274/numbre-of-cluster-in-pps-sampling-method","Creator_reputation":16}
{"_id":{"$oid":"5837a577a05283111e4d3a37"},"View_count":17418,"Display_name":"dissertationhelp","Question_score":3,"Question_content":"As part of the analysis of the data collected from a survey, I was carrying out Kruskal-Wallis Tests between some Likert scale questions and demographics such as firm size and job position. I am using this test instead of One Way Anova since the data is non-normal. For example, testing the responses of the following statement vs Job position (manager or junior):I arrive late at work.      Rated on a 5-point scale from never to always.When reporting the results of the test (if result is significant) can I state something like:Managers are more likely to arrive late than juniors (H=14.338, p\u0026lt;.01)or is this the only way one can report the result:A statistically significant difference (H=14.338, p\u0026lt;.01) exists between late arrivals at work by managers and juniors.","Creater_id":40183,"Start_date":"2014-02-14 06:19:39","Question_id":86588,"Tags":["kruskal-wallis"],"Answer_count":2,"Last_activity":"2016-08-23 03:07:59","Link":"http://stats.stackexchange.com/questions/86588/how-to-report-kruskal-wallis-test","Creator_reputation":151}
{"_id":{"$oid":"5837a577a05283111e4d3a39"},"View_count":27,"Display_name":"user82729","Question_score":0,"Question_content":"Following Gelman et al. 2008, we can use e.g. in the case of complete separation, weakly informative prior distributions to estimate a bayesian logistic regression. However I wonder, whether there is a way to do something similar for a multinominal regression (I have 3 outcomes). It would be awesome if there is something implemented for stata or r. Thanks in advance.","Creater_id":82729,"Start_date":"2016-08-23 02:00:57","Question_id":231232,"Tags":["regression","logistic","multinomial","logit","mlogit"],"Answer_count":1,"Last_activity":"2016-08-23 02:56:04","Link":"http://stats.stackexchange.com/questions/231232/is-there-a-way-to-use-weakly-informative-default-prior-distributions-for-multino","Creator_reputation":19}
{"_id":{"$oid":"5837a577a05283111e4d3a3b"},"View_count":287,"Display_name":"Gabriel","Question_score":4,"Question_content":"I need a good reference for the methods and/or the difficulties that arise when attempting such a combination.I've found Loughin, TM, A systematic comparison of methods for combining p-values from independent tests, Computational Statistics \u0026amp; Data Analysis (2004) 47(3):467–485, for example. Is there a better/more precise one? ","Creater_id":10416,"Start_date":"2012-09-27 04:31:03","Question_id":38099,"Tags":["p-value","references","combining-p-values"],"Answer_count":1,"Last_activity":"2016-08-23 01:59:31","Link":"http://stats.stackexchange.com/questions/38099/what-are-good-references-for-the-different-methods-of-combining-p-values","Creator_reputation":690}
{"_id":{"$oid":"5837a577a05283111e4d3a3d"},"View_count":151,"Display_name":"Asher11","Question_score":2,"Question_content":"Essentially this tutorial (http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#example-cluster-plot-kmeans-assumptions-py ) explains very clearly some limitations of the k-means clustering method which are rooted in its assumptions. Specifically:1) k-means assumes the variance of the distribution of each attribute (variable) is spherical;2) all variables have the same variance;3) the prior probability for all k clusters is the same, i.e., each cluster has roughly equal number of observations;Now, thanks to the silhouette method, I can handle the case with the wrong number of blobs but I am in the dark for how to handle the other cases. Could you point me to some methods to solve the other issues and if, possible, how to blend them toghether since in my datasets these behaviours coexist?","Creater_id":68970,"Start_date":"2016-08-21 11:10:39","Question_id":230989,"Tags":["clustering","k-means"],"Answer_count":1,"Last_activity":"2016-08-23 01:53:27","Link":"http://stats.stackexchange.com/questions/230989/clustering-k-means-alternatives-when-its-assumptions-do-not-hold","Creator_reputation":20}
{"_id":{"$oid":"5837a577a05283111e4d3a3f"},"View_count":75,"Display_name":"user2224259","Question_score":2,"Question_content":"I have  pairs of samples from different distributions. For each pair of samples I want to check if the samples are taken from a normal distribution with the same mean and variance. I assume the normal distributions for each pair is different and independent of other pairs. I do not know the mean and variance of any of the distributions.I am doing a two sample t-test for each of the pairs. I want to combine the  resulting p-values. I could use Fisher's method but I am wondering about the distribution of the average t-value. If these were z-values rather than t-values I would use Stouffer's z-test. I could also approximate the t-distribution with a z-distribution and than use Stouffer's z-test, but I am not sure if that is a good approximation.Is there a test like Stouffer's z-test for t-values?Is there a better way to combine the t-values?","Creater_id":68195,"Start_date":"2015-02-05 02:33:29","Question_id":136415,"Tags":["t-test","z-test","combining-p-values"],"Answer_count":1,"Last_activity":"2016-08-23 01:49:27","Link":"http://stats.stackexchange.com/questions/136415/is-there-a-t-test-equivalent-to-stouffers-z-test","Creator_reputation":11}
{"_id":{"$oid":"5837a577a05283111e4d3a41"},"View_count":48,"Display_name":"Gillian Cheung","Question_score":0,"Question_content":"I want to know how to simulate arima if there is drift.I have google it, and i search this text http://mgmt.iisc.ernet.in/CM/MG226/Handouts/Simulations.pdf but i dont understandFor instance , if i want to simulate ARIMA(1,1,1),for n=100if there is no drift,the code should be:x=arima.sim(n=100,list(ar=0.1,ma=0.2,order=c(1,1,1))However, what is the code should be if there is drift?","Creater_id":117642,"Start_date":"2016-08-22 20:54:05","Question_id":231213,"Tags":["r","time-series","self-study"],"Answer_count":1,"Last_activity":"2016-08-23 01:24:42","Link":"http://stats.stackexchange.com/questions/231213/how-to-simulate-arima-with-driftusing-r","Creator_reputation":88}
{"_id":{"$oid":"5837a577a05283111e4d3a43"},"View_count":23,"Display_name":"muni","Question_score":0,"Question_content":"How can I analyze the 5-6 way interaction. Where my variables are:Age,sex,income,occupation,education. They all are binned. I want to find the combinations that are maximizing my binary target variable rate, given individually none of them have good separation ability.","Creater_id":124691,"Start_date":"2016-08-23 01:15:03","Question_id":231229,"Tags":["interaction"],"Answer_count":0,"Last_activity":"2016-08-23 01:15:03","Link":"http://stats.stackexchange.com/questions/231229/how-to-analyse-multi-way-interaction","Creator_reputation":81}
{"_id":{"$oid":"5837a577a05283111e4d3a45"},"View_count":103,"Display_name":"Bence","Question_score":1,"Question_content":"I have implemented a classifier (simple Neural network) for classifying e-mails in both TensorFlow and Theano. The necessary pre-processing steps have been implemented in Python. I would like to develop a simple e-mail client in Android, which would be able to fetch e-mails and use the former model to classify them, however I'm having problem with finding out which approach would be the best:I had been looking for possible ways before I found one, which suggests to use a Django server to run the implemented model on.Another approach would be to somehow run the whole model on Android, but I don't know whether it is possible to run it at all (I know it should be possible to port a TensorFlow model on Android but what about the preprocessing steps?).Which one should I apply? If you think neither of these would work, please let me know what other way you would take!I would greatly appreciate any help or suggestions!","Creater_id":91265,"Start_date":"2016-08-23 00:41:27","Question_id":231228,"Tags":["classification","neural-networks","python","tensorflow","theano"],"Answer_count":0,"Last_activity":"2016-08-23 00:41:27","Link":"http://stats.stackexchange.com/questions/231228/running-tensorflow-theano-models-on-android","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3a47"},"View_count":39,"Display_name":"radm94","Question_score":0,"Question_content":"We have the equation:I should technically be able to prove that it works even when the total number of models is 1, so I go from the right hand side of the equationBut I should be able to get P(C|X). Am I missing something?","Creater_id":120198,"Start_date":"2016-08-22 19:31:53","Question_id":231203,"Tags":["conditional-probability","model-averaging"],"Answer_count":2,"Last_activity":"2016-08-23 00:25:35","Link":"http://stats.stackexchange.com/questions/231203/model-averaging","Creator_reputation":59}
{"_id":{"$oid":"5837a577a05283111e4d3a49"},"View_count":13,"Display_name":"Jenni","Question_score":1,"Question_content":"I have 425 sample points that have been classified according to land cover in two time periods. When comparing how percent land cover has changed over time, is this a one sample test (ie. because the same points are sampled), or a two sample test (ie. because two separate time periods)? I'm pretty sure that each time period should represent a separate sample and so a two sample approach should be used, but just want to check!","Creater_id":128637,"Start_date":"2016-08-22 23:57:46","Question_id":231225,"Tags":["time-series"],"Answer_count":0,"Last_activity":"2016-08-22 23:57:46","Link":"http://stats.stackexchange.com/questions/231225/one-sample-or-two-sample-when-comparing-the-same-points-over-time","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3a4b"},"View_count":35,"Display_name":"Tomas Sebastian H\u0026#228;t\u0026#228;l\u0026#228;","Question_score":0,"Question_content":"I have a question that occurred when thinking about the following use case: A bank wants to group their customers into segments using the database tables 'customer', 'account' and 'transaction'. The first table contains customer information (e.g. id, birth date, sex, age), the second one account information (e.g. id, cust_id, balance, product) and the last one transactions (e.g. id, acct_id, value). One customer can have multiple accounts and one account can have multiple associated transactions.Based on this information the bank wants to cluster their customers into segments. Traditionally, they would manually create segments like ‘age \u0026lt; 21’, ‘balance \u003e 10000’, etc. But, now they want a machine to tell them which customers are similar (so they can be targeted for marketing).There is no specific output column and the data is not labeled, thus, unsupervised machine learning algorithms like K-Means are well suited. However, K-Means for example does require the data in one single table. One cannot input multiple tables into the algorithm and link the tables through foreign constraints (at least not using common implementations like scikit-learn, h2o.ai, mahout, etc.). Therefore, the three tables need to be joined into one large table. This large table will have one row per unique transaction, since this is the most detailed information. I believe this step is called denormalization (or normalization for machine learning).Only then the ML algorithm (e.g. K-Means) can be executed to gain a list of which rows belong to which cluster. Then, as a result of the cluster algorithm the output is X clusters with Y transactions assigned each. An example could be:Transaction 1 = Cluster 1Transaction 2 = Cluster 1Transaction 3 = Cluster 2Transaction 4 = Cluster 1Transaction 5 = Cluster 3Transaction 6 = Cluster 2Transaction 7 = Cluster 3However, the initial question was to cluster the customers (not transactions) into segments. For the sake of the above example lets say transactions 1 to 3 belong to customer 1 and the rest to customer 2. Transactions that belong to customer 1 are included in clusters 1 and 2 and transactions belonging to customer 2 are in clusters 1, 2 and 3. Finally my question: Is there a best practice for this? How does the bank now get k clusters which include similar customers from this?","Creater_id":128577,"Start_date":"2016-08-22 12:45:58","Question_id":231151,"Tags":["machine-learning","clustering","unsupervised-learning"],"Answer_count":1,"Last_activity":"2016-08-22 23:26:29","Link":"http://stats.stackexchange.com/questions/231151/machine-learning-aggregate-granular-cluster-predictions-on-denormalized-data","Creator_reputation":1}
{"_id":{"$oid":"5837a577a05283111e4d3a4d"},"View_count":107,"Display_name":"drodd","Question_score":0,"Question_content":"I searched all over but was unable to find an answer to this question. Please forgive me if I missed something obvious.In order to analyze an experiment, I recently implemented a Hierarchical Beta-Binomial Model like the one detailed in this blog post. This model fit my needs because I had 5 variants and wanted to measure the proportion of \"successes\" (i.e., clicks on a particular link) in each one.(In brief, this model works by assuming that each variant's success rate can be modeled as a Binomial Random Variable and that the rate for each Binomial Random Variable is drawn from a common Beta distribution.)Now, this is awesome for my simple experiment; however, let's say I'm also interested in analyzing this experiment for two separate populations: Wizards and Warlocks.I could theoretically split my experiment into ten variants -- WizA, WarA, WizB, WarB, etc. -- but I had a few questions:Would that even be valid?Is there a better way? I suppose I could implement a version of a multilevel regression model, but I was wondering if there was a simple extension either of the conceptual model or of the code in that blog post that would elegantly handle this experimental use case: must I immediately and necessarily look into something more complex?(Also: apologies for my naivete here -- I took basic stats in college but never learned much about these Bayesian approaches. I'm hoping to refine my understanding since this seems much more elegant than running a bunch of t-tests and applying manual Bonferroni corrections, which is all I ever learned how to do.)","Creater_id":127804,"Start_date":"2016-08-15 22:08:27","Question_id":230034,"Tags":["bayesian","multilevel-analysis","hierarchical-bayesian","pymc","beta-binomial"],"Answer_count":2,"Last_activity":"2016-08-22 23:25:24","Link":"http://stats.stackexchange.com/questions/230034/extending-a-hierarchical-beta-binomial-model-to-account-for-higher-level-groups","Creator_reputation":1}
{"_id":{"$oid":"5837a577a05283111e4d3a4f"},"View_count":2302,"Display_name":"Quora Feans","Question_score":18,"Question_content":"Is stats maths or not? Given that it's all numbers, mostly taught by maths departments and you get maths credits for it, I wonder whether people just mean it half-jokingly when they say it, like saying it's a minor part of maths, or just applied maths. I wonder if something like statistics, where you can't build everything on basic axioms can be considered maths. For example, the -value, which is a concept that arose to make sense of data, but it's not a logical consequence of more basic principles.","Creater_id":35694,"Start_date":"2013-12-04 14:05:12","Question_id":78579,"Tags":["mathematics","philosophical"],"Answer_count":8,"Last_activity":"2016-08-22 23:11:24","Link":"http://stats.stackexchange.com/questions/78579/stats-is-not-maths","Creator_reputation":220}
{"_id":{"$oid":"5837a577a05283111e4d3a65"},"View_count":23,"Display_name":"A K R","Question_score":1,"Question_content":"I am estimating a state space model but I am totally confused about specification of the state/transition equations. How should I decide whether the state equation should be a random walk or random walk with draft or in any other form? For example,y1 = a + sv1*y2 + eThen how should I decide that the state variable sv1 (if time varying) follows a random walk or random walk with draft or its an AR process? I am modelling the anchorage of long run inflation expectations on lagged inflation as well as short run inflation expectations with time varying coefficients. So, I am confused so as to what should be the movement of the time varying coefficients for these two variables be like.Also, how do we decide whether intercept (a) should be a part of my model? Should it depend on the significance of the intercept? Please clarify. Thank you","Creater_id":110352,"Start_date":"2016-08-19 04:34:25","Question_id":230675,"Tags":["state-space-models"],"Answer_count":0,"Last_activity":"2016-08-22 22:32:15","Link":"http://stats.stackexchange.com/questions/230675/formulation-of-state-equations","Creator_reputation":21}
{"_id":{"$oid":"5837a577a05283111e4d3a67"},"View_count":150,"Display_name":"KevinKim","Question_score":4,"Question_content":"The notion of uncorrelated () and mean independence () are mentioned in different setting of regression assumptions. We know that  implies  (but not the other way round). Here is a specific question about the relationship between these two notions in the regression setting.We are looking at the effect of whether go to school or not on the wage of a population. Let  be the random variable denote whether individual  went to school () or not (). Let  be the wage of people . Note that if we can FORCE everyone in the population go to school, then we will have a wage distribution denoted by  and similarly, if we FORCE all people not go to school, we have a wage distribution denoted by . So we have .Note that we can always write  and , i.e., mean plus a noise with mean 0. Then, we substitute these 2 equations into equation (1), we havewhere Note that  has 0 mean clearly.So equation (2) describes the real world about wage and school without making any assumptions other than the mean of  and  is finite.Note that  will always dependent with  (but they are not necessarily correlated). Now suppose  and  are uncorrelated (first, I don't know what does this mean in practice), then we know that OLS estimator is consistent (for unbiasedness of OLS, it would require mean independence, i.e., ). So  and  is identifiable. In this case,  and  being uncorrelated is equivalent to . I wonder if someone could explain the underlying meaning of this expression in this setting.Note that a sufficient condition for  is that . I can understand this expression very well, which is \"given the information of  is not going to change the mean of the random variable \". Note that this is weaker than the notion of independence, since  independent of  means that given the information of , the distribution of  remains the same, which is much stronger than the first moment remains the same (i.e., ).The expression  can be explained intuitively if we look at this identification problem from a different angle, we have:.Note that we observe  and  and we want to identify , which requires . Note that if randomly assign school or not school to people in the population, this will guarantee  (or even if we don't have randomized assignment, but somehow, we know that , then we are still able to make this claim). However, if we only have  and  are uncorrelated, i.e., , this will not imply . But then it implies that by purely look at the group mean (i.e.,  and ) will not help us identify , but run OLS will achieve this goal. Where is my logic going wrong?","Creater_id":66461,"Start_date":"2016-08-16 20:01:00","Question_id":230200,"Tags":["regression","econometrics","linear-model","consistency","identification"],"Answer_count":1,"Last_activity":"2016-08-22 21:58:43","Link":"http://stats.stackexchange.com/questions/230200/understanding-mean-independence-in-the-regression-setting","Creator_reputation":1625}
{"_id":{"$oid":"5837a577a05283111e4d3a73"},"View_count":196,"Display_name":"hko","Question_score":2,"Question_content":"I am trying to analyze a quite large (~25,000 rows) dataset of cash flow forecasts. Receipts and expenses are aggregated, thus I may end up with the following data:Forecast = 6.0e+04Actual = -5.0e+04But also withForecast = 1.0e+06Actual = 1.5e+06or Forecast = 1.0Actual = 2e+06As you can see, the actual can differ from the forecast in order of magnitude and even signs. However, I need to find a metric for the forecast error that works for all these cases and that is scale independent.So far, I have used the Absolute Percentage Error, which works reasonably well for most of my data – but the few outliers (large forecast, small actual) render the mean absolute percentage error (MAPE) useless. I then moved to the symmetric mean absolute percentage error:mean(abs((act - forc)/(act + forc)))This limits outliers as the output is between [0,1], but not if there's a sign change (=11 for the first example).Are you aware of any metrics, that limit the influence of outliers while allowing to compare across forecasting horizons and series (scale independent) and that work with changing signs?Thanks!","Creater_id":39705,"Start_date":"2014-02-11 07:33:33","Question_id":86215,"Tags":["forecasting","outliers","error","mape"],"Answer_count":1,"Last_activity":"2016-08-22 21:02:59","Link":"http://stats.stackexchange.com/questions/86215/scale-independent-forecast-error-metric-that-works-with-changing-signs","Creator_reputation":11}
{"_id":{"$oid":"5837a577a05283111e4d3a80"},"View_count":40,"Display_name":"dontloo","Question_score":3,"Question_content":"I'm working on a classification task, where there's a large number (30k+) of categories, and I know some of these categories are much closer to each other than the others (but I don't know exactly which ones).It seems the cross entropy loss may not be the best choice because it treats every category independently.For instance say there're five almost indistinguishable categories, and my prediction is 0.2 for each and zero for others, which I think is a very good prediction, but it still gets a large cross entropy loss. I'm thinking maybe I can modify the loss to something like \"if the predicted probability is above 0.2 for the ground truth category, it causes no loss\", will that work?What should be a proper loss in my case?or what else can I do (to impose the assumption that some categories are similar)?","Creater_id":95569,"Start_date":"2016-08-22 07:08:47","Question_id":231097,"Tags":["machine-learning","classification","categorical-data","loss-functions"],"Answer_count":0,"Last_activity":"2016-08-22 20:31:17","Link":"http://stats.stackexchange.com/questions/231097/classification-with-similar-categories","Creator_reputation":2762}
{"_id":{"$oid":"5837a577a05283111e4d3a82"},"View_count":185,"Display_name":"Dave","Question_score":2,"Question_content":"I have a decent grasp of neural networks, back propagation and chain rule however I am struggling to understand auto differentiation.The below refer to auto differentiation outside the context of back propagation:How does auto differentiation compute the gradient from a matrix? What are the requirements to compute a gradient? Does a function need to be specified?What are some use cases for this (other then back propagation)?Why is it important and what are the alternatives?Am I missing something?","Creater_id":116759,"Start_date":"2016-08-21 07:53:23","Question_id":230973,"Tags":["machine-learning","neural-networks","deep-learning","tensorflow","automatic-differentiation"],"Answer_count":1,"Last_activity":"2016-08-22 20:05:49","Link":"http://stats.stackexchange.com/questions/230973/what-is-an-example-use-of-auto-differentiation-such-as-implemented-in-tensorflow","Creator_reputation":135}
{"_id":{"$oid":"5837a577a05283111e4d3a8f"},"View_count":367,"Display_name":"muhammad waseem","Question_score":1,"Question_content":"I am trying to train my model using Scikit-learn's Random forest (Regression) and have tried to use GridSearch with Cross-validation (CV=5) to tune hyperparameters. I fixed n_estimators =2000 for all cases. Below are the few searches that I performed.max_features :[1,3,5], max_depth :[1,5,10,15], min_samples_split:[2,6,8,10], bootstrap:[True, False]The best were max_features=5, max_depth = 15, min_samples_split:10, bootstrap=TrueBest score = 0.8724Then I searched close to the parameters that were best;max_features :[3,5,6], max_depth :[10,20,30,40], min_samples_split:[8,16,20,24], bootstrap:[True, False]The best were max_features=5, max_depth = 30, min_samples_split:20, bootstrap=TrueBest score = 0.8722Again, I searched close to the parameters that were best;max_features :[2,4,6], max_depth :[25,35,40,50], min_samples_split:[22,28,34,40], bootstrap:[True, False]The best were max_features=4, max_depth = 25, min_samples_split:22, bootstrap=TrueBest score = 0.8725Then I used GridSearch among the best parameters found in the above runs and found the best on as max_features=4, max_depth = 15, min_samples_split:10, Best score = 0.8729Then I used these parameters to predict for an unknown dataset but got a very low score (around 0.72). My questions are; I doing the hyperparameter tuning correctly or I am missing something? Why is my testing score very low as compared to my training and validation score and how can I improve it so that I get good predictions out of my model?Sorry, if these are basic questions as I am new to scikit-learn and ML.P.S: The training (+Cross validation data) has 26138 samples with 6 features/inputs (columns) and one output. The testing data has 1416 samples. ","Creater_id":75090,"Start_date":"2016-02-05 09:10:51","Question_id":194226,"Tags":["random-forest","python","scikit-learn","hyperparameter"],"Answer_count":1,"Last_activity":"2016-08-22 20:01:44","Link":"http://stats.stackexchange.com/questions/194226/random-forest-low-score-on-testing-data-scikit-learn","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3a9c"},"View_count":139,"Display_name":"floyd","Question_score":2,"Question_content":"i read that sigmoid function will kill the gradient as a result the network will not learnand i read that in ReLU function a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activateand that may lead “dead” neurons.so, are they the same?","Creater_id":116480,"Start_date":"2016-08-17 15:57:10","Question_id":230399,"Tags":["machine-learning","neural-networks","deep-learning","conv-neural-network"],"Answer_count":2,"Last_activity":"2016-08-22 19:05:34","Link":"http://stats.stackexchange.com/questions/230399/what-is-the-difference-between-dead-neuron-and-killing-the-gradient","Creator_reputation":77}
{"_id":{"$oid":"5837a577a05283111e4d3aaa"},"View_count":15,"Display_name":"radm94","Question_score":0,"Question_content":"I was reading chapter 1 of Neural Networks and Machine Learning by Bishop and I was trying to understand the concept of predictive probability. First of all, we have  as a training set of examples with their correct classification, along with a family  of possible joint probabilities, with a probability distribution . Does this mean that that the possible joint probabilities are a family because the parameter  can take several different values?Then we move on to risk. We say that the risk R(c,) depends on  and that we allow c to depend to depend on . Does this happen because X is parameterised by  and therefore we can say that the classification will ultimately depend on ? Why does c depend on ?We consider the goal of minimising risk. where C is the actual class that X belongs to. How did we establish the second equality? I think I understand the logic behind it (without questioning the previous assumptions), but I can't establish it mathematically.We also say  and thenI don't think I understand the concepts involved in this equation. I know that  is a parameter and that we are essentially integrating the conditional distribution over values of theta, but I don't understand how we get our original equation using this, and I also don't understand why  is conditional over  and x.I might be confused by the notation. Can I replace the \";\" with a \",\" without losing meaning?","Creater_id":120198,"Start_date":"2016-08-22 18:08:44","Question_id":231200,"Tags":["neural-networks"],"Answer_count":0,"Last_activity":"2016-08-22 18:56:25","Link":"http://stats.stackexchange.com/questions/231200/predictive-probability-in-terms-of-parameter-theta-and-risk","Creator_reputation":59}
{"_id":{"$oid":"5837a577a05283111e4d3aac"},"View_count":11983,"Display_name":"dfhgfh","Question_score":3,"Question_content":"Anyone's got a quick short educational example how to use neural networks (nnet in R for example) for the purpose of prediction? Here is an example, in R, of a time seriesT \u0026lt;- seq(0,20,length=200)Y \u0026lt;- 1 + 3*cos(4*T+2) +.2*T^2 + rnorm(200)plot(T,Y,type=\"l\")This just an example but what I have is jumpy-seasonal data.","Creater_id":18287,"Start_date":"2013-01-03 08:25:40","Question_id":46887,"Tags":["r","time-series","machine-learning","neural-networks","nnet"],"Answer_count":1,"Last_activity":"2016-08-22 17:53:42","Link":"http://stats.stackexchange.com/questions/46887/example-of-time-series-prediction-using-neural-networks-in-r","Creator_reputation":264}
{"_id":{"$oid":"5837a577a05283111e4d3ab9"},"View_count":4058,"Display_name":"JRW","Question_score":6,"Question_content":"I'm working with some real world data and the regression models are yielding some counterintuitive results. Normally I trust the statistics but in reality some of these things can not be true. The main problem that I am seeing is that an increase in one variable is causing an increase in the response when, in fact in reality, they must be negatively correlated.Is there a way to force a specific sign for each of the regression coefficients? Any R code to do this would be appreciated as well.Thanks for any and all help!","Creater_id":48288,"Start_date":"2014-06-26 09:46:48","Question_id":104890,"Tags":["r","regression","regression-coefficients","computational-statistics"],"Answer_count":3,"Last_activity":"2016-08-22 17:37:14","Link":"http://stats.stackexchange.com/questions/104890/is-it-possible-in-r-or-in-general-to-force-regression-coefficients-to-be-a-cer","Creator_reputation":61}
{"_id":{"$oid":"5837a577a05283111e4d3ac8"},"View_count":629,"Display_name":"Steve","Question_score":4,"Question_content":"I was working on a question on projection matrix. Since, projection matrix is idempotent, symmetric and square matrix, it must always be equal to  (Identity matrix). This can be shown by multiplying the inverse of projection matrix on both the sides. If it is equal to , then I do not understand the point of using it. Can anyone please explain?","Creater_id":128563,"Start_date":"2016-08-22 10:44:27","Question_id":231135,"Tags":["econometrics","matrix","projection"],"Answer_count":3,"Last_activity":"2016-08-22 17:04:25","Link":"http://stats.stackexchange.com/questions/231135/since-a-projection-matrix-is-idempotent-symmetric-and-square-why-isnt-it-just","Creator_reputation":23}
{"_id":{"$oid":"5837a577a05283111e4d3ad7"},"View_count":25,"Display_name":"slazien","Question_score":0,"Question_content":"I have been struggling with understanding the solution to one of exercises in Blitzstein's book Introduction to Probability. My question is: in part b), why can we just drop  in the term ? As I understand it, we could do this if  happened which would imply that  must have happened too in which case %X=15% gives us no new information and we could drop  (or vice versa). This is, I think, equivalent to saying that  so that  (or, again, vice versa). Please correct me if I'm wrong as I want to be crystal clear on that. However, I can't really see this situation occurring here, could someone please explain this to me? ","Creater_id":121401,"Start_date":"2016-08-22 15:41:13","Question_id":231185,"Tags":["probability","mathematical-statistics","conditional-probability"],"Answer_count":1,"Last_activity":"2016-08-22 16:31:12","Link":"http://stats.stackexchange.com/questions/231185/conditional-pmf-drop-an-event-on-which-we-condition","Creator_reputation":58}
{"_id":{"$oid":"5837a577a05283111e4d3ae4"},"View_count":13,"Display_name":"Escachator","Question_score":0,"Question_content":"I understand that, in reggression trees, the split between leaves is decided by splitting the learning samples in two sets so that the split provides the maximum loss reduction. However, once this is decided, where is the exact limit placed in a continuous feature? If you decide to sepparate the examples in two sets, at which value of the continuous feature do you set the limit? Maybe in the middle of the two most close examples that belong to different leaves? For example, imagine we have a continuous feature ranging from 0 to 5, and we have the following sample values for that feature:[1.4, 1.9, 3, 4.2, 4.8]Let us say that the optimum split is: leaf 1: [1.4, 1.9, 3] leaf 2: [ 4.2, 4.8] In order to do predictions, where do you set the limit between the two leaves?And how is it done in practice, for example in XGBOOST?","Creater_id":93768,"Start_date":"2016-08-22 15:54:52","Question_id":231188,"Tags":["classification","random-forest","cart"],"Answer_count":0,"Last_activity":"2016-08-22 15:54:52","Link":"http://stats.stackexchange.com/questions/231188/where-exactly-the-splitting-point-of-a-reggression-tree-is-set","Creator_reputation":55}
{"_id":{"$oid":"5837a577a05283111e4d3ae6"},"View_count":58,"Display_name":"Alan_dj","Question_score":3,"Question_content":"*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. Thank you all for reading and help! This question is closed.I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -http://dx.doi.org/10.1016/j.eja.2012.10.003). Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.Example:The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : non-finite value supplied by optimError in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : Convergence failed Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. ","Creater_id":128483,"Start_date":"2016-08-22 03:49:47","Question_id":231066,"Tags":["r","regression","logistic","nonlinear-regression","nonlinear"],"Answer_count":0,"Last_activity":"2016-08-22 15:46:38","Link":"http://stats.stackexchange.com/questions/231066/log-logistic-regression-model-vs-nonlinear-regression-in-seed-germination","Creator_reputation":16}
{"_id":{"$oid":"5837a577a05283111e4d3ae8"},"View_count":79,"Display_name":"raK1","Question_score":7,"Question_content":"Optimizing the full likelihood function is sometimes time consuming and contains a lot of numerical issues and instabilities especially when matrix inversion is needed.If we have 3 input vectors  let . Based on this, I want to model  \\mathbf{y}\\sim N(0,K) where K is a joint covariance matrix between all three input vectors.The Inputs are connected as shown in the figure below:  and  are independent while both  and  depend on .In other words the covariance matrix  is expressed as followsK=\\begin{pmatrix}k_{11} \u0026amp; 0 \u0026amp; k_{13}\\\\ 0\u0026amp; k_{22} \u0026amp; k_{23} \\\\ k_{13} \u0026amp;  k_{23} \u0026amp; k_{33}\\end{pmatrix}Where  is any postive semidefinite covariance function. Is there any specific way for me to exploit this independence and factorize the full likelihood function for example can I write f(\\boldsymbol{y}_{1},\\boldsymbol{y}_{2},\\boldsymbol{y}_{3})=f(\\boldsymbol{y}_{3}|\\boldsymbol{y}_{1},\\boldsymbol{y}_{2})*f(\\boldsymbol{y}_{1})*f(\\boldsymbol{y}_{2}) and optimize each part seprately? and what is the expression of  given the covariance matrix above. Also is there a specific way I can use a composite likelihood approach such as a pairwise likelihood method.Any good references on such normal likelihood factorizations is greatly appreciated","Creater_id":78448,"Start_date":"2016-08-22 11:13:46","Question_id":231141,"Tags":["maximum-likelihood","covariance","likelihood","gaussian-process"],"Answer_count":1,"Last_activity":"2016-08-22 15:34:11","Link":"http://stats.stackexchange.com/questions/231141/likelihood-factorization","Creator_reputation":600}
{"_id":{"$oid":"5837a577a05283111e4d3af5"},"View_count":43,"Display_name":"amrapaliz","Question_score":1,"Question_content":"I have 20 students, 1000 questions with five options a, b, c, d, e. First, I am using rmultinom(1000,1,c(0.2,0.2,0.2,0.2,0.2) to generate the truth. Next, I want to simulate what a student chooses for each question.Suppose for Q1, the correct answer is a, I need to simulate the probability of student 1 of choosing the correct answer to be higher than the truth. And then let the choice be uniform for the other 4 choices.Which probability function should I use to simulate the answers?","Creater_id":128592,"Start_date":"2016-08-22 14:40:58","Question_id":231172,"Tags":["r","probability"],"Answer_count":1,"Last_activity":"2016-08-22 15:15:42","Link":"http://stats.stackexchange.com/questions/231172/simulating-probability-of-choosing-the-right-answer","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3b02"},"View_count":20,"Display_name":"Poppy","Question_score":1,"Question_content":"I am trying to test whether the difference between the two proportion I have found is significant or not. I am just unsure as to whether my samples are independent or dependent and as a result what test to use.I have a sample of patients from one GP practice. Some have been sent recalls by text message and some have been sent recalls by letter. I have found the % of people who followed through with their recall for each group (those who were sent a text message and those who were sent a letter). It is possible that the same person has been sent a letter and a text message as the data is over a 2.5 year time period. What test should I use to see if the % increase or decrease is significantly different? z-test (independent sample)? chi-squared test? ","Creater_id":128593,"Start_date":"2016-08-22 14:43:21","Question_id":231173,"Tags":["statistical-significance","proportion","sample","dependent"],"Answer_count":1,"Last_activity":"2016-08-22 15:07:53","Link":"http://stats.stackexchange.com/questions/231173/independent-or-dependent-sample","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3b0f"},"View_count":14,"Display_name":"Julien","Question_score":1,"Question_content":"I am interested in estimating a nested logit model following McFadden (1978)'s formulation. It is simple to numerically verify the result in a standard situation with independently drawn error terms, but I do not know what the cdf is when error can be correlated.To review, McFadden shows that the expectation of the maximum for options  is:E[ \\max \\{V_1 + \\epsilon_1 , ... V_N + \\epsilon_N  \\}] = 0.5772 + \\Big( \\sum_{i=1}^N \\exp[V_i]\\Big)Where .This can be verified numerically - drawing plenty of  values from an Extreme Value Type I (or Gumbel) distribution with a location value of 0 and scale parameter of 1. In a nested logit model, the  are correlated with parameters . Say that that  are correlated this way. Then the expectation of maximum values is:0.5772 + \\ln\\Big( (\\sum_{k=1}^K \\exp[ \\dfrac{V_k}{1- \\rho}])^{1-\\rho} + \\sum_{i=K+1}^N \\exp[V_i] \\Big)I want to verify this numerically to make sure I am coding this second expectation correctly. However, I do not know what the cdf is for a case where draws of  are correlated with parameter . Thanks!","Creater_id":109959,"Start_date":"2016-08-22 15:05:03","Question_id":231178,"Tags":["distributions","logistic","simulation","gumbel"],"Answer_count":0,"Last_activity":"2016-08-22 15:05:03","Link":"http://stats.stackexchange.com/questions/231178/cdf-for-correlated-gumbel-distribution-in-nested-logit-simulation","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3b11"},"View_count":22,"Display_name":"Paul","Question_score":3,"Question_content":"  Let  be i.i.d. random variables from , where  is scale parameter. I know that  is the Fisher information. If I apply a reparametrization using the rate parameter  I get the new Fisher information .Now, based on this, I can get the relation between  and  by i_{\\lambda}(\\lambda)=i_{\\theta}(\\theta(\\lambda))\\left(\\frac{d\\theta}{d\\lambda}\\right)^2 soi_{\\lambda}(\\lambda)=\\frac{n}{\\lambda^2}=\\frac{n}{(\\frac{1}{\\lambda})^2}\\left(-\\frac{1}{\\lambda^2}\\right)^2=\\frac{n}{\\lambda^2}Is this correct?","Creater_id":105511,"Start_date":"2016-08-22 08:51:10","Question_id":231112,"Tags":["self-study","fisher-information"],"Answer_count":1,"Last_activity":"2016-08-22 14:58:33","Link":"http://stats.stackexchange.com/questions/231112/fisher-information-and-exponential-reparametrization","Creator_reputation":59}
{"_id":{"$oid":"5837a577a05283111e4d3b1e"},"View_count":24,"Display_name":"Wilka","Question_score":4,"Question_content":"I have a small app for helping with ranking of items. It uses lots of pairwise comparisons done by a human. I'd like to minimize the number of comparisons the human needs to do, while still getting close to the ideal ranking for each item (where \"ideal\" is what the human would produce if they considered every pair of items).I've found a StackOverflow question asking the same kind of thing but I'm wondering if there's something I should be using from the stats world, rather than just considering sorting algorithms.","Creater_id":116,"Start_date":"2016-08-22 06:57:16","Question_id":231093,"Tags":["paired-comparisons"],"Answer_count":1,"Last_activity":"2016-08-22 14:54:47","Link":"http://stats.stackexchange.com/questions/231093/approximating-ordered-list-with-minimum-number-of-comparisons","Creator_reputation":121}
{"_id":{"$oid":"5837a577a05283111e4d3b28"},"View_count":43,"Display_name":"user2035177","Question_score":0,"Question_content":"I am analyzing the occurrence of emergencies in a given area and the application of queueing theory to determine the resources an emergency service should have ready in order to answer to emergency calls.For basic analysis, the Erlang B formula gives the probability of an emergency occurring while no resources are available. Erlang B uses the offered load, the product of the arrival rate of an emergency and the time the resource is bound to the emergency.Given data from dispatch centers the arrival rate can either be counted (e.g. for each hour of a day) or the inter arrival time can be calculated. Both are empirical values which I determine for each hour, weekday combination of a given year. Both measures are also interchangeable (in an ideal setup). For example, an arrival rate of 2 per hour equals an inter arrival time of 0.5 hour or 30 minutes. So, you can calculate one from the other. However, from the empirical perspective I observe differences in the arrival rates and inter arrival times.An example:For each Sunday (52 in total in the given year) and the hour from 9AM to 10AM I got the following values:Total events: 428Arrival rate from total events: 428 / 52 = 9.269 arrivals per hourMean inter arrival time: 409.206 secondsArrival rate from mean inter arrival time: 3600 seconds / 409.206 seconds = 8.798 arrivals per hourThe difference between the rates does give different values for the Erlang B and thus does have an impact on the analysis. Depending on the chosen threshold the number of servers (resources) needed to answer every emergency directly varies. It's no big variation, but can lead to a difference of resources.In this particular example:chosen quality threshold: 99.9 %, which is (1-Erlang B) * 100needed resources from total events arrival rate: 3needed resources for inter arrival time arrival rate: 4The question now is, which result is the more appropriate and why? Is it just pure reason or is there a scientific base for the more appropriate result?Final thoughts: The arrival rate from total events is a value derived from a directly counted integer quantity. Small variations in this quantity might have a bigger impact on the arrival rate than small variations in the inter arrival time measured in seconds. However, the arrival rate derived from the mean inter arrival time is a value calculated from the mean of the observed times, which for the given example are mostly 52 values (occurrences of weekday, hour combination in a year). Intuitively, the inter arrival time option seems more appropriate to me.Finally, one might think about larger time intervals. For now, the 1 hour intervals are used because of the variation in the arrival rates over a day (24 hour analysis does not capture peeks e.g. at noon). This might involve analyzing the data for clusters of similar arrivals or inter arrival times.","Creater_id":37477,"Start_date":"2016-08-09 14:31:14","Question_id":229077,"Tags":["prediction","markov-process"],"Answer_count":1,"Last_activity":"2016-08-22 14:46:54","Link":"http://stats.stackexchange.com/questions/229077/measure-the-arrival-rate-or-inter-arrival-time-for-a-queueing-model","Creator_reputation":118}
{"_id":{"$oid":"5837a577a05283111e4d3b35"},"View_count":11,"Display_name":"dorien","Question_score":1,"Question_content":"I ran the following Quadratic regression. I want to report my findings, and I want to include the DFs for x and x^2. Can I assume they are 55? (2+53). The R output is not very clear on this. Sorry, I know it's a basic question but I just wanted some confirmation. lm(formula = y ~ x + x2)Residuals:     Min       1Q   Median       3Q      Max -0.67303 -0.13330 -0.00798  0.11700  0.84015    Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)   3.1395     0.2049  15.325  \u0026lt; 2e-16 ***x             2.4679     0.6627   3.724 0.000476 ***x2           -1.3211     0.4410  -2.995 0.004160 ** ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1   Residual standard error: 0.2869 on 53 degrees of freedomMultiple R-squared:  0.377, Adjusted R-squared:  0.3535 F-statistic: 16.03 on 2 and 53 DF,  p-value: 3.585e-06So to clarify, I want to report this in the following way, but was wondering if 55 is correct: (R^2 =.38, F(2,53)=16.03, p\u0026lt;.001), where X significantly predicts Y (\\beta = .47, t(55) = 4.28, p\u0026lt;.001), as does X^2 (\\beta = -.32, t(55) = -3.00, p\u0026lt;.01).","Creater_id":16912,"Start_date":"2016-08-22 14:36:07","Question_id":231170,"Tags":["degrees-of-freedom"],"Answer_count":0,"Last_activity":"2016-08-22 14:42:30","Link":"http://stats.stackexchange.com/questions/231170/how-to-report-degrees-of-freedom-for-each-term-in-a-quadratic-regression","Creator_reputation":187}
{"_id":{"$oid":"5837a577a05283111e4d3b37"},"View_count":66,"Display_name":"max","Question_score":3,"Question_content":"According to \"no free lunch theorem\" (also here and here), we cannot deduce just from the data alone (without any domain knowledge) which classifier is better. Of course, we use cross-validation to do precisely that.So what are the assumptions that we implicitly make that make cross-validation a meaningful procedure?","Creater_id":10117,"Start_date":"2016-04-11 17:35:01","Question_id":206753,"Tags":["machine-learning","cross-validation"],"Answer_count":1,"Last_activity":"2016-08-22 14:23:07","Link":"http://stats.stackexchange.com/questions/206753/assumptions-behind-cross-validation","Creator_reputation":401}
{"_id":{"$oid":"5837a577a05283111e4d3b44"},"View_count":1054,"Display_name":"TAK","Question_score":12,"Question_content":"Regularization using methods such as Ridge, Lasso, ElasticNet is quite common for linear regression. I wanted to know the following:Are these methods applicable for logistic regression? If so, are there any differences in the way they need to be used for logistic regression? If these methods are not applicable, how does one regularize a logistic regression?Thanks.","Creater_id":97328,"Start_date":"2016-08-08 03:29:20","Question_id":228763,"Tags":["regression","logistic","regularization"],"Answer_count":3,"Last_activity":"2016-08-22 14:07:17","Link":"http://stats.stackexchange.com/questions/228763/regularization-methods-for-logistic-regression","Creator_reputation":123}
{"_id":{"$oid":"5837a577a05283111e4d3b53"},"View_count":931,"Display_name":"Max","Question_score":8,"Question_content":"I’d like to learn about probability theory, measure  theory and finally machine learning. My ultimate goal is to use machine learning in a piece of software.I studied calculus and very basic probability in college but that’s pretty much it. Do you know some online courses or books that I could use to learn about these subjects. I’ve found many resources on the web but they all seem targeted to an expert audience. I know it’s going to take some time but where do I start if I’d like to learn from the beginning?","Creater_id":14228,"Start_date":"2016-08-22 06:55:31","Question_id":231092,"Tags":["machine-learning","probability","references","measure-theory"],"Answer_count":6,"Last_activity":"2016-08-22 13:48:23","Link":"http://stats.stackexchange.com/questions/231092/i-d-like-to-learn-about-probability-theory-measure-theory-and-finally-machine-l","Creator_reputation":142}
{"_id":{"$oid":"5837a577a05283111e4d3b65"},"View_count":40,"Display_name":"tikael","Question_score":2,"Question_content":"Each training example has 100 numeric attributes plus one output class and about 80% of the attributes are 'zero' (means no data collected). The value of attributes varies in a small range, like . I have 100 examples like this. What method/classifier should I use? I tried KNN, Naive Bayes, SVM, random forest/tree, none of these methods give me accuracy above 50% (I used 10-fold cross validation). What should I do?","Creater_id":128573,"Start_date":"2016-08-22 12:06:42","Question_id":231145,"Tags":["classification","cross-validation","small-sample"],"Answer_count":1,"Last_activity":"2016-08-22 13:38:20","Link":"http://stats.stackexchange.com/questions/231145/what-method-classifier-should-i-use-for-a-training-set-with-lots-of-attributes-b","Creator_reputation":11}
{"_id":{"$oid":"5837a577a05283111e4d3b72"},"View_count":21,"Display_name":"Tommaso","Question_score":0,"Question_content":"Suppose I have one dependent variable (3 levels) and something like 38 independent variable (the majority are dichotomous; 2 are numeric; 2 are polytomous).I want to explore which of my independent variables influences the probability of being in one of the group of the dependent variable. I'm not interested (at least now) in calculation of the relative risk ratio (RRR) of being in group-1 rather than in group-2.I choose to analyze this kind of data with the multinomial logistic regression (MLR) analysis. My idea is something similar to multiple linear regression analysis, where p-values means that the independent variable influences the dependent variable value.However in MLR analysis I could obtain (at least in R) p-values of independent variable when group-2 is compared to group-1, and when group-3 is compared to  group-1 (group-1 is the reference).In this way, it is not easy to understand if a certain covariate as an influence on the dependent variable, especially if the p-values are not all significant. Some article report Odds Ratios (OR) significance instead of p-values of the single independent covariates.Consider for example this article. Table-2 reports the ORs of the MLR analysis. Look at IL-10 row (a chemokine found in the blood): it is significant in the comparison of group-1 vs group-2, but not in the comparison of group-1 vs group-3. Can we assume that IL-10 has  a \"significant\" influences on the outcome (here, having Neuroborelliosis)?I don't remember where, but I've read that the Likelihood ratio statistic for each independent covariate is a way to express the influence of each independent covariate on the dependent variable. But we need a reference.Any example in R, or referenced article are welcome, as well as more pertinent analysis related to my problem.","Creater_id":6153,"Start_date":"2016-08-22 13:15:46","Question_id":231159,"Tags":["regression","logistic","multivariate-analysis","regression-coefficients","multinomial"],"Answer_count":0,"Last_activity":"2016-08-22 13:15:46","Link":"http://stats.stackexchange.com/questions/231159/multinomial-regression-analysis-and-significativity-of-covariates","Creator_reputation":145}
{"_id":{"$oid":"5837a577a05283111e4d3b74"},"View_count":15,"Display_name":"luchonacho","Question_score":1,"Question_content":"I have the following model (from theory): y_{ij} = \\beta x_{i} + e_{ij} where  and  are observed,  is individual and  is occupation, and there is no constant. Is  the only condition required for consistency of Pooled OLS?","Creater_id":100369,"Start_date":"2016-08-22 13:05:32","Question_id":231158,"Tags":["panel-data","least-squares","consistency"],"Answer_count":0,"Last_activity":"2016-08-22 13:05:32","Link":"http://stats.stackexchange.com/questions/231158/consistency-in-apparently-trivial-panel-regression","Creator_reputation":584}
{"_id":{"$oid":"5837a577a05283111e4d3b76"},"View_count":66,"Display_name":"parisa","Question_score":3,"Question_content":"I have used regression to estimate, now I want to calculate the bias in order to compare regression method versus other methods. Could anyone help me?for instance, I have V = 0.06*F + 0.01based on observed value for \"V\", I have an estimation for F, but I could not calculate the bias.","Creater_id":128553,"Start_date":"2016-08-22 09:13:52","Question_id":231117,"Tags":["regression","bias"],"Answer_count":1,"Last_activity":"2016-08-22 13:02:24","Link":"http://stats.stackexchange.com/questions/231117/how-to-calculate-bias-when-we-have-an-estimation-using-simple-linear-regression","Creator_reputation":8}
{"_id":{"$oid":"5837a577a05283111e4d3b83"},"View_count":1,"Display_name":"Rajesh","Question_score":0,"Question_content":"Let's say I have a system that prompts our users at random times to answer survey questions over their lifetime. The response rate is around 5%. I am trying to figure out the best interval to pop these prompts that would results in maximizing the response rate. Note that sending too many prompts will likely turn off users and cause them to ignore the prompt. Their propensity to respond also depends on their location, gender, time of day and few other well understood factors. I have plenty of historical data on when the prompts were sent and whether or not they got a response. Is there a good way to find the ideal interval between prompts other than descriptive analysis?","Creater_id":65008,"Start_date":"2016-08-22 12:47:29","Question_id":231152,"Tags":["modeling"],"Answer_count":0,"Last_activity":"2016-08-22 12:47:29","Link":"http://stats.stackexchange.com/questions/231152/whats-the-best-way-to-model-the-intervals-for-an-event-to-maximize-response","Creator_reputation":18}
{"_id":{"$oid":"5837a577a05283111e4d3b85"},"View_count":29,"Display_name":"Ryan Rothman","Question_score":0,"Question_content":"I have data that I am working with to try and quantify frugivore preference for certain types of fruits of certain colours. I have done the calculations, however the numbers don't seem to add-up, so-to-speak. My data is presented in the following manner:     17  1   1       19    14  0   2       16    5   0   0       5    3   1   0       4    6   0   1       7    0   0   0       0    1   1   0       2    4   0   0       4    2   0   0       2    1   0   0       1    2   0   0       2    1   0   0       119  1   1       2115  1   2       186   0   0       66   1   0       77   1   1       92   0   0       21   1   0       24   1   0       52   0   0       21   0   0       12   0   0       21   0   0       1Where columns represent different types of fruit, and rows represent different colours. The final column represents the SUM. So for example, a single data point represents a certain type of fruit, of a certain colour. The first set of numbers is the number of each combination eaten by the frugivore (i.e. found in the gut). The second set of numbers is the number of each combination in total (i.e. found in the environment). I know that 'observed' is meant to be proportion of food in gut and 'expected' is meant to be proportion of food in the environment. So I divide the numbers in the sets by their SUMs to get the observed and expected proportions. OBS     0.89    0.05    0.050.875   0       0.1251       0       00.75    0.25    00.85    0       0.14x       x       x0.5     0.5     01       0       01       0       01       0       01       0       01       0       0EXP 0.9     0.05    0.050.83    0.05    0.111       0       00.86    0.14    00.77    0.11    0.111       0       00.5     0.5     00.8     0.2     01       0       01       0       01       0       01       0       0Finally, to calculate preference, depending on the chosen method, it is either Observed / Expected (Foraging Ratio), Observed - Expected (Strauss Linear Index), etc. Typically a score of 0 (for Strauss) or 1 (for FR) represents no preference.  My concern is that something is not looking right. For example, The first column, 3rd row down is data about the fruit type \"drupe\" with purple flesh. The frugivore is known to eat 5 out of 6 in the environment. Yet the final calculated preference value represents \"no preference either way\" (1 - 1 = 0 ... 1/1 = 1) for this particular fruit type-colour combination, which doesn't seem to be quite right if it eats 80+% of it. Any advice would be greatly appreciated.  ","Creater_id":110948,"Start_date":"2016-08-22 12:27:01","Question_id":231149,"Tags":["mathematical-statistics","biostatistics","excel","ecology"],"Answer_count":0,"Last_activity":"2016-08-22 12:35:42","Link":"http://stats.stackexchange.com/questions/231149/preference-problems","Creator_reputation":12}
{"_id":{"$oid":"5837a577a05283111e4d3b87"},"View_count":1094,"Display_name":"Jim Johnson","Question_score":3,"Question_content":"Using the attached data that has been recently updated I am not able to obtain a statistically significant forecast. The data is extremely seasonal. The data is stored here for easy replication: http://ge.tt/1uihVfA2/v/0?c# 1. Make a R timeseries out of the rawdata: specify frequency \u0026amp; startdategIIP \u0026lt;- ts(Data, frequency=12, start=c(2003,11))print(gIIP)plot.ts(gIIP, type=\"l\", col=\"blue\", ylab=\"MTD Ships\", lwd=2,        main=\"Full data\")grid()Using the auto.arima function I don't need to factor a Box-Cox because the auto.arima factors that into selecting the best model. Upon \"selecting the best model\" I  The best model suggested was Arima(order = c(0, 0, 1),      seasonal = list(order = c(1, 0, 1), period = 12) with non-zero mean # 5. Perform estimationlibrary(forecast)library(zoo)library(stats)auto.arima(gIIP, d=NA, D=NA, max.p=12, max.q=12,           max.P=2, max.Q=2, max.order=12, max.d=2, max.D=2,           start.p=2, start.q=2, start.P=1, start.Q=1,           stationary=FALSE, seasonal=TRUE,           ic=c(\"aicc\",\"aic\", \"bic\"), stepwise=FALSE, trace=TRUE,           approximation=FALSE | frequency(gIIP)\u0026gt;12), xreg=NULL,           test=c(\"kpss\",\"adf\",\"pp\"), seasonal.test=c(\"ocsb\",\"ch\"),           allowdrift=TRUE, lambda=TRUE, parallel=FALSE, num.cores=4)then proceed to conduct accuracy diagnostics but unable to obtain any output.#Check standard error etc of \"fitted\" ARIMApos.arima \u0026lt;- function(gIIP, order = c(0, 0, 1),      seasonal = list(order = c(1, 0, 1), period = 12),      xreg = NULL, include.drift=TRUE,       transform.pars = TRUE,      fixed = NULL, init = NULL,      method = c(\"CSS-ML\", \"ML\", \"CSS\"),       optim.method = \"BFGS\",      optim.control = list(), kappa = 1e6)acf(pos.arima) pacf(pos.arima)The following step to conduct an ex ante (out of sample forecast) but also unable to obtain a statistically significant forecast---forecast with lowest standard error rate. I tested this by removing the last 5 observations to test the model. # 7. Forecast Out-Of-Sample ---this used to workfit \u0026lt;- Arima(gIIP, order = c(0, 0, 1), seasonal = list(order = c(1, 0, 1), period = 12),             xreg = TRUE, include.mean = TRUE, transform.pars = TRUE,              fixed = NULL, init = NULL, method = c(\"CSS-ML\", \"ML\", \"CSS\"),              optim.method = \"BFGS\", optim.control = list(), kappa = 1e6)plot(forecast(fit,h=9))print(forecast(fit,h=9))Used to obtain output here. Can you please help me diagnose why there ARIMA model is not working like it once did for me? Thank you for your time.  ","Creater_id":68579,"Start_date":"2015-02-17 15:48:00","Question_id":138108,"Tags":["r","time-series","arima"],"Answer_count":2,"Last_activity":"2016-08-22 12:24:52","Link":"http://stats.stackexchange.com/questions/138108/unable-to-get-suitable-forecast-for-arima-model-in-r-due-to-outliers-attached","Creator_reputation":33}
{"_id":{"$oid":"5837a577a05283111e4d3b95"},"View_count":39,"Display_name":"Andrew Haynes","Question_score":2,"Question_content":"If we use the method of moments estimator:What is the computational complexity?My initial assumption was that it would be , but after testing some values I find that it is .I am curious to know if there is a more theoretical way to show this result.","Creater_id":126912,"Start_date":"2016-08-22 07:34:17","Question_id":231102,"Tags":["r","spatial","geostatistics","variogram"],"Answer_count":0,"Last_activity":"2016-08-22 12:16:16","Link":"http://stats.stackexchange.com/questions/231102/what-is-the-computational-complexity-of-the-empirical-variogram","Creator_reputation":21}
{"_id":{"$oid":"5837a577a05283111e4d3b97"},"View_count":55,"Display_name":"Haohan Wang","Question_score":3,"Question_content":"I wonder if there some existing work of Linear Regression or Logistic Regression with partially known coefficients (). For a linear regression, , when we already have knowledge about some coefficient of , say  and still need to solve for . This should be trivial for linear regression case since we can directly deduct the known part out of , but how about Logistic Regression?And what if we don't know the actual values of , but only know that they should be non-zero. I believe these could be solved with some constrained optimization, so my question is that if there are some published manuscripts that have studied this problem? I wonder because I am interested in whether there are some interesting theoretical aspects of this more than a constrained optimization problem. ","Creater_id":42004,"Start_date":"2016-08-22 11:15:46","Question_id":231142,"Tags":["regression","optimization","constrained-regression"],"Answer_count":1,"Last_activity":"2016-08-22 12:12:25","Link":"http://stats.stackexchange.com/questions/231142/linear-regression-with-partially-known-coefficients","Creator_reputation":72}
{"_id":{"$oid":"5837a577a05283111e4d3ba4"},"View_count":27,"Display_name":"user128560","Question_score":0,"Question_content":"Imagine my instrument encodes colors using any/all possible wavelengths of light (not just the 3 features red, green, blue). Thus, it has an “infinite” number of features lying on a continuous spectrum. How could I do supervised learning to classify this type of data?","Creater_id":128560,"Start_date":"2016-08-22 10:16:45","Question_id":231131,"Tags":["classification"],"Answer_count":1,"Last_activity":"2016-08-22 11:02:51","Link":"http://stats.stackexchange.com/questions/231131/how-can-one-classify-data-if-the-range-of-possible-features-is-continuous-infini","Creator_reputation":1}
{"_id":{"$oid":"5837a577a05283111e4d3bb0"},"View_count":31,"Display_name":"Wowl","Question_score":0,"Question_content":"How would I best deal with input to a neural network that is only available upon request, but with a limited frequency?To be specific, I aim to train a driver for a Simulated Car Racing Championship (link to competition software manual) using deep reinforcement learning.Available as input are a variety of sensors providing distances from the racing car to the edge of the track, of which some can be pointed jointly in a new direction every time, but only be used again after at least 50 tics.The two parts of my question are:How do I best deal with data that is available only very infrequently,and is it feasible to have these inputs controlled by the neural network, and if so what challenges does that create and what would be a good approach?","Creater_id":127208,"Start_date":"2016-08-10 02:08:34","Question_id":229143,"Tags":["machine-learning","neural-networks"],"Answer_count":1,"Last_activity":"2016-08-22 10:56:22","Link":"http://stats.stackexchange.com/questions/229143/neural-networks-controlling-their-own-rare-input","Creator_reputation":1}
{"_id":{"$oid":"5837a577a05283111e4d3bbc"},"View_count":30,"Display_name":"colin","Question_score":1,"Question_content":"I am running a simple beta regression model in JAGS, following the example given here. Here is my JAGS model code as an R object:jags.model = \"model {#modelfor (i in 1:N){y[i] ~ dbeta(alpha[i], beta[i])alpha[i] \u0026lt;- mu[i] * phibeta[i]  \u0026lt;- (1-mu[i]) * philogit(mu[i]) \u0026lt;- a + b*x1[i] + c*x2[i] + d*x3[i] + e*x4[i] + f*x5[i]}#priorsa  ~ dnorm(0, .0001)b  ~ dnorm(0, .0001)c  ~ dnorm(0, .0001)d  ~ dnorm(0, .0001)e  ~ dnorm(0, .0001)f  ~ dnorm(0, .0001)t0 ~ dnorm(0, 0.010)phi \u0026lt;- exp(t0)}\" In the linked example there is a gamma prior on the phi parameter. However, I kept getting errors, so I shifted to an alternative expoential prior on phi to keep it positive. Every time I run this code using runjags I get the error: value out of range in 'gammafn', which prints out many many times until I tell the code to stop running. I am unsure what the problem is here, as I am no longer using a gamma function in any of this code. Any insight, or alternative methods to run a beta-regression in JAGS, would be appreciated. ","Creater_id":30451,"Start_date":"2016-08-16 07:14:48","Question_id":230106,"Tags":["r","bayesian","jags","beta-regression"],"Answer_count":1,"Last_activity":"2016-08-22 10:41:49","Link":"http://stats.stackexchange.com/questions/230106/error-in-jags-beta-regression-model-r-value-out-of-range-in-gammafn","Creator_reputation":266}
{"_id":{"$oid":"5837a577a05283111e4d3bc9"},"View_count":36,"Display_name":"GGA","Question_score":0,"Question_content":"I have dove LOOCV with a logistic regression model, my sample size is small (~50 subjects).My AUC value before cross-validation is 0.79, after cross validation is 0.69 so with a change of 0.1.How can I interpret this? Is it an indication that my model is poor? I know it is subjective the real value attributed to AUC, my study is a very esplorative one so the objective of my model is not immediate application to real world data.","Creater_id":97342,"Start_date":"2016-08-22 09:52:05","Question_id":231127,"Tags":["regression","cross-validation","auc"],"Answer_count":0,"Last_activity":"2016-08-22 10:38:09","Link":"http://stats.stackexchange.com/questions/231127/interpretation-of-auc-change-after-cross-validation","Creator_reputation":171}
{"_id":{"$oid":"5837a577a05283111e4d3bcb"},"View_count":49,"Display_name":"Cyrine Ezzahra","Question_score":2,"Question_content":"I learning some machine learning course, and I would like to know in which case we use linea algebra and Matrix Algebra?Thank youKind regards","Creater_id":74552,"Start_date":"2016-08-22 08:52:48","Question_id":231113,"Tags":["linear-algebra"],"Answer_count":1,"Last_activity":"2016-08-22 10:35:27","Link":"http://stats.stackexchange.com/questions/231113/linear-algebra-use-case","Creator_reputation":111}
{"_id":{"$oid":"5837a577a05283111e4d3bd8"},"View_count":35,"Display_name":"Imlerith","Question_score":0,"Question_content":"I have a number of questions about implementing adaboost:The weights start with a value of 1/n, n being the number of instances in my training set. for the next model, the weight for each instance is going to be updated based on whether the instance was correctly classified by the previous model. -My first question is more of a concern about whether I am correctly reflecting the weight change in the new training data set: I am replicating instances based on their new weight. For that, I simply multiply the weight by n(the total number of instances) for instance: if I have an instance with weight 0.5 and my training set contains 600 instances. then the aforementioned instance will be replicated 300 times.Is this the correct way to updating the training set?-What if the weight of a correctly classified instance is so small that multiplying it by the number of instances in the training set gives, say, 0.4 should I take the ceiling of that to insure that no instance will be depleted from the training set or should I take the floor of that number to ignore any instance whose weight is really small(meaning that it was previously classified correctly)-About the confidence of each model I noticed that it doesn't really account for true negatives and true positives separately it is however a kind of error rate that mixes the two. I also noticed by looking at the confusion matrix of models built during this iterative process that some of the models have a better TP rate while others have a better TN rate. How can I raise the their TP confidence when they classify something as the positive class, and similarly how can can I raise the TN confidence of a classifier when they classify something as TN-Last question, what does it mean if the confusion matrix of models do not change after a certain number of iterations ?","Creater_id":124405,"Start_date":"2016-08-22 10:10:07","Question_id":231129,"Tags":["machine-learning","classification","ensemble","adaboost"],"Answer_count":0,"Last_activity":"2016-08-22 10:28:07","Link":"http://stats.stackexchange.com/questions/231129/questions-on-implementing-adaboost","Creator_reputation":10}
{"_id":{"$oid":"5837a577a05283111e4d3bda"},"View_count":11618,"Display_name":"fmark","Question_score":64,"Question_content":"I have a dataset with around 30 independent variables and would like to construct a generalized linear model (GLM) to explore the relationship between them and the dependent variable.I am aware that the method I was taught for this situation, stepwise regression, is now considered a statistical sin.What modern methods of model selection should be used in this situation?","Creater_id":179,"Start_date":"2011-07-31 16:45:58","Question_id":13686,"Tags":["regression","generalized-linear-model","model-selection","stepwise-regression"],"Answer_count":5,"Last_activity":"2016-08-22 09:54:02","Link":"http://stats.stackexchange.com/questions/13686/what-are-modern-easily-used-alternatives-to-stepwise-regression","Creator_reputation":2040}
{"_id":{"$oid":"5837a577a05283111e4d3beb"},"View_count":3643,"Display_name":"Antonio Pedro Ramos","Question_score":6,"Question_content":"I am trying to get predictions for observations from a lme object. This is supposed to be quite straightforward. Yet, since I am get different types of errors for different trials, it seems to me I am missing something.  My model is the following:model \u0026lt;- lme(log(child_mortality) ~ as.factor(cluster)*time +         my.new.time.one.transition.low.and.middle + ttd +         maternal_educ+ log(IHME_id_gdppc) + hiv_prev-1,         merged0,na.action=na.omit,method=\"ML\",weights=varPower(form=~time),         random= ~ time| country.x,         correlation=corAR1(form = ~ time),         control=lmeControl(msMaxIter = 200, msVerbose = TRUE))It runs fine, fits the data well and the results make sense. Now to get predictions I have tried the following:test.pred \u0026lt;- data.frame(time=c(10,10,10,10),country.x=c(\"Poland\",\"Brazil\",            \"Argentina\",\"France\"),                 my.new.time.one.transition.low.and.middle=c(1,1,1,0),             ttd=c(0,0,0,0),maternal_educ=c(10,10,10,10),             IHME_id_gdppc=c(log(5000),log(8000),log(8000),log(15000)),                hiv_prev=c(.005,.005,.005,.005),              cluster=c(\"One Transition, Middle Income\",\"One Transition,                Middle Income\",\"One Transition, Middle Income\",\"Democracy,              High Income\"))\u0026gt;\u0026gt; predict(model,test.pred,level=0)Error in X %*% fixef(object) : non-conformable argumentsIf I exclude, say, France, and only include countries in which cluster=\"OneTransition, Middle Income\" then I get a different error# create a toy data settest.pred0 \u0026lt;-    expand.grid(time=20:29,country.x=c(\"Poland\",\"Brazil\",\"Argentina\"))z0 \u0026lt;-as.data.frame(cbind(my.new.time.one.transition.low.and.middle =                          c(0,0,0,0,0,0,1,2,3,4), ttd=c(0,0,0,0,0,0,1,0,0,0),                         maternal_educ=seq(from=10.0, to=12.0, length.out=10),                         IHME_id_gdppc=log(seq(from=5000, to=8000, length.out=10)),                         hiv_prev=rep(.005,10),                         cluster=rep(\"One Transition, Middle Income\",10)))z \u0026lt;- rbind(z0,z0,z0)test.pred \u0026lt;- cbind(test.pred0,z)# checkhead(test.pred)\u0026gt;  time country.x my.new.time.one.transition.low.and.middle ttd\u0026gt; maternal_educ    IHME_id_gdppc hiv_prev\u0026gt; 1   20    Poland                                         0   0\u0026gt;   10 8.51719319141624    0.005\u0026gt; 2   21    Poland                                         0   0\u0026gt; 10.2222222222222 8.58173171255381    0.005\u0026gt; 3   22    Poland                                         0   0\u0026gt; 10.4444444444444 8.64235633437024    0.005\u0026gt; 4   23    Poland                                         0   0\u0026gt; 10.6666666666667 8.69951474821019    0.005\u0026gt; 5   24    Poland                                         0   0\u0026gt; 10.8888888888889 8.75358196948047    0.005\u0026gt; 6   25    Poland                                         0   0\u0026gt; 11.1111111111111 8.80487526386802    0.005\u0026gt;                         cluster\u0026gt; 1 One Transition, Middle Income\u0026gt; 2 One Transition, Middle Income\u0026gt; 3 One Transition, Middle Income\u0026gt; 4 One Transition, Middle Income\u0026gt; 5 One Transition, Middle Income\u0026gt; 6 One Transition, Middle Income# run the predictionspredict(model,test.pred,level=0)\u0026gt; Error in `contrasts\u0026lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) :\u0026gt;   contrasts can be applied only to factors with 2 or more levelsIn this example,  the problem is due to cluster=\"One Transition, MiddleIncome\" all the time.I don't understand why this is a problem. If I want to get predict() to workI have to include all variables from the model, right? Obviously, the inputdata in the model's call will not include factor set to the same values forall cases. Yet, if I want to get predictions just for subset of the data,or for new observations, I may be interested in only in cases where somefactor is always set to be the same.  Does it make sense? How can I getpredictions in that case?","Creater_id":8315,"Start_date":"2012-05-30 13:45:53","Question_id":29513,"Tags":["r","mixed-model","multilevel-analysis","prediction","lme"],"Answer_count":1,"Last_activity":"2016-08-22 09:53:01","Link":"http://stats.stackexchange.com/questions/29513/error-in-getting-predictions-from-a-lme-object","Creator_reputation":68}
{"_id":{"$oid":"5837a577a05283111e4d3bf8"},"View_count":155,"Display_name":"Tim","Question_score":3,"Question_content":"As far as I know, it is possible to fit a linear regression model and then fit a second model to predict the residuals from the first model by using some other variables. By this you can understand their influence on the relation modelled with the higher level model. So the purpose is not to check the model fit, but to get a deeper understanding of the data. Unfortunately, I was not able to find any literature on this.Is there a name for this kind of analysis? How and why is it done? What are the pros and cons? Could you provide any literature on this?","Creater_id":35989,"Start_date":"2014-12-07 02:57:34","Question_id":127001,"Tags":["modeling","residuals","residual-analysis"],"Answer_count":2,"Last_activity":"2016-08-22 09:50:07","Link":"http://stats.stackexchange.com/questions/127001/analysing-the-residuals-themselves","Creator_reputation":25275}
{"_id":{"$oid":"5837a577a05283111e4d3c06"},"View_count":31,"Display_name":"Josiah","Question_score":0,"Question_content":"How can a pretrained network specialize without \"forgetting\" the general principles it's learned when only limited datasets are available?I am trying to train a set of neural networks to model pavement degredation. There are many different combinations, each of which behaves differently, and I am planning to train a different network for each. I have about 400 \"good\" samples showing the behavior I wish to model for each of the ~60 models.I did a proof of concept by training a model with one of the larger datasets. It was very accurate for the data I had, but failed to generalize well - in some places the pavement was predicted to improve in quality (which doesn't happen in real life) simply because I didn't have sufficient data for that combination of values.To solve this issue I decided to use a generic network as a starting place and then teach the ANNs to specialize for each specific model. The generic network works great - the curves all go in the right direction and it has plenty of data (25000+) to look at with good distribution.When training with the smaller datasets, the network loses its ability to generalize - sometimes the degredations are predicted to go in reverse after a small amount of training, as small as 10 iterations. The training data does not include any bad data of that sort, but it does have a tendency to cluster around certain common combinations.I'm looking for techniques that allow a pretrained network to specialize and not \"unlearn\" the general priciples it has picked up. If there are any improvements to architecture or to learning rates, etc. I'd be happy to hear them. This is also my first non-toy neural network, so any advice would be appreciated.My network architecture is all sigmoid neurons, arranged in a 16x8x8x1 network. I'm using a Mean Squared Error cost function (makes sense for my data, and has given good results for the proof of concept). I'm using the synaptic.js library.","Creater_id":127374,"Start_date":"2016-08-11 10:29:39","Question_id":229387,"Tags":["neural-networks"],"Answer_count":1,"Last_activity":"2016-08-22 09:38:05","Link":"http://stats.stackexchange.com/questions/229387/training-a-neural-network-to-specialize-with-insufficient-data","Creator_reputation":101}
{"_id":{"$oid":"5837a577a05283111e4d3c12"},"View_count":27,"Display_name":"Dick Elwood","Question_score":1,"Question_content":"I use a scale (like the Framingham risk scale) to predict the probability of a binary clinical outcome. I used logistic regression to predict p for each scale score. The scale does not account for all risk factors. I calculate the outcome odds ratios (ORs) for those factors. I assume these other risk factors are independent. I want to predict whether a patient's risk exceeds a defined threshold. I take the p corresponding to their score on the scale. I propose to convert the p to an odds and multiply it by each OR in succession. Is that a reasonable proposal?Now I also want to predict whether the patient's risk p exceeds the threshold by a credible margin of error. I calculate 1-tailed confidence intervals for each OR. I take the odds of a 1-tailed lower limit of the original p. I propose to multiply those odds by the lower limit of each OR in succession. Should I set a lower confidence level for serial ORs, like p x n, where n = # of ORs, sort of a backward Bonferroni correction? Is this a reasonable proposal?  I want to give non-statistician clinicians a fairly easy way to adjust patients' risk predictions for factors not accounted for by the scale.   ","Creater_id":97099,"Start_date":"2016-08-22 09:33:52","Question_id":231119,"Tags":["confidence-interval","odds-ratio"],"Answer_count":0,"Last_activity":"2016-08-22 09:33:52","Link":"http://stats.stackexchange.com/questions/231119/confidence-interval-of-applying-multiple-odds-ratios","Creator_reputation":6}
{"_id":{"$oid":"5837a577a05283111e4d3c14"},"View_count":251,"Display_name":"William","Question_score":7,"Question_content":"From what I have read and from answers to other questions I have asked here, many so-called frequentist methods correspond mathematically (I don't care if they correspond philosophically, I only care whether it corresponds mathematically) to special cases of so-called Bayesian methods (for those who object to this, see the note at the bottom of this question). This answer to a related question (not mine) supports this conclusion:   Most Frequentist methods have a Bayesian equivalent that in most circumstances will give essentially the same result.Note that in what follows, being mathematically the same means giving the same result. If you characterize two methods which can be proved to always give the same results as being \"different\", that is your right, but that is a philosophical judgment, not a mathematical one nor a practical one.Many people who self-describe as \"Bayesians\", however, seem to reject using maximum likelihood estimation under any circumstances, even though it is a special case of (mathematically) Bayesian methods, because it is a \"frequentist method\". Apparently Bayesians also use a restricted/limited number of distributions compared to frequentists, even though those distributions would also be mathematically correct from a Bayesian viewpoint.Question: When and why do Bayesians reject methods which are mathematically correct from a Bayesian viewpoint? Is there any justification for this which isn't \"philosophical\"?Background/Context: The following are quotes from answers and comments to a previous question of mine on CrossValidated:  The mathematical basis for the Bayesian vs frequentist debate is very simple. In Bayesian statistics the unknown parameter is treated as a random variable; in frequentist statistics it is treated as a fixed element...From the above I would have concluded that (mathematically speaking) Bayesian methods are more general than frequentist ones, in the sense that frequentist models satisfy all of the same mathematical assumptions as Bayesian ones, but not vice versa. However, the same answer argued that my conclusion from the above was incorrect (emphasis in what follows is mine):  Although the constant is a special case of a random variable, I would hesitate to conclude that Bayesianism is more general. You would not get frequentist results from Bayesian ones by simply collapsing the random variable to a constant. The difference is more profound...    Going to personal preferences... I do not like that Bayesian statistics uses quite a restricted subset of available distributions.Another user, in their answer, stated the opposite, that Bayesian methods are more general, although oddly enough the best reason I could find for why this might be the case was in the previous answer, given by someone trained as a frequentist.  The mathematical consequence is that Frequentists think the basic equations of probability only sometimes apply, and Bayesians think they always apply. So they view the same equations as correct, but differ on how general they are... Bayesian is strictly more general than Frequentist. Since there can be uncertainty about any fact, any fact can be assigned a probability. In particular, if the facts you're working on is related to real world frequencies (either as something you're predicting or part of the data) then Bayesian methods can consider and use them just as they would any other real world fact. Consequently any problem Frequentists feel their methods apply to Bayesians can also work on naturally.From the above answers, I have the impression that there are at least two different definitions of the term Bayesian commonly in use. The first I would call \"mathematically Bayesian\" which encompasses all methods of statistics, since it includes parameters which are constant RVs and those which are not constant RVs. Then there is \"culturally Bayesian\" which rejects some \"mathematically Bayesian\" methods because those methods are \"frequentist\" (i.e. out of personal animosity to the parameter sometimes being modeled as a constant or frequency). Another answer to the aforementioned question also seems to support this conjecture:  It is also of note that there are plenty divides between the models used by the two camps that is more related to what has been done than what can be done (i.e. many models that are traditionally used by one camp can be justified by the other camp).So I guess another way to phrase my question would be the following: Why do cultural Bayesians call themselves Bayesians if they reject many mathematically Bayesian methods? And why do they reject these mathematically Bayesian methods? Is it personal animosity for the people who most often use those particular methods?Edit: Two objects are equivalent in a mathematical sense if they have the same properties, irrespective of how they are constructed. For example, I can think of at least five different ways to construct the imaginary unit . Nevertheless, there are not at least five different \"schools of thought\" about the study of imaginary numbers; in fact, I believe there is only one, which is that group which studies their properties. To those who object that getting a point estimate using maximum likelihood is not the same thing as getting a point estimate using maximum a priori and a uniform prior because the calculations involved are different, I concede that they are different in a philosophical sense, but to the extent that they always give the same values for the estimate, they are mathematically equivalent, because they have the same properties. Maybe the philosophical difference is relevant to you personally, but it isn't relevant to this question.Note: This question originally had an incorrect characterization of MLE estimation and MAP estimation with a uniform prior.","Creater_id":113090,"Start_date":"2016-08-20 19:14:49","Question_id":230921,"Tags":["bayesian","frequentist","philosophical"],"Answer_count":3,"Last_activity":"2016-08-22 09:21:31","Link":"http://stats.stackexchange.com/questions/230921/when-and-why-do-bayesians-reject-valid-bayesian-methods","Creator_reputation":1902}
{"_id":{"$oid":"5837a577a05283111e4d3c23"},"View_count":260,"Display_name":"The Lazy Log","Question_score":3,"Question_content":"I have a deep neural network model and I need to train it on my dataset which consists of about 100,000 examples, my validation data contains about 1000 examples. Because it takes time to train each example (around 0.5s for each example) and in order to avoid overfitting, I would like to apply early stopping to prevent unnecessary computation. But I am not sure how to properly train my neural network with early stopping, several things I do not quite understand now:What would be a good validation frequency? Should I check my model on the validation data at the end of each epoch? (My batch size is 1)Is it the case that the first few epochs might yield worse result before it starts converging to better value? In that case, should we train our network for several epochs before checking for early stopping?How to handle the case when the validation loss might go up and down? In that case, early stopping might prevent my model from learning further, right?Thank you in advance.","Creater_id":128516,"Start_date":"2016-08-22 03:01:56","Question_id":231061,"Tags":["neural-networks","deep-learning"],"Answer_count":1,"Last_activity":"2016-08-22 09:17:49","Link":"http://stats.stackexchange.com/questions/231061/how-to-use-early-stopping-properly-for-training-deep-neural-network","Creator_reputation":123}
{"_id":{"$oid":"5837a577a05283111e4d3c30"},"View_count":58,"Display_name":"Enthusiast","Question_score":2,"Question_content":"I have past movie ticket bookings data. Tickets for each movie show starts getting sold at least 15 days before the day of show. There are days when tickets don't get sold at all! My goal is to predict final tickets that will be sold based on tickets that has been sold so far. Below is how my data looks likeAs you can see there are missing values in the second column which appear as 0, my cumulative series starts from 0. My problem is there are way too many 0 values or missing values and the data i presented is too idealistic. For certain dates, tickets start getting booked from 8th day onwards. So there will be 0 starting from 15 till 9th day! If i fit a regression by including seasonality variables, it does not give me a good r square. Can anyone suggest how to handle this kind of missing data or if i am missing anything?","Creater_id":68444,"Start_date":"2016-08-21 20:30:34","Question_id":231029,"Tags":["regression","forecasting","data-transformation","missing-data","data-preprocessing"],"Answer_count":1,"Last_activity":"2016-08-22 09:16:30","Link":"http://stats.stackexchange.com/questions/231029/predicting-when-there-are-many-missing-values-in-independent-variable","Creator_reputation":347}
{"_id":{"$oid":"5837a577a05283111e4d3c3d"},"View_count":40,"Display_name":"RobertF","Question_score":3,"Question_content":"I'm working on a propensity score analysis where the treatment variable is continuous (a score from 0 to 100, let's say) rather than binary (treatment vs. control). I've been reading Guo \u0026amp; Fraser's textbook Propensity Score Analysis: Statistical Methods and Applications (2nd ed.) but the process for covariate balancing still isn't entirely clear.Is this the correct procedure for checking covariate balance in a generalized propensity score model or am I oversimplifying?1) First, use a generalized linear model to predict the treatment scores from the covariates  for each observation , . 2) Divide predicted treatment scores, , into five quintile intervals .3) Divide actual treatment scores, , into five quintile intervals, .4) Within each of the five predicted treatment score intervals , use test statistics (e.g., Student's t) to compare differences in mean values of each of the covariates  in the five actual treatment score intervals,  vs.  for .","Creater_id":13634,"Start_date":"2016-08-18 08:28:20","Question_id":230525,"Tags":["propensity-scores","covariate"],"Answer_count":1,"Last_activity":"2016-08-22 09:16:16","Link":"http://stats.stackexchange.com/questions/230525/procedure-for-testing-covariate-balance-for-generalized-propensity-score-estimat","Creator_reputation":1461}
{"_id":{"$oid":"5837a578a05283111e4d3c4a"},"View_count":38,"Display_name":"user3022875","Question_score":1,"Question_content":"Lets say you do a T test for difference in means with alpha at .05. If the p value is .08 and you want to report this in words to laymen, non-statisticianswould you say:The difference in means are not statistically significant at a 5% significance level or would you say something about the 95% confidence intervals used to determine significance? if so - in what way?has to be 1 sentence IN WORDS not numbers","Creater_id":40579,"Start_date":"2016-08-17 13:08:46","Question_id":230363,"Tags":["regression","probability"],"Answer_count":1,"Last_activity":"2016-08-22 09:13:40","Link":"http://stats.stackexchange.com/questions/230363/reporting-statistical-significance","Creator_reputation":98}
{"_id":{"$oid":"5837a578a05283111e4d3c57"},"View_count":19,"Display_name":"user39531","Question_score":1,"Question_content":"I understand that PLS can indicate the cronbach's alpha between two variables in a Framework. (i) For instance, can PLS tell us, based on the direction of the arrows, if there is causation (i.e. one var influence the other)?(ii) Will PLS tell us if one var is influencing another var in the Framework?(iii) What additional info can we glean using PLS? (iv) How do we determine if there is a latent var in a Framework?","Creater_id":39531,"Start_date":"2015-10-29 17:50:10","Question_id":179328,"Tags":["regression","multivariate-analysis","pls"],"Answer_count":0,"Last_activity":"2016-08-22 09:07:09","Link":"http://stats.stackexchange.com/questions/179328/partial-least-squares-for-validating-a-framework","Creator_reputation":60}
{"_id":{"$oid":"5837a578a05283111e4d3c59"},"View_count":25,"Display_name":"HelloAndWelcome","Question_score":2,"Question_content":"I'm currently attempting to pool multiple logistic regression results together. I used SPSS to perform the logistic regressions using complex weights. The option to pool the results together is not available when using complex weights in SPSS. Unfortunately, the resources available to me are SPSS and Excel. I'm attempting to pool in the results for the variables (significance, Odds ratio, Confidence interval) but was not able to find how to do so. How would I be able to combine the results in those multiple logistic regressions?Thank you.","Creater_id":128551,"Start_date":"2016-08-22 09:04:07","Question_id":231114,"Tags":["logistic","confidence-interval","multiple-imputation","pooling"],"Answer_count":0,"Last_activity":"2016-08-22 09:04:07","Link":"http://stats.stackexchange.com/questions/231114/pooling-estimates-from-multiple-imputation-logistic-regression","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3c5b"},"View_count":44,"Display_name":"tintinthong","Question_score":1,"Question_content":"Let's construct a simple example. Below is the code.A\u0026lt;-gl(2,4) #factor of 2 levels B\u0026lt;-gl(4,2) #factor of 4 levelsdf\u0026lt;-data.frame(y,A,B)As you can see, B is nested within A. The peculiar result I am interested in the output of the model matrix when I fit for a nested model . How does R decide what is included inside the intercept? Since we are using dummy coding, the coefficients of the model is interpreted as the difference between a particular level and the reference level/the intercept for an single factor model. I understand for model ~A, A1 becomes the intercept and that for model ~A+B, A1 and B1 (both) become the intercept. I do not get why when we use a nested model, A1:B2 appears as a column inside the model matrix. Why isn't the first parameter of the interaction subspace A1:B1 or A2:B1? I think I am missing the concept. I think the intercept is A1. Hence, Why do we not compare the levels of A1:B1 and A1(intercept)  or A2:B1 and A1(intercept)?#nested model \u0026gt; mod\u0026lt;-aov(y~A+A:B)\u0026gt; model.matrix(mod)  (Intercept) A2 A1:B2 A2:B2 A1:B3 A2:B3 A1:B4 A2:B41           1  0     0     0     0     0     0     02           1  0     0     0     0     0     0     03           1  0     1     0     0     0     0     04           1  0     1     0     0     0     0     05           1  1     0     0     0     1     0     06           1  1     0     0     0     1     0     07           1  1     0     0     0     0     0     18           1  1     0     0     0     0     0     1","Creater_id":121671,"Start_date":"2016-08-22 08:49:16","Question_id":231111,"Tags":["r","regression","anova"],"Answer_count":0,"Last_activity":"2016-08-22 08:49:16","Link":"http://stats.stackexchange.com/questions/231111/how-does-r-decide-which-parameters-are-inside-the-intercept-for-dummy-coding","Creator_reputation":117}
{"_id":{"$oid":"5837a578a05283111e4d3c5d"},"View_count":29,"Display_name":"Federico Giorgi","Question_score":5,"Question_content":"I would like to integrate T-statistics values from multiple, independent tests. How can I do it? Since my degrees of freedom are many (n\u003e1000) I was thinking to use the Stouffer's method, which is really for Z-statistics score. However, with a high df value, the T distribution and the Gaussian distribution should be identical.Can I use Stouffer's method to integrate T values at df\u003e1000?","Creater_id":9640,"Start_date":"2016-08-22 07:53:27","Question_id":231106,"Tags":["combining-p-values"],"Answer_count":0,"Last_activity":"2016-08-22 08:37:07","Link":"http://stats.stackexchange.com/questions/231106/stouffers-method-to-integrate-t-values","Creator_reputation":52}
{"_id":{"$oid":"5837a578a05283111e4d3c5f"},"View_count":112,"Display_name":"Tiernan","Question_score":1,"Question_content":"I have been tasked with creating a composite indicator for the purpose of comparing the demographic/socio-economic situation of communities within a city.I'm using US Census data for three demographic/socio-economic indicators and have transformed them all to percentage measurements:% People of Color% Households Burdened by Housing Costs% Linguistically Isolated HouseholdsPercentage values provide a much more intuitive unit of comparison than the raw counts, and these indicators all share the same directionality (i.e. higher percentages indicate a more advantaged community, lower percentages indicate less advantage).Now I need to find a method of combining the values of each indicator to form an index score, which itself will be used to divide the communities into quintile groups.Here's a look at the data distributions:While all three variables are percentage measurements (and therefore share the same scale), an important assumption of this exercise is that the indicators should have equal influence on the index score.The index score should reflect a given community's relationship with the hypothetical \"average community\" (one that has the mean value for all three indicators). But what unit provides interpretable comparative value? It seems that many composite indicator methodologies use z-scores to standardize data before combining them (see here). This means that the unit of comparison between communities would be standard deviations from the \"average community\" score.QuestionsWhat can I do if the data are not normally distributed? A quick glance at the histograms above suggests that these are not normal distributions. Should I try further transforming the data  (e.g., log, logit, arcsine) with the hope that the distribution will more closely resemble normality? Or should I leave the data as-is and abandon the idea that I will be able to interpret the final index's comparative unit (as seen here: \"Standardizing a non normal dataset\"; \"Z-Score and Normal Distribution\")?Any general advice on the process I should go through to create a useful composite indicator would be much appreciated!","Creater_id":99697,"Start_date":"2016-08-20 14:43:54","Question_id":230905,"Tags":["standardization","z-score"],"Answer_count":3,"Last_activity":"2016-08-22 08:33:03","Link":"http://stats.stackexchange.com/questions/230905/how-to-find-an-appropriate-standardization-method-for-combining-non-normally-dis","Creator_reputation":8}
{"_id":{"$oid":"5837a578a05283111e4d3c6e"},"View_count":235,"Display_name":"Stoneman","Question_score":6,"Question_content":"I've taken an ML course previously, but now that I am working with ML related projects at my job, I am struggling quite a bit to actually apply it. I'm sure the stuff I'm doing has been researched/dealt with before, but I can't find specific topics.All the machine learning examples I find online are very simple (e.g. how to use a KMeans model in Python and look at the predictions). I am looking for good resources on how to actually apply these, and maybe code examples of large scale machine learning implementations and model trainings. I want to learn about how to effectively process and create new data that can make the ML algorithms much more effective.","Creater_id":119326,"Start_date":"2016-06-08 08:49:22","Question_id":217938,"Tags":["machine-learning","references","train","application"],"Answer_count":3,"Last_activity":"2016-08-22 08:11:02","Link":"http://stats.stackexchange.com/questions/217938/good-examples-books-resources-to-learn-about-applied-machine-learning-not-just","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3c7d"},"View_count":28,"Display_name":"Caro Sluijter","Question_score":1,"Question_content":"I am looking at confounding factors for relation 'reporting type' (NR or SR) --\u003e Completeness of a colorectal report (Yes (1) or No (0))I have a possible confounding factor (CRM) that is only applicable for rectal cancer and not for colon cancer.How do I test this confounding factor? It is a categorical factor, so CRM is free (1) or not free (0).I usually use bivariable analysis to calculate the Mantel-Haenszel Odds Ratio to look for differences in the Odds Ratio compared to the crude Odds ratio.So I would like to do look at the relation: complete report = constant + type of reportX + CRMX2Normally this would work, however CRM is only applicable in 30% of my cases (i.e. the rectal tumors). So if I adjust for CRM I will loose the colon cancer cases.Do I re-code the colon cancer cases as not applicable or do I just leave them as being missing?Or do I split my dataset in colon and rectum cases?Or is there another option?","Creater_id":127231,"Start_date":"2016-08-22 05:23:58","Question_id":231076,"Tags":["logistic","missing-data","confounding"],"Answer_count":0,"Last_activity":"2016-08-22 07:53:13","Link":"http://stats.stackexchange.com/questions/231076/logistic-regression-confounding-factor-and-missing-or-not-applicable-data","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3c7f"},"View_count":40,"Display_name":"Hiddenllyy","Question_score":1,"Question_content":"I'm working on a quelity control misson and i'd like to valid a mesure method. So I want to use the F test to compare the variance then use the two sample t-Test to verify if the two means of these two samples are nearly the same.But generally, the company will use the percentage of the tolerant interval (TI) to judge if the mesure tool is good or not. But according to the documents that I've read, I think it's alpha (significance level) who decides we are 'tolerant' or 'strict', and the TI is not usful in my case? And besides, all the documents I've read, for the H0, we can only use 'not refuse'. So does that means that this method is less 'serious'?If we can use TI in the t-Test please explain a little for me or I've some error in comprehension, please leave a comment. Thank you.","Creater_id":128507,"Start_date":"2016-08-22 01:38:38","Question_id":231055,"Tags":["hypothesis-testing","statistical-significance","t-test"],"Answer_count":0,"Last_activity":"2016-08-22 07:52:21","Link":"http://stats.stackexchange.com/questions/231055/does-the-interval-of-confidance-can-be-counted-into-the-two-sample-t-test","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3c81"},"View_count":70,"Display_name":"user3497385","Question_score":4,"Question_content":"I was reading an article about using the CDF to calculate the area between 2 points on the normal curve. They gave a sample of 7 for illustration purposes:-4.0,-3.0,0.8,1.8,3.9,6.2,6.5From this sample, the sample mean is 1.743, the SD = 4.154 , n=7Then it says that given this information and assuming the sample was normally distributed, the sample would have the following characteristics expected sample values    CDF at each sample point    z score at sample point-4.343                             1/14                      -1.465-1.545                             3/14                      -0.792 0.222                             5/14                      -0.366 1.743                             1/2                        0.000 3.264                             9/14                       0.366 5.031                             11/14                      0.792 7.829                             13/14                      1.465Please would somebody kindly explain how the fractions would have been calculated? the sample is 7, so where did 14 come from etc. The text clearly explains how the Z scores were calculated (NORMSINV at the sample point) and the expected values (NORMINV) but I couldn't understand the fractions for CDF at each sample point.Kind regardsDec","Creater_id":43207,"Start_date":"2016-08-22 00:22:14","Question_id":231050,"Tags":["normal-distribution","pdf","normality","cdf"],"Answer_count":0,"Last_activity":"2016-08-22 07:50:59","Link":"http://stats.stackexchange.com/questions/231050/testing-for-normality-cdf","Creator_reputation":40}
{"_id":{"$oid":"5837a578a05283111e4d3c83"},"View_count":30,"Display_name":"Yaga","Question_score":0,"Question_content":"In a probability distribution, is it true that  is NOT . That is,  is a joint distribution of  and  and  is a function of ?","Creater_id":128539,"Start_date":"2016-08-22 07:01:10","Question_id":231094,"Tags":["probability","distributions"],"Answer_count":1,"Last_activity":"2016-08-22 07:45:15","Link":"http://stats.stackexchange.com/questions/231094/joint-distributions-and-function-of-a-random-variable","Creator_reputation":4}
{"_id":{"$oid":"5837a578a05283111e4d3c90"},"View_count":78,"Display_name":"Carol","Question_score":2,"Question_content":"I have panel data.  When I use the Hausman test to decide on a fixed vs random model, I get an note saying that: \"the rank of the differenced variance matrix (8) does not equal the number of coefficients being tested (11)\".  I am unsure what this means.  I repeated this and converted my some of my factor and catagorical independent variables to simpler variables and scaled one variable to produce coefficients more similar to others which increased the rank to 9. Still, the note appeared as the rank, though improved, was unequal to the number of coefficients being tested.  In all cases, the significant p value of the Hausman test indicated a fixed model.I moved on to the xtoverid test.  After running a random model (with and without the vce cluster option), there was an error message saying \"saved RE estimates are degenerate (sigma_u=0) and equivalent to pooled OLS\" implying I should use standard OLS regression.Then I attempted the Mundlak approach - just for fun! The p value was not significant implying I should use a random model.Any thoughts to which I should use and where I have gone would be very much appreciated!   ","Creater_id":128541,"Start_date":"2016-08-22 07:36:04","Question_id":231103,"Tags":["panel-data","stata","random-effects-model","fixed-effects-model","hausman"],"Answer_count":0,"Last_activity":"2016-08-22 07:36:04","Link":"http://stats.stackexchange.com/questions/231103/fixed-or-random-model-hausman-xtoverid-mundlak-approach-all-giving-different","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3c92"},"View_count":23,"Display_name":"Simone","Question_score":1,"Question_content":"I wanted your thoughts on the following to have a second opinion. I work with words found in tweets. The words are substituted with numbers and are my outcome/DV. I think that these words(numbers) are statistically not indpendent. What is your opinion?Currently I am using non-parametric methods (Wilxocon rank-sum and loooking at Friedman's \"ANOVA\"), but wondered if there is another way to deal with it, such as the Welch-Satterthwaite correction deals with unequal variances for example. I have searched this forum and found a great question but with not answer yet. Another example treats the issue on a more theoretical level without providing an actual solution. What are the options?","Creater_id":115835,"Start_date":"2016-08-22 03:02:20","Question_id":231062,"Tags":["nonparametric","independence","parametric","methodology"],"Answer_count":0,"Last_activity":"2016-08-22 07:29:02","Link":"http://stats.stackexchange.com/questions/231062/data-statistically-not-indepdenent-non-parametric-methods","Creator_reputation":21}
{"_id":{"$oid":"5837a578a05283111e4d3c94"},"View_count":58,"Display_name":"KaatV","Question_score":1,"Question_content":"Agricultural Experimental design: a split-split plot in which a field is divided into 3 replications; each replication is divided into 2 to apply different pesticide spraying programs and each spray-plot is again divided into 2 to apply monocropping in one of the subplots and intercropping in the other subplot. Data collection: in each plot, 5 plants were sampled to count the insects on the main crop, thereby creating pseudoreplication. The simplest approach is to just take the mean of the counts on these 5 plants to deal with this kind of pseudoreplication. Instead of integers, my new dataset will have now decimal numbers.Research question: we wish to test whether the spraying program and the cropping system have any effect on insect density on the main crop. Analyzing approach: I have COUNT data and furthermore spatial pseudoreplication arising from the split-plot design, so I decide to work with General Mixed Effect Models. I'm sure I will have problems with overdispersion, so instead of poisson distribution (for Count data), I would like to work with negative binomial distribution. Problem: most of us will know, Negative Binomial (or even Poisson) distribution don't work for non-integers and my dataset contains now means of the counts, so decimal numbers. If I just ignore the pseudoreplication due to the 5 plants per plot, and run the glmer.nb() function, my degrees of freedom are too high. However, I read that using the appropriate mixed-effect model would remove pseudoreplication. My function looks like this: model\u0026lt;-glmer.nb(Mean.Count~Spraying.Program*Cropping.System+(1|Replication/Spraying.Program),data=Incidence)Questions: (1) How can I deal with the pseudoreplication arising from the sampling of 5 plants/plot?(2) Is it possible to adapt my formula and still use mixed-effect models on my data, or should I go for another strategy?","Creater_id":null,"Start_date":"2016-08-22 06:34:04","Question_id":231101,"Tags":["r"],"Answer_count":0,"Last_activity":"2016-08-22 07:23:18","Link":"http://stats.stackexchange.com/questions/231101/pseudoreplication-in-a-split-split-plot-design-on-count-data-in-r","Creator_reputation":null}
{"_id":{"$oid":"5837a578a05283111e4d3c96"},"View_count":19,"Display_name":"Shariar rahman","Question_score":1,"Question_content":"i want to do anova but my data is not normal? i can run Kruskal-Wallis H test instead but the post hoc test for Kruskal-Wallis is not easily found.the skewness is 1.33 and kurtosis .536 of my data- can i still use anova and tukey hsd?also i convert the data into percentage.Under 5 groups i collected 1000 data but hence the sample was not equal for 7 area i convert them into percentage,Which made 1000 data into 35 data. What should i do? ","Creater_id":128535,"Start_date":"2016-08-22 06:26:01","Question_id":231087,"Tags":["anova","normality","kruskal-wallis"],"Answer_count":0,"Last_activity":"2016-08-22 06:46:58","Link":"http://stats.stackexchange.com/questions/231087/to-what-degree-can-normality-be-violated-for-one-way-anova","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3c98"},"View_count":121,"Display_name":"prashanth","Question_score":3,"Question_content":"If I have a sample data set of 5000 points with many features and I have to generate a dataset with say 1 million data points using the sample data. It is like oversampling the sample data to generate many synthetic out-of-sample data points. The out-of-sample data must reflect the distributions satisfied by the sample data. The data here is of telecom type where we have various usage data from users. Is there any techniques available for this? Can SMOTE be applied for this problem?","Creater_id":86202,"Start_date":"2016-06-02 04:08:54","Question_id":215938,"Tags":["random-generation","out-of-sample"],"Answer_count":2,"Last_activity":"2016-08-22 06:06:58","Link":"http://stats.stackexchange.com/questions/215938/generate-synthetic-data-to-match-sample-data","Creator_reputation":403}
{"_id":{"$oid":"5837a578a05283111e4d3ca6"},"View_count":86,"Display_name":"Stan Shunpike","Question_score":6,"Question_content":"My econometrics professor used the term \"identified\" in class. We are considering data generating processes of the form Y = \\beta_0 + \\beta_1 X + U where  is a random variable and  is a random error term. Our regression lines take the form of Y = \\hat{\\beta_0}+\\hat{\\beta_1}XHe gave the following definition of \"identified\":   , are identified if a data set  contains enough information to \"pin down\" unique values for ,I am dissatisfied with this definition because he neither specifies what \"information\" is nor what \"pin down\" means. A Bit of ContextIn one of our exercises, we were given . According to my professor, this violates an assumption called \"Exogeneity\" which is necessary for a model to be 'identifiable.' Specifically, according to his lecture notes,   Exogeneity Assumption: The error term is uncorrelated with the regressors, or  for all . By assumption of , this can be rewritten as \\operatorname{Cov}(U_n,X_{nk}) = \\Bbb E(U_nX_{nk}) =0 for all It seems in our problem, he is trying to get us to understand why, if this Exogeneity assumption fails, a model cannot be identified. So hopefully this can give answerers context for how he is using the term. My QuestionCan someone clarify what he means by \"information\" and \"pin down\"? Or give a better definition altogether. EDIT:Pulled from Wikipedia:Observationally Equivalent --- two parameter values are considered observationally equivalent if they both result in the same probability distribution of observable data. Identified --- any situation where a statistical model will invariably have more than one set of parameters that generate the same distribution of observations, meaning that multiple parametrizations are observationally equivalent.This still doesn't really explain where \"exogeneity\" comes in and why it's related to being \"identified.\" ","Creater_id":68473,"Start_date":"2016-04-23 22:39:35","Question_id":209007,"Tags":["regression","model","identifiability","data-generating-process"],"Answer_count":1,"Last_activity":"2016-08-22 06:03:35","Link":"http://stats.stackexchange.com/questions/209007/how-can-i-tell-if-a-statistical-model-is-identified","Creator_reputation":936}
{"_id":{"$oid":"5837a578a05283111e4d3cb3"},"View_count":29,"Display_name":"Andrew_123","Question_score":1,"Question_content":"I would like to ask if somebody can help me with the following problem. I would like to generate synthetic data for dimension reduction algorithm testing. Specifically, I would like to have for example a matrix with 1000 rows (=points) and 50 columns (=features) but the real dimension of such matrix after dimension reduction should have only 10 features. How can I generate such matrix? I prefer python code but any advice helps me.I found out that I can multiply a random matrix (with 1000 rows and 10 columns) by the transposed first matrix from SVD decomposition (with 10 rows and 50 columns) but I do not know why.","Creater_id":120996,"Start_date":"2016-07-06 12:25:39","Question_id":222487,"Tags":["dataset","dimensionality-reduction","random-generation","svd"],"Answer_count":0,"Last_activity":"2016-08-22 06:02:23","Link":"http://stats.stackexchange.com/questions/222487/generate-data-for-dimension-reduction-algorithm-testing","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3cb5"},"View_count":15,"Display_name":"Andrea Chi Zhang","Question_score":-1,"Question_content":"I am trying to simulate some data for linear regression, where p = 30. The pairwise correlations are given, 0.85. I am using R package mvtnorm, with rmvnorm(length, mean, sigma). However this is using covariance instead of correlation, and the random matrix I simulated has a different correlation (and even covariance) from what I set up.Can some one tell me why this happened, and how can I generate data with pairwise correlations? ","Creater_id":127149,"Start_date":"2016-08-22 05:24:54","Question_id":231078,"Tags":["regression","correlation","covariance","random-generation"],"Answer_count":1,"Last_activity":"2016-08-22 06:01:22","Link":"http://stats.stackexchange.com/questions/231078/multivariate-number-generation-with-pairwise-correlation","Creator_reputation":3}
{"_id":{"$oid":"5837a578a05283111e4d3cc2"},"View_count":4967,"Display_name":"mlee","Question_score":11,"Question_content":"I would like to detect changes in time series data, which usually has the same shape. So far I've worked with the changepoint package for R and the cpt.mean(), cpt.var() and cpt.meanvar() functions. cpt.mean() with the PELT method works well when the data usually stays on one level. However I would also like to detect changes during descents. An example for a change, I would like to detect, is the section where the black curve suddenly drops while it actually should follow the examplary red dotted line. I've experimented with the cpt.var() function, however I couldn't get good results. Have you got any recommendations (those don't have to necessarily use R)?Here is the data with the change (as R object):dat.change \u0026lt;- c(12.013995263488, 11.8460207231808, 11.2845153487846, 11.7884417180764, 11.6865425802022, 11.4703118125303, 11.4677576899063, 11.0227199625084, 11.274775836817, 11.03073498338, 10.7771805591742, 10.7383206158923, 10.5847230134625, 10.2479315651441, 10.4196381241735, 10.467607842288, 10.3682422713283, 9.7834431752935, 9.76649842404295, 9.78257968297228, 9.87817694914062, 9.3449034905713, 9.56400153361727, 9.78120084558148, 9.3445162813738, 9.36767436354887, 9.12070987223648, 9.21909859069157, 8.85136359917466, 8.8814423003979, 8.61830163359642, 8.44796977628488, 8.06957847272046, 8.37999165387824, 7.98213210294954, 8.21977468333673, 7.683960439316, 7.73213584532496, 7.98956476021092, 7.83036046746187, 7.64496198988985, 4.49693528397253, 6.3459274845112, 5.86993447552116, 4.58301192892403, 5.63419551523625, 6.67847511602895, 7.2005344054883, 5.54970477623895, 6.00011922569104, 6.882667104467, 4.74057284230894, 6.2140437333397, 6.18511450451019, 5.83973575417525, 6.57271194428385, 5.36261938326723, 5.48948831338016, 4.93968645996861, 4.52598133247377, 4.56372558828803, 5.74515428123725, 5.45931581984165, 5.58701112949141, 6.00585679276365, 5.41639695946931, 4.55361875158434, 6.23720558202826, 6.19433060301002, 5.82989415940829, 5.69321394985076, 5.53585871082265, 5.42684812413063, 5.80887522466946, 5.56660158483312, 5.7284521523444, 5.25425775891636, 5.4227645808924, 5.34778016248718, 5.07084809927736, 5.324066161355, 5.03526881241705, 5.17387528516352, 5.29864121433813, 5.36894461582415, 5.07436929444317, 4.80619983525015, 4.42858947882894, 4.33623051506001, 4.33481791951228, 4.38041031792294, 3.90012900415342, 4.04262777674943, 4.34383842876647, 4.36984816425014, 4.11641092254315, 3.83985887104645, 3.81813419810962, 3.85174630901311, 3.66434598962311, 3.4281724860426, 2.99726515704766, 2.96694634792395, 2.94003031547181, 3.20892607367132, 3.03980832743458, 2.85952185077593, 2.70595278908964, 2.50931109659839, 2.1912274016859)","Creater_id":69488,"Start_date":"2015-02-27 13:58:56","Question_id":139660,"Tags":["r","time-series","change-point","structural-change"],"Answer_count":4,"Last_activity":"2016-08-22 05:54:31","Link":"http://stats.stackexchange.com/questions/139660/detecting-changes-in-time-series-r-example","Creator_reputation":58}
{"_id":{"$oid":"5837a578a05283111e4d3cd2"},"View_count":7727,"Display_name":"gabriel","Question_score":7,"Question_content":"I have a question about ARIMA models. Let's say I have a time series  that I would like to forecast and an  model seems like a good way to conduct the forecasting exercise. \\Delta Y_t = \\alpha_1  \\Delta Y_{t-1} + \\alpha_2 \\Delta Y_{t-2} + \\nu_{t} + \\theta_1 \\nu_{t-1} + \\theta_2 \\nu_{t-2}Now the lagged 's imply that my series today is influenced by prior events. This makes sense. But what is the interpretation of the errors? My prior residual (how off I was in my calculation) is influencing the value of my series today? How are the lagged residuals calculated in this regression as it is the product / remainder of the regression? ","Creater_id":16149,"Start_date":"2012-10-20 14:23:48","Question_id":40905,"Tags":["regression","time-series","interpretation"],"Answer_count":3,"Last_activity":"2016-08-22 05:46:35","Link":"http://stats.stackexchange.com/questions/40905/arima-model-interpretation","Creator_reputation":36}
{"_id":{"$oid":"5837a578a05283111e4d3ce0"},"View_count":50,"Display_name":"Sharah","Question_score":1,"Question_content":"I am very bad with statistics therefore I hope you guys can be kind.I have two groups of data, say:Stroke (n=5)Healthy (n=30)where n is the number of subjects. each subject have 10 reading for every parameters I am taking. I would like to do a comparison between stroke and healthy if there are any significant different between the group, so how should i do it?The thing that I don't really understand is because for each subject in each group have 10 readings. How can I compare the mean between group?P.s. the reason why I am taking 10 readings is because I am not taking the cholesterol level or stuff like that, but grip force of a person, which will be different (slightly) from trial to trial","Creater_id":128522,"Start_date":"2016-08-22 04:14:43","Question_id":231071,"Tags":["t-test"],"Answer_count":0,"Last_activity":"2016-08-22 05:46:25","Link":"http://stats.stackexchange.com/questions/231071/t-test-for-different-group-size-equal-variance","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3ce2"},"View_count":20,"Display_name":"Adel","Question_score":0,"Question_content":"I am fitting logistic binary multilevel models (with interactions: using R2Mlwin) and struggling a bit with finding an optimal way of presenting the results. I know that the first derivative (marginal effect) is not as straightforward as in linear estimations. In logistic estimations, it includes using the chain rule. E.g. see this post hereMy question to you is whether one could still assume linearity (as the function is additive) on the logit dimension within the link function and thus calculate what we could call the logit-marginal effect (of course, without using the chain rule)? In other words, if I am only interested in the slope of a covariate within the link function, would you then agree that this approach is mathematically and statistically sound?Many thanks in advance for your feedback","Creater_id":49364,"Start_date":"2016-08-22 04:22:54","Question_id":231072,"Tags":["logistic","logit","derivative","marginal-effect"],"Answer_count":0,"Last_activity":"2016-08-22 04:22:54","Link":"http://stats.stackexchange.com/questions/231072/is-there-such-a-thing-as-a-logit-marginal-effect","Creator_reputation":36}
{"_id":{"$oid":"5837a578a05283111e4d3ce4"},"View_count":1988,"Display_name":"RockTheStar","Question_score":1,"Question_content":"When we see a curve (or a normal distribution), we describe the highest value as \"peak\". Let's imagine we have a curve with only a minimum value (like an inverted hat), what do we call that minimum value?","Creater_id":41749,"Start_date":"2015-07-27 22:30:11","Question_id":163486,"Tags":["terminology","curves"],"Answer_count":3,"Last_activity":"2016-08-22 04:11:42","Link":"http://stats.stackexchange.com/questions/163486/in-statistical-terminology-what-is-the-opposite-of-peak","Creator_reputation":1638}
{"_id":{"$oid":"5837a578a05283111e4d3cf3"},"View_count":41,"Display_name":"Krombopulos Michael","Question_score":0,"Question_content":"At the moment I´m working on a huge dataset of genetic data.It contains about 1500 variables. My goal is to classify a disease risk group (20% of subjects) based on this data.At first I have run a series of Mann–Whitney U tests and identified 32 variables on a significant \u0026lt;0.01 level. If I'm not mistaken, 15 of these variables could be result of random processes.But my goal was to train a neural network to make successful classifications. I have 400 subjects and it was not possible to successfully train a model with the full set of variables.If I only take the 32 variables from the significant u-test I get a decent model. My question is. Is this some kind of circular reasoning? Thanks in advance!","Creater_id":121161,"Start_date":"2016-08-22 04:08:27","Question_id":231068,"Tags":["neural-networks","feature-selection"],"Answer_count":0,"Last_activity":"2016-08-22 04:08:27","Link":"http://stats.stackexchange.com/questions/231068/circular-reasoning","Creator_reputation":18}
{"_id":{"$oid":"5837a578a05283111e4d3cf5"},"View_count":34,"Display_name":"Sam Wilding","Question_score":1,"Question_content":"I'm trying to understand confidence intervals for linear combinations of parameters (lincom command in STATA). Let's say I'm interested in whether smoking is associated with low birth weight (using the lbw dataset, see example in help logit).webuse lbwlogit low age lwt i.race smoke ptl ht uiage     -.0271003   .0364504    -0.74   0.457   -.0985418   .0443412lwt     -.0151508   .0069259    -2.19   0.029   -.0287253   -.0015763race    black   1.262647    .5264101    2.40    0.016   .2309024    2.294392other   .8620792    .4391532    1.96    0.050   .0013548    1.722804smoke   .9233448    .4008266    2.30    0.021   .137739     1.708951ptl     .5418366    .346249     1.56    0.118   -.136799    1.220472ht      1.832518    .6916292    2.65    0.008   .4769494    3.188086ui      .7585135    .4593768    1.65    0.099   -.1418484   1.658875_cons   .4612239    1.20459     0.38    0.702   -1.899729   2.822176As I understand, the logit for a non-smoker is .46, which has a 95% confidence interval of -1.9 ; 2.82 (_cons). The additional effect for being a smoker is .92, which has a 95% confidence interval of .14 ; 1.71 (smoke), and this effect is significant at the 95% level (p = .021). I want to calculate the logit and confidence interval of low birth weight for: 1) non-smokers 2) smokers, so I turn to the lincom command.lincom _cons (redundant as this is the _cons coefficient)(   1)  [low]_cons  =   0.4612239    1.20459     0.38    0.702   -1.899729   2.822176lincom _cons+smoke[low]smoke  +   [low]_cons  =   01.384569    1.155967    1.20    0.231   -.8810857   3.650223The 95% confidence interval for smokers overlaps with that of non-smokers, even though the 95% confidence interval for the additional effect of smoking !=0 with 95% confidence. I'm finding it difficult to understand why this is, surely the linear combined estimate for smokers should not overlap with the constant?","Creater_id":128520,"Start_date":"2016-08-22 03:59:57","Question_id":231067,"Tags":["logistic","confidence-interval"],"Answer_count":0,"Last_activity":"2016-08-22 03:59:57","Link":"http://stats.stackexchange.com/questions/231067/understanding-lincom-confidence-intervals-stata","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3cf7"},"View_count":169,"Display_name":"greg","Question_score":4,"Question_content":"What is the correct way to specify a difference in difference model with individual level panel data?Here is the setup: Assume that I have individual-level panel data embedded in cities for multiple years and the treatment varies on the city-year level. Formally, let  be the outcome for individual  in city  and year  and  be a dummy for whether the intervention affected city  in year . A typical DiD estimator such as the one outlined in Bertrand et al (2004, p. 250) is based on a simple OLS model with fixed effect terms for city and year: y_{ist} = A_{s} + B_t + cX_{ist} + \\beta D_{st} + \\epsilon_{ist} But does that estimator ignore the individual-level panel structure (i.e. multiple observations for each individual within cities)? Does it make sense to extend this model with an individual-level fixed effect term ? Many DiD applications use repeated cross-section data without the individual-level panel data.Bertrand, Marianne, Esther Duflo, and Sendhil Mullainathan. 2004. “How Much Should We Trust Differences-in-Differences Estimates?” Quarterly Journal of Economics 119(1):249–75.","Creater_id":18644,"Start_date":"2016-08-15 14:29:23","Question_id":229996,"Tags":["econometrics","panel-data","fixed-effects-model","difference-in-difference"],"Answer_count":1,"Last_activity":"2016-08-22 03:41:26","Link":"http://stats.stackexchange.com/questions/229996/difference-in-differences-with-individual-level-panel-data","Creator_reputation":115}
{"_id":{"$oid":"5837a578a05283111e4d3d04"},"View_count":13,"Display_name":"sztal","Question_score":1,"Question_content":"Is there any established technique (something like the Dunnet test) for performing many-to-one post-hoc comparisons after the Friedman rank test for repeated measures?I would be also very grateful for pointing me to any R implementations of such a technique if there are any.EDIT.By many-to-one post-hoc comparison I of course mean for instance comparing all measurements to a baseline.","Creater_id":111884,"Start_date":"2016-08-22 03:41:05","Question_id":231065,"Tags":["repeated-measures","nonparametric","post-hoc","friedman-test"],"Answer_count":0,"Last_activity":"2016-08-22 03:41:05","Link":"http://stats.stackexchange.com/questions/231065/many-to-one-post-hoc-comparisons-after-friedman-test","Creator_reputation":69}
{"_id":{"$oid":"5837a578a05283111e4d3d06"},"View_count":73,"Display_name":"sponge_knight","Question_score":1,"Question_content":"Power is the probability of rejecting the null hypothesis when the null is truly False. It depends on effect size, sample size and the signifance level.If I want to estimate the sample size I need to run my experiment: I need to determine: effect size, signifance level, power. Power is typically 80% and alpha is typically 0.05. But how do I determine the effect size?Thanks for any help!","Creater_id":46925,"Start_date":"2016-03-21 11:33:47","Question_id":202849,"Tags":["statistical-significance","sample-size","effect-size","power-analysis","power"],"Answer_count":3,"Last_activity":"2016-08-22 03:11:28","Link":"http://stats.stackexchange.com/questions/202849/what-is-a-good-effect-size-in-power-analysis","Creator_reputation":1512}
{"_id":{"$oid":"5837a578a05283111e4d3d15"},"View_count":42,"Display_name":"marwa89","Question_score":0,"Question_content":"How can i validate the modeling step of my dataset with PLS regression? In other words can i calculate an X_hat and Y_hat using the modeling factors (T,P,Q,U,B,W) and compare it with the original X and Y. I'm using the algorithm PLS2.","Creater_id":128104,"Start_date":"2016-08-22 01:44:15","Question_id":231057,"Tags":["regression","prediction","pls"],"Answer_count":1,"Last_activity":"2016-08-22 02:35:37","Link":"http://stats.stackexchange.com/questions/231057/validation-of-modeling-step-of-pls","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3d22"},"View_count":44,"Display_name":"pa_ka","Question_score":2,"Question_content":"I heard just recently about PLS-DA and I was wondering how it differs from multinomial logistic regression, since logistic regression can be also used for categorical dependent variables. ","Creater_id":125357,"Start_date":"2016-08-21 22:27:07","Question_id":231042,"Tags":["regression","logistic","pls"],"Answer_count":0,"Last_activity":"2016-08-22 02:23:13","Link":"http://stats.stackexchange.com/questions/231042/whats-the-difference-between-logistic-regression-and-pls-da","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3d24"},"View_count":58,"Display_name":"DZinoviev","Question_score":2,"Question_content":"I have a set of sets and I want to assign a measure of variability to it. If all member sets are the same (e.g., S0={{a,b},{a,b},{a,b}}), the measure shall be 0. If some members are different (e.g., S1={{a,b},{a,b},{a,b,c}} and S2={{a,b},{a,b,c,d,e},{a,b,f,g,h}}), it shall be strictly greater than 0, and the magnitude shall be bigger for the sets which are intuitively more diverse (S1 is less diverse than S2).The measure does not have to be normalized, so I am thinking of some sort of entropy, but cannot figure out how to calculate it.","Creater_id":128313,"Start_date":"2016-08-19 11:52:19","Question_id":230761,"Tags":["variability"],"Answer_count":1,"Last_activity":"2016-08-22 02:13:11","Link":"http://stats.stackexchange.com/questions/230761/how-to-measure-the-variability-of-a-set-of-sets","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3d31"},"View_count":37,"Display_name":"User9523","Question_score":0,"Question_content":"An insurance company has a portfolio of two year policies. Aggregate annual claims from the portfolio follow an exponential distribution with mean  ( independent from year to year ). Annual premiums of  are payable at the start of each year. The insurer starts with no capital. We need to calculate the probability that the insurer is not ruined by the end of the second year. I tried solving this in the following manner : Let , U = Initial capital , U(t) = Capital at time t , c = Premium per unit time , S(1) = Aggregate annual claim. { U(t) = U + ct - s(t) }We need to find .=\u003e  =\u003e Now , we're given ~  , thus ~ .And we know that , when  ~  then , ~.Thus using this result and then using the  result I was able to calculate the probability as But the solution is given as follows : Let  and  be the aggregate amount of claims in the first and the second year respectively , thus the probability is found out as :  or  which gives the value .I know the values come out to be different but isn't the first solution good 'approximation'  to the second one ? Since the second method becomes a little clumsy , isn't the first method considerable ?Please help !   ","Creater_id":128502,"Start_date":"2016-08-21 23:49:11","Question_id":231048,"Tags":["probability"],"Answer_count":0,"Last_activity":"2016-08-22 01:42:07","Link":"http://stats.stackexchange.com/questions/231048/ruin-theory-problem","Creator_reputation":149}
{"_id":{"$oid":"5837a578a05283111e4d3d33"},"View_count":17,"Display_name":"Caro Sluijter","Question_score":1,"Question_content":"For my research I want look at the following:I have a dataset in which there is the variable type of report (NR and SR).Now I have a prognostic parameter X  on survival and I want to test whether the prognostic value of the parameter is better for SR than NR. So whether it differs between the two groups. Is it possible to compare the AIC-values of both groups? Or should I use some other kind of statistical method?Thnx in advance!","Creater_id":127231,"Start_date":"2016-08-10 05:42:40","Question_id":229167,"Tags":["aic"],"Answer_count":0,"Last_activity":"2016-08-22 01:12:31","Link":"http://stats.stackexchange.com/questions/229167/use-of-aic-in-comparing-prognostic-value-of-a-parameter-between-two-groups","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3d35"},"View_count":44,"Display_name":"pche8701","Question_score":1,"Question_content":"For Bayes rule, if my likelihood, and prior distribution are both uniform, is my posterior distribution also guaranteed to be uniform? In addition to this, if I apply some transformation to a uniform distribution such as:If  then is , then is  guaranteed to be still a valid pdf, so long as I apply a normalising constant to ? Could someone correct me if I'm incorrect. Thanks.","Creater_id":117574,"Start_date":"2016-08-21 23:11:38","Question_id":231047,"Tags":["bayesian","uniform","transform"],"Answer_count":1,"Last_activity":"2016-08-22 00:46:43","Link":"http://stats.stackexchange.com/questions/231047/bayes-rule-uniform-distribution","Creator_reputation":39}
{"_id":{"$oid":"5837a578a05283111e4d3d42"},"View_count":45,"Display_name":"Ravi","Question_score":3,"Question_content":"I want to create a simple statistical model for the bowler versus batsman contest in a game of cricket (it's similar to pitcher versus batter in baseball). Let's say the simple the model is this:One bowler bowls one ball to one batsman. There are 3 possible outcomes:The batsman is out and scores 0 runsThe batsman is not out and scores 0 runsThe batsman is not out and scores 1 or more runsMy goal is to is calculate the likelihood of each outcome based on exactly who the batsman is and who the bowler is. To do this I have a very large data set of balls bowled from real cricket matches.In another question a helpful comment suggested Elo rankings for the players would be a good approach to achieve this. I initially planned to score the three possible outcomes in a similar way to how win/draws/losses are scored by Elo rankings in Chess. However, I then thought that perhaps instead of thinking of a single event with three outcomes it might be better to think of it as two events each with two outcomes:Event 1The batsman is outThe batsman is not outEvent 2The batsman scores 0 runsThe batsman scores 1 or more runsThen I might calculate 2 separate Elo rankings for each player - one based on the contest between the bowler and batsman to get the batsmen out, and another based on the contest between the bowler to limit the scoring of runs and the batsman to score as many runs as possible.My question is whether I will be able to achieve my goal using this approach? Or is there some better way?Intuitively, based on my knowledge of cricket, it does seems to make sense to have two ratings rather than one. This is because players are not simply considered to be weak or strong but are also categorized by how they approach the game. Considering batsmen for example:Some are easy to get out and also score runs at a low rate (they just aren't very good)Some are aggressive and score runs at a high rate but as they take a lot of risks they get out quicklySome are defensive and score runs at a low rate but because they don't take risks they are difficult to get outSome are difficult to get out but are also able to score runs quickly (only the best players can do this)","Creater_id":128029,"Start_date":"2016-08-18 08:07:52","Question_id":230518,"Tags":["probability","modeling","games","elo"],"Answer_count":1,"Last_activity":"2016-08-22 00:38:38","Link":"http://stats.stackexchange.com/questions/230518/elo-ranking-for-attack-and-defence-seperately","Creator_reputation":63}
{"_id":{"$oid":"5837a578a05283111e4d3d4f"},"View_count":125,"Display_name":"user36878","Question_score":2,"Question_content":"I am running multiple imputation for a set of variables including clinical data. I am wondering if I can use (or should use) outcome variable (follow-up is 99%) to predict missing clinical data. There is about 12 % of non complete cases due to mostly one variable. I intend to use Amelia package (R)?I would be most grateful for answer.","Creater_id":36878,"Start_date":"2014-01-04 15:02:57","Question_id":81244,"Tags":["cox-model","multiple-imputation"],"Answer_count":1,"Last_activity":"2016-08-22 00:13:23","Link":"http://stats.stackexchange.com/questions/81244/multiple-imputation","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3d5c"},"View_count":40,"Display_name":"luchonacho","Question_score":4,"Question_content":"I have read quite a bit of bootstrapping, but the issue I want to address seem not to appear. Consider a simple regression model: y_{i} =  \\beta_{0} + \\beta_{1}x_{i} + e_{i}I am aware that bootstrapping is quite useful for obtaining standard error of estimated coefficients  and , and for other statistics of the regression. But my interests lays in the predicted errors . Every bootstrap iteration generates a set of . Thus, for each unit of observation , I have  values of . Can I use these  values to obtain a standard error for  for each ? This would be very useful, for example, to identify units with high standard errors, which might be an indication of measurement errors or coding errors, etc. But there are other uses too.Intuitively, this seems possible to me, but I want to confirm this makes sense theoretically.","Creater_id":100369,"Start_date":"2016-08-21 08:16:37","Question_id":230977,"Tags":["regression","bootstrap","standard-error"],"Answer_count":0,"Last_activity":"2016-08-22 00:11:45","Link":"http://stats.stackexchange.com/questions/230977/can-i-use-bootstrap-results-at-the-observation-level","Creator_reputation":584}
{"_id":{"$oid":"5837a578a05283111e4d3d5e"},"View_count":41,"Display_name":"Asterix","Question_score":5,"Question_content":"This is a follow-up to this question: Do M-estimators and L-estimators overlap?. In particular, the answers to that question suggest that there are L-estimators which are not M-estimators, but do not give such an example. More concretely, suppose I have an L-estimator  which is a linear combination of order-statistics of . Can I always write for some function ?","Creater_id":53814,"Start_date":"2016-08-21 15:42:39","Question_id":231011,"Tags":["estimation","robust","order-statistics","m-estimation"],"Answer_count":1,"Last_activity":"2016-08-22 00:01:59","Link":"http://stats.stackexchange.com/questions/231011/is-every-l-estimator-an-m-estimator","Creator_reputation":50}
{"_id":{"$oid":"5837a578a05283111e4d3d6b"},"View_count":84,"Display_name":"Andrew Milne","Question_score":4,"Question_content":"First let me give some background; I will summarize my questions at the end.The Beta distribution, parameterized by its mean  and , has , where  is the variance function. In a beta regression (e.g., using the betareg package in R), the regression assumes beta-distributed errors and estimates the fixed effects and the value of .In glm regression, it is possible to define a \"quasi\" distribution with a variance function of . So here the model assumes errors with the same variance function as Beta. The regression then estimates the fixed effects and the \"dispersion\" of the quasi distribution. I may be missing something important, but it would seem that these two methods are essentially identical, perhaps differing only in their estimation method.I tried both methods in R, regressing on a DV called \"Similarity\", which is in the interval :Call:betareg(formula = Similarity ~ N + NK + Step_ent, data = TapData, link = \"logit\")Coefficients (mean model with logit link):             Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)  0.715175   0.067805  10.547   \u0026lt;2e-16 ***N           -0.063806   0.003858 -16.537   \u0026lt;2e-16 ***NK          -0.362716   0.015008 -24.168   \u0026lt;2e-16 ***Step_ent    -0.696895   0.070233  -9.923   \u0026lt;2e-16 ***Phi coefficients (precision model with identity link):      Estimate Std. Error z value Pr(\u0026gt;|z|)    (phi)  10.6201     0.2084   50.96   \u0026lt;2e-16 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Type of estimator: ML (maximum likelihood)Log-likelihood:  3817 on 5 DfPseudo R-squared: 0.2633Number of iterations: 18 (BFGS) + 1 (Fisher scoring) Call:glm(formula = Similarity ~ N + NK + Step_ent, family = quasi(link = \"logit\", variance = \"mu(1-mu)\"), data = TapData)Coefficients:             Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)  0.777451   0.069809  11.137   \u0026lt;2e-16 ***N           -0.069348   0.003983 -17.411   \u0026lt;2e-16 ***NK          -0.364702   0.016232 -22.468   \u0026lt;2e-16 ***Step_ent    -0.704680   0.072491  -9.721   \u0026lt;2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for quasi family taken to be 0.0838547)    Null deviance: 566.25  on 4974  degrees of freedomResidual deviance: 422.76  on 4971  degrees of freedomAIC: NANumber of Fisher Scoring iterations: 4The coefficients of the two models are similar, as are their standard errors. The  parameter is also similar: I assume that the Dispersion parameter (as reported by glm) and  have the following relationship , in which case they are 10.6201 and 10.9254, respectively.However, none of these values is identical.Is this because the only thing that actually differs in the two methods is their estimation procedure? Or is there some more fundamental difference I am missing? Also, is there any reason to prefer one method over the other?","Creater_id":128484,"Start_date":"2016-08-21 17:37:24","Question_id":231017,"Tags":["generalized-linear-model","lme4","beta-regression","quasi-binomial"],"Answer_count":1,"Last_activity":"2016-08-21 23:59:44","Link":"http://stats.stackexchange.com/questions/231017/what-is-the-difference-between-beta-regression-and-quasi-glm-with-variance-mu","Creator_reputation":23}
{"_id":{"$oid":"5837a578a05283111e4d3d78"},"View_count":60,"Display_name":"Cherryl","Question_score":0,"Question_content":" is sample of size  from population with normal distribution . What kind of distribution does the statistic  have?How do I solve these kind of questions?What I know is that the average of variables with  has . I guess  has chi-square distribution but I can't solve for this combination. Any help? ","Creater_id":97622,"Start_date":"2016-08-21 06:27:50","Question_id":230966,"Tags":["normal-distribution","sums-of-squares"],"Answer_count":1,"Last_activity":"2016-08-21 22:39:44","Link":"http://stats.stackexchange.com/questions/230966/normal-distribution","Creator_reputation":33}
{"_id":{"$oid":"5837a578a05283111e4d3d85"},"View_count":57,"Display_name":"Nari2","Question_score":1,"Question_content":"I have the counts of genes in particular pathways from three organisms. How to statistically prove that few counts are significantly high or low as compared to another organism.My data looks like this : Is it possible to calulate p value for particular count compared between organisms.e.g. how to check in row 3 whether value, 5 (Org3) is significantly different (lower) from 25 and 21  (i.e. from org1 and org2) or not?Please suggest which test is suitable and how to go about.  UPDATED ONE POSSIBLE ANSWER MYSELF.","Creater_id":30084,"Start_date":"2016-06-28 03:16:45","Question_id":221002,"Tags":["statistical-significance","chi-squared","p-value"],"Answer_count":1,"Last_activity":"2016-08-21 22:16:18","Link":"http://stats.stackexchange.com/questions/221002/how-to-prove-significantly-high-or-low-values","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3d92"},"View_count":239,"Display_name":"Selden","Question_score":9,"Question_content":"I plot something to make a point to myself or someone else.  Usually, a question starts this process, and often the person asking hopes for a particular answer.How can I learn interesting things about the data in a less biased way?Right now I'm roughly following this method:Summary statistics.Stripchart.Scatter plot.Maybe repeat with an interesting subset of data.But that doesn't seem methodical or scientific enough.  Are there guidelines or procedures to follow that reveal things about the data I wouldn't think to ask? How do I know when I have done an adequate analysis?","Creater_id":5335,"Start_date":"2011-07-08 15:37:15","Question_id":12822,"Tags":["data-visualization","eda","knowledge-discovery"],"Answer_count":3,"Last_activity":"2016-08-21 22:12:55","Link":"http://stats.stackexchange.com/questions/12822/guidelines-for-discovering-new-knowledge-in-data","Creator_reputation":54}
{"_id":{"$oid":"5837a578a05283111e4d3da1"},"View_count":40,"Display_name":"nagimov","Question_score":0,"Question_content":"Background. I'm modeling a complex thermodynamic system using a set of a few hundreds of ordinary differential and algebraic equations. As a result of my simulation, I have a comparison plot of an experimental and model data in the form of a few pairs of corresponding time series:I would like to be able to compare these results using pure statistics, without introducing any information about the nature of simulated processes.The first and simplest approach I thought about is to simply calculate the integral of every error_data = model_data - experiment_data time series and divide it by integral of the experiment_data integral, so the result will show a sum of relative errors per each time series. I don't really like this approach since sometimes big spikes in error between model and experiment data (e.g. the difference between two violet lines on the plot at around 11 hours) give me errors larger than I feel they should be.Another approach I discovered through the deepest knowledge database (youtube) suggests using a sampling method. So I sampled a random subset of error_data time series gazillion times and plotted a histogram of the relative error (axis x is a relative error in percents):Can I use a p-value of 0.05 on this histogram to say *statistically the error of my model in comparison with experimental data is 3 plus/minus 10 percent\" ?Is there any better approach?","Creater_id":128476,"Start_date":"2016-08-21 15:26:01","Question_id":231010,"Tags":["time-series","modeling"],"Answer_count":1,"Last_activity":"2016-08-21 22:05:28","Link":"http://stats.stackexchange.com/questions/231010/comparison-of-the-time-series-experiment-and-model","Creator_reputation":1}
{"_id":{"$oid":"5837a578a05283111e4d3dae"},"View_count":37,"Display_name":"user3801801","Question_score":2,"Question_content":"I did learn about collinearity issue where it is possible that 2 significant independent variables when used together in the model can make either or both insignificant. However does the vice-versa case exist? and under which circumstances?","Creater_id":126702,"Start_date":"2016-08-21 21:32:19","Question_id":231033,"Tags":["statistical-significance","modeling","multicollinearity"],"Answer_count":1,"Last_activity":"2016-08-21 21:52:14","Link":"http://stats.stackexchange.com/questions/231033/is-it-possible-that-two-independent-variables-were-insignificant-when-present-al","Creator_reputation":116}
{"_id":{"$oid":"5837a578a05283111e4d3dbb"},"View_count":35,"Display_name":"pche8701","Question_score":0,"Question_content":"If I have a multivariate Gaussian distribution (assumed 2D):,I can plot the pdf (probability density function) contours over all the independent axes, since we know the full functional form for a multivariate Gaussian. But I was wondering if there is any significance to formulating the histogram from this pdf. That is, sample some random  and place these on a histogram. If this histogram is normalised is this not also some sort of pdf? Do we gain any new insight of the data from this histogram?Explicitly, I understand that the mean of this histogram should equal the mean for the full multivariate Gaussian pdf, but I'm unsure as to the significance of the std value of the histogram. Since the full multivariate pdf may have a full covariance matrix, but this histogram will only be 1D, and so I'm not sure how to interpret what this 1D histogram's std should be representing. It seems like we can take a distribution of arbitrary dimensionality, and arrive at a 1D histogram with a point std. Could someone clarify this for me? I think I have some basic concept muddled up. Thanks!","Creater_id":117574,"Start_date":"2016-08-21 20:54:41","Question_id":231031,"Tags":["pdf","histogram"],"Answer_count":0,"Last_activity":"2016-08-21 20:54:41","Link":"http://stats.stackexchange.com/questions/231031/probability-density-function-versus-histogram","Creator_reputation":39}
{"_id":{"$oid":"5837a578a05283111e4d3dbd"},"View_count":35,"Display_name":"TsTeaTime","Question_score":0,"Question_content":"Suppose that you know the mean should equal 2.02, the minimum value should be 0.8, and the maximum value should be 3.6. What is the best method to create as close to an approximately normal distribution given that the selected set of values falls within the min and max values? Is there an R studio package that has this type of distribution?","Creater_id":120082,"Start_date":"2016-08-21 19:24:59","Question_id":231025,"Tags":["r","distributions"],"Answer_count":0,"Last_activity":"2016-08-21 19:24:59","Link":"http://stats.stackexchange.com/questions/231025/distributions-with-uneven-min-max-boundaries-that-approximates-a-normal-distri","Creator_reputation":175}
{"_id":{"$oid":"5837a578a05283111e4d3dbf"},"View_count":15,"Display_name":"psych 101","Question_score":0,"Question_content":"I just got a review back on paper and the reviewers want me to report a correlation between the variable at time one and time two controlling for baseline scores.  Is a correlation that includes a control for baseline scores possible, or can that only be done using SEM?","Creater_id":125032,"Start_date":"2016-08-21 14:51:04","Question_id":231006,"Tags":["correlation","control"],"Answer_count":1,"Last_activity":"2016-08-21 19:16:22","Link":"http://stats.stackexchange.com/questions/231006/correlation-controlling-for-baseline-scores","Creator_reputation":5}
{"_id":{"$oid":"5837a578a05283111e4d3dcc"},"View_count":28,"Display_name":"kyk","Question_score":0,"Question_content":"I found the website which explain about residuals after chisq. test.The website explain below the Method of calculation.resresiduals           [,1]       [,2]       [,3]        [,4][1,]  1.6001894  0.2860388 -1.0124568 -0.85634884[2,] -1.5275252  0.5916080  0.3825184  0.06900656[3,]  0.1825742 -1.4142136  0.8616404  1.15470054resstdres), lower.tail=FALSE)*2           [,1]       [,2]      [,3]      [,4][1,] 0.02761930 0.64263616 0.1099059 0.2653552[2,] 0.02194677 0.29564182 0.5104301 0.9220851[3,] 0.82658070 0.04550026 0.2356052 0.1904303I understand from res-expected to res-stdres, but I cannot understand why pnorm(abs(res$stdres), lower.tail=FALSE)*2 can calculate each residuals p-value.Does anyone know the principle, or reference that related this method?I'm not good at English, so I'm sorry if it is difficult for you to understand my English.","Creater_id":128465,"Start_date":"2016-08-21 12:39:48","Question_id":230994,"Tags":["r","chi-squared","p-value","residual-analysis"],"Answer_count":1,"Last_activity":"2016-08-21 18:50:37","Link":"http://stats.stackexchange.com/questions/230994/does-anyone-know-about-the-principle-of-pnorm-method-in-residual-after-chisq-tes","Creator_reputation":1}
{"_id":{"$oid":"5837a578a05283111e4d3dd8"},"View_count":82,"Display_name":"Peter Smith","Question_score":6,"Question_content":"I have read in The Elements of Statistical Learning book and particularly in the Partial Least Squares (PLS) section:   Orthogonalize each  with respect to .I would like to know what \"orthogonalize\" means in this statement or general.1- what orthogonal means? 2- how to orthogonalise?","Creater_id":124456,"Start_date":"2016-08-20 09:00:56","Question_id":230866,"Tags":["terminology","linear-algebra","pls"],"Answer_count":1,"Last_activity":"2016-08-21 18:50:20","Link":"http://stats.stackexchange.com/questions/230866/what-does-orthogonalize-mean","Creator_reputation":41}
{"_id":{"$oid":"5837a578a05283111e4d3de5"},"View_count":44,"Display_name":"ali48112","Question_score":5,"Question_content":"What's the best method to see if there's a strong correlation between two types of ratings on a product level (product 1, 2, ..., n) where each product has ratings of type 'a' and 'b'? For the paired averages of ratings (of type a and b) on a product level, would it be biasing my results if I filter out products with small number of ratings per type of rating, or low standard error in either average of ratings, or a combination of both? Which would be best: pearson's or spearman's? I'm interested in understanding what's the most valid way to observe a potential existence of a relationship? Also, would said method allow for more explanatory factor variables, like product type?The overall goal is to see if there is a correlated (or non-random) relationship between the two averages on a product level (since they're what's surfaced on the marketplace).","Creater_id":128486,"Start_date":"2016-08-21 17:18:18","Question_id":231015,"Tags":["r","correlation"],"Answer_count":0,"Last_activity":"2016-08-21 17:32:17","Link":"http://stats.stackexchange.com/questions/231015/what-metric-to-calculate-correlation-between-average-user-ratings-of-products","Creator_reputation":26}
{"_id":{"$oid":"5837a578a05283111e4d3de7"},"View_count":85,"Display_name":"Pinocchio","Question_score":1,"Question_content":"I was watching his coursera course video on RMSProp and he said in a paraphrase:  Gradients magnitude vary widely.I was wondering, why is it that they vary widely? I had a guess but wanted to understand what he meant on the video. My guess is:Stochastic Gradient Descent gives it a variance because of its randomness.The activation functions for some reason gave it this property too (not sure why or which ones do and which ones don't)I was wondering if those were the reasons and even if they were, why and how they contributed to this issue or there were additional explanations that I was overlooking.","Creater_id":37632,"Start_date":"2016-06-22 21:46:17","Question_id":220245,"Tags":["machine-learning","neural-networks","gradient-descent"],"Answer_count":1,"Last_activity":"2016-08-21 17:08:04","Link":"http://stats.stackexchange.com/questions/220245/why-does-geoffrey-hinton-say-in-his-coursera-course-that-gradient-magnitudes-can","Creator_reputation":775}
{"_id":{"$oid":"5837a578a05283111e4d3df4"},"View_count":26,"Display_name":"Grover","Question_score":1,"Question_content":"Specifically, I'm using a CNN for image classification and its architecture is:data-\u003econv1-\u003epool1-\u003econv2-\u003epool2-\u003eclassifier/softmaxWhen propagating errors from conv2 to pool1, how do I do that? I think that I have to convolve each channel from errors of conv2 with each filter and then add the results together to get the errors from pool1. Is that ok? ","Creater_id":121264,"Start_date":"2016-06-25 14:46:06","Question_id":220659,"Tags":["machine-learning","neural-networks","conv-neural-network"],"Answer_count":0,"Last_activity":"2016-08-21 17:07:47","Link":"http://stats.stackexchange.com/questions/220659/how-do-i-perform-backpropagation-in-a-convolutional-neural-network-with-2-convol","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3df6"},"View_count":161,"Display_name":"Andrey","Question_score":3,"Question_content":"I'm following Google's TensorFlow Deep MNIST for Experts tutorial.Here is my code:http://pastebin.com/ePktssrnThe networks seems to get close to 100% accuracy after about training 1000 steps, but all of its kernels on the outermost layer remain completely unchanged.Here's a visualization I made: http://i.imgur.com/gCmxq40.pngEach row represents one of 32 5x5 kernels defined by W_conv1; leftmost image in the row is initial state of the kernel, and each next image horizontally is how that kernel looks after 5 training steps.As it can be clearly seen from the picture, kernels seem to be completely static.I used the same code to draw kernels for a shallow network with convolutions, and they train very well - you can see shapes slowly forming out of noise. But on that particular example, kernels stay completely same.Is this normal? Is this a flaw in google's example of deep CNN? Is this some mistake I made following the tutorial?","Creater_id":121915,"Start_date":"2016-07-01 05:40:06","Question_id":221669,"Tags":["classification","deep-learning","conv-neural-network","tensorflow"],"Answer_count":0,"Last_activity":"2016-08-21 17:07:39","Link":"http://stats.stackexchange.com/questions/221669/tensorflow-deep-mnist-for-experts-tutorial-kernels-seem-to-never-learn-anything","Creator_reputation":16}
{"_id":{"$oid":"5837a578a05283111e4d3df8"},"View_count":10,"Display_name":"user39531","Question_score":0,"Question_content":"Objective: To find out the level of enjoyment experienced by two groups of viewers watching an advertisement (in HD or BluRAY version), when the sound quality is compressed or not compressed. Basically each group (HD and BluRAY Group) viewed the advert two times, when the sound quality was compressed and not compressed.The design in Table 1 can be either (i) 2x2 in-between subjects test or (ii) Mixed ANOVA with one between and one within subject factor, or both. How do we determine whether Table 1 reflects (i) or (ii)?","Creater_id":39531,"Start_date":"2016-08-20 11:38:54","Question_id":230888,"Tags":["anova","t-test","ancova","manova"],"Answer_count":0,"Last_activity":"2016-08-21 16:36:46","Link":"http://stats.stackexchange.com/questions/230888/mixed-anova-or-2x2-between-subject-test","Creator_reputation":60}
{"_id":{"$oid":"5837a578a05283111e4d3dfa"},"View_count":6162,"Display_name":"Tim","Question_score":2,"Question_content":"(1) I am looking for a package for computing the power of a matrix. If you have some good recommendation please let me know.(2) I searched on the internet and followed what some said to install a package called \"Malmig\" in R but after selecting the mirror site, it failed:  In install.packages(\"Malmig\") : package ‘Malmig’ is not availableSome idea why?Thanks!","Creater_id":1005,"Start_date":"2010-11-08 09:18:53","Question_id":4320,"Tags":["r"],"Answer_count":4,"Last_activity":"2016-08-21 16:14:54","Link":"http://stats.stackexchange.com/questions/4320/compute-the-power-of-a-matrix-in-r","Creator_reputation":5507}
{"_id":{"$oid":"5837a578a05283111e4d3e0a"},"View_count":47,"Display_name":"Florian","Question_score":2,"Question_content":"I have a question regarding the statistical significance in a biological experiment I conducted. There are cells (n=51), which are treated with chemical P (n=27) or untreated (¬P, n=24). I measure a rate of events happening inside the cells which seems to follow a Poisson distribution (mean around 1 events/s).Directly after measuring this “baseline” rate (with 10 time points per cell), I add either chemical N or chemical C to the respective cell and keep measuring (i.e. I have a paired measurement before and after treatment on the same cell). In total I have measured 13 cells with ¬P and added N, 11 cells with ¬P and added C, 13 cells with P and added N, and 14 cells with P and added C. After treatment I have 60 time points per cell.My first hypothesis is that P treated cells have a reduced rate with respect to untreated (¬P) cells. This should be independent of N or C, i.e. also after treatment with C, the P group gives me smaller rates. The same goes for treatment with N.My second hypothesis is that N increases the rate compared to the baseline.Regarding the effect of C, I would like to test the hypothesis that C has no effect.Glen_b suggested in a previous version of the question that it might be possible to use a Poisson GLM (given that it is set up correctly) or a GLMM, but I have a hard time setting it up correctly (or at all). Could someone help me with this or suggest alternative means of analysis?","Creater_id":127989,"Start_date":"2016-08-17 05:40:28","Question_id":230265,"Tags":["statistical-significance","anova","poisson","wilcoxon","paired-comparisons"],"Answer_count":1,"Last_activity":"2016-08-21 16:13:25","Link":"http://stats.stackexchange.com/questions/230265/statistical-analysis-of-double-treated-sample-data-from-poisson-distribution","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3e17"},"View_count":51,"Display_name":"jmwagg","Question_score":4,"Question_content":"Want to understand if I'm using the binary logit regression correctly and that my data does not violate any assumptions. An example of my data is below. I'm attempting to determine if a customer will reorder a pizza based on their past order history. The column \"survived\" indicates whether or not the customer survived to order again. I'm assuming that as orders grow, customers will be less likely to reorder the same pizza (maybe they get tired of the same old pizza?). My goal is to be able to determine, given my customer base, and how many orders they've placed to this date, what the probability of a re-order is. My concern is that the yes/no survived in period X is dependent on the  survival of the customer in period X-1. Thank you for the help/tips. [ {   \"Customer\": 1,   \"OrderNum\": 1,   \"PizzaType\": 1,   \"Survived\": 1 }, {   \"Customer\": 1,   \"OrderNum\": 2,   \"PizzaType\": 1,   \"Survived\": 1 }, {   \"Customer\": 1,   \"OrderNum\": 3,   \"PizzaType\": 1,   \"Survived\": 0 }, {   \"Customer\": 2,   \"OrderNum\": 1,   \"PizzaType\": 1,   \"Survived\": 0 }, {   \"Customer\": 3,   \"OrderNum\": 1,   \"PizzaType\": 2,   \"Survived\": 1 }, {   \"Customer\": 3,   \"OrderNum\": 2,   \"PizzaType\": 2,   \"Survived\": 0 }]","Creater_id":2644,"Start_date":"2016-08-21 13:00:25","Question_id":230996,"Tags":["regression","binomial","binary-data","logit"],"Answer_count":1,"Last_activity":"2016-08-21 15:49:51","Link":"http://stats.stackexchange.com/questions/230996/correct-use-of-binary-logistic-regression","Creator_reputation":48}
{"_id":{"$oid":"5837a578a05283111e4d3e23"},"View_count":13,"Display_name":"HIL","Question_score":0,"Question_content":"I am using Lavaan (R) to test for mediation and moderation.Currently I have one model that checks for moderated mediation (moderator affects b and c') and I have another model (other variables) that tests for mediated moderation (moderation leading to mediator, which improves prediction of outcome Y).I am wondering where to put the covariates, as I don't want to correct twice for it, but also don't want to skip it.For the moderated mediation, I have entered the covariates in formulas predicting Y and the mediator:Y = c1*X1+covariatesY= c2*WY = c3*X1*WM = a1 *X1+covariatesM = a2*WM = a3*X1*WY = b1*MFor the mediated moderation, I have entered the covariates for outcome Y and mediator as well. :Y = c1*X1+covariatesY = c2*WY = c3*X1*WM = a1*X1+covariatesM = a2*WM= a3*X1*WY = b1*MIs this correct or am I overcorrecting my models? Thanks!","Creater_id":113780,"Start_date":"2016-08-21 15:14:02","Question_id":231008,"Tags":["r","interaction","mediation","covariate"],"Answer_count":0,"Last_activity":"2016-08-21 15:14:02","Link":"http://stats.stackexchange.com/questions/231008/covariate-placement-in-mediation-moderation","Creator_reputation":15}
{"_id":{"$oid":"5837a578a05283111e4d3e25"},"View_count":18,"Display_name":"user1700890","Question_score":1,"Question_content":"So, I guess I am completely lost on subject of regression trees. Here is my code:import pandas as pdimport xgboost as xgbimport matplotlib.pyplot as pltimport numpy as npx = np.linspace(0, 10, 20)train_X = pd.DataFrame({\"A\":x})train_y = pd.Series(np.sin(x))gbm = xgb.XGBClassifier(max_depth=7, n_estimators=300, learning_rate=0.01).fit(train_X, train_y)predictions = gbm.predict(train_X)plt.plot(x, predictions)plt.show()The above code generates straight line as prediction. I expected some step function approximating sin. Any suggestions? How do regression trees work?","Creater_id":79465,"Start_date":"2016-08-21 14:50:43","Question_id":231005,"Tags":["python","cart","gbm"],"Answer_count":1,"Last_activity":"2016-08-21 15:12:19","Link":"http://stats.stackexchange.com/questions/231005/gbm-and-1-dim-approximation","Creator_reputation":122}
{"_id":{"$oid":"5837a578a05283111e4d3e32"},"View_count":269,"Display_name":"Joonatan Samuel","Question_score":2,"Question_content":"Given difficult learning task (e.g high dimensionality, inherent data complexity) Deep Neural Networks become hard to train. To ease many of the problems one might:Normalize \u0026amp;\u0026amp; handpick quality data choose a different training algorithm (e.g RMSprop instead of Gradient Descent)pick a steeper gradient Cost function (e.g Cross Entropy instead of MSE)Use different network structure (e.g Convolution layers instead of Feedforward)I have heard that there are clever ways to initialize better weights. For example you can choose the magnitude better:Glorot and Bengio (2010)for sigmoid units: sample a Uniform(-r, r) with or hyperbolic tangent units: sample a Uniform(-r, r) with Is there any consistent way of initializing the weights better?","Creater_id":74835,"Start_date":"2016-03-28 07:47:15","Question_id":204114,"Tags":["machine-learning","neural-networks","deep-learning","randomization"],"Answer_count":4,"Last_activity":"2016-08-21 14:57:07","Link":"http://stats.stackexchange.com/questions/204114/deep-neural-network-weight-initialization","Creator_reputation":28}
{"_id":{"$oid":"5837a578a05283111e4d3e42"},"View_count":17,"Display_name":"hotsource","Question_score":0,"Question_content":"I am trying to estimate a factor model that explains forward stock returns with time-varying factor loading and assume the factor loading follows a random walk:y_t = x_t  \\beta_t + e_t\\beta_t = \\beta_{t-1} + u_tUsing a Kalman filter we can filter for  if the variance of  and  are known at each time period. But if they are unknown, what methods will help to estimate ?  I have been reading on Gibbs sampling but not sure if it will help to sample (, var(),var()) together. Any advice will be appreciated.","Creater_id":62226,"Start_date":"2016-08-21 14:32:05","Question_id":231004,"Tags":["bayesian","mcmc","gibbs"],"Answer_count":0,"Last_activity":"2016-08-21 14:32:05","Link":"http://stats.stackexchange.com/questions/231004/methods-to-estimate-time-varying-factor-loading","Creator_reputation":16}
{"_id":{"$oid":"5837a578a05283111e4d3e44"},"View_count":129,"Display_name":"Ravi","Question_score":9,"Question_content":"I have a data set detailing a large number of cricket games (a few thousand). In cricket \"bowlers\" repeatedly throw a ball at a succession of \"batsmen\". The bowler is trying to get the batsman \"out\". In this respect it's quite similar to pitchers and batters in baseball.If I took the whole dataset and divided the total number of balls that got a batsman out by the total number of balls bowled, I can see that I would have the average probability of a bowler getting a batsman out - it will be around 0.03 (hopefully I haven't gone wrong already?)What I am interested in is what I can do to try and calculate the probability of a specific batsman being bowled out by a specific bowler on the next ball.The dataset is large enough that any given bowler will have bowled thousands of balls to a wide range of batsmen. So I believe that I could simply divide the number of outs a bowler achieved by the number of balls he has bowled to calculate a new probability for that specific bowler getting an out from the next ball.My problem is the dataset is not large enough to guarantee that a given bowler has bowled a statistically significant number of balls at any given batsmen. So if I'm interested in calculating the probability of an out for a specific bowler facing a specific batsmen I don't think this can't be done in the same simplistic way.My question is whether the following approach is valid:Across the whole dataset the probability of a ball getting an out is 0.03.If I calculate that on average bowler A has a probability of getting on out of 0.06 (ie twice as likely as an average bowler),and on average batsman B had a probability of being out of 0.01 (a third as likely as an average batsmen),is it then valid to say the probability of that specific batsman being out on the next ball to that specific bowler is going to be 0.06 * (0.01 / 0.03) = 0.02?","Creater_id":128029,"Start_date":"2016-08-17 10:50:34","Question_id":230337,"Tags":["probability","modeling","games"],"Answer_count":2,"Last_activity":"2016-08-21 14:20:09","Link":"http://stats.stackexchange.com/questions/230337/modeling-cricket-bowlers-getting-batsmen-out","Creator_reputation":63}
{"_id":{"$oid":"5837a578a05283111e4d3e52"},"View_count":37,"Display_name":"user3761369","Question_score":2,"Question_content":"I am looking to determine if two or more raters have statistically different observations of the same data, which is continuous in nature.I am thinking a reasonable approach to this problem is to do a paired t-test if there are two raters and a one-way repeated measures ANOVA if there are more than two raters.  I've never seen a t-test or F-test used to look at rater disagreement in this way.  We're also using an ICC and Bland-Altman plotting to explore this agreement/disagreement between raters of continuous data, but would like to be able to say that the raters 'were' or 'were not' statistically different from each other.Does our approach seem reasonable or do others have suggestions?","Creater_id":67661,"Start_date":"2016-08-19 12:09:55","Question_id":230770,"Tags":["anova","repeated-measures","agreement-statistics","bayesian-anova"],"Answer_count":1,"Last_activity":"2016-08-21 14:05:04","Link":"http://stats.stackexchange.com/questions/230770/using-repeated-measures-anova-to-test-rater-disagreement","Creator_reputation":18}
{"_id":{"$oid":"5837a578a05283111e4d3e5f"},"View_count":17,"Display_name":"Graeme","Question_score":3,"Question_content":"I'm currently trying to integrate some proteomic and genomic data.  This involves plotting the log2 fold change in mRNA levels against the corresponding log2 fold change in protein levels.  The values for approximately 2000 mRNA/protein pairs are plotted in each scatterplot (grey points) and I've created a script which will highlight the points corresponding to a particular subset which belong to a specific biological pathway or process.  Out of the several dozen scatterplots which I've created two examples are shown below - the subset represented by the red data points clearly cluster in one quadrant while the subset represented by the blue points appear to have the same distribution as the underlying distribution for all points (grey).   My question is what test would be the most appropriate to find whether the distribution of the subset is significantly different from that of the general population?  A chapter on mRNA and protein correlation in the book \"Computational and Statistical Methods for Protein Quantification by Mass Spectrometry\" suggests that a statistical test exists but merely states \"These calculations are, however, not straight forward\". Would I be correct in thinking that the following tests are applicable? - Hotelling's two-sample T-square statistic, two-sample Kolmogorov–Smirnov test, and cross-match testI understand that I would have to correct these results for multiple testing as I have a lot of scatterplots.","Creater_id":113565,"Start_date":"2016-08-20 15:20:39","Question_id":230910,"Tags":["statistical-significance","multivariate-analysis","biostatistics","bivariate","bioinformatics"],"Answer_count":0,"Last_activity":"2016-08-21 12:33:06","Link":"http://stats.stackexchange.com/questions/230910/test-to-determine-if-the-distribution-of-a-subset-of-points-in-a-bivariate-scatt","Creator_reputation":16}
{"_id":{"$oid":"5837a578a05283111e4d3e61"},"View_count":73,"Display_name":"lmcshane","Question_score":0,"Question_content":"I want to compare 2 datasets. The first is categorical data on a 4 point scale: 1-4. The second dataset has continuous data that has been normalized (raw scores on a 0-100 scale, subtract the respondent’s mean score across items and divide by their standard deviation across items). These two datasets measure the same thing, personality, so I want to compare scores between them. What's the best way to normalize a categorical variable in order to compare between these scores?","Creater_id":107127,"Start_date":"2016-08-21 11:43:19","Question_id":230991,"Tags":["categorical-data","normalization","continuous-data"],"Answer_count":0,"Last_activity":"2016-08-21 11:43:19","Link":"http://stats.stackexchange.com/questions/230991/normalizing-categorical-data","Creator_reputation":136}
{"_id":{"$oid":"5837a578a05283111e4d3e63"},"View_count":39,"Display_name":"Josh Robertson","Question_score":2,"Question_content":"So I ran a binomial glm to look at the effect of minimum temperature (continuous data) and moon phase (categorical data with 3 categories) on lion incidents. I removed the intercept to look at all 3 categories of moon by using + 0 in my glm.These were the results:I then calculated the odd ratios and confidence intervals using the code: exp(cbind(OR = coef(GLMoon), confint(GLMoon)))which gave me this:I'm not fully clear on how to interpret these as most of the odds ratios I've seen have been above 1, but from what I've read I guess it would mean that the odds for incidents during the 'new.moonthe rest' phase are 45% lower than during other phases? And that holding moon phases at a fixed value, we will see a 1.14% increase in the odds of an incident, for a one-unit decrease in temperature?I'd prefer to calculate probabilities of attack, but I'm unsure how to do this from the coefficients or odds ratios when the intercept is removed. I'd really appreciate any help!Cheers","Creater_id":128130,"Start_date":"2016-08-21 11:06:11","Question_id":230988,"Tags":["probability","generalized-linear-model","interpretation","odds-ratio","intercept"],"Answer_count":1,"Last_activity":"2016-08-21 11:36:51","Link":"http://stats.stackexchange.com/questions/230988/interpreting-odds-ratios-for-logistic-regression-with-intercept-removed","Creator_reputation":20}
{"_id":{"$oid":"5837a578a05283111e4d3e70"},"View_count":41,"Display_name":"tintinthong","Question_score":1,"Question_content":"I am performing an experiment to identify significance in certain factors and covarites. I am using an ANCOVA model with 6 regression covariates, 1 treatment factor and a blocking factor and am including interactions between the covariates and the treatment factor. Are my sums of squares(via aov function) still valid if I get unestimable coefficients in R? I am not interested in making inference on the coefficients, so I was thinking whether non-estimable coefficients can just be ignored.EDIT for example to MDEWEY comment.data.frame(x1,x2,x3,x4,x5,x6)    x1 x2 x3 x4 x5 x61   12  0  0  0  0  02   12  0  0  0  0  03   12  0  0  0  0  04   12  0  0  0  0  05    0 12  0  0  0  06    0 12  0  0  0  07    0 12  0  0  0  08    0 12  0  0  0  09    0  0 12  0  0  010   0  0 12  0  0  011   0  0 12  0  0  012   0  0 12  0  0  013   0  0  0 12  0  014   0  0  0 12  0  015   0  0  0 12  0  016   0  0  0 12  0  017   0  0  0  0 12  018   0  0  0  0 12  019   0  0  0  0 12  020   0  0  0  0 12  021   0  0  0  0  0 1222   0  0  0  0  0 1223   0  0  0  0  0 1224   0  0  0  0  0 1225   6  6  0  0  0  026   6  6  0  0  0  027   6  6  0  0  0  028   6  6  0  0  0  029   6  0  6  0  0  030   6  0  6  0  0  031   6  0  6  0  0  032   6  0  6  0  0  033   6  0  0  6  0  034   6  0  0  6  0  035   6  0  0  6  0  036   6  0  0  6  0  037   6  0  0  0  6  038   6  0  0  0  6  039   6  0  0  0  6  040   6  0  0  0  6  041   6  0  0  0  0  642   6  0  0  0  0  643   6  0  0  0  0  644   6  0  0  0  0  645   0  6  6  0  0  046   0  6  6  0  0  047   0  6  6  0  0  048   0  6  6  0  0  049   0  6  0  6  0  050   0  6  0  6  0  051   0  6  0  6  0  052   0  6  0  6  0  053   0  6  0  0  6  054   0  6  0  0  6  055   0  6  0  0  6  056   0  6  0  0  6  057   0  6  0  0  0  658   0  6  0  0  0  659   0  6  0  0  0  660   0  6  0  0  0  661   0  0  6  6  0  062   0  0  6  6  0  063   0  0  6  6  0  064   0  0  6  6  0  065   0  0  6  0  6  066   0  0  6  0  6  067   0  0  6  0  6  068   0  0  6  0  6  069   0  0  6  0  0  670   0  0  6  0  0  671   0  0  6  0  0  672   0  0  6  0  0  673   0  0  0  6  6  074   0  0  0  6  6  075   0  0  0  6  6  076   0  0  0  6  6  077   0  0  0  6  0  678   0  0  0  6  0  679   0  0  0  6  0  680   0  0  0  6  0  681   0  0  0  0  6  682   0  0  0  0  6  683   0  0  0  0  6  684   0  0  0  0  6  685   4  4  4  0  0  086   4  4  4  0  0  087   4  4  4  0  0  088   4  4  4  0  0  089   4  4  0  4  0  090   4  4  0  4  0  091   4  4  0  4  0  092   4  4  0  4  0  093   4  4  0  0  4  094   4  4  0  0  4  095   4  4  0  0  4  096   4  4  0  0  4  097   4  4  0  0  0  498   4  4  0  0  0  499   4  4  0  0  0  4100  4  4  0  0  0  4101  4  0  4  4  0  0102  4  0  4  4  0  0103  4  0  4  4  0  0104  4  0  4  4  0  0105  4  0  4  0  4  0106  4  0  4  0  4  0107  4  0  4  0  4  0108  4  0  4  0  4  0109  4  0  4  0  0  4110  4  0  4  0  0  4111  4  0  4  0  0  4112  4  0  4  0  0  4113  4  0  0  4  4  0114  4  0  0  4  4  0115  4  0  0  4  4  0116  4  0  0  4  4  0117  4  0  0  4  0  4118  4  0  0  4  0  4119  4  0  0  4  0  4120  4  0  0  4  0  4121  4  0  0  0  4  4122  4  0  0  0  4  4123  4  0  0  0  4  4124  4  0  0  0  4  4125  0  4  4  4  0  0126  0  4  4  4  0  0127  0  4  4  4  0  0128  0  4  4  4  0  0129  0  4  4  0  4  0130  0  4  4  0  4  0131  0  4  4  0  4  0132  0  4  4  0  4  0133  0  4  4  0  0  4134  0  4  4  0  0  4135  0  4  4  0  0  4136  0  4  4  0  0  4137  0  4  0  4  4  0138  0  4  0  4  4  0139  0  4  0  4  4  0140  0  4  0  4  4  0141  0  4  0  4  0  4142  0  4  0  4  0  4143  0  4  0  4  0  4144  0  4  0  4  0  4145  0  4  0  0  4  4146  0  4  0  0  4  4147  0  4  0  0  4  4148  0  4  0  0  4  4149  0  0  4  4  4  0150  0  0  4  4  4  0151  0  0  4  4  4  0152  0  0  4  4  4  0153  0  0  4  4  0  4154  0  0  4  4  0  4155  0  0  4  4  0  4156  0  0  4  4  0  4157  0  0  4  0  4  4158  0  0  4  0  4  4159  0  0  4  0  4  4160  0  0  4  0  4  4161  0  0  0  4  4  4162  0  0  0  4  4  4163  0  0  0  4  4  4164  0  0  0  4  4  4","Creater_id":121671,"Start_date":"2016-08-20 17:07:11","Question_id":230915,"Tags":["regression","anova","estimation"],"Answer_count":0,"Last_activity":"2016-08-21 10:32:28","Link":"http://stats.stackexchange.com/questions/230915/does-sums-of-squares-make-sense-if-unestimable-coefficients","Creator_reputation":117}
{"_id":{"$oid":"5837a578a05283111e4d3e72"},"View_count":36,"Display_name":"CPerkins","Question_score":0,"Question_content":"I'm trying to do some hypothesis testing for work, but I have to admit that it's a bit trickier when you have to formulate the question yourself.I have some data of the number of errors in a software we provide in the first 3 months after going live. I also have the number of those errors that are \"critical errors\". The hypothesis I want to test is that the number of critical errors in the first three months is equal to zero. I'm used to doing this in R, but my boss wanted it done in excel. Here's what I've got so far, but I'm not sure if I'm setting things up correctly:Total Errors   | Criticals     24        |     1     31        |     0      2        |     1      8        |     3      2        |     0      0        |     0      2        |     0      4        |     0      4        |     0      5        |     0      5        |     0      9        |     0      6        |     0      7        |     1      0        |     0     12        |     0     10        |     1     13        |     0     19        |     0Totals 163     |     7               |s.e. criticals | 0.74059196mean criticals | 0.04294479h0             | 0t-value        | 0.74032982p-value        | 0.76991425Sorry for the horrible format. I tried to copy it directly from excel. For SD and mean I just used excel's functions. I then calculated the t-statistic manually, and used excel's t-test function to find the p-value.From what I've got so far I would say that I can't reject the null hypothesis that mean of critical errors in the first three months is equal to zero.","Creater_id":null,"Start_date":"2016-08-20 08:21:58","Question_id":230891,"Tags":["hypothesis-testing"],"Answer_count":1,"Last_activity":"2016-08-21 10:18:19","Link":"http://stats.stackexchange.com/questions/230891/setting-up-hypothesis-testing-problems","Creator_reputation":null}
{"_id":{"$oid":"5837a578a05283111e4d3e7f"},"View_count":16,"Display_name":"BigData","Question_score":0,"Question_content":"A bag contains n differently coloured balls, If three balls are successively drawn at random, with replacement, what is the probability that at least two balls of the same colour are drawn?This is a question in my note and the answer is provided. My question is:the answer said the sample is ORDERED and I dont understand why.My thought is that if I draw three balls, for example:red, red, blueblue, red, redred, blue, redThey are all the same, because I just need to know what colour and how many of each colour were drawn. In this case, 2 red and 1 blue, and the order does not matter.Anyone knows why this should be an ordered sample instead of unordered?","Creater_id":125543,"Start_date":"2016-08-21 10:13:16","Question_id":230984,"Tags":["probability"],"Answer_count":0,"Last_activity":"2016-08-21 10:13:16","Link":"http://stats.stackexchange.com/questions/230984/counting-rules-ordered-and-unordered-sample","Creator_reputation":17}
{"_id":{"$oid":"5837a578a05283111e4d3e81"},"View_count":43,"Display_name":"SJ.STEVEN","Question_score":4,"Question_content":"Recently, I’m working on the multivariate conditional estimation issue. Considering  variables:\\{X_{1},X_{2},\\dots,X_{n},Y_{1},Y_{2},\\dots,Y_{n}\\}where each follows an empirical distribution through fitting the corresponding dataset. Thus, we can obtain the joint distribution FJ using Copula. If givenX_{1}=x_{1},X_{2}=x_{2},\\dots X_{n}=x_{n}we can also derive the conditional joint distributionF\\{Y_{1},Y_{2},\\dots,Y_{n}|X_{1}=x_{1},X_{2}=x_{2},\\dots,X_{n}=x_{n}\\}for . Now, I want to generate random vectors for  based on . How can I achieve this? This problem has perplexed me for a long while.","Creater_id":128435,"Start_date":"2016-08-21 05:43:34","Question_id":230964,"Tags":["sampling","multivariate-analysis","random-generation","copula","empirical"],"Answer_count":0,"Last_activity":"2016-08-21 09:36:07","Link":"http://stats.stackexchange.com/questions/230964/how-to-sample-from-a-multivariate-empirical-distribution","Creator_reputation":21}
{"_id":{"$oid":"5837a578a05283111e4d3e83"},"View_count":106,"Display_name":"Piotr Falkowski","Question_score":3,"Question_content":"I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:ID - participant ID Trial - 60 for each participantMemory - between subject binary factorState - within subject binary factor  Correct - whether classification a participant made was correct or notRating - the judgement made after each trial on four point Likert scaleProcedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial,               control = glmerControl(optimizer=\"bobyqa\", optCtrl = list(maxfun=100000)))Is this approach correct? I'll appreciate your input.Relevant resources I used: Formulae in R: ANOVA and other models, mixed and fixedThe Difference Between Crossed and Nested Factors When is it ok to remove the intercept in a linear regression model?Nested random factor with confounding (random?) variable","Creater_id":96761,"Start_date":"2016-08-20 15:35:28","Question_id":230911,"Tags":["r","mixed-model","generalized-linear-model","glmm","glmer"],"Answer_count":1,"Last_activity":"2016-08-21 09:05:00","Link":"http://stats.stackexchange.com/questions/230911/glmm-between-within-and-nested","Creator_reputation":153}
{"_id":{"$oid":"5837a578a05283111e4d3e90"},"View_count":58,"Display_name":"anup","Question_score":2,"Question_content":"There are many algorithms for learning mixture of Gaussians but typically k-means/EM is used in practice. My question is related to the performance of k-means/EM for MoG.Recently, I came across this paper \"Statistical Guarantees for EM algorithm\" (https://arxiv.org/abs/1408.2156) which gives some guarantees for learning mixture of Gaussians using EM. The main idea as I understood is if initialized close to the optimal solution, EM converges to the optimal solution.My question is what other theoretical guarantees are known about the performance of k-means/EM for learning mixtures of Gaussians. Particularly, using k-means, assume it is initialized with one random center from each cluster. Is it guaranteed to converge to optimal solution? Any other reference is highly appreciated.","Creater_id":128448,"Start_date":"2016-08-21 09:00:37","Question_id":230981,"Tags":["k-means","expectation-maximization","gaussian-mixture"],"Answer_count":0,"Last_activity":"2016-08-21 09:00:37","Link":"http://stats.stackexchange.com/questions/230981/convergence-of-k-means-or-em-on-mixture-of-gaussians","Creator_reputation":133}
{"_id":{"$oid":"5837a578a05283111e4d3e92"},"View_count":32,"Display_name":"Viola Green","Question_score":2,"Question_content":"This is the model: lme(score ~ 0 + rule, random=~1|subject, data=mydata)My response variable is called score, my explanatory variable is called rule. The same subjects were tested on all the rules, so  random=~1|subject accounts for the subject effect. Can someone explain to me: what is 0 in this model and why it is needed there? Note: this code was advised to me, I did not come up with it myself (I am still a beginner).","Creater_id":123259,"Start_date":"2016-08-20 18:14:49","Question_id":230919,"Tags":["lme"],"Answer_count":1,"Last_activity":"2016-08-21 08:54:55","Link":"http://stats.stackexchange.com/questions/230919/lme-model-general-question-on-the-formula","Creator_reputation":20}
{"_id":{"$oid":"5837a578a05283111e4d3e9f"},"View_count":17,"Display_name":"Cyrine Ezzahra","Question_score":0,"Question_content":"I am studying an analytic use case concerning exceeding energy equipments detection. As a beginner in data analytics I am not sure how to begin to resolve this problem.The use case is resumed as below : There is a list of 5 equipments. For each equipment we measured  the activation time  for each timestamp.For example :                 8h-10h   10h-12h   12h-14h   14h-16h   16h-18h equipement 1     100      100       10        0         20 equipement 2     100      40        10        10        0 equipement 3     100      32        0         20        0 equipement 4     0        0         0         100      90 equipement 5     100      0         0         0         0And also, I recorded for each timestamp if there in an exceeding energy or not :          8h-10h       10h-12h     12h-14h          14h-16h      16h-18h         exceeding     exceeding   no exceeding     exceeding    no exceedingMy objective in this study is to detect which equipments  are responsible for exceeding energy.Can you propose me some idea to analyze and resolve this problem?","Creater_id":74552,"Start_date":"2016-08-21 07:57:11","Question_id":230974,"Tags":["data-mining"],"Answer_count":0,"Last_activity":"2016-08-21 08:04:19","Link":"http://stats.stackexchange.com/questions/230974/diagnosis-analytics","Creator_reputation":111}
{"_id":{"$oid":"5837a578a05283111e4d3ea1"},"View_count":29,"Display_name":"sophiaF","Question_score":0,"Question_content":"I need to see if the proportions in a rare disease  for four groups  of 500 patients differ,  each group of 500 receiving a different treatment (1-4). The observed frequencies are as follows:G1: 16, G2:12, G3: 2, G4: 4.  As far as I understand the chi-square test requires a count of at least 5 in each cell and I cannot group the patients G3-G4. Is there an alternative to chi-square in this case.As a further step ib case the null hypothesis is rejected how could I prove that treatment G4 has lower incidence than G1 and G2?Thanks in advance for answering.","Creater_id":128442,"Start_date":"2016-08-21 07:32:03","Question_id":230972,"Tags":["hypothesis-testing","contingency-tables","frequency"],"Answer_count":1,"Last_activity":"2016-08-21 07:59:27","Link":"http://stats.stackexchange.com/questions/230972/proportion-test-multiple","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3eae"},"View_count":48,"Display_name":"FoxRyerson","Question_score":1,"Question_content":"I'm working through the example code given by Matlab, but I can't seem to exactly reproduce the ROC curve that is plotted. I want to make sure I am understanding the thresholding concept properly. Could anyone help me to understand why the two figures plotted below are different?clear; clc; load fisheriris;pred = meas(51:end,1:2);resp = (1:100)'\u0026gt;50;  % Versicolor = 0, virginica = 1mdl = fitglm(pred,resp,'Distribution','binomial','Link','logit');scores = mdl.Fitted.Probability;[X,Y,T,AUC] = perfcurve(species(51:end,:),scores,'virginica');figure; plot(X,Y);xlabel('False positive rate'); ylabel('True positive rate');title('ROC , built-in');tpr = nan(length(T),1); fpr = nan(length(T),1);for ind_F = 1:1:length(T)  t_true = scores \u0026gt;= T(ind_F);  group = resp; grouphat = t_true;  t_cm = confusionmat(group,grouphat);  % ROC : TPR / FPR  tpr(ind_F) = t_cm(1,1)/sum(t_cm(1,:));  fpr(ind_F) = t_cm(2,1)/sum(t_cm(2,:));endfigure; plot(fpr,tpr); xlabel('fpr'); ylabel('tpr');title('ROC , derived');Thanks for the help.","Creater_id":128379,"Start_date":"2016-08-20 08:56:57","Question_id":230865,"Tags":["regression","matlab","roc"],"Answer_count":1,"Last_activity":"2016-08-21 06:59:05","Link":"http://stats.stackexchange.com/questions/230865/matlab-roc-curve-calculation-question","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3eba"},"View_count":40,"Display_name":"Ray ben ","Question_score":0,"Question_content":"Dataset for User-Page view, which represent in matrix, rows are Users (2000+ or more) and columns ( Pages may be 100+),entries represent frequency(number of time user visit that page), I would like to know what type of MDS algorithm should i used ( classical, metric, non-metric etc..)","Creater_id":105797,"Start_date":"2016-08-21 05:36:39","Question_id":230963,"Tags":["algorithms","dimensionality-reduction","representative"],"Answer_count":1,"Last_activity":"2016-08-21 06:50:44","Link":"http://stats.stackexchange.com/questions/230963/what-type-of-mds-multidimensional-scaling-algorithm-should-i-used","Creator_reputation":9}
{"_id":{"$oid":"5837a578a05283111e4d3ec7"},"View_count":95,"Display_name":"Vaidy","Question_score":6,"Question_content":"I am a stats \u0026amp; R beginner and am trying to understand GLMs. I have a very basic question on the link function which is the followingIf I understand correctly the mean of the response variable Y is getting mapped to eta through the link function g(.)My question is, why the mean? Is it that we are calculating a single value, namely the mean of Yi's? Why not use the Yi's themselves?A related question to the above is, that if we consider linear regression (which is a special case of GLM with link=identity), the response variable is NOT mean(Yi), but Yi. But according to GLM theory we should be using mean(yi) for the link function mapping.Sorry if the question is very basic, and thanks in advance.I  have gone through many enlightening posts such as Meaning of link functions (GLM)Difference between logit and probit modelsbut I couldn't find the answer, no doubt my limitation.","Creater_id":128439,"Start_date":"2016-08-21 06:29:44","Question_id":230967,"Tags":["generalized-linear-model","link-function"],"Answer_count":1,"Last_activity":"2016-08-21 06:49:27","Link":"http://stats.stackexchange.com/questions/230967/basic-question-on-link-function-in-glm","Creator_reputation":31}
{"_id":{"$oid":"5837a578a05283111e4d3ed4"},"View_count":3213,"Display_name":"SAMA","Question_score":0,"Question_content":"I need to use the survdiff function to statistically compare (using log-rank test) the following survival functions:(1) Male (Sex=1) and Female (Sex=2)(2) Patients \u0026lt;= 65 years-old and Patients \u003e 65 years-old  I used the following command Male \u0026lt;- survdiff(Surv(time,Status)~sex==1,data=myeloma)Female \u0026lt;- survdiff(Surv(time,Status)~sex==2,data=myeloma)is that correct ?","Creater_id":55132,"Start_date":"2014-09-04 01:56:55","Question_id":114304,"Tags":["r","survival","logrank"],"Answer_count":2,"Last_activity":"2016-08-21 05:52:37","Link":"http://stats.stackexchange.com/questions/114304/log-rank-test-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3ee2"},"View_count":67,"Display_name":"Donbeo","Question_score":1,"Question_content":"I have a time series containing the daily close price for a stock and I would like to perform a 10 days forecast of the volatility. I'm trying to follow this tutorial: https://talksonmarkets.files.wordpress.com/2012/09/time-series-analysis-with-arima-e28093-arch013.pdfThis is my data and its autocorrelation:\u0026gt; closing_priceCloseDayPriceA)\u0026gt; aAutocorrelations of series ‘closing_priceCloseDayPriceA)or alternatively by fitting an AR(1) model and taking the residuals:arimaA = arima(closing_priceresiduals, trace = F)    acf(residuals(garchA))My questions are:Does the procedure that I am following make sense?How can I make a 10 days forecast and compute the 10 days volatility?EDIT:This should be the code:A = closing_price$CloseDayPriceAA.log = log(A)log_rtn = diff(A.log)A.garch = garchFit(formula = ~garch(1, 1), data = log_rtn, trace = F)A.est = predict(A.garch, 30, plot=T)","Creater_id":25392,"Start_date":"2016-08-21 03:06:05","Question_id":230955,"Tags":["time-series","forecasting","garch","finance","volatility-forecasting"],"Answer_count":1,"Last_activity":"2016-08-21 04:01:27","Link":"http://stats.stackexchange.com/questions/230955/forecasting-with-arima-and-garch-does-my-plan-look-alright","Creator_reputation":968}
{"_id":{"$oid":"5837a578a05283111e4d3eef"},"View_count":59,"Display_name":"Hercules Apergis","Question_score":1,"Question_content":"This generalized variance is a term that I have trouble understanding, simply because I can't find much on it on the internet,.i.e. which is the formula and some of its advantages and/or disadvantages? I came across this question while reading multivariate statistics and then in PCA, where I have stumbled upon the following two problems.Suppose that two variables have total variance equal to 10. And generalized variance equal to 9. Both variables have equal variances. Find the correlation of the two variables.Suppose that a Princepal Component(PC) explains the 90% of the total variance. One variable has variance equal to 10. Find the correlation between that variable and the PC.","Creater_id":128427,"Start_date":"2016-08-21 02:38:11","Question_id":230951,"Tags":["variance","pca"],"Answer_count":0,"Last_activity":"2016-08-21 03:53:29","Link":"http://stats.stackexchange.com/questions/230951/generalized-variance-in-pca","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3ef1"},"View_count":19890,"Display_name":"Joseph Weissman","Question_score":38,"Question_content":"I'm sorry if this seems a bit too basic, but I guess I'm just looking to confirm understanding here. I get the sense I'd have to do this in two steps, and I've started trying to grok correlation matrices, but it's just starting to seem really involved. I'm looking for a concise explanation (ideally with hints towards a pseudocode solution) of a good, ideally quick way to generate correlated random numbers. Given two pseudorandom variables height and weight with known means and variances, and a given correlation, I think I'm basically trying to understand what this second step should look like:   height = gaussianPdf(height.mean, height.variance)   weight = gaussianPdf(correlated_mean(height.mean, correlation_coefficient),                         correlated_variance(height.variance,                         correlation_coefficient))How do I calculate the correlated mean and variance? But I want to confirm that's really the relevant problem here. Do I need to resort to matrix manipulation? Or do I have something else very wrong in my basic approach to this problem?","Creater_id":14728,"Start_date":"2012-10-07 12:45:21","Question_id":38856,"Tags":["probability","correlation","conditional-probability","random-generation"],"Answer_count":3,"Last_activity":"2016-08-21 03:33:48","Link":"http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of","Creator_reputation":293}
{"_id":{"$oid":"5837a578a05283111e4d3f00"},"View_count":10,"Display_name":"chainhomelow33","Question_score":1,"Question_content":"I have two kind of disparate questions. I am attempting to replicate the following paper: http://www.sciencedirect.com/science/article/pii/S1053811916300428I am using SPM to perform preprocessing and first-level GLM analyses on fear conditioning data while using the GIFT toolbox to perform ICA on this data. Experiment background: We had 3 groups that all underwent fear conditioning in an MRI where they saw 2 different shapes and got shocked.Group 1: Delay - Saw shapes for 8s. One shape always coterminated with a shock (CS+) while the other never did (CS-)Group 2: Trace - Saw shapes for 4s, then a black screen (Trace Interval; TI) for 4s, at the end of which they got shocked during one shape (CS+) but not the other (CS-).Group 3: Unpaired - Control group. Saw 2 shapes for 8s but shocks were supplied during the intertrial intervals that followed and followed each shape an equal number of times (e.g. CS1 \u0026amp; CS2). As a beginner the statistics part of this is confusing me conceptually and technically. After I've done all the preprocessing I fit a GLM to the data. For all groups I'm going to have the first same 2 task-based regressors (e.g. the CS+ and CS- since I'm just going to randomly assign the Unpaired group's CS1 to CS+ and CS2 to CS- as they are counterbalanced) but the durations of the stimuli will be different, yes? (i.e. 8 for both CSs in the Delay and Unpaired groups and 4 for the CSs in the Trace group). And since I am also interested in activity during the TIs in the trace group I would have 2 FURTHER task-based regressors - 4s in duration - in the Trace participants' GLM which will NOT appear in the other groups' GLMs?My second question here has to do with regressors of no interest. In addition to motion parameters I want to include shock as a regressor of no interest but should I do there - where the shock is NOT convolved with any basis function (i.e. the value of 1 when shock is administered and a value of 0 everywhere else) or should I include it as a convolved regressor? I'm asking because, while the shock is a discrete stimulus I would have thought there would be a fairly typical response similar to those seen to the CSs, but I'm not interested in response to shock as part of my experimental question so I don't know where to put it.At this point I have also taken the preprocessed data and run it through ICA using the GIFT software. I have gotten out ~20 pretty good looking ICs. At this point the paper says they say the following:  \"To identify task-relatedness of ICs, a general linear model (GLM) was  fitted to each IC's time course. First, subject-specific regressors for each  combination of stimulus and context were created for each of four imaging  runs in SPM8 using convolution of a canonical hemodynamic response  function with the stimulus onsets. Then, the beta-estimates of  each regressor in the GLM that best predicted the back-reconstructed  IC time course were estimated. Finally, a 2 × 2 × 2 ANOVA on beta estimates  with the factors stimulus (CS+, CS−), context (acquisition,  extinction), and time (initial or repeated presentation) was used to  identify significant differences in functional connectivity between CS+  and CS− presentations for each context.\"I think I get this up to a point. So the line that starts with, \"...subject-specific regressors...\" means exactly what I already did but creating the GLM and including the appropriate onsets and conditions for each subjects, yes? The next line is what I don't understand. I understand that when you run the GLM you get beta estimates for each condition (e.g. one for the CS+ and one for the CS- for everyone but then also one for each TI for the Trace group) and the beta estimates are just numbers representing how well that ONE variable timecourse (e.g. the CS+ timecourse not taking into account the other variables?) fits the data, correct? But what does it mean to estimate the regressor that best predicted the IC timecourse? I think I get how to do the ANOVA so it's just this one line that's confusing me at this point.","Creater_id":83519,"Start_date":"2016-08-18 10:54:25","Question_id":230950,"Tags":["matlab","generalized-linear-model","neuroscience"],"Answer_count":0,"Last_activity":"2016-08-21 02:37:35","Link":"http://stats.stackexchange.com/questions/230950/neuroimaging-in-spm-gift-regressor-convolution-and-fitting-a-glm-to-ica-data","Creator_reputation":18}
{"_id":{"$oid":"5837a578a05283111e4d3f02"},"View_count":41,"Display_name":"thethakuri","Question_score":0,"Question_content":"I know that Principal Component Analysis (PCA) is the eigenvector of the covariance matrix. It is used as a tool for dimensional reduction. What I am confused about is whether the PCA give weights to original features in order to find out which features explain the data the most or does it come up with new set of abstract features that explain the greatest variance in the data set.","Creater_id":85663,"Start_date":"2016-08-19 02:22:31","Question_id":230658,"Tags":["pca"],"Answer_count":0,"Last_activity":"2016-08-21 02:17:23","Link":"http://stats.stackexchange.com/questions/230658/does-pca-create-new-features-or-give-weights-to-old-ones","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3f04"},"View_count":66,"Display_name":"user128418","Question_score":0,"Question_content":"I am looking for statistical methods used to compare frequency of observations between two groups. suppose I have two groups of pigeons with the same sample sizes, say 10. The number of  red blood cells of any pigeon are counted. The observations are as follows:Group 1: 66, 58, 64,48,58,65,52,57,54,58Group 2: 66, 59, 64,58,62,59,57,66,54,58.Can you suggest a statistical test to compare frequency of red blood cells in two groups? How to compare two samples of frequencies ? - ResearchGate. Available from: https://www.researchgate.net/post/How_to_compare_two_samples_of_frequencies [accessed Aug 21, 2016].","Creater_id":128418,"Start_date":"2016-08-20 23:43:19","Question_id":230939,"Tags":["spss","computational-statistics","frequency"],"Answer_count":1,"Last_activity":"2016-08-21 01:00:25","Link":"http://stats.stackexchange.com/questions/230939/how-to-compare-two-samples-of-frequencies","Creator_reputation":1}
{"_id":{"$oid":"5837a578a05283111e4d3f11"},"View_count":64,"Display_name":"Nicolas Bourbaki","Question_score":5,"Question_content":"Let  be a probability space and  a -algebra. I have seen it referred to many times that  is the \"information\" which is available to us. I think I kinda understand it, but I am not satisfied in my own understanding of it. Let us say that  is an event and . Let  be a sample point of the experiment. We do not know which of the events in  contain . Now  certaintly contains , but we already knew that, so we do not gain any insight. However, either  or  will contain . If we somehow knew that  contains , then that gives us additional insight. 1) Does anyone have a better way of thinking of  as our \"information\"? 2) If , then how do we think of  as having \"more information\"? Obviously, it is a larger -algebra, and it has more events, but ignoring set theory, what should one's intuition be for ? Follow up question. 3) Let  be a -measurable. I have seen people refer to  as \"a random variable whose information is known from \", or something along those lines. What is the motivation for this?","Creater_id":68480,"Start_date":"2016-08-20 22:41:08","Question_id":230934,"Tags":["probability"],"Answer_count":1,"Last_activity":"2016-08-20 23:54:20","Link":"http://stats.stackexchange.com/questions/230934/visualizing-of-sigma-algebras-as-information","Creator_reputation":528}
{"_id":{"$oid":"5837a578a05283111e4d3f1e"},"View_count":51,"Display_name":"Kodiologist","Question_score":4,"Question_content":"A month ago, I answered a question that asked, in a nutshell, whether for a given finite set of dependent Bernoulli-distributed random variables, you could construct a set of independent Bernoulli random variables that carried the same information. I formalized the idea like this:  Let  be a probability space. If  are Bernoulli random variables on , then there exist independent Bernoulli random variables  on  and a function  such that for all , .(Intuitively, the conjecture says that the values of the independent s suffice to determine the values of the not necessarily independent s. Notice that  need not equal , and indeed may need to be much larger.)This statement can be readily disproved by choosing a small finite probability space  that has no pair of independent Bernoulli random variables. But suppose we require  to be [0, 1] with the Borel σ-algebra and Lebesgue measure. Now we have a roomier and more familiar underlying probability space. Is the conjecture true now?","Creater_id":14076,"Start_date":"2016-08-20 21:17:25","Question_id":230930,"Tags":["mathematical-statistics","independence","bernoulli-distribution"],"Answer_count":1,"Last_activity":"2016-08-20 23:53:44","Link":"http://stats.stackexchange.com/questions/230930/decomposing-dependent-bernoulli-random-variables-into-independent-bernoulli-rand","Creator_reputation":8337}
{"_id":{"$oid":"5837a578a05283111e4d3f2b"},"View_count":32,"Display_name":"knzhou","Question_score":0,"Question_content":"Suppose I want to define some probability distribution in a paper, and exhibit its PDF. It seems the usual way to do this requires two separate sentences, one to define the distribution and one to define its PDF. For example, one would write:  We will show that this quantity is blah distributed. We say a random variable  is blah distributed if . Then the probability density function of such a random variable  is  f(x; \\alpha, \\beta) = \\beta I \\alpha h \\times \\ldots.This notation is unsuitable to me for two reasons.It's clunky. I only ever need the PDF, so I want a notation that avoids taking an extra step to initialize an object that won't ever be used. (I'm working in a non-stats, non-math field where everybody knows what a PDF is, but nobody ever uses the word 'random variable'. The general reaction to the above phrasing would be that I'm just being pedantic for no reason.) It's missing information. Somebody just skimming through, looking at just the equations, has no idea what  is. The notation has no way to indicate which PDF it is.I tried to fix these two issues by combining these into a single line:  We will show that this quantity is blah distributed, i.e. its probability density function is  \\text{Blah}(\\alpha, \\beta)(x) = \\beta I \\alpha h \\times \\ldots.But this feels weird. Is this okay? Is there a better way to do this?","Creater_id":79510,"Start_date":"2016-08-20 20:13:42","Question_id":230923,"Tags":["notation"],"Answer_count":1,"Last_activity":"2016-08-20 22:23:14","Link":"http://stats.stackexchange.com/questions/230923/one-line-notation-for-defining-a-pdf-of-a-distribution","Creator_reputation":103}
{"_id":{"$oid":"5837a578a05283111e4d3f38"},"View_count":42,"Display_name":"Science11","Question_score":1,"Question_content":"I have a bunch of numbersprior = c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0,0)Is there a simple function that calculates the C.I for these numbers ?? I am trying to avoid these 15 lines of computation using mean, stdev, stderr etc below  for C.Imean_prior = mean(prior)stdv_prior = sd(prior)t_critical = qt(0.975, df=(length(prior)-1)) margin_of_error \u0026lt;- t_critical * (stdv_prior / sqrt((length(prior)-1))) # Calculate margin of errorconfidence_interval  \u0026lt;- c(mean_prior - margin_of_error,  # Calculate the the interval                          mean_prior + margin_of_error)  print(\"Confidence interval:\")round(confidence_interval,4)thats all.Thanks in advance.","Creater_id":64064,"Start_date":"2016-08-20 20:51:53","Question_id":230927,"Tags":["r","descriptive-statistics","summary-statistics"],"Answer_count":0,"Last_activity":"2016-08-20 21:25:01","Link":"http://stats.stackexchange.com/questions/230927/r-function-to-calculate-confidence-interval-of-random-numbers","Creator_reputation":28}
{"_id":{"$oid":"5837a578a05283111e4d3f3a"},"View_count":34,"Display_name":"haff","Question_score":1,"Question_content":"I am interested in testing for significant differences with a Likert scale question between three groups. There are plenty of questions about comparing medians here, but I am interested in differences in both medians and dispersion - basically any difference in the distributions. For example:Research question: Are there significant differences between groups in the level of agreement with the following statement: \"I enjoy playing video games\"?Groups: Black, Hispanic, WhitePossible responses: Strongly disagree, disagree, neutral, agree, strongly agreeMy initial thought was to use the Kruskal-Wallis test first, but  this site states that the test assumes that the groups have the same shape and variability (which would also mean that Kruskal-Wallis isn't capturing differences in dispersion? Only medians?). Upon reading this, I thought I ought to test for dispersion first using the Brown-Forsythe test. My questions are thus: (1) Is my proceeding appropriate? Should the Brown-Forsythe test (or some reasonable alternative) always be used prior to Kruskal-Wallis (when using ordinal variables) to ensure equal dispersion?(2) If I only had two groups, Mann-Whitney would be more appropriate from my understanding. This site also says that an appropriate alternative hypothesis for the Mann-Whitney test is that 'the distributions between groups are different.' Given this, would I still need to test for dispersion differences between groups first? If the Kruskal-Wallis test is an extension of Mann-Whitney for more than two groups, can the assumptions be different?(3) Is there a different test that I should be using? I know that there are some tests designed specifically for comparing distributions, but these seem to have some shortcomings based on my needs. For example, Komolgorov-Smirnov requires continuous data, Chi-square doesn't take into account the ordinal nature of data (so it's not as powerful?), etc.Note(s):I know that there is considerable debate about whether Likert scale questions should be assumed interval or ordinal, but for this question can we just assume that they're ordinal? :)  ","Creater_id":126943,"Start_date":"2016-08-07 17:13:17","Question_id":228711,"Tags":["distributions","ordinal","likert","median","dispersion"],"Answer_count":1,"Last_activity":"2016-08-20 21:00:45","Link":"http://stats.stackexchange.com/questions/228711/testing-for-differences-in-distributions-medians-and-dispersion-between-groups","Creator_reputation":106}
{"_id":{"$oid":"5837a578a05283111e4d3f47"},"View_count":90,"Display_name":"Matthew Drury","Question_score":7,"Question_content":"I'm a working data scientist with solid experience in regression, other machine learning type algorithms, and programming (both for data analysis and general software development).  Most of my working life has been focused on building models for predictive accuracy (working under various business constraints), and building data pipelines to support my own (and other's) work.I have no formal training in statistics, my university education focused on pure mathematics.  As such have missed out on learning many of the classical topics, especially the various popular hypothesis tests and inferential techniques.Are there any references for these topics that would be appropriate for someone with my background and level of experience?  I can handle (and appreciate) mathematical rigour, and also enjoy algorithmic perspectives.  I tend to like references that offer the reader guided exercises, with both (or either) a mathematical and (or) programming focus.","Creater_id":74500,"Start_date":"2016-08-20 14:25:24","Question_id":230904,"Tags":["hypothesis-testing","references","inference","frequentist","inferential-statistics"],"Answer_count":1,"Last_activity":"2016-08-20 21:00:17","Link":"http://stats.stackexchange.com/questions/230904/reference-request-classical-statistics-for-working-data-scientists","Creator_reputation":12722}
{"_id":{"$oid":"5837a578a05283111e4d3f54"},"View_count":53,"Display_name":"mohsen hs","Question_score":1,"Question_content":"I am working on a computer project which needs statistical analysis and I am not much of a statistic person. I have a Bluetooth (BT) detector device which detects passing Bluetooth devices  (i.e one BT passed at time 0, one BT passed at time 3,....) and I used AIC to select the best distribution which describes the interarrival times. Now, I would like to do a Monte Carlo simulation to see what is the effect of detectable BT ratio (shown by lambda below and increased by 0.05 at each step) on the outcome.I have two options:  D = distribution with lowest AIC which describes interarrival times theta = paramters of D which obtained by mlez = number of detected BT  for (lambda in 0.05,0.1,0.15,..., 1){    P=generate z*lambda random numbers with D and theta    (like Lnorm(z*lambda,meanlog=10,sdlog=2) in r)    Find the best distribution and its paramters which fits P for further processing}this second approach is suggested by a statistician fiend:Assume that you have fitted a log-Normal distribution to the BT data, and found parameters  and  for the BT interarrival times: \\newcommand{\\Var}{{\\rm Var}}\\newcommand{\\LogNormal}{{\\rm LogNormal}}\\newcommand{\\shape}{{\\rm shape}}\\newcommand{\\scale}{{\\rm scale}}Y \\sim \\LogNormal(\\mu, \\sigma^2)with mean and variance of the BT interarrival times are\\begin{align} E[Y]   \u0026amp;= \\exp(\\mu + 0.5*\\sigma^2)  \\\\   \\Var[Y] \u0026amp;= (\\exp(\\sigma^2)-1) \\exp(2*\\mu + \\sigma^2)\\end{align}However you know that only a proportion lambda of devices are actually detected.  That means that the true BT interarrival times are shorter than this.  If X is the true interarrival time random variable then X is approximately  on average, so you expectandi.e., andwhere .This suggests thatNow, I have three questions and appreciate any help.I cannot understand how he concluded the last line (i.e )  (SOLVED)How would the second approach work for the Weibull distribution, I know that:  \\begin{align}E[\\ln(W)]   \u0026amp;= \\ln(\\scale)- (\\gamma /\\shape),  \\\\\\Var[\\ln(W)] \u0026amp;= (\\pi^2)/(6 * \\shape^2),   \\end{align}where  is the Euler constant, but I do not know how to extend this and find the effect of lambda on scale and shape in a similar way to the second approach. (SOLVED)Why did my friend suggest the second approach, and what is wrong with the first approach?","Creater_id":128404,"Start_date":"2016-08-20 17:42:28","Question_id":230918,"Tags":["monte-carlo","fitting","lognormal","weibull"],"Answer_count":0,"Last_activity":"2016-08-20 20:18:07","Link":"http://stats.stackexchange.com/questions/230918/monte-carlo-simulation-for-fitting-distributions-weibull-and-log-normal","Creator_reputation":13}
{"_id":{"$oid":"5837a578a05283111e4d3f56"},"View_count":72,"Display_name":"Shieryn","Question_score":0,"Question_content":"I want to test stationarity of variants using Box-Cox Lambda for Time series assumption (Stationarity). Null-Hypothesis : Lambda is not zero (Data is stationarity of variants)Alternative-Hypothesis : Lambda is zero (Data is not stationarity of variants)But I got confused to obtain that lambda. Do you know the formula to obtain that lambda?Thankyou","Creater_id":92551,"Start_date":"2016-08-20 10:41:16","Question_id":230883,"Tags":["time-series","data-transformation","assumptions","stationarity"],"Answer_count":1,"Last_activity":"2016-08-20 19:48:43","Link":"http://stats.stackexchange.com/questions/230883/how-to-calculate-lambda-of-box-cox-paramater-to-test-stationarity-of-variants-in","Creator_reputation":15}
{"_id":{"$oid":"5837a578a05283111e4d3f63"},"View_count":28,"Display_name":"Marwah Soliman","Question_score":1,"Question_content":"I want to know how to extract the variables name in the glmnet path for LASSOso the output is as follows:fit.b=glmnet(xx,y,family=\"binomial\",alpha=1)     Df       %Dev    Lambda [1,]  0 -5.679e-15 0.1775000 [2,]  1  1.839e-02 0.1617000 [3,]  1  3.385e-02 0.1473000 [4,]  1  4.689e-02 0.1342000 [5,]  1  5.790e-02 0.1223000 [6,]  3  6.967e-02 0.1114000 [7,]  3  8.489e-02 0.1015000 [8,]  4  1.000e-01 0.0925300 [9,]  4  1.130e-01 0.0843100[10,]  4  1.241e-01 0.0768200[11,]  4  1.335e-01 0.0699900[12,]  4  1.414e-01 0.0637800[13,]  4  1.482e-01 0.0581100[14,]  5  1.556e-01 0.0529500[15,]  5  1.625e-01 0.0482400[16,]  5  1.684e-01 0.0439600[17,]  5  1.735e-01 0.0400500[18,]  5  1.778e-01 0.0364900[19,]  5  1.815e-01 0.0332500[20,]  6  1.847e-01 0.0303000[21,]  6  1.877e-01 0.0276100[22,]  6  1.902e-01 0.0251500[23,]  6  1.923e-01 0.0229200[24,]  6  1.941e-01 0.0208800[25,]  6  1.956e-01 0.0190300[26,]  6  1.969e-01 0.0173400so I want to know the variable name in each step in the path (i.e in the df)Thank you, so 0 represent the intercept , then 1 represent which variable??","Creater_id":117854,"Start_date":"2016-08-20 19:22:42","Question_id":230922,"Tags":["r"],"Answer_count":0,"Last_activity":"2016-08-20 19:22:42","Link":"http://stats.stackexchange.com/questions/230922/how-to-extract-the-variables-name-from-the-path-in-glmnet","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3f65"},"View_count":20,"Display_name":"Zainab","Question_score":1,"Question_content":"How could I attach points to the line segment pattern using R language Functionfor example I have point pattern on linear networkand I have marked line segment pattern I try to attach points in point pattern to marked line segment pattern.","Creater_id":108142,"Start_date":"2016-08-20 17:30:29","Question_id":230916,"Tags":["machine-learning"],"Answer_count":0,"Last_activity":"2016-08-20 17:30:29","Link":"http://stats.stackexchange.com/questions/230916/point-patterns-and-line-segment-patterns","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3f67"},"View_count":16,"Display_name":"stars83clouds","Question_score":1,"Question_content":"Can you use the root mean squared error formula to compare the quality of fit between simulated data and a model?","Creater_id":35645,"Start_date":"2016-08-20 16:35:38","Question_id":230914,"Tags":["rms"],"Answer_count":0,"Last_activity":"2016-08-20 16:35:38","Link":"http://stats.stackexchange.com/questions/230914/fitting-simulated-data-using-rmse","Creator_reputation":116}
{"_id":{"$oid":"5837a578a05283111e4d3f69"},"View_count":28,"Display_name":"Ian","Question_score":1,"Question_content":"I'm using Frank Harrell's rms package to model a dependent variable that is not normally distributed. I'm using a Box-Cox transformation to correct for this nonnormality. I understand from Frank Harrell's comment here, that I should be penalizing my model for uncertainty in the estimated lambda parameter. Any idea how to do this using the rms package?","Creater_id":128402,"Start_date":"2016-08-20 15:03:11","Question_id":230909,"Tags":["data-transformation","regression-strategies"],"Answer_count":0,"Last_activity":"2016-08-20 15:46:06","Link":"http://stats.stackexchange.com/questions/230909/using-box-cox-transformations-in-ols-modelling","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3f6b"},"View_count":33,"Display_name":"DataGuy","Question_score":2,"Question_content":"I need some advice when interpreting the output of the M5P model as shown below; I have some questions:How is the 0.5 that determines the construction of each LM derived?What do the numbers in the parentheses refer to?What does 115% in LM8 mean? How can it be over 100%?region=1,3,4 \u0026lt;= 0.5 : LM1 (20000/19.779%)region=1,3,4 \u0026gt;  0.5 : |   in-store=0 \u0026lt;= 0.5 : |   |   region=4 \u0026lt;= 0.5 : |   |   |   region=3,4 \u0026lt;= 0.5 : |   |   |   |   age \u0026lt;= 55.5 : |   |   |   |   |   age \u0026lt;= 31.5 : |   |   |   |   |   |   age \u0026lt;= 24.5 : LM2 (1107/77.967%)|   |   |   |   |   |   age \u0026gt;  24.5 : LM3 (2649/71.098%)|   |   |   |   |   age \u0026gt;  31.5 : LM4 (8937/70.946%)|   |   |   |   age \u0026gt;  55.5 : LM5 (3307/37.787%)|   |   |   region=3,4 \u0026gt;  0.5 : LM6 (10999/37.952%)|   |   region=4 \u0026gt;  0.5 : LM7 (13001/77.69%)|   in-store=0 \u0026gt;  0.5 : LM8 (20000/115.636%)I read in Wang \u0026amp; Whitten that the splits are supposed to be created by taking the SD(node.examples) \u0026lt;0.05*SD, but that doesnt match the output above.Wang, Y., \u0026amp; Witten, I. H. (1996). Induction of model trees for predicting continuous classes.","Creater_id":77400,"Start_date":"2016-08-07 19:15:29","Question_id":228724,"Tags":["regression","classification","cart"],"Answer_count":1,"Last_activity":"2016-08-20 15:43:36","Link":"http://stats.stackexchange.com/questions/228724/m5p-interpretations-and-questions","Creator_reputation":111}
{"_id":{"$oid":"5837a578a05283111e4d3f77"},"View_count":63,"Display_name":"LDBerriz","Question_score":4,"Question_content":"I am trying to explain in simple words the Causal Markov condition to establish probabilistic causation.  The original definition  (Hausman and Woodward 1999) is the following:  “Let G be a causal graph with vertex set V and P be a probability distribution   over the vertices in V generated by the causal structure represented by G. G   and P satisfy the Causal Markov Condition if and only if for every X in V, Y   is independent of V\\(Descendants(X) ∪ Parents(X)) given Parents(X)”  My explanation is that a Causal Markov condition is satisfied if the set of  variables in a causal relationship with given probability distributions are independent of all the other variables unless they are their parents or their descendants.  This is slightly different than other definitions around so, is my explanation first, correct?, second clear?Any advice will be appreciated.","Creater_id":128189,"Start_date":"2016-08-20 13:02:06","Question_id":230897,"Tags":["conditional-probability","causality"],"Answer_count":1,"Last_activity":"2016-08-20 15:02:23","Link":"http://stats.stackexchange.com/questions/230897/causal-markov-condition-simple-explanation","Creator_reputation":120}
{"_id":{"$oid":"5837a578a05283111e4d3f84"},"View_count":74,"Display_name":"Pinocchio","Question_score":3,"Question_content":"Sorry if this is basic, but I was reviewing hypothesis testing after 6 years and came across khan academy's video explaining it. At some point at minute 4.03 the speaker says \"sampling distribution\", what does he mean rigorously with that? From the spoken video and what he write it seems that he mean the distribution of the sample mean. i.e. if we have i.i.d. random variable  and we define a new r.v.: M_m = \\frac{1}{m} \\sum^m_{i=1} X_i then it seems that he refers to the sampling distribution as the distribution of . It seems confusing because the sampling distribution for me in my head should be the distribution from where we obtain samples, i.e. the distribution of  which is of  course the population distribution. I know this is probably mostly a conceptual question but why does he refer to the sampling distribution as ? It seems weird because we obtain samples from the population not from the distribution according to . I think I am probably wrong so I wanted to understand why.  ","Creater_id":37632,"Start_date":"2016-08-19 16:22:49","Question_id":230799,"Tags":["distributions","sampling","definition"],"Answer_count":1,"Last_activity":"2016-08-20 14:53:08","Link":"http://stats.stackexchange.com/questions/230799/what-does-the-term-sampling-distribution-mean-conceptually-and-rigorously","Creator_reputation":775}
{"_id":{"$oid":"5837a578a05283111e4d3f91"},"View_count":140,"Display_name":"SKM","Question_score":0,"Question_content":"I am having a tough time in understanding how I can estimate the data in the lower dimensional space. I have understood the theory that is described in the paper Tipping and Bishop, Probabilistic Principal Component Analysis. I want to estimate the source S using Expectation Maximization. The model isX = A*Swhere X is the observed higher dimensional data that is assumed to be generated from a lower dimensional space S and A is the transformation matrix.But, according to the equations, I can only get an expected value of the source. Can somebody please inform how it is possible to estimate S so that the estimated value is close to the true?The implementation of the theory is presented in https://www.mathworks.com/matlabcentral/fileexchange/55883-probabilistic-pca-and-factor-analysis/content/ppca/demo.mI have modified the code and the modified version is below% demos%EM algorithm for fitting PCA and FA model. %This is probabilistic treatment of dimensional reduction.%https://www.mathworks.com/matlabcentral/fileexchange/55883-probabilistic-pca-and-factor-analysis/content/ppca/demo.mclear allclcn1 = 10; %d dimensionn2 = 100; % number of examplesthreshold = 0.5;ncomp = 2; % target reduced dimension%Generating data according to the model% X(i,j) = A(i,:)*S(:,j) + noiseAr = orth(randn(n1,ncomp))*diag(ncomp:-1:1);T = 1:n2; Data = [ exp(-T/150).*cos( 2*pi*T/50 )          exp(-T/150).*sin( 2*pi*T/50 ) ];S=Data\u0026gt;0;% Normalizing to zero mean and unit varianceS = ( S - repmat( mean(S,2), 1, n2 ) );S = S ./ repmat( sqrt( mean( S.^2, 2 ) ), 1, n2 );Xr = Ar * S;X = Xr;  %no noise  Xprobe = X ;%% EM probabilistic PCA[W,mu,beta,llh] = ppcaEm(X,ncomp);plot(llh)","Creater_id":21160,"Start_date":"2016-08-03 17:16:42","Question_id":227184,"Tags":["time-series","self-study","pca","matlab","dimensionality-reduction"],"Answer_count":0,"Last_activity":"2016-08-20 14:39:28","Link":"http://stats.stackexchange.com/questions/227184/probabilsitic-principal-component-analysis-how-to-apply-the-em-technique","Creator_reputation":99}
{"_id":{"$oid":"5837a578a05283111e4d3f93"},"View_count":239,"Display_name":"user3577904","Question_score":0,"Question_content":"I would like to compare predictive power of large number of models- +/- 50.I itended to use the ttrTests package and SPA, but I was not able to install that one since it was removed from CRAN.Please, does anyone have any suggestions, how to compare predictive power of large number of models in R?Thank you!","Creater_id":72555,"Start_date":"2015-03-17 10:37:22","Question_id":142318,"Tags":["r","prediction"],"Answer_count":1,"Last_activity":"2016-08-20 13:45:11","Link":"http://stats.stackexchange.com/questions/142318/r-comparing-predictive-power","Creator_reputation":1}
{"_id":{"$oid":"5837a578a05283111e4d3fa0"},"View_count":22,"Display_name":"Reza_Research","Question_score":0,"Question_content":"Imagine that I am given an HMM model called lamda=(A,B,pi).What does it mean to calculate log likelihood of model on test data?I want to interpret this:","Creater_id":126608,"Start_date":"2016-08-20 13:34:30","Question_id":230899,"Tags":["log-likelihood"],"Answer_count":0,"Last_activity":"2016-08-20 13:34:30","Link":"http://stats.stackexchange.com/questions/230899/log-likelihood-for-an-hmm","Creator_reputation":48}
{"_id":{"$oid":"5837a578a05283111e4d3fa2"},"View_count":56,"Display_name":"C.R. Peterson","Question_score":3,"Question_content":"I'm trying to implement the following model in Stan:\\begin{align} \\text{Pr}(y|n,\\theta) \u0026amp; \\sim \\text{Binomial}(n,\\theta)\\\\\\text{Pr}(n|\\lambda) \u0026amp; \\sim \\text{Poisson}(\\lambda)\\end{align}In this model,  and  are non-negative integers, , , and  .  Because Stan does not allow for discrete parameters, I need to marginalize over  (i.e., find the probability mass function of  given  and . The PMFs of  and  are as follows:\\begin{align} \\text{Pr}(y|n,p) = \u0026amp; \\frac{n!}{y!(n-y)!}p^{y}(1-p)^{n-y}\\\\\\text{Pr}(n|\\lambda) =\u0026amp; \\frac{\\lambda^{n}e^{-\\lambda}}{n!}\\end{align}My attempt to marginalize over  is as follows: \\begin{alignat}{4} \\text{Pr}(y|p,\\lambda) \u0026amp; = \u0009\\sum_{n=y}^{\\infty}\\text{Pr}(y|n,p)\\text{Pr}(n|\\lambda)\\\\\\\\\u0026amp; = \\sum_{n=y}^{\\infty}\\frac{\\lambda^{n}e^{-\\lambda}}{y!(n-y)!}p^{y}(1-p)^{n-y}\\\\\\\\\u0026amp; = \\frac{e^{-\\lambda p}p^{y}\\lambda^{y}}{\\Gamma(y+1)} \\end{alignat}   The final step was done by Wolfram Alpha, as my own mathematical skills are a bit rusty. Does this seem correct?To implement this, I first took the log PMF:\\text{log-Pr}(y|p,\\lambda)=-\\lambda p+y(\\text{log}p+\\text{log}\\lambda)-\\text{log}\\Gamma(y+1) The Stan code for the function is:  real binomial_poisson_lpmf(int[] y, vector p, vector lambda) { // vectorized;  assumes that y, p, and lambda are all of the same length.    vector[size(y)] out;    for(i in 1:size(y))      out[i] = -lambda[i] * p[i] + y[i] * (log(p[i]) + log(lambda[i])) - lgamma(y[i]+1));    return(sum(out));  }Anyway, I would appreciate any feedback on whether or not this seems valid; I would also like to know if anyone knows of any citations on the subject.Thank you for your time.","Creater_id":95049,"Start_date":"2016-08-19 14:29:44","Question_id":230789,"Tags":["probability","distributions","marginal","stan"],"Answer_count":1,"Last_activity":"2016-08-20 13:27:13","Link":"http://stats.stackexchange.com/questions/230789/marginalizing-a-poisson-distribued-count-parameter-in-a-binomial-distribution","Creator_reputation":463}
{"_id":{"$oid":"5837a578a05283111e4d3faf"},"View_count":100,"Display_name":"cww","Question_score":1,"Question_content":"I have the following model  fit1 \u0026lt;- glmer(Res~FA+FB+FC+(1|fsite), family=binomial(), data=DATA)the result of summary() is:  summary(fit1)Generalized linear mixed model fit by maximum likelihood  (Laplace Approximation) ['glmerMod'] Family: binomial  ( logit )Formula: Res ~ FA + FB + FC + (1 | fsite)   Data: DATA     AIC      BIC   logLik deviance df.resid    202.3    229.9    -92.1    184.3      150 Scaled residuals:     Min      1Q  Median      3Q     Max -2.1768 -0.6167 -0.4967  0.6815  2.0132 Random effects: Groups Name        Variance Std.Dev. fsite  (Intercept) 0        0       Number of obs: 159, groups:  fsite, 28Fixed effects:            Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)  1.55573    0.55830   2.787 0.005327 ** FA2         -0.11914    0.37344  -0.319 0.749692    FB2         -1.38652    0.39967  -3.469 0.000522 ***FC2         -0.14976    0.61984  -0.242 0.809076    FC3         -0.06794    0.63171  -0.108 0.914350    FC4         -1.20114    0.61670  -1.948 0.051452 .  FC5         -1.44951    0.62817  -2.308 0.021025 *  FC6         -1.13590    0.65427  -1.736 0.082538 .  ---Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1Correlation of Fixed Effects:        (Intr) fspcs2    FB2    FC2    FC3    FC4    FC5 FA2     -0.466                                          FB2     -0.456  0.169                                   FC2     -0.572 -0.021  0.017                            FC3     -0.596  0.050  0.036  0.506                     FC4     -0.582 -0.005  0.020  0.519  0.509              FC5     -0.558  0.019 -0.038  0.508  0.500  0.511       FC6     -0.391 -0.101 -0.288  0.485  0.467  0.486  0.492Why are the variance and Std.Dev of the random effects zero?How do I check for overdispersion in this model?What should do if there is overdispersion?","Creater_id":128277,"Start_date":"2016-08-19 07:44:19","Question_id":230721,"Tags":["r","mixed-model","binomial","lme4","overdispersion"],"Answer_count":1,"Last_activity":"2016-08-20 13:13:56","Link":"http://stats.stackexchange.com/questions/230721/how-to-check-overdispersion-of-binomial-glmms-lme4-package","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3fbc"},"View_count":30,"Display_name":"Luli","Question_score":0,"Question_content":"Hello everyone this is my first question here!I want to study the relationship between gestational age, birth weight, etc and results on different language tests in very preterm infants. In order to choose the right stats test I need your help in determining if the following data is considered discrete.In one of the tests the child is classified as having answered 0 out of 4 questions correctly, or 1 out of 4 questions correctly, or 2 out of 4 correctly, etc.0 correct answers1 correct answer2 correct answers 3 correct answers4 correct answersShould I consider it as discrete data?Thanks!!!","Creater_id":128073,"Start_date":"2016-08-17 15:45:23","Question_id":230391,"Tags":["classification","dataset","ratio","interval"],"Answer_count":1,"Last_activity":"2016-08-20 13:12:43","Link":"http://stats.stackexchange.com/questions/230391/what-type-of-data-is-the-number-of-correct-answers","Creator_reputation":6}
{"_id":{"$oid":"5837a578a05283111e4d3fc9"},"View_count":125,"Display_name":"ArnoXf","Question_score":1,"Question_content":"I'm working on a project where I want to detect classic soccer balls in live camera pictures using a Convolutional Neural Network. My Network is built up as follows:conv1: 11x11, 96, ReLU, max pooling, normalizationconv2: 5x5, 256, ReLU, max pooling, normalizationconv3: 3x3, 384, ReLUconv4: 3x3, 384, ReLUconv5: 3x3, 256, ReLU, max pooling, normalizationfc1: 3072, TanH, dropout 0.5fc2: 3072, TanH, dropout 0.5fc3: 2, softmaxmomentum, categorical crossentropy, learning rate 0.001I want to predict whether there is a soccer ball in a room or not, so I am using images like the following of about 200 in each class for training:Unfortunately, I cant get the model working. I tried learning rates like 0.001, 0.01-0.05 and 0.1, everything \u0026lt;= 0.03 results in too fast loss decrease (0.00001) and accuracy 100% but with wrong predictions afterwards. Training with \u003e0.03 always stays around loss 0.7 and acc 0.5.I also tried different batch sizes and converted the former color pictures to gray scale but nothing works. Shall I use other activation functions, a sigmoid last layer or are my convolutions wrong? Or is my hole 5-3-structure not practicable? ","Creater_id":115266,"Start_date":"2016-05-14 05:11:58","Question_id":212536,"Tags":["machine-learning","classification","deep-learning","conv-neural-network","tensorflow"],"Answer_count":0,"Last_activity":"2016-08-20 13:12:21","Link":"http://stats.stackexchange.com/questions/212536/binary-classification-with-cnn-for-soccer-ball-detection-doesnt-converge","Creator_reputation":11}
{"_id":{"$oid":"5837a578a05283111e4d3fcb"},"View_count":4200,"Display_name":"utdiscant","Question_score":1,"Question_content":"I am trying to do regression with a deep convolutional network. I am using the code from http://deeplearning.net/tutorial/lenet.htmlCurrently, the network is using logistic regression as its last layer which I want to remove. If I simply remove the layerlayer3 = LogisticRegression(input=layer2.output, n_in=500, n_out=10)from the network, I will need to specify a cost instead of:cost = layer3.negative_log_likelihood(y)How can I specify an appropriate cost for doing regression? Each of my outputs should be 0 to 1, but not necessarily sum to 1.","Creater_id":7339,"Start_date":"2014-01-23 04:18:18","Question_id":83107,"Tags":["neural-networks","python","deep-learning","conv-neural-network"],"Answer_count":3,"Last_activity":"2016-08-20 13:12:02","Link":"http://stats.stackexchange.com/questions/83107/regression-layer-in-convolutional-neural-network","Creator_reputation":570}
{"_id":{"$oid":"5837a578a05283111e4d3fda"},"View_count":142,"Display_name":"Simbi","Question_score":2,"Question_content":"I am currently experimenting with a Convolutional Neural Network, trying to get a good performance on the Cats \u0026amp; Dogs challenge at Kaggle.By now, the best result I could get using my network was 77% accuracy (after ~30 epochs the network starts to overfit heavily). I feel that it should be possible to get better results. (Reaching about 85-90% shouldn't be that big of a deal, should it?)Image preprocessing:Downsample all images: smaller edge = 96pxCut out the center part of the image, if it is not already quadratic.Network architecture:I did the most recent run with these parameters, although I achieve about the same performance with a network that is a lot smaller# HYPERPARAMETERSC1 = (50, 1, 9, 9) # 50 kernels of shape 9x9C2 = (80, 50, 5, 5) # 80 kernels of shape 5x5C3 = (100, 80, 3, 3) # 100 kernels of shape 5x5POOL1 = (2, 2) # 2 by 2 max poolingPOOL2 = (2, 2) # 2 by 2 max poolingPOOL3 = (2, 2) # 2 by 2 max poolingH1OUT = 900 #number of neurons in hidden layer 1H2OUT = 1200 #number of neurons in hidden layer 2For the Convolutional layers I am using tanh as activation function, the hidden layers operate using Rectified linear.Training:I'm training the network on 20000 images (validating the results on 2500)Batch Size: 128Regularization: L2 on all weights across the networkLearning Rate: Linearly decreasing from 0.02 to 0.001 (50 epochs)What do you guys think I should try next?Increasing the number of filters doesn't seem to do the trick.I thought about increasing the number of training images by flipping or warping the images I already got.","Creater_id":62359,"Start_date":"2015-01-20 09:26:10","Question_id":134226,"Tags":["machine-learning","conv-neural-network"],"Answer_count":0,"Last_activity":"2016-08-20 13:11:48","Link":"http://stats.stackexchange.com/questions/134226/convolutional-neural-network-performance-cats-dogs","Creator_reputation":116}
{"_id":{"$oid":"5837a578a05283111e4d3fdc"},"View_count":484,"Display_name":"Run2","Question_score":2,"Question_content":"I have read several papers on Convolutional Neural Nets but I am yet to come across any that has used thresholds on tanh or sigmoid to decide whether the neuron will fire or not. Obviously this works for ReLU, but why is it not used for tanh or sigmoid ? Having a 0.0001 coming out of an activation unit can do less good and much harm to a deep neural net because I am loosing sparsity.On the other hand, I have encountered deep neural nets not able to use ReLU at all (specially in the fully connected layers) because it leads to explosion of gradients and un trainable nets. So I have to fall back on tanh and sigmoid.Any advice will be really helpfulRegards","Creater_id":60597,"Start_date":"2015-01-28 12:15:26","Question_id":135376,"Tags":["neural-networks","deep-learning","conv-neural-network","deep-belief-networks"],"Answer_count":0,"Last_activity":"2016-08-20 13:11:26","Link":"http://stats.stackexchange.com/questions/135376/threshold-on-tanh-or-sigmoid-in-convolutional-neural-network","Creator_reputation":215}
{"_id":{"$oid":"5837a578a05283111e4d3fde"},"View_count":756,"Display_name":"Adorn","Question_score":2,"Question_content":"Recently I have started to implement my own Convolutional Neural Network. I have few questions. I will talk with reference to an example, so that we all remain on the same page. Suppose,input: 64X64X1 that is gray-channel only.------------Output - 64X64X1C1: 5X5X6 that is 6 conv_maps, each of size 5X5-Output - 60X60X6P1: Max-Pooling - non_overlapping size = 2X2--Output - 30X30X6C2: 9X9X8 - 8 conv_maps, each of size 9X9--------Output - 22X22X48//Subject_To_ChangeP2: Max-Pooling - Non_overlapping size = 2X2--Output - 11X11X48//Subject_To_ChangeOk, Now following are the questions:ReLUAs I understand, ReLU is applied to every neuron. That is, in C1,first time 5X5 patch is moved over input - Then the sum ofconvolution has to pass through transform_function. And no transform_function at Pooling layer. Am I correct in understanding it?Which function to use as transfer_function?Softplus? Noisy one? Leaky one?Also, same transfer function should be used for FeedForward part, right? Or can I change to sigmoid there?Convolution-Feature_Map ConnectionsHow to carry out next convolution? The P1 layer has 6 maps of 30X30. There are going to be 8 convolutional kernels, each of size 9X9. But I have NEVER seen this producing 6*8 maps. Specifically, LeNet has output of 16 maps. How to produce those maps is given in this paper on page 8. After reading it again and again I DO NOT get how to generate next feature maps. Are they doing it like this --\u003eAlso, isn't the method mentioned in the paper specific to 'OCR'? I am very confused about how to write program for them in a user-friendly way. For e.g. if I want to see the output of different architecture, how to define these rules of connections programmatically? I definitely did not understand \"It forces a break of symmetry ..\" thing from the above mentioned paper. Please if you could elaborate. I am not able to visualize problem of symmetry here.About BiasInitially I thought bias as a window of kernel size, but now I think its just a number between 0-1. But How do I add a bias? If I treat kernel as a matrix, say 5X5, then how possibly I can add a single number to matrix? We get the sum after the convolution, I think I am supposed to add the bias to this sum and then apply the transform function. Right?","Creater_id":79502,"Start_date":"2015-06-10 23:55:34","Question_id":156445,"Tags":["machine-learning","neural-networks","conv-neural-network","c++"],"Answer_count":1,"Last_activity":"2016-08-20 13:11:16","Link":"http://stats.stackexchange.com/questions/156445/implementing-convolutional-neural-network-problems","Creator_reputation":113}
{"_id":{"$oid":"5837a578a05283111e4d3feb"},"View_count":492,"Display_name":"pir","Question_score":2,"Question_content":"I've watched an online lecture regarding CNN (https://www.youtube.com/watch?v=wORlSgx0hZY) that confused me a bit. At roughly 8:35 in the lecture it was stated that it is important to use the absolute value of the tanh function on the output from the convolutional layer. I've never heard that before and I can't quite understand it just based on the lecture. Can anyone expand on that?The lecture connects this to how \"the polarity of features is often irrelevant to recognize objects\" and \"the rectification eliminates cancellations between neighboring filter outputs when combined with average pooling\".I've attached an excerpt of the slide used in the lecture below:","Creater_id":29025,"Start_date":"2014-11-30 19:54:41","Question_id":126097,"Tags":["machine-learning","neural-networks","conv-neural-network","feature-construction"],"Answer_count":1,"Last_activity":"2016-08-20 13:11:03","Link":"http://stats.stackexchange.com/questions/126097/convolutional-neural-network-using-absolute-of-tanh-on-convolution-output","Creator_reputation":623}
{"_id":{"$oid":"5837a578a05283111e4d3ff8"},"View_count":318,"Display_name":"RockTheStar","Question_score":5,"Question_content":"In CNN, we will learn filters to produce feature map in convolutional layer.In Autoencoder, each layer's single hidden unit can be considered as filter.What the difference between the filters learned in these two network?","Creater_id":41749,"Start_date":"2015-03-06 11:43:21","Question_id":140698,"Tags":["machine-learning","neural-networks","conv-neural-network","autoencoders"],"Answer_count":1,"Last_activity":"2016-08-20 13:09:51","Link":"http://stats.stackexchange.com/questions/140698/what-are-the-differences-between-filters-learned-in-autoencoder-and-convolutiona","Creator_reputation":1638}
{"_id":{"$oid":"5837a578a05283111e4d4005"},"View_count":6079,"Display_name":"Run2","Question_score":15,"Question_content":"So I am trying to do pre training on images of humans using convolutional nets. I read the papers http://people.idsia.ch/~ciresan/data/icann2011.pdf and http://ai.stanford.edu/~ang/papers/nips10-TiledConvolutionalNeuralNetworks.pdf And this stackoverflow link http://stackoverflow.com/questions/24752655/unsupervised-pre-training-for-convolutional-neural-network-in-theanobut I am not sure I am understand the structure of the nets (it is not well defined in the papers).QuestionsI can have my input followed by a noise layer followed by a conv layer, followed by a pooling layer - there after - do I de-pool before I give my output (which is same a my input image)?. Say I have several (135,240) images. If I use 32, (12,21) kernels, followed by (2,2) pooling, I will end up with 32 (62, 110) feature maps. Now do I de-pool to get 32 (124, 220) feature maps and then flatten them ? before giving my (135,240) output layer ?If I have multiple such conv-pool layers, should I train them one by one - like in stacked denoised autoencoders ? Or - can I have something like input-conv-pool-conv-pool-conv-pool-output(output being same as input) ? In that case, how is the pooling, depooling supposed to be managed ? Should I only de-pool in the last pool layer before output ? And again - what should be the resize factor of that de-pooling ? Is the intention to bring the feature maps back to the shape of the input ?Should I be introducing noise layers after every conv-pool-depool layer ?And then when fine tuning - am I supposed to just remove the de-pooling layers and leave the rest the same. Or should I remove both the noise layers and de-pooling layers Can any one point me to a url/paper which has detailed the architecture of such a stacked convolutional auto encoder to do pre training on images ?Thanks a lot","Creater_id":60597,"Start_date":"2015-02-13 01:28:39","Question_id":137537,"Tags":["neural-networks","deep-learning","autoencoders","deep-belief-networks"],"Answer_count":2,"Last_activity":"2016-08-20 13:09:09","Link":"http://stats.stackexchange.com/questions/137537/what-is-the-architecture-of-a-stacked-convolutional-autoencoder","Creator_reputation":215}
{"_id":{"$oid":"5837a578a05283111e4d4013"},"View_count":41,"Display_name":"Pedestrian","Question_score":0,"Question_content":"I have become aware of Clark \u0026amp; McCracken (2001) showing that the asymptotics of the Diebold-Mariano test will potentially collapse when comparing forecast accuracy of nested models (such as GARCH / GJR-GARCH / APARCH). How badly will this affect my results?References:Clark, Todd E., and Michael W. McCracken. \"Tests of equal forecast accuracy and encompassing for nested models.\" Journal of Econometrics 105.1 (2001): 85-110.","Creater_id":122805,"Start_date":"2016-08-18 13:20:44","Question_id":230566,"Tags":["forecasting","asymptotics","accuracy"],"Answer_count":1,"Last_activity":"2016-08-20 13:05:54","Link":"http://stats.stackexchange.com/questions/230566/diebold-mariano-test-in-case-of-nested-models-clark-mccracken-2001","Creator_reputation":16}
{"_id":{"$oid":"5837a578a05283111e4d4020"},"View_count":149,"Display_name":"Pini","Question_score":2,"Question_content":"In my thesis I use both 'lm' and 'nls'. I learned that the 'logLik' command of R produces different results even if the 'lm' and 'nls' objects represent the same model (see numerical example below).If I understand correctly:For 'lm' objects, the log-likelihood is calculated as:For 'nls' objects, the log-likelihood is calculated as:The differences between the two are the multiplication by n in the first term and the use of squared weights in the last term (both are marked in bold in the second equation).The 'logLik' documentation that I found contains a reference for the log-likelihood of 'lm' objects. The derivation of this equation is clear to me.Regarding the log-likelihood equation for 'nls' objects, I did not find a reference in the documentation. Could someone please provide guidance where can I find explanations about this equation, its derivation, and the reason for the difference from the 'lm' case?Numerical ExampleData Generation:\u0026gt; set.seed(1)\u0026gt; E \u0026lt;- numeric()\u0026gt; Y \u0026lt;- numeric()\u0026gt; W \u0026lt;- numeric()\u0026gt; X \u0026lt;- c(101:150)\u0026gt; for(i in 1:50){+   E \u0026lt;- c(E, rnorm(1, 0, sqrt(1+X[i])))+ }\u0026gt; Y \u0026lt;- 1+X+E\u0026gt; for(i in 1:50){+   W \u0026lt;- c(W, 1/(1+X[i]))+ }Model Fitting:\u0026gt; L \u0026lt;- lm(Y ~ X, weights = W)\u0026gt; summary(L)Call:lm(formula = Y ~ X, weights = W)Weighted Residuals:     Min       1Q   Median       3Q      Max-2.32374 -0.46873  0.01821  0.62034  1.47708Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)(Intercept)  2.64566   11.48112    0.23    0.819X            0.99582    0.09209   10.81 1.84e-14 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.8399 on 48 degrees of freedomMultiple R-squared:  0.709, Adjusted R-squared:  0.7029F-statistic: 116.9 on 1 and 48 DF,  p-value: 1.84e-14\u0026gt; NL \u0026lt;- nls(Y ~ A+B*X, start = list(A = 1, B = 1), weights = W)\u0026gt; summary(NL)Formula: Y ~ A + B * XParameters:  Estimate Std. Error t value Pr(\u0026gt;|t|)A  2.64566   11.48113    0.23    0.819B  0.99582    0.09209   10.81 1.84e-14 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.8399 on 48 degrees of freedomNumber of iterations to convergence: 1Achieved convergence tolerance: 1.068e-07Log-Likelihood Value:\u0026gt; L_Log_Lik1 \u0026lt;- logLik(L)\u0026gt; L_Log_Lik2 \u0026lt;- 0.5*(sum(log(W))-50*log(2*pi)-50+50*log(50)+                    -50*log(sum(W*(Y-predict(L))^2)))\u0026gt; L_Log_Lik1'log Lik.' -182.0457 (df=3)\u0026gt; L_Log_Lik2[1] -182.0457\u0026gt; NL_Log_Lik1 \u0026lt;- logLik(NL)\u0026gt; NL_Log_Lik2 \u0026lt;- 0.5*(50*sum(log(W))-50*log(2*pi)-50+50*log(50)+                     -50*log(sum(W^2*(Y-predict(NL))^2)))\u0026gt; NL_Log_Lik1'log Lik.' -5983.21 (df=3)\u0026gt; NL_Log_Lik2[1] -5983.21Best regards,Pini","Creater_id":127445,"Start_date":"2016-08-03 03:55:12","Question_id":227180,"Tags":["r"],"Answer_count":1,"Last_activity":"2016-08-20 13:04:40","Link":"http://stats.stackexchange.com/questions/227180/issue-about-loglik-command-of-r","Creator_reputation":18}
{"_id":{"$oid":"5837a579a05283111e4d402d"},"View_count":23,"Display_name":"Enthusiastic","Question_score":-4,"Question_content":"I was reading a proof on a paper online and he said that the exponential is a special case of the Gamma distribution, how's that?","Creater_id":128381,"Start_date":"2016-08-20 12:51:28","Question_id":230895,"Tags":["distributions"],"Answer_count":1,"Last_activity":"2016-08-20 12:52:22","Link":"http://stats.stackexchange.com/questions/230895/how-do-we-go-from-exp-distribution-to-gamma-distribution","Creator_reputation":58}
{"_id":{"$oid":"5837a579a05283111e4d403a"},"View_count":74,"Display_name":"Emrah Bilgi\u0026#231;","Question_score":0,"Question_content":"I have a data set n=175 and for 2 different clustering (A and B) I have 5 and 6 clusters. The table for similarity of clusterings is below. First I calculated the Rand Index both manually with Excel and with \"cluster_similarity\" function in R and I got 63,4%.Than I calculated the Adjusted Rand index both with Excel and \"adjustedRandIndex\" function in R. I got 0,003 even not %3. Why is this big difference? I am very confused, I was planning to use Rand Index for my paper work but I am afraid if I have to use the adjsuted one. There are some zeros and ones in the table, may be those are problem.","Creater_id":59038,"Start_date":"2016-08-20 01:38:55","Question_id":230838,"Tags":["r","clustering","data-mining","hierarchical-clustering"],"Answer_count":1,"Last_activity":"2016-08-20 12:51:25","Link":"http://stats.stackexchange.com/questions/230838/is-this-big-difference-meaningful-63-rand-index-but-0-004-adjusted-rand-index","Creator_reputation":84}
{"_id":{"$oid":"5837a579a05283111e4d4047"},"View_count":891,"Display_name":"Andy K","Question_score":12,"Question_content":"Multi-arm bandits work well in situation where you have choices and you are not sure which one will maximize your well being. You can use the algorithm for some real life situations. As an example, learning can be a good field:   If a kid is learning carpentry and he is bad at it, the algorithm will tell him/her that he/she probably should need to move on. If he/she is good at it, the algorithm will tell him/her to continue to learn that field. Dating is a also a good field:  You're a man on your putting a lot of 'effort' in pursuing a lady. However, your efforts are definitely unwelcomed. The algorithm should \"slightly\" (or strongly) nudge you to move on.What others real-life situation can we use the multi-arm bandit algorithm for?PS: If the question is too broad, please leave a comment. If there is a consensus, I'll remove my question.","Creater_id":27900,"Start_date":"2016-08-18 08:22:12","Question_id":230523,"Tags":["algorithms","reinforcement-learning","multiarmed-bandit"],"Answer_count":4,"Last_activity":"2016-08-20 12:45:14","Link":"http://stats.stackexchange.com/questions/230523/in-what-kind-of-real-life-situations-can-we-use-a-multi-arm-bandit-algorithm","Creator_reputation":166}
{"_id":{"$oid":"5837a579a05283111e4d4057"},"View_count":44,"Display_name":"BigData","Question_score":0,"Question_content":"I have a question:If Find My answer:P(A\\text{ and }B | A \\cup B)=\\frac{P[(A\\text{ and }B)\\text{ and }(A \\cup B)]}{P(A \\cup B)}The denominator is 0.5 + 0.3 - 0.1 = 0.7But what about the numerator?By drawing the Venn diagram I know that  is actually . But is there any way to do it mathematically ?I cannot simply multiply  and  because A and B are not independent right? So how can I do it?","Creater_id":125543,"Start_date":"2016-08-20 10:48:03","Question_id":230884,"Tags":["probability","self-study","conditional-probability"],"Answer_count":1,"Last_activity":"2016-08-20 12:32:11","Link":"http://stats.stackexchange.com/questions/230884/basic-probability-conditional-probability","Creator_reputation":17}
{"_id":{"$oid":"5837a579a05283111e4d4064"},"View_count":44,"Display_name":"Lau99","Question_score":1,"Question_content":"my apologies if this is in the wrong place. There are similar questions on here, by I'm really confused about what is appropriate for my data. My goal is to present average population densities of various species per habitat type, with confidence intervals (data collected from literature review). The data below are a sample, but I have about 150 species with 6+ habitats each, with very varied sample sizes (from 350+ to \u0026lt;10) and distributions (most are heavily skewed right, but some are scattered all over the place....!). The analysis has to be quick and repeatable, so I need to use the same approach for all species. After researching the options, it seems presenting a median would be best? For the confidence intervals, I've come across an approach using the Median Absolute Value, but the constant in this is dependent on the distribution - so not repeatable for all samples. I've also read about bootstrapping for the CI's. So, my questions are:  Is the median the most robust (best??) average where data are not necessarily normal?  If so, what is the best way to estimate CI's for a median?Is there a simple way of calculating CI's that is robust to different distributions (i.e. repeatable for different species / habitats) or is this asking too much??Are there any existing functions for this in R?  I'm sure there are lots of ways to present these data, but I can't seem to find a standard accepted approach - if there isn't one I just need to do something justifiable i guess!  Ests \u0026lt;- \"Habitat Estimate 1          Woodland         4.82          Woodland         2.73          Woodland         0.64          Woodland         05          Woodland         0.96          Woodland         17          Woodland         88          Woodland         39          Woodland         2.110         Woodland         1.111         Woodland         012         Woodland         013         Woodland         014         Woodland         015         Woodland         816         Woodland         517         Woodland         118         Woodland         219         Woodland         1.219         Woodland         0.919         Woodland         419         Woodland         319         Woodland         0.320         Grassland        321         Grassland        222         Grassland        723         Grassland        924         Grassland        1725         Grassland        2526         Grassland        027         Grassland        428         Grassland        1629         Grassland        1730         Grassland        3731         Grassland        1732         Grassland        833         Grassland        834         Grassland        1535         Grassland        2236         Grassland        1537         Grassland        2038         Grassland        14\".Data \u0026lt;- read.table(text=Ests, header = TRUE)ggplot(Data, aes(x=Estimate)) +     geom_histogram(binwidth=.5, colour=\"black\", fill=\"white\") +     facet_grid(Habitat ~ .)EDIT - I've had a go at bootstrapping using boot.ci in R. There are 4 options (normal, basic, percentile and bias-corrected). Maindonald \u0026amp; Braun's \"Data Analysis and Graphics Using R: An Example-Based Approach\" suggests that narrower CI's for the bias-corrected method than the percentile means that more resamples - I've tried up to 10,000 but it doesn't make much difference. Does this mean the sample is too small??The bias-corrected method seems most popular, but I'm guessing that depends on the data, I'm not sure which I should use? Also, a final problem is that the data should be bound at zero (can't have negative pop densities), and some of the CI's are coming back as negative - can I simply report these as zero, or is that a terribly unacceptable thing to do?? Thanks very much for any help offered.  ","Creater_id":110403,"Start_date":"2016-08-20 08:41:20","Question_id":230877,"Tags":["r","median","confidence-interval"],"Answer_count":0,"Last_activity":"2016-08-20 12:21:44","Link":"http://stats.stackexchange.com/questions/230877/are-median-and-confidence-intervals-appropriate","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4066"},"View_count":53,"Display_name":"Sukhram singh","Question_score":0,"Question_content":"\\mathrm{Var}(\\beta) = E [ (\\beta- E\\hat{\\beta})^2 ]  My question is: Why is there \"\" instead of summation?","Creater_id":128391,"Start_date":"2016-08-20 10:25:03","Question_id":230879,"Tags":["regression","estimation","empirical"],"Answer_count":2,"Last_activity":"2016-08-20 11:48:09","Link":"http://stats.stackexchange.com/questions/230879/variance-of-a-regression-coefficient","Creator_reputation":14}
{"_id":{"$oid":"5837a579a05283111e4d4074"},"View_count":3174,"Display_name":"Jonathan Dobbie","Question_score":6,"Question_content":"I have two samples of data, a baseline sample, and a treatment sample.The hypothesis is that the treatment sample has a higher mean than the baseline sample.Both samples are exponential in shape.  Since the data is rather large, I only have the mean and the number of elements for each sample at the time I will be running the test.How can I test that hypothesis?  I'm guessing that it is super easy, and I've come across several references to using the F-Test, but I'm not sure how the parameters map.","Creater_id":34869,"Start_date":"2013-11-15 13:41:53","Question_id":76689,"Tags":["hypothesis-testing","statistical-significance","exponential"],"Answer_count":2,"Last_activity":"2016-08-20 11:07:58","Link":"http://stats.stackexchange.com/questions/76689/how-to-compare-the-mean-of-two-samples-whose-data-fits-exponential-distributions","Creator_reputation":33}
{"_id":{"$oid":"5837a579a05283111e4d4082"},"View_count":31,"Display_name":"lacerbi","Question_score":1,"Question_content":"I am performing  distinct, independent experiment , ,  to ideally measure the same quantity  of interest.For each experiment, I can compute the posterior pdf of  via MCMC, according to some underlying model, which we call , , . There is a chance, hard to quantify and model, that the experiments are not measuring the same thing.What would be the most reasonable/established method to:test whether the  estimates are compatible (we can focus for simplicity on the case );and/or quantify such compatibility? For example, as a test I could simply compare the 95% credible intervals (computed via empirical quantiles of the MCMC samples), and check that they all overlap; as a quantitative metric I could perhaps report the percentage of overlap between the 95% CI, or something along these lines.","Creater_id":80479,"Start_date":"2016-07-14 04:12:54","Question_id":223750,"Tags":["estimation","references","pdf","posterior"],"Answer_count":1,"Last_activity":"2016-08-20 11:04:25","Link":"http://stats.stackexchange.com/questions/223750/quantify-compatibility-between-posterior-estimates","Creator_reputation":1737}
{"_id":{"$oid":"5837a579a05283111e4d408f"},"View_count":88,"Display_name":"LetsPlayYahtzee","Question_score":0,"Question_content":"I am trying to visualize a dataset with two classes with seaborn. Specifically I am using thissb.lmplot('degree_centr', 'path', data=ds, hue='label', scatter=True, fit_reg=False, x_jitter=0.2, y_jitter=0.3, palette=sb.color_palette(\"husl\", 2))degree_centr and path are the two features in that example colored by label.The problem is that whenever class 1 and class 0 coexist on the plot the color of 1 dominates the whole region even if the examples of class 0 are more. For example have a look at thatIn the region where degree centrality is 0 and path 2 although the examples of class 0 are more you need to zoom to observe the class 0 examples. I have used jitter but still I cannot seem to produce practical results. Any ideas?","Creater_id":107476,"Start_date":"2016-08-20 10:38:04","Question_id":230881,"Tags":["data-visualization","python","matplotlib"],"Answer_count":0,"Last_activity":"2016-08-20 10:38:04","Link":"http://stats.stackexchange.com/questions/230881/scatter-plot-in-seaborn-dominated-by-one-color","Creator_reputation":108}
{"_id":{"$oid":"5837a579a05283111e4d4091"},"View_count":63,"Display_name":"Sweta95","Question_score":1,"Question_content":"I need help with the line of my thinking, and how to conclude it because I'm unsure about my conclusion.A bivariate normal distribution with correlation coefficient between the random variables=1 does not have a pdf. But suppose we try to sketch the function by finding the probabilities obtained in different intervals where Y=aX+b, for all a,b(which are constants) from the real line (because X and Y are linearly related). If we sketch the function, we get a straight line in 3D over all intervals, we get a probability -say p- over the intervals where X and Y are linearly related (p belongs to [0,1]) and probability everywhere else is 0.My question is, can I conclude that a bivariate normal pdf with correlation coefficient between the random variables=1 can have a function drawn, despite not having a pdf?","Creater_id":102886,"Start_date":"2016-08-20 07:37:00","Question_id":230860,"Tags":["pdf"],"Answer_count":1,"Last_activity":"2016-08-20 10:33:38","Link":"http://stats.stackexchange.com/questions/230860/pdf-of-a-bivariate-normal-distribution-with-correlation-coefficient-between-rand","Creator_reputation":30}
{"_id":{"$oid":"5837a579a05283111e4d409e"},"View_count":51,"Display_name":"Christine ","Question_score":3,"Question_content":"I have a problem that I have a hard time figuring out. We have a sequence of 10 numbers, 0 to 9. We want to find all the combinations where, when choosing from the sequence 18 times with replacement, we will have 4 of them repeated 3 times each, and the rest appearing once only.Here is what my thought process is:Choosing 4 numbers out of 10: Each repeating 3 times: Unique sequences of the remaining 6 numbers: Combination of all the unique sequences: Thus combining them I will have: Could someone please tell me if this is correct? and if not where did I go wrong.","Creater_id":115209,"Start_date":"2016-05-10 13:24:46","Question_id":211899,"Tags":["probability","self-study","combinatorics"],"Answer_count":1,"Last_activity":"2016-08-20 10:07:11","Link":"http://stats.stackexchange.com/questions/211899/alternating-random-number-sequence-combination","Creator_reputation":16}
{"_id":{"$oid":"5837a579a05283111e4d40ab"},"View_count":28,"Display_name":"JavierM88","Question_score":0,"Question_content":"I have 5 sets with three elements each:A can be either 2,1 or 0B can be either 2,1 or 0..E can be either 2,1 or 0You can only pick one element from one of the sets.For instance, I know there is only one way of obtaining 10 and that is by picking the element 2 from the 5 sets. But I want to know how many combinations there is for example to get 5? And what would be the probability for this? I would need to get the total number of possible combinations, but I don't know how to obtain this?I thought of 15 choose 5. But that is way too many combinations as I can only pick one element from each set.","Creater_id":114045,"Start_date":"2016-04-30 03:14:20","Question_id":210140,"Tags":["combinatorics"],"Answer_count":1,"Last_activity":"2016-08-20 10:05:02","Link":"http://stats.stackexchange.com/questions/210140/counting-the-number-of-combinations-from-5-equals-sets","Creator_reputation":1}
{"_id":{"$oid":"5837a579a05283111e4d40b8"},"View_count":39,"Display_name":"Easthaven","Question_score":1,"Question_content":"I have a binary classification problem (lets say, whether or not an observation will experience action x). I train a random forest model on a training set where about 50% have done action x and 50% have not. I test the model on a test set (again, about 50% did action x, 50% did not), and its about 85% or so accurate and has an overall error rate of about 15% or so. A year passes and I get new data and I want to see how the model performed. It predicted that about 9% of the data will experience action x. 9% of the data did in fact experience action x but it failed to accurately predict the individual observations that would experience action x. In order words, the individual observations it predicted would experience action x did not actually experience action x. And the individual observations that did in fact experience action x, the model did not predict it so.So essentially what does it mean that the model gets the aggregate correct but fails to make accurate predictions at the micro level? Is there a mathematical explanation on how this might occur? Maybe something to do with aggregating the probabilities? Is it still useful at predicting totals?","Creater_id":122742,"Start_date":"2016-08-20 09:43:34","Question_id":230872,"Tags":["r","classification","predictive-models","random-forest","prediction"],"Answer_count":0,"Last_activity":"2016-08-20 09:55:45","Link":"http://stats.stackexchange.com/questions/230872/model-fails-individual-predictions-but-gets-the-total-right","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d40ba"},"View_count":3771,"Display_name":"Roman","Question_score":39,"Question_content":"Roughly speaking a p-value gives a probability of the observed outcome of an experiment given the hypothesis (model). Having this probability (p-value) we want to judge our hypothesis (how likely it is). But wouldn't it be more natural to calculate the probability of the hypothesis given the observed outcome?In more details. We have a coin. We flip it 20 times and we get 14 heads (14 out of 20 is what I call \"outcome of experiment\"). Now, our hypothesis is that the coin is fair (probabilities of head and tail are equal to each other). Now we calculate the p-value, that is equal to the probability to get 14 or more heads in 20 flips of coin. OK, now we have this probability (0.058) and we want to use this probability to judge our model (how is it likely that we have a fair coin). But if we want to estimate the probability of the model, why don't we calculate the probability of the model given the experiment? Why do we calculate the probability of the experiment given the model (p-value)?","Creater_id":2407,"Start_date":"2010-12-17 03:36:49","Question_id":5591,"Tags":["likelihood","p-value"],"Answer_count":9,"Last_activity":"2016-08-20 09:50:45","Link":"http://stats.stackexchange.com/questions/5591/why-do-people-use-p-values-instead-of-computing-probability-of-the-model-given-d","Creator_reputation":69}
{"_id":{"$oid":"5837a579a05283111e4d40cf"},"View_count":23,"Display_name":"Alex","Question_score":0,"Question_content":"I am working on GPS data and I have plotted the speed distribution across a time frame for 3 different moving objects. Data length is not the same for my 3 entities. i.e object 1 has 4500 rows and object 2 has 2700 for each attribute. moreover, time is in seconds, starting from the first measurement.my code is as follows:plot(object1speed)which gives me the following graphs:[1[2I am trying to find a way to compare them. that is measuring similarity. I am also trying to see if there is a similarity in some particular part of the distribution if not all. I do not want to correlate the distributions as this will give me just one value. I am trying to find similarities in all or part of the sequence.does anyone know how to do this?","Creater_id":null,"Start_date":"2016-08-17 12:33:16","Question_id":230873,"Tags":["r"],"Answer_count":0,"Last_activity":"2016-08-20 09:47:26","Link":"http://stats.stackexchange.com/questions/230873/similarity-between-graphs-in-r","Creator_reputation":null}
{"_id":{"$oid":"5837a579a05283111e4d40d1"},"View_count":36,"Display_name":"Enthusiastic","Question_score":2,"Question_content":"If  s are iid, and  has a Burr distribution (https://en.wikipedia.org/wiki/Burr_distribution), what is the distribution of , .","Creater_id":128381,"Start_date":"2016-08-20 09:06:30","Question_id":230867,"Tags":["distributions"],"Answer_count":0,"Last_activity":"2016-08-20 09:24:13","Link":"http://stats.stackexchange.com/questions/230867/if-barx-has-a-burr-distribution-what-is-the-distribution-of-x-i","Creator_reputation":58}
{"_id":{"$oid":"5837a579a05283111e4d40d3"},"View_count":149,"Display_name":"stats566","Question_score":1,"Question_content":"Using the simple linear regression model: , where E[]=0 and var[...If  and (where  and  are ordinary least squares estimates of  and ) how would I go about deriving ? Here is what I have been thinking:If the work that I've done is correct, is there any more steps that I can take?","Creater_id":44384,"Start_date":"2014-04-23 22:12:03","Question_id":94970,"Tags":["regression","mathematical-statistics","least-squares","covariance"],"Answer_count":1,"Last_activity":"2016-08-20 09:15:23","Link":"http://stats.stackexchange.com/questions/94970/what-is-textcov-haty-i-haty-i","Creator_reputation":10}
{"_id":{"$oid":"5837a579a05283111e4d40e0"},"View_count":8,"Display_name":"Enthusiastic","Question_score":1,"Question_content":"Suppose I designed a hypothesis test and I want to find the -value for this test. Do I need to do the permutation test, every single time I want to do the test? or is the -value obtained for one test, is valid for all the next tests?","Creater_id":128381,"Start_date":"2016-08-20 09:15:22","Question_id":230869,"Tags":["hypothesis-testing","p-value"],"Answer_count":0,"Last_activity":"2016-08-20 09:15:22","Link":"http://stats.stackexchange.com/questions/230869/p-value-obtained-from-the-permutation-test","Creator_reputation":58}
{"_id":{"$oid":"5837a579a05283111e4d40e2"},"View_count":774,"Display_name":"Alexander D\u0026#252;r","Question_score":3,"Question_content":"I have a large data set consisting of ca. 40 categorical data items and a few interval data items (real numbers, less than 5 such items). Most categories should have a lot of values that repeat themselves over and over and very few that appear very rarely. Some categories are also overcategories of others (like country and city). The outcome of each data is either 1 if the event occured or 0 if it did not occur.The idea is to calibrate a machine learning model or a staistical model that can predict for every given data row the probability that the outcome is 1. The data set I will use will have at least 1 million rows.What machine learning approaches and statistical models will perform well on such a task? My initial thoughts are logarithmic regression and support vector machines (with extensions like random forest)How do I deal with the interval data items? The easiest approach is obiously to convert different ranges into categories, which I think will not be a problem.What libraries and tools can I use, when my data set has a size of 10 GB? I am interested in tools/libraries that include machine learning algorithms but also statistical tools to help me find attributes with significant influence on the outcome. I can code in Java and C++ at the moment. I looked into Root, a data analysis tool from CERN for lare data sets and its machine learning module TMVA but it can only handle real numbers and integers as input as far as I know.","Creater_id":27545,"Start_date":"2013-07-02 07:05:58","Question_id":63134,"Tags":["machine-learning","categorical-data","data-mining","feature-selection","prediction"],"Answer_count":2,"Last_activity":"2016-08-20 08:41:58","Link":"http://stats.stackexchange.com/questions/63134/event-prediction-through-machine-learning","Creator_reputation":23}
{"_id":{"$oid":"5837a579a05283111e4d40f0"},"View_count":101675,"Display_name":"vrish88","Question_score":47,"Question_content":"What are principal component scores (PC scores, PCA scores)?","Creater_id":191,"Start_date":"2010-07-19 22:37:46","Question_id":222,"Tags":["pca","definition"],"Answer_count":7,"Last_activity":"2016-08-20 08:27:48","Link":"http://stats.stackexchange.com/questions/222/what-are-principal-component-scores","Creator_reputation":418}
{"_id":{"$oid":"5837a579a05283111e4d4103"},"View_count":29702,"Display_name":"Rob Hyndman","Question_score":75,"Question_content":"Suppose I have a large set of multivariate data with at least three variables. How can I find the outliers? Pairwise scatterplots won't work as it is possible for an outlier to exist in 3 dimensions that is not an outlier in any of the 2 dimensional subspaces.I am not thinking of a regression problem, but of true multivariate data. So answers involving robust regression or computing leverage are not helpful.One possibility would be to compute the principal component scores and look for an outlier in the bivariate scatterplot of the first two scores. Would that be guaranteed to work? Are there better approaches?","Creater_id":159,"Start_date":"2010-07-19 22:02:33","Question_id":213,"Tags":["multivariate-analysis","outliers"],"Answer_count":13,"Last_activity":"2016-08-20 08:26:22","Link":"http://stats.stackexchange.com/questions/213/what-is-the-best-way-to-identify-outliers-in-multivariate-data","Creator_reputation":26604}
{"_id":{"$oid":"5837a579a05283111e4d411c"},"View_count":42,"Display_name":"user3801801","Question_score":1,"Question_content":"When we specify the “family=” argument inside glm() in R, how is the distribution being used to regress between the dependent and independent.?In simple linear regression( lm() in R ) ,we simply calculate the mean of Y for each X and that becomes the predicted value. How different is this when we mention a family? I do know that each distribution has few paramters that describes it ( mean, shape, scale etc). So how are they used to get predictions?Bascially I would like to learn what does it mean to fit a distribution to a data.Edit : to clarify the questionLets say I trained my model on a dataset of 1lac obs with 2 independent variables and the coefficients are 1,2. i.e. beta1 = 1 and beta2 = 2          Y    x1   x21st obs.  2    .5    12nd obs.  1    .25  .25So if I choose Poisson distribution, then1st obs. mean(mu) = exp(.5*1+1*2) = 12.18similarily we get a mean for the second obs and so on. Now how is this mean related to what you gonna predict? I am not able to connect this. Another major concern is also how the shape/scale i.e.(sigma, nu, tau for gamlss) being modeled. However they are secondary and for now wanna focus on glm My questions may be stupid but even any resources/links will be helpful and I shall sincerely read them ","Creater_id":126702,"Start_date":"2016-08-20 05:54:21","Question_id":230851,"Tags":["r","regression","machine-learning","lm"],"Answer_count":0,"Last_activity":"2016-08-20 07:46:41","Link":"http://stats.stackexchange.com/questions/230851/what-does-it-mean-to-perform-regression-using-a-specific-distribution","Creator_reputation":116}
{"_id":{"$oid":"5837a579a05283111e4d411e"},"View_count":28,"Display_name":"Tom Dally","Question_score":2,"Question_content":"I've been running GLMMs in the R package \"glmmadmb\" looking at the effects of different sizes of pan trap on the abundance of their catch, using the following code: glmm5 \u0026lt;- glmmadmb(abPan_size+abNectar+1)+log(abMax_temp+abTotal_polls  Df Chisq Pr(\u0026gt;Chisq)    abTreatment              2 14.8347  0.0006007 ***log(abMean.nectar + 1)   1  5.0591  0.0244971 *  abSeason                 4 46.4576  1.978e-09 ***Residuals               212                       ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1and...  summary(glmm5)Call:glmmadmb(formula = abPan_size + abNectar + 1) + log(abMax_temp + abYear) + (1 | abPan_size2              0.5272     0.1587    3.32  0.00089 ***abPan_size12             0.9026     0.1538    5.87  4.4e-09 ***abTreatment48            0.4973     0.1291    3.85  0.00012 ***log(abMean.nectar + 1)   0.0934     0.0415    2.25  0.02450 *  abSeason5                0.6176     0.2051    3.01  0.00260 ** abSeason7                0.7909     0.2338    3.38  0.00072 ***abYear=2, abYear             Variance   StdDev(Intercept) 1.142e-07 0.000338Group=abPan_size))) (using the glht interface in R package \"Lsmeans\") it gives me this:     Simultaneous Tests for General Linear HypothesesFit: glmmadmb(formula = abTreatment + abNectar + 1) + log(abMax_temp + abYear) + (1 | abpan_size in the model, but that doesn't improve things.Here's a list of the basic code that I'm using:#pollinator abundance vs. pan trap size and time left activeab\u0026lt;-read.csv(\"Total_polls.csv\")abnames(ab)str(ab)summary(ab)# create factors from numerical variablesabYear)abTransect)abTreatment)abPan_size)abSeason)library(glmmADMB)library(RVAideMemoire)library(car)glmm5 \u0026lt;- glmmadmb(abTreatment+abNectar+1)+log(abMax_temp+abYear)+(1|abPan_size)))glht2 \u0026lt;- (glht(glmm5, lsm(pairwise ~ abSeason)))summary(glht1)I'm using R version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\". Could anyone help me to sort this out? I'm not exactly stats savvy, so you may have to be kind with the mathematical language.Many thanks,Tom","Creater_id":128279,"Start_date":"2016-08-19 08:55:59","Question_id":230734,"Tags":["r","mixed-model","glmm","post-hoc"],"Answer_count":0,"Last_activity":"2016-08-20 07:31:08","Link":"http://stats.stackexchange.com/questions/230734/post-hoc-output-and-glmm-output-dont-add-up","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d4120"},"View_count":117093,"Display_name":"sorin","Question_score":6,"Question_content":"I do have a big list of numeric values (including duplicates) and I do want to group them into  ranges in order to see if how do they distribute.Let's say there are 1000 values ranging from 0 to 2.000.000 and I do want to group them. How can I achieve this, preferably in Excel or SQL.","Creater_id":1901,"Start_date":"2010-11-09 04:36:58","Question_id":4341,"Tags":["excel","sql"],"Answer_count":8,"Last_activity":"2016-08-20 07:31:05","Link":"http://stats.stackexchange.com/questions/4341/how-do-i-group-a-list-of-numeric-values-into-ranges","Creator_reputation":136}
{"_id":{"$oid":"5837a579a05283111e4d4134"},"View_count":114,"Display_name":"elisa","Question_score":0,"Question_content":"According to a recent paper (open pdf here), one can specify both within- and between-subjects effects in a Bayes factor ANOVA. In the example they give (p. 28), this is specified in the following way:bf = anovaBF(rt~a*d*p+s, data = dat, whichModels=\"withmain\", whichRandom=\"s\", iterations = 100000)With:a = age , hence a between-subjects effectd = distanceand p = presentation as experimental manipulations and hence within-subjects effects. And s = subjectI just want to double check: is it not necessary at all to specify which effects are between- and which are within-subjects?","Creater_id":36158,"Start_date":"2016-08-17 01:55:08","Question_id":230224,"Tags":["r","anova","bayes-factors"],"Answer_count":2,"Last_activity":"2016-08-20 07:23:00","Link":"http://stats.stackexchange.com/questions/230224/mixed-bayesian-anova-using-bayesfactor-package-in-r","Creator_reputation":198}
{"_id":{"$oid":"5837a579a05283111e4d4142"},"View_count":32,"Display_name":"Venkat.V.S","Question_score":2,"Question_content":"I recently read that eigenvalue indicates the variance for an attribute/dimension. But is there a relation/equation between eigenvalue and variance? Is is right to say eigenvalue is equal to variance (but the direction can be different)?I tried a simple example in R:x= matrix(c(-1,2,2,2,2,-1,2,-1,2), 3, 3)var (x)eigen(x)Variance was 3, 3, 3, eigenvalue was 3, 3, -3.Can someone please explain me the relationship?","Creater_id":117207,"Start_date":"2016-08-20 07:02:43","Question_id":230855,"Tags":["variance","eigenvalues"],"Answer_count":1,"Last_activity":"2016-08-20 07:20:17","Link":"http://stats.stackexchange.com/questions/230855/eigenvalue-vs-variance","Creator_reputation":13}
{"_id":{"$oid":"5837a579a05283111e4d414f"},"View_count":30,"Display_name":"Contagious Flame","Question_score":1,"Question_content":"I'm looking to compare two groups and their number of successes (on a hit/miss task) to see whether the difference is statistically significant. This is similar but slightly different to a comparison of portions, which is what most of the formulas show that I've found.The formula I am trying to figure out is as follows:  A critical ratio used to decide whether the numbers of hits obtained  under two conditions (or by two groups of subjects) differ  significantly from each other; it is obtained by dividing the  difference between the two total-hits scores by the standard deviation  of the difference.I am stuck on what the \"standard deviation of the difference\" is. I know that the standard deviation of one of the groups would just be sqrt(npq), where n = number of trials, p = probability of hit which in this case is 1/5, and q = 1 - p. But what about the standard deviation of the difference?","Creater_id":128362,"Start_date":"2016-08-20 06:24:06","Question_id":230854,"Tags":["binomial","standard-deviation","ratio","standard"],"Answer_count":0,"Last_activity":"2016-08-20 06:24:06","Link":"http://stats.stackexchange.com/questions/230854/test-if-two-binomial-number-of-successes-are-statistically-different-from-each-o","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4151"},"View_count":8595,"Display_name":"KennyPeanuts","Question_score":11,"Question_content":"I am looking for input on how others organize their R code and output.  My current practice is to write code in blocks in a text file as such:#=================================================# 19 May 2011date()# Correlation analysis of variables in sed summaryload(\"/media/working/working_files/R_working/sed_OM_survey.RData\")# correlation between estimated surface and mean perc.OM in epi samplescor.test(surveyDepth == \"epi\"],     surveyDepth   == \"epi\"]))#==================================================I then paste the output into another text file, usually with some annotation.  The problems with this method are:The code and the output are not explicitly linked other than by date.The code and output are organized chronologically and thus can be hard to search.I have considered making one Sweave document with everything since I could then make a table of contents but this seems like it may be more hassle than the benefits it would provide.  Let me know of any effective routines you have for organizing your R code and output that would allow for efficient searching and editing the analysis.","Creater_id":4048,"Start_date":"2011-05-19 10:42:19","Question_id":10987,"Tags":["r","project-management","sweave"],"Answer_count":3,"Last_activity":"2016-08-20 06:13:22","Link":"http://stats.stackexchange.com/questions/10987/what-are-efficient-ways-to-organize-r-code-and-output","Creator_reputation":434}
{"_id":{"$oid":"5837a579a05283111e4d4160"},"View_count":39,"Display_name":"Shankar","Question_score":1,"Question_content":"When I compare the p-values from summary of lm() models and anova(), for one dataset I get the same p-values, while for another dataset they are different.xx.2 \u0026lt;- as.data.frame(matrix(c(8.788269, 1, 0,                           7.964719, 6, 0,                           8.204051, 12, 0,                           9.041368, 24, 0,                           8.181555, 48, 0,                           8.041419, 96, 0,                           7.992336, 144, 0,                           7.948658, 1, 1,                           8.090211, 6, 1,                           8.031459, 12, 1,                           8.118308, 24, 1,                           7.699051, 48, 1,                           7.537120, 96, 1,                           7.268570, 144, 1), byrow=T, ncol=3))names(xx.2) \u0026lt;- c(\"value\", \"time\", \"treat\")mod1 \u0026lt;- lm(value~time+treat, data=xx.2)\u0026gt; summary(mod1)Call:lm(formula = value ~ time + treat, data = xx.2)Residuals:     Min       1Q   Median       3Q      Max -0.54627 -0.10533 -0.04574  0.11975  0.61528 Coefficients:             Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)  8.539293   0.132545  64.426 1.56e-15 ***time        -0.004717   0.001562  -3.019  0.01168 *  treat       -0.502906   0.155626  -3.232  0.00799 ** ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.2911 on 11 degrees of freedomMultiple R-squared:   0.64, Adjusted R-squared:  0.5746 F-statistic: 9.778 on 2 and 11 DF,  p-value: 0.003627\u0026gt; anova(mod1)Analysis of Variance TableResponse: value          Df  Sum Sq Mean Sq F value   Pr(\u0026gt;F)   time       1 0.77259 0.77259  9.1142 0.011677 * treat      1 0.88520 0.88520 10.4426 0.007994 **Residuals 11 0.93245 0.08477                    ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026gt; anova(mod1)Analysis of Variance TableResponse: value          Df  Sum Sq Mean Sq F value   Pr(\u0026gt;F)   time       1 0.77259 0.77259  9.1142 0.011677 * treat      1 0.88520 0.88520 10.4426 0.007994 **Residuals 11 0.93245 0.08477                    ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  Here as you might realize, the p-values are the same. Now here's an example where that isn't true.library(MASS)mod2 \u0026lt;- lm(Sepal.Length~Sepal.Width+Petal.Length, data = iris)\u0026gt; summary(mod2)Call:lm(formula = Sepal.Length ~ Sepal.Width + Petal.Length, data = iris)Residuals:     Min       1Q   Median       3Q      Max -0.96159 -0.23489  0.00077  0.21453  0.78557 Coefficients:             Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)   2.24914    0.24797    9.07 7.04e-16 ***Sepal.Width   0.59552    0.06933    8.59 1.16e-14 ***Petal.Length  0.47192    0.01712   27.57  \u0026lt; 2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.3333 on 147 degrees of freedomMultiple R-squared:  0.8402,    Adjusted R-squared:  0.838 F-statistic: 386.4 on 2 and 147 DF,  p-value: \u0026lt; 2.2e-16\u0026gt; anova(mod2)Analysis of Variance TableResponse: Sepal.Length              Df Sum Sq Mean Sq F value    Pr(\u0026gt;F)    Sepal.Width    1  1.412   1.412  12.714 0.0004902 ***Petal.Length   1 84.427  84.427 760.059 \u0026lt; 2.2e-16 ***Residuals    147 16.329   0.111                      ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1I thought their corresponding hypothesis were different? So should I expect them to be equal? If yes, why and under what circumstances?Would really appreciate any assistance. Thanks in advance.","Creater_id":92663,"Start_date":"2016-08-19 14:36:34","Question_id":230791,"Tags":["regression","anova","t-test","p-value","f-test"],"Answer_count":1,"Last_activity":"2016-08-20 05:46:32","Link":"http://stats.stackexchange.com/questions/230791/comparing-p-values-from-lm-and-anova","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d416c"},"View_count":25,"Display_name":"Mahmudul Hasan","Question_score":1,"Question_content":"How do we interpret the meaning of a percentile?What I have learned from books is as follows: \"If a score is the 40th percentile, this means that it is larger than 40% of the distribution, while the other 60% of scores are greater than or equal to the score.\"My question is when that specific score equals to the previous score, shouldn't we say 40% of the distribution is \"less or equal\" to that score, instead of \"less than\" that score?For example, say there are 500 students, and they are scored from 0 to 100. Say student A gets a score 80 and he is in 85th percentile, which traditionally means 85 percent student is less than his score and 15% is greater or equal to his score. In a specific case let 250 higher scorer students get all equal to a score of 80. Then student A is not higher than 85%. Or his score is not higher than the other student who also get 80. Is he?","Creater_id":128344,"Start_date":"2016-08-19 22:38:52","Question_id":230827,"Tags":["quantiles"],"Answer_count":1,"Last_activity":"2016-08-20 05:45:29","Link":"http://stats.stackexchange.com/questions/230827/interpretation-0f-percentile","Creator_reputation":106}
{"_id":{"$oid":"5837a579a05283111e4d4179"},"View_count":52518,"Display_name":"user333","Question_score":91,"Question_content":"If you have a variable which perfectly separates zeroes and ones in target variable, R will yield the following \"perfect or quasi perfect separation\" warning message:Warning message:glm.fit: fitted probabilities numerically 0 or 1 occurred We still get the model but the coefficient estimates are inflated. How do you deal with this in practice?","Creater_id":333,"Start_date":"2011-05-22 03:37:08","Question_id":11109,"Tags":["r","logistic","hauck-donner-effect"],"Answer_count":6,"Last_activity":"2016-08-20 05:33:12","Link":"http://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression","Creator_reputation":1918}
{"_id":{"$oid":"5837a579a05283111e4d418b"},"View_count":31,"Display_name":"luchonacho","Question_score":0,"Question_content":"This is a fundamental question I have, and so far I have not found a convincing answer.My theory is this:w_{ijt} = \\theta_{jt}a_{ij}where  is wage of worker  in occupation  in period ,  is the \"efficiency wage\" paid to every worker in occupation , in period , and  is the ability of worker  in occupation . (Since workers are observationally identified by  and , we could define )Now, we observe wages but not ability nor \"efficiency wage\". So, to estimate these two components, I use a two-way error component model (taking logs):log(w_{it}) = log(\\theta_{jt}) + log(a_{ij}) + e_{it}Notice that this decomposition is possible because each component varies across different dimensions.Now, from my estimations I get  and . The question: How can I know that the units of measurement of my estimated components are the same than those of my theory?Does the answer change if I extend my estimation with:log(w_{it}) = X\\beta + log(\\theta_{jt}) + log(a_{ij}) + e_{it}where  are other determinant of wages?","Creater_id":100369,"Start_date":"2016-08-20 02:11:37","Question_id":230842,"Tags":["regression","units"],"Answer_count":0,"Last_activity":"2016-08-20 04:09:46","Link":"http://stats.stackexchange.com/questions/230842/units-of-measurement-when-going-back-from-regression-to-theory","Creator_reputation":584}
{"_id":{"$oid":"5837a579a05283111e4d418d"},"View_count":38,"Display_name":"ali","Question_score":0,"Question_content":"I have a monhtly data set taken from datamarket. I have applied two different ARIMA models with different periods in R. The estimation results are reported below.Model 1:ARIMA(3,1,1)(0,1,1)[35]                    Coefficients:         ar1     ar2     ar3      ma1     sma1      0.5363  0.0365  0.0545  -0.9199  -0.8472s.e.  0.0903  0.0787  0.0754   0.0614   0.1530sigma^2 estimated as 874694:  log likelihood=-1959.58AIC=3931.17   AICc=3931.53   BIC=3951.92                   ME     RMSE      MAE       MPE     MAPE      MASE        ACF1Training set 6.818351 861.6035 445.7387 -3.906734 13.14426 0.5771561 0.004052349Model 2:ARIMA(3,1,1)(1,1,1)[23]                    Coefficients:         ar1     ar2     ar3     ma1    sar1     sma1      0.5161  0.1210  0.0326  -0.937  0.0515  -0.9359s.e.  0.0832  0.0757  0.0741   0.057  0.0956   0.2221sigma^2 estimated as 820158:  log likelihood=-2049.93AIC=4113.85   AICc=4114.32   BIC=4138.42                   ME     RMSE      MAE       MPE     MAPE     MASE         ACF1Training set 12.01683 854.0288 456.0118 -3.864165 13.66146 0.590458 0.0005883881With these results, I am having trouble to choose one of them. One of them has better RMSE but the other one has better MAE and MAPE.How should I interpret these results and which one should be chosen for better forecasts?","Creater_id":127498,"Start_date":"2016-08-14 22:45:56","Question_id":229862,"Tags":["forecasting","arima","model-selection"],"Answer_count":1,"Last_activity":"2016-08-20 03:35:51","Link":"http://stats.stackexchange.com/questions/229862/selecting-between-two-arima-models","Creator_reputation":3}
{"_id":{"$oid":"5837a579a05283111e4d419a"},"View_count":60,"Display_name":"Nesvold ","Question_score":2,"Question_content":"I used Bayesian model averaging and gamboostLSS for variable selection on real data with 23 covariates. A 10-fold CV was used to asses performance. I tested stepwise regression out of curiosity and found that it predicted almost as well as the other two methods. I know stepwise regression has a lot of short comings so I actually expected it do much worse. I'm not sure how to interpret this. Could there be something wrong with my codes? Is it reasonable to expect stepwise regression to do worse than BMA and gamboostLSS, which are much more solid methods? My data is relatively small with 660 observations over 11 years. Any thoughts on the matter? ","Creater_id":122544,"Start_date":"2016-08-19 17:38:59","Question_id":230809,"Tags":["bayesian","predictive-models","feature-selection","stepwise-regression","gamboost"],"Answer_count":1,"Last_activity":"2016-08-20 02:00:50","Link":"http://stats.stackexchange.com/questions/230809/stepwise-regression-predicts-as-well-as-bayesian-model-averaging-and-boosting-w","Creator_reputation":26}
{"_id":{"$oid":"5837a579a05283111e4d41a7"},"View_count":12,"Display_name":"Pretty_Girl","Question_score":1,"Question_content":"I'm stuck in somewhere to understand the behavior of those two tests. The problem is sometimes I see both the tests are doing the same thing.The Repeated Measures ANOVA (RM ANOVA) can have only one condition at the same time right? where two way ANOVA (TW ANOVA) can have multiple conditions at the same time right? For instance: Eg1:Researcher gives ONE drug (drug A) at beginning of the week1, and then gives the same drug (drug A) after beginning of the week2 to a group (population) and then she wants to find out if it has any effect (has any difference) before giving the drug, after end of one week, after end of two week time. So she measure at those time points. (the researcher don't want to see which makes the difference, she just want to see if any of them has difference or make any significance)For above example the RM ANOVA can be used right, because it only has one drug (only one condition)? drug A.Eg2:Researcher gives TWO drugs (drug A and drug B) at beginning of the week1, and then gives the same drugs (drug A and drug B) after beginning of the week2 to a group (population) and then she wants to find out if both of the drugs has any effect (has any difference) before giving the two drugs, after end of one week, after end of two week time. So she measure at those time points. (the researcher don't want to see which makes the difference, she just want to see if any of them has difference or make any significance)For above example the TW ANOVA can be used right, because it has TWO drugs (two conditions)? drug A \u0026amp; drug B.Further, if we consider the factors, TW ANOVA says it has two \"factors\" which is (treatments (drug A \u0026amp; drug B)) and \"time\". But, RM ANOVA also has those two factors as I see \"treatment\" (drug A) and \"time\" .. so what's the difference of two tests if it tests the same factors? I really don't understand unique uses of the two tests.And statistics is not my stream, I'm just considering to use this for one of my researches :).","Creater_id":128352,"Start_date":"2016-08-20 01:59:37","Question_id":230841,"Tags":["anova","spss","repeated-measures"],"Answer_count":0,"Last_activity":"2016-08-20 01:59:37","Link":"http://stats.stackexchange.com/questions/230841/repeated-measures-anova-two-way-anova-is-the-difference-is-first-one-has-sin","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d41a9"},"View_count":80,"Display_name":"Demenyi Norbert","Question_score":0,"Question_content":"In machine learning, for a given input instances you get an output what are present at the same time. But in stock market you have to predict the next price based on previous inputs. So if you want to predict the next price (output) with machine learning, how you do it lacking new input instances (for example: high price, low price, open price, close price, volume, etc.)?I want to use a simple example to be clear what I want to understand here. For example :I use high price, low price, open price and volume as inputs and close price as output. I train the algorithm with input-output samples. Then I want to predict an output (close price). But the problem is that inputs and output appears  together so this way I can`t predict the next price because it has already appeared with inputs. So how is that, how do they apply?","Creater_id":127424,"Start_date":"2016-08-19 21:18:46","Question_id":230823,"Tags":["machine-learning","time-series","forecasting","finance"],"Answer_count":1,"Last_activity":"2016-08-20 01:57:33","Link":"http://stats.stackexchange.com/questions/230823/how-can-machine-learning-be-applied-to-stock-price-prediction","Creator_reputation":16}
{"_id":{"$oid":"5837a579a05283111e4d41b6"},"View_count":58,"Display_name":"user45409","Question_score":1,"Question_content":"mydata \u0026lt;- read.csv(\"http://www.ats.ucla.edu/stat/data/binary.csv\")#create a binary variable just for the purpose of experimentationmydatarank \u0026lt;- factor(mydatabin \u0026lt;- factor(mydataadmit)log(0.3008130081/(1-0.3008130081))Result:    -0.843429383TalliedNext, do the same with rank variable alone in the modelmy.mod \u0026lt;- glm(admit ~ rank, data = mydata, family = \"binomial\")summary(my.mod)Coefficients:              Estimate Std. Error  z value    Pr(\u003e|z|)(Intercept)  0.1643031  0.2569384  0.63946    0.522521rank2       -0.7500300  0.3079693 -2.43540    0.014875rank3       -1.3646980  0.3353867 -4.06903 0.000047210rank4       -1.6867296  0.4093073 -4.12094 0.000037733 The intercept is simply logodds of admit == 1 when rank2 = 0 \u0026amp; rank3 = 0 \u0026amp; rank4 = 0; in other words rank == 1Check:x = subset(mydata, rank == \"1\" )mean(xadmit) log(0.6363636364/(1-0.6363636364))Result:      0.5596157881 Not tallied!","Creater_id":45409,"Start_date":"2016-08-20 01:40:16","Question_id":230839,"Tags":["logistic"],"Answer_count":0,"Last_activity":"2016-08-20 01:40:16","Link":"http://stats.stackexchange.com/questions/230839/interpreting-intercept-in-logistic-regression-when-there-is-more-than-one-catego","Creator_reputation":23}
{"_id":{"$oid":"5837a579a05283111e4d41b8"},"View_count":67,"Display_name":"new_analyst","Question_score":2,"Question_content":"I'm hoping that this won't be marked as a duplicate - although it's related to a few other questions on self study vs degree, those questions focus more on the best way to start learning or further existing knowledge, while I want to ask about the ROI of both options.To provide context: My undergraduate degree was in a social sciences area, with some basic stats subjects but no serious math or programming. I did a lot of self study in web development but decided that I am more interested in data science and enrolled in a data science masters program at a mid tier university (world ranking between 250-350). The top schools in my country wouldn't take me because I didn't have a computer science or math/stats bachelor degree, nor was I eligible for a straight stats Masters - and I really didn't want to have to do another Bachelor degree. Now after the first semester I am questioning the value of the degree. I've gotten top grades (about 3.8 GPA) but I feel that I have learnt very little from the university and almost all of my learning has been from self study anyway. But, there is a lot of work involved - a lot of assignments, a lot of report writing, and the time involved in doing these assignments has actually taken me away from learning more advanced concepts. The assignments were not very intellectually challenging, they were just long and tedious. My course is supposed to be more mathsy and less ITish when compared to other data science degrees, but would be less so than a Master of Statistics. In addition, lecturers have almost zero interest in their teaching responsibilities (which I know is fairly common across universities), and some of the smaller weekly practical assessments are ridiculously bad (one was basically to paraphrase some of the help from SAS Enterprise Miner - not joking).Personally, I do feel that I would learn faster by dropping the degree and learning on my own. I am not claiming to be particularly talented, but I think that the way university is taught is really ineffective - I would rather we spend more time exploring more advanced topics and less time writing long reports (which I have been doing since undergrad and do very well). However, realistically, when it comes to convincing people to hire you, it's a bit hard if your degree is not at all technical / quantitative. Most jobs list it as their first requirement.So my question is, how would you characterise the ROI of a Masters degree once you take the actual teaching and knowledge part out of the equation, especially if we're not talking about a top 30 university? Basically, to become a data scientist, what is the value of the piece of paper on it's own? And while we're at it, is a Master of Stats a better / more rigorous option, even for data scientists, when it comes to content, and ROI.","Creater_id":117321,"Start_date":"2016-08-20 01:06:40","Question_id":230836,"Tags":["self-study","education","courses"],"Answer_count":0,"Last_activity":"2016-08-20 01:06:40","Link":"http://stats.stackexchange.com/questions/230836/true-value-of-a-graduate-degree-vs-self-study","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d41ba"},"View_count":86,"Display_name":"kanimbla","Question_score":1,"Question_content":"I have a cross-sectional real estate dataset with information on roughly 100000 properties, including rental price, square meter size, number of bedrooms etc. In addition, the dataset contains information about the region for each property. The total number of regions in the dataset is 400.I would like to play around with different machine learning methods in the caret package to predict rental prices using the relevant features in the dataset. I obviously need to incorporate regional information to capture region-specific price effects.    Update: To be more formal, I have the following two model specifications in mind:where  refers to the rental price of property ,  denotes square meter size,  denotes the number of rooms,  is a matrix of dummy variables capturing different characteristics like balcony, kitchen, cellar and so on, and  is a matrix containing the 399 regional dummies.Alternatively, the model could be specified as follows:where  is a vector containing the median or average square meter price in each region . I guess inclusion of interaction effects could also make sense (see comment below by seanv507), but I am not sure about them right now.The obvious advantage of the last equation is that I only need one variable in my model to proxy for a regional effect. However, I also do not see too much of a problem for the first equation given the relatively large number of observations in the dataset. I have two questions:Which of the two specifications would be preferable or are therebetter alternative ways to deal with this issue?Which machine learning methods could be promising for this type of model?","Creater_id":79243,"Start_date":"2016-08-19 00:02:50","Question_id":230636,"Tags":["r","categorical-data","caret"],"Answer_count":1,"Last_activity":"2016-08-20 00:48:30","Link":"http://stats.stackexchange.com/questions/230636/how-to-deal-with-large-number-of-dummy-variables-in-machine-learning","Creator_reputation":74}
{"_id":{"$oid":"5837a579a05283111e4d41c7"},"View_count":11,"Display_name":"MSIS","Question_score":0,"Question_content":"I am trying to reconcile an apparent contradiction in the results of myregression against three independent variables  :1) For each of the , I regressed   and in every case,the slopecoefficient  in  was negative.2) When I regressed , only two of the (non-constant, i.e., not including  )  coefficients came out negative.But don't we interpret these  to mean that a change in one unit in  in  in  , while holding other  to be zero, leads to a change in  units of output in , apparently contradicting 1) ?  Also, shouldn't the coefficients in  equal to Cov(Y,x_i) , or does this last require that the  are standardized ( so that we would have , so that the standardized coefficients should all be negative in order to agree with the condition in 1)? Thanks. ","Creater_id":122311,"Start_date":"2016-08-20 00:09:20","Question_id":230832,"Tags":["multiple-regression"],"Answer_count":0,"Last_activity":"2016-08-20 00:09:20","Link":"http://stats.stackexchange.com/questions/230832/interpreting-coefficients-in-linear-regression-yx1-x2-x3","Creator_reputation":138}
{"_id":{"$oid":"5837a579a05283111e4d41c9"},"View_count":1356,"Display_name":"Silverfish","Question_score":9,"Question_content":"It is well-known that the asymptotic relative efficiency (ARE) of the Wilcoxon signed rank test is  compared to Student's t-test, if the data are drawn from a normally distributed population. This is true for both the basic one-sample test and the variant for two independent samples (the Wilcoxon-Mann-Whitney U). It is also the ARE of a Kruskal-Wallis test compared to an ANOVA F-test, for normal data.Does this remarkable (for me, one of the \"most unexpected appearances of \") and remarkably simple result have have an insightful, remarkable or simple proof? ","Creater_id":22228,"Start_date":"2014-12-28 16:39:16","Question_id":130562,"Tags":["mathematical-statistics","nonparametric","wilcoxon","asymptotics","efficiency"],"Answer_count":3,"Last_activity":"2016-08-19 23:40:02","Link":"http://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared","Creator_reputation":10249}
{"_id":{"$oid":"5837a579a05283111e4d41d7"},"View_count":233,"Display_name":"sudo make install","Question_score":5,"Question_content":"I have measured a continuous outcome variable in three groups of patients, where the groups are related to the severity of disease (control, mild disease, severe disease).  I would like to test for a \"trend\" among these three groups--that is, does the continuous variable become \"worse\" from control to mild to severe.Is it possible/sensical to test for trend in this type of data?If so, what tests should I consider/research?Could you recommend an R package that would carry out this analysis?Best, and thanks,--Davis","Creater_id":30231,"Start_date":"2015-01-02 20:13:45","Question_id":131044,"Tags":["r","trend"],"Answer_count":2,"Last_activity":"2016-08-19 23:35:32","Link":"http://stats.stackexchange.com/questions/131044/test-for-trend-ordinal-predictor-continuous-outcome","Creator_reputation":138}
{"_id":{"$oid":"5837a579a05283111e4d41e5"},"View_count":55,"Display_name":"Abite","Question_score":3,"Question_content":"I want to do systematic review and meta-analysis but I am facing difficulties: how to take adjusted odds ratio (AOR) when studies classify the explanatory variables differently? For example one study may put the AOR for education in three ways \u0026mdash; \"No education\" as a reference category, \"Primary education\"  with AOR (CI) , Secondary education with AOR(CI) \u0026mdash; and the other study might put e.g. \"Primary education and below\" as a reference, \"Secondary and above\" with AOR (CI). So in the first study there are two adjusted odds ratios and in the second one only one adjusted ratio. Is it possible to take the crude odds ratio by calculating manually for meta-analysis? ","Creater_id":128197,"Start_date":"2016-08-18 16:54:22","Question_id":230595,"Tags":["meta-analysis","odds-ratio"],"Answer_count":2,"Last_activity":"2016-08-19 23:33:27","Link":"http://stats.stackexchange.com/questions/230595/how-we-took-adjusted-odds-ratio-for-meta-analysis-when-different-studies-put-it","Creator_reputation":16}
{"_id":{"$oid":"5837a579a05283111e4d41f3"},"View_count":12,"Display_name":"ms_caalis","Question_score":0,"Question_content":"I have two independent variables, one of which is in nominal scale (binary), while the second is continuous. I want to determine whether any of the combinations of the two are associated with a change in a dependent variable (also continuous). Which test could help me do that?","Creater_id":120525,"Start_date":"2016-06-19 16:21:57","Question_id":219689,"Tags":["scales"],"Answer_count":0,"Last_activity":"2016-08-19 23:27:57","Link":"http://stats.stackexchange.com/questions/219689/which-test-to-use-for-associations-between-2-iv-nominalcontinuous-and-1-dv-c","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d41f5"},"View_count":13,"Display_name":"A K R","Question_score":0,"Question_content":"I want to estimate a state space model. Do the series need to be transformed into stationary series for a state space estimation?","Creater_id":110352,"Start_date":"2016-08-19 23:20:14","Question_id":230829,"Tags":["state-space-models"],"Answer_count":0,"Last_activity":"2016-08-19 23:20:14","Link":"http://stats.stackexchange.com/questions/230829/state-space-model-with-non-stationary-time-series-data","Creator_reputation":21}
{"_id":{"$oid":"5837a579a05283111e4d41f7"},"View_count":28,"Display_name":"Tonmoy Roy","Question_score":1,"Question_content":"I am using this source to \"standardize\" my data: LINKFor this I have to carry out calculations of subtracting off the mean and dividing by the standard deviation. Does it matter if I get negative values in my data after subtracting the mean?","Creater_id":128337,"Start_date":"2016-08-19 19:38:44","Question_id":230814,"Tags":["pca","normalization","standardization"],"Answer_count":1,"Last_activity":"2016-08-19 22:29:34","Link":"http://stats.stackexchange.com/questions/230814/does-it-matter-if-standardization-makes-the-data-negative-in-principal-component","Creator_reputation":8}
{"_id":{"$oid":"5837a579a05283111e4d4204"},"View_count":47,"Display_name":"Paul Uszak","Question_score":2,"Question_content":"This is a statistics question but I don't know how to phrase it in statistician's parlance, so I'm going to ask it in electrical engineering terms.  You should be able to follow it easily though.  I'm hoping that it won't be migrated /closed as it really is just applied statistics.I have a totally random voltage that is called white noise.  It's the stuff you hear in between radio stations on your tranny.  That means that it is effectively normally distributed.  By the definition of white noise, there is no simple upper limit to the rate at which this noise can change it's value.  It may be millions of times a second.  I don't know the mean or standard deviation, but when I look at a graph of it's values, they kinda go from 0 - 1 volt.  Some times due to the random nature, they go higher.  They might reach 1.5 volts.  This is all assessed by inspection alone.  I have no control over this noise as it is generated by physical processes and quantum mechanics.I now sample this random voltage 10,000 times during the period of 1 second.  With these samples recorded, is it possible to say with any confidence what the maximum voltage might be?  And what would that confidence be?This question is reminiscent of my A Level maths statistics but I'm having trouble applying  it.  I believe that it should be possible to determine something like it is \u0026lt;1.8 volts for 99% of the time.  I'm really looking for concrete numbers rather than a theorem as I need to build stuff to utilise these findings.","Creater_id":74762,"Start_date":"2016-08-19 17:17:05","Question_id":230806,"Tags":["sampling","random-variable"],"Answer_count":2,"Last_activity":"2016-08-19 22:12:59","Link":"http://stats.stackexchange.com/questions/230806/what-is-the-maximum-value-of-a-measured-random-variable","Creator_reputation":173}
{"_id":{"$oid":"5837a579a05283111e4d4212"},"View_count":20,"Display_name":"user128341","Question_score":0,"Question_content":"I have a question regarding which construction of a hierarchical regression I should be using to assess two- and three-way interactions.Specifically, I'm looking to predict my DV, Y, using these IVs: Condition, X1, and X2, controlling for X3.I'm getting confused because per some models, there are significant two-way interactions, per other models, the two-ways disappear, and when I use the Hayes PROCESS macro to probe the slopes of the two-ways, the p-values and betas get even more wacky.I'll do my best to clearly lay out below what I believe to be the possible regression constructions, and how they differ (and accordingly, how the differences are tripping me up). I am using SPSS, so I hope that my references to \"blocks\" in the regression are clear.MODEL 1 (This is my \"full\" analysis, which includes all main effects of my IVs, a significant three-way interaction, and the three two-ways necessary to test/confirm that three-way):Block 1: Condition, X1, X2, (control)X3 Block 2: Condition * X1, Condition * X2, X1 * X2Block 3: Condition*X1*X2Using this model, I get significant main effects from X2 and my control, X3. I also get two significant two-way interactions, Condition * X1 and Condition * X2. I also find a significant three-way interaction, Condition*X1*X2.(Side note: we did run an even broader model which tested for all six possible two-way interactions between our non-control IVs, and all four possible three-way interactions between our non-control IVs. This model returned identical results to the simpler model above, i.e., the same two significant main effects, the same two significant two-ways and the same significant three-way.)Seeing this, I got excited about the two-way interactions and decided to probe them with a simple slopes analysis. I first checked out the Condition * X1 interaction. I used the Hayes PROCESS macro in SPSS; I entered DV=Y, IV=X1, moderator=Condition, controlling for X3. But, strangely, PROCESS found that the interaction wasn't significant. What could be going on to cause the interaction to come up as significant in the \"full\" model, but not in the simple slopes analysis?Confused, I tried running the Condition * X2 interaction through PROCESS (DV=Y, IV=X2, moderator=Condition, controlling for X3). And, while PROCESS actually did find the interaction to be significant, the p-values were way different than in my \"full\" model.Then, even more disconcerted, I tried simplifying my hierarchical regressions to just test for each of the two-ways that I found in my \"full\" model on their own. I ran the following models:MODEL 2 (I ran this model to further examine what I believe to be the interaction effect Condition * X1):Block 1: Condition, X1, X2, (control)X3Block 2: Condition * X1As in my PROCESS simple slopes analysis, the regression found no significant two-way. Any explanation for this? Is there a reason why the interaction would disappear in this model, and should I be inclined to believe that the \"full\" model accurately detects an interaction, or should I believe this smaller model?MODEL 3 (to test just the supposed interaction of Condition * X2): Block 1: Condition, X1, X2, (control)X3Block 2: Condition * X2This returned a significant interaction, as in the \"full\" model. But as in the PROCESS test, the p-value was quite different than in my initial \"full\" model, even though it thankfully remained significant.What should I be trusting? My full model, which suggests that there are two significant two-way interactions with p-values of A and B, or PROCESS/MODEL 2/3, which suggest that there is only one significant two-way interaction with p-value C?And, if I should be trusting the full model (two sig. 2-ways, one sig. 3-way), is there a simple slopes analysis I could do that wouldn't spit back the confusing (possibly inaccurate?) results I'm getting from PROCESS/MODEL 2/3?So, in sum, I'm hoping for some pointers to figure out what interactions are real, what model is the \"best\" model, why some models would find the interactions and some wouldn't, and accordingly, what results I should be paying attention to.Thanks in advance!","Creater_id":null,"Start_date":"2016-08-19 21:31:26","Question_id":230824,"Tags":["regression","multiple-regression","spss","interaction","hierarchical"],"Answer_count":0,"Last_activity":"2016-08-19 21:44:38","Link":"http://stats.stackexchange.com/questions/230824/how-should-i-construct-my-hierarchical-regression-to-assess-several-interaction","Creator_reputation":null}
{"_id":{"$oid":"5837a579a05283111e4d4214"},"View_count":38,"Display_name":"Godgog Arsenal","Question_score":3,"Question_content":"I am about to compute the variance of the following two variables: and  which almost surely converge to  I evaluated before. I have done  and How can I evaluate the variance? through the expectation or  (how to eliminate the brackets?)?I have no idea with that especially . Hope any help! is the variance of  i mean. is an i.i.d sequence that has distribution  obtaining value  with probability .Am I right that:D(u_n)=D(\\frac{X_1+X_2+ \\ldots +X_n}{n}-1)= \\frac{1}{n^2}D(X_1)=\\frac{1}{n^2}[E(X_1^2)-(E(X_1))^2] then apply that E(X_1^2)=\\frac{1}{m}(1^2+\\ldots+m^2)","Creater_id":128214,"Start_date":"2016-08-18 19:59:33","Question_id":230620,"Tags":["variance","standard-deviation"],"Answer_count":0,"Last_activity":"2016-08-19 20:50:48","Link":"http://stats.stackexchange.com/questions/230620/compute-the-deviation-of-u-n-2-fracx-1x-2-ldots-x-nn-1-and-w-n-max","Creator_reputation":16}
{"_id":{"$oid":"5837a579a05283111e4d4216"},"View_count":30,"Display_name":"mandy","Question_score":0,"Question_content":"I was given this problem as homework and am confused on how to approach itWe want to know if a chemistry students gender is a statistically significant factor in predicting the average chemistry test score.  The list below gives test scores by gender.  These scores were taken from as a simple random sample from the population of all chemistry studentsfemale: 59 77 95 98 82 83male: 56 77 78 79 79 66 88 89 90 90 Do a hypothesis test to decide if the gender of the chemistry student is statistically significant for performance on a chem test.  (alpha = 0.05)","Creater_id":128339,"Start_date":"2016-08-19 20:09:43","Question_id":230818,"Tags":["hypothesis-testing","self-study"],"Answer_count":1,"Last_activity":"2016-08-19 20:30:31","Link":"http://stats.stackexchange.com/questions/230818/hypothesis-testing","Creator_reputation":4}
{"_id":{"$oid":"5837a579a05283111e4d4223"},"View_count":388,"Display_name":"user13253","Question_score":8,"Question_content":"I am doing some descriptive statistics of daily returns on stock indexes. I.e. if  and  are the levels of the index on day 1 and day 2, respectively, then  is the return I'm using (completely standard in literature). So the kurtosis is huge in some of these. I'm looking at about 15 years of daily data (so around  time series observations)                      means     sds     mins    maxs     skews     kurtsARGENTINA          -0.00031 0.00965 -0.33647 0.13976 -15.17454 499.20532AUSTRIA             0.00003 0.00640 -0.03845 0.04621   0.19614   2.36104CZECH.REPUBLIC      0.00008 0.00800 -0.08289 0.05236  -0.16920   5.73205FINLAND             0.00005 0.00639 -0.03845 0.04622   0.19038   2.37008HUNGARY            -0.00019 0.00880 -0.06301 0.05208  -0.10580   4.20463IRELAND             0.00003 0.00641 -0.03842 0.04621   0.18937   2.35043ROMANIA            -0.00041 0.00789 -0.14877 0.09353  -1.73314  44.87401SWEDEN              0.00004 0.00766 -0.03552 0.05537   0.22299   3.52373UNITED.KINGDOM      0.00001 0.00587 -0.03918 0.04473  -0.03052   4.23236                   -0.00007 0.00745 -0.09124 0.06405  -1.82381  63.20596AUSTRALIA           0.00009 0.00861 -0.08831 0.06702  -0.74937  11.80784CHINA              -0.00002 0.00072 -0.40623 0.02031   6.26896 175.49667HONG.KONG           0.00000 0.00031 -0.00237 0.00627   2.73415  56.18331INDIA              -0.00011 0.00336 -0.03613 0.03063  -0.22301  10.12893INDONESIA          -0.00031 0.01672 -0.24295 0.19268  -2.09577  54.57710JAPAN               0.00008 0.00709 -0.03563 0.06591   0.57126   5.16182MALAYSIA           -0.00003 0.00861 -0.35694 0.13379 -16.48773 809.07665My question is: Is there any problem?I want to do extensive time series analysis over this data - OLS and Quantile regression analysis, and also Granger Causality.Both my response (dependent) and predictor (regressor) will have this property of gigantic kurtosis. So i'll have these return processes on either side of the regression equation. If the non-normality spills over into the disturbances that will only make my standard errors high variance right? (Perhaps I need a skewness robust bootstrap?)","Creater_id":null,"Start_date":"2012-08-17 21:42:30","Question_id":34565,"Tags":["distributions","finance","skewness","kurtosis"],"Answer_count":2,"Last_activity":"2016-08-19 19:07:16","Link":"http://stats.stackexchange.com/questions/34565/gigantic-kurtosis","Creator_reputation":null}
{"_id":{"$oid":"5837a579a05283111e4d4231"},"View_count":97,"Display_name":"Arthur B.","Question_score":2,"Question_content":"Let  be a -dimensional multivariate normal random variable. Are there efficient techniques for computing ?The best I'm coming up with right now is straight MC integration, or maybe HMC sampling of the minimum itself, but I'm looking for something very fast, even if approximate.","Creater_id":8175,"Start_date":"2016-08-18 17:20:52","Question_id":230598,"Tags":["computational-statistics","multivariate-normal","minimum"],"Answer_count":1,"Last_activity":"2016-08-19 18:54:13","Link":"http://stats.stackexchange.com/questions/230598/computing-the-expectation-for-the-coordinate-wise-minimum-of-a-single-multivaria","Creator_reputation":1691}
{"_id":{"$oid":"5837a579a05283111e4d423d"},"View_count":96,"Display_name":"William","Question_score":2,"Question_content":"I have been reading James V. Stone's very nice books \"Bayes' Rule\" and \"Information Theory\". I want to know which sections of the books I did not understand and thus need to re-read further. The following notes which I wrote down seem self-contradictory:The MLE always corresponds to the uniform prior (the MAP of the uniform prior is the MLE).Sometimes a uniform prior is not possible (when the data lacks an upper or lower bound).Non-Bayesian analysis, which uses the MLE instead of the MAP, essentially sidesteps or ignores the issue of modeling prior information and thus always assumes that there is none.Non-informative (also called reference) priors correspond to the maximizing the Kullback-Leibler divergence between posterior and prior, or equivalently the mutual information between the parameter  and the random variable .Sometimes the reference prior is not uniform, it can also be a Jeffreys prior instead.Bayesian inference always uses the MAP and non-Bayesian inference always uses the MLE.  Question: Which of the above is wrong?     Even if non-Bayesian analysis does not always correspond to \"always use the MLE\", does MLE estimation always correspond to a special case of Bayesian inference?     If so, under which circumstances is it a special case (uniform or reference priors)?Based on the answers to questions [1][2][3][4] on CrossValidated, it seems like 1. above is correct.The consensus of a previous question I asked seems to be that non-Bayesian analysis cannot be reduced to a special case of Bayesian analysis. Therefore my guess is that 6. above is incorrect. ","Creater_id":113090,"Start_date":"2016-08-19 17:47:54","Question_id":230810,"Tags":["bayesian","estimation","maximum-likelihood","prior","maximum-entropy"],"Answer_count":1,"Last_activity":"2016-08-19 18:13:31","Link":"http://stats.stackexchange.com/questions/230810/when-does-the-maximum-likelihood-correspond-to-a-reference-prior","Creator_reputation":1902}
{"_id":{"$oid":"5837a579a05283111e4d423f"},"View_count":39,"Display_name":"guy","Question_score":4,"Question_content":"This is a question regarding the Generalized Linear Models book of Mccullough and Nelder. It's available here. Starting on page 204 there is an example regarding shipping incidents; one of the conclusions from fitting a model with interactions is  There is inconclusive evidence of an interaction between ship type and  year of construction, the deviance being reduced from 38.7 with 25  degrees of freedom to 14.6 with 10.The 25 degrees of freedom is obtained from  observations with  parameters. My issue is that the interaction term has 5 levels for one factor and 4 levels for the other, meaning it should account for  degrees of freedom, so we should go from 25 degrees of freedom to 13. You only get 10 degrees of freedom if you do  parameters for the interactions. Am I missing something obvious here?EDIT: Added a simulation, which is just adding to my confusion.X \u0026lt;- expand.grid(1:5, 1:4, 1:2)idx_0 \u0026lt;- c(7,15,23,31,34,39)X \u0026lt;- X[-idx_0,]f \u0026lt;- function() {  ## Simulate from the null model. The value 10.5 is the mean number of  ## incidents for the actual ship data  Y \u0026lt;- apply(X, 1, function(row) rpois(1,10.5))  fit_Y \u0026lt;- glm(Y ~ as.factor(X[,1]) * as.factor(X[,2]) + as.factor(X[,3]),                family = poisson)    deviance(fit_Y)}set.seed(1234)foo \u0026lt;- replicate(10000, f())plot(ecdf(foo))xlim \u0026lt;- c(0, max(foo))plot(function(x) pchisq(x, 13), add = TRUE, col = 'forestgreen', lwd = 3,      from = xlim[1], to = xlim[2])plot(function(x) pchisq(x, 10), add = TRUE, col = 'dodgerblue3', lwd = 3,      from = xlim[1], to = xlim[2])","Creater_id":5339,"Start_date":"2016-08-18 20:49:22","Question_id":230625,"Tags":["generalized-linear-model","poisson-regression"],"Answer_count":0,"Last_activity":"2016-08-19 18:04:34","Link":"http://stats.stackexchange.com/questions/230625/is-this-degrees-of-freedom-calculation-from-mccullagh-and-nelder-wrong","Creator_reputation":3124}
{"_id":{"$oid":"5837a579a05283111e4d4241"},"View_count":30,"Display_name":"pche8701","Question_score":1,"Question_content":"This is possibly a silly conceptual question, ... but anyway:Imagine I have a function:where  for example.For a naive uncertainty propagation I can MC (Monte Carlo) sample  and , and find: hist() =  (unnormalized),by passing my samples through  and placing each subsequent output onto a histogram. This histogram is 1D, because  has a scalar output. If I want to do a Laplace approximation on this histogram / uncertainty distribution, I would require a Hessian but I'm at a loss as to where this Hessian should come from. Should it:Come directly from an equation for the histogram (which I don't have since I only have a frequency distribution) ORBe built on  . With this approach the Hessian would be 2x2 because  is a function of two variables. However as stated before my histogram output is only 1D, so my Laplace approximation should only have a scalar s.d. to work withHence I'm not sure which method (or maybe none of these?) is the correct way for calculating the Hessian for the Laplace approximation in this problem. Each way seems to be difficult in some respect. Thanks!","Creater_id":117574,"Start_date":"2016-08-18 18:52:38","Question_id":230613,"Tags":["approximation","hessian"],"Answer_count":1,"Last_activity":"2016-08-19 17:45:20","Link":"http://stats.stackexchange.com/questions/230613/hessian-for-laplace-approximation-in-uncertainty-propagation","Creator_reputation":39}
{"_id":{"$oid":"5837a579a05283111e4d4243"},"View_count":81,"Display_name":"Kate","Question_score":1,"Question_content":"I am analyzing data from cohort of 500 calves investigating the impact of disease on growth.My outcome variables are normally distributed, continuous data. I am using hierarchical models with calf nested within farms and testing for the longer term impacts of disease.The problem I am having is with how to include disease data. I have variables for the number of weeks a calf had disease and the total score over a validated threshold for diagnosisAs I am inexperienced in uploading images, here are the tabulated results of the data above:Disease Duration (weeks) 0   1    2   3   4   5   6 Frequency               266 128  50  33   8   5   2Total Score  0   1   2   3    4   5   6   7   8   9  10  13  14  15 Frequency   266  88  51  30  20  13   2   6   4   5   3   1   2   1 Obviously, this data is far from normal. But there a lot of levels to use a dummy coded categorical variable, and I think an ordinal scale better represents the data. What do you think it the best way to include this data as an independent variable in my LME models? (n.b. I don't include both in the same model just one or the other)The models do return results without convergence errors or other warnings when I include these variables but it doesn't feel like very good practice and I am unsure of what sort of transformation I could do to make this data better (e.g. log transformation leaves the data looking very odd and plots of the raw data make it look like a linear relationship is the most likely) Here is an example of what I would like to improve:(adj_w_63 - calf weight,weeks_brd - weeks with disease (as described above),rid - a normally distributed continuous variable,milksolids_total - a normally distributed continuous variable)library(lme4)model1\u0026lt;-lmer(adj_w_63 ~ weeks_brd + rid + milksolids_total + (1|farm_ac), data=comp)summary(model1)Linear mixed model fit by REML ['lmerMod']Formula: adj_w_63 ~ weeks_brd + rid + milksolids_total + (1 | farm_ac)   Data: compREML criterion at convergence: 3247Scaled residuals:     Min      1Q  Median      3Q     Max -3.5180 -0.5525 -0.0458  0.5945  6.1674 Random effects: Groups   Name        Variance Std.Dev. farm_ac  (Intercept) 30.10    5.487    Residual             83.37    9.131   Number of obs: 443, groups:  farm_ac, 11Fixed effects:                 Estimate Std. Error t value(Intercept)      68.06279    3.30996  20.563weeks_brd        -1.00200    0.42089  -2.381rid               0.11010    0.04981   2.210milksolids_total  0.19904    0.07679   2.592Correlation of Fixed Effects:            (Intr) wks_br rid   weeks_brd   -0.174              rid         -0.285  0.141       mlkslds_ttl -0.795  0.038 -0.016Thank you so much for your help.","Creater_id":128328,"Start_date":"2016-08-19 16:42:06","Question_id":230802,"Tags":["r","mixed-model","data-transformation","ordinal","lme4"],"Answer_count":1,"Last_activity":"2016-08-19 17:37:32","Link":"http://stats.stackexchange.com/questions/230802/including-ordinal-independent-variables-in-a-linear-mixed-effects-model-using-t","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4245"},"View_count":53,"Display_name":"eagerbeaver","Question_score":1,"Question_content":"Imagine my dataset consists of three factors relating to, as an example, gross domestic product. I want to create a model estimating GDP given a percentage weighting of each factor.If my three factors were the number of deck chairs, total honey consumption, and stock market value then I would want an estimate based on a model which, for example, gave a 50% weighting to deck chairs and 25% to the other two factors.One thought that I had was standardising, multiplying by x%, and adding up to create a new \"composite\" variable. However, I have no formal stats training and am unsure what the downsides of this solution would be. In other words, I want to know why a certain solution is a good idea too.The application, if it helps, is a consumer-facing data service. I want my customers to create their own models using their own weights/hypothesis about the data.","Creater_id":128251,"Start_date":"2016-08-19 04:37:40","Question_id":230676,"Tags":["regression","multiple-regression","weighted-data"],"Answer_count":1,"Last_activity":"2016-08-19 17:14:41","Link":"http://stats.stackexchange.com/questions/230676/how-to-weight-factors-in-my-model","Creator_reputation":8}
{"_id":{"$oid":"5837a579a05283111e4d4247"},"View_count":17,"Display_name":"Abiologist","Question_score":1,"Question_content":"This question is deliberately similar to a previous question: test for difference between two differences (proportions) because although the answer by StevenP was very useful, it only answered part of the question. I have data for an analyte measured in the same people at two different times, and a fixed effect factor (genotype) which separates the subjects into 3 groups. We suspect that the percentage change (between times 1 and 2) between the 3 groups differs (note that the absolute change is not thought to be so important).StevenP's answer was to use ANOVA with repeated measures and to check assumptions (presumably the sphericity assumption is not necessary as there are only 2 times). However, the interaction between group x time will give an indication of whether the absolute changes over time are significantly different between the 3 groups, not the percentage changes.My question is, therefore, how to proceed with a legitimate ANOVA assessment of percentage changes ? I suppose this question is really about how the variance is assessed and included in the analysis when the outcome is a combination of several variables. In the case of the absolute differences, for the combination outcome \"measurement at time 2 minus mesaurement at time 1\", a repeated measures ANOVA nicely takes into account the variance at time 2 and the variance at time 1.However, what happens when the combination outcome is \"measurement at time 2 minus measurement at time 1, divided by measurement at time 1\" ?? My own feeling is that the variance at time 1 is already included in the percentage along with the variance at time 2, when the proportion or percentage was created (for each subject), and therefore a simple one-way ANOVA is needed for the percentage data only - is this correct ?Many thanks in advance.","Creater_id":128319,"Start_date":"2016-08-19 15:58:01","Question_id":230795,"Tags":["anova","proportion"],"Answer_count":0,"Last_activity":"2016-08-19 16:45:57","Link":"http://stats.stackexchange.com/questions/230795/anova-for-difference-between-three-percentage-differences","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4249"},"View_count":496,"Display_name":"jainp","Question_score":2,"Question_content":"I am trying to use vowpal wabbit to do Latent Dirichlet Analysis (LDA) on a corpus. I am running into a few issues regarding the output. To test it, I was using a file with just 3 lines (3 documents as per the VW input format):| now let fit a topic model on this dataset| now let a good model on this dataset| this is a document about sportsI ran VW LDA in the following manner:vw --lda 2 --lda_D 3 --readable_model lda.model.txt -k --passes 10    --cache_file doc_tokens.cache -d 1.txt -p prediction.dat --lda_rho 0.1The code runs fine and generates two output files prediction.dat and lda.model.txt. My questions related to it are:Except the first column, both the output files have a sequence of floating point numbers. Like262130 0.100008 0.100009262131 0.100013 0.100021262132 0.100008 0.100010262133 0.100018 0.100008262134 0.100005 0.100008262135 0.100010 0.100007262136 0.100008 0.100026262137 0.100005 0.100012262138 0.100008 0.100014262139 0.100005 0.100018262140 0.100006 0.100007262141 0.100006 0.100006262142 0.100019 0.100023262143 0.100019 0.100007I thought giving --readable_model will give the strings representing the topics. Am I doing something wrong? No matter how many documents I give, the output file (lda.model.txt) has 262143 rows of examples. Why is it doing that? ","Creater_id":69812,"Start_date":"2015-02-25 12:40:05","Question_id":139300,"Tags":["topic-models","vowpal-wabbit"],"Answer_count":2,"Last_activity":"2016-08-19 16:45:54","Link":"http://stats.stackexchange.com/questions/139300/vowpal-wabbit-lda","Creator_reputation":13}
{"_id":{"$oid":"5837a579a05283111e4d424b"},"View_count":32,"Display_name":"pooja p","Question_score":1,"Question_content":"Can MSE be used to measure the quality of fit for beta regression?","Creater_id":115086,"Start_date":"2016-05-09 15:35:11","Question_id":211708,"Tags":["mse","beta-regression"],"Answer_count":1,"Last_activity":"2016-08-19 16:33:24","Link":"http://stats.stackexchange.com/questions/211708/mse-as-a-measure-of-quality-of-fit-for-beta-regression","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d424d"},"View_count":29,"Display_name":"Stuart Lacy","Question_score":1,"Question_content":"I've got a dataset with the dependent variable being a proportion in (0, 1), and 6 categorical predictors all with between 2-5 levels. I've fit a beta regression in R using the betareg package, and now want to determine the importance of each predictor. If this were a standard linear regression I'd use an ANOVA and look at the F-test scores. However, there isn't an ANOVA method available for betareg objects. Instead, the documentation recommends using likelihood ratio tests.What I've done is then calculate the log likelihood for the full model minus each of the predictors in turn. The bigger the difference between the loglikelihood of the full model and the model minus a predictor, the more 'important' that predictor is to the final model.This is useful in that I get a measure of variable importance, but due to the large sample size (240 000 rows), the p-values are miniscule when I run a likelihood ratio tests for each reduced model compared to the full model.Is there a more formal way of testing the variable importance in this scenario, or is this difference in loglikelihood as good as I can get?","Creater_id":56832,"Start_date":"2016-08-04 13:16:28","Question_id":228328,"Tags":["regression","inference","beta-regression"],"Answer_count":1,"Last_activity":"2016-08-19 16:26:08","Link":"http://stats.stackexchange.com/questions/228328/how-to-assess-importance-of-regression-predictors-with-large-sample-size","Creator_reputation":156}
{"_id":{"$oid":"5837a579a05283111e4d424f"},"View_count":40,"Display_name":"As3adTintin","Question_score":1,"Question_content":"I'm working through Joe Blitzstein's online Stat 110 and am caught up on this one part.It's Practice 7/HW7/Question 7/ part (c).(http://projects.iq.harvard.edu/files/stat110/files/strategic_practice_and_homework_7.pdf)The Full Question:Shakespeare wrote 884647 words in his known works. Many words are used more than once; the number of distinct words in his known writings is 31534. Suppose a new poem of Shakespear were uncovered. Give a good prediction of the number of words in the new poem that do not appear anywhere in Shakespear's previously known works.Let N be the number of distinct words that he knew, and assume these words are numbered from 1 to N. Suppose for simplicity that Shakespeare wrote only two plays, A and B. The plays are reasonably long and they are of the same length. Let Xj be the number of times that word j appears in play A and Yj be the number of times it appears in play B, for 1 \u0026lt;= j \u0026lt;=N.part(a) establishes taht Xj and Yj are Poissonpart(b) let the number of occurrences of the word \"eyeball\" in the two plays be independent Pois(lambda) r.v.s. Show that the probability that \"eyeball\" is used in play B but not in play A is e^(-lambda)(lambda - lambda^(2)/2! + lambda^(3)/3!...)part(c) Now assume that lambda from (b) is unknown and is itself taken to be a random variable to refelct this uncertainty. So elt lambda have a PDF f0. Let X be the number of times the word \"eyeball\" appears in play A and Y be the corresponding value for play B. Assume that the conditional distribution of X,Y given lambda is that they are independent Pois(lambda) r.v.s. Show that the probability that \"eyeball\" is used in play B not in play A is the alternating series P(X=1) - P(X-2) + P(X=3) ...Hint: condition on lambda and use (b)Answer:Now lambda is a random variable. Given lambda, the calculation from (b) holds. By the law of total probability,P(X=0, Y\u003e0) = \\int P(X=0,Y\u0026gt;0 | lambda) f0(lambda) \\, dlambda = \\int P(X=1|lambda) f0(lambda) \\, dlambda  - \\int P(X=2 | lambda) f0(lambda) \\, dlambda + \\int P(X=3|lambda) f0(lambda) \\, dlambda  .....Where I get Caught up:I trying to figure out how we get from P(X=0, Y\u003e0|lambda) to P(x=1 | lambda), etc.. (which is basically the essence of the question, I think).I think I understand part (b), but am trying to figure out how it relates. I see that they both alternate, but that's about it.Any suggestions? Thank you!","Creater_id":116074,"Start_date":"2016-08-18 19:08:04","Question_id":230614,"Tags":["probability","conditional-probability","pdf"],"Answer_count":0,"Last_activity":"2016-08-19 16:23:39","Link":"http://stats.stackexchange.com/questions/230614/conditional-join-probability-law-of-total-probability","Creator_reputation":106}
{"_id":{"$oid":"5837a579a05283111e4d4251"},"View_count":34,"Display_name":"Peter Albertson","Question_score":1,"Question_content":"I want to test my  linear mixed effect model for interactions using MCMCglmm:MCMCglmm(cbind(dv1, dv2, dv3, dv4) ~ time*treat*gen*age,+ family=c(\"poisson\",\"gaussian\",\"poisson\",\"gaussian\"), + random=~individual, data=mydata)Giving the following error:Error in MCMCglmm(cbind(dv1, dv2, dv3, dv4) ~ time * treat * :R-structure miss-specified: each residual must be unique to a data pointUsing just one dependent variable, the MCMCglmm works just fine.dv1 is count data,dv2 is continous numeric,dv3 is count data,dv4 is continous numeric,individual is a group of 20 individuals with repeated measurestime is a fixed effect with 7 timepoints per individualgen is a factor of 2treat is a factor of 2age is a factor of 2 (old and young)Anybody got an idea?","Creater_id":126632,"Start_date":"2016-08-19 16:13:29","Question_id":230798,"Tags":["multivariate-analysis","interaction","glmer"],"Answer_count":0,"Last_activity":"2016-08-19 16:13:29","Link":"http://stats.stackexchange.com/questions/230798/multivariate-mcmcglmm-pmcmc-for-interaction","Creator_reputation":8}
{"_id":{"$oid":"5837a579a05283111e4d4253"},"View_count":32,"Display_name":"Jamie Murphy","Question_score":1,"Question_content":"I am new to R coding and was hoping someone could help. Am trying to create a regression line where the dependent variable is a proportion (I only have the proportion, not the denominator and numerator). With it being a proportion a linear regression line isn't appropriate as it needs to sit between 0-1, I think a sigmoid shape would be best. So far I have had limited success with the Loess function, however ideally I want to be able to gain the coefficients and AIC from the regression. So far the best shape I have been able to obtain is using the binomial family in ggplot2, but I don't think this is an appropriate distribution.I have attached my code, am wondering if anyone could suggest an improvement. c \u0026lt;- ggplot(dat, aes(y=ITN_Coverage, x=Study_Date))c + stat_smooth(method =\"glm\",  method.args = list(family=\"binomial\"), size=0.5, col = \"black\") + geom_point(aes(color = Country)) + labs(title = \"Scatter plot: Insecticide treated net coverage against year\", x= \"Study date\", y= \"ITN coverage\")","Creater_id":128327,"Start_date":"2016-08-19 16:12:39","Question_id":230797,"Tags":["r","ggplot2"],"Answer_count":0,"Last_activity":"2016-08-19 16:12:39","Link":"http://stats.stackexchange.com/questions/230797/glm-family-choice-for-proportional-data-in-ggplot2","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4255"},"View_count":348,"Display_name":"Amelio Vazquez-Reina","Question_score":8,"Question_content":"This is a rather general question (i.e. not necessarily specific to statistics), but I have noticed a trend in the machine learning and statistical literature where authors prefer to follow the following approach: Approach 1: Obtain a solution to a practical problem by formulating a cost function for which it is possible (e.g. from a computational standpoint) to find a globally optimal solution (e.g. by formulating a convex cost function).rather than:Approach 2: Obtain a solution to the same problem by formulating a cost function for which we may not be able to obtain a globally optimal solution (e.g. we can only get a locally optimal solution for it).Note that rigorously speaking the two problems are different; the assumption is that we can find the globally optimal solution for the first one, but not for the second one.Other considerations aside (i.e. speed, ease of implementation, etc.), I am looking for:An explanation of this trend (e.g. mathematical or historical arguments)Benefits (practical and/or theoretical) for following Approach 1 instead of 2 when solving a practical problem.","Creater_id":2798,"Start_date":"2011-05-03 07:46:00","Question_id":10277,"Tags":["optimization","function"],"Answer_count":2,"Last_activity":"2016-08-19 16:05:55","Link":"http://stats.stackexchange.com/questions/10277/advantages-of-approaching-a-problem-by-formulating-a-cost-function-that-is-globa","Creator_reputation":3863}
{"_id":{"$oid":"5837a579a05283111e4d4257"},"View_count":65,"Display_name":"OgW","Question_score":3,"Question_content":"I'm trying to understand the difference between these a bit better. I understand pretty well how random forests work but I guess I'm more hazy on rulefit and how exactly it's different. I know rulefit will incorporate linear components and so can fit linear trends better. What other ways do they differ?","Creater_id":94538,"Start_date":"2016-04-23 07:48:31","Question_id":208930,"Tags":["machine-learning","random-forest"],"Answer_count":1,"Last_activity":"2016-08-19 16:01:53","Link":"http://stats.stackexchange.com/questions/208930/difference-between-rulefit-and-random-forest","Creator_reputation":18}
{"_id":{"$oid":"5837a579a05283111e4d4259"},"View_count":1378,"Display_name":"Dave M","Question_score":3,"Question_content":"Is it possible to calculate r-squared for an \"average model\"?Lets say I have 4 different response variables that I want to model to a set (or subset) of 4 independent variables.  I'd then like to compare the variance explained (R-squared) for the best model for each response variable.  Unfortunately, for each of the 4 response variables there is no clear best model, so I need to model average to account for this model selection uncertainty.  Now, is there still a way to compare the variance explained by the averaged model for each of the response variables?  Or do I simply compare the r-squared values of the global model for each response variable?My question is similar to this old unanswered question;Generalized  for average modelThanks for any help and please let me know if I can improve this question.","Creater_id":38125,"Start_date":"2014-12-12 15:31:36","Question_id":128915,"Tags":["model-selection","r-squared"],"Answer_count":2,"Last_activity":"2016-08-19 15:03:10","Link":"http://stats.stackexchange.com/questions/128915/model-uncertainty-model-averaging-and-r-squared-r2","Creator_reputation":124}
{"_id":{"$oid":"5837a579a05283111e4d425b"},"View_count":28,"Display_name":"Ashley","Question_score":2,"Question_content":"I am using the lmer function in R to examine the effect of skill (continuous) and context on eye movement data.  I am new to lmer and not by nature statistically inclined. It is my understanding that it is best to include random intercepts and random slopes for subject and item like below.  lmer(GD~condition+skill+condition:skill*(1+condition*skill|subject)*(1+condition*skill|item)However, I am particularly interested in the effect of skill on the eye movement record.  It seems to me that the subject random effect may be diminishing the ability of the model to find significant skill interactions with context (condition).  Am I correct in thinking the subject random effect is parceling out the variability for my skill measure?Also, I am really only interested in those who scored either high or low on the skill measure.  The dataset I am using contains those in the middle where there is a lot of variability.  Would it be appropriate to remove these subjects from analysis?Finally, I am unsure whether or not subject should be nested within skill as each subject was given only one value for skill. Any guidance would be greatly appreciated.","Creater_id":128322,"Start_date":"2016-08-19 14:12:23","Question_id":230788,"Tags":["lmer"],"Answer_count":1,"Last_activity":"2016-08-19 14:45:32","Link":"http://stats.stackexchange.com/questions/230788/specifying-random-effects-for-individual-differences-data","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d425d"},"View_count":14,"Display_name":"autumntimes","Question_score":1,"Question_content":"pvexp.result1 \u0026lt;- plm(export ~ partycongress + inflation + crudepetrol + lngdp + lngrowth + lnlcratio, data=private.export, index=\"year\", model=\"within\", effect=\"time\") summary(pvexp.result1)I want to assess the effect of partycongress on export while controlling time and inflation + crudepetrol + lngdp + lngrowth + lnlcratio. This code trunk didn't work. Please help!","Creater_id":100231,"Start_date":"2016-08-19 14:37:57","Question_id":230792,"Tags":["regression","fixed-effects-model"],"Answer_count":0,"Last_activity":"2016-08-19 14:37:57","Link":"http://stats.stackexchange.com/questions/230792/time-fixed-effect-with-a-single-country","Creator_reputation":106}
{"_id":{"$oid":"5837a579a05283111e4d425f"},"View_count":55,"Display_name":"floyd","Question_score":-2,"Question_content":"Why should I select variables due to my intuition if there are builtin functions in sklearn python like SelectKBest() and PCA()If I plot graphs of features of the data to see if they can detect the pattern of my categorical response:does that mean that I should plot hundreds and thousands of scatter plots?Additional question:In my analysis I created a variable resulting from the product of 2 variables. This variable gives nice performancebut if I use the log() of this variable it gives me an even better performance.I don't understand completely why taking the log of that variable resulted in a better perfomance.","Creater_id":116480,"Start_date":"2016-05-23 16:41:08","Question_id":214243,"Tags":["feature-selection"],"Answer_count":2,"Last_activity":"2016-08-19 14:34:35","Link":"http://stats.stackexchange.com/questions/214243/why-should-i-choose-features-or-plot-them-manually-when-there-are-built-in-funct","Creator_reputation":77}
{"_id":{"$oid":"5837a579a05283111e4d4261"},"View_count":24,"Display_name":"rconway91","Question_score":0,"Question_content":"I have a dataset with known errors in both the X and Y and want to perform a simple linear regression. From reading other posts, it seems I want to TLS over OLS due to the presence of error in both variables. However, the error is no constant, so I also want to weight measurements. It seems that weighting with the inverse of the variance is common in weighted OLS. Could I weight with the inverse of the sum of X and Y variance for my TLS regression?","Creater_id":86169,"Start_date":"2016-08-19 13:48:54","Question_id":230785,"Tags":["measurement-error","weighted-regression","total-least-squares"],"Answer_count":0,"Last_activity":"2016-08-19 13:48:54","Link":"http://stats.stackexchange.com/questions/230785/what-weights-for-weighted-total-least-square-tls-regression","Creator_reputation":57}
{"_id":{"$oid":"5837a579a05283111e4d4263"},"View_count":32,"Display_name":"pidosaurus","Question_score":1,"Question_content":"In rough terms, I am given a t-statistic of 3.00 of the coefficient of variable , ,  after regressing the independent variable  on the dependent : Y = \\alpha + \\beta_1 X.I have previously formed the null hypothesis that  explains .The normal interpretation would be:\"At the significance level of 1% I cannot reject the null hypothesis that  explains \". However, adding some controls in different multiple regressions, say  in one specification and  in another, the t-statistic of  drops to roughly 2.00, significant at 5% only, for only a couple of the tested control specifications.What do I say then for interpreting the null hypothesis? \"I cannot reject the null hypothesis that  explains  at the significant level %1 or 5%? I understand that the null hypothesis considers one specification only, but what is the common approach?Added clarificationIn a study, consider I form the variables based on answers on questions of a specific group of people.I then runY=\\alpha+\\beta_1X ~~~~~H_0: \\beta_1=0andY=\\alpha+\\beta_1X+\\gamma Z ~~~~~H_0: \\beta_1=0Next day, as an added robustness check and assuming nothing should/has change/changed regarding the answers for forming the variables ,  and , I ask the same group of people the same questions, and I run the same regression on the new dataset.Now,  of the simple model is still significant at 1%, but it is significant at 5% only for the multiple regression model.How do I treat this case?In rough terms, the two dataset should give the same results.But now, the new dataset is not exactly replicating the result of the former dataset, in terms of statistical significance. How do I interpret this result in my study as a concluding sentence?I say\"I cannot reject the null hypothesis that  explains  at the significant level %1 or 5%? ","Creater_id":44627,"Start_date":"2016-08-19 12:55:58","Question_id":230779,"Tags":["hypothesis-testing","multiple-regression"],"Answer_count":1,"Last_activity":"2016-08-19 13:42:29","Link":"http://stats.stackexchange.com/questions/230779/how-to-interpret-a-1-significant-relation-when-adding-a-control-diminishes-the","Creator_reputation":164}
{"_id":{"$oid":"5837a579a05283111e4d4265"},"View_count":30,"Display_name":"user27108","Question_score":1,"Question_content":"I have a problem where I need by vectors to be invariant to shifting the feature around. The order of the features doesn't matter as long as their relative distances from one in other is preserved (I think, given a priori knowledge about the problem)For example if my feature vector is(1, 2, 3, 4, 5)then the vector(5, 1, 2, 3, 4) should produce the same classification (binary problem)Currently I'm using an SVM with an RBF kernel and getting decent results but my suspicion is that for the few examples it is misclassifying is due to the possible shift. I thought of just producing artificial examples by taking all images with a -1 label (since this is a detection problem and the feature should highlight a defect) and shifting them. I've also seen papers of creating artificial examples by doing something like this to the support vectors.My question is either (1) is this a sound idea or is there some other approach to try and (2) how can I evaluate this classifier. I've been using 10-fold cross validation but that would possibly put artificial examples created from real examples in both the training set and testing set giving my a far optimistic estimate. I could also just split before hand(80/20) and create examples from the 80% but given that I don't have millions of examples and unbalanced classes I get an estimator with pretty high variance.","Creater_id":102600,"Start_date":"2016-08-19 13:39:09","Question_id":230784,"Tags":["cross-validation","svm"],"Answer_count":0,"Last_activity":"2016-08-19 13:39:09","Link":"http://stats.stackexchange.com/questions/230784/shift-invariance-in-an-svm","Creator_reputation":8}
{"_id":{"$oid":"5837a579a05283111e4d4267"},"View_count":26,"Display_name":"Viola Green","Question_score":0,"Question_content":"The analysis consists of comparing the means for 6 phonological constraints to the level of indifference, defined as 3.5. The same participants provided judgments for all the 6 constraints, which makes it a repeated measures design. I basically need to establish if subjects are sensitive to any of these constraints. If they are, then the means should be significantly different from 3.5. The code that was advised to me is:summary( lme(offsetVal ~ 0 + constraint, random=~1|subject, data=FRdata) )Where the offsetVal is a number that you get when subtracting the 'indifference level' (3.5) from the means for each constraint from each participant.OUTPUT:Linear mixed-effects model fit by REML Data: FRdata        AIC      BIC    logLik  226.4168 246.4153 -105.2084Random effects: Formula: ~1 | subject        (Intercept)  ResidualStdDev:   0.1704487 0.6919447Fixed effects: offsetVal ~ 0 + constraint                Value Std.Error DF    t-value p-valueconstraint1 -0.35000 0.1781573 75 -1.9645562  0.0532constraint2 -0.20000 0.1781573 75 -1.1226036  0.2652constraint3  0.29375 0.1781573 75  1.6488240  0.1034constraint4  0.08125 0.1781573 75  0.4560577  0.6497constraint5  0.36875 0.1781573 75  2.0698003  0.0419constraint6 -0.03750 0.1781573 75 -0.2104882  0.8339 Correlation:             cnstr1 cnstr2 cnstr3 cnstr4 cnstr5constraint2 0.057constraint3 0.057  0.057constraint4 0.057  0.057  0.057constraint5 0.057  0.057  0.057  0.057constraint6 0.057  0.057  0.057  0.057  0.057 Standardized Within-Group Residuals:        Min          Q1         Med          Q3         Max -3.30462921 -0.42615935  0.04149831  0.53233941  2.33326486 NOW my question is: how do I get the effect size for this analysis? I tried lm.beta(mymodel) with a QuantPsyc package, but I get an error:Error in summary(MOD)$coef[-1, 1] : incorrect number of dimensionsNot sure where to go from here. Also, I was advised not to use the correction for this analysis, because the constructs are entered into the model simultaneously, like in a regression. I was told that correction is not needed in this particular case. Can someone confirm that or elaborate on that?","Creater_id":123259,"Start_date":"2016-08-19 13:31:45","Question_id":230782,"Tags":["effect-size"],"Answer_count":0,"Last_activity":"2016-08-19 13:31:45","Link":"http://stats.stackexchange.com/questions/230782/effect-size-standarized-beta-for-lme","Creator_reputation":20}
{"_id":{"$oid":"5837a579a05283111e4d4269"},"View_count":88,"Display_name":"manetsus","Question_score":1,"Question_content":"I have started learning bioinformatics. There are some matter of finding expected value. But I think I am very weak in calculating such types of things. As expected value is related to statistics, its explanation is skipped in bioinformatics. So, I am posting it here.Question:Suppose, I have 500 strings, each having length 1000.Now, I have to calculate the expected number of occurrences of a sub-string having length exactly 9.Notice that, the string contains only four letters A, T, G, C with same probability (each 0.25).Another thing to be noted: Overlapping strings should be counted.My Approach:The probability of existing a 9-length sub-string among all 9-length sub-strings = The number of occurrences of a 9-length sub-string in a string having length 1000 = If the number of such string becomes 500, then the number of occurrences would be = But I did wrong somewhere, may be in assumption or in calculation. Could you please guide me to get the actual solution?Source:This problem is a part of Bioinformatics course track in Coursera.Accuracy:Allowable error = 0.0001","Creater_id":89481,"Start_date":"2016-08-19 11:08:54","Question_id":230752,"Tags":["self-study","expected-value"],"Answer_count":1,"Last_activity":"2016-08-19 13:31:38","Link":"http://stats.stackexchange.com/questions/230752/what-is-the-expected-number-of-occurrences-of-a-9-mer-in-500-random-dna-strings","Creator_reputation":108}
{"_id":{"$oid":"5837a579a05283111e4d426b"},"View_count":152,"Display_name":"chris elgoog","Question_score":5,"Question_content":"With the well known \"Survival of passengers on the Titanic\" data set I get a strange behaviour by plotting the fare vs. the age (see below). Without a constraint on Pclass the correlation is positive. In contrast for all Pclasses the correlations seems to be negative.I assume that's a form of \"Simpson's Paradox\". But I am not sure. How can this behaviour best explained for this special case?# df is a pandas dataframe with the titanic data set# see https://www.kaggle.com/c/titanicimport seaborn as snssns.jointplot(\"Age\", \"Fare\", df, kind='reg')sns.lmplot(\"Age\", \"Fare\", df, col=\"Pclass\")","Creater_id":56090,"Start_date":"2015-12-04 09:42:26","Question_id":185047,"Tags":["simpsons-paradox"],"Answer_count":1,"Last_activity":"2016-08-19 13:29:32","Link":"http://stats.stackexchange.com/questions/185047/is-this-simpsons-paradox-on-the-titanic-data-set","Creator_reputation":43}
{"_id":{"$oid":"5837a579a05283111e4d426d"},"View_count":23,"Display_name":"HIL","Question_score":1,"Question_content":"I ran mediation analyses with Lavaan (using 5000 iterations bootstrapping).My model is a simple mediation model where M mediates the relationship between X and Y.But there is also a moderator (Mod) that influences the relationship between X and M and between X and Y.Looking at the results, I get the following:Regressions:               Estimate  Std.Err  Z-value  P(\u0026gt;|z|) Y ~                                                 X  (c1)   -0.017    0.008   -2.116    0.034  Mod (c2)   -9.613    3.655   -2.630    0.009  X*Mod (c3)    0.017    0.007    2.475    0.013 M ~                                            X  (a1)    0.003    0.002    1.463    0.143  covariate1           0.004    0.003    1.564    0.118  covariate2               0.038    0.028    1.387    0.165  covariate3           0.004    0.004    1.053    0.293  Mod  (a2)    1.874    0.972    1.927    0.054  X*Mod (a3)   -0.003    0.002   -1.766    0.077Y ~                                                Mod  (b1)   -0.892    0.437   -2.040    0.041 covariate1          -0.007    0.014   -0.486    0.627 covariate2               0.600    0.156    3.850    0.000 covariate3           0.059    0.027    2.195    0.028Variances:               Estimate  Std.Err  Z-value  P(\u0026gt;|z|) Y               0.593    0.073    8.142    0.000 M          0.021    0.005    4.563    0.000Defined Parameters (conditional effects at different levels of Mod):               Estimate  Std.Err  Z-value  P(\u0026gt;|z|)directXM1          -0.000    0.000   -1.129    0.259directXM2           -0.001    0.000   -2.646    0.008directXM3         -0.001    0.000   -2.846    0.004indirectXY1           0.000    0.000    1.017    0.309indirectXY2           0.001    0.000    1.784    0.074indirectXY3          0.001    0.001    1.703    0.089directXY1           0.001    0.002    0.381    0.703directXY2            0.003    0.001    1.951    0.051directXY3          0.005    0.002    2.974    0.003I notice that the X*Mod on M (a3) is not significant (p = 0.077), but the confidence interval for this  [-0.008,  -0.001] (confidence interval comes from the bootstrapped results)Furthermore, I also noticed that the indirect effect is not significant (at any levels of the moderator).How should I interpret this? I was thinking of removing XMod on M from the model... but then 0.07 is not that far from significant (and looking at the CI...)?The indirect effect is not significant... When I remove this interaction (XMod on M) and calculate the indirect effect as a1*b1, the indirect effect is even less significant (p=0.104).I am totally lost on what it means that the indirect effect is not significant. Does that mean that there is no real mediation effect?Many thanks!BestH","Creater_id":113780,"Start_date":"2016-08-19 13:00:45","Question_id":230780,"Tags":["interaction","mediation","lavaan"],"Answer_count":0,"Last_activity":"2016-08-19 13:00:45","Link":"http://stats.stackexchange.com/questions/230780/interpreting-indirect-effect-in-moderated-mediation","Creator_reputation":15}
{"_id":{"$oid":"5837a579a05283111e4d426f"},"View_count":47,"Display_name":"Murray Lynch","Question_score":1,"Question_content":"What are some examples of specific machine learning algorithms being used in the private sector? I've far and wide to no avail, and my same question on Quora has been unanswered for a while now.Thanks in advance!","Creater_id":128316,"Start_date":"2016-08-19 12:54:07","Question_id":230778,"Tags":["machine-learning","application","example"],"Answer_count":0,"Last_activity":"2016-08-19 12:58:06","Link":"http://stats.stackexchange.com/questions/230778/real-world-applications-of-ml-algorithms-in-the-private-sector","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4271"},"View_count":14,"Display_name":"skip","Question_score":2,"Question_content":"I have a binary outcome variable (Disease/No disease).In a diagnostic test for this disease, 20 different sensors record a time series value. These time series are relatively correlated, but each sensor measures a different thing.In order to predict the disease, the standard in my field is to average all of the sensors' outputs together and use the maximum value obtained at any one time as a regression covariate.My hypothesis is that the dynamics of the time series are very important, and could be more predictive than using just the maximum value of all the time series averaged together. Can anyone help me conceptualize how I could fit a predictive model to this data? I have looked around for examples of using time series as covariates but I have been unsuccessful.","Creater_id":77927,"Start_date":"2016-08-19 12:32:49","Question_id":230774,"Tags":["predictive-models"],"Answer_count":0,"Last_activity":"2016-08-19 12:32:49","Link":"http://stats.stackexchange.com/questions/230774/using-a-time-series-as-covariate-in-regression-model","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d4273"},"View_count":68,"Display_name":"yetanotherion","Question_score":1,"Question_content":"As far as I understood,  when implementing a learning algorithm that integrates model selection/hyper parameters tuning in itself, nested cross-validation is necessary to lower the bias in the performance estimate.I’d propose to summarize the algorithm to compute the estimation of that performance as:performances = []for training, test in partition(data):    model = find_best_model(data_to_choose_best_model=partition(training))    performances.append(model.fit_and_measure_performance(training, test))return some_method_to_aggregate_for_ex_average(performances)As we don’t have an infinite amount of time, we’re obliged to restrict the number of model/parameters to browse during find_best_model. Taking aside the fact that we don’t use the models we don’t know, I’d enumerate two ways of selecting that subset of model/parameters:experience/gut feeling,exploration/plotting some curves to evaluate how an algorithm reacts to a given data.My question is the following:Is there is a way to implement 2., for example, in the way to select/explore the data, that would permit lowering the bias it creates ? Indeed, implementing 2 ourselves, i.e. out of the “find_best_model” method in the algorithm above, seems to be a “seemingly benign short cut” that may induce a non negligible “magnitude of [...] bias” (taking expressions from the very instructive first answer in Use of nested cross-validation). Said otherwise, it seems similar to tuning hyper parameters without going through nested cross-validation.","Creater_id":128233,"Start_date":"2016-08-19 10:22:30","Question_id":230749,"Tags":["cross-validation","nested"],"Answer_count":0,"Last_activity":"2016-08-19 12:27:06","Link":"http://stats.stackexchange.com/questions/230749/choosing-an-algorithm-without-nested-cross-validation","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4275"},"View_count":39,"Display_name":"F\u0026#225;bio Perez","Question_score":3,"Question_content":"When training deep architectures, I want to be able to select hyperparameters in a feasable time. However, if my dataset is very big, this can be very time-consuming.Is it OK to reduce the size of the dataset to see which hyperparameters perform better and then train the full dataset? Would this bias my hyperparameters selection since the training setups are not the same?","Creater_id":128306,"Start_date":"2016-08-19 12:05:39","Question_id":230768,"Tags":["machine-learning","deep-learning"],"Answer_count":1,"Last_activity":"2016-08-19 12:15:45","Link":"http://stats.stackexchange.com/questions/230768/is-it-ok-to-decrease-the-size-of-a-dataset-to-speedup-hyperparameters-search","Creator_reputation":116}
{"_id":{"$oid":"5837a579a05283111e4d4277"},"View_count":43,"Display_name":"user1569341","Question_score":2,"Question_content":"I have a data set with a lot of 0 values for the continuous response variable (about 50%). I want to understand how well gradient boosting/random forest deals with this problem. My colleague suggested doing a two part model with classification as the first step to predict the 0's and second step doing regression. Is this necessary? p.s. I'm using xgboost in R.","Creater_id":96785,"Start_date":"2016-08-19 12:02:04","Question_id":230765,"Tags":["random-forest","boosting","gradient"],"Answer_count":0,"Last_activity":"2016-08-19 12:08:38","Link":"http://stats.stackexchange.com/questions/230765/regression-with-zero-inflated-continuous-response-variable-using-gradient-boosti","Creator_reputation":23}
{"_id":{"$oid":"5837a579a05283111e4d4279"},"View_count":143,"Display_name":"JeanDrayton","Question_score":1,"Question_content":"I am trying to fit a mixed effects model with a binary outcome. I have one fixed effect (Offset) and one random effect (chamber, with muliple data points coming from each chamber).In the text book \"The R Book\", (2007), pg 604, Crawley suggests using the lmer function with a binomial family for the analysis of binomial data where each participant contributes multiple responses (analagous to each of my chambers contributing multiple outcomes). Based on this example, I have used the following script for my data:    ball=lmer(Buried~Offset+(1|Chamber), family=binomial, data=ballData)When I run this model, I get this warning:    calling lmer with 'family' is deprecated; please use glmer() insteadWhen I change my code to the following, the model works:    ball=glmer(Buried~Offset+(1|Chamber), family=binomial, data=ballData)Based on other questions/answers that I have read on Cross Validated, lmer should only be used for data where the outcome is normally distributed, and glmer is the correct function to use for a binomial outcome. My questions are:1) Could anyone clarify the discrepency between Crawleys advice and the fact that lmer would not work for me (nor, based on what I have read on CVed, is it recommended to use this function for binomial data)2) Is glmer indeed the correct function to use to model a binomial outcome with random factors?3) Assuming that glmer is the correct function to use, I want to compare a model with and without random effects to determine if including random effects improves the fit of the model. I understand that glmer estimates model parameters via maximum likelihood. What function can I use to create a model with no random effects for a binary outcome using maximum likelihood? I was playing around with glm however the help file for this function states that the method of estimation is iteratively reweighted least squares (which is beyond me, but it isn't ML...)","Creater_id":127429,"Start_date":"2016-08-18 23:22:34","Question_id":230634,"Tags":["logistic","mixed-model","random-effects-model","lmer","glmer"],"Answer_count":2,"Last_activity":"2016-08-19 12:06:46","Link":"http://stats.stackexchange.com/questions/230634/glmer-vs-lmer-what-is-best-for-a-binomial-outcome","Creator_reputation":51}
{"_id":{"$oid":"5837a579a05283111e4d427b"},"View_count":19,"Display_name":"TLK","Question_score":1,"Question_content":"I am trying to write a short paragraph on why a person would use AR(1) aka autoregressive in analyzing logistic regression for repeated behaviours instead of Exchangeable, and I can't find anything really solid on the topic. Any idea why? ","Creater_id":128314,"Start_date":"2016-08-19 12:02:39","Question_id":230766,"Tags":["repeated-measures","autoregressive"],"Answer_count":0,"Last_activity":"2016-08-19 12:02:39","Link":"http://stats.stackexchange.com/questions/230766/ar1-vs-exchangable","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d427d"},"View_count":20,"Display_name":"ks-min","Question_score":1,"Question_content":"In short: I'm trying to find out how one goes about solving for regression coefficients when the problem has the following setup:There is a date column, d, which groups the records.Each record is the representation of a custom manufacturing job, and is described by two independent variables, x and y, both discrete and bounded on some interval . So there are a finite number of different 'custom' configurations a job may take on.x and y, along with an error term , determine completely the job's completion time, t.Unfortunately in the dataset we do not have job-level time data. Instead, we have the total number of man-hours needed to complete all the jobs that were worked each day. The problem is to solve for the regression coeffecients for x and y, such that the estimated time for any one particular job can be found, while only having available the daily total man-hours.I generated some toy data to illustrate- d is the 'date' column; x and y are sampled with replacement from 1:10 and 10:100, respectively; t, which is not actually available in the real dataset, but which I included to hopefully make things clearer, is calculated as:  And then t_agg, which is the only thing we'd have in the actual dataset, is the sum of all the t values for each value of d. Here are the first few rows:       d     x     y   t   t_agg1      1     3    68  68.9 217.02      1     4    25  40.8 217.03      1     7    50  74.7 217.04      1     2    29  32.6 217.05      2     9    68  98.8 296.56      2     1    47  44.1 296.57      2     4    24  40.8 296.58      2     6    12  40.0 296.59      2     4    64  72.8 296.510     3     4    56  64.0 163.211     3     1    92  80.1 163.212     3     1    17  19.1 163.213     4     4    20  34.5 212.214     4     1    57  51.6 212.215     4     8    19  57.4 212.216     4     8    34  68.7 212.2So, if we only had d, x, y, and t_agg, how could we go about configuring this regression, with one added catch:Setting aside the linearity of this example dataset, where we can simply sum the x and y values for each day, then regress against t_agg - I instead have a dataset that is not quite so easy to work with in this way. Instead of:  My data is approximated more closely by the following:  x and y are both still left-bounded and discrete, and follow approximately the same ranges (still 1:10 for x, y is more like 10:500), but there is non-linearity that prevents me from simply summing the values for each day and regressing against the total time.My approach right now is to aggregate the date column, say by quarters (Q1, Q2, etc.), and then one-hot encode every job in the dataset (2-3 years total) according to its level in x, so that there would be 10 different 'categories' of jobs. Each job would retain its y field, which is a covariate and not a factor.By decreasing the granularity of the date field, I'd hope to have enough records in each subgroup that every level in x would be represented multiple times, and with differing values of y. Having the set of subgroups, along with one-hot encoded x levels, would then allow me to sum through each subgroup in the same way we could sum using the linear toy dataset above (no?)If the columns were x1, x2, x3, and so on, followed by x1y, x2y, x3y, etc, then it seems each 'job type' (i.e. levels of x) would have its own coefficients for both the level of x itself, and the coefficient to multiply y by for each level of x. If that's the case, am I wrong to think that I could sum and regress against the sum of all man-hours per subgroup to get very coarse estimates for the coefficients? From there my hope was to use these estimates and their standard errors to set priors for coefficient estimates and run MCMC regressions at the monthly level, and maybe even at the daily level. This is what I meant by 'multi-objective' in the title; I'm not sure if I've used it correctly, but it seems to me a matter of making the coefficients 'fit' as well as they can across multiple levels of aggregation, so that the error must be minimized according to several different functions at once. Any guidance or corrections to my approach would be much appreciated, thank you.","Creater_id":128287,"Start_date":"2016-08-19 12:00:22","Question_id":230764,"Tags":["bayesian","multiple-regression","optimization","mcmc"],"Answer_count":0,"Last_activity":"2016-08-19 12:00:22","Link":"http://stats.stackexchange.com/questions/230764/need-help-with-difficult-multi-objective-regression-problem","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d427f"},"View_count":31,"Display_name":"previousval","Question_score":1,"Question_content":"Suppose you want to predict an outcome variable  using previous values of the variable as a predictor. For suppose  is the daily temperature in California and  are the daily temperatures in Ohio, Delaware, and Florida. How would you use the previous values of  along with  to predict ?","Creater_id":128307,"Start_date":"2016-08-19 11:13:21","Question_id":230753,"Tags":["regression","machine-learning","time-series"],"Answer_count":1,"Last_activity":"2016-08-19 11:58:52","Link":"http://stats.stackexchange.com/questions/230753/using-previous-values-if-outcome-variable-as-predictor","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4281"},"View_count":25,"Display_name":"Jaco","Question_score":1,"Question_content":"So this is just a general question from someone with a decent math background and weak statistics background.You might want to know what I am running the tests for, so I'll tell you if that helps. I'm analyzing the frequency of word choice surrounding a given word (collocation, in linguistics). These tests will help suggest how much independence exists between the two words.I'm not sure exactly how the program that I'm using calculates expected frequencies for these associated words. That's why I'm looking for some general insights that could help me make a decision.Thanks.edit: Okay more details. I'm taking a collocation indexing the word \"health\" while looking at words three words to the left and right of it. I am generated a frequency list of every word that appears in this net for a corpus of 250 texts on family health.Some words, however, are just common to use in general, or dominantly operate as a certain part of speech more generally. So the software that I'm using has some formula (taking into account these aspects) that determines expected frequency of each word. The tests I'm asking about will calculate statistics which will determine the probability that these words would be placed as they are given the null hypothesis that each word is independent of my indexed word.","Creater_id":113507,"Start_date":"2016-08-19 11:27:53","Question_id":230758,"Tags":["probability","chi-squared"],"Answer_count":0,"Last_activity":"2016-08-19 11:52:46","Link":"http://stats.stackexchange.com/questions/230758/chi-squared-vs-log-likelihood-when-to-use","Creator_reputation":8}
{"_id":{"$oid":"5837a579a05283111e4d4283"},"View_count":17,"Display_name":"Asma","Question_score":1,"Question_content":"I need to do a multinomial logistic regression with a nominal variable, and I've heard about the Gmulti package in r ,and how it provides automatic selection methods models, However all the examples that i found are only applied on binaire logistic regression, so I wonder if it's even working on a multinomial logistic regression and in case is true, is the Gmulti take in consideration the problem of multicolinearity between the independents variables. Please help me thank you ","Creater_id":121657,"Start_date":"2016-08-19 11:42:33","Question_id":230760,"Tags":["r","regression","logistic","multinomial"],"Answer_count":0,"Last_activity":"2016-08-19 11:42:33","Link":"http://stats.stackexchange.com/questions/230760/how-to-use-the-gmulti-package-with-a-multinomial-logistic-regression-in-r","Creator_reputation":16}
{"_id":{"$oid":"5837a579a05283111e4d4285"},"View_count":27,"Display_name":"Roel","Question_score":1,"Question_content":"I have a dataset that consists of an ordered series of categorical samples that are not distributed perfectly random; every sample has an elevated chance of being the same as the sample before it (i.e. there is a tendency for 'streaks' of the same value). I want to take a subset of this dataset (using a selection function, let's say S()) and then from this subset take a uniform sample of say 10% of all values in the original dataset. Now I want to know if the distribution of the result of this is the same as the distribution in the original data set; in other words, I want to measure if my selection function S() changes the distribution, or in other words still, if my resulting selection is an accurate representation of the whole population.To get the sample size for a normally distributed quantity at a given confidence level and margin of error for the data set of a certain size, I'd normally use a 'sample size calculator' online and call it a day. But in the case described above, I'm not quite sure how all pieces fit together - does it matter that my data is categorical? Does the order of the samples matter? Is this an appropriate case for using the Chi-square test of indepence?I think my question reduces to 'how can I compare the distribution of two series of ordered categorical values' but I'm not even sure if that question makes sense - hence my admittedly clumsy description above. Can anyone clarify how I could go about this?","Creater_id":35442,"Start_date":"2016-08-19 09:45:06","Question_id":230742,"Tags":["distributions","categorical-data","discrete-data"],"Answer_count":1,"Last_activity":"2016-08-19 11:41:05","Link":"http://stats.stackexchange.com/questions/230742/comparing-distributions-of-ordered-categorical-values","Creator_reputation":108}
{"_id":{"$oid":"5837a579a05283111e4d4287"},"View_count":37015,"Display_name":"hslc","Question_score":75,"Question_content":"What's the difference between probability and statistics, and why are they studied together?","Creater_id":327,"Start_date":"2010-07-26 13:17:17","Question_id":665,"Tags":["probability","teaching","mathematical-statistics"],"Answer_count":18,"Last_activity":"2016-08-19 11:39:09","Link":"http://stats.stackexchange.com/questions/665/whats-the-difference-between-probability-and-statistics","Creator_reputation":481}
{"_id":{"$oid":"5837a579a05283111e4d4289"},"View_count":39,"Display_name":"Guilherme Salom\u0026#233;","Question_score":1,"Question_content":"Consider a Wiener process . I want to calculate . The text I am reading says the following:  (...) and  is a Wiener process with the  jointly Gaussian, , conditional means  for , which implies .However, I managed to obtain the following:\\begin{align}\\Cov(W_s,W_t)\u0026amp;=\\Cov(W_s-W_0,W_t-W_s+W_s-W_0)\\\\\u0026amp;=\\Cov(W_s-W_0,W_s-W_0)+\\Cov(W_s-W_0,W_t-W_s)\\\\\u0026amp;=s+0\\end{align}Am I doing something wrong? What is it? How can I get the quoted result?Thanks for helping! :D","Creater_id":25824,"Start_date":"2016-08-18 18:20:47","Question_id":230605,"Tags":["probability","covariance","stochastic-processes"],"Answer_count":2,"Last_activity":"2016-08-19 11:33:49","Link":"http://stats.stackexchange.com/questions/230605/covariance-of-wiener-process","Creator_reputation":380}
{"_id":{"$oid":"5837a579a05283111e4d428b"},"View_count":39,"Display_name":"gbhrea","Question_score":2,"Question_content":"I have printed the structure of a CART decision tree, from sci-kit learn,  but I don’t understand it.It’s multiclass classification, there are 4 possible labels, and 5 features. There are 5 different values for each feature. This is what the data looks likeLabel  Feat1  Feat2  Feat3  Feat4  Feat5 A      A      B      A      C      A B      B      A      A      B      B C      A      C      C      A      A D      A      B      B      D      DIn order to discretize these categorical variables, I have used a LabelEncoder and OneHotEncoder.This is the result of printing the structure of the Decision Tree. I know that the gini impurity is the decision tree splitting metric, what I really don’t understand is the top of each box, for example [X7]= 0.5 and the value.","Creater_id":127243,"Start_date":"2016-08-19 08:47:44","Question_id":230732,"Tags":["predictive-models","cart","scikit-learn","multi-class"],"Answer_count":1,"Last_activity":"2016-08-19 11:15:33","Link":"http://stats.stackexchange.com/questions/230732/understanding-multiclass-categorical-decision-tree-structure","Creator_reputation":16}
{"_id":{"$oid":"5837a579a05283111e4d428d"},"View_count":19,"Display_name":"Yuan","Question_score":1,"Question_content":"I have two different dependent variables (Y1 and Y2). Both of them are ordered variables, but for Y1, it has 7 categories, while for Y2, it has 6 categories.Now, I am using probit model to analyze the two dependent variable, employing the same set of independent variables. I am just wondering if it is possible to compare the two models?Thanks in advance.","Creater_id":128299,"Start_date":"2016-08-19 10:17:21","Question_id":230748,"Tags":["logistic","ordinal","model","probit","model-comparison"],"Answer_count":0,"Last_activity":"2016-08-19 10:17:21","Link":"http://stats.stackexchange.com/questions/230748/how-to-compare-two-non-linear-models-with-different-dependent-variables","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d428f"},"View_count":99,"Display_name":"David Fernandez","Question_score":2,"Question_content":"I am trying to calculate the probability of surviving to year 1 of age (ie, 365 days) for a group of monkeys. Data set has three columns: animal ID, days to death, and censored status (1= individual died during study; 0 = individual alive at the end of the study). Like this:ID   days   statusmad  45     1bad  135    1kom  1564   0How do I use a Kaplan-Meier estimate to calculate the probability that an individual will survive the first 365 days of its live?When I use this code it does not even compute a median value:  survfit(Surv(comp.dat[,\"days\"], comp.dat[,\"status\"]) ~ 1 ) Call: survfit(formula = Surv(comp.dat[, \"days\"], comp.dat[, \"status\"]) ~ 1)  n  events  median 0.95LCL 0.95UCL  41      14      NA     293      NA Thanks!","Creater_id":128258,"Start_date":"2016-08-19 05:23:29","Question_id":230685,"Tags":["r","survival","kaplan-meier"],"Answer_count":1,"Last_activity":"2016-08-19 10:15:00","Link":"http://stats.stackexchange.com/questions/230685/how-can-i-calculate-survival-probability-to-year-1-using-survival-package-in-r","Creator_reputation":13}
{"_id":{"$oid":"5837a579a05283111e4d4291"},"View_count":14,"Display_name":"Anonymous_Statsman","Question_score":2,"Question_content":"My knowledge of statistics is described as \"working\" or more accurately a very immature \"work in progress\" so please excuse me if this question is extremely simple.I am currently writing my masters thesis and as part of my thesis I was provided with output from univariate and multivariate regressions investigating the relationship between 1 dependent variable (A measure of someone's attitude) and approximately 15 independent variables, many of which are categorical with some being continuous.Two of these independent variables ask effectively opposing questionsVar_1: \"Does your practice provide any monitoring services?\"Var_2: \"Does your practice provide monitoring service X?\" - Where service X is in most cases the bare minimum service provided, and it would be expected that those who respond no to the first  question will respond yes to the first question. Summary statistics describing the responses to these two questions more or less confirms this.I have very limited formal knowledge of statistics however what I have observed in the univariate analysis is that both have p values below 0.025 but skyrocket to well over 0.3 and 0.6 in multivariate regression. Would I be correct in saying that as the two variables are obviously antagonistic and dependent on one another, this explains why they lose statistical significance in the multivariate analysis?","Creater_id":128295,"Start_date":"2016-08-19 10:02:36","Question_id":230746,"Tags":["regression","multivariate-analysis","linear"],"Answer_count":0,"Last_activity":"2016-08-19 10:02:36","Link":"http://stats.stackexchange.com/questions/230746/two-opposing-categorical-variables-and-moving-from-univariate-to-multivariate-li","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d4293"},"View_count":366,"Display_name":"esemef","Question_score":6,"Question_content":"Can coefficients of dummy variables be more than  or less than ? I am getting coefficients ranging from  to . I am specifically asking about the coefficients of dummy variables, not the values of dummy variables.","Creater_id":128236,"Start_date":"2016-08-19 01:08:08","Question_id":230647,"Tags":["regression","categorical-data","regression-coefficients"],"Answer_count":1,"Last_activity":"2016-08-19 09:58:54","Link":"http://stats.stackexchange.com/questions/230647/can-the-coefficients-of-dummy-variables-be-more-than-1-or-less-than-0","Creator_reputation":31}
{"_id":{"$oid":"5837a579a05283111e4d4295"},"View_count":122,"Display_name":"LoulouChameau","Question_score":5,"Question_content":"Context: I'm constructing a CNN classifier for text categorization. I have a dataset with 20 different classes and approximately 20,000 labeled features (the 20 News Group dataset for those interested).I'm wondering if I'm training my model on too many epochs, which would make it really good at categorizing the features from my training dataset, but unable to adapt to new / slightly different inputs. Is that what we call \"overfitting\"? The term is not clear to me.Also I would like to clarify the term \"convergence\" of a neural network. Is this convergence attained when the accuracy starts plateauing? Or is it related to the loss value?","Creater_id":127872,"Start_date":"2016-08-19 01:58:45","Question_id":230653,"Tags":["classification","neural-networks","convergence","conv-neural-network","overfitting"],"Answer_count":2,"Last_activity":"2016-08-19 09:51:13","Link":"http://stats.stackexchange.com/questions/230653/is-it-possible-to-over-train-a-classifier","Creator_reputation":90}
{"_id":{"$oid":"5837a579a05283111e4d4297"},"View_count":15,"Display_name":"Xpector","Question_score":1,"Question_content":"I stumbled upon a classification rule learner which accepts numerical (interval or ratio) features and produces a rule set like    {      if (feature[1]\u0026lt;feature[2] \u0026amp;\u0026amp; feature[2]\u0026gt;feature[4]) return 1;      if (feature[1]\u0026gt;feature[2] \u0026amp;\u0026amp; feature[3]\u0026gt;feature[2]) return 1;      return 0;    }This looks like cutting the feature space by diagonal hyperplanes, as opposed to RIPPER and many others, which make their cuts orthogonal to axis (feature[1]\u0026gt;0.12345).  I wonder if the above mentioned rule learner has a common name? I digged so far through Lantz (ISBN 978-1-78216-214-8), Kuhn (ISBN 978-1-4614-6848-6), James (ISBN 978-1-4614-7137-0), but found only algorithms comparing features with (carefully chosen) constant thresholds, never with other features.For completeness I probably should mention that the rule learner I stumbled upon is built into Zorro as \"Pattern Analyzer\" and PriceActionLab.  ","Creater_id":96780,"Start_date":"2016-08-10 08:37:37","Question_id":229210,"Tags":["machine-learning"],"Answer_count":1,"Last_activity":"2016-08-19 09:41:29","Link":"http://stats.stackexchange.com/questions/229210/name-of-rule-learner-algorithm-which-only-compares-features-with-features","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4299"},"View_count":25523,"Display_name":"Tomek Tarczynski","Question_score":83,"Question_content":"Random forests are considered to be black boxes, but recently I was thinking what knowledge can be obtained from a random forest?The most obvious thing is the importance of the variables, in the simplest variant it can be done just by calculating the number of occurrences of a variable.The second thing I was thinking about are interactions. I think that if the number of trees is sufficiently large then the number of occurrences of pairs of variables can be tested (something like chi square independence). The third thing are nonlinearities of variables. My first idea was just to look at a chart of a variable  Vs score, but I'm not sure yet whether it makes any sense.  These things are probably well studied (just an intuition). I would be grateful if anyone could point me how to examine those things properly. Added 23.01.2012Motivation  I want to use this knowledge to improve a logit model. I think (or at least I hope) that it is possible to find interactions and nonlinearities that were overlooked.","Creater_id":1643,"Start_date":"2012-01-16 04:09:29","Question_id":21152,"Tags":["machine-learning","data-mining","interaction","random-forest","cart"],"Answer_count":8,"Last_activity":"2016-08-19 09:35:29","Link":"http://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest","Creator_reputation":1117}
{"_id":{"$oid":"5837a579a05283111e4d429b"},"View_count":18,"Display_name":"Praveen Sriram","Question_score":2,"Question_content":"Given two independent random variables  and  and two measurable functions  and , how do we show that  and  will also be independent?","Creater_id":128292,"Start_date":"2016-08-19 09:34:09","Question_id":230739,"Tags":["random-variable","measure-theory"],"Answer_count":0,"Last_activity":"2016-08-19 09:34:09","Link":"http://stats.stackexchange.com/questions/230739/proving-independence-of-functions-of-random-variables","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d429d"},"View_count":46,"Display_name":"BigData","Question_score":2,"Question_content":"Assume we have a rule {X} -\u003e {Y} which means customers buy item X, they also buy item Y in a supermarket. I know the LIFT is used to measure the independence of X and Y which is equal , the higher the better for the rulesLets consider 2 cases:1) We have the following probabilities for a rule: and  The LIFT would be :\\frac{0.5}{0.52(0.77)}=1.252) We have the following probabilities for another rule {A}-\u003e{B}: and  The LIFT would be :\\frac{0.18}{0.21(0.4)}=2.14A and B should be less dependent to each other compare to X and Y since they have less conditional probabilities ( and ), but they still have higher lift.Can it be said that lift does not linearly correlated with dependence and thus is not a good measure?","Creater_id":125543,"Start_date":"2016-08-19 08:11:24","Question_id":230723,"Tags":["probability","association-measure","association-rules"],"Answer_count":1,"Last_activity":"2016-08-19 09:11:10","Link":"http://stats.stackexchange.com/questions/230723/probability-association-rules-lift-is-useless","Creator_reputation":17}
{"_id":{"$oid":"5837a579a05283111e4d429f"},"View_count":38,"Display_name":"Md. Sahidul Islam","Question_score":1,"Question_content":"I want to implement semi supervised hierarchical clustering technique on iris data. I want to use 10% label (90% class label missing) training data. I have tried following R code -library(tibble)library(DBI)library(lpSolve)library(RSSL)library(dplyr,warn.conflicts = FALSE)library(ggplot2,warn.conflicts = FALSE)### Calling datasetdata(iris)data\u0026lt;-irisdata2\u0026lt;-data[,1:4]### Training and test data...inTrain = sample(1:150,120)dfTrain=data[inTrain,]dfTest=data[-inTrain,]#### 10% labeling is Semi Supervised datasets....# Randomly remove labelsdfTrain \u0026lt;- data %\u0026gt;% add_missinglabels_mar(data[,5]~.,prob=0.90)It will be helpful if anyone can write some R code or give idea.","Creater_id":82907,"Start_date":"2016-08-19 08:57:05","Question_id":230735,"Tags":["r"],"Answer_count":0,"Last_activity":"2016-08-19 08:57:05","Link":"http://stats.stackexchange.com/questions/230735/how-to-perform-semi-supervised-hierarchical-clustering","Creator_reputation":106}
{"_id":{"$oid":"5837a579a05283111e4d42a1"},"View_count":22,"Display_name":"James Oliver","Question_score":1,"Question_content":"I've built myself a linear model which has a fairly good predictive rate.  I need to use this to explain the difference in customer satisfaction from one month to the other.  My intention is to have a waterfall chart showing the impact of environment factors (things that can't be controlled), process factors, and unknowns.The sum of these factors builds the bridge from one time period from the other.What I can't get my head around is how to calculate the difference by the different factors.  I'm thinking of it as a price volumne challenge that you might have for explaining change in performance, where you hold one thing the same but I run into challenges.  Imagine that I have 2 variables Age (Environment) and AHT (Process), mocked up data below.  How would you go about explaining how much of the change in Customer Satisfaction from average of 5.5 in time period 0, to 6 in time 1, is due to age and how much due to AHT.   df\u0026lt;-data.frame(Time=rep(0:1,each=4),Age=rep(c(25,40,30,35),each=2),AHT=rep(c(1,2,1,2),each=2),               CustSat=c(4,2,10,6,5,3,9,7))dftestfit\u0026lt;-lm(CustSat~Age+AHT,data=df)summary(testfit)This model then calculates a Customer Satisfaction score for every line.  This then suggests to me that using mean values isn't appropriate.This becomes a further challenge when what I actually need to calculate is NPS (net promotor score) which uses the proportion of the detractors (6 and unders) and the proportion of promotors (9 and 10s)I was thinking could I just replace the values for Age in one time period to another, but these aren't the same customers and what happens if I then have a different number of customers in one period to the other.Maybe this is simple, but I'd appreciate a bit of guidance on splitting out the intepreation of the model.  Thanks in advance for your time,James","Creater_id":92724,"Start_date":"2016-08-19 08:46:52","Question_id":230731,"Tags":["generalized-linear-model","interpretation"],"Answer_count":0,"Last_activity":"2016-08-19 08:46:52","Link":"http://stats.stackexchange.com/questions/230731/identify-the-difference-in-two-samples-using-parts-of-a-linear-regression-model","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d42a3"},"View_count":70,"Display_name":"user3658307","Question_score":2,"Question_content":"I am interested in the differences between using large, deep learning networks vs Probabilistic graphical models (PGMs), like Random Field models, for structured learning (e.g. on images, or labels of arbitrary graphs on surfaces, etc...).For instance, what areas/fields/problems would one or the other be preferred? Are there theoretical guarantees for one vs another? How can they be used together (e.g. in papers like this)?I have heard lately that deep learning (i.e. conv-networks) has replaced classic structured learning approaches like conditional random fields (CRFs), and was interested in the thoughts of professionals in the field as to the truth of this. I would of course prefer literature and math to just anecdotes (so no one closes the question :) ).","Creater_id":128284,"Start_date":"2016-08-19 08:32:42","Question_id":230727,"Tags":["machine-learning","deep-learning","graphical-model","conditional-random-field","structured-prediction"],"Answer_count":0,"Last_activity":"2016-08-19 08:46:26","Link":"http://stats.stackexchange.com/questions/230727/deep-learning-vs-structured-learning","Creator_reputation":11}
{"_id":{"$oid":"5837a579a05283111e4d42a5"},"View_count":215,"Display_name":"anderstood","Question_score":7,"Question_content":"Let  be for example your number of days remaining to live. A doctor 1 evaluates the distribution of  as a Gaussian: . Another independent doctor 2 evaluates . Both doctors are equally reliable. How to combine both information?In this blog article, the author says that   If we have two probabilities and we want to know the chance that both are true, we just multiply them together. So, we take the two Gaussian blobs and multiply them:   Edit Most people (I first asked this question on math.SE) have answered that this is the trivial independency relation  but I am still having difficulty understanding what would  and  be in this context: probably not events such as \"the dice will give a 3\" or \"the patient is sick\". Also, there is probably something more, because the product of two densities is not a probability density since in general . So it's probably not as simple as that.Let's take another example. An expert 1 tells you that a dice is perfectly balanced. Another expert 2 tells you, independently the same. Then the probability of the dice giving a 3 is certainly not .","Creater_id":67791,"Start_date":"2016-08-18 17:06:48","Question_id":230596,"Tags":["probability","normal-distribution"],"Answer_count":2,"Last_activity":"2016-08-19 08:41:52","Link":"http://stats.stackexchange.com/questions/230596/why-do-the-probability-distributions-multiply-here","Creator_reputation":140}
{"_id":{"$oid":"5837a579a05283111e4d42a7"},"View_count":8,"Display_name":"user3001937","Question_score":1,"Question_content":"I would like to know some approaches or techniques that are used to tackle this problem. I would like to estimate the weight of every node over time. I have temporal-network with 50 nodes. At every time t (every 10 minutes) in a day di, I have snapshot of number of elements that moved from nodei to nodej. I denote weight of edge e(i,j) as weij, and the weight of node i as wi = Sum(Weij -Wki) it's equal to all the weights coming to node j minus the nodes going out from node i. As  a results I would have matrice X = \\time of day| 1 | 2 | ... | 144 |node_1      | x1t1 | .. ..  | xit144..node_i      | xit1 | .. ..  | xit144node_49      | x49t1 | .. ..  | x49t144node_50      | x50t1 | .. ..  | x49t144What's the name of these techniques that take into account the weigh of node instead of focusing on weight of edges ? Are you aware of any application/paper where they use this weight of node to estimate future weight of node ?  ","Creater_id":64196,"Start_date":"2016-08-19 08:39:30","Question_id":230729,"Tags":["graph-theory","spatio-temporal"],"Answer_count":0,"Last_activity":"2016-08-19 08:39:30","Link":"http://stats.stackexchange.com/questions/230729/how-do-we-estimate-weights-of-nodes-in-temporal-networks","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d42a9"},"View_count":226,"Display_name":"MiniQuark","Question_score":12,"Question_content":"Why do people use Quadratic Programming techniques (such as SMO) when dealing with kernelized SVMs? What is wrong with Gradient Descent? Is it impossible to use with kernels or is it just too slow (and why?).Here is a little more context: trying to understand SVMs a bit better, I used Gradient Descent to train a linear SVM classifier using the following cost function:I am using the following notations: is the model's feature weights and  is its bias parameter. is the  training instance's feature vector. is the target class (-1 or 1) for the  instance. is the number of training instances. is the regularization hyperparameter.I derived a (sub)gradient vector (with regards to  and ) from this equation, and Gradient Descent worked just fine.Now I would like to tackle non-linear problems. Can I just replace all dot products  with  in the cost function, where  is the kernel function (for example the Gaussian RBF, ), then use calculus to derive a (sub)gradient vector and go ahead with Gradient Descent?If it is too slow, why is that? Is the cost function not convex? Or is it because the gradient changes too fast (it is not Lipschitz continuous) so the algorithm keeps jumping across valleys during the descent, so it converges very slowly? But even then, how can it be worse than Quadratic Programming's time complexity, which is ? If it's a matter of local minima, can't Stochastic GD with simulated annealing overcome them?Thanks!","Creater_id":109561,"Start_date":"2016-05-31 03:16:43","Question_id":215524,"Tags":["svm","kernel-trick","gradient-descent"],"Answer_count":3,"Last_activity":"2016-08-19 08:30:28","Link":"http://stats.stackexchange.com/questions/215524/is-gradient-descent-possible-for-kernelized-svms-if-so-why-do-people-use-quadr","Creator_reputation":429}
{"_id":{"$oid":"5837a579a05283111e4d42ab"},"View_count":30,"Display_name":"Ryan Rothman","Question_score":0,"Question_content":"I am in the process of forming an index for frugivore preference/avoidance of certain fruit types of differing colours. It is hypothesised that for a given fruit type of a certain colour, the frugivore will either prefer or avoid it. For simplicity purposes, let's assume the fruit types are A, B, and C. And the colours are 1, 2, and 3.I know that for a preference index, I need to know the observed and the expected values. Observed should be the proportion of food type x in the 'gut' whereas expected should be the proportion of food type x in the environment of all available food.I am fairly certain that Observed should then be written off as, for example:number of Fruit Type A of Colour 1 eaten / Sum (Fruit of colour 1 eaten)I am having a bit of trouble grasping the expected value. I am imagining it as being either:number of Fruit Type A of Colour 1 total / Sum (Fruit of colour 1 total)ornumber of Fruit Type A total / Sum (all fruit total)My data will be set-up in a spreadsheet in the following way:Fruit Type Capsule   Drupe   FigandFruit ColourYellowOrangeRedBrownGreenAny comments or thoughts on this would be greatly appreciated!","Creater_id":110948,"Start_date":"2016-08-19 07:56:39","Question_id":230722,"Tags":["proportion","biostatistics","excel"],"Answer_count":1,"Last_activity":"2016-08-19 08:20:11","Link":"http://stats.stackexchange.com/questions/230722/preference-index-observed-v-expected","Creator_reputation":12}
{"_id":{"$oid":"5837a579a05283111e4d42ad"},"View_count":28,"Display_name":"discretetimeisnice","Question_score":1,"Question_content":"I work in the field of behavioural interventions and I use dynamic models to gain better insight into (explain) the process of behaviour change and to help inform future behavioural interventions (exploratory research using system dynamics as model-based theory building).I currently choose state space models (SSM) as a suitable class of models I intend to explore. However, given the various flavours of SSM i.e. ARIMA, hidden Markov models, regime switching models, hybrid models; I have a couple of questions.Should I base my modelling choice on:a goodness-of-fit (GoF) measure e.g. I obtain AIC and BIC for all models under considerationor on domain expertise e.g. based on my understanding of the domain, I expect the process to follow a stepwise (HMM) rather than a linear process (ARIMA) Ideally, I would expect the two model selection strategies to agree. Since the choice of model (and thus, its parameters) will be used in theory building, I was wondering what would I do when they don't.Additionally, I am aware that AIC can be used as a measure only when comparing models fitted with the same estimation method (MLE) on the same data. Can I use information criteria such as the AIC to compare an ARIMA(p,d,q) with a  HMM? (where d \u003e0 i.e. differenced data) Or do I restrict my model space to ARMA and other flavours of SSMs. Thanks!(I am aware of What are disadvantages of state-space models and Kalman Filter for time-series modelling?, I do not intend to comment on the features of a particular modelling framework, I am more curious to identify an objective measure that might help guide my modelling choice)","Creater_id":122349,"Start_date":"2016-08-19 08:15:41","Question_id":230724,"Tags":["time-series","goodness-of-fit","aic","eda","state-space-models"],"Answer_count":0,"Last_activity":"2016-08-19 08:15:41","Link":"http://stats.stackexchange.com/questions/230724/time-series-model-selection-in-exploratory-research","Creator_reputation":80}
{"_id":{"$oid":"5837a579a05283111e4d42af"},"View_count":46,"Display_name":"Jack Yates","Question_score":1,"Question_content":"I have a dataset of the properties of a population over time. The data come from a (sort-of) agent-based model, where agents reproduce (with evolution) and die. After some time, the population should reach a \"steady state\", where the agents have evolved to a survival strategy that is stable/self-sustaining over a long period of time. I would like to determine when the population has reached this point (has stopped changing or become stable over time). It's possible that there is more than one survival strategy, so there could be two different sub-populations with different values for the property, both of which are stable. Even after this point, the distribution will change slightly as agents die and reproduce, so there should be some tolerance for variation in whatever test I use. But agents that are not well-adapted should die more quickly than agents that are optimal, so the distribution should not change too much once the optimal strategy (or strategies) has been reached. I'm not interested in outliers especially - there will always be outliers with this sort of model, but they generally die very quickly. The mean and median are pretty meaningless (pun not intended) for this data, because there could be more than one strategy that is viable (i.e. two peaks in the density distribution). As noted above, the size of the population can change too, so the standard deviation is probably not a particularly useful metric either. Due to the sheer amount of data I have, I'm looking for a few numbers that I can put in a table rather than something like a QQ plot.What's the correct test to use here? I'm looking for something like an Augmented Dickey-Fuller test only on a distribution, rather than a single point. What I had come up with myself was to use a Kolmogorov-Smirnov statistic between the distribution at time 0 and the distribution at time  (call this ), and between the distribution at time  and at time  (call this ). When the distribution is in a steady state,  should be stationary around some constant (the distribution will have moved away from the distribution at , but is otherwise not changing) and  should be approximately stationary around 0 (the distribution has stopped changing between time steps). I could then even use a ADF test on the  and  timeseries to test the stationarity of those. This is probably somewhat crude, but I don't need to be especially rigorous.I am a physicist, not a mathematician, so it's most likely a lack of terminology that has meant I am unable to find such a test by myself. Answers that I can implement in Python are preferred!","Creater_id":128275,"Start_date":"2016-08-19 07:13:59","Question_id":230716,"Tags":["distributions","stationarity","kolmogorov-smirnov"],"Answer_count":0,"Last_activity":"2016-08-19 08:01:12","Link":"http://stats.stackexchange.com/questions/230716/how-do-i-determine-when-a-distribution-has-reached-a-steady-state","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d42b1"},"View_count":97,"Display_name":"Dark_Knight","Question_score":2,"Question_content":"Suppose I have a discrete set of (finite) data (values are only positive integers) (which always remains discrete whenever the observation/survey is taken), divided into some identical categories . Now, I want to calculate the mean values of some random variable for each category, hence getting a sequence of mean values. Considering mean as a random variable (let's call it ), it's taking decimal values also (obviously possible). But I am not sure if  is a discrete or a continuous random variable. My intuition suggest me that it should be discrete. Since, my original data from which I calculated the sets of mean is discrete so the mean can take finitely many values which I suppose is not the case with a continuous random variable where the mean can take infinitely many values within an interval. So is my intuition right or wrong?  Is it true that a discrete random variable takes finitely many values and continuous R.V. can take infinitely many possible values? ","Creater_id":106322,"Start_date":"2016-08-19 06:34:42","Question_id":230703,"Tags":["random-variable","discrete-data"],"Answer_count":3,"Last_activity":"2016-08-19 07:53:03","Link":"http://stats.stackexchange.com/questions/230703/is-my-random-variable-discrete-or-continuous","Creator_reputation":158}
{"_id":{"$oid":"5837a579a05283111e4d42b3"},"View_count":40,"Display_name":"Mato","Question_score":1,"Question_content":"I have question about output of PROXIMUS algorithm. In the example ofuse here.In Case Study section is all clear, but I do not know what way they usefor get clustered words to groups: Group 1 (computers) = {intel, computer, software, linux, windows, Firefox, explorer, programming}Group 2 (authors) = {kuth, shakespeare, grisham, asimov, book} Group 3 (noise) = {love}How can I get this output for my data? Next inthe example section is graph. How do I read this graph?My data are:objects cat1    cat2    cat3    cat4 ...A       TRUE    FALSE   FALSE   FALSEB       TRUE    FALSE   TRUE    FALSEC       TRUE    FALSE   FALSE   FALSED       FALSE   TRUE    TRUE    TRUEE       TRUE    TRUE    TRUE    TRUEF       TRUE    FALSE   TRUE    FALSEAfter apply of Proximus algorithm I get this output:pr \u0026lt;- proximus(x, max.radius=8, debug=TRUE)#Non-Zero: 55#Sparsity: 0.48#  0 [6,3,5] 1 \u0026gt;#  1 [3,3,5] 1 * 1#  1 [3,1,0] 1 \u0026gt;#  2 [1,1,0] 1 * 2#  2 [2,1,0] 1 \u0026gt;#  3 [1,1,0] 1 * 3#  3 [1,1,0] 1 * 4summary(pr)#Size Length Radius Error Fnorm Jsim Valid#1    3     16      5  0.16     3 0.81  TRUE#2    1      9      0  0.00     0 1.00  TRUE#3    1      4      0  0.00     0 1.00  TRUE#4    1      2      0  0.00     0 1.00  TRUESo it means that 3 objects are in one cluster and all other objects haveown cluster. What way I can use for get list of objects in cluster? And I get this:  How do I read this graph? Many thanks for your help!","Creater_id":127449,"Start_date":"2016-08-19 07:15:25","Question_id":230717,"Tags":["r","clustering","binary-data"],"Answer_count":0,"Last_activity":"2016-08-19 07:48:42","Link":"http://stats.stackexchange.com/questions/230717/problem-with-output-of-proximus-algorithm-in-r","Creator_reputation":13}
{"_id":{"$oid":"5837a579a05283111e4d42b5"},"View_count":731,"Display_name":"Diana","Question_score":2,"Question_content":"I have to conduct a meta-analysis. Following problem occured: a single study have measured Quality of Life and therefore, used two questionnaires. Now is my question whether I have two choose one of the instrument to include in meta-analysis or is there a possibility to combine the means of the two scales to provide one standard mean difference in the meta-analysis?THANK YOU!!!!","Creater_id":42880,"Start_date":"2014-04-01 06:51:24","Question_id":92134,"Tags":["meta-analysis"],"Answer_count":1,"Last_activity":"2016-08-19 07:43:45","Link":"http://stats.stackexchange.com/questions/92134/combining-outcome-measures-for-meta-analysis","Creator_reputation":13}
{"_id":{"$oid":"5837a579a05283111e4d42b7"},"View_count":396,"Display_name":"user2896492634","Question_score":1,"Question_content":"Why do we use rectified linear units (ReLU) with neural networks? How does that improve neural network?Why do we say that ReLU is an activation function? Isn't softmax activation function for neural networks? I am guessing that we use both, ReLU and softmax, like this:  neuron 1 with softmax output ----\u003e ReLU on the output of neuron 1, which isinput of neuron 2 ---\u003e neuron 2 with softmax output --\u003e ...so that the input of neuron 2 is basically ReLU(softmax(x1)). Is this correct?","Creater_id":125388,"Start_date":"2016-08-02 10:26:10","Question_id":226923,"Tags":["neural-networks"],"Answer_count":2,"Last_activity":"2016-08-19 07:34:42","Link":"http://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it","Creator_reputation":8}
{"_id":{"$oid":"5837a579a05283111e4d42bf"},"View_count":3618,"Display_name":"jatalah","Question_score":9,"Question_content":"Could someone advise me on how to interpret the estimates from a logistic regression using a cloglog link?I have fitted the following model in lme4:glm(cbind(dead, live) ~ time + factor(temp) * biomass,    data=mussel, family=binomial(link=cloglog))For example, the estimate of time is 0.015. Is it correct to say the odds of mortality per unit time is multiplied by exp(0.015) = 1.015113 (~1.5% increase per unit time).In other words are the estimates obtained in a cloglog expressed in log odds as is the case for a logit logistic regression?","Creater_id":66127,"Start_date":"2015-01-07 18:42:36","Question_id":132627,"Tags":["logistic","regression-coefficients"],"Answer_count":1,"Last_activity":"2016-08-19 07:30:13","Link":"http://stats.stackexchange.com/questions/132627/interpreting-estimates-of-cloglog-logistic-regression","Creator_reputation":48}
{"_id":{"$oid":"5837a579a05283111e4d42cb"},"View_count":37,"Display_name":"Antoni Parellada","Question_score":1,"Question_content":"My understanding is that the cost function is not really part of the calculation of coefficients in OLS, which can be derived in close form. However, it comes into play when regularization is introduced.Differentiating the cost function with respect to the estimated coefficients is the method.The cost function would be generally expressed as:J(\\hat \\beta)= (y - {\\bf X}\\hat \\beta)^T(y- {\\bf{X} \\hat \\beta})= \\displaystyle \\sum_{i=1}^n (y_i - x_i^T\\hat \\beta)^2= \\sum_{i=1}^n(y_i - \\hat y_i)^2Expanding the quadratic in matrix notation:J(\\hat \\beta)= (y - {\\bf X}\\hat \\beta)^T(y- {{\\bf X} \\hat \\beta})= y^Ty + \\color{blue}{\\hat \\beta^T\\,X^TX\\,\\hat \\beta} - 2y^TX\\hat \\betaThe term in blue is the only non-scalar term remaining, and I presume that if setting the equation equal to zero to calculate the coefficients with a minimum cost function has to work,   must be positive definite. I know that  is positive semidefinite. But if all the above statements are correct, how can we proof that  is positive definite?","Creater_id":67822,"Start_date":"2016-08-19 07:02:38","Question_id":230713,"Tags":["regression"],"Answer_count":1,"Last_activity":"2016-08-19 07:28:35","Link":"http://stats.stackexchange.com/questions/230713/positive-definite-ness-of-the-regression-cost-function","Creator_reputation":7682}
{"_id":{"$oid":"5837a579a05283111e4d42d8"},"View_count":58,"Display_name":"M. Beausoleil","Question_score":4,"Question_content":"When I'm reading model equations, I want to spot which factor is random or fixed. In this pdf they are showing an equation (see image) and saying that:   residual effects are randomMy question is is there a way to spot just by looking to an equation like this which factor is fixed or random? Also, is there a notation to make a difference between a fixed and a random factor? ","Creater_id":93498,"Start_date":"2016-08-15 07:44:20","Question_id":229927,"Tags":["mixed-model","random-variable","mathematics"],"Answer_count":1,"Last_activity":"2016-08-19 06:58:52","Link":"http://stats.stackexchange.com/questions/229927/how-to-distinguish-fixed-from-random-effects-in-a-model-equation","Creator_reputation":272}
{"_id":{"$oid":"5837a579a05283111e4d42e5"},"View_count":43,"Display_name":"probabilityislogic","Question_score":7,"Question_content":"I am well aware of the problems of stepwise/forward/backward selection in regression models.  There are numerous cases of researchers denouncing the methods and pointing to better alternatives.  I was  curious if there are any stories that exist where a statistical analysis:has used stepwise regression;made some important conclusions based on the final modelthe conclusion was wrong, resulting in negative consequences for the individual, their research, or their organisationMy thought on this if stepwise methods are bad, then there should be consequences in the \"real world\" for using them.","Creater_id":2392,"Start_date":"2016-08-19 06:55:32","Question_id":230710,"Tags":["regression"],"Answer_count":0,"Last_activity":"2016-08-19 06:55:32","Link":"http://stats.stackexchange.com/questions/230710/howlers-caused-by-using-stepwise-regression","Creator_reputation":15777}
{"_id":{"$oid":"5837a579a05283111e4d42e7"},"View_count":29,"Display_name":"Irene","Question_score":0,"Question_content":"I have been researching for my thesis/dissertation but I guess my knowledge about this is not so wide. I am just quite confused about my model, so if you think this question is easy to answer just doing more research, I would really appreciate some references, because I really did a research and I am still lost.I made a survey experiment, 2x2 between subject design. I have two categorical/dummies independent variables and the dependent variable is a 7-point Likert Scale (it was a single question, so maybe a likert item). Now I am not sure if I have to perform a 2x2 ANOVA due to my experiment design, or there is another way. My hypotheses make reference first to main effects (with that two independent variables) and then I have to introduce other control variables and gender as a moderator.So, can I use ANOVA with an ordinal variable if I violate the assumptions, or should I use Kruskal-Wallis H test, or directly an ordinal regression analysis?Sorry for the long post. I would really appreciate some guidance. Thank you for your time and have a nice day! ","Creater_id":128271,"Start_date":"2016-08-19 06:43:02","Question_id":230706,"Tags":["categorical-data","modeling","experiment-design","ordinal","predictor"],"Answer_count":1,"Last_activity":"2016-08-19 06:51:56","Link":"http://stats.stackexchange.com/questions/230706/2x2-between-subjects-experimental-design-with-ordinal-dependent-variable","Creator_reputation":1}
{"_id":{"$oid":"5837a579a05283111e4d42f4"},"View_count":32,"Display_name":"greg","Question_score":1,"Question_content":"I'm learning about regression decision trees and I was wondering if there always exists a split in each of the terminal nodes that will reduce the sum of squared errors (assuming there is more than one data point in the node). If there is, is there a proof?Edit: I don't mean reduced in the strict sense.","Creater_id":128262,"Start_date":"2016-08-19 05:50:00","Question_id":230690,"Tags":["machine-learning","cart"],"Answer_count":1,"Last_activity":"2016-08-19 06:35:04","Link":"http://stats.stackexchange.com/questions/230690/decision-trees-is-there-always-a-split-that-reduces-sum-of-squared-errors","Creator_reputation":6}
{"_id":{"$oid":"5837a579a05283111e4d4301"},"View_count":28,"Display_name":"Ali Turab Lotia","Question_score":4,"Question_content":"From the time I started the study of statistics I have noticed that the tails of distributions from samples are usually the ones that deviate from theoretical distributions.For example, most Q-Q plots fail to fit the y=x line around the start and end points.What is the reason for this and what are some of the techniques used to address this issue?Thank you.","Creater_id":124010,"Start_date":"2016-08-19 05:56:41","Question_id":230693,"Tags":["distributions"],"Answer_count":1,"Last_activity":"2016-08-19 06:26:20","Link":"http://stats.stackexchange.com/questions/230693/why-are-the-tails-of-distributions-estimated-from-samples-most-prone-to-deviatio","Creator_reputation":69}
{"_id":{"$oid":"5837a579a05283111e4d430e"},"View_count":15,"Display_name":"Vikas Chandra","Question_score":1,"Question_content":"I have list of order numbers, each containing ordered SKU's(One order may have multiple SKU's) and their ordered Quantities. Now after initial analysis I found that 16/550 SKU;s are there in 50% Orders. Also 5 or more of these 16 SKU's are there in 20% orders. Question: I want to identify top 15-20 SKU's which are present in most of Orders in significant volume(High Quantity). I heard about cluster analysis can cluster these high perfroming SKU's. Can anyone help how to find these high performig SKU's? This concept is known as forward area in warehousing.ThanksSource:Sample Raw dataMaterial Code,  Delivery Number,    Quantity orderedThanks","Creater_id":128032,"Start_date":"2016-08-17 10:07:36","Question_id":230329,"Tags":["spss","excel","spss-modeler"],"Answer_count":1,"Last_activity":"2016-08-19 06:15:45","Link":"http://stats.stackexchange.com/questions/230329/cluster-high-performing-skus-based-on-frequency-of-order-together-and-quantity","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d4318"},"View_count":34,"Display_name":"Roger","Question_score":1,"Question_content":"The distribution of the outcome variable in my predictive model has data points that are constructed in the following way: x = a^2 + b^2 + c^2. As I'm reading Applied Predictive Modeling I get the feeling that I have to transform this outcome variable to get a better model without predicting a, b and c separately. Using a Box-Cox transform doesn't help me unfortunately. Any suggestions?caret::BoxCoxTrans(df$outcome.var)## Lambda could not be estimated; no transformation is applied","Creater_id":113227,"Start_date":"2016-08-16 05:38:21","Question_id":230092,"Tags":["machine-learning","pca","predictive-models","data-transformation","caret"],"Answer_count":0,"Last_activity":"2016-08-19 06:08:50","Link":"http://stats.stackexchange.com/questions/230092/transforming-an-outcome-dependent-variable-in-predictive-modeling","Creator_reputation":113}
{"_id":{"$oid":"5837a57aa05283111e4d431a"},"View_count":22,"Display_name":"Plissken","Question_score":1,"Question_content":"I have tried to look for an answer online but so far I have not succeeded. Is there a Bayesian equivalence of the Breusch-Godfrey test to no autocorrelation or of the Ljung-Box test? I have found some references on the Durbin-Watson test but as is well known this test is not of much use in applied work.In a classical framework one would estimate the model of interest and then run an auxiliary regression to test for autocorrelation up to lag . How is this done in a Bayesian setting?I could think of obtaining the distribution of residuals and then do an auxiliary regression of these. I could then test the significance of the coefficients on lags using a Bayesian F-test or using the marginal likelihood to compare models. However, running the auxiliary regression would require a prior. Something which I do not have. Could the prior mean of the residuals be set to zero as that is what I expect them to be?Further, would it be enough to just select a model based on the marginal likelihood and just disregard a test for autocorrelation?","Creater_id":45326,"Start_date":"2016-08-19 06:02:03","Question_id":230695,"Tags":["hypothesis-testing","bayesian","autocorrelation"],"Answer_count":0,"Last_activity":"2016-08-19 06:02:03","Link":"http://stats.stackexchange.com/questions/230695/bayesian-hypothesis-tests-for-autocorrelation","Creator_reputation":829}
{"_id":{"$oid":"5837a57aa05283111e4d431c"},"View_count":35,"Display_name":"morphy_richards","Question_score":2,"Question_content":"I need to compare two outputs of electricity consumption measurement system coming from one source (one company) and assess whether they are corresponding. If not, it may prove that one of the systems may be faulty. The outputs are in form of hourly consumption data sets, 8784 entries, over the period of one year.Besides correlation, what other methods can I use to achieve this task?","Creater_id":127899,"Start_date":"2016-08-16 12:37:09","Question_id":230160,"Tags":["time-series","correlation","measurement","agreement-statistics"],"Answer_count":2,"Last_activity":"2016-08-19 05:58:55","Link":"http://stats.stackexchange.com/questions/230160/how-to-assess-if-two-electricity-consumption-sets-are-corresponding","Creator_reputation":11}
{"_id":{"$oid":"5837a57aa05283111e4d432a"},"View_count":274,"Display_name":"Antoni Parellada","Question_score":5,"Question_content":"BACKGROUND: This is probably an extremely simple question to answer, but it's one of the downsides of being self-taught to get ensnarled in unexpected difficulties with basic stuff. This question stems from the last sentence in this post and the first sentence in the answer.QUESTION: The definition of a random vector is not that difficult - a list of random variables. Or, using the nice explanation by @whuber on the same post, we can tell that a vector is random by contemplating \"what changes would occur if the data-collection process were replicated.\"Part of the problem is understanding why the coefficients of the model matrix are treated as constants in regression. even if they are random variables (columns of a dataset). And probably this is connected somehow to my difficulties seeing why non-random vectors have zero covariance.In the end, given two numeric vectors of equal length, it seems like it should always be possible to get their means and perform the operation:  an eventually get the mean of the resulting multiplications, seemingly following the equation .I am sure there are many misunderstandings in these statements, but this is the question: Why is the covariance of non-random vectors always zero when it seems as though given any pair of numeric vectors of equal length, we can get their covariance? It may have to due with the difference between deriving the covariance of a probability density function versus the covariance between samples.In this sense I can calculate the covariance between two deterministically calculated vectors without any noise, corresponding to power functions of . The resultant vectors are not random, yet the correlation values still make some sense as a measure of proximity:par(mfrow = c(1,2))x \u0026lt;- seq(1, 10, by = 0.1)z \u0026lt;- seq(-3.1, -0.1, by = 0.1)o \u0026lt;- x^-3.1plot(o ~ x, col ='blue4', pch = 19, cex=0.1,     ylab = \"x to i power\")for (i in seq(z)){    b \u0026lt;- x^(z[i])    points(b  ~  x, pch = 20, cex = 0.1, col = i)}COR \u0026lt;- 0for (i in seq(z)){    b \u0026lt;- x^z[i]    COR[i] \u0026lt;- cov(b, o)}plot(COR ~ z, type=\"l\", lwd=2, col='blue4',      xlab=\"Exponent\", ylab = 'COVARIANCE')Is it purely a definitional type of difference - ... by definition covariance only applies to random variables...?I can see that a very straightforward way to prove this is to think that in a constant vector , and hence, , so perhaps the question really boils down to seeing what constitutes a random vector on a data set.INITIAL SELF-ANSWER ATTEMPT:[I am posting a very tentative answer in the hope of being corrected, and because I don't think that the opposite of a random vector is a vector with identical elements in every position as an example in the answers given seem to imply.]Perhaps the answer is as simple as to say that it is not a computational difference. Random and nonrandom vectors are indistinguishable unless we know how they should be interpreted: a vector is random if the assumption is that it represents a sample from a population. Random vector are a collection of random variables associated with the same events.For instance in the fictional dataset:     Subject    Weight   BP    Glucose1      AG       194      85      992      ST       185      86      1083      PS       180      81      1024      SS       167      87      100a 2-dimensional random vector can be defined as:and a sample from this random vector is precisely what the data frame contains - in this case 4 observations:We can estimate the mean of the population as  And use this sample mean to center  by subtracting column wise  from . This resultant center matrix could be called .In this way the covariance can be expressed as .These operations could naturally be performed on vectors that did not share the link of being observations on the same subjects, as in this case, and subject to noise as random variables. Or if they were deterministically calculated through algebraic equations as in the example in the OP.However, it seems as though there is simply no real need to extrapolate the concept of covariance to deterministic vectors, even beyond the obvious violation of the term \"co - variance\" when there is no variance in a nonrandom vector. For instance, in the case illustrated on the plot, the difference between the algebraic functions generating the vectors, would supersede any need for the \"covariance\" between the vectors. And I supposed the arithmetic subtraction of two otherwise unspecified deterministic vectors would do the same.I found the discussion in this forum of help, and to avoid broken links I will paste a couple of answers:\"Let's see, \"mean\", \"variance\" and \"covariance\" can be defined for samples of random variables or for the random variables themselves. When they are defined for the random variables themselves they are computed from the distribution functions for the random variables by integrations ( counting summation as a type of integration). So these procedures are performed on a specific function (which is what the original post means by a \"deterministic\" function, I suppose.) The same sort of integrations can be applied to a function that is not a distribution function.\" \"Perhaps the original post concerns whether this is every done [sic] and whether the things that are computed this way are still called \"mean\", \"variance\" and \"covariance\".\" \"I think a function that is not a distribution can have a \"first moment\" which is computed like a mean. It can have an \"L2-norm\" which is like the square root of the sum of the squares. So I think the same sorts of integrations are indeed done on functions that are not distribution functions. Whether a given field (like signal processing) uses the terminology \"mean\", \"variance\" and \"covariance\" for the things computed, I don't know.\"Reference https://www.physicsforums.com/threads/what-is-covariance-with-both-random-and-deterministic-variables.561504/\"","Creater_id":67822,"Start_date":"2015-10-29 20:55:15","Question_id":179346,"Tags":["covariance"],"Answer_count":3,"Last_activity":"2016-08-19 05:54:34","Link":"http://stats.stackexchange.com/questions/179346/covariance-of-non-random-vectors-equal-to-zero","Creator_reputation":7682}
{"_id":{"$oid":"5837a57aa05283111e4d4339"},"View_count":55,"Display_name":"ahmedmar","Question_score":1,"Question_content":"if  X∼N(2,1) then the expected value of −5X ? E(−5X) = -5E(X)if Y~N=(4,1) then the expected value of X+Y? E(X+Y)=E(X)+E(Y)Can you please tell me what is the meaning of multiplying or summing expected values ? ","Creater_id":100365,"Start_date":"2016-08-19 04:17:13","Question_id":230672,"Tags":["expected-value"],"Answer_count":2,"Last_activity":"2016-08-19 05:50:05","Link":"http://stats.stackexchange.com/questions/230672/regarding-the-expected-value-whats-the-meaning-of-e-5x-and-exy","Creator_reputation":216}
{"_id":{"$oid":"5837a57aa05283111e4d4347"},"View_count":39,"Display_name":"Vikash B","Question_score":0,"Question_content":"I have a quite big dataset of 10000 training data, i held out 2000 points for validation.I am using a Convolutional Neural Network and using minibatch stochastic gradient descent to minimize the RMS error, with minibatch size of 50,i am training for 1000 epochs.The training loss in the final epoch turns out to be 2.72 (averaging over the minibatch (size 50)).However the validation loss comes out much lesser, 0.33 to be precise (averaging over the 2000 validation points).How do i interpret these values, i am thinking that averaging over 2000 values compared to only 50 in the minibatch is playing a part.","Creater_id":128229,"Start_date":"2016-08-19 03:46:29","Question_id":230667,"Tags":["neural-networks","validation","train"],"Answer_count":1,"Last_activity":"2016-08-19 05:22:03","Link":"http://stats.stackexchange.com/questions/230667/interpreting-validation-and-training-loss","Creator_reputation":127}
{"_id":{"$oid":"5837a57aa05283111e4d4354"},"View_count":138,"Display_name":"rnso","Question_score":3,"Question_content":"I have a compilation of following data on 10 studies, all of which had 2 groups: a control group which received usual treatment and an active group which received an intervention. The subjects were followed up for one year and any adverse events were counted. The data is as follows: study   events_control  total_control   events_active       total_active'First et al'       25              100         38                  200'Second et al'      30              150         45                  400etcHence, in the first study, 25 of 100 in control group had events while 38 of 200 had events in active group. And so on.How can I combine all these data to determine if events in active group were more or less than those in control group? Thanks for your help.Edit: I was thinking of this option: since I have all the numbers, can I combine all 4 columns and get events in control and active groups and also total number of subjects with all studies taken together. Then I can compute overall odds ratio and its confidence interval. Using this method 55/250 events occurred in the control group and 83/600 in active group, giving the overall OR is 0.57 with 95% CI of 0.39 and 0.83. These are not too different from the estimate of 0.536 (CI: 0.156, 0.916) using metafor package, as detailed in the excellent answer by @user33. Can this be a reasonable strategy? If not, what are the drawbacks?Edit2: To take into account that data has come from different studies, can we use a mixed effects anova as follows: aov(event_rate ~ treatment_type + Error(study_id/treatment_type), data=mydata)","Creater_id":56211,"Start_date":"2015-06-11 04:53:58","Question_id":156480,"Tags":["meta-analysis"],"Answer_count":1,"Last_activity":"2016-08-19 05:14:23","Link":"http://stats.stackexchange.com/questions/156480/how-to-combine-data-several-studies-with-events","Creator_reputation":2782}
{"_id":{"$oid":"5837a57aa05283111e4d4361"},"View_count":297,"Display_name":"Stefan","Question_score":2,"Question_content":"I'm looking for a way to combine effect sizes (mean difference) and variances for several single studies but I don't want to conduct an overall meta-analysis. That means I'm only looking for an average score for each study.Example:Study 1 has 2 outcomes -\u003e average ES and variance of ES for Study 1Study 2 has 4 outcomes -\u003e average ES and variance of ES for Study 2Problem:I can calculate the mean of the effect sizes but that does not work for the variance. To average the variances I would need the correlations between the outcomes but these are rarely reported.Also please note that the outcomes within each study are very similar (e.g. two different measures for word problem solving).  Ideas so far (I also refer to this post):Estimate the covariance between outcomes using the formula by Gleser, L. J., \u0026amp; Olkin, I. (2009) (example in metafor): In principle this would work. However in some of my studies the sample sizes are not the same for every outcome. So I can't really apply the formula to every pair of outcome to calculate their covariance because of different Ns.In R-package MAd the mean of sample sizes between two outcomes is used to avoid these kind of problems. And also I have to guess the correlation between outcomes.Robust Variance Estimation: Does not seem suitable to combine multiple effect sizes of only one study because the variance between studies is needed to estimate an average variance (?). And the number of studies is too small for a decent estimation.Using Borenstein's (2009) formula to combine ES and variances. This would work because only effect sizes and variances of effect sizes are needed. However I would have to guess a correlation between every ES.I can also do some non-statistical methods like selecting only one ES per study.  I would prefer Borenstein's method. Compared to Gleser \u0026amp; Olkin I also have to guess a correlation but don't need to bother with different sample sizes.What do you think? Looking forward to your opinion!","Creater_id":85820,"Start_date":"2015-10-29 09:51:51","Question_id":179242,"Tags":["variance","covariance","meta-analysis","effect-size"],"Answer_count":1,"Last_activity":"2016-08-19 05:13:08","Link":"http://stats.stackexchange.com/questions/179242/combining-multiple-outcomes-in-studies-no-meta-analysis","Creator_reputation":48}
{"_id":{"$oid":"5837a57aa05283111e4d436d"},"View_count":53,"Display_name":"Dan K","Question_score":2,"Question_content":"I have identified 17 studies for use in a meta-analysis.  In order to make sure that I analyze the data correctly, I had a couple of questions regarding how to approach two of them.  First, one study uses two different Cognitive-Behavioural techniques (one more cognitive and one more behavioural) and uses the same outcome measure to test the difference between groups.  No statistical difference was demonstrated between the two and pre-post scores were used to measure effects without the use of control groups.  I am interested in CBT techniques overall, so would it be best to pool the scores together?Also, two studies were conducted whereby the first one used a delayed-treatment group as a control for an RCT, comparing an outcome measure after the first group received treatment.  Four years later, the data from those two groups were combined and then divided between 'depressed' and 'non depressed' and included follow-up data for up to two years after treatment.  This means that there is one study that uses an RCT model with fewer participants in the treatment group or a pre-post design with more participants.  I am particularly interested in the impact of self-esteem interventions on depression, so my gut instinct is to use the 'depressed' group from the latter study.  Would that be reasonable or would it be better to use an RCT instead?  For that matter, would it be best to perform a t-test on the effect sizes between the two?Thank you for any suggestions.","Creater_id":117503,"Start_date":"2016-06-10 15:48:29","Question_id":218389,"Tags":["meta-analysis"],"Answer_count":0,"Last_activity":"2016-08-19 05:10:18","Link":"http://stats.stackexchange.com/questions/218389/combining-data-in-the-same-study","Creator_reputation":16}
{"_id":{"$oid":"5837a57aa05283111e4d436f"},"View_count":40,"Display_name":"colin","Question_score":0,"Question_content":"I have modeled a relationship using beta-regression in the betareg package for R. I can predict values and associated variances of those predictions using the predict function in R. I'd like to convert these variances to standard deviations. This seems striaghtforward, just take the square root of the variance. However, there is an issue with this. First, the standard deviation is larger than the variance in beta regerssion, since the variance is always less than 1, and the square-root of a value less than 1 is always larger than the initial value. Second, and more concerning, when I convert to standard deviation by taking the square root of the variance, I get confidence intervals that overlap 0 and 1. This is a problem, because by its very nature beta-regression is designed to model values on the interval (0,1). Uncertainties beyond 0 and 1 are nonsensical. How can I go about calculating standard deviation from a variance estimate from a beta-regression model, such that the standard deviation envelope will never be less than 0 or greater than 1?","Creater_id":30451,"Start_date":"2016-08-18 07:19:43","Question_id":230501,"Tags":["variance","standard-deviation","beta-regression"],"Answer_count":1,"Last_activity":"2016-08-19 04:57:43","Link":"http://stats.stackexchange.com/questions/230501/variance-vs-standard-deviation-in-beta-regression","Creator_reputation":266}
{"_id":{"$oid":"5837a57aa05283111e4d437c"},"View_count":41,"Display_name":"KubiK888","Question_score":1,"Question_content":"I have the following data structure:Percent-\"yes\"    Average-score    Region70%              0.03             A66%              0.07             B57%              0.09             C85%              0.06             D.... etcI have aggregated data by regions. The outcome variable is Percent-\"yes\" (indicating the percent of people answering \"yes\" for a survey question) and explanatory variable is Average-score (indicating the average of a score by all the people in that region).The statistical model used is beta regression using logit link function. The beta-coefficient for Average-score is 0.06.I interpret it as \"For 1 unit increase in Average-score, there is a 0.06 units increase in log-odds of Percent-\"yes\"\". A colleague told me the 2 bolded elements in the statement are wrong, I have a tough time figuring out why that is the case despite exhausting all my reference material. Any insights or material I can check out to figure it out?","Creater_id":73456,"Start_date":"2016-08-18 19:11:03","Question_id":230615,"Tags":["logistic","regression-coefficients","logit","beta-distribution","beta-regression"],"Answer_count":1,"Last_activity":"2016-08-19 04:45:24","Link":"http://stats.stackexchange.com/questions/230615/interpreting-different-outputs-from-beta-regression","Creator_reputation":93}
{"_id":{"$oid":"5837a57aa05283111e4d4389"},"View_count":141,"Display_name":"Bel","Question_score":5,"Question_content":"I am trying to calculate weights for inverse probability weighting. For ATE and ATET the process is straightforward. For example in Stata:predict ps if e(sample)gen ate=1/ps if treatment==1replace ate=1/(1-ps) if treatment==0gen atet=1 if treatment==1replace atet=ps/(1-ps) if treatment==0My question is: how can i calculate the weights for the non-treated (ATENT)?","Creater_id":127854,"Start_date":"2016-08-16 06:24:42","Question_id":230095,"Tags":["econometrics","stata","propensity-scores"],"Answer_count":1,"Last_activity":"2016-08-19 04:41:56","Link":"http://stats.stackexchange.com/questions/230095/calculating-weights-for-inverse-probability-weighting-for-the-treatment-effect-o","Creator_reputation":26}
{"_id":{"$oid":"5837a57aa05283111e4d4396"},"View_count":122,"Display_name":"matlab_newby","Question_score":4,"Question_content":"I have read around a lot and tried different ways to carry out my cluster analysis.In the first case, I have carried out a hierarchical cluster analysis on my raw data (200 watersheds and 16 variables) in matlab and mapped the clusters.My second attempt incorporated carrying out a Principal Components Analysis on the raw data and then using the scores as inputs to my hierarchical clustering.The clusters produced in both cases are the exact same, which I did not expect. Can anyone explain to me why they are the same?","Creater_id":113452,"Start_date":"2016-08-17 09:31:41","Question_id":230319,"Tags":["clustering","pca","hierarchical-clustering"],"Answer_count":1,"Last_activity":"2016-08-19 04:20:59","Link":"http://stats.stackexchange.com/questions/230319/why-are-the-cluster-analysis-results-using-raw-data-the-same-as-the-ones-using-p","Creator_reputation":27}
{"_id":{"$oid":"5837a57aa05283111e4d43a3"},"View_count":38,"Display_name":"SunWuKung","Question_score":2,"Question_content":"I have two IRT adaptive online ability tests.Both have a relatively large item bank (n\u003e200) relatively few items displayed in a test (n=25) and quite a lot of test results (n\u003e10k) that I got using adaptive testing - so I have two large, but sparse response matrices.My hypothesis is that there is a significantly larger learning effect (people get better by doing the test, though I know this violates IRT assumptions) in one of the tests. How could I test this hypothesis?","Creater_id":67117,"Start_date":"2016-08-18 03:53:44","Question_id":230472,"Tags":["irt"],"Answer_count":1,"Last_activity":"2016-08-19 04:03:52","Link":"http://stats.stackexchange.com/questions/230472/detecting-and-comparing-learning-effect-in-adaptive-irt-tests","Creator_reputation":62}
{"_id":{"$oid":"5837a57aa05283111e4d43b0"},"View_count":1160,"Display_name":"Firat Kara","Question_score":5,"Question_content":"Suppose we have two patient groups, healthy and ill patients, and we measure the chemical concentrations of multiple chemicals. Now we want to determine which chemical concentrations are different for the two groups. Can we just apply a separate student T-test for each of the chemicals?It seems that for a large enough number of chemicals, you will always get a significant result for one of them. ","Creater_id":7158,"Start_date":"2011-11-01 05:01:59","Question_id":17803,"Tags":["t-test","multiple-comparisons"],"Answer_count":2,"Last_activity":"2016-08-19 03:47:26","Link":"http://stats.stackexchange.com/questions/17803/how-to-compare-two-groups-when-there-are-many-outcome-variables","Creator_reputation":26}
{"_id":{"$oid":"5837a57aa05283111e4d43be"},"View_count":44,"Display_name":"Darrell Leong","Question_score":1,"Question_content":"I need to transform a function of Rayleigh distributed variates  to one in standard normal space .The transformation is governed by equal exceedance probabilities in both spaces, such that the CDFs of the Standard Normal  and Rayleigh  must be equal:Where The transformation equation hence reduces to:As a check to be sure that this is correct, I had to validate that the origin in the standard normal space would translate to the mean value of , , ie:For Rayleigh distributed variates, the mean  and parameter  share the relationship:However, substituting this into the derived transformation equation doesn't give me the mean:Hence failing the validation check. What gives? Is this an approximation error?","Creater_id":128215,"Start_date":"2016-08-18 20:34:47","Question_id":230623,"Tags":["normal-distribution","standard","rayleigh","transform"],"Answer_count":1,"Last_activity":"2016-08-19 03:36:21","Link":"http://stats.stackexchange.com/questions/230623/how-can-i-transform-a-rayleigh-distribution-to-standard-normal-space","Creator_reputation":20}
{"_id":{"$oid":"5837a57aa05283111e4d43cb"},"View_count":14,"Display_name":"Santhe","Question_score":1,"Question_content":"I have 2 groups of patients. Each group has 22 patients. I wish to compare the variance in bladder volume for each patients from planning bladder volume(referance) to weekly bladder volume (week 1, week 2, week3, week 4 and week 5) in group 1 and group 2.Group 1             PLANNING BV  WEEK 1  WEEK 2   WEEK 3    WEEK 4   WEEK 5patient 1    132           100      127     156       120      100patient 2    150           148      153     176       101      171 patient 3    140           110      167     117       154      162I also wish to compare which group has more variance.I am not comparing means because i find there is a large variation between each week","Creater_id":128223,"Start_date":"2016-08-18 23:18:14","Question_id":230633,"Tags":["hypothesis-testing","statistical"],"Answer_count":0,"Last_activity":"2016-08-19 03:15:58","Link":"http://stats.stackexchange.com/questions/230633/compare-variance-between-2-group-and-within-group","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d43cd"},"View_count":23,"Display_name":"slakov","Question_score":1,"Question_content":"I have a table of variables (levels of different chemicals in the blood) for 2 groups of patients -- healthy and sick. I would like to find variables significantly different in those 2 groups. I can do t-test, but it does not account that the patients are of different age and gender, which can also affect the chemical content of the blood. Gender is binomial so we can do t-test on subgroups, but what is the most appropriate way to encounter for (continuous) Gender and, possibly, other confounders e.g. weight? Thank you! ","Creater_id":120897,"Start_date":"2016-08-19 02:58:18","Question_id":230663,"Tags":["t-test","normalization"],"Answer_count":0,"Last_activity":"2016-08-19 02:58:18","Link":"http://stats.stackexchange.com/questions/230663/correction-of-variables-for-age-and-gender","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d43cf"},"View_count":45,"Display_name":"Jeannie","Question_score":0,"Question_content":"The Paper Triple seasonal methods for short-term electricity demand forecasting by Taylor(2010) mentioned about Triple Seasonal Holt Winters Model, but in I can only find the function for Double Seasonal dshw() in R. Does anyone know whether there is a R function for the triple seasonal one?","Creater_id":114130,"Start_date":"2016-08-19 02:55:27","Question_id":230662,"Tags":["r","time-series","multiple-seasonalities"],"Answer_count":0,"Last_activity":"2016-08-19 02:55:27","Link":"http://stats.stackexchange.com/questions/230662/triple-seasonal-holt-winters-model-in-r","Creator_reputation":59}
{"_id":{"$oid":"5837a57aa05283111e4d43d1"},"View_count":17,"Display_name":"asdir","Question_score":0,"Question_content":"On panels of macroeconomic data I frequently hear the critique that China could drive a variable's effect. Short of running the same set of variables without the China data (or finding a variable that covers the criticized effects more precisely (an interaction with a China-FE dummy?)), is there a way to show that Chinese data is or isn't driving the variable's effect?My particular case right now, which serves as an example here, is an analysis of state-owned enterprises' influence on energy investments. Due to the large Chinese investments in energy (relative to GDP and GDP per capita) and the prevalence of SOEs in China, my results are frequently made out as just a \"China-effect\" (which they very well might be). I am wondering if there is a less crude way of testing this China effect than just excluding the China data and compare with the original analysis.Thanks for any help. Questions are welcome, of course.","Creater_id":27427,"Start_date":"2016-08-19 02:47:09","Question_id":230660,"Tags":["panel-data"],"Answer_count":0,"Last_activity":"2016-08-19 02:47:09","Link":"http://stats.stackexchange.com/questions/230660/how-to-deal-with-omitted-variable-bias-in-macro-panels-including-large-countries","Creator_reputation":50}
{"_id":{"$oid":"5837a57aa05283111e4d43d3"},"View_count":132,"Display_name":"Creatron","Question_score":2,"Question_content":"I am reading the Wikipedia article on statistical models here, and I am somewhat perplexed as to the meaning of \"non-parametric statistical models\", specifically:  A statistical model is nonparametric if the parameter set   is infinite dimensional.  A statistical model is semiparametric if  it has both finite-dimensional and infinite-dimensional parameters.   Formally, if  is the dimension of  and  is the number of  samples, both semiparametric and nonparametric models have  as .  If   as , then the model is semiparametric;  otherwise, the model is nonparametric.I get that if the dimension, (I take that to literally mean, the number of parameters) of a model is finite, then this is a parametric model. What does not make sense to me, is how we can have a statistical model that has an infinite number of parameters, such that we get to call it \"non-parametric\". Furthermore, even if that was the case, why the \"non-\", if in fact there are an infinite number of dimensions? Lastly, since I am coming at this from a machine-learning background, is there any difference between this \"non-parametric statistical model\" and say, \"non-parametric machine learning models\"? Finally, what might some concrete examples be of such \"non-parametric infinite dimensional models\" be?","Creater_id":27158,"Start_date":"2016-08-16 00:09:03","Question_id":230044,"Tags":["nonparametric","model"],"Answer_count":2,"Last_activity":"2016-08-19 02:46:35","Link":"http://stats.stackexchange.com/questions/230044/what-are-real-life-examples-of-non-parametric-statistical-models","Creator_reputation":452}
{"_id":{"$oid":"5837a57aa05283111e4d43e1"},"View_count":55,"Display_name":"Vikash B","Question_score":0,"Question_content":"I have seen that in many most learning algorithms, including decision tree learning algorithms, missing values are handled through imputation or estimation using EM algorithms and such.I wanted to know since decision trees make their decision based on rules, can't we have a tree which checks if the particular attribute is missing and proceed with separate rules for that.The following link describes this http://0agr.ru/wiki/index.php/Decision_Tree_%28Data_Mining%29#Handling_Missing_Values.Is this a good idea and will it give a better result than simply replacing the missing values with the mean.Are there any good libraries which implement this, the current one i am using is scikit-learn which doesn't do this.","Creater_id":128229,"Start_date":"2016-08-19 00:12:40","Question_id":230638,"Tags":["random-forest","missing-data","cart"],"Answer_count":1,"Last_activity":"2016-08-19 02:02:46","Link":"http://stats.stackexchange.com/questions/230638/handling-missing-values-for-decision-tree","Creator_reputation":127}
{"_id":{"$oid":"5837a57aa05283111e4d43ee"},"View_count":94,"Display_name":"PaoloH","Question_score":3,"Question_content":"(Sorry I'm neither a native english speaker or good at statistics) I've tried to make a short summary of what I'm trying to do but it wasn't clear at all, so sorry for the length of the post.I'm currently trying to study the length of sick leaves in administrations and so i have a database where have been put the lengths, dates, and other infos for thousands of leaves. I'm a complete beginner in that type of analysis. First of all, I'm studying the percentage of sick days that happens beyond the x-th day of a sick leave*. I've used the survival analysis package (survival) to simplify my work but this package is focused specifically on survival analysis and the ratio that i study isn't the survival ratio. So i'm looking for a package or a method that could help with what i'm actually looking at.*The actual ratio that I'm studying is the number of sick leave's days that happened after the x-th day divided by the total number of sick leave's days. (For instance if i have 3 leaves that are 2, 8 and 10 days long : the ratio for day 5 is (3+5)/(2+8+10)=0,4)The data that i have is a dataframe where in each line I have a single sick leave with pretty much any information that could be useful in a statistical analysis BUT I'm not supposed to use every piece of information. I'm supposed to study this in order to help for the pricing of contracts and the only variables that are communicated to them (and therefore are relevant in my work) are : -the type of contract (there are two types : I(mostly part-time workers) or C(full-time)),-the type of sick leaves (3 types : Ordinary(short \u0026lt;1 year), Rare(\u0026lt;3 year), very Life threatening(\u0026lt;5years))-I also have the amount of deductible days in the contract of the person who had a sick day (to know wheter or not deductible days induce a bias)-A few other things (I'll go into details if it is necessary)-Also my data was censored so I have a column with 0s for censored data and 1s for data that isn't.The only thing that has been taken into account for the censoring is this : the last time the dataframe has been updated was the 31/12/2013 any sick leaves that was still going on at that time has been considered censored. To be more accurate, what I have is the date on the medical report that allowed the sick leave so for instance I can have a sick leave where the end date is 25/02/2014 but it is not the actual end of the sick leave cause afterward it might have been extended so I put it has been censored at the 25/02/2014.First of all in order to correct censoring I used the survival package like so :  s \u0026lt;- Surv(time2013,event2013)  fKM \u0026lt;- survfit(s ~ 1,data=mydata)  res \u0026lt;- summary( fKM)Afterwards I only considered the results that were given to me by the survival model.What I've been asked to do was to determine what variable had enough influence. (I haven't been told what enough actually was so what I did so far, was to say if, for instance, when I compare sick leaves where the work contract was : I(part-time) with C(full-time), if I have more than ~4-5% of difference in my ratios then the variable had an influence and so I divided the population in two categories that i studied separately.)The problem I have now is to find a statistical way to tell if a variable has enough influence, so  I have the following plot (the two types of contract) I've said that there was enough difference so that when pricing the contracts they should price their insurance differently for each type of contract. Red is I(part time), Black is C(full time) the plot tells me that full-time workers have slightly longer ordinary(\u0026lt;1 year) leaves. What i would like to do now is find a way(most likely with an algorithm or another package) to measure the difference between those two plots. In order, to be able for instance to compare this difference with the difference there is in rare leaves(\u0026lt;3 years). So that I can know if a difference is significant by comparing it to the other differences. If you think another approach would be more useful I have no problem with this, comparing the differences was just what seemed the most intuitive analysis to me.Considering the Cox model, unless I'm making an error, I don't think (I might be wrong) that it is working in my situation because of the results that I get.For instance I have two situations ; the first one here is the plot of the ratio I'm studying (not the ratio that the cox model is using, I didn't want to put too many images the post is already too long) for a population that is divided in 2 groups (Red is full-time workers and Black is part-time workers). I apply \"coxph\" I get a coefficient for the type of contract of 0.223 which is interpreted, if I understood correctly, as : \"full-time workers have longer sick leaves\", which we don't really see on the plot of the percentages of days beyond the x-th day :But on another situation where I have this plot :When I use \"coxph\" I get a coefficient of 0.2175 which is basically,  giving me the same conlusion. But intuitively i would have said that in the first situation the differences were insignificant and in the second they were meaningful. That's why I haven't used \"coxph\". Maybe you can find an issue in the code that i used :mydata_MONON_CENSURE is 0 for censored data and 1 for not censored data;IND_AG_CONTRAT is 1 for full-time, 0 for part-timetimeMO\u0026lt;-mydata_MONON_CENSUREcoxph.fit \u0026lt;- coxph(Surv(timeMO,eventMO) ~ IND_AG_CONTRAT, data=mydata_MO)summary(coxph.fit)Sadly, I can not share the data, but what i believe is the issue is that both cases are very similar except that there are a few extremely long leaves in the second case that are not in the first situation which is why there is such a big difference on the graphs.","Creater_id":127240,"Start_date":"2016-08-18 02:10:35","Question_id":230644,"Tags":["r","time-series","survival"],"Answer_count":1,"Last_activity":"2016-08-19 01:50:34","Link":"http://stats.stackexchange.com/questions/230644/what-r-packages-or-algorithms-would-you-use-in-that-kind-of-analysis","Creator_reputation":58}
{"_id":{"$oid":"5837a57aa05283111e4d43fb"},"View_count":12,"Display_name":"luchonacho","Question_score":0,"Question_content":"I am estimating a panel data model of log wage, where I add an indicator for each period. The idea is to see if there is a trend in the baseline wage over time.I estimate the model using both random and fixed effects, and a Hausman test provides conclusive evidence that the FE is to be used (assuming as always that the FE model is consistent in the first place).Now, when I plot the time effects, for the RE they are increasing whereas for the FE they are decreasing!According to the textbooks (Cameron Trivedi 2010, Wooldridge 2010), both RE and FE estimate the same parameter  for each period. Is this result further evidence that the RE model is not consistent because of the unobserved heterogeneity? Or there is a problem in the FE model?","Creater_id":100369,"Start_date":"2016-08-19 01:43:50","Question_id":230651,"Tags":["panel-data","random-effects-model","fixed-effects-model","hausman"],"Answer_count":0,"Last_activity":"2016-08-19 01:43:50","Link":"http://stats.stackexchange.com/questions/230651/inconsistency-in-time-factors-in-random-versus-fixed-effects","Creator_reputation":584}
{"_id":{"$oid":"5837a57aa05283111e4d43fd"},"View_count":32,"Display_name":"user128231","Question_score":2,"Question_content":"I am interested in ensemble modeling and have researched a lot a about how people create their ensemble model.But there are different ways that people create it and I am confused which one is correct.I will list out the types and please tell me which one is the correct way to create an ensemble classification model.Type 1. Creates 10 models with the same algorithm but different parameters.For example, KNN(k=1),KNN(k=2),KNN(k=3),...KNN(k=10).Type 2. Creates 10 models with 10 different algorithms.For example, SVC,NuSVC,Naive Bayes,Bernoulli Naive Bayes, Gaussian Naive Bayes,...p.s. I have learned that SVC and NuSVC is SVM but is implemented differently. Should these two be treated as different algorithms because they do the same thing. Same about the different Naive Bayes algorithms","Creater_id":128231,"Start_date":"2016-08-19 00:09:28","Question_id":230637,"Tags":["machine-learning","classification","ensemble"],"Answer_count":1,"Last_activity":"2016-08-19 01:40:39","Link":"http://stats.stackexchange.com/questions/230637/question-about-ensemble-modeling","Creator_reputation":11}
{"_id":{"$oid":"5837a57aa05283111e4d440a"},"View_count":63,"Display_name":"Max","Question_score":0,"Question_content":"I've built a model that tries to evaluate the percentage chance of winning for all NHL games.  For example:  Ana vs Bos (Bos- 57% chance of winning)Chi vs Mtl (Mtl - 51% chance of winning)LA vs Det (Det- 80% chance of winning)The model is updated daily to take into account what happened in the latest games. At the end of the season, how can I validate my model against the actual results?If my percentages were a fixed set (55%, 75%, 95%), it would be easy to validate. My model should be right 55% of time for all games that have a 55% chance of winning. My model should be right 95% of time for all games that have a 95% chance of winning.But since my percentages are all different I don't know how to deal with them.  For example, I have predicted percentages like: 51.5%, 53,2%, 54.9%, 62.4%, 88.1%, etc. These came from my model. I'm using a lot of historical data to predict the winner (past games, goalie stats, active players, home vs away, etc., it is similar to the odd column on this page).  ","Creater_id":14228,"Start_date":"2016-08-18 07:01:34","Question_id":230499,"Tags":["forecasting","predictive-models","prediction-interval","model-evaluation"],"Answer_count":2,"Last_activity":"2016-08-19 01:33:17","Link":"http://stats.stackexchange.com/questions/230499/how-to-validate-predicted-proportions-from-a-model-when-the-proportions-are-not","Creator_reputation":142}
{"_id":{"$oid":"5837a57aa05283111e4d4417"},"View_count":34,"Display_name":"Keerthi","Question_score":0,"Question_content":"I have a set of temperature datasets all from different products. I wish to calculate the variation in the products. I have tried using coefficient of variation. But because the temperature data varies from say 20-40 deg C, the mean would be high and standard deviation low, the coefficient of variation is very low. I wish to know an alternative in this case to give a proper representation.It would be great if you can help.Ex. 27.2    30.4    30.4    31.0    31.9    35.2Mean:31.0;SD=2.6; CV=8.4","Creater_id":128234,"Start_date":"2016-08-19 00:33:56","Question_id":230639,"Tags":["coefficient-of-variation"],"Answer_count":1,"Last_activity":"2016-08-19 01:00:56","Link":"http://stats.stackexchange.com/questions/230639/alternatives-to-coefficient-of-variation","Creator_reputation":1}
{"_id":{"$oid":"5837a57aa05283111e4d4424"},"View_count":33,"Display_name":"user106927","Question_score":1,"Question_content":"This is my first question on Cross Validated so please bear with me if my question is lagging in any dimension.My question regards how to evaluate a Jacobian matrix when one variable is binary. I have googled quite extensively but found no answer. Setup:I have three variables ,  and .  and  come from a copula and thus have uniform marginal distributions and covariance . The binary variable  is independent of  and  and .I define the vector  with mean  and variance :\\mathbf{x}=\\begin{pmatrix}p\\\\r\\\\s\\end{pmatrix}, \\mathbf{\\bar x}=\\begin{pmatrix}\\bar p\\\\\\bar r\\\\w\\end{pmatrix},\\mathbf{\\Sigma}_\\mathbf{x}=\\begin{pmatrix}\\sigma_p^2\u0026amp;\\sigma_{pr}\u0026amp;0\\\\\\sigma_{pr}\u0026amp;\\sigma_p^2\u0026amp;0\\\\0 \u0026amp;0\u0026amp;w(1-w)\\end{pmatrix}The variables  and  are used to construct a new variable  with the function  defined as:\\Lambda(r,s)=wG\\left(s{G}^{-1}(r)+(1-s){F}^{-1}(r)\\right)+(1-w)F\\left(s{G}^{-1}(r)+(1-s){F}^{-1}(r)\\right) and  are continuous distributions with support on the positive real line.I want to know the covariance between  and the new variable created by . My approach has been to use the Delta method.I define a new vector:\\mathbf{y}=q(\\mathbf{x})=\\begin{pmatrix}p\\\\\\Lambda(r,s)\\end{pmatrix}In the case where all variables where continuous I would use the delta method to find an expression for the covariance matrix of :\\mathbf{\\Sigma}_\\mathbf{y}=\\mathbf{D}\\mathbf{\\Sigma}_\\mathbf{x}\\mathbf{D}^Twhere  is the Jacobian evaluated at , .QuestionThe last part is my central issue: how (if possible) can I calculate  when  is a binary variable?My own SuggestionI have been toying around with central differences. But I don't know if this approach works for the delta method. The Jacobian of  in this case could be defined as:\\mathbf{J}=\\begin{pmatrix}  1\u0026amp;0 \u0026amp;0\\\\  0 \u0026amp;\\frac{\\partial\\Lambda(r,s)}{\\partial r} \u0026amp;\\frac{\\Lambda(r,1)-\\Lambda(r,0)}{2} \\end{pmatrix}But how do I get from  to ? It is not obvious to me how to evaluate the  and , which is necessary for knowing .","Creater_id":106927,"Start_date":"2016-08-18 13:26:48","Question_id":230569,"Tags":["covariance-matrix","discrete-data","delta-method","taylor-series","jacobian"],"Answer_count":0,"Last_activity":"2016-08-19 00:59:42","Link":"http://stats.stackexchange.com/questions/230569/delta-method-with-mix-of-continuous-and-discrete-variables","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d4426"},"View_count":93,"Display_name":"Lin Ma","Question_score":1,"Question_content":"I quote the whole related documents about Bayesian theory, and I am confused by a specific part \"from the Bayesian viewpoint there is only a single data set D (namely the one that is actually observed)\". My question is, suppose we have a few choices for values of parameter w, but we only have one observation of data set D, and if data set D is limited and bias, there is possible no co-existence of some possible value of w and D together, when how do we calculate in such case P(D|w)? Thanks. For example, if we want to calculate P(D = win lottery | w = women) and in observed data set D, there is no data about women, how do we going to do? Treating it as zero posterior (P(w = woman| D = win lottery)) seems not very perfect?regards,Lin","Creater_id":18254,"Start_date":"2016-08-18 22:07:09","Question_id":230629,"Tags":["probability","bayesian","maximum-likelihood","likelihood"],"Answer_count":1,"Last_activity":"2016-08-19 00:53:31","Link":"http://stats.stackexchange.com/questions/230629/calculate-likelihood-with-only-one-observation-of-data","Creator_reputation":108}
{"_id":{"$oid":"5837a57aa05283111e4d4433"},"View_count":16,"Display_name":"nymerion","Question_score":1,"Question_content":"I am currently working on implementing a multi-task model for Gaussian process prediction, but am having some trouble interpreting the materials online (conference talks, papers, presentations/slides from the authors, etc.) as a lot of what is described is understandably assumed knowledge that I'm slowly working my head around. Note: apologies for the raw links below, as I couldn't post more than 2 proper ones given my \u0026lt;10 reputation.The paper in particular I'm looking at is Bonilla et al's paper (papers.nips.cc/paper/3189-multi-task-gaussian-process-prediction.pdf). What specifically does each 'task' refer to in multi-task learning - is it the relationship between features, or something else entirely? I've been (perhaps incorrectly) trying to link the logic of it to a co-regionalisation tutorial here (nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/coregionalized_regression_tutorial.ipynb), where there is an improvement on a standard GP's result from the below:To this:.This would lead me to think that it is a correlation between features that allows the improved model - but then, I am left confused as to how that would differ from the covariance between the training and test sets in this simple dataset.I also realise there is only a single output here (with other threads on Cross Validated saying multiple outputs is a necessary but not sufficient condition of multi-task learning - stats.stackexchange.com/questions/161025/in-the-context-of-anns-is-there-multi-task-learning-iff-the-network-has-more-th), but this paragraph in the paper mentioned above:  In the geostatistics literature, the prior model for f· given in eq. (1) is known as the intrinsic correlation  model [7], a specific case of co-kriging. A sum of such processes is known as the linear  coregionalization model (LCM) [7] for which [6] gives an EM-based algorithm for parameter estimation.Has left me unsure on that front as well.Any help regarding what the tasks are in a concrete example in multi-task learning and how they relate to the simple 1D coregionalisation example (if applicable) would be greatly appreciated. Thanks in advance!","Creater_id":123554,"Start_date":"2016-08-19 00:35:13","Question_id":230640,"Tags":["machine-learning","gaussian-process","multitask-learning"],"Answer_count":0,"Last_activity":"2016-08-19 00:35:13","Link":"http://stats.stackexchange.com/questions/230640/what-are-tasks-in-multi-task-transfer-learning-linking-to-co-regionalisation","Creator_reputation":13}
{"_id":{"$oid":"5837a57aa05283111e4d4435"},"View_count":188,"Display_name":"Nick Sabbe","Question_score":6,"Question_content":"I'm currently working on asymptotic properties of penalized regression. I've read a myriad of papers by now, but there is an essential issue that I cannot get my head around.To keep things simple, I'm going to look at the minimization of-l(\\beta, X, Y) + n \\lambda p(\\beta)for some reasonable penalty function  (and  the loglikelihood). In theorems regarding the asymptotic properties of the resulting estimator, typically a requirement is imposed on , or more precisely two requirements: an upper and lower bound on its behaviour for large  (e.g.  and  for . This is a requirement that shows up in papers by Fan en Li (SCAD), Zou (Adaptive Lasso) and some others.My issue with this is that it is never specified how to impose such boundaries. In practice, you have a single dataset and try to find the best possible value for the tuning parameter , but of course in this case the sample size doesn't change and definitely is no approaching infinity.My guess is that it means that your method to select the best value for  (e.g. crossvalidation, AIC or BIC or similar) should be such that the limiting behaviour is as required, but noone ever proves this, or at least I have not been able to find it.So, in short: can any of you explain to me how to work with these requirements for , or point me to papers/books/.../suggest a simulation experiment/ whatever that makes these issues clear. I'm hoping to prove similar asymptotic properties in settings beyond maximum likelihood, but then I need to understand the state of the art to its fullest.EDIT:Reading, re-reading and re-re-reading some of the papers, I finally realised that the asymptotic properties (of interest to me: consistent model selection and by extension, the oracle properties) perhaps do not require the tuning parameter selection to uphold the limiting behaviour on the parameter itself. The theorems typically show that any  series that satisfies the limiting conditions will result in an estimator with the properties of interest.As such, I just have to pick my  that performs best, and \"virtually promise\" if I were to redo the analysis on a larger/smaller dataset, that I would scale that  accordingly.If this is correct, this only leaves me with my classical problem around crossvalidation: here, the effectiveness of the model is evaluated on (e.g.) 9/10 of the data. Even if I scale  the right way, what guarantees that whichever criterion I'm using, scales along with it? This appears to be less of a problem with other methods of choosing the tuning parameter. Can anybody shed some light on this (I'm still trying to get my head around @Stefan Wager's comment, so maybe it's in there already)?","Creater_id":4257,"Start_date":"2013-04-25 06:35:00","Question_id":57219,"Tags":["regression","lasso","regularization","parameterization","penalized"],"Answer_count":0,"Last_activity":"2016-08-18 23:45:52","Link":"http://stats.stackexchange.com/questions/57219/asymptotic-property-of-tuning-parameter-in-penalized-regression","Creator_reputation":8214}
{"_id":{"$oid":"5837a57aa05283111e4d4437"},"View_count":57,"Display_name":"Arijit Mitra","Question_score":2,"Question_content":"I have a bunch of independent variables which are skewed and have negative and zero values. I am seeing a lot of suggestions of using cube root as a transformation. What would be the harm in using  instead?","Creater_id":119325,"Start_date":"2016-06-08 09:41:25","Question_id":217941,"Tags":["regression","data-transformation"],"Answer_count":1,"Last_activity":"2016-08-18 23:44:39","Link":"http://stats.stackexchange.com/questions/217941/transformation-of-skewed-independent-variables-with-negative-values","Creator_reputation":16}
{"_id":{"$oid":"5837a57aa05283111e4d4444"},"View_count":14,"Display_name":"GGA","Question_score":0,"Question_content":"In a paper by Altman (doi: 10.1093/jnci/86.11.829) the use of optimal cutpoint is discussed and a formula is proposed to correct the p-value obtained with this method.POT = -1.63*Pmin*(1 + 2.35 log(PMn)) for epsilon = 10%the minimum p-value obtained maxing the log-rank test is entered in this formula and a corrected p-value can be calculated.If a variable is still significant after this correction, if I use the variable in a model, do I have to correct also the subsequent p-values in the model or not since the original correction was significant?","Creater_id":97342,"Start_date":"2016-08-18 23:17:37","Question_id":230632,"Tags":["p-value"],"Answer_count":0,"Last_activity":"2016-08-18 23:17:37","Link":"http://stats.stackexchange.com/questions/230632/altman-correction-for-optimal-cut-point","Creator_reputation":171}
{"_id":{"$oid":"5837a57aa05283111e4d4446"},"View_count":43,"Display_name":"Richard Hardy","Question_score":2,"Question_content":"Consider a dependent variable , independent variables , a model  y = X \\beta + \\varepsilon, and an estimated coefficient . If the model is correctly specified, the true conditional mean of  given  is  \\mathbb{E}(y|X) = X \\beta. Since we do not know , we use its sample estimate  to get the estimated conditional mean \\hat{\\mathbb{E}}(y|X) = X \\hat\\beta. For a new set of observations , we can predict the conditional mean of the corresponding  using the new s and the estimated coefficient  as follows: \\hat{\\mathbb{E}}(y_i|x_{1,i},\\dotsc,x_{K,i}) = (x_{1,i},\\dotsc,x_{K,i}) \\hat\\beta. Now let us evaluate the forecast accuracy. We take the realized value  and compare it to the predicted value . If the two are close, we say that the forecast is accurate.Here is what bugs me: Aren't we predicting the true conditional mean  rather than the actual realization ? If so, we are committing a measurement error when using  in place of the unobserved  when evaluating forecast accuracy. Isn't this problematic?(One may also think about modelling higher order moments, such as conditional variance. There it is more obvious that the population moment being forecasted is unobservable, and hence measuring forecast accuracy is nontrivial.)","Creater_id":53690,"Start_date":"2016-04-07 03:27:18","Question_id":206020,"Tags":["forecasting","mean","expected-value","accuracy"],"Answer_count":1,"Last_activity":"2016-08-18 23:03:36","Link":"http://stats.stackexchange.com/questions/206020/measuring-forecast-accuracy-of-the-conditional-mean","Creator_reputation":12907}
{"_id":{"$oid":"5837a57aa05283111e4d4453"},"View_count":17,"Display_name":"Guest1","Question_score":2,"Question_content":"I am comparing the frequencies of a disease presenting in particular season.The expected proportion would be 0.25 and the observed frequencies for spring, summer, autumn, and winter were 7, 9, 11, and 16 respectively (total n=43).  Could you tell me if what I have done is correct?When I ran a Kolmogorov-Smirnov goodness-of-fit test the p value was 0.207, so not a significant deviation from what would be expected.However, I wanted to specifically see if the disease was more likely to present in Winter so I performed a binomial test essentially 16/43 vs 1/4. The binomial probability in this case was 0.026.  I'm not sure if this was the correct use of this test?  Thanks for your help with this.    ","Creater_id":128226,"Start_date":"2016-08-18 22:37:26","Question_id":230630,"Tags":["dataset","frequency"],"Answer_count":0,"Last_activity":"2016-08-18 22:37:26","Link":"http://stats.stackexchange.com/questions/230630/how-do-i-compare-frequency-data-of-more-than-two-groups","Creator_reputation":11}
{"_id":{"$oid":"5837a57aa05283111e4d4455"},"View_count":81,"Display_name":"MiniQuark","Question_score":6,"Question_content":"There are so many regularization techniques, it's not practical to try out all combinations:l1/l2max normdropoutearly stopping...It seems that most people are happy with a combination of dropout + early stopping: are there cases where using other techniques makes sense?For example, if you want a sparse model you can add in a bit of l1 regularization. Other than that, are there strong arguments in favor of sprinkling in other regularization techniques?I know about the no-free-lunch theorem, in theory I would have to try out all combinations of regularization techniques, but it's not worth trying if it almost never yields a significant performance boost.","Creater_id":109561,"Start_date":"2016-08-18 10:43:22","Question_id":230544,"Tags":["neural-networks","regularization"],"Answer_count":1,"Last_activity":"2016-08-18 22:11:58","Link":"http://stats.stackexchange.com/questions/230544/are-early-stopping-and-dropout-sufficient-to-regularize-the-vast-majority-of-dee","Creator_reputation":429}
{"_id":{"$oid":"5837a57aa05283111e4d4462"},"View_count":18,"Display_name":"cgreen","Question_score":2,"Question_content":"The Wiener–Khinchin theorem states that, for stationary processes, the covariance is the Fourier dual of the spectral density. I'm wondering if this statement is true for non-stationary processes if, say, the process only changes slowly. Within some range in which the process is mostly stationary, does the spectral density relate to the covariance?","Creater_id":59667,"Start_date":"2016-08-18 21:57:10","Question_id":230627,"Tags":["gaussian-process"],"Answer_count":0,"Last_activity":"2016-08-18 21:57:10","Link":"http://stats.stackexchange.com/questions/230627/local-spectrum-of-a-non-stationary-gaussian-process","Creator_reputation":138}
{"_id":{"$oid":"5837a57aa05283111e4d4464"},"View_count":22,"Display_name":"beroe","Question_score":2,"Question_content":"We are comparing the prevalence of a trait across 40 depth bins in the ocean. The notable part is that the percentage of organisms with this trait is nearly the same (80%) across all depth bins, despite different total counts and diversity of entities. My question is how to summarize these percentages and show the low variability of this value across the depths. We are not as interested in a multiple comparison / null hypothesis test, as I have found those.  It feels wrong to average and derive standard deviation for percentages. The total count of data points is around 480k, but N can be a couple orders of magnitude different between bins, so the ratio we are comparing could be 8/10 or 8000/10000. ","Creater_id":40067,"Start_date":"2016-08-16 22:04:34","Question_id":230206,"Tags":["meta-analysis","percentage"],"Answer_count":1,"Last_activity":"2016-08-18 21:14:20","Link":"http://stats.stackexchange.com/questions/230206/summarizing-several-percentages-of-uneven-size-samples","Creator_reputation":128}
{"_id":{"$oid":"5837a57aa05283111e4d4471"},"View_count":22,"Display_name":"Arc","Question_score":1,"Question_content":"I have a time series of around 45 data points which represent sales in 50 consecutive months. However, the first 25 months are highly influenced by hiring of new salesmen every month. The first 25 months almost show linearly increasing sales. The last 20 months, are however, more like a time series. What should be my approach for such a problem?","Creater_id":1944,"Start_date":"2016-08-18 19:35:13","Question_id":230618,"Tags":["r","machine-learning","time-series","python","arima"],"Answer_count":0,"Last_activity":"2016-08-18 19:35:13","Link":"http://stats.stackexchange.com/questions/230618/time-series-with-external-regressors-for-a-part-of-it","Creator_reputation":118}
{"_id":{"$oid":"5837a57aa05283111e4d4473"},"View_count":21,"Display_name":"user1205901","Question_score":0,"Question_content":"I'm looking at the tradeoff between the accuracy of particular forecasting methods (measured in mean squared error) with cost of setting up the methods (measured in the number of forecasts people need to use). This is assessed across various datasets, and since the scales wildly vary the MSEs also vary wildly between datasets. The number of forecasts also varies a bit across datasets. Ultimately I want to compare across datasets and to reach conclusions about which methods work best under particular assumptions.My inclination is to first standardize the MSEs that are produced by various methods. Then, multiply a constant into the number of forecasts used by a method, and add the resulting value to the MSE it is connected with.If the constant is 1 then we're essentially saying that 1 forecast costs a whole SD of MSE. Then the best method would be the one that used the fewest forecasts, since forecasting here is so expensive. If the constant is .00000000001 units then forecasts are really cheap. Therefore, the best method will be the one that delivers the lowest MSE, no matter if it uses a lot of forecasts in order to obtain that low MSE.If the constant is some intermediate value, then an economical (in terms of number of forecasts) method might or might not outperform a more expensive one. Taking a particular value of the constant, I can look across datasets and say which method is best.Is this approach sound? If not, what should I be doing instead (e.g. normalization instead of standardization)?Is there some principled way I can decide what values of the constant to look at?","Creater_id":9162,"Start_date":"2016-08-18 07:38:28","Question_id":230510,"Tags":["forecasting","valuation"],"Answer_count":0,"Last_activity":"2016-08-18 19:17:53","Link":"http://stats.stackexchange.com/questions/230510/is-this-a-sound-method-of-comparing-the-costs-benefits-of-forecast-methods-acros","Creator_reputation":2023}
{"_id":{"$oid":"5837a57aa05283111e4d4475"},"View_count":25,"Display_name":"Buck Shlegeris","Question_score":0,"Question_content":"Suppose I have a bunch of data on whether people like different TV shows. I have a bunch of data about whether particular viewers like particular TV shows.For every viewer, I also have their answer (as a score from 1 to 5) to the question “How much does the quantity of explosions in TV shows you watch affect your enjoyment of the show?” And for every show, I have a score (again from 1 to 5) of how many explosions there are.I want to see whether people are correct about whether they enjoy explosions in TV shows.One approach would be: look for the correlation between rating and quantity of explosions on every user’s viewing history to get the strength of their preference for explosions, and then try to measure the correlation between this correlation strength and the user’s professed explosion preference. But this doesn’t take into account the fact that some users have rated more TV shows than others, and the users with more ratings should provide more evidence.What is the best way to test this?","Creater_id":107796,"Start_date":"2016-08-18 18:51:04","Question_id":230612,"Tags":["regression","t-test"],"Answer_count":0,"Last_activity":"2016-08-18 19:03:40","Link":"http://stats.stackexchange.com/questions/230612/trying-to-measure-strength-of-correlation-among-correlations","Creator_reputation":81}
{"_id":{"$oid":"5837a57aa05283111e4d4477"},"View_count":169,"Display_name":"putut purwandono","Question_score":3,"Question_content":"I have a tough challenge using the DID. I have only 2 year data set, 2010 and 2015. The first is the baseline and the latter is follow-up year. In order to obtain true causal effect using DID, I should ensure that common trend assumption is satisfied, which is in my case It is impossible to test due to data limitation. Another case is that the covariates between treated units and the controlled units are very likely unbalance. Basically I compare apples with oranges.This condition makes me to use what I called as propensity score-weighted DID. So, I run a probit regression first to obtain propensity scores for each units using baseline data. I use the propensity score as weight to each sample in implementing the DID which is a panel data set-based. The weight for treated units is 1 and for the controlled units is p/(1-p) where p is propensity scores of each controlled units. Then I will get DID estimator.Did I do the right procedures? Thank you.","Creater_id":124643,"Start_date":"2016-08-04 19:05:09","Question_id":228367,"Tags":["econometrics","matching","propensity-scores","difference-in-difference"],"Answer_count":1,"Last_activity":"2016-08-18 18:35:53","Link":"http://stats.stackexchange.com/questions/228367/propensity-scores-weighted-did","Creator_reputation":21}
{"_id":{"$oid":"5837a57aa05283111e4d4484"},"View_count":12760,"Display_name":"Arya","Question_score":31,"Question_content":"I'm curious about the nature of . Can anybody tell something intuitive about \"What does  say about data?\"Edit:Thanks for repliesAfter taking some great courses, I'd like to add some points:It is measure of information, i.e.,  is amount of info along the direction .Duality: Since  is positive definite, so is , so they are dot-product norms, more precisely they are dual norms of each other, so we can derive Fenchel dual for the regularized least squares problem, and do maximization w.r.t dual problem. We can choose either of them, depending on their conditioning.Hilbert space: Columns (and rows) of  and  span the same space. So there is not any advantage (other that when one of these matrices is ill-conditioned) between representation with  or Bayesian Statistics: norm of  plays an important role in the Bayesian statistics. I.e. it determined how much information we have in prior, e.g., when covariance of the prior density is like   we have non-informative (or probably Jeffreys prior)Frequentist Statistics: It is closely related to Fisher information, using the Cramér–Rao bound. In fact, fisher information matrix (outer product of gradient of log-likelihood with itself) is Cramér–Rao bound it, i.e.  (w.r.t positive semi-definite cone, i.e. w.r.t. concentration ellipsoids). So when  the maximum likelihood estimator is efficient, i.e. maximum information exist in the data, so frequentist regime is optimal. In simpler words, for some likelihood functions (note that functional form of the likelihood purely depend on the probablistic model which supposedly generated data, aka generative model), maximum likelihood is efficient and consistent estimator, rules like a boss. (sorry for overkilling it)","Creater_id":31645,"Start_date":"2013-10-22 05:00:53","Question_id":73463,"Tags":["bayesian","maximum-likelihood","covariance","matrix"],"Answer_count":2,"Last_activity":"2016-08-18 18:20:51","Link":"http://stats.stackexchange.com/questions/73463/what-does-the-inverse-of-covariance-matrix-say-about-data-intuitively","Creator_reputation":338}
{"_id":{"$oid":"5837a57aa05283111e4d4492"},"View_count":28,"Display_name":"William","Question_score":0,"Question_content":"   Question: If we have a sequence of -valued random variables  which converge in the metric space of random variables defined by the energy distance to the -valued random variable , i.e. D(X_n,X) \\to 0 then is this equivalent to the sequence  converging to  in , i.e. \\mathbb{E}[(X_n - X)^2] \\to 0?My thinking is that  convergence implies that \\E X_n^2 - 2\\E X_n X + \\E X^2 \\to 0 which looks similar to the formula of covariance, and thus makes me think that there might be a relationship with Brownian covariance and distance covariance. Also energy distance is equal up to a constant factor to the Cramer distance (the formula for which looks like the definition of  convergence) for real-valued random variables. This is fortuitous because I am interested primarily in the real-valued case.Finally there seems like there might be a relationship between energy (in physics) going to zero and  convergence, so if there is an analogous relationship between energy distance and  convergence, that would explain why energy distance is called energy distance (Wikipedia doesn't explain the etymology of the name). I have asked questions about this relationship before on StackExchange here, here, and here.Also it would be interesting too because then one could interpret least-squares regression as \"energy-minimizing\" regression. But my main goal is to argue that the  convergence that one has in the definition of stochastic integrals (as opposed to more conventional almost sure convergence) is natural in the sense that it is equivalent pointwise convergence of the \"energy difference\" between the approximating processes and the stochastic integral to zero for each moment in time.","Creater_id":113090,"Start_date":"2016-08-18 17:51:58","Question_id":230601,"Tags":["machine-learning","mathematical-statistics","convergence","distance","distance-covariance"],"Answer_count":1,"Last_activity":"2016-08-18 18:14:10","Link":"http://stats.stackexchange.com/questions/230601/energy-distance-and-l2-convergence","Creator_reputation":1902}
{"_id":{"$oid":"5837a57aa05283111e4d449f"},"View_count":28,"Display_name":"Christof","Question_score":2,"Question_content":"I need to generate a sequence of pseudorandom numbers with a period larger than , using a seed larger than 80 bits, on a 32-bit embedded  microprocessor. What are my possible options (in terms of PRNGs), and which one is the fastest. The PRNG should pass the NIST 800-22 randomness tests.","Creater_id":128204,"Start_date":"2016-08-18 17:54:38","Question_id":230602,"Tags":["random-generation","randomness"],"Answer_count":0,"Last_activity":"2016-08-18 17:54:38","Link":"http://stats.stackexchange.com/questions/230602/a-pseudorandom-number-generator-with-a-seed-greater-than-80-bits","Creator_reputation":11}
{"_id":{"$oid":"5837a57aa05283111e4d44a1"},"View_count":574,"Display_name":"user1398057","Question_score":5,"Question_content":"I have a matrix  in which I would like to compute the correlation matrix of. The matrix in R looks like:\u0026gt; G     [,1] [,2] [,3] [,4] [,5] [,6][1,]   -1    0   -1   -1    0   -1[2,]   -1    0   -1    1    1    0[3,]   -1    0    1   -1    0    1[4,]   -1    0    1    1   -1    0[5,]    1   -1    0   -1    0    1[6,]    1   -1    0    1   -1    0[7,]    1    1    0   -1    0   -1[8,]    1    1    0    1    1    0Now, I would like to find the correlation matrix. However, when I use the function cor(), I get the following:\u0026gt; cor(G)     [,1] [,2] [,3] [,4] [,5] [,6][1,]    1  0.0  0.0    0  0.0  0.0[2,]    0  1.0  0.0    0  0.5 -0.5[3,]    0  0.0  1.0    0 -0.5  0.5[4,]    0  0.0  0.0    1  0.0  0.0[5,]    0  0.5 -0.5    0  1.0  0.0[6,]    0 -0.5  0.5    0  0.0  1.0Why were two rows omitted here? Am I missing something?","Creater_id":53410,"Start_date":"2016-02-01 09:47:54","Question_id":193502,"Tags":["r","correlation","covariance-matrix"],"Answer_count":2,"Last_activity":"2016-08-18 17:24:42","Link":"http://stats.stackexchange.com/questions/193502/why-does-the-correlation-function-in-r-cor-return-a-matrix-with-fewer-rows-th","Creator_reputation":618}
{"_id":{"$oid":"5837a57aa05283111e4d44af"},"View_count":97,"Display_name":"Mark Mitchell","Question_score":3,"Question_content":"I will note before starting that this question is related to my own work, in which i am aiming to extend and replicate the methods used in Attanasio et. al. (2015), \"Human Capital Development and Parental Investment in India\"**.I am carrying out a large confirmatory factor analysis where the relationship between observed measurements and latent variables can be written as follows: y_{i,j} = \\lambda_{i,j} \\theta_j + \\varepsilon_{i,j} where  is measurement  on latent variable ,  is a factor loading, and  a measurement error. My aim is to estimate the joint distribution of the latent factors - the . For my application i cannot assume normality of this distribution, so instead i assume their joint distribution to be a mixture of two normals. I also assume that the latent variables are independent of measurement errors and that the latter are normally distributed. First, writing the measurement system in matrix form, we have    \\textbf{y} = \\Lambda \\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon} where  is a vector of all measurements on each latent factor,  is a matrix containing all the factor loadings,  a vector containing all latent variables, and  is a vector containing all the measurement errors. Using basic rules about the distribution of sums of independent random variables and the convolution operator, the theoretical joint distribution of the of the measurements,  is given by:p(\\textbf{y})  = \\tau \\underbrace{\\int  g(\\textbf{y}  -\\boldsymbol{\\Lambda }\\boldsymbol{\\theta})f_A(\\boldsymbol{\\Lambda}\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}_{p_A({\\cdot})} + (1-\\tau) \\underbrace{ \\int  g(\\textbf{y} -\\boldsymbol{\\Lambda }\\boldsymbol{\\theta})f_B(\\boldsymbol{\\Lambda}\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}_{p_B({\\cdot})}with  and  being multivariate normal probability density functions representing the two mixture components of the joint distribution of the latent factors and  is the joint density of the measurement errors. With the assumption made  is also a mixture of two normals.I then use a ML approximation of the joint distribution of measurements (assuming it is in fact a mixture of two normals) and obtain estimates of the moments of  and , denoted , , , , and  where A,and B represent the first a second mixture components.. Once these are estimated, i would like to set them equal to the implied structure of the distribution given in the above equation. As such i am left with the following equalities: \\tau \\boldsymbol{\\mu}_A^{\\boldsymbol{\\theta}} + (1-\\tau)\\boldsymbol{\\mu}_b^{\\boldsymbol{\\theta}} = 0 \\nonumber \\\\[5pt]\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}^{\\boldsymbol{\\theta}}_A = \\hat{\\boldsymbol{\\mu}}_A^{\\tilde{\\textbf{y}}} \\nonumber \\\\[5pt]\\label{momentconditions}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}^{\\boldsymbol{\\theta}}_B = \\hat{\\boldsymbol{\\mu}}_B^{\\tilde{\\textbf{y}}} \\\\[5pt]\\boldsymbol{\\Lambda} \\boldsymbol{\\Theta}_A \\boldsymbol{\\Lambda}' + \\boldsymbol{\\Psi} = \\hat{\\boldsymbol{\\Sigma}}_A \\nonumber \\\\[5pt]\\boldsymbol{\\Lambda} \\boldsymbol{\\Theta}_B \\boldsymbol{\\Lambda}' + \\boldsymbol{\\Psi} = \\hat{\\boldsymbol{\\Sigma}}_B \\nonumber Where  is still the matrix of factor loadings, and   for  are the vector of means and the covariance matrix from the corresponding component of the mixed joint distribution of the latent factors. All of the quantities on the right hand side of the above equations and  have been estimated.The matrices ,  and  have a structure implied by the measurement equation, and so my question is: can their parameters be estimated via minimum distance or any other suitable method in Matlab or STATA? If anyone knows of existing functions that can be used to do so, or how to go about this manually, then any comments or advice would be much appreciated.Thanks in advance.** URL to the original paper: http://egcenter.economics.yale.edu/sites/default/files/files/cdp1052.pdfThis question is linked to a previous question of mine that inquired about the first stage of this procedure - deriving and estimating the mixture of normal distribution for the observed measurements. Link:How to calculate the sum of a \u0026quot;function\u0026quot; of a Gaussian mixture and a Gaussian variable?","Creater_id":122240,"Start_date":"2016-08-17 12:39:47","Question_id":230354,"Tags":["normal-distribution","matlab","econometrics","stata","covariance-matrix"],"Answer_count":1,"Last_activity":"2016-08-18 16:58:07","Link":"http://stats.stackexchange.com/questions/230354/how-best-to-fit-factor-loadings-mean-vectors-and-covariance-matrices-to-a-nonl","Creator_reputation":28}
{"_id":{"$oid":"5837a57aa05283111e4d44bc"},"View_count":36,"Display_name":"Dai_thai","Question_score":0,"Question_content":"I am working with economic models which measure the value of information associated with medical research. What I know about the functional form:The amount of information generated by a hypothetical medical trial begins at zero for a trial with zero sample size () and increases with sample size until it reaches a maximum. This maximum represents the value of information from an infinitely sized trial (). The problem and what I want to do: For policy decision making it is useful to know the value of information () for all plausible sample sizes () but each evaluation of them model takes a long time to run i.e. estimating a particular point () on the asymptotic curve eats a lot of computation time. I want to learn the shape of the curve without running an infinite number of model evaluations.What data I have: I have an estimate of the asymptotic limit for the curve  which is estimated with uncertainty using a spline model. I also have a set of estimates of  at various values of ;.My current guess: If I assume that the curve is increasing at a constant rate and the errors are homoskedastic then it is possible to transform the values of  into a linear form using  and then run an OLS and re-transform. However this is not general enough as a lot of the time this does not hold. Also possible to do this for a two parameter model which allows increase at an increasing or decreasing rate. This is analogous to a survival model and one line of attack could be to keep adding parameters to this model.What would be ideal would be some way to select from the set of all possible concave asymptotic curves with an origin at zero, but my math is not up to that! ","Creater_id":128116,"Start_date":"2016-08-18 04:32:21","Question_id":230477,"Tags":["regression","predictive-models","modeling","econometrics","nonparametric"],"Answer_count":2,"Last_activity":"2016-08-18 16:46:07","Link":"http://stats.stackexchange.com/questions/230477/how-to-select-an-asymptotic-curve-given-an-estimate-of-its-limit-and-a-sample-of","Creator_reputation":1}
{"_id":{"$oid":"5837a57aa05283111e4d44ca"},"View_count":44,"Display_name":"dayum","Question_score":3,"Question_content":"I just started Sutton's book, and am curious as to how to think about the answer to Exercise 1.1: Self-Play. Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?One could also think of the following related sub-questions, but they haven't made my thoughts any clearer.1. Would removing the random part of the learning change the situation- i.e. always following optimal policy and not exploring?2. How would it depend on who is the first mover?","Creater_id":92890,"Start_date":"2016-08-16 20:41:32","Question_id":230202,"Tags":["reinforcement-learning"],"Answer_count":0,"Last_activity":"2016-08-18 16:41:47","Link":"http://stats.stackexchange.com/questions/230202/reinforcement-learning-by-sutton-tic-tac-toe-self-play","Creator_reputation":16}
{"_id":{"$oid":"5837a57aa05283111e4d44cc"},"View_count":151,"Display_name":"Kim","Question_score":3,"Question_content":"I have an unknown process that produces binary results. I am trying to determine if this process is a Bernoulli trial.From wikipedia:  In the theory of probability and statistics, a Bernoulli trial (or binomial trial) is a random experiment with exactly two possible outcomes, \"success\" and \"failure\", in which the probability of success is the same every time the experiment is conducted.I do not know the actual success rate, if the success rate of each trial is equal, or if the trials are independent from each other.However, I can collect data from the process in sequential trials with its corresponding successful trials.Initial set of trials yielded 5 successes out of 999 trials.Success 1 was on trial 196Success 2 was on trial 388success 3 was on trial 593Success 4 was on trial 792Success 5 was on trial 999What is the ideal way to demonstrate (perhaps with test statistics) if this process is or is not a Bernoulli trial. i.e.I already know it outputs binary results.does each trial have an equal or varying probability of success?are the trials independent?Context:A computer game uses an unspecified item drop system. Most item drop systems in computer games are Bernoulli trials with equal probability of success (i.e. an equal drop chance). However, I suspect that this particular system uses a count to determine a successful drop (e.g. success after ~200 trials). I want to design an experiment that will prove that the drop system uses an equal drop chance (Bernoulli trial) or a different system (e.g. a count based system).My background:I had only very basic statistics lesson in college many years ago. I am very fuzzy with test statistics. I have fair understanding of Bernoulli trials and binomial distributions.My attempt:Since I am trying to determine if the process is a Bernoulli trial with an equal chance of success p, if I can demonstrate that separate cumulative trials have different p, I can prove that the drop system does not use Bernoulli trial with an equal success chance.To do this I collected 3 sets of data. Set A was a sequential 999 trials that resulted in 5 successes as stated above. Set B was 5 x 190 trials with a gap in between them. Set C was the gap between the 5 sets of 190 trials in B. i.e.999 trials (A) - 190 (B) - 15 (C) - 190 (B) - 11 (C) - 190 (B) - 9 (C) - 190 (B) - 1 (C) - 190 (B) - 3 (C) - 190 (B)This sampling yielded:A: 5 successes in 999 trialsB: 0 successes in 950 trialsC: 5 successes in 39 trialsThe reason it is sampled this way is because I suspected that the drop counter is between 190 and 210, and that the drop counter resets after each successful trial. Every time I obtain a successful trial in Set C, I switch back to collecting Set B.Using Clopper-Pearson interval (a.k.a. 'exact' method), α = 0.05, I calculated the interval of which Success chance p lies in, for each set of data.Set A: Success rate PA: 0.00163 - 0.01164, 95% confidenceSet B: Success rate PB: 0.00003 - 0.00388, 95% confidenceSet C: Success rate PC: 0.04297 - 0.2743, 95% confidenceSince,PC ≠ PAPC ≠ PBI conclude that the drop system observed is not a binomial experiment with equal success chance.Is this an acceptable solution? If not, what is the correct method to approach this problem?Research:I've read through several posts related to this topic, however they are difficult for me to understand and I cannot be sure how to use those solutions on this particular question.Test if two binomial distributions are statistically different from each otherTest if two binomial distributions comply with the same pIs there a test for independence in a Bernoulli process?Test on binomial distributionProbablity distribution for different probabilitiesestimate probability mass function from observed sample","Creater_id":127845,"Start_date":"2016-08-16 05:03:10","Question_id":230087,"Tags":["hypothesis-testing","sampling","binomial","inference","goodness-of-fit"],"Answer_count":2,"Last_activity":"2016-08-18 16:40:33","Link":"http://stats.stackexchange.com/questions/230087/unknown-process-outputs-binary-results-how-to-prove-that-this-process-is-or-no","Creator_reputation":41}
{"_id":{"$oid":"5837a57aa05283111e4d44da"},"View_count":11,"Display_name":"GloriaD","Question_score":1,"Question_content":"I'm doing an internal validation of a binary logistic regression model. My goal is to provide clinicians with accurate predictions of baseline risk in untreated patients.  There are no sources of data for an external validation.My statisticians have run 500 bootstraps and calculated \"bias-corrected\" predicted probabilities for each subject.  I then asked them for the bias-corrected model and they were not able to produce it, but did send along the intercept and coefficients from each of the bootstraps.  Is it appropriate to calculate the mean or median of these estimates and to create the final model, and would it be correct to call this a \"bias-corrected\" model?  Additionally, I've seen output from the rms package which returns various original and optimism-corrected estimates from bootstraps (intercept, slope, r-squared, etc) but not the model coefficients.  Can we get these, and if not, should we use the mean or median coefficients?  Thank you for your help.  ","Creater_id":128194,"Start_date":"2016-08-18 16:28:26","Question_id":230591,"Tags":["bootstrap"],"Answer_count":0,"Last_activity":"2016-08-18 16:28:26","Link":"http://stats.stackexchange.com/questions/230591/bias-corrected-model-from-bootstrapped-logistic-regression","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d44dc"},"View_count":35,"Display_name":"royappa","Question_score":2,"Question_content":"This is a beginner's questions about statistical tools.We have a spreadsheet with many columns from a survey.The first few columns describe each individual:Gender (discrete values Male, Female)City (discrete values X,Y,Z)College Year (discrete values 1,2,3,4)BMI (continuous values in the range 10-50)Then there is a column \"Score\" for each person, which is continuous from 1 to 5.We want to find out what interesting correlations between sub-populations and Scoring ranges. For example, \"Males in City Y have scores between 1.5 and 2.5\" or \"Females with BMI between 24.8 and 28.2 have scores between 2.3 and 3.7\"I can crunch this by trial and error using Excel, Access, or C++ code, but then I have to think up of each query and write the code or formulas for it, or try various ranges and combinations methodically.But then I may miss some pattern. Can the major tools like SPSS or R extract sub-populations that correspond most strongly to a given scoring range? In other words I want it to look at the data and output a fact like the example above. Ideally I don't even want to provide the Score range, the tool should infer and extract the interesting patterns.Thank you.","Creater_id":128190,"Start_date":"2016-08-18 15:01:09","Question_id":230582,"Tags":["r","spss","data-mining","sas"],"Answer_count":1,"Last_activity":"2016-08-18 16:27:42","Link":"http://stats.stackexchange.com/questions/230582/can-tools-like-spss-find-out-which-columns-correspond-to-a-certain-data-range","Creator_reputation":11}
{"_id":{"$oid":"5837a57aa05283111e4d44e9"},"View_count":9,"Display_name":"chris","Question_score":0,"Question_content":"I'm running a regression that includes categorical variables. The standard deviation and standard errors output by the regression are aggregated for all the data. However, in this case it's likely that the standard errors and standard deviation differ depending on which classes are combined. Is there a way of breaking out standard deviation and standard errors for different class combinations? Are there other regressions I can do to get these figures?Thanks!","Creater_id":71665,"Start_date":"2016-08-18 16:02:26","Question_id":230586,"Tags":["regression","linear"],"Answer_count":0,"Last_activity":"2016-08-18 16:02:26","Link":"http://stats.stackexchange.com/questions/230586/different-standard-deviations-fo-classes-in-a-regression","Creator_reputation":23}
{"_id":{"$oid":"5837a57aa05283111e4d44eb"},"View_count":32,"Display_name":"nonameswereavailable","Question_score":1,"Question_content":"I'm attempting to work through an LSTM network and have a question regarding the final output. A LSTM appears to function similarly to an RNN with a few additional processes. The question that I have is pertains to the bottom two images. The first image is of a standard RNN and the second is of an LSTM. In the first image each state  get outputted up the layer  before going through to the loss function . In addition  gets sent to the the next time step . If I look at the image for a LSTM network I do not see  being outputted to some other layer before being going through to the loss function and was wondering if that is the case or if that was simply omitted from the image?","Creater_id":116337,"Start_date":"2016-08-14 22:27:18","Question_id":229861,"Tags":["machine-learning","lstm"],"Answer_count":1,"Last_activity":"2016-08-18 15:53:14","Link":"http://stats.stackexchange.com/questions/229861/lstm-output-clarification","Creator_reputation":111}
{"_id":{"$oid":"5837a57aa05283111e4d44f7"},"View_count":235,"Display_name":"siby","Question_score":7,"Question_content":"At what point do we start classifying multi layered neural networks as deep neural networks or to put it in another way 'What is the minimum number of layers in a deep neural network?'","Creater_id":127022,"Start_date":"2016-08-12 16:44:42","Question_id":229619,"Tags":["neural-networks","terminology","deep-learning"],"Answer_count":3,"Last_activity":"2016-08-18 15:40:06","Link":"http://stats.stackexchange.com/questions/229619/minimum-number-of-layers-in-a-deep-neural-network","Creator_reputation":72}
{"_id":{"$oid":"5837a57aa05283111e4d4506"},"View_count":42,"Display_name":"Stefan Klisarov","Question_score":1,"Question_content":"My problem is the following, my data has a lot of branch off points and the tree grows very rapidly. The end result is not readable, the end nodes are overlapped and even conversion to rules is more or less useless. I am using the rpart package. #Scoring modeld = sort(sample(nrow(Memmbers),nrow(Memmbers)* .6))#select training sampletrain\u0026lt;-Memmbers[d, ]test\u0026lt;-Memmbers[-d, ]s\u0026lt;-glm(verifikation ~ . - userId,data = Memmbers,family = binomial())summary(s)library(ROCR)#score test data set testscore,testt\u0026lt;-predict(fit1,type='class',test)################## PLot tree with priors #score test data testscore1[,2],test$verifikation)perf5\u0026lt;- performance(pred5,\"tpr\",\"fpr\")#90-10 priors with smaller complexity parameter to allow more complex treesfit2 \u0026lt;- rpart(verifikation ~ . - userId , data = train,method = \"class\",parms = list(prior=c(.9,.1)),cp=.0002)plot(fit2);text(fit2,pos=2,cex=0.1,col=\"blue\");#compare complexityprintcp(fit1)printcp(fit2)plotcp(fit2)#convert trees to rules amess\u0026lt;-asRules(fit2)t.b\u0026lt;-rpart.rules.table(fit2)library(rattle)library(rpart.plot)library(RColorBrewer)fancyRpartPlot(fit2)And here is the output of fancyRpartPlot(fit2) My goal is to extract some useful rules from the entire process to implement in a score card. ","Creater_id":128150,"Start_date":"2016-08-18 14:48:52","Question_id":230581,"Tags":["r","cart"],"Answer_count":0,"Last_activity":"2016-08-18 15:27:58","Link":"http://stats.stackexchange.com/questions/230581/decision-tree-too-large-to-interpret","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d4508"},"View_count":35,"Display_name":"Josh Robertson","Question_score":1,"Question_content":"So I'm looking at the number of lion-livestock predation incidents in one area. I created a loop which extracts 390 lion attack days and 390 random non-attack days from 2 data sets to see how climatic variables affect attacks. Obviously each time I run a binomial GLM I get different results based on the different non-attack days. So, I  created a bootstrap to run a binomial GLM 1000 times and extract the associated T-values and coefficient distributions for my independent variables. I can plot these to look at their significance. But is there any simple way to test and report the significance of my 1000 GLMs, and all the 1000 t values and coefficient distributions for my variables?Cheers!","Creater_id":128130,"Start_date":"2016-08-18 06:37:30","Question_id":230493,"Tags":["statistical-significance","generalized-linear-model","binomial","bootstrap","regression-coefficients"],"Answer_count":1,"Last_activity":"2016-08-18 15:20:59","Link":"http://stats.stackexchange.com/questions/230493/how-to-test-the-significance-of-and-report-a-bootstrap-of-a-glm","Creator_reputation":20}
{"_id":{"$oid":"5837a57aa05283111e4d4515"},"View_count":49,"Display_name":"Greconomist","Question_score":1,"Question_content":"I am using the rugarch package in R to forecast volatility using 5 minute data. The package has an a conditional variance model exclusively written for high frequency data, the Multiplicative Component Garch, I know that. The thing is that I invested a whole month to to find tick data and transform them to 5 minute intervals which I then seasonally adjusted using techniques from the extant bibliography. I am trying to manipulate the package even though It is not efficient for 5 minute data to extract the conditional variance forecasts. My data span the period from 2015-01-01 17:00:00 to 2015-12-31 17:00. (I am willing to reduce my sample to find meaningful results).This is how my data look like with five fivemin_returns[,2] being the standardised (deseasonalized) returns and click this to download my datahead(fivemin_returns, 3)                           DPRICE STD_DPRICE2015-01-01 17:00:00  0.000000e+00  0.00000002015-01-01 17:05:00  9.797714e-05  0.23244512015-01-01 17:10:00  2.027022e-04  0.9845100tail(fivemin_returns, 3)                           DPRICE STD_DPRICE2015-12-31 16:50:00 -0.0010186764 -6.09131342015-12-31 16:55:00 -0.0006841370 -3.93763382015-12-31 17:00:00  0.0002481227  1.1458867So I want to create several conditional variance models and extract the forecasted sigma values. My methodology draws heavily from this article Does anything NOT beat the GARCH(1,1)?. I fit a MA(1) process to my returns without the conditional mean.library(xts)library(rugarch)model = c('sGARCH', 'gjrGARCH', 'eGARCH', 'apARCH', 'csGARCH', 'fGARCH', 'fGARCH', 'fGARCH')submodel = c(NA, NA, NA, NA, NA, 'AVGARCH', 'NGARCH', 'NAGARCH')spec1 = vector(mode = 'list', length = 8)for (i in 1:8) spec1[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i \u0026gt; 5) submodel[i] else NULL))spec2 = vector(mode = 'list', length = 8)for (i in 1:8) spec2[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i \u0026gt; 5) submodel[i] else NULL), distribution = 'sstd')spec3 = vector(mode = 'list', length = 8)for (i in 1:8) spec3[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i \u0026gt; 5) submodel[i] else NULL), distribution = 'nig')spec4 = vector(mode = 'list', length = 8)for (i in 1:8) spec4[[i]] = ugarchspec(mean.model = list(armaOrder = c(0, 1), include.mean = FALSE), variance.model = list(model = model[i], submodel = if (i \u0026gt; 5) submodel[i] else NULL), distribution = 'jsu')spec = c(spec1, spec2, spec3, spec4)cluster = makePSOCKcluster(15)clusterExport(cluster, c('spec', 'fivemin_returns'))clusterEvalQ(cluster, library(rugarch))# Out of sample estimationn = length(spec)fitlist = vector(mode = 'list', length = n)for (i in 1:n) {    tmp = ugarchroll(spec[[i]], fivemin_returns[,2], n.ahead = 1, forecast.length = 8640, refit.every = 50, refit.window = 'moving', windows.size = 1500, solver = 'hybrid', calculate.VaR = FALSE, cluster = cluster, keep.coef = FALSE)    if (!is.null(tmp@modelnoncidx))            fitlist[[i]] = NA    } else {        fitlist[[i]] = as.data.frame(tmp, which = 'density')    }}I tried to run the code but it is impossible due to the amount of data and the wrong formulation of the ugarchroll(spec[[i]], R, n.ahead = 1, forecast.length = 8640, refit.every = 50, refit.window = 'moving', windows.size = 1500, solver = 'hybrid', calculate.VaR = FALSE, cluster = cluster, keep.coef = FALSE) argument I reckon. I am not really sure about the refit.every = 50, refit.window = 'moving', windows.size = 1500How can I make it work? Decrease the sample size to 4 months maybe and try to forecast the last 15 days?Edit 1: decreasing the sample and forecasting the condiational variance for the last 15 days is not the most practical thing to do as foreacting evaluation techniques with 15 (aggregated) observations is not a sensible thing to do. That is why I see at least one year of tick data in other papers and makes me think how much computing power have these scientists at their disposal? With my mere I3 processor and 4 gb ram I can't do anything.","Creater_id":112822,"Start_date":"2016-08-18 09:33:51","Question_id":230538,"Tags":["r","garch","large-data","volatility-forecasting"],"Answer_count":0,"Last_activity":"2016-08-18 15:05:46","Link":"http://stats.stackexchange.com/questions/230538/rugarch-r-volatility-forecasts-with-5-minute-data-problem","Creator_reputation":13}
{"_id":{"$oid":"5837a57aa05283111e4d4517"},"View_count":36,"Display_name":"Joey Peters","Question_score":0,"Question_content":"I have behavioral response data (counts of acts within an hour trial) that I would like to analyze using a GLMM. I'm new to GLMM's but after months of digging through papers and forums, I've concluded this is likely the best approach to analyze my data.The experiment was an exposure study where small estuarine crabs were exposed to fluoxetine over a 60-day study. We observed their behaviors using ethograms during day and night trials. We also observed their behaviors day and night with a predator added to the tank. There were 4 trials per week (Day -Pred, Day +Pred, Night -Pred, Night +Pred). Our question was whether exposure to the drug ultimately altered their behaviors (being still, mobile, foraging, predator avoidance, etc.) during these trials and if there was an effect of exposure time (i.e., greater effect with longer exposure)?Because the data are counts within a specified trial period there are observed proportions (i.e., observed behavioral acts / 12 possible observation windows) I believe I should use either a Poisson or a binomial distribution to fit the model. We decided to subset the main dataset into trial types because day/night and +Pred/-Pred aren't treatments per se, so we constructed models for each subset rather than using one over-arching model. For example we built a model to compare the active behaviors across treatments within the Day+Predator subset: For reference the variable terms are:  actsuc= successful active behaviorsactfail= active behavior failuresFixed effects:  Treatment = 3 levels: Control, 3ng, 30ng fluoxetine; exposure = 20 days, 40 days, 60 days; crabsex = Dominant male, subordinate female, subordinate male;Random effects:  TankID= to account for non-independence of crab within the same tank (3 in each)Trial = to account for non-independence of multiple trials over exposure studyCrab. = to account for non-independence of multiple observations on a single crab within and across trials Model with interaction terms  actfit.glmm.interact = glmer(cbind(actsuc, actfail) ~ Treatment:exposure +                               Treatment:crabsex + (1|TankID) + (1|Trial) + (1|Crab.),                              family=binomial, data=pday)I need help understanding interaction and nested notation using glmer. I'm not sure if I've set this up correctly. I get the following error message:fixed-effect model matrix is rank deficient so dropping 1 column / coefficientWarning message:In checkConv(attr(opt, \"derivs\"), optcheckConv,  :  Model failed to converge with max|grad| = 0.0247253 (tol = 0.001, component 6)Further, I cannot use the predict function with this model as it is. See script and resultant error message below:pday$predict.act \u0026lt;- predict(actfit.glmm.interact, newdata = pday,  re.form=NA,                             type=\"response\")Error in X %*% fixef(object) : non-conformable argumentsI have used the 'full' version of this model without interactions among fixed factors and was able to get predicted values without any error message:actfit.glmm.full = glmer(cbind(actsuc, actfail) ~ Treatment + exposure + crabsex +                          (1|TankID) + (1|Trial) + (1|Crab.), family=binomial, data=pday)Is this more appropriate, to treat the fixed effects without an interaction? I would like to compare behaviors across genders (crabsex) between treatments, but would like to know if the effect changes with exposure period (which I would assume would require an interaction). I've seen in previous posts that its useful to upload data or a screen shot of the dataframe for reference. Once I figure out how to do that I will upload here. ","Creater_id":127271,"Start_date":"2016-08-10 13:17:23","Question_id":229260,"Tags":["r","mixed-model","categorical-data","lme4","glmer"],"Answer_count":1,"Last_activity":"2016-08-18 14:46:59","Link":"http://stats.stackexchange.com/questions/229260/how-to-specify-factors-nested-within-treatments-using-a-glmm","Creator_reputation":1}
{"_id":{"$oid":"5837a57aa05283111e4d4524"},"View_count":54,"Display_name":"Wilson Freitas","Question_score":2,"Question_content":"Let  be a log-normal variate and  is the affine transformation X.Is  log-normal? I suspect it is not.Since  is log-normal, its expected value isE[X] = \\exp(M + S^2/2)The expected value of  is:E[Y] = aE[X] + b = a \\exp(M + S^2/2) + bTo characterize  as a log-normal variate I should write its mean in the same form of  mean, and obviously reproduce the same approach to others moments.Does that make sense? or Is there another way to characterize  as log-normal? For example, using characteristic function.","Creater_id":61941,"Start_date":"2016-08-18 07:41:35","Question_id":230511,"Tags":["probability","distributions","lognormal"],"Answer_count":0,"Last_activity":"2016-08-18 14:34:09","Link":"http://stats.stackexchange.com/questions/230511/what-is-the-distribution-of-an-affine-transformation-of-a-log-normal-variable","Creator_reputation":21}
{"_id":{"$oid":"5837a57aa05283111e4d4526"},"View_count":34,"Display_name":"Albert","Question_score":0,"Question_content":"I have created a trading strategy which operate every single day on the DAX 30, for the last 1700 trading sessions (some years). I have the daily returns of my strategy and also the daily returns of my index. I'm using R, therefore i can get sd, mean, var ecc...The big issue that impresses the market watchers and financial types is the ability to consistently make above-market returns.What are the main important instrument to test the significance of a trading strategy?Is it sufficient a simple t-test? Which type (paired, unpaires ecc..)? What are your suggestions?","Creater_id":128187,"Start_date":"2016-08-18 13:47:30","Question_id":230572,"Tags":["r","statistical-significance","finance"],"Answer_count":1,"Last_activity":"2016-08-18 14:24:15","Link":"http://stats.stackexchange.com/questions/230572/test-statistical-significance-of-a-trading-strategy","Creator_reputation":1}
{"_id":{"$oid":"5837a57aa05283111e4d4531"},"View_count":7,"Display_name":"grayQuant","Question_score":1,"Question_content":"I am considering taking a course in generalized linear models at an American university this year. I have good knowledge of probability at the level of Ross and also Tsitsiklis . My calculus and linear algebra knowledge I am quite confident in. For any instructors of such classes or students that have taken the class, do you think I have the proper background for such a class? The class follows the textbook by McCullagh and Nelder which I've began reading and find it moderately challenging but still unsure if the course will require more statistics background from me.","Creater_id":21121,"Start_date":"2016-08-18 14:10:18","Question_id":230578,"Tags":["generalized-linear-model","linear-model"],"Answer_count":0,"Last_activity":"2016-08-18 14:10:18","Link":"http://stats.stackexchange.com/questions/230578/prerequisites-for-generalized-linear-models-class","Creator_reputation":123}
{"_id":{"$oid":"5837a57aa05283111e4d4533"},"View_count":47,"Display_name":"S. Ming","Question_score":0,"Question_content":"The assumption  says that the distribution of the unobserved factors in the population is zero. It just doesn't make any sense to me. I really can't figure it out. In my textbook the assumption is illustrated by the following example:In  factors like rainfall, land quality, and so on are contained. Why would the unobserved factors affecting the yield have an average of zero in the population of all cultivated plot?","Creater_id":128181,"Start_date":"2016-08-18 13:19:52","Question_id":230565,"Tags":["regression","econometrics","expected-value"],"Answer_count":2,"Last_activity":"2016-08-18 14:09:48","Link":"http://stats.stackexchange.com/questions/230565/why-is-the-average-value-of-the-disturbance-term-u-in-the-population-zero-zero","Creator_reputation":46}
{"_id":{"$oid":"5837a57aa05283111e4d4541"},"View_count":43,"Display_name":"mjalam","Question_score":2,"Question_content":"I estimate the following model for estimating the standard errors for random coefficients:library(nlme)Model1 \u0026lt;- lme(MRPK ~ 1, random = (~1|industry/id), data=Sample_Data,  control=lmeControl(opt='optim'), method=\"REML\")Model1$apVarI am getting the following error message: \"Non-positive definite approximate variance-covariance\"However, it does work with method=\"ML\"I am working with multiple datasets. Opposite is also true. In some datasets, method=\"REML\" is working but method=\"ML\" is not working. How to identify this problem? I am very new in this literature. Thanks in advance!","Creater_id":113900,"Start_date":"2016-08-18 14:03:38","Question_id":230575,"Tags":["r","mixed-model","multilevel-analysis"],"Answer_count":0,"Last_activity":"2016-08-18 14:03:38","Link":"http://stats.stackexchange.com/questions/230575/mixed-model-non-positive-definite-approximate-variance-covariance","Creator_reputation":16}
{"_id":{"$oid":"5837a57aa05283111e4d4543"},"View_count":14,"Display_name":"Deepend","Question_score":0,"Question_content":"I have user rating data for a series of products each of which has received a range of standard treatments. Treatment 0 or T0 below is the control.   The experiment was an incomplete counterbalanced measure study set up using a Latin cube in reduced from.Users where assigned to one of 9 'Paths' through the experiment to ensure that they only experienced each Product and Treatment once. Once assigned to a Path, the order in which they experienced the assigned Products and Treatments was also randomised.I have indicated one such user path, Path 6, through the latin cube below with *P1T1 = Product 1, Treatment 1* = Path 6P1T0    P1T1    P1T2    P1T3   P1T4    P1T5*   P1T6    P1T7    P1T8P2T0    P2T1    P2T2    P2T3   P2T4    P2T5    P2T6*   P2T7    P2T8P3T0    P3T1    P3T2    P3T3   P3T4    P3T5    P3T6    P3T7*   P3T8P4T0    P4T1    P4T2    P4T3   P4T4    P4T5    P4T6    P4T7    P4T8*P5T0*   P5T1    P5T2    P5T3   P5T4    P5T5    P5T6    P5T7    P5T8P6T0    P6T1*   P6T2    P6T3   P6T4    P6T5    P6T6    P6T7    P6T8P7T0    P7T1    P7T2*   P7T3   P7T4    P7T5    P7T6    P7T7    P7T8P8T0    P8T1    P8T2    P8T3*  P8T4    P8T5    P8T6    P8T7    P8T8P9T0    P9T1    P9T2    P9T3   P9T4*   P9T5    P9T6    P9T7    P9T8Originally I set up the experiment so that participants were randomly assigned to one of the 9 paths. Once a path got e.g. 15 participants it would close, thus the remaining participants would be assigned to one of the remaining paths until the experiment reached its total maximum of participants. However I did not take into account my coding ability... and as a result Path 6 above has twice the amount of participants than the other paths. As a result each of its Product/Treatment combinations has received twice the amount of user ratings compared to the other Product/Treatment combinations. Now I had planed to analyse my data with a series of Paired T-Tests comparing the control of each product to each of its treatments. I also planed to undertake Z-Tests to compare the mean score for each treatment to the mean of the control.I am using SPSS to work out the stats.My questions are:Should I just discard the extra submissions? I could simply use the first 15 and discard the extra 15, thus ensuring that it was done randomly. Note because of the design of the 'Paths' I think this would effect all my planed comparisons fairly equally. I also note this answer recommending the opposite but that was for a different scenario.If I should keep the extra submission will this cause any issues with my planed T-Tests and Z-Tests? I have read that unequal sample sizes are not a problem for ANOVA as SPSS will use the right formula, is the same true for Paired T-Tests?Is there anything else I should consider given the above scenario?As always, thanks for any helpP.S. I am not 100% I have the best or appropriate tags on this questions, if anyone could refine my selections I would be in your debt","Creater_id":39684,"Start_date":"2016-08-18 13:52:56","Question_id":230574,"Tags":["t-test","spss","unbalanced-classes","sample-mean"],"Answer_count":0,"Last_activity":"2016-08-18 13:52:56","Link":"http://stats.stackexchange.com/questions/230574/what-to-do-when-additional-submissions-create-unbalanced-sample-sizes","Creator_reputation":53}
{"_id":{"$oid":"5837a57aa05283111e4d4545"},"View_count":52,"Display_name":"Wuchen","Question_score":3,"Question_content":"Let  be a -dimensional Gaussian random vector. Suppose I have  samples . Consider the large sample regime where  is fixed and  goes to infinity, the MLE estimator of  is . My question is how to establish the consistency result that  and the asymptotic distribution of .Thank you!","Creater_id":101690,"Start_date":"2016-08-18 13:26:16","Question_id":230568,"Tags":["maximum-likelihood","normality","covariance-matrix","asymptotics","consistency"],"Answer_count":1,"Last_activity":"2016-08-18 13:43:46","Link":"http://stats.stackexchange.com/questions/230568/how-to-prove-consistency-and-asymptotic-normality-of-the-inverse-of-sample-covar","Creator_reputation":23}
{"_id":{"$oid":"5837a57aa05283111e4d4552"},"View_count":75,"Display_name":"Zslice ","Question_score":0,"Question_content":"I'm using the glmnet package in R to do ridge regression. When I have a full set of dummy variables (if you took a horizontal sum of all these dummy variables you would get the constant), ridge regression with lambda = 0 is NOT dropping any of the dummy variables. In contrast, OLS gives the expected result by dropping at least 1 of the dummies to prevent perfect multi-collinearity. I'd like to know why the discrepancy exists.  library(glmnet) set.seed(1)make_dummies_out_of_factors\u0026lt;- function(your_df, names_of_factor_variables) {  indices\u0026lt;- which(names(your_df) %in% names_of_factor_variables) #Finds columns corresponding to factor variables  model_matrices_list\u0026lt;- lapply(indices, function(x) {    model.matrix(~your_df[,x] - 1, your_df)  })  #create a model matrix for each factor variable, and stores each one as a list  model_matrices_together\u0026lt;- do.call(cbind, model_matrices_list)  #Column bind all model matrices which are stored as lists  final\u0026lt;- cbind(your_df, model_matrices_together)  #Column bind all the model matrices to the original data  final\u0026lt;- final[,-indices]  #Get rid of the original factor variables  names(final)\u0026lt;- gsub(\"your_df.*\\\\]\", \"dummy_\", names(final))  #Give appropriate names to the dummies  return(final)}test_df\u0026lt;- data.frame(numeric1 = rnorm(1000), numeric2 = rnorm(1000),                      state = rep(letters[1:4], 250), year = rep(c(\"yr1\", \"yr2\"), 500)) #This data frame has 2 factor variablestest_df\u0026lt;- make_dummies_out_of_factors(test_df, names_of_factor_variables = c(\"state\", \"year\"))linear_alldum\u0026lt;- lm(test_dfnumeric1 + test_dfdummy_yr2 + test_dfdummy_b + test_dfdummy_d)X_test\u0026lt;- as.matrix(test_df[,-1]) #Remove dependent variable out of X matrixy_test\u0026lt;- test_df[,1] #This is the dependent variableridge_alldum\u0026lt;- glmnet(x = X_test, y = y_test, lambda = seq(200, 0, by = -1), alpha = 0)comparison = data.frame(as.matrix(coef(ridge_alldum))[,201], coefficients(linear_alldum))names(comparison)[1]\u0026lt;- \"coefficients_ridge_l0\"names(comparison)[2]\u0026lt;- \"coefficients_linear_reg\"#Note that coefficients aren't identical, and that ridge regression doesn't drop coefficients. prediction_linear\u0026lt;- predict(linear_alldum)prediction_ridge\u0026lt;- predict(ridge_alldum, newx = X_test, s = 0)predictions\u0026lt;- data.frame(prediction_linear, prediction_ridge = prediction_ridge)names(predictions)[2]\u0026lt;- \"prediction_ridge\"#Note that the predictions using linear regression and ridge regression aren't the same. sapply(predictions, mean) #Means of predictions using linear and ridge.sapply(predictions, sd) #SDs of predictions using linear and ridge. ","Creater_id":58821,"Start_date":"2016-08-12 16:28:46","Question_id":230201,"Tags":["r","machine-learning","generalized-linear-model","glmnet"],"Answer_count":0,"Last_activity":"2016-08-18 13:15:26","Link":"http://stats.stackexchange.com/questions/230201/ridge-regression-with-lambda-0-and-ols-not-yielding-same-coefficients","Creator_reputation":95}
{"_id":{"$oid":"5837a57aa05283111e4d4554"},"View_count":33,"Display_name":"GGA","Question_score":1,"Question_content":"I have some pre-treatment variables which are significantly associated in a cox model with progression-free survival (PFS) in a medical study.I was wondering if taking a defined cutpoint (for example 3 months or 6 months PFS) and classifying patients according to progression in these time period is possible to do a logistic regression with my pre-treatment variables and see the performance of the model in the prediction of 3 months or 6 months PFS.Is possible/correct from a statistical point of view?","Creater_id":97342,"Start_date":"2016-08-18 12:00:53","Question_id":230555,"Tags":["regression","predictive-models","survival"],"Answer_count":1,"Last_activity":"2016-08-18 13:13:56","Link":"http://stats.stackexchange.com/questions/230555/logistic-regression-at-a-defined-cutpoint-for-survival-analysis","Creator_reputation":171}
{"_id":{"$oid":"5837a57aa05283111e4d4561"},"View_count":15,"Display_name":"B. Rowan","Question_score":0,"Question_content":"I have 110 panelists and they are each given 7 questions. Each of the 7 questions has a different piece of info but the same 10 point Likert scale on how influential the information was. I want to know if there is a test that would tell me which piece of information was most influential. ","Creater_id":125561,"Start_date":"2016-08-18 12:52:08","Question_id":230563,"Tags":["chi-squared","likert","manova"],"Answer_count":0,"Last_activity":"2016-08-18 12:52:08","Link":"http://stats.stackexchange.com/questions/230563/need-help-with-likert-scale-data","Creator_reputation":16}
{"_id":{"$oid":"5837a57aa05283111e4d4563"},"View_count":41,"Display_name":"Adrian","Question_score":2,"Question_content":"Is there a closed-form solution for the M-step of the expectation maximization (EM) algorithm for a multivariate Gaussian hidden Markov model (HMM) where observations are missing at random?My HMM notation, largely borrowed from https://www.princeton.edu/~rvan/orf557/hmm080728.pdf, is: is a discrete hidden state with transitionmatrix  and initial distribution The state space  is finiteThe observations are If the observations are scalars,  and  are simply vectors of means and variances (one for each hidden state)If the observations are multivariate normal, say , each  is a -vector of means and each  is a -by- covariance matrixI'm interested in estimating , ,  and  using the EM algorithm.  Let  denote an -by- matrix of observed data.  Let  denote the posterior probability over hidden state  for observation , computed under the previous iteration's parameters, i.e. .The notes I linked to earlier give the M step when  = 1, i.e. scalar Gaussian observations (section 6.2, page 87):\\begin{equation*}\\mathbf{m}_i = \\frac{\\sum^n_{t=1} \\mathbf{y}_t \\, \\pi_{it}}{\\sum^n_{t=1} \\pi_{it}}\\end{equation*}\\begin{equation*}\\mathbf{v}_i = \\frac{\\sum^n_{t=1} \\left(\\mathbf{y}_t - \\mathbf{m}_i\\right)^2 \\, \\pi_{it}}{\\sum^n_{t=1} \\pi_{it}}\\end{equation*}I'm interested mainly in the updates to  and  so I'll omit the updates to  and  for clarity.The M-step updates for multivariate Gaussian observations are quite similar.  The update to  is identical (the only difference being that  is now a -vector), and the update for the covariance matrices is:\\begin{equation*}\\mathbf{v}_i = \\frac{\\sum^n_{t=1} \\left(\\mathbf{y}_t - \\mathbf{m}_i\\right) \\, \\left(\\mathbf{y}_t - \\mathbf{m}_i\\right)^{\\intercal} \\, \\pi_{it}}{\\sum^n_{t=1} \\pi_{it}}\\end{equation*}This equation appears on slide 9 of http://personal.ee.surrey.ac.uk/Personal/P.Jackson/tutorial/hmm_tut4.pdf, for example, with slightly different notation.What is the M-step if elements of  (the -by- matrix of observations) are missing at random?  My guess is to first define a matrix  containing 1s where  is observed and 0s where it is missing (i.e. a matrix of indicators for non-missing data).  I then recode the missing observations in  as 0.My updates (after the recoding described above) are:\\begin{equation*}\\mathbf{m}_i = \\frac{\\sum^n_{t=1} \\mathbf{y}_t \\, \\pi_{it}}{\\sum^n_{t=1} \\tilde{\\mathbf{y}}_t \\, \\pi_{it}}\\end{equation*}\\begin{equation*}\\mathbf{v}_i = \\frac{\\sum^n_{t=1} \\left(\\mathbf{y}_t - \\mathbf{m}_i\\right) \\, \\left(\\mathbf{y}_t - \\mathbf{m}_i\\right)^{\\intercal} \\, \\pi_{it}}{\\sum^n_{t=1} \\tilde{\\mathbf{y}}_t \\, \\tilde{\\mathbf{y}}_t^{\\intercal} \\pi_{it}}\\end{equation*}In the equation above, both the numerator and denominator are -by- matrices.  The fraction should be read as an element-wise division.Is this correct -- is it even guaranteed to produce a positive definite covariance matrix?  Does a correct analytical update exist?  Any pointers to papers or lecture notes that discuss multivariate Gaussian HMMs with observations missing at random would be much appreciated.A relevant paragraph from https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices:  In the general case, the unbiased estimate of the covariance matrix  provides an acceptable estimate when the data vectors in the observed  data set are all complete: that is they contain no missing elements.  One approach to estimating the covariance matrix is to treat the  estimation of each variance or pairwise covariance separately, and to  use all the observations for which both variables have valid values.  Assuming the missing data are missing at random this results in an  estimate for the covariance matrix which is unbiased. However, for  many applications this estimate may not be acceptable because the  estimated covariance matrix is not guaranteed to be positive  semi-definite. This could lead to estimated correlations having  absolute values which are greater than one, and/or a non-invertible  covariance matrix.Do any references discuss how to deal with this problem in the context of multivariate Gaussian HMMs?Here's a little R example showing covariance matrix estimates with negative eigenvalues:library(mixtools)  # For rmvnormget_sigma_hat \u0026lt;- function(n_obs, sigma, pr_missing) {    ## Generate random multivariate data y with observations missing at random    ## Return estimated variance-covariance matrix    stopifnot(0 \u0026lt;= pr_missing \u0026amp;\u0026amp; pr_missing \u0026lt; 1)    stopifnot(all(eigen(sigma)values \u0026gt; 0.0)))}set.seed(543987)sigma \u0026lt;- matrix(c(1.0, 0.5, 0.6,                  0.5, 2.0, 0.7,                  0.6, 0.7, 3.0), 3, 3)  # True variance-covariance matrix for observations in R^3n_reps \u0026lt;- 1000reps \u0026lt;- replicate(n_reps, get_sigma_hat(n_obs=500, sigma=sigma, pr_missing=0.10), simplify=FALSE)mean(sapply(reps, function(x) xsigma_hat[1, 1]))  # Close to sigma[1, 1]mean(sapply(reps, function(x) xsigma_hat_positive_definite))  #  Some negative eigenvalues!sigma_hat \u0026lt;- Filter(function(x) !xsigma_hateigen(sigma_hat)sigma_hat_positive_definite, reps)[[2]]values  # Again, third eigenvalue is negativecov2cor(another_sigma_hat)  # This one has some correlations above 1The code simulates a simplified version of the problem I describe above, where the s have mean zero and there are no weights .","Creater_id":9330,"Start_date":"2016-08-12 10:34:35","Question_id":229580,"Tags":["references","missing-data","hidden-markov-model","expectation-maximization","multivariate-normal"],"Answer_count":1,"Last_activity":"2016-08-18 12:50:22","Link":"http://stats.stackexchange.com/questions/229580/what-is-the-m-step-for-expectation-maximization-for-a-multivariate-gaussian-hidd","Creator_reputation":1769}
{"_id":{"$oid":"5837a57aa05283111e4d4570"},"View_count":127,"Display_name":"Yang Mei Lian","Question_score":2,"Question_content":"I have two vectors, actual and predict with the same length.Is  it possible to calculate R-square value for these two vectors in R, or it is only possible if predict comes from a linear regression?If the answer is yes, what is difference between R-square and RMSE? What metric is better to measure performance of a predictor?Thank you very much,","Creater_id":null,"Start_date":"2016-08-18 04:51:31","Question_id":230556,"Tags":["r","r-squared","rms"],"Answer_count":1,"Last_activity":"2016-08-18 12:48:19","Link":"http://stats.stackexchange.com/questions/230556/calculate-r-square-in-r-for-two-vectors","Creator_reputation":null}
{"_id":{"$oid":"5837a57aa05283111e4d457d"},"View_count":12,"Display_name":"user25004","Question_score":0,"Question_content":"Surrogate functions are common in machine learning as an alternative for minimizing a non-convex function. The question is whether a good surrogate should be an upper bound of the original criterion or a lower bound?The reason I ask is that for zero-one loss always convex upperbounds such as hinge loss are used. On the contrary, for rank regularizer, a convex lower bound, the trace norm is used. Also for the cardinality, the  norm which is a convex lower bound is used (e.g. in lasso)Which one makes sense?If I have a non-convex objective function and want to drive a surrogate for that should I try to derive a tight lower bound or a tight upper bound?","Creater_id":113642,"Start_date":"2016-08-18 12:41:40","Question_id":230562,"Tags":["regularization","loss-functions","train","convex"],"Answer_count":0,"Last_activity":"2016-08-18 12:41:40","Link":"http://stats.stackexchange.com/questions/230562/surrogate-function-should-be-an-upper-bound-or-a-lower-bound","Creator_reputation":101}
{"_id":{"$oid":"5837a57aa05283111e4d457f"},"View_count":21,"Display_name":"SMA.D","Question_score":0,"Question_content":"One way to produce pseudo-random numbers is to use a linear recurrence (multiple recursive generator or MRG). Which is defined by the recurrence: x_{i+1}=k_0+k_1x_{i}+k_2x_{i-1}+...+k_{m+1}x_{i-m} \\pmod{m}The properties of this generator should be a deeply investigated topic since several decades ago.I'm looking for a theoretical formula (or an upper bound) for the (sample) auto-correlation function of these PRNGs [and cross-correlation function between two different MRG], when  is a power of .I've searched a lot on the internet, however couldn't find any useful results. Does there exist any theoretical formula for auto- and cross-correlation of MRGs?","Creater_id":127220,"Start_date":"2016-08-18 12:29:24","Question_id":230560,"Tags":["autocorrelation","random-generation","cross-correlation"],"Answer_count":0,"Last_activity":"2016-08-18 12:39:03","Link":"http://stats.stackexchange.com/questions/230560/auto-correlation-of-a-multiple-recursive-pseudo-random-number-generator","Creator_reputation":106}
{"_id":{"$oid":"5837a57aa05283111e4d4581"},"View_count":51,"Display_name":"brauchle_andi","Question_score":2,"Question_content":"Currently I'm working on my master thesis about the application of data mining in football, I'm trying to predict matches based on some stats of the two involved Teams (using RapidMiner).My use case is the German Bundesliga and I will predict the last season (15/16) based on the three seasons before (12/13 to 14/15); the object to predict is the match result (home win, draw, away win).Stats I am using are for example the market values of the teams (transfermarkt.de), the position in the table right before the match, the position at the end of the last season and some data to picture the 'form' of the teams, something like percentage of wins / goals shot / goals conceded / goal difference etc. in the last X matches. All in all I have like 20 attributes.I already applied some algorithms on the data set, f.e. decision trees, neural nets, bayes rules and so on. But I don't know how to optimize my results. First approach would be like creating a very good model for the seasons 2012-2014 and apply it to 2015. But in this case, the model has a performance of something like 65% to 70% for the training data (with overfitting), but for the test data it's only like 47% to 49%. Second approach was to change the optimization process to get a optimal model for 2015 (of course here the performance is way better), but this is not very realistic, because in reality you don't know the results of the season to predict, therefore this is not a valid way.The only reasonable way is to create a model with the test data of 2012-2014 and then apply the model to 2015, where the model is neither optimized for 2012-2014, nor for 2015. It should somehow be a model with an average performance.I thought about splitting up the test data, f.e. create a model with 2012+2013 and apply it to 2014 (or using Cross Validation); here i am able to do the optimization for 2014, since we know the results. Here i am getting a performance of something like 48% up to 53%, but this depends on my optimization parameters and the selection of my attributes. F.e. sometimes the algorithm finds a model, that has a performance of like 60% for the training data and 48% for the test data; sometimes the result is a model with 55% for the training data and 52% for the test data.Any ideas on this topic?","Creater_id":128178,"Start_date":"2016-08-18 12:36:46","Question_id":230561,"Tags":["predictive-models","prediction","performance","rapidminer"],"Answer_count":0,"Last_activity":"2016-08-18 12:36:46","Link":"http://stats.stackexchange.com/questions/230561/prediction-of-soccer-matches-process-setup-and-optimization","Creator_reputation":11}
{"_id":{"$oid":"5837a57aa05283111e4d4583"},"View_count":22,"Display_name":"MANOVAboard","Question_score":1,"Question_content":"I have been wrestling with some model problems for a few weeks now and would like some help.I am trying to determine whether fish catch rates have changed following an event using GAMM.  Because catch rates are highly seasonal, Day of year (DOY) is used as a smoother.  Fixed effects include season, pre/post event, and their interaction; Fishing zone is included as a random effect to account for repeated visits.  I have added a variance structure (VarIdent) to allow variation based on month.  Here is the full model:    gamm1.1\u0026lt;-gamm(CPUE~s(DOY, fx=FALSE, k=-1, bs=\"cr\") + ts3Season, data=ts3, random=c(list(Year=~ 1),(list(FZ=~1))), control=lmeControl(niterEM=1000, msMaxIter=500), weights.lme=varIdent(form=~1 |fMonth),  method=\"ML\") I am interested in including an offset for soak time, since it varies seasonally- in cold months we do not catch much, and as a result soak time is longer.  Here is the offset added:    gamm1.2\u0026lt;-gamm(CPUE~s(DOY, fx=FALSE, k=-1, bs=\"cr\") + ts3Season + offset(ts3$soak.hr), data=ts3, random=c(list(Year=~ 1),(list(FZ=~1))), control=lmeControl(niterEM=1000, msMaxIter=500), weights.lme=varIdent(form=~1 |fMonth),  method=\"ML\") However, adding an offset does 2 things:1) Greatly increases the AIC value.  I assume this is because soak time is a continuous variable and it is adding a lot of model complexity.2) Creates an extreme pattern in the residuals.Here is the gam check and some other diagnostics on the model without the offset (first 8 plots)And look at what happens to the residuals vs. fitted when the offset is included:My questions are:1) Why does adding an offset do this?  It is correlated with season, but I don't understand why the pattern is so strong.  2) Are these two models comparable with AIC?  The AIC of the offset model is triple that of the other model.3) Finally, is there a way to account for this new residual pattern while keeping the offset, or is using an offset in this situation not advisable?Thanks in advance.","Creater_id":62044,"Start_date":"2016-08-18 11:59:37","Question_id":230554,"Tags":["mixed-model","model","validation","gam","offset"],"Answer_count":0,"Last_activity":"2016-08-18 12:06:36","Link":"http://stats.stackexchange.com/questions/230554/how-to-account-for-patterns-in-gamm-residuals-due-to-the-addition-of-an-offset","Creator_reputation":60}
{"_id":{"$oid":"5837a57aa05283111e4d4585"},"View_count":45,"Display_name":"Eli","Question_score":3,"Question_content":"I have two variation of the same problem.A) Assume you sample  points, , drawn i.i.d from a continuous distribution  of a random variable . We reorder the points so . You may assume there exists a density . To each point you assign the weight (or probability) of the interval  (we set , ). Namely:P(X'=x_i)=\\int_{(x_{i-1}+x_{i})/2}^{(x_{i+1}+x_{i})/2} f(x)dx This provides a granularization of the continuous random variable  to a discrete RV .I'm interested in obtaining bounds on the expected value of |E(X)-E(X')| as a function of the number of points . If it is possible to obtain bounds on \\Pr\\left(|E(X)-E(X')|\u0026gt;\\delta \\right) this would be great. Any tip would be helpful!B) A similar version of the problem is when there are  i.i.d samples drawn from a distribution of a random variable . We denote the empirical average as .  A subset of  points are selected from the set of  samples. We have . The weights of each point ,  is the number of points (from the set of  initial points) in the interval . As before, I'm interested in |\u0026lt;x^{(n)}\u0026gt;-\u0026lt;x^{(m)}\u0026gt;| as a function of  and .Any help would be greatly appreciated!","Creater_id":59572,"Start_date":"2016-08-17 22:52:53","Question_id":230422,"Tags":["sampling","continuous-data"],"Answer_count":0,"Last_activity":"2016-08-18 11:51:46","Link":"http://stats.stackexchange.com/questions/230422/discretization-of-a-continuous-distribution","Creator_reputation":26}
{"_id":{"$oid":"5837a57aa05283111e4d4587"},"View_count":26,"Display_name":"learnerX","Question_score":2,"Question_content":"We have data from the border firewalls pertaining to all ports used on our networks. For example, we have counts of all connection attempts to all ports each day on our networks.date, port, count8/17/2016,23,1234445238/17/2016,80,45653345...8/18/2016,23,244533556...We are trying to create normal patterns around these counts. For example, in the last 2 weeks, port 23 observed these sets of counts pertaining to connection attempts each day: 122334234,12445523,123454332,.... If somewhere around those counts are normal then 555335534343 will be detected as an anomaly on a particular day and send an alert.Also, create a list of top 50 ports each day (by connection attempt counts). If a port has jumped more than, let's say, 20 places in this list then there's something fishy probably and we can generate an alert.What other patterns could be formed in the data to detect out-of-the-ordinary observations and, more importantly, what machine learning or pattern recognition algorithms can we use for these purposes?","Creater_id":119223,"Start_date":"2016-08-18 11:38:13","Question_id":230552,"Tags":["r","machine-learning","algorithms","pattern-recognition","anomaly-detection"],"Answer_count":0,"Last_activity":"2016-08-18 11:38:13","Link":"http://stats.stackexchange.com/questions/230552/best-way-to-form-patterns-in-data-relevant-to-commonly-used-ports-in-the-your-ne","Creator_reputation":128}
{"_id":{"$oid":"5837a57aa05283111e4d4589"},"View_count":3387,"Display_name":"David LeBauer","Question_score":16,"Question_content":"BackgroundI am overseeing the input of data from primary literature into a database. The data entry process is error prone, particularly because users must interpret experimental design, extract data from graphics and tables, and transform results to standardized units.Data are input into a MySQL database through a web interface. Over 10k data points from \u003e 20 variables, \u003e 100 species, and \u003e 500 citations have been included so far. I need to run checks of the quality of not only the variable data, but also the data contained in lookup tables, such as the species associated with each data point, the location of the study, etc.  Data entry is ongoing, so QA/QC will need to be run intermittently. The data have not yet been publicly released, but we are planning to release them in the next few months.Currently, my QA/QC involves three steps: a second user checks each data point.visually inspect histogram each variable for outliers. users report questionable data after spurious results are obtained.QuestionsAre there guidelines that I can use for developing a robust QA/QC procedure for this database?The first step is the most time consuming; is there anything that I can do to make this more efficient?","Creater_id":1381,"Start_date":"2011-02-21 13:24:52","Question_id":7467,"Tags":["dataset","meta-analysis","quality-control","database"],"Answer_count":2,"Last_activity":"2016-08-18 11:36:49","Link":"http://stats.stackexchange.com/questions/7467/quality-assurance-and-quality-control-qa-qc-guidelines-for-a-database","Creator_reputation":3970}
{"_id":{"$oid":"5837a57aa05283111e4d4597"},"View_count":408,"Display_name":"David Dickson","Question_score":0,"Question_content":"I have two models that predicted a value that is area based. I want to measure the RMSE to compare the models, but I would like it to be weighted for area so that errors on a large area are given more weight then errors on a small area. I have come up with the following function in R:weighted.rmse \u0026lt;- function(actual, predicted, weight){  sqrt(mean((predicted-actual)^2*weight/sum(weight)))}For weight I will use the area of prediction. So to my question, is this a valid method for model comparison? Is there a better way?","Creater_id":62080,"Start_date":"2016-08-18 08:05:54","Question_id":230517,"Tags":["r","rms"],"Answer_count":2,"Last_activity":"2016-08-18 11:35:25","Link":"http://stats.stackexchange.com/questions/230517/weighted-root-mean-square-error","Creator_reputation":28}
{"_id":{"$oid":"5837a57aa05283111e4d45a5"},"View_count":23,"Display_name":"rk567","Question_score":1,"Question_content":"I performed a negative binomial regression and here is my output (variable names changed from my original output):                               Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)                   4.041e+00  8.978e-02  45.006  \u0026lt; 2e-16 ***langB                        -1.143e-01  1.181e-01  -0.968 0.333137    langC                        -6.581e-02  1.080e-01  -0.609 0.542311    langD                         5.237e-01  9.540e-02   5.489 4.03e-08 ***langE                        -1.603e-01  1.076e-01  -1.490 0.136289    langF                         9.649e-02  1.042e-01   0.926 0.354362    langG                         1.775e-01  1.043e-01   1.702 0.088696 .  num_users.m                   5.675e-02  7.949e-03   7.139 9.39e-13 ***num_attributes.m              3.030e-04  9.860e-05   3.073 0.002116 ** num_lines.m                   7.902e-05  4.538e-05   1.741 0.081679 .  num_distractions.m            1.892e-02  3.182e-02   0.595 0.552041    type_freq.m                   1.613e-06  4.183e-07   3.855 0.000116 ***prop_attended.m               1.222e+00  4.645e-02  26.299  \u0026lt; 2e-16 ***I am going through the example in http://www.ats.ucla.edu/stat/r/dae/nbreg.htm to understand if I'm interpreting it correctly. As I understand, generally we interpret the effect size of one predictor holding other variables constant. For my case, taking num_users as an example, I would say for a unit increase in the number of users, the expected log count of my response variable increases by 0.06, holding all else constant or at their means.  I'm however wondering what constant means in case of a categorical variable like lang. Would it be langA here, for it is chosen to be the reference? But when I did recordslang, \"C\")my other coefficients still didn't seem to change. So does it then mean that for the two instances I compare, I should hold the language constant, but it doesn't really matter what that constant language is?I've read the wonderful explanation given by @gung as an answer to What does \u0026quot;all else equal\u0026quot; mean in multiple regression? but I find no mention of categorical variables there. Could somebody clarify this please? Thanks!","Creater_id":43500,"Start_date":"2016-08-18 11:25:24","Question_id":230550,"Tags":["categorical-data","regression-coefficients","negative-binomial"],"Answer_count":0,"Last_activity":"2016-08-18 11:25:24","Link":"http://stats.stackexchange.com/questions/230550/what-is-held-constant-in-case-of-categorical-variables-when-multiple-levels-are","Creator_reputation":187}
{"_id":{"$oid":"5837a57aa05283111e4d45a7"},"View_count":24,"Display_name":"B. Rowan","Question_score":1,"Question_content":"So I have 110 participants in a survey. Each participant was given the same 7 pieces of information and then asked to rate how that piece of information was more or less likely to do a task on a 10 point scale. They were asked this each time they were given a piece of information. So my independent variables are the 7 pieces of info. My dependent is the measure of influence. The way they were asked these questions makes it seem like a matched pairs type of test. I was wondering if there was a type of matched pairs analysis that would account for 7 independent variables? ","Creater_id":125561,"Start_date":"2016-08-18 10:25:43","Question_id":230542,"Tags":["hypothesis-testing","anova","chi-squared"],"Answer_count":0,"Last_activity":"2016-08-18 11:19:32","Link":"http://stats.stackexchange.com/questions/230542/need-help-deciding-what-test-to-use","Creator_reputation":16}
{"_id":{"$oid":"5837a57aa05283111e4d45a9"},"View_count":343,"Display_name":"Joe Bob","Question_score":0,"Question_content":"I have been trying to gain some familiarity with the Lasagne libraries for machine learning, specifically LSTMs so I set up the following toy problem to determine the parity of a sequence of bits using code from another example.  (I realize that the network is completely overkill for the application).Whenever I actually start training, the network makes no progress (every attempt is a 50-50 split between the two parity classes).  I have tried varying the hyper-parameters but that doesn't seem to have any effect.In theory this should be a simple task for the network to learn, so there must be something that I am fundamentally misunderstanding.  Any advice would be greatly appreciated.  Code is below.from __future__ import print_functionimport numpy as npimport theanoimport theano.tensor as Timport lasagne#Lasagne Seed for Reproducibilitylasagne.random.set_rng(np.random.RandomState(1))# Sequence LengthSEQ_LENGTH = 7# Number of units in the two hidden (LSTM) layersN_HIDDEN = 40# Optimization learning rateLEARNING_RATE = .1# All gradients above this will be clippedGRAD_CLIP = 100# How often should we check the output?PRINT_FREQ = 500# Number of epochs to train the netNUM_EPOCHS = 50# Batch SizeBATCH_SIZE = 100data_size = 100def buildNetwork():    print(\"Building network ...\")    # First, we build the network, starting with an input layer    # Recurrent layers expect input of shape    # (batch size, SEQ_LENGTH, num_features)        l_in = lasagne.layers.InputLayer(shape=(None, None, 1))    #Two stacked LSTM layers    l_forward_1 = lasagne.layers.LSTMLayer(        l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,        nonlinearity=lasagne.nonlinearities.tanh)    l_forward_2 = lasagne.layers.LSTMLayer(        l_forward_1, N_HIDDEN, grad_clipping=GRAD_CLIP,        nonlinearity=lasagne.nonlinearities.tanh, only_return_final=True)    #Slice the output from the LSTM layers to only take the final prediction    l_forward_slice = lasagne.layers.SliceLayer(l_forward_2, -1, 1)    l_out = lasagne.layers.DenseLayer(l_forward_2, num_units=2, W = lasagne.init.Normal(), nonlinearity=lasagne.nonlinearities.softmax)    # Theano tensor for the targets    target_values = T.ivector('target_output')    # lasagne.layers.get_output produces a variable for the output of the net    network_output = lasagne.layers.get_output(l_out)    # The loss function is calculated as the mean of the (categorical) cross-entropy between the prediction and target.    cost = T.nnet.categorical_crossentropy(network_output,target_values).mean()    # Retrieve all parameters from the network    all_params = lasagne.layers.get_all_params(l_out,trainable=True)    # Compute AdaGrad updates for training    print(\"Computing updates ...\")    updates = lasagne.updates.adagrad(cost, all_params, LEARNING_RATE)    # Theano functions for training and computing cost    print(\"Compiling functions ...\")    train = theano.function([l_in.input_var, target_values], cost, updates=updates, allow_input_downcast=True)    compute_cost = theano.function([l_in.input_var, target_values], cost, allow_input_downcast=True)    #Generate a probability distribution    probs = theano.function([l_in.input_var],network_output,allow_input_downcast=True)    return (train, compute_cost, probs)def gen_data(batch_size = BATCH_SIZE, return_target=True):    #Generate a sequence of 0s and 1s.  Target value is the parity of the sequence    #i.e. x=0001101 -\u0026gt; y=1  and x=00110011 -\u0026gt; 0     x = np.zeros((batch_size,SEQ_LENGTH,1))    y = np.zeros((batch_size,))    for n in range(batch_size):        x[n] = np.random.randint(2, size=(1,SEQ_LENGTH,1))        if(return_target):            if (x[n].sum()%2==0):                y[n] = 0            else:                y[n] = 1    return x, np.array(y,dtype='int32')def try_it_out(probs):    #Print a test case during training    x,y = gen_data(1)    # Pick the class with highest probability    ix = np.argmax(probs(x).ravel())    print(\"Sequence:\",x)    print(\"Target:\", y)    print(\"probs(x)\", probs(x).ravel())    print(\"Predicted:\",ix)def runIterations(train, compute_cost, probs, num_epochs=NUM_EPOCHS):    print(\"Training ...\")    try:        for it in xrange(data_size * num_epochs / BATCH_SIZE):            try_it_out(probs) #Run a test            avg_cost = 0;            for _ in range(PRINT_FREQ):                x,y = gen_data()                                avg_cost += train(x, y)            print(\"Epoch {} average loss = {}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))    except KeyboardInterrupt:        passin_vars = buildNetwork()(train, compute_cost, probs) = in_varsrunIterations(train, compute_cost, probs)","Creater_id":112955,"Start_date":"2016-04-20 18:26:56","Question_id":208502,"Tags":["classification","neural-networks","python","deep-learning","lstm"],"Answer_count":1,"Last_activity":"2016-08-18 11:17:08","Link":"http://stats.stackexchange.com/questions/208502/problem-training-an-lstm-network-in-lasagne-for-simple-task-determining-parity","Creator_reputation":1}
{"_id":{"$oid":"5837a57aa05283111e4d45b6"},"View_count":103688,"Display_name":"Neil McGuigan","Question_score":169,"Question_content":"Maybe the concept, why it's used, and an example.","Creater_id":74,"Start_date":"2010-07-19 16:21:05","Question_id":165,"Tags":["bayesian","mcmc","teaching"],"Answer_count":8,"Last_activity":"2016-08-18 11:05:16","Link":"http://stats.stackexchange.com/questions/165/how-would-you-explain-markov-chain-monte-carlo-mcmc-to-a-layperson","Creator_reputation":4211}
{"_id":{"$oid":"5837a57aa05283111e4d45ca"},"View_count":78,"Display_name":"them","Question_score":7,"Question_content":"Consider 3 iid samples  drawn from the uniform distribution  , where  is parameter. I want to find \\mathbb{E}\\left[X_{(2)}| X_{(1)}, X_{(3)}\\right] where  is order statistic . I would expect the result to be \\mathbb{E}\\left[X_{(2)}| X_{(1)}, X_{(3)}\\right] = \\frac{X_{(1)}+ X_{(3)}}{2}But the only way I can show this result seems to be too lengthy, I cannot come up with simple solution, am I missing something, is there some shortcut? What I do is the following:  I find the conditional density f(x_{(2)}| x_{(1)}, x_{(3)}) = \\frac{ f(x_{(1)}, x_{(2)}, x_{(3)})}{f(x_{(1)}, x_{(3)})}I integrate\\mathbb{E}\\left[X_{(2)}| X_{(1)}, X_{(3)}\\right] = \\int x f(x| x_{(1)}, x_{(3)}) dx Details: I adopt general formula for density of order statistic(with  an indicator of set )f_{x_{(1)},\\ldots , x_{(n)}}(x_1,\\cdots, x_n) = n! \\prod_{i=1}^n f_{x}(x_i)\\mathbb{I}_{\\{x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}\\}}(x_1,\\cdots, x_n) to obtain for my casef_{x_{(1)}, x_{(2)}, x_{(3)}}(x_1, x_2, x_3) = 3!\\frac{1}{\\theta^3}\\mathbb{I}_{\\{x_1 \\leq x_2 \\leq \\cdots \\leq x_n\\}}(x_1,\\cdots, x_3) marginal of  is f_{x_{(1)}, x_{(3)}}(u, v) = \\int f_{x_{(1)}, x_{(2)}, x_{(3)}}(u, x_2, v) dx_2 that is f_{x_{(1)}, x_{(3)}}(u, v) = \\int 3!\\frac{1}{\\theta^3}\\mathbb{I}_{\\{x_1 = u \\leq x_2 \\leq x_3 = v\\}}(u, x, v) dx = 3!\\frac{1}{\\theta^3} [v-u]therefor  f(x_{(2)}| x_{(2)} = u, x_{(3)} = v) =\\frac{ f(x_{(1)} = u, x_{(2)}, x_{(3)} = v)}{f(x_{(1)}= u, x_{(3)} = v)} = \\frac{3!\\frac{1}{\\theta^3}\\mathbb{I}_{u \\leq x_2 \\leq \\cdots \\leq v}(u,x_2, v) }{3!\\frac{1}{\\theta^3} [v-u]}= [v-u]^{-1}\\mathbb{I}_{\\{u\u0026lt;x_2\u0026lt;v\\}}which gives \\mathbb{E}\\left[X_{(2)}| X_{(1)} = u, X_{(3)} = v\\right] = [v-u]^{-1}\\int_{u}^{v} x dx = [v-u]^{-1}\\frac{[v^2 - u^2]}{2} = \\frac{u+v}{2}","Creater_id":94074,"Start_date":"2016-08-18 08:35:18","Question_id":230528,"Tags":["uniform","conditional-expectation","order-statistics"],"Answer_count":1,"Last_activity":"2016-08-18 10:59:38","Link":"http://stats.stackexchange.com/questions/230528/easier-way-to-find-mathbbe-leftx-2-x-1-x-3-right","Creator_reputation":116}
{"_id":{"$oid":"5837a57aa05283111e4d45d7"},"View_count":68,"Display_name":"mmngreco","Question_score":1,"Question_content":"I have problems of how to calculate properly the third and fourth order moments: my data is a stratified sampling with three strata.The goal for me is to make a descriptive analysis: mean, variance, coefficient of variation, the problems become when I try calculate the skewness and kurtosis.Here the notation and math that I follow: = sample mean in stratum h = total sample in stratum h. = population size of stratum h. = sample size of stratum h. = elevation factor in stratum h.To calculate:Total population:Mean population:where  is sample quasivariance of stratum h.So, how should I proceed to calculate the moments of third and fourth order (unbiased?) with this type of data.I appreciate any help, guidance or feedback.","Creater_id":73003,"Start_date":"2016-08-03 05:23:08","Question_id":227052,"Tags":["survey","moments","stratification","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-18 10:45:24","Link":"http://stats.stackexchange.com/questions/227052/calculate-properly-statistical-moments-with-stratified-sample-data","Creator_reputation":15}
{"_id":{"$oid":"5837a57aa05283111e4d45e4"},"View_count":39,"Display_name":"AXK","Question_score":1,"Question_content":"I've been following instructions on how to use the Kalman filter and came across the following code which uses the Nile data and produces a plot.model_Nile \u0026lt;- SSModel(Nile ~ SSMtrend(1, Q = list(matrix(NA))), H = matrix(NA)) model_Nile \u0026lt;- fitSSM(model_Nile, c(log(var(Nile)), log(var(Nile))), method = \"BFGS\")$model out_Nile \u0026lt;- KFS(model_Nile, filtering = \"state\", smoothing = \"state\") conf_Nile \u0026lt;- predict(model_Nile, interval = \"confidence\", level = 0.9) pred_Nile \u0026lt;- predict(model_Nile, interval = \"prediction\", level = 0.9) ts.plot(cbind(Nile, pred_Nile, conf_Nile[, -1]), col = c(1:2, 3, 3, 4, 4), ylab = \"Predicted Annual flow\", main = \"Nile\")However, if I replace the Nile data with some other data:Nile \u0026lt;- ts(c(1.60,  1.24,  0.87,  0.89,  0.80,  0.30,  0.34,  0.49,  0.11,  0.22,  0.10, -0.11, -0.68, -0.27,  0.67,  0.65,  0.06,  0.17,  0.12, 0.29,  0.60,  0.02,  0.43,  0.36,  0.37,  0.09, -0.16, -0.36,  0.16,  0.41, -0.04,  0.15, -0.74, -0.66, -0.48,  0.10, -0.73, -0.55, -0.78, -0.42, -0.32, -0.67, -0.32, -0.68,  0.34,  0.56,  0.38, -0.11,  0.08,  0.40,  0.27,  0.36,  0.22,  0.30,  0.40,  0.65,  0.24, 0.01, -0.02,  0.27,  0.54,  0.27,  0.32,  0.48,  0.39,  0.42,  0.18,  0.15,  0.10, -0.02,  0.78,  0.48,  0.50,  0.55,  1.14,  0.47, 0.43,  0.55,  1.32,  1.19,  1.63,  2.17,  1.37,  1.03,  0.55,  0.69,  0.04,  0.15, -0.55, -0.70, -1.37, -0.96, -1.33, -1.98, -2.01, -2.00, -1.69, -2.00, -1.28, -1.30, -0.10, -0.18,  0.03, -0.08, -0.03,  0.16,  0.22,  0.28,  0.00,  0.04,  0.33,  0.44,  0.10,  0.21, 0.06, -0.05,  0.95,  0.88,  0.94,  0.95,  0.92,  1.00,  1.06,  1.15,  1.16,  1.10,  1.11,  1.11,  0.90,  0.97,  0.64,  0.61,  0.35, 0.48,  0.39, -0.10, -0.10, -0.04, -0.02, -0.21, -0.39, -0.77, -0.65, -0.82, -0.59, -0.80, -1.05, -0.97, -1.25, -1.25, -1.15, -1.00, -0.97, -0.73, -0.55, -0.75, -1.12, -0.89, -0.31, -0.67, -0.30, -0.33, -0.18, -0.40, -0.64, -0.57, -0.64, -0.60, -0.54,  -0.88, -1.06, -1.26, -1.19, -1.05))I get a straight line estimated after running the same above code:I don't understand why this is. Could someone please explain.edit: I have found that multiplying the series by 1000 yields some proper results. Could someone explain why this is?","Creater_id":128162,"Start_date":"2016-08-18 09:30:26","Question_id":230537,"Tags":["r","time-series","kalman-filter"],"Answer_count":0,"Last_activity":"2016-08-18 10:34:03","Link":"http://stats.stackexchange.com/questions/230537/straight-line-when-running-the-kalman-filter-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d45e6"},"View_count":36,"Display_name":"Eli Korvigo","Question_score":1,"Question_content":"I've implemented a rather simple stacked autoencoder using lasagne and Theanofrom functools import reduceimport itertoolsimport numpy as npimport theanoimport lasagneThis is how I initialise the weights. class SigmoidInit(lasagne.init.Initializer):    def __init__(self, n_hid, n_vis):        \"\"\"        :type n_hid: int        :param n_hid: the number of hidden units        :type n_vis: int        :param n_vis: the number of visible (output) units        \"\"\"        if not isinstance(n_hid, int) or n_hid \u0026lt; 1:            raise ValueError(\"`n_hid` must be a positive integer\")        if not isinstance(n_vis, int) or n_hid \u0026lt; 1:            raise ValueError(\"`n_vis` must be a positive integer\")        self.n_hid = n_hid        self.n_vis = n_vis    def sample(self, shape):        weights_array = np.asarray(            np.random.uniform(low=-4 * np.sqrt(6. / (self.n_hid + self.n_vis)),                              high=4 * np.sqrt(6. / (self.n_hid + self.n_vis)),                              size=shape),            dtype=theano.config.floatX        )        return theano.shared(value=weights_array, name='W', borrow=True)And the rest of the stuff    def yield_batches(arrays, batch_size, shuffle=True):    \"\"\"    Yields batches of arrays.    :type arrays: collections.Sequence[np.ndarray]    :param arrays: a sequence of arrays to generate batches from, e.g.                   X-values arrays and Y-values array    :type batch_size: int    :param batch_size: the number of entries per batch    :type shuffle: bool    :param shuffle: shuffle order (doesn't affect `x_train` and `y_train`                    objects)    :rtype: Generator[tuple[np.ndarray]]    :return: a generator, yielding tuples of array batches    \"\"\"    if len(set(len(array) for array in arrays)) != 1:        raise ValueError(\"arrays have different length\")    n_entries = len(arrays[0])    indices = np.arange(n_entries)    if shuffle:        np.random.shuffle(indices)    return (tuple(array[indices[i:i+batch_size]] for array in arrays)            for i in range(0, n_entries, batch_size))def create_theano_variable(ndim, dtype, name=None):    try:        return {1: tensor.vector(name, dtype=dtype),                2: tensor.matrix(name, dtype=dtype),                3: tensor.tensor3(name, dtype=dtype),                4: tensor.tensor4(name, dtype=dtype)}[ndim]    except KeyError:        raise ValueError(\"`ndim` must be an integer in [1, 4]\")def tensor_from_array(array, name=None):    # TODO docs    \"\"\"    :type array: np.ndarray    :param array:    :type name: str    :param name:    :rtype: T.TensorVariable    \"\"\"    return create_theano_variable(ndim=array.ndim,                                  dtype=str(array.dtype).split(\".\")[-1],                                  name=name)def inject_random_noise(x, p=0.5):    mask = np.random.binomial(1, p, x.size).reshape(x.shape).astype(bool)    x_hat = x.copy()    x_hat[mask] = 0    return x_hatdef build_autoencoder(n_inp, n_hid, nonlinearity):    input_shape = (None, n_inp)    l_inp = lasagne.layers.InputLayer(input_shape)    l_hid = lasagne.layers.DenseLayer(l_inp, n_hid,                                      W=SigmoidInit(n_hid, n_inp),                                      nonlinearity=nonlinearity)    # init output with tied weights    l_out = lasagne.layers.DenseLayer(l_hid, n_inp, W=l_hid.W.T)    return l_outdef train(network, x, y, epochs, batchsize, loss_fn, update_fn, learning_rate,          **kwargs):    # create target var    # note: I use my own `tensor_from_array` instead of `theano.shared`,    #       because for whatever reason Theano says I can't use a shared    #       variable here and that I should pass it via the `givens`    #       parameter, whatever that is.    input_var = lasagne.layers.get_all_layers(network)[0].input_var    target_var = tensor_from_array(x)    # training functions    prediction = lasagne.layers.get_output(network,                                           deterministic=True)    loss = loss_fn(prediction, target_var).mean()    params = lasagne.layers.get_all_params(network, trainable=True)    updates = update_fn(loss, params, learning_rate=learning_rate, **kwargs)    train_fn = theano.function([input_var, target_var],                               loss, updates=updates)    def run_epoch(x_, y_):        train_batches = yield_batches((x_, y_), batchsize)        train_err = np.mean([train_fn(*batch) for batch in train_batches])        return train_err    return (run_epoch(x, y) for _ in itertools.repeat(None, epochs))def pretrain(autoencoders, x, y, epochs, batchsize, loss_fn, update_fn,             learning_rate, **kwargs):    \"\"\"    :param networks: a sequence of autoencoders.    \"\"\"    ... # simply iteratively train each subsequent autoencoder to recover the former's hidden representation.My data are 100000k vectors of 16 real numbers in . I'm using adadelta update function with  and . Learning rate is 1.0, input random noise level is set to 0.2, batchsize is 5000. The first denoising autoencoder with 30 hidden units, which gets the original data, shows some shocking error rates. I trained it for 10k generations, just to find the squared error decreased from ~1200 to ~1000 (yes, this high) and got stuck. Subsequent autoencoders (with 20, 14, 20 and 30 hidden units respectively) trained pretty well reaching error rates of ~ 0.002. After stacking the encoders together and starting the fine-tuning, the overall error rate decreased from 1100 to 750 in 20000 generations. Still, this is something insane. Any recommendations? I can give the data and the rest o the code, if needed. Not all variables are pair-wise correlated, but for each variable there is at least 1 pair with significant Spearman correlation. ","Creater_id":68146,"Start_date":"2016-08-18 09:36:56","Question_id":230539,"Tags":["neural-networks","autoencoders"],"Answer_count":0,"Last_activity":"2016-08-18 10:09:28","Link":"http://stats.stackexchange.com/questions/230539/stacked-autoencoder-poor-training-performance","Creator_reputation":219}
{"_id":{"$oid":"5837a57aa05283111e4d45e8"},"View_count":15141,"Display_name":"gakera","Question_score":8,"Question_content":"In R, the drop1command outputs something neat.These two commands should get you some output:example(step)#-\u0026gt; swissdrop1(lm1, test=\"F\")Mine looks like this:   \u0026gt; drop1(lm1, test=\"F\")Single term deletionsModel:Fertility ~ Agriculture + Examination + Education + Catholic +     Infant.Mortality                 Df Sum of Sq    RSS    AIC F value     Pr(F)    \u0026lt;none\u0026gt;                        2105.0 190.69                      Agriculture       1    307.72 2412.8 195.10  5.9934  0.018727 *  Examination       1     53.03 2158.1 189.86  1.0328  0.315462    Education         1   1162.56 3267.6 209.36 22.6432 2.431e-05 ***Catholic          1    447.71 2552.8 197.75  8.7200  0.005190 ** Infant.Mortality  1    408.75 2513.8 197.03  7.9612  0.007336 ** ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 What does all of this mean? I'm assuming that the \"stars\" help in deciding which input variables are to be kept. Looking at the output above, I want to throw away the \"Examination\" variable and focus on the \"Education\" variable, is interpretation this correct?Also, the AIC value, lower is better, yes?Ed. Please note the Community Wiki answer below and add to it if you see fit, to clarify this output. ","Creater_id":1994,"Start_date":"2010-11-17 08:59:25","Question_id":4639,"Tags":["r","regression","self-study","stepwise-regression"],"Answer_count":2,"Last_activity":"2016-08-18 10:03:38","Link":"http://stats.stackexchange.com/questions/4639/interpreting-the-drop1-output-in-r","Creator_reputation":201}
{"_id":{"$oid":"5837a57aa05283111e4d45f6"},"View_count":41,"Display_name":"Stuart","Question_score":4,"Question_content":"Suppose you have a set of data (eg. [a, b, a, a, b, b, etc.]), and you have the suspicion that the set of data follows the binomial distribution.Your Null Hypothesis is:  The probability of success p=0.6.You run a binomial test on the data with a probability of success p=0.6, and the resulting P-value of the test is 0.55.Is this result evidence that your data set follows the binomial distribution with probability of success p=0.6?  I ask because the thought that the data follows a binomial distribution was not known for certain from the start, but the results of the binomial test seem to confirm it.  Or do they?You could also use a chi-squared test to test if the data fits a binomial distribution with probability of success p=0.6.  But if the results of the initial binomial test were as described (p-value = 0.55), is there any reason to do this?Thanks!","Creater_id":128163,"Start_date":"2016-08-18 09:52:08","Question_id":230540,"Tags":["hypothesis-testing","distributions","chi-squared","p-value","binomial"],"Answer_count":0,"Last_activity":"2016-08-18 09:52:08","Link":"http://stats.stackexchange.com/questions/230540/use-chi-squared-or-binomial-test-if-distribution-is-not-known","Creator_reputation":21}
{"_id":{"$oid":"5837a57aa05283111e4d45f8"},"View_count":18,"Display_name":"Eli Weissman","Question_score":1,"Question_content":"I used JMP to fit logistic dose-response curves to assay data, and used these curves to predict EC50 values. The EC50s are concentrations for different plant compounds. I want to find out statistically if one plant compound has a lower or higher EC50 than another. I tested the compounds on several annual weeds and plant pathogens. JMP gives EC50 estimates, SE values, and confidence intervals related to these EC50 estimates. I have discussed this issue with a statistician at my University and also found a document online, both sources suggest I use the equations: tstatistic= (EC50A - EC50B)/SE for difference between EC50sSE difference btwn means= sqrt[ SE(EC50A)^2 + SE(EC50B)^2]I think the SE difference between means equation is fine, even if sample sizes are not equal. But I am not sure.If the sample sizes are equal then, from what my sources tell me:df= (sample size producing both logistic curves) - (# of parameters in both curves) - (2)So I have two main questions:Is this methodology correct for doing a t-test with EC50 values? If not, how should I modify it?If variances or sample sizes are not equal, how does this change calculating SE for difference between EC50s, and the tcritical degrees of freedom?Thanks!Eli","Creater_id":128157,"Start_date":"2016-08-18 09:25:23","Question_id":230535,"Tags":["logistic","statistical-significance","t-test","prediction"],"Answer_count":0,"Last_activity":"2016-08-18 09:37:44","Link":"http://stats.stackexchange.com/questions/230535/how-to-compare-ec50s-from-fitted-logistic-functions-with-a-t-test","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d45fa"},"View_count":30,"Display_name":"brpinney","Question_score":2,"Question_content":"I am interested in model monitoring techniques. To be clear, for production of a statistical model, let's say GLM, with a set of covariates (continuous). The model will go into production (live scoring of data) soon. I have used PSI in the past, with some success. It is at a high level. I have been thinking about using something more granular, perhaps control charts for the predictors. Is that the right approach? ","Creater_id":128153,"Start_date":"2016-08-18 08:21:05","Question_id":230522,"Tags":["r","generalized-linear-model"],"Answer_count":0,"Last_activity":"2016-08-18 09:01:17","Link":"http://stats.stackexchange.com/questions/230522/post-production-model-monitoring","Creator_reputation":21}
{"_id":{"$oid":"5837a57aa05283111e4d45fc"},"View_count":374,"Display_name":"Tal Galili","Question_score":10,"Question_content":"Empirical CDF functions are usually estimated by a step function. Is there a reason why this is done in such a way and not by using a linear interpolation? Does the step function has any interesting theoretical properties which make us prefer it?Here is an example of the two:ecdf2 \u0026lt;- function (x) {  x \u0026lt;- sort(x)  n \u0026lt;- length(x)  if (n \u0026lt; 1)     stop(\"'x' must have 1 or more non-missing values\")  vals \u0026lt;- unique(x)  rval \u0026lt;- approxfun(vals, cumsum(tabulate(match(x, vals)))/n,                     method = \"linear\", yleft = 0, yright = 1, f = 0, ties = \"ordered\")  class(rval) \u0026lt;- c(\"ecdf\", class(rval))  assign(\"nobs\", n, envir = environment(rval))  attr(rval, \"call\") \u0026lt;- sys.call()  rval}set.seed(2016-08-18)a \u0026lt;- rnorm(10)a2 \u0026lt;- ecdf(a)a3 \u0026lt;- ecdf2(a)par(mfrow = c(1,2))curve(a2, -2,2, main = \"step function ecdf\")curve(a3, -2,2, main = \"linear interpolation function ecdf\")","Creater_id":253,"Start_date":"2016-08-18 03:05:22","Question_id":230458,"Tags":["r","distributions","ecdf"],"Answer_count":1,"Last_activity":"2016-08-18 09:00:42","Link":"http://stats.stackexchange.com/questions/230458/why-does-ecdf-uses-a-step-function-and-not-a-linear-interpolation","Creator_reputation":7686}
{"_id":{"$oid":"5837a57aa05283111e4d4609"},"View_count":18,"Display_name":"elisa","Question_score":1,"Question_content":"Trying to get the Bayes Factor for a correlation between two variables in my data, I tried three different functions. All implement the Jeffreys–Zellner–Siow (JZS) prior, but I get quite different results with the three approaches. Two questions:Is this suspicious, or is it reasonable that they produce different values, as the implementations are slightly different?Is there a consensus on the best measure to use?  My data:a=rnorm(100,1,2)b=rnorm(100,.8,1.5)myData \u0026lt;- data.frame(a=a, b=b)I try the jzs_corbf function, described and implemented here (shorter version)cor.resu.a_b \u0026lt;- cor.test(myDatab, method=c(\"pearson\"))cor.resu.a_bestimatejzs_corbf(r,n)[1] 0.08206358I also tried the convenience function from the BayesFactor package:require(BayesFactor)regressionBF(b ~ a, data = myData, progress=FALSE)Bayes factor analysis--------------[1] a : 0.2181081 ±0%Against denominator:  Intercept only ---Bayes factor type: BFlinearModel, JZSAnd I also tried the a function described recently (code)bf10JeffreysIntegrate(n=100, r=r)      cor 0.1297927While in this case the differences are only numerical, in my real data I get quite big differences that make it more difficult to decide on an interpretation. Related","Creater_id":36158,"Start_date":"2016-08-18 08:58:16","Question_id":230532,"Tags":["r","correlation","bayes-factors"],"Answer_count":0,"Last_activity":"2016-08-18 08:58:16","Link":"http://stats.stackexchange.com/questions/230532/different-bayes-factor-values-for-correlation-with-the-same-jzs-approach","Creator_reputation":198}
{"_id":{"$oid":"5837a57aa05283111e4d460b"},"View_count":24,"Display_name":"v.kasto","Question_score":2,"Question_content":"I have a matrix of (scaled) co-occurence counts which I would like to summarise using (classical, i.e. PCA-related) Multi-Dimensional Scaling (MDS), and then rotate (with varimax(), quartimax(), or really any other optimisation routine).Let Azra be the matrix of (scaled) co-occurence counts:Azra \u0026lt;- structure(c(1, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.222222222222222, 0.333333333333333, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.166666666666667, 0.222222222222222, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.333333333333333, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 1, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 1, 0.111111111111111, 0.277777777777778, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0, 0.111111111111111, 0.0555555555555556, 0, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.111111111111111, 1, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.333333333333333, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.277777777777778, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.222222222222222, 0.277777777777778, 0, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.166666666666667, 1, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0.222222222222222, 0.333333333333333, 0.0555555555555556, 0, 0.222222222222222, 0.333333333333333, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.333333333333333, 0.222222222222222, 0.277777777777778, 0.388888888888889, 0.222222222222222, 0.333333333333333, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0, 0.222222222222222, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 1, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0.0555555555555556, 0, 0, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0, 1, 0, 0, 0, 0, 0.111111111111111, 0.111111111111111, 0, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0, 1, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0, 0.166666666666667, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 1, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0, 0, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0, 0.166666666666667, 0, 0.0555555555555556, 0, 0.166666666666667, 0.0555555555555556, 1, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.166666666666667, 0.111111111111111, 0.166666666666667, 1, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.111111111111111, 0, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 1, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.0555555555555556, 0.111111111111111, 0.333333333333333, 0.222222222222222, 0.166666666666667, 0.333333333333333, 0.333333333333333, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.222222222222222, 1, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.388888888888889, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.277777777777778, 0.444444444444444, 0.277777777777778, 0.388888888888889, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 1, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0, 0.166666666666667, 0, 0.0555555555555556, 0, 0.166666666666667, 0.0555555555555556, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 1, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0, 0.166666666666667, 0.0555555555555556, 0.222222222222222, 0, 0.111111111111111, 0, 0.111111111111111, 0, 0.0555555555555556, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0, 1, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.166666666666667, 0.277777777777778, 0.111111111111111, 0.166666666666667, 0.277777777777778, 0.333333333333333, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.388888888888889, 0.111111111111111, 0.111111111111111, 0.222222222222222, 1, 0.111111111111111, 0.333333333333333, 0.222222222222222, 0.222222222222222, 0.166666666666667, 0.277777777777778, 0.333333333333333, 0.277777777777778, 0.333333333333333, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0, 0.0555555555555556, 0.0555555555555556, 0, 0.166666666666667, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 1, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.333333333333333, 0.111111111111111, 1, 0.166666666666667, 0.166666666666667, 0, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.222222222222222, 0, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 1, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.333333333333333, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0, 0, 0, 0.166666666666667, 0.222222222222222, 0, 0, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 1, 0.166666666666667, 0.222222222222222, 0.222222222222222, 0.166666666666667, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0, 0.166666666666667, 0.0555555555555556, 0, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 1, 0.166666666666667, 0.222222222222222, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.0555555555555556, 0, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.0555555555555556, 0.166666666666667, 0.277777777777778, 0.0555555555555556, 0, 0.222222222222222, 0.277777777777778, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.166666666666667, 1, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0, 0.222222222222222, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.333333333333333, 0.166666666666667, 0.222222222222222, 0.277777777777778, 0.388888888888889, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.277777777777778, 0.444444444444444, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.333333333333333, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 0.222222222222222, 1, 0.277777777777778, 0.388888888888889, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.222222222222222, 0.166666666666667, 0, 0.222222222222222, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.277777777777778, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.277777777777778, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.277777777777778, 1, 0.277777777777778, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.111111111111111, 0.277777777777778, 0.333333333333333, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.388888888888889, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.333333333333333, 0.166666666666667, 0.222222222222222, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.388888888888889, 0.277777777777778, 1, 0.0555555555555556, 0.222222222222222, 0.222222222222222, 0.111111111111111, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 1, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0, 0.111111111111111, 0.222222222222222, 0, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.166666666666667, 0.111111111111111, 0, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 1, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.222222222222222, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0, 0, 0.0555555555555556, 0.166666666666667, 0, 0, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.0555555555555556, 1, 0, 0.111111111111111, 0, 0, 0.0555555555555556, 0, 0.166666666666667, 0.111111111111111, 0, 0.166666666666667, 0, 0.0555555555555556, 0, 0.166666666666667, 0.0555555555555556, 0.222222222222222, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.222222222222222, 0, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0, 0.166666666666667, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 1, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.222222222222222, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.222222222222222, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.222222222222222, 0.222222222222222, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 1, 0.166666666666667, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 1, 0.0555555555555556, 0, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0, 0.111111111111111, 0, 0.0555555555555556, 0, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.166666666666667, 0, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0, 0.111111111111111, 0.111111111111111, 0, 0.0555555555555556, 0, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 1, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0, 0.111111111111111, 0.166666666666667, 0.111111111111111, 0.166666666666667, 0, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 0, 0, 0.0555555555555556, 1, 0.0555555555555556, 0.0555555555555556, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0.111111111111111, 0.0555555555555556, 0, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.166666666666667, 0.0555555555555556, 0.111111111111111, 0, 0.0555555555555556, 0.0555555555555556, 0.166666666666667, 0.111111111111111, 0.111111111111111, 0.111111111111111, 0, 0.111111111111111, 0, 0, 0.0555555555555556, 0.111111111111111, 0.111111111111111, 0.0555555555555556, 0.0555555555555556, 1), .Dim = c(35L, 35L), .Dimnames = structure(list(items = c(\"but-how\", \"encyclopedia\", \"alien\", \"language-of-bees\", \"bad-hen\", \"correspondence\", \"bamboozable\", \"inventions\", \"gray-hair\", \"i-we\", \"the-same\", \"the-better\", \"cockatoo\", \"caravan\", \"comma\", \"headturner\", \"crocodile\", \"countries\", \"level\", \"morning\", \"idiom\", \"lullaby\", \"riddle\", \"eating-grandpa\", \"easter-bunny\", \"alphabet-of-swearing\", \"guilt\", \"trouble\", \"eating-animals\", \"random-poetry\", \"comparatively\", \"one-letter\", \"resistance\", \"conjugation\", \"censored\"), items = c(\"but-how\", \"encyclopedia\", \"alien\", \"language-of-bees\", \"bad-hen\", \"correspondence\", \"bamboozable\", \"inventions\", \"gray-hair\", \"i-we\", \"the-same\", \"the-better\", \"cockatoo\", \"caravan\", \"comma\", \"headturner\", \"crocodile\", \"countries\", \"level\", \"morning\", \"idiom\", \"lullaby\", \"riddle\", \"eating-grandpa\", \"easter-bunny\", \"alphabet-of-swearing\", \"guilt\", \"trouble\", \"eating-animals\", \"random-poetry\", \"comparatively\", \"one-letter\", \"resistance\", \"conjugation\", \"censored\")), .Names = c(\"items\", \"items\")))I first calculate a distance matrix as per this recommendation.Then I plug that into classical MDS aka. Principal Coordinates Analysis (PCoA) aka. Torgerson's MDS which, I understand, is basically a Principal Components Analysis, only based on Euclidian distances, not correlations or covariances.Then we plot the results in slices (because we MDS on 3 dimensions), just for kicks and giggles.distances \u0026lt;- sqrt(1-Azra)md_scaled \u0026lt;- cmdscale(d = distances, k = 3, eig = TRUE)md_scaled \u0026lt;- as.data.frame(md_scaled)g1 \u0026lt;- ggplot(data = md_scaled, mapping = aes(x = V1, y = V2, label = rownames(md_scaled))) + geom_text()g2 \u0026lt;- ggplot(data = md_scaled, mapping = aes(x = V1, y = V3, label = rownames(md_scaled))) + geom_text()g3 \u0026lt;- ggplot(data = md_scaled, mapping = aes(x = V2, y = V3, label = rownames(md_scaled))) + geom_text()grid.arrange(g1, g2, g3, ncol = 2)So far so good.Can I now rotate the resulting matrix as I would rotate a loadings matrix from a PCA?It sure is technically possible to rotate and MDS result with varimax() and the results are different:any(varimax(as.matrix(md_scaled))$loadings == as.matrix(md_scaled))#\u0026gt; [1] FALSEAnyway, can I just rotate the results from this MDS as I please?","Creater_id":127875,"Start_date":"2016-08-18 08:40:33","Question_id":230530,"Tags":["pca","similarities","multidimensional-scaling","factor-rotation","pcoa"],"Answer_count":0,"Last_activity":"2016-08-18 08:45:18","Link":"http://stats.stackexchange.com/questions/230530/can-i-rotate-a-classical-mds-result-with-varimax-etc","Creator_reputation":36}
{"_id":{"$oid":"5837a57aa05283111e4d460d"},"View_count":107,"Display_name":"Clemen5","Question_score":2,"Question_content":"I wonder what the difference between the terms \"classifier\" and \"model\" is with relation to classification methodologies for machine learning.Thanks in advance for your answers!","Creater_id":128143,"Start_date":"2016-08-18 07:24:17","Question_id":230505,"Tags":["classification","terminology"],"Answer_count":3,"Last_activity":"2016-08-18 08:43:29","Link":"http://stats.stackexchange.com/questions/230505/difference-between-classifier-and-model-in-classification","Creator_reputation":13}
{"_id":{"$oid":"5837a57aa05283111e4d461c"},"View_count":34,"Display_name":"fgregg","Question_score":0,"Question_content":"The log conditional probability for the autologistic model isFrom Cressie's Statistics for Spatial Data, I follow how the conditional probability follows from the definition of negpotentials, and how that implies that the log likelihood have the form.where  is the set of all configurations of the N sites.For my own understanding, I've been trying to find a derivation of the full likelihood from the conditional probabilities.","Creater_id":82,"Start_date":"2016-08-05 13:25:49","Question_id":228504,"Tags":["autocorrelation","markov-random-field"],"Answer_count":1,"Last_activity":"2016-08-18 08:36:51","Link":"http://stats.stackexchange.com/questions/228504/derive-data-likelihood-for-conditional-probability-for-autologistic-model","Creator_reputation":775}
{"_id":{"$oid":"5837a57aa05283111e4d4629"},"View_count":497,"Display_name":"Hiếu Nguyễn Phi","Question_score":0,"Question_content":"I'm new here and also new in Ox environment. Those below are results I obtained from estimating AR(5)-FIGARCH(1,,1) models.Normality Test                   Statistic       t-Test      P-ValueSkewness           -0.028508      0.70640      0.47994Excess Kurtosis       2.1108       26.159  7.8388e-151Jarque-Bera           683.85         .NaN  3.1896e-149Q-Statistics on Standardized Residuals**--\u0026gt; P-values adjusted by 5 degree(s) of freedom Q( 10) =  41.5645   [0.0000001]**Q( 15) =  51.7216   [0.0000001]**Q( 20) =  58.0098   [0.0000006]**Q( 25) =  65.8658   [0.0000008]**Q( 30) =  67.4852   [0.0000090]**H0 : No serial correlation ==\u0026gt; Accept H0 when prob. is High [Q \u0026lt; Chisq(lag)]**Q-Statistics on Squared Standardized Residuals**--\u0026gt; P-values adjusted by 2 degree(s) of freedom Q(  5) =  4.98843   [0.1726460]  Q( 10) =  14.6018   [0.0673662]  Q( 15) =  16.7924   [0.2089693]  Q( 20) =  17.1298   [0.5142000]  Q( 25) =  18.1990   [0.7466068]  Q( 30) =  22.0400   [0.7794358]  H0 : No serial correlation ==\u0026gt; Accept H0 when prob. is High [Q \u0026lt; Chisq(lag)]**Diagnostic test based on the news impact curve (EGARCH vs. GARCH)**                                     Test  P-valueSign Bias t-Test                  1.55988  0.11879Negative Size Bias t-Test         0.01090  0.99130Positive Size Bias t-Test         2.38646  0.01701Joint Test for the Three Effects  5.97232  0.11296**ARCH-LM test**ARCH 1-2 test:    F(2,3674) =  0.10906 [0.8967]  ARCH 1-5 test:    F(5,3668) =  0.98214 [0.4270]  ARCH 1-10 test:   F(10,3658)=   1.4771 [0.1412]  ARCH 1-15 test:   F(15,3648)=   1.1139 [0.3371]  ARCH 1-20 test:   F(20,3638)=  0.85385 [0.6478]  ARCH 1-25 test:   F(25,3628)=  0.73108 [0.8299]  ARCH 1-30 test:   F(30,3618)=  0.74544 [0.8398]**Nyblom test**Joint Statistic of the Nyblom test of stability: 14.2373Individual Nyblom Statistics: Cst(M)           0.36252 #Constant in meanAR(1)            3.48168AR(2)            0.39023AR(3)            0.41737AR(4)            1.55922AR(5)            1.23312Cst(V)           0.52501 #Constant in varianced-Figarch        4.12791ARCH(Phi1)       4.29420GARCH(Beta1)     4.55406Asymmetry        1.30355Tail             0.90821Rem: Asymptotic 1% critical value for individual statistics = 0.75.Asymptotic 5% critical value for individual statistics = 0.47.**Adjusted Pearson Chi-square Goodness-of-fit test**# Cells(g)  Statistic      P-Value(g-1)     P-Value(g-k-1)      40       40.4779         0.404929          0.046194   50       46.9408         0.556995          0.126769   60       57.9894         0.512788          0.130698Rem.: k = 12 = # estimated parametersQuestions:Why -test of JB is .NaN?How do I explain Box-Pierce test on residuals? For standardized residuals, it has no autocorrelation because residuals are filtered from AR(5). However, for squared standardized residuals, why are they still autocorrelated?What does it mean if positive size effect is significant at 5% but sign effect is not significant?ARCH effect here no longer exists, right?Just 3 parameters have statistics below the critical value, so just 3 of them are constant over time. Does my model look good for forecasting?I absolutely know nothing about the last test. Why are there 2 columns of -values? Because I think -value is too large so the model does fit the data, but one -value is significant, i.e low, so how do I conclude about model?","Creater_id":107641,"Start_date":"2016-03-11 07:26:13","Question_id":201165,"Tags":["garch","diagnostic"],"Answer_count":1,"Last_activity":"2016-08-18 08:26:06","Link":"http://stats.stackexchange.com/questions/201165/garch-model-diagnostics-how-to-interpret-test-results","Creator_reputation":11}
{"_id":{"$oid":"5837a57aa05283111e4d4636"},"View_count":27,"Display_name":"Neph","Question_score":3,"Question_content":"I have the typical linear regression model:y_i = x_i^T\\beta + e_i,where , iid. However, in my case, some (not all of them, only around 1/3 of them) response variables  are right-censored. As far as I understand, I cannot apply LASSO (R package lars) directly. What alternative methods (I would be interested in a LASSO-type method) are there for selecting variables with right-censored outcomes? If they are implemented in R, much better.","Creater_id":128154,"Start_date":"2016-08-18 08:18:46","Question_id":230521,"Tags":["r","regression","feature-selection","lasso","censoring"],"Answer_count":0,"Last_activity":"2016-08-18 08:18:46","Link":"http://stats.stackexchange.com/questions/230521/variable-selection-with-right-censored-data","Creator_reputation":16}
{"_id":{"$oid":"5837a57aa05283111e4d4638"},"View_count":129,"Display_name":"Nik Tuzov","Question_score":6,"Question_content":"McCullagh \u0026amp; Nelder, 2nd edition, p 91 claim that to make comparisons \"fair\", it's best to use a single estimate of overdispersion parameter, usually derived from the most complex model. I noticed that the same thing is done in this SAS example. Burnham and Anderson (2nd edition, p 68) are also adamant about using just the global model to estimate the dispersion parameter and then plugging it into smaller models.Does anyone know what is meant by \"fair\" and why re-estimating a variance parameter is unfair? If (which is very likely) the most complex model is overfitted then its deviance is too small, meaning that the dispersion parameter is underestimated. Is it still a good idea to use the biased estimate for all models? McCullagh \u0026amp; Nelder (p 127) admit that estimating the dispersion parameter is similar to estimating  in linear regression. Correspondingly,  I recalled a similar reasoning in Neter et al (4th edition, p 342) where  criterion for linear models is described. We are suggested that the largest model delivers \"an unbiased estimator of \" that we are supposed to use to assess the bias-variance tradeoff for smaller models.For both linear and binomial setups, all the authors above agree that one must get the structural part of the model right before estimating the dispersion parameter. However, if we know what the correct structural part is, it is not clear why we should proceed with model selection instead of just using the global model.Another contradiction is that, if we want to use AIC and similar criteria for linear model selection, then I have never heard of a recommendation to estimate  using the global model and then plug it into the smaller ones. Likewise, I haven't heard of such recommendation for the dispersion parameter in Negative Binomial regression.Some comments below suggested that the problem is that in Binomial regression  is not estimated via MLE, but in Negative Binomial regression it is. So, as long as there is a way to estimate dispersion via MLE, no one has a problem with allowing the dispersion parameter to vary across models, and vice-versa? For instance, if we use Williams variance functionwhere  is estimated by IWLS, then we must use the same  for all the models in the pool. However, if we apply Beta-binomial regression that can be estimated by MLE given that , then it's fine to allow  to vary across the models. The problem with such reasoning is that Beta-binomial is a particular case of Williams variance function, where . It doesn't make sense to disallow  to vary simply because it was not estimated by \"pure\" MLE. IWLS and MLE estimates of  are probably close anyway, and may be even identical because in practice MLE is often implemented via IWLS (e.g. Negative Binomial regression in R).","Creater_id":123427,"Start_date":"2016-07-28 11:00:31","Question_id":226152,"Tags":["binomial","model-selection","overfitting","overdispersion","beta-binomial"],"Answer_count":2,"Last_activity":"2016-08-18 08:09:52","Link":"http://stats.stackexchange.com/questions/226152/should-one-use-the-same-overdispersion-parameter-when-comparing-binomial-models","Creator_reputation":165}
{"_id":{"$oid":"5837a57aa05283111e4d4646"},"View_count":1475,"Display_name":"Alan","Question_score":22,"Question_content":"The answers (definitions) defined on Wikipedia are arguably a bit cryptic to those unfamiliar with higher mathematics/statistics.  In mathematical terms, a statistical model is usually thought of as a  pair (), where  is the set of possible  observations, i.e. the sample space, and  is a set of  probability distributions on .    In probability and statistics, a probability distribution assigns  a probability to each measurable subset of the possible outcomes of a  random experiment, survey, or procedure of statistical inference.  Examples are found whose sample space is non-numerical, where the  distribution would be a categorical distribution.I am a high school student very interested in this field as a hobby and am currently struggling with the differences between what is a statistical model and a probability distribution My current, and very rudimentary, understanding is this:statistical models are mathematical attempts to approximate measured distributionsprobability distributions are measured descriptions from experiments that assigns probabilities to each possible outcome of a random eventconfusion is further compounded by the tendency in literature to see the words \"distribution\" and \"model\" used interchangeably - or at least in very similar situations (e.g. binomial distribution vs binomial model) Can someone verify/correct my definitions, and perhaps offer a more formalized (albeit still in terms of simple english) approach to these concepts?","Creater_id":100529,"Start_date":"2016-05-02 00:40:04","Question_id":210403,"Tags":["distributions","model","terminology"],"Answer_count":6,"Last_activity":"2016-08-18 08:07:45","Link":"http://stats.stackexchange.com/questions/210403/in-laymans-terms-what-is-the-difference-between-a-model-and-a-distribution","Creator_reputation":256}
{"_id":{"$oid":"5837a57aa05283111e4d4658"},"View_count":58,"Display_name":"Ricardo UES","Question_score":2,"Question_content":"I have a time series about demand of a product with many zeros.I was reading the forecasting book by Hyndman and Athanasopoulos [1] (section 2.3), which mentioned the average method for forecasting time series.  I was wondering if is it possible to use a median method for forecasting time series? [1] Hyndman, R.J. and Athanasopoulos, G. (2013)Forecasting: principles and practice.OTexts: Melbourne, Australia. Section 2/3. https://www.otexts.org/fpp/2/3Accessed on August 18 2016.","Creater_id":82662,"Start_date":"2016-08-17 14:07:52","Question_id":230377,"Tags":["time-series","forecasting","median"],"Answer_count":2,"Last_activity":"2016-08-18 08:04:32","Link":"http://stats.stackexchange.com/questions/230377/median-for-forecasting-in-time-series","Creator_reputation":34}
{"_id":{"$oid":"5837a57aa05283111e4d4666"},"View_count":16,"Display_name":"Karel Macek","Question_score":0,"Question_content":"Let us have a realization of binary process . Let us assume that there are some trends in it (e.g. periodic weekly patterns).The question is: How to construct a classifier that would forecast  for a given ?My attempt: To prepare the same regression matrix as for the frequency-domain linear regression (http://uk.mathworks.com/help/signal/ug/frequency-domain-linear-regression.html ). However, to apply instead of linear regression the logistic regression.","Creater_id":56418,"Start_date":"2016-08-18 08:03:41","Question_id":230515,"Tags":["time-series","logistic","classification","binary-data","stochastic-processes"],"Answer_count":0,"Last_activity":"2016-08-18 08:03:41","Link":"http://stats.stackexchange.com/questions/230515/classification-detection-of-season","Creator_reputation":626}
{"_id":{"$oid":"5837a57aa05283111e4d4668"},"View_count":85,"Display_name":"26hmkk","Question_score":2,"Question_content":"I have designed a simple feed-forward neural network using stochastic gradient descent. I use 22 inputs, 4 hidden layers, 1 output and am using a learning rate of 0.7 and momentum of 0.3. I have about 700 points in my training set. As I began training, I noticed that my MSE (calculated on a validation set of about 200 points) was increasing with each epoch. I then decided to reduce the number of hidden layers I was using from 4 to 3. This - for whatever reason - worked, and the MSE dropped with each epoch and the network converged. This leads me to lack confidence in my implementation of the backpropagation algorithms.Why would this occur? Do extra, unneeded hidden layers prevent convergence? Could it be my algorithm?","Creater_id":127901,"Start_date":"2016-08-16 18:50:14","Question_id":230193,"Tags":["machine-learning","neural-networks","convergence","mse","backpropagation"],"Answer_count":4,"Last_activity":"2016-08-18 07:45:04","Link":"http://stats.stackexchange.com/questions/230193/do-extra-hidden-layers-prevent-convergence","Creator_reputation":113}
{"_id":{"$oid":"5837a57aa05283111e4d4678"},"View_count":58,"Display_name":"M. Beausoleil","Question_score":3,"Question_content":"Some authors are saying that a mean and variance should be set for random variable, but that   The means of the random effects are usually assumed to be zero. Later they are saying:   If a factor is random, then it is assumed to be a variable that is  sampled from a population that has a particular mean and varianceMy question are:  Why in this case you would set a mean of 0?Why not assuming the mean of the parameter (to create thedistribution of that random effect) your assuming to be random?What is the difference between a random effect that has a mean of 0 vs another number? ","Creater_id":93498,"Start_date":"2016-08-15 08:11:05","Question_id":229932,"Tags":["mixed-model","random-variable"],"Answer_count":1,"Last_activity":"2016-08-18 07:44:58","Link":"http://stats.stackexchange.com/questions/229932/what-does-it-mean-to-have-a-random-effect-with-or-without-a-mean-equals-0","Creator_reputation":272}
{"_id":{"$oid":"5837a57aa05283111e4d4685"},"View_count":69,"Display_name":"LAWRENCE","Question_score":0,"Question_content":"This is a problem that I met in real regression analysis using real data. I am using Random forest as well as Adaboost method to do regression. X has 5 dimension and y has 1 dimension. The sample size is 110k. However, if I add more feature into X (X has more dimension). Then the out of sample test of the model is reduced by 30%.My knowledge says this is very unlikely to happen but this indeed happened. I am wondering the underlying mechanisms and how shall we deal with this problem.","Creater_id":93309,"Start_date":"2016-08-17 06:14:49","Question_id":230271,"Tags":["machine-learning","random-forest","adaboost"],"Answer_count":1,"Last_activity":"2016-08-18 07:22:38","Link":"http://stats.stackexchange.com/questions/230271/more-feature-leads-to-worse-result-in-random-forest-and-adaboosting-method","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d4691"},"View_count":30,"Display_name":"zedd","Question_score":0,"Question_content":"Im sure theres a name for this concept, but i cant find it (im not familiar with  statistical terms):Suppose i have a series of numbers, and an average of those. Im wondering how correct this average is to predict future values. Say the average is 5, then if my series all consisted of fives, i would say that the next number in the series has a high probability of being five. But the series could just as well have been half 10:s and half 0:s (still giving an average of 5), and that would give a low probability of the next number in the series beeing 5. How do i calculate this, or express the distance of my series to the avg?int[] series = [5, 5, 5, 5, 5, 5]; //avg is 5, series is close to avgint[] series = [10, 10, 10, 0, 0, 0] //avg is 5, series is far from avgEdit: A calculation like this would give some indication of how much the series values differs from the average. The blue box is what im looking to calc or find a name for (its the average diff expressed as a multiple of the average).","Creater_id":128109,"Start_date":"2016-08-18 03:14:34","Question_id":230462,"Tags":["variance","average"],"Answer_count":1,"Last_activity":"2016-08-18 07:10:15","Link":"http://stats.stackexchange.com/questions/230462/how-far-is-my-average-from-the-series-values","Creator_reputation":103}
{"_id":{"$oid":"5837a57aa05283111e4d469e"},"View_count":219,"Display_name":"Nik Bernou","Question_score":0,"Question_content":"I have obtained genes with ratios. As an small example you can see my data belowGene    Control1     Control2   Control3    Treated1    Treated2    Treated3pps-1   324680000   211350000   356350000   269770000   258080000   292830000R11A8.7 477490000   610780000   539550000   533590000   530810000   578290000ugt-21  105080000   103430000   74137000    78915000    42381000    31415000spp-18  1042800000  615030000   332720000   538340000   448280000   412310000Now My question is that I have Three controls and Three Treated, Control has two biological replicate and Treated has two biological replicateHow can I calculate the fold change for it?I see two waysThe first way I take the average of my control group , lets call it A (one column)I take the average of my treated group, lest call it B (one column) Then I calculate the fold change (B/A) This way, I can check also whether the correlation between all biological replicate of control or treated are high which indicates taking the average is fineThe second wayI perform multi comparison test on both group I find up regulated genes and down regulated genes I discard the rest of the genesI take the average of my control group , lets call it A (one column)I take the average of my treated group, lest call it B (one column) Then I calculate the fold change (B/A) which one of them make more sense?   My main concern is how to calculate the fold change when I have  biological replicate ,I posted in biology group they said it is better I post it here   How then can one calculate p-values for fold change if it is based on  average","Creater_id":127951,"Start_date":"2016-08-18 05:18:59","Question_id":230484,"Tags":["statistical-significance","computational-statistics","biostatistics","bioinformatics"],"Answer_count":1,"Last_activity":"2016-08-18 06:47:06","Link":"http://stats.stackexchange.com/questions/230484/how-to-calculate-fold-change-when-we-have-replicate","Creator_reputation":16}
{"_id":{"$oid":"5837a57aa05283111e4d46ab"},"View_count":55,"Display_name":"maths","Question_score":1,"Question_content":"I'm a chain store and have many stores across the country that all sell a particular shampoo product P.I have made a plot as shown in the image below. For example, 30 stores each sold 1 of P in the past two weeks. The sum of the areas of the bars is  the total number of P sold across all stores in the past two weeks. What should be the theoretical shape of this plot? You may assume anything that gives a clean answer to this. Thanks!","Creater_id":128110,"Start_date":"2016-08-18 03:24:44","Question_id":230466,"Tags":["distributions"],"Answer_count":0,"Last_activity":"2016-08-18 06:39:10","Link":"http://stats.stackexchange.com/questions/230466/what-distribution-should-my-data-on-sales-follow","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d46ad"},"View_count":44,"Display_name":"Programmer2134","Question_score":1,"Question_content":"To prevent any confusion, I am not considering the Bayesian interpretation here (though anyone is welcome to include that in an answer if it helps clarification).The Wikipedia page on the \"classical definition of probability\" states that   The classical definition of probability was called into question, [and] The frequentist definition of probability became widely accepted as a result of [this] criticismI did some reading, but I don't quite understand the difference between the classical interpretation and the frequentist interpretation, since (in general terms) they both deal with frequencies.So what is the essential difference?","Creater_id":127096,"Start_date":"2016-08-18 01:40:01","Question_id":230438,"Tags":["probability","interpretation","definition","frequentist"],"Answer_count":1,"Last_activity":"2016-08-18 06:28:04","Link":"http://stats.stackexchange.com/questions/230438/difference-between-classical-and-frequentist-interpretation-of-probability","Creator_reputation":143}
{"_id":{"$oid":"5837a57aa05283111e4d46ba"},"View_count":44,"Display_name":"SakiSam","Question_score":1,"Question_content":"I am trying to determine the accuracy of a set of forecast values. Not the accuracy of the Forecast to the Actuals (if that makes sense).Here is my data sample:Set    Forecast        Actual        O/(U)1       3.1             2.1           1.1 2       2.5             4.6          (2.1)3       1.9             5.3          (3.3)(there is some rounding here)Now I know to calculate the error to actual error is =ABS(O/U)/Actual(My preference.) My people want to do something different, can I compare the accuracy to the forecast with just this information? ie: =1-(ABS(O/U)/Forecast).If so in the case of set #3 the accuracy is 3.3/1.9 = 173%.  It seems that the accuracy should be between 0 and 100 %.  Am I wrong? Or is there a better formula/method of getting what they want?To try and clarify;My group wants to determine the accuracy of a forecast by using the Forecast it as the base in their calculations, instead of the Actuals.1) For determining forecast accuracy is the formula, =1-(absolute value of the O/(U) divided by the Forecast value), an acceptable way of determining how accurate the forecast is.2) If we are looking at the accuracy, shouldn't the result always be between 0 and 100 %.Example #3 is 1-(|-3.3|/1.9) ---\u0026gt; 1-(3.3/1.9) ---\u0026gt; 1-(1.73) ---\u0026gt; -0.73 ---\u0026gt; -73% 3) If the formula in question 1 is not the best method to calculate the accuracy, what is? Using only the sample data elements provided.","Creater_id":128070,"Start_date":"2016-08-17 14:36:00","Question_id":230384,"Tags":["forecasting","accuracy","performance"],"Answer_count":0,"Last_activity":"2016-08-18 05:54:14","Link":"http://stats.stackexchange.com/questions/230384/accuracy-of-forecast-not-actuals","Creator_reputation":6}
{"_id":{"$oid":"5837a57aa05283111e4d46bc"},"View_count":217,"Display_name":"Andy Eschbacher","Question_score":2,"Question_content":"For two regions which have independent probabilities of an event happening and share an overlapping region, what is the probability of an event happening in the combined region? I'm speaking of physical space here, not abstract set intersections.For instance, if I am given one region that has a 40% probability of an event happening there, and another that has a 15% probability of the same type of event, what is the probability in the intersecting region? We can assume that the probabilities are independent of one another. This could apply to counties overlapping with census tracts, arbitrary geographical shapes, etc.Here's an image to visualize what I'm looking for:","Creater_id":77056,"Start_date":"2015-10-24 15:33:51","Question_id":178512,"Tags":["probability","geostatistics"],"Answer_count":0,"Last_activity":"2016-08-18 05:24:39","Link":"http://stats.stackexchange.com/questions/178512/combining-probabilities-for-overlapping-regions","Creator_reputation":113}
{"_id":{"$oid":"5837a57ba05283111e4d46be"},"View_count":43,"Display_name":"marwa89","Question_score":1,"Question_content":"Does it make sense to run partial least squares (PLS) regression on a data set where there are more dependent variables (output Y(2000*16))  than the independent variables(input X(2000*4))?I'm using plsregress in Matlab.","Creater_id":128104,"Start_date":"2016-08-18 02:24:01","Question_id":230449,"Tags":["regression","matlab","pls"],"Answer_count":0,"Last_activity":"2016-08-18 05:23:28","Link":"http://stats.stackexchange.com/questions/230449/pls-regression-when-there-are-more-y-variables-than-x-variables","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d46c0"},"View_count":24,"Display_name":"deedee","Question_score":1,"Question_content":"I have a questionnaire with Google translations where some of these translations are correct and some are not. Students had to tick if they agree or disagree with these translations.Now I don't know which method to use to analyse these results. I would like to group the correct translations and the incorrect ones and then analyse those groups based on students' gender, grade, etc. to see if there are any differences. Or if you have a better solution please let me know.Thank you very much","Creater_id":128124,"Start_date":"2016-08-18 05:18:26","Question_id":230483,"Tags":["spss","chi-squared","analysis"],"Answer_count":0,"Last_activity":"2016-08-18 05:18:26","Link":"http://stats.stackexchange.com/questions/230483/comparing-participants-answers-to-actual-correct-answers","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d46c2"},"View_count":22,"Display_name":"Alex","Question_score":0,"Question_content":"For example, in 10-fold cross validation, should one randomly draw an integer (with uniform probability) between 1-10 for each data point, or should you make up a vector of fold indices of equal length to the data, and randomly permute them? In the latter case, what do you do if the number of data points is not divisible by 10?","Creater_id":22199,"Start_date":"2016-08-18 04:22:09","Question_id":230475,"Tags":["cross-validation"],"Answer_count":2,"Last_activity":"2016-08-18 04:51:43","Link":"http://stats.stackexchange.com/questions/230475/how-should-one-assign-data-randomly-to-cross-validation-folds","Creator_reputation":717}
{"_id":{"$oid":"5837a57ba05283111e4d46d0"},"View_count":29,"Display_name":"Siva Kg","Question_score":0,"Question_content":"I want to analyse the interactions between the following data:tourist arrivals (my variable of interest)income of the touristsaccomodation capacity (in number of rooms in hotels)a confidence index (an index asking \"how do you evaluate your current financial situation?\")My hypothesis is that tourist arrivals are mostly influenced by the confidence of tourists in the short term, and by accomocation capacity in the long run. So, I estimated a VAR with these variables (monthly data), and want to make this analysis via impulse response fonctions. My expectation is that the confidence has a high impact on arrivals in the first months and then fades away. The accomodation capacity on the other hand, has a lasting impact.But, the problem is that I need to see the simulataneous interactions between variables; how the accomodation capacity and confidence affect tourist arrivals in the same month. How can I do that?I use Eviews, which does not allow to make a lag=0 VAR.Is this normal? If so, why can't I make a lag=0 VAR?Thank you very much!","Creater_id":115286,"Start_date":"2016-08-18 02:36:31","Question_id":230450,"Tags":["var","simultaneous-equation"],"Answer_count":1,"Last_activity":"2016-08-18 04:25:27","Link":"http://stats.stackexchange.com/questions/230450/how-to-estimate-a-var-with-simultaneous-interactions","Creator_reputation":1}
{"_id":{"$oid":"5837a57ba05283111e4d46dd"},"View_count":20,"Display_name":"Walter T","Question_score":0,"Question_content":"Since I have an 8gigs memory constraint minus the Win7 OS \u0026amp; my training file uses about 3 gigs, is there a way to score a 3gigs other file with my random forest model without consuming memory for the original 3 gigs training file? I need memory management tips.Waltert","Creater_id":127257,"Start_date":"2016-08-18 04:20:41","Question_id":230474,"Tags":["random-forest"],"Answer_count":0,"Last_activity":"2016-08-18 04:20:41","Link":"http://stats.stackexchange.com/questions/230474/how-to-score-other-not-training-or-validation-file-in-random-forest-without-us","Creator_reputation":24}
{"_id":{"$oid":"5837a57ba05283111e4d46df"},"View_count":81,"Display_name":"Zelphir","Question_score":3,"Question_content":"I had an online course, where I learned, that unbalanced classes in the training data might lead to problems, because classification algorithms go for the majority rule, as it gives good results if the unbalance is too much. In an assignment one had to balance the data via undersampling the majority class.In this blog however, someone claims that balanced data is even worse:https://matloff.wordpress.com/2015/09/29/unbalanced-data-is-a-problem-no-balanced-data-is-worse/So which one is it? Should I balance the data or not? Does it depend on the algorithm used, as some might be able to adept to the unbalanced proportions of classes? If so, which ones are reliable on unbalanced data?","Creater_id":124558,"Start_date":"2016-08-03 07:59:20","Question_id":227088,"Tags":["machine-learning","classification","unbalanced-classes"],"Answer_count":1,"Last_activity":"2016-08-18 04:06:49","Link":"http://stats.stackexchange.com/questions/227088/when-should-i-balance-classes-in-a-training-data-set","Creator_reputation":121}
{"_id":{"$oid":"5837a57ba05283111e4d46ec"},"View_count":65,"Display_name":"Brendan Hill","Question_score":0,"Question_content":"Consider the following code in R:data = c(603, 103, 225, 201, 1445, 1077, 1309, 6085, 469, 2309)f = fitdistr(data, densfun=\"gamma\")rate = floglikThis generates the following error:Error in stats::optim(x = c(603, 103, 225, 201, 1445, 1077, 1309, 6085,  :   non-finite finite-difference value [1]In addition: Warning messages:1: In densfun(x, parm[1], parm[2], ...) : NaNs produced2: In densfun(x, parm[1], parm[2], ...) : NaNs producedOther posts here indicate that this is due to the extreme scaling of the data leading to unwanted 0's and infinities in the optimization routine:http://stackoverflow.com/questions/15974773/errors-while-trying-to-fit-gamma-distribution-with-r-fitdistrmassI note from these posts that the following workaround (scaling the data, then scaling back the rate) is possible:f = fitdistr(data / 10, densfun=\"gamma\")  ## scaled the data by 0.1rate = floglik  THIS IS WRONG NOW!! :(Can you suggest:1) Parameters to fitdistr which might work around the problem?2) An alternative distribution fitting library for R which might not suffer from the original problem?3) A quick and easy alternative approach in a non-R environment to do the same job? (Python, Matlab etc)","Creater_id":51993,"Start_date":"2016-08-17 22:45:10","Question_id":230420,"Tags":["r","fitting","gamma-distribution"],"Answer_count":2,"Last_activity":"2016-08-18 03:54:36","Link":"http://stats.stackexchange.com/questions/230420/alternatives-to-fitdistr-for-gamma-in-r","Creator_reputation":108}
{"_id":{"$oid":"5837a57ba05283111e4d46fa"},"View_count":46,"Display_name":"letreetlneant","Question_score":2,"Question_content":"I am trying to solve an problem from Rice's Mathematical Statistics and Data Analysis (Problem 9.41) and I got stuck doing some computation. Namely, let  for . I am supposed to devise a log-likelihood ratio test for the null hypothesis , alternative being that not all are equal, and also to find its large sample distribution. I have computed the likelihood ratio to be\\Lambda = \\frac{\\prod_{i = 1}^m \\hat p_i^{x_i}(1-\\hat p_i)^{n_i-x_i}}{\\hat p^{\\sum_{i=1}^mx_i}(1-\\hat p)^{\\sum_{i=1}^m n_i-x_i}},where  and \\hat p= \\frac{\\sum_{i=1}^m x_i}{\\sum_{i=1}^m n_i}. However, when I try to compute , I do not get anything remotely useful. I know that the large sample distribution is supposed to be , but I do not have any idea how to arrive at that conclusion.","Creater_id":128042,"Start_date":"2016-08-18 02:53:09","Question_id":230455,"Tags":["hypothesis-testing","self-study","estimation"],"Answer_count":1,"Last_activity":"2016-08-18 03:38:09","Link":"http://stats.stackexchange.com/questions/230455/likelihood-ratio-test-computation-help","Creator_reputation":11}
{"_id":{"$oid":"5837a57ba05283111e4d4707"},"View_count":263,"Display_name":"user3639557","Question_score":5,"Question_content":"According to Wikipedia, the parameter in a Bernoulli distribution should be . I am reading this famous paper proposing Hierarchical Dirichlet Process, and on page 1580, A.6 and the sentence right after it, they state q(s_j|\\alpha_0)\\propto\\Bigg(\\frac{n_{j..}}{\\alpha_0}\\Bigg)^{s_j}but  is an integer which can take any large value, while  is a real value which almost always is way below . Also  is a binary value either 0, or 1. How can I convert it to an actual Bernoulli?","Creater_id":56676,"Start_date":"2016-08-17 19:20:00","Question_id":230413,"Tags":["random-variable","hierarchical-bayesian","bernoulli-distribution","dirichlet-process"],"Answer_count":1,"Last_activity":"2016-08-18 03:37:52","Link":"http://stats.stackexchange.com/questions/230413/how-can-we-convert-values-proportional-to-probabilities-to-bernoulli-probabiliti","Creator_reputation":298}
{"_id":{"$oid":"5837a57ba05283111e4d4714"},"View_count":69,"Display_name":"Gerard Sanroma","Question_score":4,"Question_content":"I am training a deep neural network using cross entropy loss and L2 regularization, so the final cost function looks something like this:E = - \\frac{1}{N_{samples}} \\sum_{i=1}^{N_{samples}} \\text{cross_entropy}\\left(x_i, y_i\\right) + \\lambda \\sum_{j=1}^{N_{layers}}\\sum_{k=1}^{N_{units}^j}\\sum_{l=1}^{N_{units}^{j+1}} \\left(w^j_{k,l}\\right)^2where the first term is the cross entropy over classes (averaged over the size of the training set) and the second term is the sum of squared weights involved in the network ( is the weight from -th unit in -th layer to -th unit in -th layer), and  is a regularization strength parameter.My question is: won't the number of layers and units affect the scale of the regularization term ?Therefore, wouldn't it make more sense to normalize the second term by the number of weights (i.e., replacing  for ).Unfortunately, I've not found any reference about this. I've just found in Bengio's paper [1] (weight decay subsection) that they recommend to scale according to the number of mini-batches in each epoch (which I do not really see the reason why).[1] Practical Recommendations for Gradient-Based Training of Deep Architectures","Creater_id":124941,"Start_date":"2016-08-03 06:30:23","Question_id":227074,"Tags":["deep-learning","regularization"],"Answer_count":1,"Last_activity":"2016-08-18 03:35:58","Link":"http://stats.stackexchange.com/questions/227074/should-l2-regularization-be-corrected-for-scale","Creator_reputation":28}
{"_id":{"$oid":"5837a57ba05283111e4d4721"},"View_count":38,"Display_name":"NullCanBeARealCoolValue","Question_score":1,"Question_content":"The theorem states that  is a sufficient statistic for  iff  where  is the conditional pdf of  and  and  are some positive functions.What I'm wondering is what role  plays here. I am trying to prove that something is NOT a sufficient statistic and if I have to pick  where  is the conditional pdf of  and  are arbitrary then I am done. But of course this doesn't strictly preclude the existence of some other function for which the condition holds. I'm struggling to understand the role of , simply put. Can anyone shed some intuition, given my admittedly diffuse preamble?","Creater_id":128114,"Start_date":"2016-08-18 03:34:58","Question_id":230467,"Tags":["sufficient-statistics"],"Answer_count":0,"Last_activity":"2016-08-18 03:34:58","Link":"http://stats.stackexchange.com/questions/230467/fisher-neyman-factorization-theorem-role-of-g","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d4723"},"View_count":181,"Display_name":"user67275","Question_score":0,"Question_content":"I have an empirical joint distribution function Can I generate bivariate random number from this distribution with a certain condition such as  ?","Creater_id":15997,"Start_date":"2015-10-22 01:49:12","Question_id":178119,"Tags":["random-generation","joint-distribution","bivariate"],"Answer_count":1,"Last_activity":"2016-08-18 03:21:07","Link":"http://stats.stackexchange.com/questions/178119/generate-bivariate-random-numbers-from-joint-distribution-function","Creator_reputation":384}
{"_id":{"$oid":"5837a57ba05283111e4d4730"},"View_count":22,"Display_name":"Peter Albertson","Question_score":0,"Question_content":"My question is: what is the right sequence of analyzing? And is it the right way to ggplot errorbars from that model?1) Setup modelI expect possible interactions between treatment, race, age and time, so i start with the full model:dependent variable ~ time * treatment * race * age, random=~subjecttime:      7 time points(1min, 2min, ... to 7min)treatment: treatment and no treatmentrace:      race1 and race2age:       age1 (10 months old) and age2 (20 months old)subject:   random factor for repeated measures within one subjectSo far i'm doing it the following way (with R):I'm interested in the change of dependent variable over time in dependence of age, race, and treatment (all with possible interactions)so i set up a model:library(nlme)model \u0026lt;- lme(dv ~ time*treatment*race*age, random=~1|subject, data=mydata, method = \"ML\")2) I try to justify the model (I use either a or b. What is better / enough?)Is it justified to use a, even though i continue using the lme-function from the library(nlme)? That's why i rather tend to use version b...a) glmmMCMC(dv ~ time * treatment * race * age, random=~subject, data=mydata)pMCMC is \u0026lt; 0.05 for time time * treatmenttime * treatment * agetime * treatment * age * race (only for some dv = dependent variables)Does is justify the full model (dv ~ time * treatment * race * age) for all dependent variables, even those who are not significant for the last row of pMCMC-values? Or do i need to setup a new model?model.new \u0026lt;- lme(dv ~ time + time*treatment + time*treatment*age, ...)for those dv, where the last line is not significantAnd do i need to adjust those p-values? Because i got a a lot of them (each fixed effect alone and each combination). Or is it already adjusted or doesn't need to have its p-values adjusted after this b) comparison with reduced modelsa.null = lme(dv ~ 1, random = ~1|subject, data=mydata, method=\"ML\")a.1    = lme(dv ~ time, random = ~1|subject, data=mydata, method=\"ML\")a.2    = lme(dv ~ time * treatment, random = ~1|subject, data=mydata, method=\"ML\")a.3    = lme(dv ~ time * treatment * race, random = ~1|subject, data=mydata, method=\"ML\")a.4    = lme(dv ~ time * treatment * race * age, random = ~1|subject, data=mydata, method=\"ML\")anova(a.null, a.1, a.2, a.3, a.4)All models excepct a.1 are significant (each compared to the previous model) --\u003e enough justification of the model? Do i need to adjust the p-values from the anova results?3) I check for assumptions to use the LMM instead of the GLMM:hist(scale(residuals(model))) ---\u0026gt; check for normal distributionqqnorm(scale(residuals(model))) ---\u0026gt; check for normal distributionplot(fitted(model),residuals(model)) --\u0026gt; check for homoscedasticitythe breadboard does not indicate to collinearity between time, treatment, race, ageAccording to the graphs, there are no obvious violations of the assumptions needed for the linear mixed effect model. (If there are in some cases, i add a weights factor, in order to continue using the linear mixed model instead to switch to the GLMM, because there are some packages i want to use that i can only use with the LMM)4) Visualisationlibrary(ggplot2)ggplot(mydata, aes(time, dv, color = treatment))+ stat_summary(fun.y = mean, geom = \"line\")+ stat_summary(fun.data = mean_se, geom=\"pointrange\")+ facet_grid(race ~ age)Is this the right use of error-bars?I think not, because the ggplot does not account for repeated measures, and thus the error_bars are way too small... So what would be the correct way of plotting error_bars into this ggplot?edit:I have 4 dependent variables dv1, dv2, dv3 and dv4.The statement in 2)a) \"time * treatment * age * race (only for some dv = dependent variables)\" could be read as followed:For some dependent variables i got significant pMCMC-values for time * treatment * age * race, for others not. Am i still allowed to use the full model in those cases?","Creater_id":126632,"Start_date":"2016-08-18 01:40:15","Question_id":230439,"Tags":["interaction","assumptions","glmm","ggplot2","nlme"],"Answer_count":1,"Last_activity":"2016-08-18 03:15:12","Link":"http://stats.stackexchange.com/questions/230439/correct-sequence-of-model-interpretation","Creator_reputation":8}
{"_id":{"$oid":"5837a57ba05283111e4d473d"},"View_count":15,"Display_name":"Alb_a","Question_score":0,"Question_content":"I am analyzing some longitudinal data using MATLAB and its function \"lmefit\". I am new to LME modelling, and I'm having some trouble while trying to understand the results I get. I've first modelled my data with the following LME model: Score ~ 1 + var_1+ var_2+ var_3+ Sex + Years_school+ Ethnic + Age + Converter + (1 | subject_id)where the intercept varies by subject. I got the following results: Model fit statistics:    AIC          BIC          LogLikelihood    Deviance      1.697e+05    1.698e+05    -84840           1.6968e+05Fixed effects coefficients (95% CIs):    Name                   Estimate       SE            tStat      DF       pValue         Lower          Upper          '(Intercept)'          33.07          2.1824        15.153    55111     9.1844e-52         28.792         37.347    'var_1'               -0.00015437     9.4001e-05   -1.6422    55111        0.10055    -0.00033861      2.987e-05    'var_2'                0.0016275      0.0003015     5.3979    55111     6.7719e-08      0.0010365      0.0022184    'var_3'               -0.0014147      0.0004624    -3.0595    55111       0.002218     -0.0023211    -0.00050842    'Sex'                  1.0446         0.61809       1.69      55111       0.091036        -0.1669          2.256    'Years_school'         0.17227        0.078834      2.1852    55111        0.02888       0.017751        0.32678    'Ethnic_BLACK'        -0.92839        0.66718      -1.3915    55111        0.16408        -2.2361        0.37929    'Age'                 -0.098263       0.0034307    -28.643    55111    4.1094e-179       -0.10499      -0.091539    'Converter'           -1.0267         0.40218      -2.5528    55111        0.01069        -1.8149       -0.23839Random effects covariance parameters (95% CIs):Group: oadc (37 Levels)    Name1                Name2                Type         Estimate    Lower      Upper     '(Intercept)'        '(Intercept)'        'std'        1.0958      0.87232    1.3765Group: Error    Name             Estimate    Lower     Upper 'Res Std'        1.1251      1.1184    1.1317Looking at the results, I understand that: var_2, var_3, Years_school, Age and being converter or not affect in the 'Score' at baseline. The relationship between Score and var_2 and Years_school is positive, while Score and var_3, Age and being Converter are negatively associated.  QUESTION SET 1: Are my conclusions right? Can I drawn any other conclusion? Are these conclusion right in general or are they specific to the baseline level?Then, I've added \"Age\" as an interaction variable to variables 1, 2 and 3. My new model is as follows: Score ~ 1 + var_1*Age+ var_2*Age+ var_3*Age + Sex + Years_school+ Ethnic + Converter + (1 | subject_id)And the results are the following:\u0026gt; Model fit statistics:\u0026gt;     AIC           BIC           LogLikelihood    Deviance  \u0026gt;     1.6969e+05    1.6982e+05    -84833           1.6967e+05\u0026gt; \u0026gt; Fixed effects coefficients (95% CIs):\u0026gt;     Name                   Estimate       SE            tStat       DF       pValue         Lower          Upper      \u0026gt;     '(Intercept)'          33.34          2.1949        15.19      55108     5.2844e-52         29.038         37.642\u0026gt;     'var_1'                0.00049271     0.0015476     0.31837    55108         0.7502     -0.0025406       0.003526\u0026gt;     'var_2'               -0.0134         0.0046293    -2.8947     55108      0.0037964      -0.022474      -0.004327\u0026gt;     'var_3'                0.021706       0.0067893     3.1971     55108      0.0013889      0.0083991       0.035013\u0026gt;     'Sex'                  1.0419         0.61934       1.6823     55108       0.092514       -0.17199         2.2558\u0026gt;     'Years_school'         0.17182        0.078994      2.1752     55108       0.029623       0.016995        0.32665\u0026gt;     'Ethnic_BLACK'        -0.9278         0.66855      -1.3878     55108        0.16521        -2.2382        0.38255\u0026gt;     'Age'                 -0.10121        0.0040362    -25.076     55108    5.4022e-138       -0.10912      -0.093301\u0026gt;     'Converter'           -1.0262         0.40299      -2.5464     55108       0.010885         -1.816       -0.23633\u0026gt;     'var_1:Age'           -7.4047e-06     1.7741e-05   -0.41739    55108         0.6764    -4.2176e-05     2.7367e-05\u0026gt;     'var_2:Age'            0.00017364     5.3049e-05    3.2731     55108      0.0010643      6.966e-05     0.00027761\u0026gt;     'var_3:Age'           -0.0002704      7.9485e-05   -3.4019     55108     0.00066956     -0.0004262    -0.00011461\u0026gt; \u0026gt; Random effects covariance parameters (95% CIs): Group: oadc (37\u0026gt; Levels)\u0026gt;     Name1                Name2                Type         Estimate    Lower      Upper \u0026gt;     '(Intercept)'        '(Intercept)'        'std'        1.098       0.87404    1.3793\u0026gt; \u0026gt; Group: Error\u0026gt;     Name             Estimate    Lower     Upper \u0026gt;     'Res Std'        1.1249      1.1183    1.1316Now, the results show that var_2 and var_3 remain being associated to Score, but with the inverse sign (var_2 is now negatively associated to Score while var_3 is positively associated). Regarding the interaction terms, I don't understand too well how should I interpret them.QUESTION SET 2: Why have the signs of var_2 and var_3 changed? Have their meaning been changed now that interaction terms have been introduced? How should I interpret the new interaction terms? Is it correct to say that \"for every additional point in baseline var_2, Score increases 0.00017364 every year\"? Thank you so much for your time and help! ","Creater_id":108784,"Start_date":"2016-08-18 02:57:10","Question_id":230457,"Tags":["mixed-model","data-mining","interpretation","random-effects-model"],"Answer_count":0,"Last_activity":"2016-08-18 02:57:10","Link":"http://stats.stackexchange.com/questions/230457/understanding-linear-mixed-effect-model","Creator_reputation":50}
{"_id":{"$oid":"5837a57ba05283111e4d473f"},"View_count":39,"Display_name":"f311a","Question_score":1,"Question_content":"Is there a reason why the most popular criterion for node splitting is MSE?Why can't we use RMSLE (Root Mean Square Logarithmic Error), for example?","Creater_id":120952,"Start_date":"2016-08-17 09:25:23","Question_id":230316,"Tags":["regression","random-forest","mse"],"Answer_count":1,"Last_activity":"2016-08-18 02:49:31","Link":"http://stats.stackexchange.com/questions/230316/random-forest-regression-and-mse","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d474c"},"View_count":18,"Display_name":"Constantin","Question_score":0,"Question_content":"In Calvet, L. et al. (2015). Robust filtering, on p.1594, the authors state some conditions for robustness of a Bayesian filter, and derive a solution by \"'Huberizing' the derivative of the log-observation density  and then integrating it to obtain the robust density,\" where  is the observation density, conditional on the system state  and past observations .Could someone provide any intuition as to what is meant by \"Huberizing\" and possibly recommend some supplementary literature? I'm assuming that \"Huberizing\" refers to the Huber loss function, but I don't quite understand how the observation density in a state-space model can be \"Huberized.\"","Creater_id":60577,"Start_date":"2016-08-18 02:39:55","Question_id":230451,"Tags":["robust"],"Answer_count":0,"Last_activity":"2016-08-18 02:39:55","Link":"http://stats.stackexchange.com/questions/230451/huberizing-the-derivative-of-the-log-observation-density","Creator_reputation":259}
{"_id":{"$oid":"5837a57ba05283111e4d474e"},"View_count":43,"Display_name":"Tirsha","Question_score":1,"Question_content":"I have a table of frequencies of symptoms between 4 different age categories, and I want to see if the observed frequencies are significantly different between the groups. It is about a scoring instrument which consists of 21 items. We want to see if the frequency of each item differs between the 4 different age groups. What are my possibilities?","Creater_id":127680,"Start_date":"2016-08-14 15:46:41","Question_id":229833,"Tags":["statistical-significance","frequency"],"Answer_count":2,"Last_activity":"2016-08-18 02:27:33","Link":"http://stats.stackexchange.com/questions/229833/should-i-use-the-chi-squared-test","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d475c"},"View_count":85,"Display_name":"PaoloH","Question_score":0,"Question_content":"First of all, this might makes zero sense, I pretty much don't know anything about statistics and i'm not a native english speaker to make things harder.I have a few sets of data that are very similar where, I have some ratio* in the Y-axis and the time in days in the X-axis, I'm trying to do a regression on it. Here is what the plot looks like :*The data is about sick leaves and I study their durations, the ratio is actually : the number of sick leave's days that happen beyond the x-th day divided by the total number of sick leave's days. (For instance if i have 3 leaves that are 2, 8 and 10 days long : the ratio for day 5 is (3+5)/(2+8+10)=0,4)So here is what I did, I tried to find what kind of data this looks like to make a regression afterwards and get my coefficients. I thought it looked like the inverse of an exponential function so i compared it to that, and here is what I got :What i wanted to do afterwards was to compare the coefficients of each sets of data that I have to be able to tell how different they were.I dont know if there is any point in what I'm trying to do so if I need to be more specific about some things please tell me to. To explain the situation more specifically, here is what I'm trying to do, I'm in an internship and I've been asked by an actuary to find a way to prove wether or not there is a \"statistical difference\" between those different sets of data that I have. The problem is that I find that a bit vague and I don't know exactly what to do. I asked advice to a statistician, who wasn't sure of what was expected but he hinted that I should look into linear regression.","Creater_id":127240,"Start_date":"2016-08-17 06:27:23","Question_id":230275,"Tags":["regression","time-series","descriptive-statistics"],"Answer_count":1,"Last_activity":"2016-08-18 02:05:46","Link":"http://stats.stackexchange.com/questions/230275/is-a-linear-regression-the-appropriate-method-and-if-so-whats-next","Creator_reputation":58}
{"_id":{"$oid":"5837a57ba05283111e4d4769"},"View_count":65,"Display_name":"dontloo","Question_score":0,"Question_content":"I'm reading this chapter about EM (9.3.1) of the book \"Pattern Recognition and Machine Learning\". I understand the basic EM algorithm for GMM, but I'm having some problems understanding the probabilistic interpretation of the E step.Most of the following formulas make sense to me except 9.39. Can someone please explain it for me? Thanks a lot.It seems that it is in the form of E[z_{nk}]=\\frac{\\sum_{z_{nk}}z_{nk}f(n,k)}{normalizer} where the numerator is a summation over two possible states of  (0 and 1), so it becomes  in the second step, am I right so far?If so, what are we summing over here in the denominator? 0s and 1s of all possible s (but the result doesn't quite match)? Why  is used as an unnormalized probability?It would make more sense if the denominator is the unnormalized zeroth moment and the numerator is the unnormalized first moment, did I understand it correctly? ","Creater_id":95569,"Start_date":"2016-05-04 01:07:10","Question_id":210815,"Tags":["probability","self-study","expectation-maximization","gaussian-mixture"],"Answer_count":1,"Last_activity":"2016-08-18 02:03:08","Link":"http://stats.stackexchange.com/questions/210815/understanding-the-e-step-of-em-for-gmm","Creator_reputation":2762}
{"_id":{"$oid":"5837a57ba05283111e4d4776"},"View_count":9,"Display_name":"Hernan_L","Question_score":0,"Question_content":"I ran a within-subject design experiment and I am trying to figure out the best way to do a sensible analysis. I have 8 blocks. 4 of them are 'experimental' stimuli and the other 4 are there as control stimuli. The 4 experimental blocks differ on two factors (complexity, with levels simplex and complex, and frequency, with levels high and low). Thus, the experimental conditions are: Simplex-High, Simplex-low, Complex-high, Complex-low. The 4 control conditions are there to validate the effect of the experimental conditions. That means that a for every experimental conditions, there is a specific control condition that must or must not be significantly different from it. So, not all experimental conditions can be compared with all control conditions. There is one control for one experimental condition.The more interesting comparisons, however, are between experimental conditions. To do this, I have to compare the difference between a given experimental condition and its corresponding control, to the difference of another experimental conditions and its control. I intuitively think a simple t-test would do the job here.I should add this is an EEG-ERP experiment, so there will also be a factor of electrode site (left, mid-line and right). So, simply running an ANOVA with factors Complexity (Simple vs Complex), Frequency (High vs Low), Condition (experimental vs control) and Electrode site (left, mid, right) does not seem to cut it, because I expect specific differences between specific experimental and control stimuli. For example, I expect Simple to be bigger than their controls, but complex to be equal or smaller than their controls. And this different modulations would also differ orthogonally with frequency. If you have read until here, I appreciate it already. Any advice is most welcome.","Creater_id":11205,"Start_date":"2016-08-18 01:56:20","Question_id":230441,"Tags":["anova","repeated-measures","inferential-statistics"],"Answer_count":0,"Last_activity":"2016-08-18 01:56:20","Link":"http://stats.stackexchange.com/questions/230441/advice-to-run-a-manova-on-a-within-subject-design","Creator_reputation":44}
{"_id":{"$oid":"5837a57ba05283111e4d4778"},"View_count":13,"Display_name":"kurtkim","Question_score":1,"Question_content":"As the title, I am looking for the concept of this term. However, I failed to obtain a clear explanation on the web. Any comments would help.","Creater_id":70877,"Start_date":"2016-08-18 01:22:12","Question_id":230434,"Tags":["distributions","terminology","asymptotics","definition"],"Answer_count":0,"Last_activity":"2016-08-18 01:25:46","Link":"http://stats.stackexchange.com/questions/230434/what-is-the-first-second-and-higher-order-asymptotic-distribution","Creator_reputation":78}
{"_id":{"$oid":"5837a57ba05283111e4d477a"},"View_count":55,"Display_name":"Simon","Question_score":2,"Question_content":"Lets say I have 4 variables: A1, A2, B1, B2 and I want to simplify my model. I have reason to suspect that A1 and A2 are measuring a similar thing, likewise with B1 and B2One way to simplify my model (reduce my dimension count) is to run PCA to condense A1 and A2 down into a single variable/component, and B1 and B2 into a second variable/component. This would leave me with Y = A + BIf my matrix contains all 4 variables, and run PCA to extract only 2 components, theres no guarantee that component 1 corresponds to A, and component 2 corresponds to BIf, however, I create 2 matrices where M1 contains A1 and A2, and M2 contains B1 and B2 and then run PCA on the 2 matrices separately extracting only 1 component each then the components would correspond to A and B respectivelyMy question: is this a valid approach to reduce the dimensions in my dataset? I'm leaning towards no because by splitting the matrix into 2 and running PCA separately we have no way of modelling any relationship that might exist between A and B. Is that logic correct?","Creater_id":68268,"Start_date":"2016-08-16 16:05:46","Question_id":230180,"Tags":["pca","matrix","dimensionality-reduction"],"Answer_count":2,"Last_activity":"2016-08-18 01:11:30","Link":"http://stats.stackexchange.com/questions/230180/dimensionality-reduction-pca-after-splitting-a-matrix","Creator_reputation":180}
{"_id":{"$oid":"5837a57ba05283111e4d4788"},"View_count":45,"Display_name":"Mpizos Dimitris","Question_score":1,"Question_content":"Definition of the problem: I am having some data with lets say 2 features but there not labeled. I manually create weights and label the data with a scoreI present the data to a user with order and the user gives his own score to some of the data.By the feedback of the user I want to update the weights, assign the new scores and give back the list in the new order.This should continue everytime the user give new labelsIn a point of view is a pairwise online learning to rank problem.A simple Illustration:Assuming I am having the following data which are not labeled:import numpy as npX = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])I create manually weights and label the data (which doesnt mean that this weights are the 'correct' so we aren't sure that the labels are the correct one). With the mannually create weights the labels are the following:Y = np.array([1, 1, 2, 2])And I am training a SGD stochastic gradient descent algorithm using sklearn ,with ‘hinge’ loss which give linear SVM (used for 'learning to rank'):from sklearn import linear_modelclf = linear_model.SGDClassifier()clf.fit(X, Y)After a period I am getting the correct labels of some of the previous data(name it true_data  and use partial_fit to fit it in the previous model:x_true_data = np.array([[-1,-1]])y_true_data = np.array([1])clf.partial_fit(x_new,y_new)This update the weights. Some concerns that I have gennerally:is there a way to 'reward' the previous algorithm if it predicted correctly the result or do the opposite if not of the true_data? Something similar as adaboost works.How reliable this algorithm would be if the true_data data is not many ( between 5-20)Can i adjust the learning rate in order to give higher 'value' to the true_data?Which parameters of SGD should I consider in such a problem?Is there another way to think the problem? Should I consider other algorithms except SVM?Some links that I fould usefull but didnt clear my mind if I sould continue in this path:How to update an SVM model with new datalog.regression vs SGDClassifierAny feedback is welcome.","Creater_id":94937,"Start_date":"2016-08-17 14:03:51","Question_id":230374,"Tags":["machine-learning","python"],"Answer_count":0,"Last_activity":"2016-08-18 00:33:38","Link":"http://stats.stackexchange.com/questions/230374/machine-learning-updating-weights-from-true-labeled-data","Creator_reputation":116}
{"_id":{"$oid":"5837a57ba05283111e4d478a"},"View_count":250,"Display_name":"Paul","Question_score":1,"Question_content":"I am currently developing a Vector Autoregressive Model, and I have the model fully specified as follows: X_t=AX_{t-1} +Z_twhere  and  are  column vectors, and  is an  matrix.In one dimension, I know how to use a best linear predictor by looking at the autocovariance matrix. However, it is not so clear how to do this in multiple dimensions, since there is not only an autocovariance matrix for each dimension, but also cross terms in the serial covariances. I have a hunch about how this could be done, but would really appreciate it if anyone had any tips or a reference to look at regarding best linear predictors. On a closely related note, I did find this paper: http://faculty.washington.edu/ezivot/econ584/notes/varModels.pdfwhich explains how to forecast using VAR models (see section 11.3). However, this forecast simply uses an iterative procedure in order to get the prediction. So another question is: is this approach equivalent to the best linear predictor method? ","Creater_id":28466,"Start_date":"2015-01-20 16:44:35","Question_id":134280,"Tags":["forecasting","prediction","var"],"Answer_count":1,"Last_activity":"2016-08-18 00:21:48","Link":"http://stats.stackexchange.com/questions/134280/prediction-in-var-models","Creator_reputation":38}
{"_id":{"$oid":"5837a57ba05283111e4d4797"},"View_count":31,"Display_name":"pkpkPPkafa","Question_score":1,"Question_content":"I am sitting with a couple of time-series that I am analysing using ARIMA models. I have a question regarding prediction intervals. When predicting using a model that takes a first difference (a SARIMA(1,1,0)x(1,0,0) model), I get an increasing size of the prediction interval. Without I get a very constant and narrow band (see below):The corresponding results are as follows:Can anyone explain why the band is so constant? First I thought it was because of a large significant MA coefficient. This, however, I removed and the \"problem\" persisted. Then I though it was because the ARIMA without differencing automatically included an intercept. However, again, when I specified include.mean = FALSE, nothing changed.Any help would be appreciated.","Creater_id":61768,"Start_date":"2016-08-17 06:04:27","Question_id":230269,"Tags":["r","forecasting","arima","prediction-interval"],"Answer_count":1,"Last_activity":"2016-08-18 00:01:38","Link":"http://stats.stackexchange.com/questions/230269/effect-of-differencing-on-prediction-intervals-in-arima-models","Creator_reputation":8}
{"_id":{"$oid":"5837a57ba05283111e4d47a3"},"View_count":53,"Display_name":"statBeginner","Question_score":1,"Question_content":"I have a random variable  that follows an exponential distribution with rate  in the interval  and an exponential distribution with rate  in the interval . I want to estimate the probability that there is no event in the time interval , where .Here is how I have approached this problem.Is the correct way to approach this problem? Should the second integral be ?In general, what is the procedure to find the waiting time to the 1st event in a case where the inter-arrival process is non-homogenous?","Creater_id":55780,"Start_date":"2016-08-17 11:45:41","Question_id":230345,"Tags":["exponential","cdf"],"Answer_count":1,"Last_activity":"2016-08-17 23:54:33","Link":"http://stats.stackexchange.com/questions/230345/waiting-time-for-the-first-event-in-an-exponential-distribution-with-different-r","Creator_reputation":333}
{"_id":{"$oid":"5837a57ba05283111e4d47b0"},"View_count":41,"Display_name":"Laxmidhar Panda","Question_score":-1,"Question_content":"My R code:library(rugarch)rtn\u0026lt;-rnorm(500)vxr\u0026lt;-rnorm(500)specgarch \u0026lt;- ugarchspec(variance.model=list(model=\"sGARCH\",             external.regressors=matrix(vxr)),             mean.model=list(armaOrder=c(0,0),external.regressors=matrix(vxr)),             distribution=\"norm\")garchfit \u0026lt;- ugarchfit(data=rtn, spec=specgarch)garchfit#resultOptimal Parametersvxreg1  0.000000    0.013308   0.000000  1.00000At any set of data I found vxreg provides the same result 0.0000 (p-value=1). Kindly help me regarding this problem.","Creater_id":128089,"Start_date":"2016-08-17 22:07:19","Question_id":230417,"Tags":["r","statistical-significance","garch"],"Answer_count":1,"Last_activity":"2016-08-17 23:42:45","Link":"http://stats.stackexchange.com/questions/230417/external-regressor-always-insignificant-in-garch-model-in-r","Creator_reputation":8}
{"_id":{"$oid":"5837a57ba05283111e4d47bd"},"View_count":85,"Display_name":"Nik Bernou","Question_score":1,"Question_content":"I asked a question here how can i see if my data are coming from two different population? and seems like I cannot correctly communicate and the person who is writing to me is making me confused. So I try to explain what I want , if you know any method, please just tell me the method, I will try to do it myself. I have two groups Group A with n number of samples Group B with m number of samples The measurements have replicate I want to see if the Group A is different from Group B.for example by the mean, or whatever else which makes statistically different or not different. Is there someone who can really guide me what to do?Many thanksNik","Creater_id":127951,"Start_date":"2016-08-17 11:41:48","Question_id":230343,"Tags":["r","hypothesis-testing","statistical-significance","multiple-comparisons","method-comparison"],"Answer_count":3,"Last_activity":"2016-08-17 23:05:11","Link":"http://stats.stackexchange.com/questions/230343/how-can-i-compare-two-groups-of-data","Creator_reputation":16}
{"_id":{"$oid":"5837a57ba05283111e4d47cc"},"View_count":1547,"Display_name":"Germaniawerks","Question_score":1,"Question_content":"Accidentally, I found (by simulation) that the range of a normally distributed sample is related to the estimator of the standard deviation:X \\sim \\mathcal{N}\\left(\\mu, \\sigma\\right)s(X) = \\frac{1}{3}\\left(\\max(X) - \\min(X)\\right)Why is that? I can't find any references. Is it related to the three sigma rule?","Creater_id":36545,"Start_date":"2013-12-23 15:26:47","Question_id":80444,"Tags":["standard-deviation","sample"],"Answer_count":2,"Last_activity":"2016-08-17 22:11:04","Link":"http://stats.stackexchange.com/questions/80444/sample-range-to-estimate-stdev","Creator_reputation":646}
{"_id":{"$oid":"5837a57ba05283111e4d47da"},"View_count":45,"Display_name":"Subhan Ahmad","Question_score":1,"Question_content":"I have applied Fama-Macbeth cross-sectional regression on Fama and French five-factor model (2014). On the left-hand side are the portfolio returns for sixteen size - B/M portfolios. On the right-hand-side are five the factors i.e. market, size, investment, and profitability factor. The issue I am facing is that in 16-time series regressions, my intercepts were all positive and significant. Factor loadings for each factor were also strong and significant. However, when I applied t cross-section regressions (3439 days), the intercept of cross-section regression turned out to be negative and insignificant. Also, the risk premiums are very small (down to 4 decimal points). Kindly help me make sense out of this.Edit: I have added cross-section regression statistics as an example.On the left-hand side were portfolio returns of sixteen size - profitability sorted portfolios (4 x 4 portfolio design). On the right-hand side were the betas of market, size, profitability and investment factors obtained from time-series regression. I am also providing the intercepts obtained from time-series regression.","Creater_id":128051,"Start_date":"2016-08-17 11:36:34","Question_id":230342,"Tags":["regression","interpretation","regression-coefficients"],"Answer_count":0,"Last_activity":"2016-08-17 21:44:08","Link":"http://stats.stackexchange.com/questions/230342/fama-macbeth-cross-sectional-regression-interpretation","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d47dc"},"View_count":72,"Display_name":"Walter T","Question_score":2,"Question_content":"I started tinkering with the random forest (RF) model in R recently. I made a long list of coding mistakes on my way to getting a final solution. The output from the mistake that surprised me was when I used the sample operation incorrectly and used the same records for training and validation. The RF algorithm accurately classified all but about 5 of 2000 responders. This shocked me. I later learned how to correctly sample.The idea that RF could memorize specific records is the focus of my concern. Even though each tree votes on whether the record is responder or non-responder, for example, I think probability has been short-circuited for validation or non-training scoring files by a type of specification bias that ignores maximum likelihood if the validation or scoring record matches a record combination (of fields) that was used in training. This circumstance is how a Relation Database System (RDB) dictionary helps the Oracle or DB2 optimizer (it memorizes performance info to optimize the next exact job/query). The very nature of variability on earth (including statistics) implies that there will be some degree of inconsistencies in observed phenomenon. RF ignores at least some potential variability and the very nature of maximum likelihood probabilities if it correctly classifies almost 100% of records if the validation records are the training records. What happened to the accumulation of (or count of) observed inconsistencies in the data? I think this is a type of specification bias that ignores inherent variability for the validation or non-training scoring data file.The idea that RF has outperformed Logistic Regression doesn't preempt the issue.Could we agree that RF is more \"Rule Induction\" than probability?","Creater_id":127257,"Start_date":"2016-08-16 14:05:00","Question_id":230167,"Tags":["r","random-forest","prediction"],"Answer_count":0,"Last_activity":"2016-08-17 19:36:32","Link":"http://stats.stackexchange.com/questions/230167/is-r-random-forest-really-probability-or-something-else","Creator_reputation":24}
{"_id":{"$oid":"5837a57ba05283111e4d47de"},"View_count":92,"Display_name":"nabakin","Question_score":3,"Question_content":"I play a game called Summoner's War.  It is RNG-based and as such, has a chance of success for each sample.Data has shown that in a particular part of the game where one may 'summon' from a small pool of monsters, there is a success rate of 5.67% per sample for a certain type of monster.Based upon this data, the Expected Value of obtaining this type of monster is believed to be one every ~17.64 samples (100/5.67).We can now assume that after 17.64 samples that we have a 50% chance to obtain said monster because we know 17.64 is the average number of samples it takes to obtain said monster.Since the success rate is 5.67% and one has a 50% chance to obtain said monster at 17.64, shouldn't we be able to also calculate the number of samples (17.64) from a binomial formula that equals 0.5, the average?To reiterate, the formula 1-(1-P)^x = 0.5.  Shouldn't x result in 17.64? A 50% chance to summon the monster.It doesn't seem to work for me.1-(1-0.0567)^(17.64) results in ~0.64 or 64% chance to summon the monster.1-(1-0.0567)^x = 0.5 results in x = ~11.8749 or about 11.8749 samples to summon the monster.What am I missing?  Is the average value not 50%, but instead 64%?","Creater_id":127787,"Start_date":"2016-08-15 17:40:31","Question_id":230014,"Tags":["probability","mathematical-statistics","binomial","descriptive-statistics"],"Answer_count":3,"Last_activity":"2016-08-17 19:14:31","Link":"http://stats.stackexchange.com/questions/230014/given-a-5-67-chance-for-success-and-using-its-expected-value-shouldnt-this-bi","Creator_reputation":18}
{"_id":{"$oid":"5837a57ba05283111e4d47ed"},"View_count":14,"Display_name":"Marwah Soliman","Question_score":1,"Question_content":"my question is about the Output of the simulateSNPglmwhat is the values of the SNPs representI got values 1,2,3I'm not sure what they areWhat I mean Is the simulated data gives Y the response and SNP1, SNP2 ,....as the explanatory variables the values in each SNP is 1 or 2 or 3what does they mean (1,2 and 3)data looks like the following:Y    SNP1  SNP2    0     1     20     2     31     1     11     3     1 Thanks","Creater_id":117854,"Start_date":"2016-08-17 15:17:03","Question_id":230389,"Tags":["r"],"Answer_count":0,"Last_activity":"2016-08-17 18:39:53","Link":"http://stats.stackexchange.com/questions/230389/package-scrime-in-r-using-simulatesnpglm","Creator_reputation":11}
{"_id":{"$oid":"5837a57ba05283111e4d47ef"},"View_count":43,"Display_name":"S. Cow","Question_score":0,"Question_content":"I got this exercise from the book of Paul Newbold:A pharmaceutical manufacturer is concerned that theimpurity concentration in pills should not exceed 3%. Itis known that from a particular production run impurityconcentrations follow a normal distribution with astandard deviation of 0.4%. A random sample of 64 pillsfrom a production run was checked, and the samplemean impurity concentration was found to be 3.07%.a. Test at the 5% level the null hypothesis that thepopulation mean impurity concentration is 3%against the alternative that it is more than 3%.b. Find the p-value for this test.c. Suppose that the alternative hypothesis had beentwo-sided, rather than one-sided, with the nullhypothesis  : . State, without doing the calculations,whether the p-value of the test would behigher than, lower than, or the same as that found inpart (b). Sketch a graph to illustrate your reasoning.d. In the context of this problem, explain why a onesidedalternative hypothesis is more appropriatethan a two-sided alternative.Well, my answer from a) and b) are (i think there are correct):Reject the Null if Since , i dont reject the null.The p value is Now, my problem is about the items c) and d)Can anyone give some intuition about the answers?Thanks a lot!","Creater_id":111402,"Start_date":"2016-08-17 17:34:05","Question_id":230405,"Tags":["hypothesis-testing","p-value"],"Answer_count":0,"Last_activity":"2016-08-17 17:34:05","Link":"http://stats.stackexchange.com/questions/230405/how-is-the-p-value-greater-here","Creator_reputation":43}
{"_id":{"$oid":"5837a57ba05283111e4d47f1"},"View_count":50,"Display_name":"neek","Question_score":3,"Question_content":"I have difficulties following a seemingly elementary claim from Tukey (1960):  It is well known that, in large samples, the relative efficiency as a measure of scale of the mean deviation compared with the standard deviation is 88% when the underlying population is normal.I'm trying to simulate that by generating samples from the normal distribution and arrive at value of  for , i.e. not surprisingly, I see .The aforementioned 88% is then picked up in the Ripley lecture notes (2004). It says , where  ( is the absolute mean deviation) and  is deviation of the optimal estimator. With these details, it seems to me that  should converge to  since  and . I think still that both are unlikely to be typos and I simply misunderstand what they are trying to say. Could someone, please, point me where the \"well known\" 88% comes from?References:Tukey, John W. \"A survey of sampling from contaminateddistributions.\" Contributions to probability and statistics 2 (1960):448-485. Ripley, B.D. \"Robust Statistics\", lecture notes, (2004) (https://www.stats.ox.ac.uk/pub/StatMeth/Robust.pdf)","Creater_id":127970,"Start_date":"2016-08-17 04:19:41","Question_id":230248,"Tags":["normal-distribution","robust","efficiency"],"Answer_count":1,"Last_activity":"2016-08-17 17:18:41","Link":"http://stats.stackexchange.com/questions/230248/relative-efficiency-mean-deviation-vs-standard-deviation","Creator_reputation":118}
{"_id":{"$oid":"5837a57ba05283111e4d47fd"},"View_count":188,"Display_name":"ricardo","Question_score":3,"Question_content":"I am going to perform an experiment to test for the pathogenicity of several bacterial strains. For this I will infect several animals (e.g. 5 per dose per bacterial strain) with increasing doses of bacteria (e.g. 10^5, 10^6, etc) and then obtain a binary response variable, which basically is whether the animal survived or died after exposure to a given bacterial strain/dose combination. I will then determine the dose response curves and their respective lethal dose 50 (LD50). Before I start doing this experiment I have been trying to understand what is the best method to analyse this data. I have been thinking that the best approach for me would be to use a binomial GLM, however I have started to read a bit more of the ecotoxicology literature and found the R package drc (by Christian Ritz \u0026amp; Jens C. Streibig) that implements several models for non-linear regression. Given that my knowledge of nonlinear regression is low, I am now wondering which of the two methods would be better to analyse the data (drc or glm), so any help on making a decision would be greatly appreciated.","Creater_id":11958,"Start_date":"2012-09-20 07:27:38","Question_id":37676,"Tags":["generalized-linear-model","nonlinear-regression"],"Answer_count":1,"Last_activity":"2016-08-17 16:39:51","Link":"http://stats.stackexchange.com/questions/37676/dose-response-and-lethal-dose-50-analysis","Creator_reputation":50}
{"_id":{"$oid":"5837a57ba05283111e4d480a"},"View_count":7,"Display_name":"Turbo","Question_score":0,"Question_content":"Suppose for given  we know  for every  where  holds and each  is independent AWGN with mean  and variance  then what is the best estimate for ?Assume  holds true.","Creater_id":78031,"Start_date":"2016-08-17 16:38:33","Question_id":230404,"Tags":["estimators","geometric-distribution"],"Answer_count":0,"Last_activity":"2016-08-17 16:38:33","Link":"http://stats.stackexchange.com/questions/230404/estimator-for-geometric-progression-multiples","Creator_reputation":106}
{"_id":{"$oid":"5837a57ba05283111e4d480c"},"View_count":51,"Display_name":"BobbyJohnsonOG","Question_score":3,"Question_content":"I have a couple of pipelines:pipeline 1: CV'd feature selection, CV'd hyperparameter selection forclassifier A pipeline 2: CV'd feature selection, CV'd hyperparameterselection for classifier B pipeline 3: CV'd feature selection, CV'dhyperparameter selection for classifier C pipeline 4: CV'd featureselection, CV'd hyperparameter selection for classifier DI want to figure out what the best model process is. So I put all of this into another CV loop to do nested CV:for pipeline in [pipeline1, pipeline2, pipeline3, pipeline4]:    for folds in my CV:        run pipeline        score pipeline    get average score across folds for pipelineThat should give me an average score for each pipeline, and I choose the one that maximizes my score.But if I want a final unbiased estimate of model performance, do I: Use the average score from the CV loop? Split data into a train/test BEFORE I run the nested-CV, and then run nested-CV on train, choose my model, and get a final performance metric from training it on the initial train set and testing it on the test set?","Creater_id":125252,"Start_date":"2016-08-12 04:27:06","Question_id":229509,"Tags":["cross-validation","model-selection","out-of-sample"],"Answer_count":1,"Last_activity":"2016-08-17 16:21:09","Link":"http://stats.stackexchange.com/questions/229509/do-i-need-an-initial-train-test-split-for-nested-cross-validation","Creator_reputation":34}
{"_id":{"$oid":"5837a57ba05283111e4d4819"},"View_count":103,"Display_name":"AlanKinene","Question_score":0,"Question_content":"I am working on my thesis using decision trees. I am presenting the resulting tree to show how they help in exploring data. My issue is that since the tree is big, I want to break it down into parts, e.g. print the first 4 levels, then to go deeper. I am using the R package rpart, then plot.rpart(prp)). I tried coercing the rpart object to party, to print a subtree from a given node - but I did not manage to obtain a nice looking tree with no errors at the terminal nodes. I only want the predicted count to be visible and that the nodes don't overlap. How can I obtain a nice looking yet error free tree?library(rpart)library(rpart.plot)library(\"partykit\")reg.tree\u0026lt;-rpart(formula,data=mydata.train,method=\"anova\",cp=0.001)#AK first coerce the rpart tree to partypfit \u0026lt;- as.party(reg.tree)#AK OR Another route is to subset the subtree starting from node 83 #AK and then taking the data from that:pfit40 \u0026lt;- pfit[83]plot(pfit40, main = \"subtree from node 31\",type =  \"simple\",    tp_args = list(),    inner_panel = node_inner,     edge_panel = edge_simple, ep_args = list(),    drop_terminal = NULL,    tnex = NULL, pop = FALSE, gp = gpar(fontsize = 7))","Creater_id":90654,"Start_date":"2016-06-19 10:56:46","Question_id":219651,"Tags":["r","machine-learning","cart","partykit"],"Answer_count":1,"Last_activity":"2016-08-17 16:13:39","Link":"http://stats.stackexchange.com/questions/219651/plot-a-subtree-from-a-big-decision-tree","Creator_reputation":11}
{"_id":{"$oid":"5837a57ba05283111e4d4825"},"View_count":40,"Display_name":"Pratik","Question_score":1,"Question_content":"I am working on the Pareto model. After applying Reduced log likelihood for ECM algorithm , the MLE values of one of the parameters is negative. Can this be accepted?","Creater_id":128069,"Start_date":"2016-08-17 14:04:04","Question_id":230375,"Tags":["maximum-likelihood","pareto"],"Answer_count":1,"Last_activity":"2016-08-17 16:04:06","Link":"http://stats.stackexchange.com/questions/230375/can-the-values-of-mles-be-negative","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d4832"},"View_count":35,"Display_name":"Benjamin Lindqvist","Question_score":1,"Question_content":"Suppose we have a sequence of tuples of random variables  where  and  are correlated, where  is unknown but  is known. We get to observe  of these tuples . The problem is to optimally estimate  as a function of . Any references on the problem would be appreciated, as I don't even know which field this belongs to, let alone if the specific problem has a name.Example: Given the probability that a woman gives birth to four or more children in her lifetime ( is the indicator of this event), we want to estimate the probability that she gives birth at all before age 20 ( is indicator), where  is allowed to be some arbitrary function of income, education level, geographic location or whatever. The main point is that it is assumed to be known.EDIT: Maybe it needs to be stated explicitly that  for  is entirely possible. We don't have a sequence of observations from the same distribution, they're all from (potentially) different distributions. Given  observations of , we want to estimate  given perfect knowledge of the DISTRIBUTION of , but prior to actually observing it.","Creater_id":43059,"Start_date":"2016-08-17 13:21:19","Question_id":230366,"Tags":["machine-learning","estimation","maximum-likelihood"],"Answer_count":1,"Last_activity":"2016-08-17 15:52:56","Link":"http://stats.stackexchange.com/questions/230366/estimating-probabilities-given-the-probabily-of-correlated-event","Creator_reputation":154}
{"_id":{"$oid":"5837a57ba05283111e4d483f"},"View_count":13,"Display_name":"BrainPermafrost","Question_score":0,"Question_content":"For the following question, I thought about abstracting the data and question to drawing red, green, or blue balls from a ball producing machine, but I decided to describe the data much closer to watch I am actually using (simplified it slightly for readability, but not much)..I hope this does not bug anyone.I have forecast data for two different weather models that forecast what the temperature will be 10 days from now. The forecasts are actually categorical: cold, normal, and hot. I am interested in the distributions of the verifications when the models are both forecasting the 'hot' category.  I can calculate the distributions for the data I have. For example, I look at all the verification for when model A and model B are both forecasting 'hot'.  There are 45 data points for this combination. 30 verified hot, 10 verified normal, and 5 cold.  So, when I see this forecast again (both models 'hot') I may say there is a 67% chance of the weather verifying hot, 22% chance of normal, and 11% chance of cold (rounding to the nearest whole percentage). But, I'd like to be able to put some kind of confidence interval around these probabilities. (or credible interval - I realize which to use is another debate).As a first step, I can pretend that each verification is uncorrelated and that the 45 days are 'iid'.As a second step, can I somehow deal with the fact that the verifications are highly correlated? (i.e there may have been a series of 5 forecasts where both models were forecasting 'hot', and the verification was 'normal'.  This is really one weather event even though it is 5 days.  So the 45-day sample size, could, in reality, be a sample of just 7 to 10 events.","Creater_id":11647,"Start_date":"2016-08-17 15:46:17","Question_id":230393,"Tags":["confidence-interval","estimation","multinomial","credible-interval"],"Answer_count":0,"Last_activity":"2016-08-17 15:46:17","Link":"http://stats.stackexchange.com/questions/230393/multinomial-parameter-estimation-confidence-interval-or-credible-interval","Creator_reputation":18}
{"_id":{"$oid":"5837a57ba05283111e4d4841"},"View_count":15,"Display_name":"CFR","Question_score":1,"Question_content":"I notice that Octave Forge does support corrcoef function. However when I am using a program with this function, I get a message error: help: the 'corrcoef' function is not yet implemented in OctavePlease read `http://www.octave.org/missing.html' to learn how you cancontribute missing functionality.How do I get Octave to link to this function?","Creater_id":127591,"Start_date":"2016-08-17 15:17:27","Question_id":230390,"Tags":["function"],"Answer_count":0,"Last_activity":"2016-08-17 15:17:27","Link":"http://stats.stackexchange.com/questions/230390/corrcoef-function","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d4843"},"View_count":23,"Display_name":"Buna","Question_score":2,"Question_content":"For my research, I am using Gaussian Process Regression using 4-D inputs and 3 outputs. I have 3 separates GPs for the 3 separate outputs. I am using the Matern 3/2 covariance function. I am implementing everything in MATLAB. For real time purposes, I need the prediction to be carried out in around a millisecond (the performance requirement). I know there are a number of sparse, fast, parallel, online GP regression methods but none of them have been able to do such fast prediction. Does anybody know of any other fast prediction techniques that they have used and have had such performance ? ","Creater_id":97228,"Start_date":"2016-08-17 15:13:59","Question_id":230387,"Tags":["gaussian-process"],"Answer_count":0,"Last_activity":"2016-08-17 15:13:59","Link":"http://stats.stackexchange.com/questions/230387/fast-real-time-gaussian-process-prediction","Creator_reputation":33}
{"_id":{"$oid":"5837a57ba05283111e4d4845"},"View_count":64,"Display_name":"user122358","Question_score":3,"Question_content":"I want to use PCA before clustering, and then I want to run a clustering algorithm such as K-Means.My understanding is that I run PCA and find loadings for each original variable, then calculate scores for each record with linear combinations of row values multiplied by each PC loadings, then run clustering on the calculated PCA scores.Is it correct or do I need to do more before to run clustering on them?","Creater_id":116165,"Start_date":"2016-05-19 06:59:34","Question_id":213471,"Tags":["clustering","pca"],"Answer_count":1,"Last_activity":"2016-08-17 14:45:49","Link":"http://stats.stackexchange.com/questions/213471/if-i-use-pca-before-clustering-do-i-need-to-use-pca-scores-principal-component","Creator_reputation":148}
{"_id":{"$oid":"5837a57ba05283111e4d4852"},"View_count":54,"Display_name":"Benn","Question_score":0,"Question_content":"I am trying to construct a rule-based classifier on a dataset with 332 instances and 14 features. I am just confused how can I validate the classification model? 10-fold cross validation or holdout method should be used? Can I just apply the 10-fold cross validation for validation or the model has to be tested by a different set?","Creater_id":107584,"Start_date":"2016-08-11 14:28:54","Question_id":229431,"Tags":["cross-validation"],"Answer_count":2,"Last_activity":"2016-08-17 14:35:04","Link":"http://stats.stackexchange.com/questions/229431/10-fold-cross-validation-or-hold-out-method","Creator_reputation":24}
{"_id":{"$oid":"5837a57ba05283111e4d4860"},"View_count":59,"Display_name":"Katie","Question_score":4,"Question_content":"I am trying to predict the rainfall in a desert with a regression model. However, as you might expect, most of my training examples have zeroed labels. I have two questions:a. What is an appropriate performance measure?For classification problems, it seems conventional to evaluate the confusion matrix, F1 score or other metrics (e.g. kappa) normalized for imbalanced classes.What about in a regression setting? Any model output with near constant zero prediction will achieve a very low RMSE/MAE but doesn't give good intuition on how good my model will be ultimately at predicting the amount of rainfall.b. What is an appropriate model?It seems that one common strategy with zero-inflated data is to split this into a two-step problem with a binary classification problem for {rain, no rain}, pick my favorite classifier from cross-validation, then split my data set with that classifier to run a separate regression problem conditional on predicted rain.The main concern I have with this approach is that I have limited data by the regression step (there's very few training examples conditional on predicted rain).Is there a better approach I can take?","Creater_id":128040,"Start_date":"2016-08-17 10:38:48","Question_id":230334,"Tags":["regression","classification","unbalanced-classes","zero-inflation","model-evaluation"],"Answer_count":1,"Last_activity":"2016-08-17 14:24:47","Link":"http://stats.stackexchange.com/questions/230334/dealing-with-imbalanced-zero-inflated-training-examples-for-regression","Creator_reputation":21}
{"_id":{"$oid":"5837a57ba05283111e4d486d"},"View_count":19,"Display_name":"Charlie Glez","Question_score":1,"Question_content":"I'm looking for some advice as to how to best show graphically the results of my GLMER model. In my model I try to explain Y (a count variable for a business related outcome), in terms of X, Xsquared, and FirmSize, with random effects for Industry/Firm, by year. My results, here below, appear to show a positive relationship between Y and X, but with an inverted-U shape (the Xsquared term is negative and significant). Ideally I would like to obtain a graph showing this inverted-U relationship, but I'm clueless as to how to move forward (I'm relatively new to R and GLMER models).  My GLMER model:model \u0026lt;- glmer(Y ~ Year + X + Xsquared + Size + (1 + Year|Industry/Firm), data = mydata, family = poisson)My results:Random effects: Groups        Name        Variance  Std.Dev. Corr Firm:Industry (Intercept) 5.840e-01 0.764205                    cYear       5.546e-03 0.074474 0.38 Industry      (Intercept) 8.243e-05 0.009079                    cYear       2.011e-05 0.004485 0.54Number of obs: 436, groups:  Firm:Industry, 109; Industry, 37Fixed effects:            Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept) -2.87753    0.47970  -5.999 1.99e-09 ***cYear       -0.02303    0.02778  -0.829    0.407    X            1.32358    0.33632   3.935 8.30e-05 ***Xsquared    -0.27628    0.14217  -1.943    0.049 *  FirmSize     0.31789    0.04989   6.371 1.87e-10 ***","Creater_id":41784,"Start_date":"2016-08-17 14:13:23","Question_id":230378,"Tags":["r","poisson-regression","glmer"],"Answer_count":0,"Last_activity":"2016-08-17 14:13:23","Link":"http://stats.stackexchange.com/questions/230378/how-to-graph-results-of-a-glmer-model-with-squared-term","Creator_reputation":101}
{"_id":{"$oid":"5837a57ba05283111e4d486f"},"View_count":28,"Display_name":"MrMonkeyBum","Question_score":1,"Question_content":"I was wondering if someone on here could helpI recently ran a Spatial Durbin Regression model in R which came back with two of my three independent variables had significant beta coefficients. A colleague then advised me that I should run a sensitivity test using a negative binomial model to see if I get the same results. However the results are different as all my beta coefficients become significant.What I am trying to understand is would this likely be due to the incorporation of the spatially lagged independent variable (which has a significant rho value in the spatial Durbin regression)? And does it make sense to use the negative binomial as a 'sensitivity test' as I would think the assumptions would be different, particularly around spatial autocorrelation. ","Creater_id":128067,"Start_date":"2016-08-17 13:57:21","Question_id":230372,"Tags":["r","regression","spatial","negative-binomial"],"Answer_count":0,"Last_activity":"2016-08-17 13:57:21","Link":"http://stats.stackexchange.com/questions/230372/can-a-negative-binomial-model-be-used-as-a-sensitivity-test-for-spatial-regressi","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d4871"},"View_count":52,"Display_name":"M. Beausoleil","Question_score":1,"Question_content":"I'm running a Bayesian model and I'm stuck on some aspect of the model that I have difficulty to understand. Since my knowledge about Bayesian inference is limited, I would like to have some good references to understand Bayesian statistics. People are teaching Bayesian statistics could be interested in how to think in bayesian versus frequentist statistics. But I'm interested in finding examples of model building and how should we create, from scratch new models and diagnose them. A good reference is this book, but I need something more practical.","Creater_id":93498,"Start_date":"2016-08-17 13:26:01","Question_id":230368,"Tags":["bayesian","hierarchical-bayesian"],"Answer_count":1,"Last_activity":"2016-08-17 13:41:15","Link":"http://stats.stackexchange.com/questions/230368/how-did-you-learn-bayesian-statistics-and-what-would-you-recommend-as-a-reliable","Creator_reputation":272}
{"_id":{"$oid":"5837a57ba05283111e4d487e"},"View_count":34,"Display_name":"Nik Bernou","Question_score":1,"Question_content":"I have a data looks like this ( I used log2 to normalise my data) zeros means missing values. is there a way to impute it which does not interrupt statistical test ?df\u0026lt;- structure(list(Group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Subject = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 15L, 15L, 15L),     Value = c(29.89577946, 29.51885854, 29.77429604, 33.20695108,     32.09027292, 31.90909894, 30.88358173, 30.67547731, 30.82494595,     31.70128247, 31.57217504, 31.61359752, 30.51371055, 30.42241945,     30.44913954, 26.90850496, 0, 0, 0, 0, 0, 28.94047335, 29.27188604,     29.78511206, 28.18475423, 27.54266717, 26.99873401, 29.26941344,     28.50457189, 28.78050443, 31.39038527, 31.19237052, 30.74053275,     28.68618888, 28.42109545, 28.58222544, 28.99337177, 29.31797,     28.4541501, 28.18475423, 27.54266717, 26.99873401, 28.07576794,     28.96344894, 28.48358437, 27.02527663, 27.1308483, 26.96091103,     27.04019758, 27.51900858, 28.14559621, 26.83569136, 26.90724462,     26.82675, 0, 0, 0, 27.62449786, 26.82335228, 26.66925534,     0, 25.81254792, 26.61666776, 26.12545858, 0, 0, 0, 0, 0,     28.84580419, 29.11003424, 29.24723895, 28.72919768, 29.70673437,     29.31274377, 30.73133587, 30.44805655, 30.61561583, 27.06896964,     27.04249553, 27.15990629, 31.54738209, 31.51643714, 31.8055509,     31.291867, 31.89146186, 31.65812735)), .Names = c(\"Group\", \"Subject\", \"Value\"), class = \"data.frame\", row.names = c(NA, -87L))","Creater_id":127951,"Start_date":"2016-08-17 13:27:18","Question_id":230369,"Tags":["r","statistical-significance","missing-data"],"Answer_count":1,"Last_activity":"2016-08-17 13:40:57","Link":"http://stats.stackexchange.com/questions/230369/imputation-of-missing-values","Creator_reputation":16}
{"_id":{"$oid":"5837a57ba05283111e4d488a"},"View_count":408,"Display_name":"RonRich","Question_score":6,"Question_content":"I am aware of extreme value theory for continuous distributions. I need to fit an extreme value distribution to the maximum observation of number of events on a day, per month. This seems to be the block maxima problem, which is approximated by the GEV family of distributions for continuous distributions.  How do I do this for count data? As a secondary question, let's assume the basic count process is ~ Poisson. Then does this lead to a different answer to the original question?Thanks!","Creater_id":31831,"Start_date":"2013-10-23 04:35:49","Question_id":73563,"Tags":["poisson","extreme-value"],"Answer_count":2,"Last_activity":"2016-08-17 13:23:44","Link":"http://stats.stackexchange.com/questions/73563/extreme-value-theory-for-count-data","Creator_reputation":38}
{"_id":{"$oid":"5837a57ba05283111e4d4898"},"View_count":35,"Display_name":"user128020","Question_score":4,"Question_content":"I have done simulations with different sample sizes of a normal distribution (mean=0, sd=10) and plotted the number of samples against statistical parameters.I understand that the relation of sample size and mean, sd  and the relative frequency of samples ouside of 2*sd is due to the law of great numbers. (Variance reduces with sample size).I do not understand the relation of sample size and variable range. Is there an intuitive explanation for this relation? ","Creater_id":128020,"Start_date":"2016-08-17 08:36:50","Question_id":230302,"Tags":["sample-size","range"],"Answer_count":1,"Last_activity":"2016-08-17 13:08:36","Link":"http://stats.stackexchange.com/questions/230302/is-there-a-relation-between-sample-size-and-variable-range","Creator_reputation":23}
{"_id":{"$oid":"5837a57ba05283111e4d48a5"},"View_count":24465,"Display_name":"user9203","Question_score":20,"Question_content":"I am wondering what the differences are between mixed and unmixed GLMs. For instance, in SPSS the drop down menu allows users to fit either:analyze-\u0026gt; generalized linear models-\u0026gt; generalized linear models \u0026amp;analyze-\u0026gt; mixed models-\u0026gt; generalized linear  Do they deal with missing values differently?  My dependent variable is binary and I have several categorical and continuous independent variables.","Creater_id":9203,"Start_date":"2012-07-16 16:47:32","Question_id":32419,"Tags":["mixed-model","generalized-linear-model","glmm","gee"],"Answer_count":3,"Last_activity":"2016-08-17 12:51:51","Link":"http://stats.stackexchange.com/questions/32419/difference-between-generalized-linear-models-generalized-linear-mixed-models","Creator_reputation":199}
{"_id":{"$oid":"5837a57ba05283111e4d48b4"},"View_count":126,"Display_name":"mwarrior","Question_score":3,"Question_content":"Suppose that someone gives you a coin with some unknown weighting (maybe it's a fair coin, or maybe it's just 25% likely to get heads, or etc). How many times would you have to flip the coin to determine, within some confidence, the weighting of the coin (the probability the coin will get heads in general)?Edit:To clarify, I'm looking to see how the coin is loaded/weighted: for example, an \"unloaded\" coin would be weighted such that the probability of heads is 50%, whereas a \"loaded\" coin may have the heads probability at 70%, 90%, or etc. I want to know when I can stop flipping the coin with some level of confidence that the probability of heads is x% (where x% is calculated using the data, not assumed prior to having data).Edit: To clarify further, I'll give an example: suppose I have some system that outputs a 0 or a 1 after each trial. After 5 trials, I end up with {01111}. So the two questions it raises are a) how do I find the probability that the next result is a 1, given only my 5 previous trials and b) how do I find the confidence of the calculation performed in a (based on the answers given so far, I'm guessing I can use a confidence as a stopping point, ie once a confidence level of 80% is reached I can stop doing more trials)?Thank you in advance for your help.","Creater_id":127758,"Start_date":"2016-08-15 11:03:46","Question_id":229966,"Tags":["confidence-interval","sample-size","sample"],"Answer_count":2,"Last_activity":"2016-08-17 12:48:14","Link":"http://stats.stackexchange.com/questions/229966/sample-size-calculation-for-obtaining-coin-fairness","Creator_reputation":16}
{"_id":{"$oid":"5837a57ba05283111e4d48c2"},"View_count":582,"Display_name":"FihopZz","Question_score":3,"Question_content":"An interview problem is like the following:  Given a coin you don’t know it’s fair or unfair. Throw it 6 times and   get 1 tail and 5 head. Determine whether it’s fair or not. What’s your  confidence value?I came out the following solution:   the coin is fair   the coin is unfair    : is the number of heads    Rejection region: , i.e.,     Significance level alpha:               because , we do not have enough evidence to reject , and  we  accept , so the coin is fairIs the above test a good one? And I did not know how to calculate the confidence value?","Creater_id":45339,"Start_date":"2015-09-07 07:35:03","Question_id":171451,"Tags":["hypothesis-testing","self-study"],"Answer_count":2,"Last_activity":"2016-08-17 12:47:36","Link":"http://stats.stackexchange.com/questions/171451/check-whether-a-coin-is-fair","Creator_reputation":278}
{"_id":{"$oid":"5837a57ba05283111e4d48d0"},"View_count":10,"Display_name":"statsguy","Question_score":1,"Question_content":"I am fitting a model with a dichotomous outcome, large number of subjects measured at differing visit (visit 1-14) with four other categorical and cont outcome variable....the main question is if the probability of infection varies in one variable of interest....thoughts on model selection GLMM vs GEE?","Creater_id":128060,"Start_date":"2016-08-17 12:41:04","Question_id":230356,"Tags":["glmm","gee"],"Answer_count":0,"Last_activity":"2016-08-17 12:41:04","Link":"http://stats.stackexchange.com/questions/230356/gee-vs-glmm-in-this-example","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d48d2"},"View_count":46,"Display_name":"Marcus Nunes","Question_score":1,"Question_content":"Some people I know conducted a survey. One of their goals is to estimate the hourly salary of a group of people. There are two questions in the survey that, supposedly, could answer this question:What is you weekly salary?How many hours do you work per week?Well, this is easy, right? I have the salary per week and the number of worked hours. Hence, my first thought was to divide the salary by the number of hours.However, the salary is not a quantitative variable. It is an ordinal variable. For example, suppose there are five categories and they areless than \\101-\\201-\\301-\\600 Yes, the categories have unequal ranges. How can I convert these data do quantitative? I know a lot of information was lost, but there is a way to estimate some of the salaries, at least? I've been searching the internet for an answer without luck.","Creater_id":12945,"Start_date":"2016-08-17 12:00:28","Question_id":230348,"Tags":["estimation","data-transformation","ordinal","interpolation"],"Answer_count":1,"Last_activity":"2016-08-17 12:40:02","Link":"http://stats.stackexchange.com/questions/230348/convert-ordinal-data-to-continuous-data","Creator_reputation":108}
{"_id":{"$oid":"5837a57ba05283111e4d48de"},"View_count":95,"Display_name":"Daniel V","Question_score":6,"Question_content":"I have data that is believed to be Rayleigh distributed (according to some academic papers). However, when I plot the histogram (probability normalized below) it looks like a Rayleigh distribution with an offset. Is there a way to \"standardize\" such a random variable so I can fit a Rayleigh parameter to it? I am thinking of when we take Normal RV's and subtract the mean and divide by the variance to make it standard (zero mean, unit variance).If that doesn't sound like a correct thing to do, is there a similar distribution that has a parameter for this kind of shift?","Creater_id":109667,"Start_date":"2016-07-18 16:02:05","Question_id":224416,"Tags":["fitting","rayleigh"],"Answer_count":1,"Last_activity":"2016-08-17 12:21:30","Link":"http://stats.stackexchange.com/questions/224416/parameter-estimation-of-a-rayleigh-random-variable-with-an-offset","Creator_reputation":110}
{"_id":{"$oid":"5837a57ba05283111e4d48eb"},"View_count":18,"Display_name":"nk379","Question_score":1,"Question_content":"I have to assign marketing budget to different marketing programs across regions for the next fiscal year. For this, I want to assign weightages to different parameters that might affect the outcome of spending. The data I have is as follows:- last 2 years outcome of some (not all) marketing programs (at program level, not available at customer level). Also historically, there is high variance in outcome of programs (for whom data is available) from one period to another.- last 3 years overall revenues and market share- share of wallet of purchasers- stage in life cycle of purchasers (new, existing, etc.)- last 3 years market size and projected FY18 market size  How do I use all this data to come up with marketing spending allocation?Coming up with subjective numbers based on intuition/experience is not an option.I know different businesses use different approaches for marketing allocation, but I want some concrete approach best suited for this case.","Creater_id":128049,"Start_date":"2016-08-17 12:03:53","Question_id":230350,"Tags":["time-series","optimization","model","marketing"],"Answer_count":0,"Last_activity":"2016-08-17 12:03:53","Link":"http://stats.stackexchange.com/questions/230350/could-someone-help-me-with-approaches-for-marketing-expenditure-allocation","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d48ed"},"View_count":11,"Display_name":"ihadanny","Question_score":0,"Question_content":"related to Fisher\u0026#39;s score function has mean zero - what does that even mean? I'm trying to follow eric zivot's maximum likelihood estimation course. I'm looking at page 12, where they compute the information for an observation: I(\\pi|x_i) = var(u(\\pi|x_i)) = var(\\frac{x_i}{\\pi} - \\frac{1-x_i}{1-\\pi}) = var(\\frac{x_i-\\pi}{\\pi(1-\\pi)}) = \\frac{var(x_i)}{\\pi^2(1-\\pi)^2} Which I agree with. BUT then they continue and say: \\frac{var(x_i)}{\\pi^2(1-\\pi)^2}=  \\frac{\\pi(1-\\pi)}{\\pi^2(1-\\pi)^2} = \\frac{1}{1-\\pi}   I think this is wrong! I think that the Bernoulli model says that there's a true parameter  from which the observations are sampled, and the information matrix is:   \\frac{var(x_i)}{\\pi^2(1-\\pi)^2}=  \\frac{\\pi_0(1-\\pi_0)}{\\pi^2(1-\\pi)^2}   ONLY when evaluated at the true parameter the information evaluates to:.. the same as explained here about the expectancy of the score, only this time its variance instead of expectancy.Am I correct? or am I missing something? ","Creater_id":37793,"Start_date":"2016-08-13 09:21:09","Question_id":229679,"Tags":["likelihood","bernoulli-distribution","fisher-information"],"Answer_count":1,"Last_activity":"2016-08-17 11:56:10","Link":"http://stats.stackexchange.com/questions/229679/bernoulli-model-information-matrix-evaluated-not-at-the-real-parameter-value","Creator_reputation":280}
{"_id":{"$oid":"5837a57ba05283111e4d48fa"},"View_count":40,"Display_name":"tintinthong","Question_score":2,"Question_content":"I know that we can use different coding scheme for a factorial experiment. By changing the coding inside your linear model, you change the hypothesis of each coefficient. eg for dummy coding you will test difference of level with base line and for a different coding the coefficient estimates would be different. Why does the sums of squares not change when changing the coding? A deeper question is, if your design matrix is non-orthogonal how can you get a unique decomposition of sums of square?I have a good feeling both these questions are strongly related. Let me write an R example to give some context. #set-upfactor\u0026lt;-gl(4,4) response\u0026lt;-rnorm(16)#fitting model 1model1\u0026lt;-lm(response~factor) #using contr.treatment (default)M1\u0026lt;-model.matrix(model1) #model matrix#fitting model 2contrasts(factor)=contr.sum(4) #changing contrasts to contr.summodel2\u0026lt;-lm(response~factor) #using contr.sum M2\u0026lt;-model.matrix(model2) #model matrix#anova for both modelsanova(model1)\u0026gt; Response: response          Df  Sum Sq Mean Sq F value Pr(\u0026gt;F)factor     3  1.1735 0.39118  0.1891 0.9018Residuals 12 24.8234 2.06861     anova(model2)\u0026gt; Response: response          Df  Sum Sq Mean Sq F value Pr(\u0026gt;F)factor     3  1.1735 0.39118  0.1891 0.9018Residuals 12 24.8234 2.06861     #model matrix for both models\u0026gt; M1   (Intercept) factor2 factor3 factor41            1       0       0       02            1       0       0       03            1       0       0       04            1       0       0       05            1       1       0       06            1       1       0       07            1       1       0       08            1       1       0       09            1       0       1       010           1       0       1       011           1       0       1       012           1       0       1       013           1       0       0       114           1       0       0       115           1       0       0       116           1       0       0       1\u0026gt; M2   (Intercept) factor1 factor2 factor31            1       1       0       02            1       1       0       03            1       1       0       04            1       1       0       05            1       0       1       06            1       0       1       07            1       0       1       08            1       0       1       09            1       0       0       110           1       0       0       111           1       0       0       112           1       0       0       113           1      -1      -1      -114           1      -1      -1      -115           1      -1      -1      -116           1      -1      -1      -1As you can see, the M2, which is the model matrix with contr.sum contrasts, has orthogonal columns ,whereas, M1,which is the model matrix with contr.treatment contrasts, has non-orthogonal columns, yet the anova table is still identical. M1 is non-orthogonal for example (Intercept). factor2= 4. ","Creater_id":121671,"Start_date":"2016-08-17 08:05:19","Question_id":230296,"Tags":["regression","anova","contrasts"],"Answer_count":0,"Last_activity":"2016-08-17 11:55:00","Link":"http://stats.stackexchange.com/questions/230296/why-different-coding-gives-the-same-sums-of-square","Creator_reputation":117}
{"_id":{"$oid":"5837a57ba05283111e4d48fc"},"View_count":71,"Display_name":"PiCubed","Question_score":0,"Question_content":"In a group of 5 students, 2 are males and 3 are females. Two students arerandomly selected (without replacement). Let X be the number of males in the two selectedstudents.(a) Find the (probability) distribution of X (i.e., list all possible values of X and their correspondingprobabilities).(b) Find the expected value of X, and the standard deviation of X.let n = 2 = number of trialsP = 2/5 = probability of successQ = 3/5 = probability of failurek = number of successFor a) I used the equation nCkP^kQ^(n-k), and gotP(X=0)= 0.36P(X=1)= 0.48P(X=2)= 0.16but the solution key tells me thatP(X=0)= 0.3P(X=1)= 0.6P(X=2)= 0.1And for b)E(X)= np = 0.8SD = sqrt(npq) = 0.48but the solution key tells me that E(X) = 0.8SD = 0.6And now i'm confused... I'm not sure if I should approach this question using the binomial probability or is it completely unrelated to binomial distribution.Please give me a hint on how to approach this question, thank you.","Creater_id":127612,"Start_date":"2016-08-13 17:12:10","Question_id":229718,"Tags":["probability","self-study","distributions","binomial","expected-value"],"Answer_count":1,"Last_activity":"2016-08-17 11:52:04","Link":"http://stats.stackexchange.com/questions/229718/how-to-find-a-probability-of-a-sample-without-replacement","Creator_reputation":3}
{"_id":{"$oid":"5837a57ba05283111e4d4909"},"View_count":836,"Display_name":"Kroll DU","Question_score":5,"Question_content":"I'm learning PCA in R language. I met two problems right now that I don't understand.I am performing a PCA analysis in R on a 318×17 dataset using some custom code.  I take eigen function in R  to find eigenvalues and eigenvectors.  But my 1st and 3rd eigenvectors are of the opposite sign to my handbook.  My second eigenvectors is almost the same.I know that given a square matrix A, the condition that characterizes an eigenvalue, , is the existence of a nonzero vector  such that ; this equation can be rewritten as follows: .Now I calculate covariance of my data and have eigenvalues. I want to solve this linear combination equation to find  and compare with initial eigenvectors. When I take solve function in R, my  vector is always zero.Here are my questions: Why the sign is different? How to use solve function in R  to find a non-zero vector ?","Creater_id":78021,"Start_date":"2015-05-30 03:32:18","Question_id":154716,"Tags":["r","pca","eigenvalues"],"Answer_count":1,"Last_activity":"2016-08-17 11:30:17","Link":"http://stats.stackexchange.com/questions/154716/pca-eigenvectors-of-opposite-sign-and-not-being-able-to-compute-eigenvectors-wi","Creator_reputation":33}
{"_id":{"$oid":"5837a57ba05283111e4d4916"},"View_count":36,"Display_name":"wwl","Question_score":1,"Question_content":"I ran an experiment with a treatment group and a control group. All subjects were asked to choose an integer between 1 and 80 (which was the variable of interest), to indicate the amount of money they were willing to pay for an object. There were a total of 180 subjects, so some integers were not chosen.I want to see whether the distributions were different in the treatment group compared to the control group. Which test is better?1) The Kolmogorov-Smirnov test (which is strictly speaking for continuous data)2) Contingency tables with Fisher's exact test? ","Creater_id":79671,"Start_date":"2016-08-16 10:16:17","Question_id":230140,"Tags":["kolmogorov-smirnov","contingency-tables","fishers-exact"],"Answer_count":1,"Last_activity":"2016-08-17 11:25:09","Link":"http://stats.stackexchange.com/questions/230140/contingency-tables-vs-kolmogorov-smirnov","Creator_reputation":111}
{"_id":{"$oid":"5837a57ba05283111e4d4923"},"View_count":23,"Display_name":"user22108","Question_score":1,"Question_content":"How can I find the conditional distribution of a stochastic process to another stochastic process?Given  and  and  and  are zero mean independent terms with variance resepctively of  and . I need to find the conditional distribution of  but I don't know how to start. I started from the mean...so far I have written only  but I don't think it is the right way to proceed. Can anyone give me a hint on how to set up the problem?","Creater_id":128011,"Start_date":"2016-08-17 10:30:34","Question_id":230333,"Tags":["time-series","self-study","stochastic-processes","conditional-expectation"],"Answer_count":0,"Last_activity":"2016-08-17 11:17:49","Link":"http://stats.stackexchange.com/questions/230333/exercise-on-conditional-distribution-of-a-stochastic-process-to-another-process","Creator_reputation":16}
{"_id":{"$oid":"5837a57ba05283111e4d4925"},"View_count":66,"Display_name":"Joe","Question_score":1,"Question_content":"I have a basic probability doubt. If I have 5 different vehicles backstage, out of which 3  random can be showcased to the public. What is the probability of each vehicle to get to showcase? Once a car is in a showcase, it will not be returned to backstage.I thought by doing the calculating the combinations- .And probability of 3 slots getting filled (let's say ) - .So, total probability is .Or will it be ?Or any other solution?","Creater_id":119431,"Start_date":"2016-07-01 11:35:34","Question_id":221728,"Tags":["probability","combinatorics"],"Answer_count":2,"Last_activity":"2016-08-17 11:00:00","Link":"http://stats.stackexchange.com/questions/221728/3-vehicles-out-of-5-randomly-showcased-what-is-the-probability-of-each-vehicle","Creator_reputation":10}
{"_id":{"$oid":"5837a57ba05283111e4d4933"},"View_count":30,"Display_name":"Zack Newsham","Question_score":1,"Question_content":"Some background:I'm trying to build a Bayesian inference model for the probability that an event has occurred based on observed data. As of now I am assuming the data is normally distributed (will check later, it will likely be Poisson).I'm struggling to calculate values for P(E | H). The hypothesis being tested (H) is that the event has occurred. Of the observations I am gathering, I have some known values of what to expect when the event hasn't occurred, but no values for when the event has occurred.This question has four parts:1) Is there a generic method of returning the probability that a single observation (or perhaps a few, but not many) came from a known distribution. If there isn't a general method, is there one for normal and Poisson distributions?2) Given a Bayesian inference model, is some form of 1-p where p is the probability as described above a good estimate of P(E | H), i.e., the probability that the data is not from the \"good\" distribution is 1- the probability that the event has occurred.3) Many of the observations won't be different from the \"good\" distribution, but some of them will be enormously different, unfortunately some of the enormously different ones will be spurious, and some will be because the event has occurred.4) Am I just doing this wrong, e.g., does a Bayesian model not work for this type of data, is there a better way?If possible, code examples in R would be appreciated.","Creater_id":24169,"Start_date":"2016-08-17 10:55:32","Question_id":230338,"Tags":["probability","bayesian","predictive-models"],"Answer_count":0,"Last_activity":"2016-08-17 10:55:32","Link":"http://stats.stackexchange.com/questions/230338/determine-the-probability-that-a-value-came-from-a-distribution","Creator_reputation":265}
{"_id":{"$oid":"5837a57ba05283111e4d4935"},"View_count":27,"Display_name":"Carlos Stein","Question_score":2,"Question_content":"How can I show that the fractional moments of the (unit variance) Laplacian distribution are higher than of the standard normal distribution, for moments higher than 2? Formally, if  and , show that, for ,.","Creater_id":127977,"Start_date":"2016-08-17 05:09:30","Question_id":230256,"Tags":["moments","laplace-distribution","heavy-tailed"],"Answer_count":1,"Last_activity":"2016-08-17 10:47:44","Link":"http://stats.stackexchange.com/questions/230256/fractional-moments-of-the-laplacian-distribution-larger-than-of-the-normal","Creator_reputation":41}
{"_id":{"$oid":"5837a57ba05283111e4d4941"},"View_count":165,"Display_name":"Vinicius","Question_score":1,"Question_content":"I am trying to adjust a set of raw p-values with FDR method (BH). One of the assumptions in BH method is test independence. I know that some tests are clones and another ones are testing a very similar event.Consequently, FDR correction is biased. I applied the simpleM method (described by Gao et. al here)and estimated my effective number of independent tests, M(eff), to be equal to 1096, while my total number of tests was 3243.The simpleM gave me the M(eff), however, I don´t know which tests have to be considered and carried to FDR correction and subsequent analysis.Any ideas to subset all my tests in a representative group? ","Creater_id":76560,"Start_date":"2015-05-15 11:07:26","Question_id":152538,"Tags":["r","multiple-comparisons","false-discovery-rate"],"Answer_count":0,"Last_activity":"2016-08-17 10:43:51","Link":"http://stats.stackexchange.com/questions/152538/effective-number-of-independent-tests-meff-to-fdr-adjusted-p-values","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d4943"},"View_count":18,"Display_name":"Steve","Question_score":1,"Question_content":"I am creating an item selection system that allows a group of people to vote on their favorite item by ranking it between 1 (no good / least favorite) to 5 (very good/favorite). I received a bunch of votes and now I need to select one. One option is to sum their scores from all votes or, perhaps better, sum the square of their votes to favor items with very high votes.One issue I've found is that some items have very high votes AND very low votes --- they are quite controversial! I'm thinking to prefer items that are highly ranked AND non-controversial. To this end, I thought I would penalize controversial items by dividing by (1+variance(item_votes) so score for each item isscore(item) = sum_squared(item_votes)/(1+variance(item_votes)Is there a superior, standard way to prefer uncontroversial items?","Creater_id":128038,"Start_date":"2016-08-17 10:40:21","Question_id":230335,"Tags":["voting-system"],"Answer_count":0,"Last_activity":"2016-08-17 10:40:21","Link":"http://stats.stackexchange.com/questions/230335/scoring-a-item-based-on-votes","Creator_reputation":106}
{"_id":{"$oid":"5837a57ba05283111e4d4945"},"View_count":1021,"Display_name":"aL3xa","Question_score":5,"Question_content":"This definitely sounds like a homework, but I assure you that it's not. You're probably familiar with the Risk game. Now, friend of mine rolled 3 aces in one hand. I reckon that probability of such event isC(n,k) = \\frac{(n-1+k)!}{(n-1)!k!}so that's , so the probability is . Am I correct?Problem starts here: he rolled 3 aces in his 2nd attack, so he asked me: now, tell me 'bout the odds, you do statistics! And I must admit that I'm still stunned by his tremendous attacks (he lost 3 tanks both times).Is the probability of such two consecutive events ?","Creater_id":1356,"Start_date":"2010-08-08 17:00:13","Question_id":1424,"Tags":["probability","games","dice"],"Answer_count":1,"Last_activity":"2016-08-17 10:37:49","Link":"http://stats.stackexchange.com/questions/1424/the-risk-game-dice-problem","Creator_reputation":1140}
{"_id":{"$oid":"5837a57ba05283111e4d4952"},"View_count":46,"Display_name":"hxd1011","Question_score":1,"Question_content":"I have a simple code to compute the logistic loss but have problem with NaN (not a number). x=as.matrix(scale(mtcars[,names(mtcars) %in% c(\"wt\", \"mpg\")]))y=mtcarsw^Tx1.0\\log(0)\\sin(x)/xx=01$? How about some functions that have a lot of such points? I may not possible to manually do all conditions and the code will be a mess.","Creater_id":113777,"Start_date":"2016-08-17 09:27:55","Question_id":230318,"Tags":["r","machine-learning","logistic","optimization","computational-statistics"],"Answer_count":0,"Last_activity":"2016-08-17 10:35:11","Link":"http://stats.stackexchange.com/questions/230318/how-to-deal-with-not-a-number-nan-in-logistic-loss","Creator_reputation":4423}
{"_id":{"$oid":"5837a57ba05283111e4d4954"},"View_count":39,"Display_name":"Naveenan","Question_score":1,"Question_content":"I have continuous outcome variable and continuous independent variable. I am trying to bin the independent variable that maximizes homogeneity within bins based on the outcome and maximize separation. Is there any technique that would help this ?     ","Creater_id":76660,"Start_date":"2016-08-17 10:16:51","Question_id":230332,"Tags":["feature-construction","binning","segmentation"],"Answer_count":0,"Last_activity":"2016-08-17 10:16:51","Link":"http://stats.stackexchange.com/questions/230332/supervised-binning","Creator_reputation":18}
{"_id":{"$oid":"5837a57ba05283111e4d4956"},"View_count":563,"Display_name":"Vyasraj Vaidya","Question_score":0,"Question_content":"I am new here, and I have found only this and this to be useful. However, I still have some queries to make after doing the following things:Extraction of the features from the Audio files.Scaled the features. (I also normalized just to compare which was better for my data set.)Used the Imputer for any missing data. Then use the SVM to classify the data. I find that the classification is average to say the least. The precision and recall are about 50%.I have also tried using the decision trees classifiers and also the k-neighbours technique, but none seems to improve over the SVM. Research papers suggest that using the SVM should yield values of precision close to 90%. I do not know why the values are so low! Maybe I am missing something. Can someone tell me if there is a general procedure to follow while performing Audio/Acoustic Classification ? Any suggestions in the right direction will be helpful. Thanks! Edit : Since the comment section takes only limited characters : here is some more information - I have to classify the given sounds into environmental classes - street music or siren etc. I think you can get the picture here. I have extracted the information : the feature matrices from the sound files. Extract meaning I get the MFCC, the Spectral Flatness Measure and several other features (in total 10) that are needed to classify a signal. I push them into a numpy array using python. Several of the features are multi-dimensional like the MFCC which has a min of 15 dimensions.Each of the sounds are stored in a folder with a specific name : When I read the sound file to extract the information/featureMatrix, I assign a target/classID to each of the sounds - stored as target in a numpy array. Then I perform some pre-processing to remove the empty values and normalize the data before I feed the feature matrix into the SVM. My intention is to use these features and classify the sounds into a specific class. The classifier must read a feature as input one. The other input for the classifier will be the identification number I assigned while reading it from the directory of sound files. The SVM was able to classify the sounds but not with high precision which was max of 49% !  Please tell me if this was enough. Thank you. ","Creater_id":82269,"Start_date":"2015-09-14 23:45:14","Question_id":172556,"Tags":["machine-learning","svm"],"Answer_count":1,"Last_activity":"2016-08-17 10:11:40","Link":"http://stats.stackexchange.com/questions/172556/using-the-svm-for-sound-classification","Creator_reputation":1}
{"_id":{"$oid":"5837a57ba05283111e4d4963"},"View_count":15,"Display_name":"StatsGuesT","Question_score":1,"Question_content":"I'm trying to duplicate a logistic model put forth in this paper: newyorkfed.org/research/capital_markets/ycfaq.html and looks like this:I have performed logistic regression using www.newyorkfed.org/medialibrary/media/research/capital_markets/allmonth.xls, where my model is ~  in python and have this output: But these coefficients aren't near the results in the paper. Is there a reason why? Am I misinterpreting the analysis, or is there a step I've missed in the regression?","Creater_id":128035,"Start_date":"2016-08-17 10:09:43","Question_id":230330,"Tags":["logistic","logit"],"Answer_count":0,"Last_activity":"2016-08-17 10:09:43","Link":"http://stats.stackexchange.com/questions/230330/logistic-regression-yield-curve-predicting-recessions","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d4965"},"View_count":38,"Display_name":"RustyStatistician","Question_score":2,"Question_content":"I am solving optimization problems where I am trying to find the minimum of a function over some sample space , i.e., . Now the optimization algorithm I am using is based on trial points  which are sampled from .  For sake of argument, let's say  is the unit interval. Now I have been solving some problems where the solution lies along the boundary, i.e.,  or  could be the solution to the minimization problem.  Now, the way I have been picking my potential solutions (trial points) is to sample  from a Uniform(0,1) distribution.No, what my question really is, is whether or not I will ever sample 0 or 1 from that Uniform distribution.  From a practical point of view I don't think it will occur, however, from a theoretical point of view I am also not sure.  Because isn't the probability of sample any one single number from a continuous distribution equal to exactly 0? Or is there some positive probability that I will sample the endpoints of the interval?However, running some R code sampling from a Uniform(0,1) distribution 100,000,000 times I am able to sample 1, but not 0 (well maybe in machine precision it is?)\u0026gt; x = runif(100000000)\u0026gt; min(x)[1] 2.142042e-08\u0026gt; max(x)[1] 1\u0026gt;   ","Creater_id":98920,"Start_date":"2016-08-17 09:26:31","Question_id":230317,"Tags":["sampling","uniform"],"Answer_count":1,"Last_activity":"2016-08-17 10:03:46","Link":"http://stats.stackexchange.com/questions/230317/is-it-possible-practically-to-sample-any-point-on-0-to-1-under-uniform-samplin","Creator_reputation":126}
{"_id":{"$oid":"5837a57ba05283111e4d4972"},"View_count":39,"Display_name":"Carsten Stobwasser","Question_score":1,"Question_content":"In a particular SEM Model after running, I cannot view the path coefficients in the graphical representation. The button seems to be disabled. This has not happened in other models. Any ideas?  Cannot seem to find a work-around. ","Creater_id":128034,"Start_date":"2016-08-17 10:02:04","Question_id":230326,"Tags":["spss","sem","confirmatory-factor","amos","path-model"],"Answer_count":0,"Last_activity":"2016-08-17 10:03:34","Link":"http://stats.stackexchange.com/questions/230326/why-does-spss-amos-not-show-path-coefficients","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d4974"},"View_count":123,"Display_name":"WojciechF","Question_score":1,"Question_content":"I want to recreate a regression model based on what was given in a scientific paper. They gave intercept and coefficient terms. I know how to create regression models in R, but is this possible to do without the original database? I would use these models on my own database to perform model comparison and test their predictive capabilities. The special case here is that I am mostly interested in logistic regression. But I guess this question is scalable to all types of regression models. So in other words: how can we create regression model objects (e.g. glm) using only beta values. ","Creater_id":55722,"Start_date":"2016-08-17 01:33:04","Question_id":230218,"Tags":["regression","multiple-regression","generalized-linear-model","modeling"],"Answer_count":1,"Last_activity":"2016-08-17 10:01:19","Link":"http://stats.stackexchange.com/questions/230218/how-to-create-a-regression-model-object-from-intercept-and-coefficients-values-o","Creator_reputation":88}
{"_id":{"$oid":"5837a57ba05283111e4d4980"},"View_count":62,"Display_name":"Blain Waan","Question_score":2,"Question_content":"I wanted to generate two negatively correlated exponentially distributed variables (remission time  and survival time , for example) with the constraint that . I wanted that the mean of  be around  and the mean of  be around . I tried the following codes:rho   \u0026lt;- -0.5mu    \u0026lt;- rep(0,2)Sigma \u0026lt;- matrix(rho, nrow=2, ncol=2) + diag(2)*(1 - rho)library(MASS)compute.tr.t \u0026lt;- function(req.n, paccept) {  req.n      \u0026lt;- round(req.n / paccept)  rawvars    \u0026lt;- mvrnorm(req.n, mu=mu, Sigma=Sigma)  pvars      \u0026lt;- pnorm(rawvars)  tr         \u0026lt;- qexp(pvars[,1], 1/1)  t          \u0026lt;- qexp(pvars[,2], 1/2)  keep       \u0026lt;- which(t \u0026gt; tr)  return(data.frame(t=t[keep],tr=tr[keep]))}req.n   \u0026lt;- npaccept \u0026lt;- 1res     \u0026lt;- data.frame()while (req.n \u0026gt; 0) {  new.res \u0026lt;- compute.tr.t(req.n, paccept)  res     \u0026lt;- rbind(res, new.res)  req.n   \u0026lt;- n - nrow(res)  paccept \u0026lt;- nrow(new.res) / n# updated paccept according to last step}But here the means of both  and  changes and I also do not get the desired correlation. mean(rest)[1] 2.859441print(cor(rest))[1] -0.237159Even if I accept the reduced correlation, is it possible to generate two exponentially distributed random variables preserving their means and the constraint that ? I asked this in stackoverflow and I was suggested to ask this here. Is it that the distributions of  and  do not remain exponential anymore?   ","Creater_id":12603,"Start_date":"2016-08-16 04:52:59","Question_id":230085,"Tags":["r","simulation"],"Answer_count":1,"Last_activity":"2016-08-17 10:00:50","Link":"http://stats.stackexchange.com/questions/230085/is-it-possible-to-generate-two-correlated-exponential-variables-with-constraint","Creator_reputation":1293}
{"_id":{"$oid":"5837a57ba05283111e4d498d"},"View_count":1487,"Display_name":"GEL","Question_score":4,"Question_content":"I've looking around Google Scholar for the earliest mention of this particular classifier and have not had much luck finding a definitive source. I've seen some sources cite as late as the 1980s and other as early as the 1930s. Does anyone know when the Naïve Bayes Classifier was developed and/or first used as a classification technique?","Creater_id":4672,"Start_date":"2011-11-10 13:56:40","Question_id":18212,"Tags":["naive-bayes","history"],"Answer_count":2,"Last_activity":"2016-08-17 09:58:03","Link":"http://stats.stackexchange.com/questions/18212/origin-of-the-na%c3%afve-bayes-classifier","Creator_reputation":135}
{"_id":{"$oid":"5837a57ba05283111e4d499a"},"View_count":765,"Display_name":"Ivan","Question_score":2,"Question_content":"ContextI'm attempting to understand how R's coxph() accepts and handles repeated entries for subjects (or patient/customer if you prefer). Some call this Long format, others call it 'repeated measures'.See for example the data set that includes the ID column in the Answers section at:Best packages for Cox models with time varying covariatesAlso assume covariates are time-varying throughout and there is exactly one censor (i.e. event) variable, which is binary.Questions1) In the above link's answer, if ID is not given as a parameter in the call to coxph() should the results be the same as including cluster(ID) as a parameter in coxph()? I attempted to search for documentation, but the following doesn't seem to clearly address (1):https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html2) If the answer to (1) is 'no', then (mathematically) why? It seems cluster() in coxph() seeks correlations between subjects as per subsection 'cluster' on pg. 20 athttps://cran.r-project.org/web/packages/survival/survival.pdf3) Vague question: how does coxph() with repeated measures compare to R's frailtypack regression methods? AddendaThe following hints at using cluster(ID):Is there a repeated measures aware version of the logrank test?as does:https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html  GEE approach: add \"+ cluster(subject)\" to the model statement in coxph  Mixed models approach: Add \" + (1|subject)\" to the model statment in coxme.Thanks in advance!","Creater_id":76943,"Start_date":"2015-10-27 14:33:56","Question_id":178944,"Tags":["r","repeated-measures","survival","cox-model","frailty"],"Answer_count":2,"Last_activity":"2016-08-17 09:56:46","Link":"http://stats.stackexchange.com/questions/178944/precisely-how-does-rs-coxph-handle-repeated-measures","Creator_reputation":118}
{"_id":{"$oid":"5837a57ba05283111e4d49a8"},"View_count":38,"Display_name":"limbonic","Question_score":1,"Question_content":"The setting I am considering is the following:In varying time-intervals we receive numerical data from a range of approx. 200 sensors (of unknown nature), which hopefully allow us to predict the internal state of our system.The readouts themselves are unreliable, in the usual case, I will only get a couple of readouts per day (I am pooling all sensory input on the level of a day) while not receiving any data from the other sensors.I would like to learn a lower-dimensional representation of my input data, which, in it's raw form, would be a 200-dimensional vector with many missing values.Would approaching the problem of dimensionality reduction with an autoencoder make sense in such a setting? Especially since the cost-function might be heavily driven by the imputed input values.What kind of imputation strategy would you follow for the missing data?","Creater_id":12488,"Start_date":"2016-08-17 09:43:21","Question_id":230321,"Tags":["missing-data","dimensionality-reduction","autoencoders"],"Answer_count":0,"Last_activity":"2016-08-17 09:48:55","Link":"http://stats.stackexchange.com/questions/230321/autoencoders-for-dimensionality-reduction-of-sensor-data-with-many-missing-value","Creator_reputation":36}
{"_id":{"$oid":"5837a57ba05283111e4d49aa"},"View_count":37,"Display_name":"GDon","Question_score":1,"Question_content":"I have finished running multiple imputation using mice in R.  I have performed fit \u0026lt;- (df, lm(y~x+z))summary(pool(fit))in order to 'pool' the imputations into one.  But how do I actually get this final imputed dataset?  What i mean by the final dataset is the data frame I originally had with the final pooled imputation figures in place of the NAs.","Creater_id":128027,"Start_date":"2016-08-17 09:40:56","Question_id":230320,"Tags":["r","multiple-imputation","pooling"],"Answer_count":0,"Last_activity":"2016-08-17 09:40:56","Link":"http://stats.stackexchange.com/questions/230320/i-have-pooled-my-data-in-mice-now-how-do-i-get-an-actual-final-dataset","Creator_reputation":13}
{"_id":{"$oid":"5837a57ba05283111e4d49ac"},"View_count":46,"Display_name":"user3698581","Question_score":0,"Question_content":"I am new to the component analysis and with my basic understanding I could gather, if we have both 'continuous' and 'categorical' variables we can use Factor analysis of Mixed Data. In my case, I have a dataset with 31 variables. 14 -- continuous17 -- categorical. My ultimate goal is to predict G3 -- a  continuous variable. I am  using FAMD in R and I wanted to have an exploratory analysis. QUESTIONS1. Whats the mistake I am using in using FAMD() in R. Please see below for output. 2. Is FAMD a proper method for exploratory analysis in the first place, when we have multiple categorical and multiple continuous variables? If no, what is?For data set : https://1drv.ms/f/s!AsnJ6YMT9dDCth47Spg3csOJMCz7FAMD(student.n, graph = TRUE)Error in aggregate.data.frame(as.data.frame(x), ...) :  arguments must have same lengthIn addition: Warning messages:1: In pcacoord[ind.quali, , drop = FALSE]/sqrt(prop) : longer object length is not a multiple of shorter object length2: In coord.quali.var^2/dist2 :longer object length is not a multiple of shorter object length3: In vtest/sqrt(nombre) :longer object length is not a multiple of shorter object length","Creater_id":97351,"Start_date":"2016-08-17 09:22:56","Question_id":230314,"Tags":["r","machine-learning","pca","factor-analysis"],"Answer_count":0,"Last_activity":"2016-08-17 09:22:56","Link":"http://stats.stackexchange.com/questions/230314/famd-in-r-error-arguments-must-have-same-length","Creator_reputation":26}
{"_id":{"$oid":"5837a57ba05283111e4d49ae"},"View_count":48,"Display_name":"reg_user333","Question_score":3,"Question_content":"Problem Statement: We currently field a psychological survey with what we believe to be too many questions. 1) We would like to remove the questions that have the least predictive power on brand purchases. 2) We would also like to introduce new questions that are at the cutting edge of pyschographic research.To accomplish these two goals, we have been advised to run multiple regressions on all of our current questions against all of the brands we measure. We take the average  and  to determine the worst performing questions for removal. And for adding new questions, we run regressions of brand purchases against the new questions fielded in a new survey. The basic idea in matrix form:We measure about 1,000 different brands:And the sets of predictors. There are about 20 questions in each group. He asked that we combine sets of predictors also.Therefore, we run each brand against the set of old questions, then each brand against the set of new questions, next the brands against the demos, and so on.We will run about  regressions, taking the average  and removing the lowest performers. But this approach seems to ignore the marginal effects of one question on another. Especially when looking at , it is a mixture of old, new, and demographic questions. Also, the  looks at the overall accuracy of the set, not how one particular predictor performed. For this, each  value should be judged. But does this become problematic since it is looking at a particular question holding all other questions constant?What do you think of this approach? How does it compare to other techniques like choice modeling? ","Creater_id":128005,"Start_date":"2016-08-17 08:01:26","Question_id":230294,"Tags":["regression","multivariate-analysis","model-selection"],"Answer_count":0,"Last_activity":"2016-08-17 09:17:00","Link":"http://stats.stackexchange.com/questions/230294/regression-methodology-help","Creator_reputation":16}
{"_id":{"$oid":"5837a57ba05283111e4d49b0"},"View_count":40,"Display_name":"John St","Question_score":0,"Question_content":"I am doing a mixed ANOVA in SPSS where I am currently comparing the duration of certain script handwriting in 2 conditions (1=eyes open and 2=eyes closed) for the same 40 participants in both conditions and I want to use handedness as my Independent Variable. So I have 2 groups (1=right-handed and 2=left-handed) by 2 conditions.The right-handed subjects are 37 and the left-handed are 3. That to me would seem like a major problem theoretically.I have ran a simple paired t-tests comparing just the two conditions mentioned above  and have found significant differences between means. When I ran the ANOVA with the between subjects factor however, the within-subjects effects  came out not significant and so did the interaction between duration and handedness.On the other hand, group variances appear equal according to Levene's test.My question is this: Is there a cut-off point where group sizes are so unequal that there is no validity (or point) in doing statistical comparisons (ANOVA in particular) or is there a way to analyse this 2x2 design despite the large group sample differences ?  ","Creater_id":127028,"Start_date":"2016-08-08 16:34:42","Question_id":228888,"Tags":["anova","repeated-measures","sample-size","group-differences"],"Answer_count":1,"Last_activity":"2016-08-17 09:12:14","Link":"http://stats.stackexchange.com/questions/228888/is-it-valid-to-do-a-mixed-anova-with-one-within-and-one-between-subjects-factor","Creator_reputation":1}
{"_id":{"$oid":"5837a57ba05283111e4d49bc"},"View_count":26,"Display_name":"Fraz","Question_score":0,"Question_content":"I have say k items.. as tuple(id, rel_score, pop_score)The idea is to combine relevance and popularity to generate the final score.Which is basically.. w_1*x_1 + w_2*x_2.. so.. it all boils down to tuning these weights.Now, I was wondering if I can use bandits to tune these weights? But I have never used bandits before.. Was wondering how, this will look like..What I am confused is the problem formulation... Here, are the two alternatives the weights.... or if I want to show k-items, are the alternatives \"k\".. That is to say.. is this two arm bandit problem (since there are two weights to select)... or is it k-arm bandit problem (as there are k-items to rank)..Also, if i have a categorical features say(id, rel_score,pop_score, city) How will formulation change?","Creater_id":7891,"Start_date":"2016-08-17 09:10:27","Question_id":230311,"Tags":["machine-learning","reinforcement-learning"],"Answer_count":0,"Last_activity":"2016-08-17 09:10:27","Link":"http://stats.stackexchange.com/questions/230311/can-i-use-multi-arm-bandits-to-rank-list-of-items","Creator_reputation":147}
{"_id":{"$oid":"5837a57ba05283111e4d49be"},"View_count":252,"Display_name":"YCR","Question_score":4,"Question_content":"I read both wikipedia pages of sensitivity analysis and model validation (here, only linear regression validation) but I don't manage to find a way to separate these two terms.I have the impression that the first one is more used in academia and engineering in general and the second one in \"data science\".One option I see is to modify the level of description of these terms: sensitivity analysis is more like a general terms to design a high level branch of methods, and model validation may be more specific and be include in sensitivity analysis.Any thought?I am more interested in the difference than in the similarities between these two notions.","Creater_id":83595,"Start_date":"2016-02-05 12:30:16","Question_id":194262,"Tags":["validation","sensitivity-analysis"],"Answer_count":2,"Last_activity":"2016-08-17 09:02:12","Link":"http://stats.stackexchange.com/questions/194262/what-is-the-difference-between-sensitivity-analysis-and-model-validation","Creator_reputation":254}
{"_id":{"$oid":"5837a57ba05283111e4d49cb"},"View_count":182,"Display_name":"Programmer2134","Question_score":1,"Question_content":"I am training a neural network using backpropagation and stochastic gradient descent in keras. However the network produces a graph that does not approximate the target funcion at all and I don't know why.I've added the code here and a plot of the target function and resulting NN approximation below.import mathimport randomimport matplotlib.pyplot as pltimport numpyfrom matplotlib import pyplotnumpy.random.seed(7)random.seed = 775print(type(random.seed))from keras.layers import Densefrom keras.models import Sequentialfrom keras.optimizers import SGDTHEANO_FLAGS = \"\"r = Sequential()numpy.arraydef setup_nn():    r.add(Dense(1, activation='sigmoid', input_dim=1, init='uniform'))    r.add(Dense(50, activation='sigmoid', input_dim=1, init='uniform'))    r.add(Dense(output_dim=1, activation='linear', input_dim=50))    sgd = SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=False)    r.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])def target_function(X):    a = math.sin(X*3)    return a*10def trainRandomX(samplesize):    X = []    Y = []    for j in range(0, samplesize):        xj = random.random()        X.append(xj)        Y.append(target_function(xj))    # X=numpy.array(X)    # Y=numpy.array(Y)    r.fit(X, Y, batch_size=100, nb_epoch=1)    returndef testRandomX():    X = [random.random()]    Y = target_function(X[0])    X = numpy.array(X)    Ypred = r.predict(X, batch_size=1)    error = Ypred[0][0] - Y    print(\"error: \", error)    # print(Ypred)    return [X, Ypred[0][0]]setup_nn()plt.interactive(False)# for i in range(0, 1):trainRandomX(10000)error = 0X = []Y = []for i in range(0, 20):    # error += abs(testRandomX())    XY = testRandomX()    X.append(XY[0][0])    Y.append(XY[1])pyplot.plot(X, Y, 'o')def plotfunction():    X = []    Y = []    for i in range(0, 100):        x = i / 100        X.append(x)        Y.append(target_function(x))    pyplot.plot(X, Y, '.')plotfunction()print(\"average error: \", error / 20)plt.show()Here the plot: the big dots are the neural network's approximation. Why don't they correspond to the target function better? ","Creater_id":127096,"Start_date":"2016-08-17 08:12:42","Question_id":230298,"Tags":["regression","machine-learning","neural-networks"],"Answer_count":1,"Last_activity":"2016-08-17 08:56:13","Link":"http://stats.stackexchange.com/questions/230298/why-does-this-neural-network-in-keras-fail-so-badly","Creator_reputation":143}
{"_id":{"$oid":"5837a57ba05283111e4d49d8"},"View_count":39,"Display_name":"rnewbie","Question_score":1,"Question_content":"I am teaching myself regression using Regression Modeling Strategies by Harell and the author goes at quite the length to showcase the importance of modeling interactions and transformations of the initial variables. I can't help but wonder how to approach this in a more structured/automated way when dealing with a lot of potential variables.Can we use recursive partitioning, for example, to somehow to do the work for us and then use the output as variables, shrink the estimates with LASSO to deal with colinearity and do a final step where we use some sort of filtering for feature importance.In my mind this will leave us with a well specified model which can be manually inspected and improved if need be, but is this reasonable? Are there other ways to approach this? Are there some resources that deal with problems like this?","Creater_id":127984,"Start_date":"2016-08-17 08:36:59","Question_id":230303,"Tags":["r","interaction","regression-strategies"],"Answer_count":1,"Last_activity":"2016-08-17 08:52:19","Link":"http://stats.stackexchange.com/questions/230303/discovering-transformations-and-interactions","Creator_reputation":8}
{"_id":{"$oid":"5837a57ba05283111e4d49e5"},"View_count":73,"Display_name":"Dhamnekar","Question_score":1,"Question_content":"You  are invited to a party. Suppose the times at which invitees arrives are independent uniform(0,1) random variables. Suppose that aside from yourself the number of other people who are invited is a Poisson random variable with mean 10.I want to find outThe expected number of people who arrive before youThe probability that you are the nth person to arriveAnswer: Let X be the number of people who arrive before you.Because you are equally likely to be the first,second,or third,...,or nth arrival.P{X=I}=ThereforeE[X]=andP{X=n}= Are these answers correct? The author has not provided the correct answers.","Creater_id":72126,"Start_date":"2016-08-16 09:31:20","Question_id":230134,"Tags":["self-study","conditional-probability","conditional-expectation"],"Answer_count":1,"Last_activity":"2016-08-17 08:52:01","Link":"http://stats.stackexchange.com/questions/230134/poisson-random-variable-self-study-question","Creator_reputation":167}
{"_id":{"$oid":"5837a57ba05283111e4d49f2"},"View_count":204,"Display_name":"LoulouChameau","Question_score":1,"Question_content":"I was wondering if there was a simple solution to get recall and precision value for the classes of my classifier?To put some context, I implemented a 20 classes CNN classifier using Tensorflow with the help of Denny Britz code : https://github.com/dennybritz/cnn-text-classification-tf .As you can see at the end of text_cnn.py he implements a simple function to compute the global accuracy :# Accuracy        with tf.name_scope(\"accuracy\"):            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")Any ideas on how i could do something similar to get the recall and precision value for the differents categories?Maybe my question will sound dumb but I'm a bit lost with this to be honest. Thanks for the help.","Creater_id":127872,"Start_date":"2016-08-16 08:57:31","Question_id":230125,"Tags":["classification","neural-networks","python","precision-recall","tensorflow"],"Answer_count":1,"Last_activity":"2016-08-17 08:49:54","Link":"http://stats.stackexchange.com/questions/230125/get-precision-and-recall-value-with-tensorflow-cnn-implementation","Creator_reputation":90}
{"_id":{"$oid":"5837a57ba05283111e4d49ff"},"View_count":44,"Display_name":"Stefan","Question_score":1,"Question_content":"I am performing PLS regression but have noticed that the R2 between predicted and measured values for the test set decreases with increasing the number of components. If I do principal components regression, as one would expect, R2 increases with increasing the number of components. Does this signal I'm doing something wrong somewhere? What are principal components in PLS at all? Could it be that the higher PCs in my PLS analysis introduce more noise than signal?Here is a snippet of R code to better illustrate what I'm doing (data file is too big to upload):PCA regression:library(\"pls\")library(\"caret\")for (x in 1:70){y_test \u0026lt;- test[, 3]testx = test[,5:length(test)-1]pcr_model \u0026lt;- pcr(as.matrix(train[\"affinity\"]) ~ ., data = train[,5:length(train)-1], scale = TRUE, validation = \"CV\")pcr_pred \u0026lt;- predict(pcr_model, testx, ncomp = x)print (cor(pcr_pred, y_test)^2)}This behaves as one would expect - R2 increases with the number of components. If I do PLS by using the plsr function rather than pcr, however, R2 is highest with only a few components and progressively decreases from then on. How could that be?PLS regression:library(\"pls\")library(\"caret\")for (x in 1:70){y_test \u0026lt;- test[, 3]testx = test[,5:length(test)-1]pcr_model \u0026lt;- plsr(as.matrix(train[\"affinity\"]) ~ ., data = train[,5:length(train)-1], scale = TRUE, validation = \"CV\")pcr_pred \u0026lt;- predict(pcr_model, testx, ncomp = x)print (cor(pcr_pred, y_test)^2)}","Creater_id":53845,"Start_date":"2016-08-16 22:06:43","Question_id":230207,"Tags":["regression","pca","pls","out-of-sample"],"Answer_count":0,"Last_activity":"2016-08-17 08:46:26","Link":"http://stats.stackexchange.com/questions/230207/comparing-pc-and-pls-regression-performance-on-the-test-data-depending-on-the-nu","Creator_reputation":123}
{"_id":{"$oid":"5837a57ba05283111e4d4a01"},"View_count":46,"Display_name":"Deepend","Question_score":0,"Question_content":"I am trying to undertake some Z-Tests to see the level of effect in Standard Deviations of 8 separate treatments compared to a control mean. However there is one thing that has constantly confused me in all my attempts. How do I set the mean rating of my control as the mean to compare all the other too? My Data             Control  T1      T2      T3     T4     T5     T6      T7     T8Mean Ratings 4.74     -1.77   -1.88   7.77   4.08   6.90   13.66   17.13  19.44The sample population for the control and each treatment is circa 105, is normally or approximately normally distributed and each participant contributed one score to each mean rating. Calculating Z-Scores - Method 1Using SPSS if I run a simple two tailed Z Test on the above using Analyse \u0026gt; Descriptive Statistics \u0026gt; Descriptives I get:            N     Minimum   Maximum   Mean      Std. DeviationVAR00023    9     -1.88     19.44     7.7856    7.63361Valid N (listwise)  9                  Control: -.39897T1:      -1.25177T2:      -1.26618T3:      -.00204T4:      -.48543T5:      -.11601T6:      .76955T7:      1.22412T8:      1.52673The above tells me that none of my treatments were significant at α=0.05 However this does not seem right as its including my control mean (4.74) in the equation.Calculating Z-Scores - Method 2If I run the same test without the control mean I get:Descriptive Statistics                                N    Minimum   Maximum    Mean    Std. DeviationVAR00002      8    -1.88     19.44      8.1663  8.06883Valid N (listwise)  8               T1:      -1.23144T2:      -1.24507T3:      -.04911T4:      -.50642T5:      -.15693T6:      .68086T7:      1.11091T8:      1.39720The above tells me much the same as the previous method, that none of my treatments were significant at α=0.05, however there are variations in each scoreThis does not seem right either as I have not specified the control mean (4.74) that I want all of the Treatments to be compared too?My QuestionCan anyone tell me which is the right track, or the closest to it, and where I might be going wrong? Neither method I have tried appears to be right for the aforementioned reasons.Any help, as always, is very much appreciatedThanksEditThanks all for taking the time to look at this. I have struggled with this for a good few days now and would really appreciate if anyone could explain the correct way to undertake a Z-Test using the above data so that I can see where my reasoning has been going wrong. ThanksNOTE: I understand from the answer to previous question I asked that I will have to correct for Family Wise Error Rate, that will be my next challenge to figure out, I just want to see if I can get this part figured out first. ","Creater_id":39684,"Start_date":"2016-08-15 05:14:52","Question_id":229904,"Tags":["hypothesis-testing","mean","z-test","z-score"],"Answer_count":0,"Last_activity":"2016-08-17 08:37:02","Link":"http://stats.stackexchange.com/questions/229904/is-it-correct-to-set-a-control-mean-to-compare-the-z-scores-of-several-sub-group","Creator_reputation":53}
{"_id":{"$oid":"5837a57ba05283111e4d4a03"},"View_count":60,"Display_name":"Deepend","Question_score":1,"Question_content":"The below is a partial data set showing the mean user ratings for a number of products, each of which is available in a number of standard versions, e.g. a common feature added to each product such as adding electric windows to a car. Each mean rating is comprised of circa 17 user ratings, though some are more. The user ratings were submitted on a scale from -100 to +100. N/A indicates a product which is not available in that particular version. I understand from this question I asked previously that I should not replace the N/A values with a 0 or an average.The data is continuous interval type data.              Control   V1        V2        V3        V4        V5        V6Product 1    1.63     -5.19     -0.48     5.79      8.89      4.19      15.73Product 2    0.60     0.84      4.47      N/A       0.52      21.17     N/AProduct 3    4.53     -15.20    -19.66    N/A       2.84      N/A       13.07Product 4    7.30     17.53     20.25     17.04     N/A       4.60      9.28Product 5    -4.05    -21.33    -14.00    -13.00    N/A       -23.71    -8.71Product 6    26.27    14.53     N/A       21.24     N/A       27.25     35.18Product 7    -3.12    N/A       N/A       N/A       N/A       7.88      17.38Mean Ratings 4.74     -1.47     -1.88     7.77      4.08      6.90      13.66I want to compare the effect of the different standard versions compared to the control.So, I think I should be using two tailed Z-Tests so I can see how far above or below the control mean the version mean or each of its individual products is. Here is my reasoning: The vast majority of user ratings that make up my means are normally distributed. I checked using Kolmogorov-Smirnov and Shapiro-Wilk scores. I checked everything that failed either of the tests with Q-Q plots and they are approximately normally distributed. My population is \u003e 30 While each individual mean is circa 15 user rateings the total of the control is circa 105 and V1 is 105, I think this is correct. I can derive the Standard Deviation forThe individual mean scores The combined mean scores of the Control and V1 shown belowI also received an affirmative reply to a recent question I posted on whether or not a Z test was appropriate to this situation.My HypothesisH0 - Version 1 will have no effect on the mean user ratingThe mean will not be significantly different to the controlHa - Version 1 will have an effect on the mean user ratingThe mean will be significantly different to the controlI need to test this claim using alpha 0.05 or +1.96 to -1.96Z-TestI first took the mean ratings for each of the versions (from above)Mean Ratings 4.74     -1.47     -1.88     7.77      4.08      6.90      13.66And used SPSS to calculate Z-Scores for eachControl   = -.13054Version 1 = -1.42611Version 2 = -.72721Version 3 = .50160Version 4 = -.26823Version 5 = .32009Version 6 = 1.73041This tells me that none of the common versions had a significant effect on the mean user ratings for the products. My questions are: Are Z tests suitable for this sort of data analysis? (Since answered Yes)And if yes is my reasoning and my attempts correct?If anyone could point out any glaringly obvious mistakes or problems with my method it would be much appreciatedAs always, any help is much appreciated. EDIT: I suspect that one issue is that I am including my control mean in my Z-Test calculation, which is skewing the results. But I am not sure how to undertake such a test when I am comparing it to a known mean...?Edit 2:In response to David Cs answer I am able to calculate the variance for each mean rating e.g. the Control = 105.999Descriptive Statistics                                N   Minimum   Maximum   Mean      Std. Deviation    VarianceControl   7   -4.05     26.27     4.7371    10.29558          105.999Valid N (listwise)  7   ","Creater_id":39684,"Start_date":"2016-08-13 12:31:45","Question_id":229698,"Tags":["hypothesis-testing","mean","z-test","z-score"],"Answer_count":1,"Last_activity":"2016-08-17 08:33:12","Link":"http://stats.stackexchange.com/questions/229698/is-this-the-correct-way-to-undertake-z-tests-on-average-user-ratings-to-compare","Creator_reputation":53}
{"_id":{"$oid":"5837a57ba05283111e4d4a10"},"View_count":27,"Display_name":"user22108","Question_score":2,"Question_content":"I need some help in order to solve this exercise for the exam of time-series analysis/econometrics. I am stuck at some points. Here is the exercise:considering the process  with  I have to calculate expected value, variance and autocovariance function. I have already caluculated the expexted value and the variance of the process which are 0 and . Now I have to calculate the covariance . I applied the useual formula and I get  but then I am unable to go forward and the resut should be 0.Here there is the second part of the exercise:considering the transformation  I need to calucalte also for  expected value, variance and autocovariance function. For the expected value there was no problem...it is . But then I am stuck with the others:   and for the covariance I don't know even where to start...the only hint I have is \"if \"If you can provide me step-by-step calculations it would be very helpful so I can understand which property is involved...I don't need only the final result.","Creater_id":128011,"Start_date":"2016-08-17 08:28:17","Question_id":230301,"Tags":["time-series","self-study","variance","econometrics","covariance"],"Answer_count":0,"Last_activity":"2016-08-17 08:28:17","Link":"http://stats.stackexchange.com/questions/230301/exercise-on-expected-value-variance-and-autocovariance-function-for-a-stochasti","Creator_reputation":16}
{"_id":{"$oid":"5837a57ba05283111e4d4a12"},"View_count":12,"Display_name":"Kalguy","Question_score":1,"Question_content":"I am learning about Kalman filters/dynamic linear models/state-space models and I am interested in whether the following scheme is possible, in which I try to estimate distribution parameters simultaneously with a sequence.Suppose that there is some sequence  which we would like to estimate (in a sense I will make precise asap). All we are given is the observations , which include some error, so  and . Let's write the prediction error as . I would like to write an algorithm that minimises the RMS of prediction errors, that is, minimises .I do not want to specify any parameters. I do not know if  is constant, linear, has 'jumps' or anything; let's only write . I am willing to make some quality of life assumptions (such as the signal error  having  mean), but would e.g. prefer not to assume that  is a constant or known value. Hence I need to adapt my Kalman gain as I go along.Finally, I want to output . Of course, for this I must first find , using a Kalman gain/adaption coefficient  to determine how much weight to give to my previous estimate versus the new observation: \\mathbb E(x_t|Y_{\\leq t}) = K_t\\mathbb E(x_t|Y_{\\leq t-1}) + (1-K_t)y_t.The parameter  is usually the signal variance divided by the total variance. I want to update my estimate of signal variance , and total variance , with each new piece of data. Of course, to update the -estimates I also need to use a Kalman gain.This is a chicken-and-egg problem. If, but  (and similarly ), how can I resolve this issue ? Is there something like a prior Kalman gain + a posterior Kalman gain, or some Bayesian tool, that can be brought to bear ? I appreciate any pointers to the literature.","Creater_id":128019,"Start_date":"2016-08-17 08:27:29","Question_id":230300,"Tags":["kalman-filter","state-space-models"],"Answer_count":0,"Last_activity":"2016-08-17 08:27:29","Link":"http://stats.stackexchange.com/questions/230300/adaptive-kalman-filtering","Creator_reputation":6}
{"_id":{"$oid":"5837a57ba05283111e4d4a14"},"View_count":22,"Display_name":"ElPresidente","Question_score":0,"Question_content":"My question is not literally the formula for measuring sensitivity and specificity of a test. Instead, I am asking how do different medical tests decide what constitutes a reported TP/TN/FP/FN, and how is that related to the structure of their trial?For example, I develop a new medical test for condition X which occurs in 10 out of every 100 individuals in the broader population. I claim my test has 95% sensitivity and 99% specificity based on trials. But wouldn't the structure of the trial change the observed accuracy? I could structure it something like:Test 1,000 random population individualsTest 1,000 individuals but over-sample those with the condition (maybe 50/50)I would think scenario 1 would experimentally show my test being less effictive while scenario 2 amplifies it's power. Does the proportion of classes in a study affect how I should infer test accuracy?Related question would be how do medical sensitivity/specificity numbers change depending on other patient observations? For example:An a patient walks into a doctor's office and a doctor randomly orders a test with 95%/99% accuracyA patient showing symptoms consistent with the disease and the doctor orders the same test","Creater_id":45224,"Start_date":"2016-08-17 06:21:24","Question_id":230273,"Tags":["classification","experiment-design","sensitivity","specificity"],"Answer_count":1,"Last_activity":"2016-08-17 08:12:09","Link":"http://stats.stackexchange.com/questions/230273/how-are-specificity-and-sensitivity-for-medical-tests-measured","Creator_reputation":169}
{"_id":{"$oid":"5837a57ba05283111e4d4a21"},"View_count":118,"Display_name":"Metariat","Question_score":3,"Question_content":"I have two related questions:Question 1:In the wikipedia definition of data dredging:  Data dredging (also data fishing, data snooping, and p-hacking) is the  use of data mining to uncover patterns in data that can be presented  as statistically significant, without first devising a specific  hypothesis as to the underlying causality.    The process of data mining involves automatically testing huge numbers of hypotheses about a single data set by exhaustively searching for combinations of variables that might show a correlation. I'm thinking of Random Forest when reading this definition. Although there is no hypothesis testing nor  use of p-value, ... But does the predicted probability calculated by Random Forest make sense when it involve the using of data mining to find a patterns in data that can be significant in some extent? IMHO: the predicted probability calculated by each single tree of RF is highly biased (because of data dredging), then RF tries to correct this bias by adding randomness to the model (variable selected at each nodes, bootstrapped training data sets)averaging the predicted probability of many trees. But can this correct all the bias made by every single tree? If the correction of the predicted probability by  this technique works? Can the same idea be applied to stepwise regression to correct its weakness cited here by @gung and @FrankHarrell? This question is applied for other data mining methods as well.Question 2:If the predicted probability of Random Forest is considered \"valid\": when facing the imbalanced data, one way to improve the performance of RF is to use downsampling technique on the training data set before making trees (resampling the data in such a way that the positive and negative class are \"balanced\" in proportion). By doing this, the probability calculated is biased, because when the data is resampled, the proportion of positive/ negative observations (predicted probability) changes in each group due to resampling. To \"correct\" this biased probability, I imagine that one can not simply multiply the predicted probability by the prior downsampling ratio because even though the prior distribution is uniformly resampled, after the using of data mining to find the patterns in data (RF), the posterior distribution goes in different directions than the prior resampling ratio. Thus the multiplication of the posterior probability by the prior resampling ratio isn't valid anymore. The question is: when using resampling technique, how to correct the predicted probability in Random Forest?Don't hesitate to modify my question if it's not clear. ","Creater_id":78313,"Start_date":"2016-08-16 02:14:24","Question_id":230057,"Tags":["probability","random-forest"],"Answer_count":1,"Last_activity":"2016-08-17 08:08:11","Link":"http://stats.stackexchange.com/questions/230057/random-forests-probability-is-biased-because-of-data-dredging","Creator_reputation":825}
{"_id":{"$oid":"5837a57ba05283111e4d4a2e"},"View_count":59,"Display_name":"Reza_Research","Question_score":0,"Question_content":"I have a ROC curve like the figure below:What can be inferred given this kind of curve in general?What are the differences of states 1, 2 and 3 in the figure?What causes the diagram to jump straight from state 1 to state 3? Most of the ROC curves on the internet look like the figure below: ","Creater_id":126608,"Start_date":"2016-08-17 07:01:50","Question_id":230279,"Tags":["roc"],"Answer_count":3,"Last_activity":"2016-08-17 08:06:10","Link":"http://stats.stackexchange.com/questions/230279/interpretation-of-roc-curve","Creator_reputation":48}
{"_id":{"$oid":"5837a57ba05283111e4d4a3d"},"View_count":99,"Display_name":"Simon","Question_score":2,"Question_content":"Ive just recently started learning about the ICC and multilevel models and I've been told that one way to determine whether a MLM is warranted is by checking the size of the ICC. I'm struggling to understand why the ICC is a good indicator for whether you should run a MLMFrom what I understand, the ICC tells you how much variability there is between your clusters/groups. If ICC is large, then there is a lot of variability between your clusters, and you should treat them separately, either allowing for random intercepts or random slopesI can sort of understand why the ICC might be useful if you were interested in running a random intercepts model. High variability between clusters might suggest that they have differing means and potentially differing intercepts, so we run a MLM that allows for a different intercept for each clusterBut does the ICC tell you anything about the likelihood of different slopes for each cluster? I cant quite wrap my head around how slopes are related to the value returned by the ICC. If the ICC the small, then it means theres little variability between the clusters, which might suggest that their means are similar and thus a random intercepts model may not be needed, but does that automatically mean a random slopes model is also not warranted?","Creater_id":68268,"Start_date":"2016-08-16 23:40:34","Question_id":230214,"Tags":["mixed-model","multilevel-analysis","random-effects-model","intraclass-correlation"],"Answer_count":1,"Last_activity":"2016-08-17 07:58:51","Link":"http://stats.stackexchange.com/questions/230214/use-of-icc-in-multilevel-modelling","Creator_reputation":180}
{"_id":{"$oid":"5837a57ba05283111e4d4a4a"},"View_count":78,"Display_name":"James","Question_score":5,"Question_content":"In SGD an epoch would be the full presentation of the training data, and then there would be N weight updates per epoch (if there are N data examples in the training set).If we now do mini-batches instead, say in batches of 20. Does one epoch now consist of N/20 weight updates, or is an epoch 'lengthened' by 20 so that it contains the same number of weight updates?I ask this as in a couple of papers learning seems to be too quick for the number of epochs stated.","Creater_id":116350,"Start_date":"2016-08-16 08:23:58","Question_id":230120,"Tags":["machine-learning","neural-networks"],"Answer_count":1,"Last_activity":"2016-08-17 07:56:47","Link":"http://stats.stackexchange.com/questions/230120/neural-networks-is-an-epoch-in-sgd-the-same-as-an-epoch-in-mini-batch","Creator_reputation":26}
{"_id":{"$oid":"5837a57ba05283111e4d4a57"},"View_count":164,"Display_name":"josh","Question_score":5,"Question_content":"The two types of data differ in that if you decide to decrease the temporal (time) resolution of the first type of data you take the mean of lower the resolutions. With the second you take the sum over the lower resolutions. Here is a concrete exampleGas Used (kWh), Outside Air Temperature (C), Time Resolution (Minutes)100, 20, 20140, 22, 20120, 21, 20Here the hourly (60 Minute) resolution data is clearly360, 21, 60and is found by averaging the temperature and summing up the gas used.What I really wish to know is what the names for each of these types of data are called. I understand that this question is not the best but it is hard to ask a question about something whose names you do not know. If you have a better way of articulating the question then please go ahead and edit it.","Creater_id":62008,"Start_date":"2015-05-28 08:37:38","Question_id":154463,"Tags":["aggregation","spatio-temporal"],"Answer_count":2,"Last_activity":"2016-08-17 07:40:43","Link":"http://stats.stackexchange.com/questions/154463/what-is-the-terminology-for-data-aggregated-via-summed-totals-versus-data-aggreg","Creator_reputation":452}
{"_id":{"$oid":"5837a57ba05283111e4d4a65"},"View_count":23,"Display_name":"emehex","Question_score":1,"Question_content":"I have data that looks like this:df \u0026lt;- data.frame(    y = c(0.348, 0.099, 0.041, 0.022, 0.015, 0.010, 0.007, 0.005, 0.004, 0.003),    x = c(458, 648, 694, 724, 756, 790, 818, 836, 848, 876))My (simplified) regression model is sort of like this:m1 \u0026lt;- glm(y ~ poly(x, 2), data = df)df$predict \u0026lt;- predict(m1, df)df#        y   x     predict# 1  0.348 458 0.349095453# 2  0.099 648 0.086238941# 3  0.041 694 0.049501859# 4  0.022 724 0.031192879# 5  0.015 756 0.016579779# 6  0.010 790 0.006614136# 7  0.007 818 0.002709055# 8  0.005 836 0.002250483# 9  0.004 848 0.002836871# 10 0.003 876 0.006980543And here's a graph of the fit:It fits pretty good. Except... looking at values at row 9 and 10... they're higher than the value at row 8. (Duh... it's a quadratic!). But I know those values are clearly wrong.How can I sacrifice a little bit of fit in order force my quadratic to pass through a value... like (900, 0)?Like this:","Creater_id":53204,"Start_date":"2016-08-17 07:20:23","Question_id":230283,"Tags":["r","regression","generalized-linear-model"],"Answer_count":0,"Last_activity":"2016-08-17 07:20:23","Link":"http://stats.stackexchange.com/questions/230283/glm-polynomial-regression-sacrifice-fit-for-value-at-zero","Creator_reputation":106}
{"_id":{"$oid":"5837a57ba05283111e4d4a67"},"View_count":4144,"Display_name":"Luca","Question_score":34,"Question_content":"I am looking at some lecture slides on a data science course which can be found here:https://github.com/cs109/2015/blob/master/Lectures/01-Introduction.pdfI, unfortunately, cannot see the video for this lecture and at one point on the slide, the presenter has the following text:Some Key PrinciplesThink like a Bayesian, check like a Frequentist (reconciliation)Does anyone know what that actually means? I have a feeling there is a good insight about these two schools of thought to be gathered from this.","Creater_id":36540,"Start_date":"2016-08-16 06:33:23","Question_id":230097,"Tags":["bayesian","data-mining","frequentist"],"Answer_count":5,"Last_activity":"2016-08-17 07:15:42","Link":"http://stats.stackexchange.com/questions/230097/think-like-a-bayesian-check-like-a-frequentist-what-does-that-mean","Creator_reputation":906}
{"_id":{"$oid":"5837a57ba05283111e4d4a78"},"View_count":33,"Display_name":"Usman Shahid","Question_score":2,"Question_content":"I have multiple records containing a single continuous variable as feature and the corresponding binary label (1, 0). I've to determine a threshold which maximally separates these two classes. Now in the attached histogram you can visually determine that the value would be somewhere between 0.4 and 0.6 (classes are in blue and red) but is there a simple way to do it mathematically?Perhaps logistic regression or SVM would be an overkill.","Creater_id":103448,"Start_date":"2016-08-17 03:22:09","Question_id":230242,"Tags":["classification"],"Answer_count":0,"Last_activity":"2016-08-17 06:59:49","Link":"http://stats.stackexchange.com/questions/230242/threshold-based-classification-using-a-single-continuous-variable","Creator_reputation":16}
{"_id":{"$oid":"5837a57ba05283111e4d4a7a"},"View_count":22,"Display_name":"Kevin Nowaczyk","Question_score":1,"Question_content":"In simple linear regression, I know the confidence interval can be calculated from:I know this equation comes from the relation Is there a way to either use Matrix Algebra to calculate this from the Standard Error matrix? where  and degrees of freedom I'm looking for a way to extend the confidence interval to multiple linear regression, including polynomial x.","Creater_id":82897,"Start_date":"2016-08-17 06:55:54","Question_id":230278,"Tags":["regression","multiple-regression","confidence-interval"],"Answer_count":0,"Last_activity":"2016-08-17 06:55:54","Link":"http://stats.stackexchange.com/questions/230278/regression-confidence-interval-in-terms-of-standard-errors-of-the-parameters","Creator_reputation":237}
{"_id":{"$oid":"5837a57ca05283111e4d4a7c"},"View_count":37,"Display_name":"user128002","Question_score":1,"Question_content":"I am doing factor analysis on dichotomous variables. I first calculated the tetrachoric correlations using tetrachoric() in the psych package. I then used the tetrachoricresultrho, factors=2, rotation='varimax'), I obtained the loadings and a plot.But on using (factanal(tetrachoricresult$rho,.....same as above), I got the following error-Error in solve.default(cv) :   system is computationally singular: reciprocal condition number = 4.05483e-18Why is there a difference int he two methods? Which function do I use?","Creater_id":128002,"Start_date":"2016-08-17 06:54:32","Question_id":230277,"Tags":["r","factor-analysis"],"Answer_count":0,"Last_activity":"2016-08-17 06:54:32","Link":"http://stats.stackexchange.com/questions/230277/factor-analysis-using-fa-and-factanal-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4a7e"},"View_count":47,"Display_name":"N_Moksnes","Question_score":1,"Question_content":"I'm forecasting electricity demand based on GDP and population. For the series that there is a unit root I have conducted the Johansen cointegration test and applied the number of cointegrating equations to the VECM.When I get the results from the VECM, I perform an OLS to find the coefficients. My question is, what should I do with the coefficients with high p-value (C(4) and C(6)). Should I exclude these coefficients and rerun the model or should I accept them as well in my analysis for the forecast?Overall the R-square is 0.758 which is a good result and also the Prob(F-statistic) is 0.00. Dependent Variable: D(ELEC)             Method: Least Squares               Date: 08/17/16   Time: 13:34                Sample (adjusted): 1974 2013                Included observations: 40 after adjustments             D(ELEC) = C(1)*( ELEC(-1) - 1.78078157078*POP(-1) + 4.86087264791 ) +                       C(2)*( GDP(-1) - 3.51459703515*POP(-1) + 8.23177201198 ) + C(3)                     *D(ELEC(-1)) + C(4)*D(ELEC(-2)) + C(5)*D(GDP(-1)) + C(6)*D(GDP(                     -2)) + C(7)*D(POP(-1)) + C(8)*D(POP(-2)) + C(9)             Coefficient Std. Error  t-Statistic Prob.  C(1)    -0.211084   0.094992    -2.222125   0.0337C(2)    0.068551    0.023368    2.933587    0.0063C(3)    0.558926    0.183732    3.042079    0.0048C(4)    0.301294    0.227807    1.322582    0.1956C(5)    -0.056782   0.026318    -2.157586   0.0388C(6)    -0.040041   0.022045    -1.816334   0.0790C(7)    -49.45054   15.47998    -3.194482   0.0032C(8)    44.98846    14.64799    3.071306    0.0044C(9)    0.550806    0.226170    2.435359    0.0208R-squared           0.758225        Mean dependent var      0.231150Adjusted R-squared  0.695831        S.D. dependent var      0.217092S.E. of regression  0.119730        Akaike info criterion  -1.212055Sum squared resid   0.444390        Schwarz criterion      -0.832057Log likelihood      33.24111        Hannan-Quinn criter.   -1.074660F-statistic         12.15227        Durbin-Watson stat      1.844170Prob(F-statistic)   0.000000    ","Creater_id":127834,"Start_date":"2016-08-17 05:54:33","Question_id":230268,"Tags":["statistical-significance","least-squares","regression-coefficients","vecm"],"Answer_count":0,"Last_activity":"2016-08-17 06:49:49","Link":"http://stats.stackexchange.com/questions/230268/what-to-do-with-insignificant-regressors-in-a-vecm","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4a80"},"View_count":51,"Display_name":"Baptiste Wicht","Question_score":1,"Question_content":"I have a question regarding the Activation Maximization technique for neural networks. Activation Maximization is a technique used to visualize the filters of a neural network: Erhan, Dumitru, et al. \"Visualizing higher-layer features of a deep network.\" University of Montreal 1341 (2009).The idea is to find an artificial sample that maximize the activation of an hidden unit and take this sample as representation of the filter. Since the weights are fixed after training, the activation of an unit only depends on x: h(x). To approximate a local maxima, we can use gradient ascent. There are two things that I don't get here:Do we use the input only b + Wx or the activation function sigmoid(b+Wx) ?How can we compute the gradients in which to move x ? ","Creater_id":49296,"Start_date":"2016-08-17 06:31:59","Question_id":230276,"Tags":["neural-networks","gradient-descent","gradient","cost-maximization"],"Answer_count":0,"Last_activity":"2016-08-17 06:31:59","Link":"http://stats.stackexchange.com/questions/230276/how-to-compute-the-gradients-for-activation-maximization-in-neural-network","Creator_reputation":188}
{"_id":{"$oid":"5837a57ca05283111e4d4a82"},"View_count":315,"Display_name":"Footy","Question_score":1,"Question_content":"Your suitcase has 4-digit code lock. You forgot the code, but you doremember that the sum of first two digits was equal to the sum oftwo second digits. How many combinations do you need to check inthe worst possible scenario?What methods can be used to solve this?","Creater_id":122959,"Start_date":"2016-07-15 08:08:42","Question_id":223954,"Tags":["self-study","combinatorics"],"Answer_count":3,"Last_activity":"2016-08-17 06:22:54","Link":"http://stats.stackexchange.com/questions/223954/number-of-combinations-for-a-4-digit-code-lock","Creator_reputation":7}
{"_id":{"$oid":"5837a57ca05283111e4d4a91"},"View_count":29,"Display_name":"user82729","Question_score":1,"Question_content":"Assume I have data on multiple people, voting. They can either vote yes or no or abstain. I would be curious how some predictors define the probability of the voters to vote yes. I am insecure with the right model choice. Just omitting people who abstain allows to choose between linear probability model ( I won't prefer), probit and logit. However, this would throw away useful data. I could use a Heckman probit/logit or any other two-step, assuming that the choice of abstain is before the choice between yes and no. However, this would impose a somewhat arbitrary assumption. I am also not very happy with that, as very few people abstain in each decision. Lastly, one could use multinominal logit/probit, which seem like a natural choice. However, it makes me wonder, whether to abstain is really the same option for the individual voter, as to vote either yes or no. Abstain is not a disjunct alternative, as is yes to no and vice versa. Is there any third type of model, that is made for this specific type of problem?I know, in some way, one could also just impute the overall result (either yes or no) to those people abstaining, because in the end, their vote expresses, that they follow the majority, but I somehow doubt, that this would be a usual way statistical educated people work. ","Creater_id":82729,"Start_date":"2016-08-17 06:21:08","Question_id":230272,"Tags":["regression","multinomial"],"Answer_count":0,"Last_activity":"2016-08-17 06:21:08","Link":"http://stats.stackexchange.com/questions/230272/regression-with-vote-data-yes-no-abstain-as-dependent-variable-is-there-a-spe","Creator_reputation":19}
{"_id":{"$oid":"5837a57ca05283111e4d4a93"},"View_count":17,"Display_name":"jadenkorr","Question_score":0,"Question_content":"Given a set of  characters, if I pick  of them, how many combinations contain a given character?A simple case: Picking two letters from ABCD has  possibilities, AB, AC, AD, BC, BD, CD and each letter appears 3 times. How does one solve for 3, the frequency of an individual letter, given a general  and ?","Creater_id":127157,"Start_date":"2016-08-09 11:40:48","Question_id":229044,"Tags":["combinatorics"],"Answer_count":1,"Last_activity":"2016-08-17 06:19:55","Link":"http://stats.stackexchange.com/questions/229044/frequency-of-item-in-combination","Creator_reputation":3}
{"_id":{"$oid":"5837a57ca05283111e4d4aa0"},"View_count":25,"Display_name":"Paparazzi","Question_score":1,"Question_content":"This happens to be about poker but I would like to ask it in a general way  There are 47 cards remaining in the deck with 2 cards to comeOf the 47 cards 9 make my hand - I only need 1 but 2 two also makes my hand  \\frac {\\binom{9}{1} \\binom{38}{1} + \\binom{9}{2}  } { \\binom{47}{2} } = 0.349676226 Pretty sure that is the correct number as it is posted in poker odds  Tried to get that same number with fractions The first card is 9/47 = 0.191489362Second cardTwo possibilities - first card hit or not  first card hit: (9/47)*(8/46) = 0.03330249first card did not hit: (1 - 9/47)*9/46 = 0.158186864Add those up 0.191489362 + 0.033302498 + 0.158186864 = 0.382978723That is not the same number as combinations 0.382978723 != 0.349676226My question why are those not the same result?With fractions this matches 9/47 + (1 - 9/47)*9/46 = 0.349676226But that does not make sense to me as it seems to me that should be the chance of 1 card hitting excluding the chance of 2 cards hittingIf I go at it 1 - not then it matches1 - ((47-9)/47*(46-9)/46)= 1 - (1 - 9/47)*(1 - 9/46)= 1 - 1 + 9/47 + 9/46 - 9*9/47/46= 9/47 + 9/46(1 - 9/47)  From the answer I found my mistake - this matches combinations  first card hits and second does not (9/47)(46-8)/46 = 0.158186864first and second (9/47)(8/46) = 0.033302498first card does not hit and second does ((47-9)/47)(9/46) = 0.158186864","Creater_id":54616,"Start_date":"2016-08-15 12:01:30","Question_id":229972,"Tags":["combinatorics"],"Answer_count":1,"Last_activity":"2016-08-17 06:16:50","Link":"http://stats.stackexchange.com/questions/229972/combination-versus-fraction","Creator_reputation":196}
{"_id":{"$oid":"5837a57ca05283111e4d4aad"},"View_count":16,"Display_name":"John Richard","Question_score":1,"Question_content":"I know that we use upper class boundaries while sketching ogives but in an example my professor used midpoints while sketching a cumulative frequency polygon. I'm wondering is it okay to do like that.","Creater_id":127994,"Start_date":"2016-08-17 05:53:56","Question_id":230267,"Tags":["frequency"],"Answer_count":0,"Last_activity":"2016-08-17 05:53:56","Link":"http://stats.stackexchange.com/questions/230267/can-midpoints-of-a-class-be-used-instead-of-upper-boundaries-in-cumulative-frequ","Creator_reputation":16}
{"_id":{"$oid":"5837a57ca05283111e4d4aaf"},"View_count":43,"Display_name":"holic","Question_score":2,"Question_content":"Imagene we have a sequence of i.i.d random variables . It is possible to derive the density of  and it is a function of parameters of interest . To have a Maximum-Likelihood estimator all i need to do is\\begin{align*}\\widehat{(p,\\rho)}^{MLE}=\\arg\\max\\limits_{p,\\rho} \\prod\\limits_{t=1}^sf(p,\\rho,y(t))\\end{align*}Sadly  isn't directly observable. With a given dataset it is possible to construct MLE for realisation of .My question is:If  in above equation is replaced by , can the estimator above still be considered as a maximum-likelihood estimator?I tend more to a \"no\" answer, but have no proof.If it isn't MLE anymore, what about asymptotic normality? Hope at least this still usable.","Creater_id":124139,"Start_date":"2016-08-17 05:34:37","Question_id":230263,"Tags":["estimation","maximum-likelihood"],"Answer_count":1,"Last_activity":"2016-08-17 05:45:14","Link":"http://stats.stackexchange.com/questions/230263/maximum-likelihood-estimator","Creator_reputation":31}
{"_id":{"$oid":"5837a57ca05283111e4d4abc"},"View_count":84,"Display_name":"m.stevensson","Question_score":6,"Question_content":"I am currently undertaking a meta-analysis of the expression profile of specific biomarkers in a specific disease. During the extraction process, I extracted the name of the biomarkers and their corresponding p-value for each included study. The p value indicates how “good” this biomarker correlates with the disease. In some studies, only p values in the form of “p \u0026lt; 0.05”, “p.\u0026lt;0.01”, “p\u0026lt;0.001”,”p\u003e0.05\" were provided. Unfortunately, the program I am using (which conducts a p-value based meta-analysis)  doesn’t recognise those p-value forms. One of my supervisor suggested me to convert p\u0026lt;0.05 to 0.05, p\u0026lt;0.01 to 0.01 and p\u003e0.05 to 0.5. However he couldn’t give me a rationale for this.Could someone provide me a rationale on how I should/could convert these p values? I think that reviewers might ask on which basis this conversion was done.","Creater_id":127966,"Start_date":"2016-08-17 02:51:55","Question_id":230237,"Tags":["p-value","meta-analysis","combining-p-values"],"Answer_count":1,"Last_activity":"2016-08-17 05:39:54","Link":"http://stats.stackexchange.com/questions/230237/using-p-values-reported-as-inequalities-such-as-p0-05-in-meta-analysis-can-i","Creator_reputation":33}
{"_id":{"$oid":"5837a57ca05283111e4d4ac9"},"View_count":30,"Display_name":"rnewbie","Question_score":0,"Question_content":"I am teaching myself regression using Regression Modeling Strategies by Harell and the author goes at quite the length to showcase the importance of modeling interactions and transformations of the initial variables. I can't help but wonder how to approach this in a more structured/automated way when dealing with a lot of potential variables. Can we use recursive partitioning, for example, to somehow to do the work for us and then use the output as variables, shrink the estimates with LASSO to deal with colinearity and do a final step where we use some sort of filtering for feature importance. In my mind this will leave us with a well specified model which can be manually inspected and improved if need be, but is this reasonable? Are there other ways to approach this? Are there some resources that deal with problems like this? ","Creater_id":127984,"Start_date":"2016-08-17 05:14:56","Question_id":230257,"Tags":["r","interaction","regression-strategies"],"Answer_count":0,"Last_activity":"2016-08-17 05:28:48","Link":"http://stats.stackexchange.com/questions/230257/discovering-interactions-and-transformations","Creator_reputation":8}
{"_id":{"$oid":"5837a57ca05283111e4d4acb"},"View_count":57,"Display_name":"Nik Bernou","Question_score":0,"Question_content":"Actually I have two groups of data and I want to know whether they are coming from the same population or are they very different ?Here is my data df\u0026lt;- structure(list(Group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Subject = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 15L, 15L, 15L),     Value = c(29.89577946, 29.51885854, 29.77429604, 33.20695108,     32.09027292, 31.90909894, 30.88358173, 30.67547731, 30.82494595,     31.70128247, 31.57217504, 31.61359752, 30.51371055, 30.42241945,     30.44913954, 26.90850496, 0, 0, 0, 0, 0, 28.94047335, 29.27188604,     29.78511206, 28.18475423, 27.54266717, 26.99873401, 29.26941344,     28.50457189, 28.78050443, 31.39038527, 31.19237052, 30.74053275,     28.68618888, 28.42109545, 28.58222544, 28.99337177, 29.31797,     28.4541501, 28.18475423, 27.54266717, 26.99873401, 28.07576794,     28.96344894, 28.48358437, 27.02527663, 27.1308483, 26.96091103,     27.04019758, 27.51900858, 28.14559621, 26.83569136, 26.90724462,     26.82675, 0, 0, 0, 27.62449786, 26.82335228, 26.66925534,     0, 25.81254792, 26.61666776, 26.12545858, 0, 0, 0, 0, 0,     28.84580419, 29.11003424, 29.24723895, 28.72919768, 29.70673437,     29.31274377, 30.73133587, 30.44805655, 30.61561583, 27.06896964,     27.04249553, 27.15990629, 31.54738209, 31.51643714, 31.8055509,     31.291867, 31.89146186, 31.65812735)), .Names = c(\"Group\", \"Subject\", \"Value\"), class = \"data.frame\", row.names = c(NA, -87L))If we perform the following , it shows how many groups there are and how many subjects table(dfSubject) #   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 1 3 3 3 3 3 3 3 3 3  3  3  3  3  3  0 # 2 3 3 3 3 3 3 3 3 3  3  3  3  3  3  3It shows that I have two groups 1 and 2and also I have 14 subjects three replicated for group1and 15 subjects three replicated for group2I appreciate if you can help me to figure this outI have tried different approaches but I was not successful, for example I came across the prop.testbut seems like it is not working with my data structure and of course I don't understand it too Thanks Nik","Creater_id":127951,"Start_date":"2016-08-17 04:55:52","Question_id":230255,"Tags":["r","hypothesis-testing","statistical-significance","multiple-comparisons","population"],"Answer_count":0,"Last_activity":"2016-08-17 04:55:52","Link":"http://stats.stackexchange.com/questions/230255/how-can-i-see-if-my-data-are-coming-from-two-different-population","Creator_reputation":16}
{"_id":{"$oid":"5837a57ca05283111e4d4acd"},"View_count":1589,"Display_name":"Alan H.","Question_score":7,"Question_content":"I hope this isn't either far too basic or redundant. I have been looking around for guidance but so far I am still uncertain of how to proceed.My data consists of counts of a particular structure used in conversations between pairs of interlocutors. The hypothesis I want to test is the following: more frequent use of this structure by one speaker will tend to increase the frequency of the structure by the other speaker (i.e., this might be evidence of a priming effect).So I just have two vectors, the counts for speaker A and the counts for speaker B are the columns, and if they are lined up each row represents a particular conversation, like this:A  B0  10  21  03  10  22  02  1There are about 420 conversations (rows). There are lots of zeros in this data.What would be the best way to analyze this data? I am using R, if that makes a difference.Here is a plot of the frequencies (counts). The x-axis is number of uses by speaker A, the y-axis number of uses by speaker B. The distinction between speakers means only that speaker A spoke first, and there's no special reason why they did. Otherwise the distinction between speaker A and speaker B is basically meaningless:And this is frequency relative to number of sentences spoken by each speaker in each conversation. :(I should mention that I have thrown out conversations with no hits at all, i.e. {0,0}.)","Creater_id":52,"Start_date":"2010-12-05 16:43:10","Question_id":5171,"Tags":["categorical-data","independence"],"Answer_count":3,"Last_activity":"2016-08-17 04:50:26","Link":"http://stats.stackexchange.com/questions/5171/testing-paired-frequencies-for-independence","Creator_reputation":1381}
{"_id":{"$oid":"5837a57ca05283111e4d4adc"},"View_count":17,"Display_name":"Saul Karl","Question_score":1,"Question_content":"I have a requirement to write a program to show a list of stock buyers with their transaction data across time. That is,2016-08-01:buyer 1 =\u0026gt; 3,000 sharesbuyer 2 =\u0026gt; 4,000 shares...buyer 200 =\u0026gt; 1,000,000 shares2016-08-01:buyer 1 =\u0026gt; 0 shares...buyer 201 =\u0026gt; 1,000 sharesThere are a number of scenarios:the number of shares can vary greatly among buyerssome buyers may have sold all their shares and they only have data for specific datesthere can be many buyersA specific requirement is to be able to highlight big changes in the number of shares per buyer across time.What is the appropriate chart for this? I tried the Google Line Chart but since the number of buyers is somewhat big and the number of shares is somewhat similar for them, the lines are plotted closely together. There is also the issue of the spread of the number of shares for different buyers. ","Creater_id":127942,"Start_date":"2016-08-17 04:47:30","Question_id":230253,"Tags":["data-visualization","summary-statistics"],"Answer_count":0,"Last_activity":"2016-08-17 04:47:30","Link":"http://stats.stackexchange.com/questions/230253/what-kind-of-chart-to-use-for-showing-big-stock-transactions","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4ade"},"View_count":38,"Display_name":"Wasser","Question_score":0,"Question_content":"I am trying to simulate survey data based on output from structural equation modeling (SEM) using the \"simsem\" package in R. Because each variable represents a question in a survey I would like the resulting data set to only contain integers, between e.g. 1 - 5. Is there a way to do this within the \"simsem\" package or is there another package more suited for this? I have been thinking that I could assign the integer values after the simulation. E.g. lowest 15% = 1 next 20% = 2 etc reflecting the frequency given by the real data (suggestions to do this in a good way would be appreciated). Any other suggestions?","Creater_id":127118,"Start_date":"2016-08-09 05:39:04","Question_id":228975,"Tags":["r","simulation","survey","sem"],"Answer_count":1,"Last_activity":"2016-08-17 04:45:17","Link":"http://stats.stackexchange.com/questions/228975/integer-values-in-a-defined-range-for-simulated-data","Creator_reputation":1}
{"_id":{"$oid":"5837a57ca05283111e4d4aeb"},"View_count":44,"Display_name":"them","Question_score":3,"Question_content":"I am taking a graduate course in mathematical statistics. Here is what I feel must be a straightforward problem in application of standard tools in \"asymptotics\" (such as application of Slutsky's theorem, Cramer's theorem, and quadratic forms). I am struggling to factorize the problem properly and end up running into cumbersome forbidding expression I am not able to develop. There must be a neat way to solve this problem.  I have little practice solving problems in statistics, any help would be very much welcome!       Suppose we have  iid samples of a bivariate Bernoulli random variable  where  and  take values 0 or 1 and are independent. Define  to be the proportion of the samples such that . Also define  (which is identified as the proportion of samples where ) and similarly . What is the asymptotic distribution of the following statistics: S(n) = \\sum_{i=0}^{1} \\sum_{j=0}^{1}\\frac{n(\\hat{p}_{ij} - \\hat{p}_{i*}\\hat{p}_{*j})^2}{\\hat{p}_{i*}\\hat{p}_{j*}}\\,.Added later: Following some comments, here is an elaboration on the definition of the quantities involved:Let  iid samples of the bivariate Bernoulli , define N_{i,j} := \\{\\text{number of indices } k ~| ~(x= i, y = j)_k\\} with this definition at hand the proportion  is defined to be \\hat{p}_{ij} := \\frac{N_{ij}}{n}\\,.and the asymptotic of  is with respect to the limit . ","Creater_id":94074,"Start_date":"2016-08-16 09:04:52","Question_id":230130,"Tags":["self-study","chi-squared","proportion","asymptotics","bernoulli-distribution"],"Answer_count":1,"Last_activity":"2016-08-17 04:45:10","Link":"http://stats.stackexchange.com/questions/230130/asymptotic-distribution-of-statistics-with-proportions","Creator_reputation":116}
{"_id":{"$oid":"5837a57ca05283111e4d4af8"},"View_count":119,"Display_name":"posdef","Question_score":1,"Question_content":"EDIT: Following a discussion on Meta, I will try to clarify this question more, hopefully giving more context which should help solve some of the issues.I have 12 observations for several hundred thousand variables, e.g. a table with dimensions 300K x 12, coming from an sequencing project. In this context the rows correspond to sequences and columns correspond to samples that were analyzed. The samples, on the other hand, correspond to a number of different processes which were applied to an original biological sample. Here's a graphical representation:The processes are expected to favor some sequences, thus increasing levels of the same process should in theory enrich (i.e. increase in abundance) some sequences while eliminating others. The goal is to be able to pick up sequences that are favored by each individual process, but not by others. I'd like to automate this categorization since: The exact setup of the experiments do vary, thus the solution needs to be as general as possible It would be more exciting to have a known method to rely on, instead of ad hoc analysis of the numbers.If I plot the abundance matrix in R, I can visualize the values as what I refer to \"trends\". (Please ignore the x-axis label here)That figure alone isn't very useful however since the absolute abundance/copy number may be misleading as the total number of sequences per sample is not fixed. Also any classification efforts using k-means/medians or DBSCAN on absolute quantities have not been fruitful.Another way to visualize would be to scale/normalize each value by the column sums and plot the relative representation of each sequence within the sample, which yields the following graph for the first 100 sequences:Here the samples 1-3 correspond to the branch of process I, 4-6 correspond to process II, 7-8 to process IV, 9-11 to process III, and sample 12 is the control/untreated sample. Of noteworthy here are the \"spikes\" around certain samples:3: corresponding to sequences enriched by process I,6: corresponding to sequences enriched by process II,8: corresponding to sequences enriched by process IV,11:corresponding to sequences enriched by process III,as well as sudden increases at 12, corresponding to sequences that exists plenty in the untreated sample but are at selective disadvantage with respect to the process I-IV.Given more detail on the origin and structure of the data, my question is if there are methods which I can use to effectively categorize sequences based on their response to the different processes? It's important that the solution is generally applicable however, since not all projects have the exact same setup.I am inclined to think that I will need to devise an algorithm from scratch for this specific scenario, but I just wanted to make sure that there isn't anything out there already. I am working with Python for data processing, and R for analysis. Suggestions? ","Creater_id":3014,"Start_date":"2016-08-10 05:38:16","Question_id":229165,"Tags":["clustering","biostatistics","trend","bioinformatics","sequence-analysis"],"Answer_count":0,"Last_activity":"2016-08-17 04:36:57","Link":"http://stats.stackexchange.com/questions/229165/categorization-of-variables-based-on-selection-process-response","Creator_reputation":392}
{"_id":{"$oid":"5837a57ca05283111e4d4afa"},"View_count":54,"Display_name":"corey979","Question_score":0,"Question_content":"Are a standard Gaussian and a skew Gaussian nested? I'd say yes, because when we set the skewness parameter  in the skew normal we get the standard Gaussian.Also, are the normal/skew normal and sinh-arcsinh distributions nested? In this case I also think they are, as setting the skewness parameter to zero and the kurtosis parameter to one reduces the sinh-arcsinh distribution to a standard Gaussian.","Creater_id":72352,"Start_date":"2015-05-23 18:48:33","Question_id":153762,"Tags":["normal-distribution","model","nested","non-nested","skew-normal"],"Answer_count":1,"Last_activity":"2016-08-17 04:31:41","Link":"http://stats.stackexchange.com/questions/153762/are-these-models-nested","Creator_reputation":313}
{"_id":{"$oid":"5837a57ca05283111e4d4b07"},"View_count":15,"Display_name":"jakab922","Question_score":0,"Question_content":"So I'm trying to understand a proof for the unbiasedness of the estimator of the variance in the simple linear regression model by which I mean:E(Y_{i}|X=x_{i}) = \\alpha + \\beta \\cdot x_{i} + \\epsilon_{i} Where we have a set of samples .  and  are the parameters we want to estimate and . And we have an estimator for  via the least squares method:\\hat{\\sigma}^2 = \\frac{1}{n} \\left( S_{YY} - \\frac{S_{xY}}{S_{xx}} \\cdot S_{xY} \\right)What I would like to prove is that  has  distribution. Now I proved that  and that the rest of the difference divided by  has . Now the only thing left to prove is that  and  are independent, but I don't know where to start. Some definitions will follow:S_{YY} = \\sum_{i = 1}^{n}{(Y_{i} - \\overline{Y})^2}S_{xx} = \\sum_{i = 1}^{n}{(x_{i} - \\overline{x})^2}S_{xY} = \\sum_{i = 1}^{n}{(x_{i} - \\overline{x}) \\cdot (Y_{i} - \\overline{Y})} and  are normal random variables where . The  variables are independent. s are values from the sample. The line over the letter is the notation for the mean as usual.The problem stems from the ommited proof on page 613  of http://www.math.louisville.edu/~pksaho01/teaching/Math662TB-09S.pdf which claims that the proof of the above can be found in the book of Franklin Graybill which has the title: \"An introduction to linear statistical models\". I was unable to locate the proof there...","Creater_id":117721,"Start_date":"2016-08-17 04:24:04","Question_id":230250,"Tags":["regression","linear-model"],"Answer_count":0,"Last_activity":"2016-08-17 04:24:04","Link":"http://stats.stackexchange.com/questions/230250/independence-proof-for-the-variance-estimator-in-the-simple-linear-regression-mo","Creator_reputation":21}
{"_id":{"$oid":"5837a57ca05283111e4d4b09"},"View_count":35,"Display_name":"user93892","Question_score":0,"Question_content":"We are require to assess the distribution of P-value, so i want to make a qq-plot for P-value against the uniform distribution manually, but I have no idea how to do it. ","Creater_id":93892,"Start_date":"2016-08-17 02:02:43","Question_id":230226,"Tags":["p-value","qq-plot"],"Answer_count":1,"Last_activity":"2016-08-17 04:24:00","Link":"http://stats.stackexchange.com/questions/230226/how-to-manually-make-a-qq-plot-of-p-value-against-uniform-distribution","Creator_reputation":58}
{"_id":{"$oid":"5837a57ca05283111e4d4b16"},"View_count":382,"Display_name":"dmca","Question_score":2,"Question_content":"I am trying to to implement a Bayesian hierarchical Model in R. I have a few predictor variables (2 metric and one categorical) and am trying to predict quarterly home sales in the US. Each sales observation is for a state that is a member of a region, which in turn make up the population as a whole. I am using the rjags package.I want to implement the model so that the coefficients for individual states are drawn from distributions determined by their region, which in turn are determined by the population. Essentially, I want to model this as a three-level hierarchy. However, I only understand how to implement the two-level hierarchy, i.e., state-level coefficients are a function of the population.I have prepared an example that highlights my question and (hopefully) provides a framework for an answer. Below is my code that generates dummy data and implements my model. I would appreciate any insight into how to accomlish this as well as other suggeestions on how to improve my code, model structure, or anything else.# load rjags for modelinglibrary(rjags)# dataNregion \u0026lt;- nlevels(state.region)Nstate \u0026lt;- length(state.abb)Nqtr \u0026lt;- 20N \u0026lt;- Nstate * Nqtr # number of obsy \u0026lt;- runif(N, 4500, 5500) # regresandx1 \u0026lt;- dplyr::lag(y); x1[is.na(x1)] \u0026lt;- 4500 # lag of y as covaritatex2 \u0026lt;- rnorm(N, 100, Nqtr) # second covariatesa1 \u0026lt;- rep(seq(1, 4), Nstate * 5) #qtr of the years \u0026lt;- as.factor(rep(state.abb, Nqtr)) # stater \u0026lt;- as.factor(rep(state.region, Nqtr)) # region# modelmodelstring = \"  model {    for ( i in 1:N ) {      y[i] ~ dnorm(y.hat[i], tau.i)      y.hat[i] \u0026lt;- b0[s[i]] + b1[s[i]] * x1[i] + b2[s[i]] * x2[i] + b3[s[i], a1[i]]    }    for ( s in 1:Nstate ) {      b0[s] ~ dnorm(b0u, b0t)      b1[s] ~ dnorm(b1u, b1t)      b2[s] ~ dnorm(b2u, b2t)      for ( j in 1:Nqtr ) {        b3[s, j] ~ dnorm(0, tau.sq.alpha)      }    }    b0u ~ dnorm(0, .0001)    b0t ~ dgamma(0.001, 0.001)    b1u ~ dnorm(0, .0001)    b1t ~ dgamma(0.001, 0.001)    b2u ~ dnorm(0, .0001)    b2t ~ dgamma(0.001, 0.001)    tau.i ~ dgamma(0.001, 0.001)    tau.sq.alpha ~ dgamma(0.001, 0.001)    sigma.sq.alpha \u0026lt;- 1 / tau.sq.alpha  }\"# write to filewriteLines(modelstring, con=\"model.txt\")# create jags model objectjags \u0026lt;- jags.model('model.txt',                   data = list('x1' = x1,                               'x2' = x2,                               'a1' = a1,                               'y' = y,                               'N' = N,                               's' = s,                               'Nstate' = Nstate,                               'Nqtr' = Nqtr),                   n.chains = 4,                   n.adapt = 100)","Creater_id":96547,"Start_date":"2015-12-05 20:53:00","Question_id":185254,"Tags":["r","regression","multilevel-analysis","hierarchical-bayesian","jags"],"Answer_count":1,"Last_activity":"2016-08-17 04:11:40","Link":"http://stats.stackexchange.com/questions/185254/multi-level-bayesian-hierarchical-regression-using-rjags","Creator_reputation":11}
{"_id":{"$oid":"5837a57ca05283111e4d4b23"},"View_count":41,"Display_name":"user2917781","Question_score":2,"Question_content":"I'm looking at a multiple regression analysis in which I'm only interested in 1 of the several variables included in the model. However, the bivariate associations are not reported. I'm wondering if I can somehow convert the standardized regression coefficient into a pearson r? Maybe there's someway to get a pearson r from the change in R2?","Creater_id":82681,"Start_date":"2016-08-15 12:39:48","Question_id":229979,"Tags":["regression","effect-size"],"Answer_count":1,"Last_activity":"2016-08-17 04:07:02","Link":"http://stats.stackexchange.com/questions/229979/effect-size-e-g-pearson-r-from-multiple-regression-analysis","Creator_reputation":46}
{"_id":{"$oid":"5837a57ca05283111e4d4b26"},"View_count":6,"Display_name":"josh","Question_score":1,"Question_content":"Lets say I have some hourly binary data over a couple days,Datetime            Value2016-01-01 00:00    12016-01-01 01:00    12016-01-01 02:00    02016-01-01 03:00    12016-01-01 04:00    0...I wish to aggregate this data up to a daily resolution and there are two obvious ways. The first is to treat it as intensive data and average over each day. Assuming there are only 5 reads on the first of Jan 2016 we would haveDate          Value2016-01-01    0.6...The second would be by indicating if there was at least one True or at least one False reading within the time period. This could be done by using the binary OR or AND functions respectively. This would giveDate          OR    AND2016-01-01    1     0...As stated in the first example this type of data is intensive - see here for more details on intensive and extensive aggregation. Is there a standard nomenclature for this second type of aggregation of data?","Creater_id":62008,"Start_date":"2016-08-17 03:53:30","Question_id":230245,"Tags":["aggregation"],"Answer_count":0,"Last_activity":"2016-08-17 03:53:30","Link":"http://stats.stackexchange.com/questions/230245/nomenclature-for-aggregation-of-binary-time-series-data-with-and-or-or","Creator_reputation":452}
{"_id":{"$oid":"5837a57ca05283111e4d4b28"},"View_count":39,"Display_name":"user2676173","Question_score":0,"Question_content":"I am using the Momocs package in R  (https://github.com/vbonhomme/Momocs) and I noticed that the LDA (linear discriminant analysis) is performed on PCA results (http://vbonhomme.github.io/Momocs/vignettes/Momocs_speed_dating.html#lda-linear-discriminant-analysis), Why? What is the advantage? Is it possible to use LDA directly on the on the efourier results? ","Creater_id":30658,"Start_date":"2016-08-17 02:50:44","Question_id":230236,"Tags":["r","pca","discriminant-analysis"],"Answer_count":0,"Last_activity":"2016-08-17 03:52:49","Link":"http://stats.stackexchange.com/questions/230236/lda-on-pca-results-in-r-momocs-package","Creator_reputation":37}
{"_id":{"$oid":"5837a57ca05283111e4d4b2a"},"View_count":29,"Display_name":"Jsl","Question_score":2,"Question_content":"Let's say we have a discrete-time Markov chain with a transition matrix . If we know the initial state , we can predict the probabilities of future states by iterating the chain as  = .But what if we know  and also some future state ? What can we say about the probabilities of states ? Besides direct answers, I would appreciate pointers to relevant literature. My Google search was fruitless, probably because I don't know what to look for. ","Creater_id":77100,"Start_date":"2016-08-12 20:09:40","Question_id":229634,"Tags":["markov-process"],"Answer_count":1,"Last_activity":"2016-08-17 03:42:17","Link":"http://stats.stackexchange.com/questions/229634/markov-process-with-future-knowledge","Creator_reputation":111}
{"_id":{"$oid":"5837a57ca05283111e4d4b2c"},"View_count":18,"Display_name":"lapierre1990","Question_score":2,"Question_content":"i want to check factorial and predictive validity of a 24 item questionnaire. For the factorial validity i calculated Measurement Models. Subsequently i wanted to calculate SEMs for predicitve validity.The answers are on a 5-point Likert skala and pretty skewed. So i calculated with MLR. Sample size is 250. There were a few factorsolutions with 3 to 5 latent factors proposed by earlier EFAs. Testing those in Measueremt Models with my sample results in an unacceptable fit. The best solution has a RMSEA of .076 and a CFI of .817.Deleting some items due to low factor loadings or setting factor loading to another latent factor after modification indices can better fit upto Rmsea=.69 / CFI=.90. In this case i still made only changes which still seam senceful because of the content or sample characteristics. Unfortunately this solution leads to unidentified models when going over to SEMs.There is no senceful solution with a good fit. So SEMs are prohibited. Is there still a possibility to calculate some kind of predictive valdity? Maybe after an EFA or with path models?Thanks for your recommendations!","Creater_id":127963,"Start_date":"2016-08-17 03:16:24","Question_id":230241,"Tags":["sem","confirmatory-factor","validity"],"Answer_count":0,"Last_activity":"2016-08-17 03:16:24","Link":"http://stats.stackexchange.com/questions/230241/efa-sem-next-steps-after-bad-fit-of-measurement-models","Creator_reputation":11}
{"_id":{"$oid":"5837a57ca05283111e4d4b2e"},"View_count":31,"Display_name":"Isaac Raymond","Question_score":1,"Question_content":"If I draw five simple random sample observations from a non-normal population, calculate their mean, and repeat 24 more times, will the 25 means I've computed approximate a normal distribution?  The central limit theorem tells us that a sample mean of sufficient size approaches a normal distribution.  My sample means have a size of 5, however.  Does the central limit theorem still apply?","Creater_id":127919,"Start_date":"2016-08-16 16:16:40","Question_id":230181,"Tags":["central-limit-theorem"],"Answer_count":1,"Last_activity":"2016-08-17 03:14:55","Link":"http://stats.stackexchange.com/questions/230181/are-25-sample-means-each-of-size-5-approximately-normally-distributed","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4b30"},"View_count":28,"Display_name":"Fahmida","Question_score":1,"Question_content":"I have four items with binary (yes/no) outcome and one item with nominal outcomes and one respondent can pick at most three of those nominal outcomes. What is the right IRT model for assessing scores in this scenario? I was looking in to grid response and could not find any literature yet. Can anybody tell me if R has any model for grid response?","Creater_id":91416,"Start_date":"2016-08-16 20:49:47","Question_id":230204,"Tags":["irt"],"Answer_count":0,"Last_activity":"2016-08-17 03:02:05","Link":"http://stats.stackexchange.com/questions/230204/item-response-theory-model-for-binary-and-nominal-variables-grid-response","Creator_reputation":4}
{"_id":{"$oid":"5837a57ca05283111e4d4b32"},"View_count":46,"Display_name":"sbmm","Question_score":1,"Question_content":"I have to do two way anova on the RNA-seq data and find out the variability that can be explained by each factor:Time \u0026lt;- factor(rep(1:3,4), levels = 3:1)Sex \u0026lt;- factor(rep(c(\"Female\", \"Male\"), each = 6), levels = c(\"Male\", \"Female\"))design \u0026lt;- model.matrix(~ Sex*Time)I want to calculate the amount of the variance can be explained by factor Sex, the amount of the variance can be explained by factor Time, the amount of the variance can be explained by interaction between Sex and Time.Would this be possible to do it using edgeR, DESeq, or limma? and if yes, how?In more detail, I'm looking for the sum of squares (SS) and Mean squares (MS) for each gene separately like in anova. I need to calculate the variability of each factor and their interactions separately for each gene, like the following example by using anova:res \u0026lt;- anova(lm(values ~ Sex*Time, data))res_ss \u0026lt;- res\"Mean Sq\"res_df \u0026lt;- res\"Df\"*res_ms[4])/(sum(res_ss)+res_ms[4]))[1]while v1 is just the proportion of SS explained by Sex and v2 is a less biased indicator of variance explained in the population by a predictor variable:(SS_factor - df*(MS_residual))/(SS_total + MS_residual)Would this be possible that I do voom normalization on my data and then do exactly the same analysis as above on the voom-normalized results?however I would like to do it also with edgeR to check for the similarities at least between two methods like limma and edgeR, or edgeR and DESeq.Thanks a lot and looking forward.","Creater_id":60433,"Start_date":"2016-08-17 01:59:31","Question_id":230225,"Tags":["anova","variance","two-way"],"Answer_count":0,"Last_activity":"2016-08-17 02:41:12","Link":"http://stats.stackexchange.com/questions/230225/rna-seq-two-factor-anova-how-to-find-the-variability-explained-by-each-of-the","Creator_reputation":33}
{"_id":{"$oid":"5837a57ca05283111e4d4b34"},"View_count":39,"Display_name":"DC2","Question_score":1,"Question_content":"I have modeled the relative mortality of a US cancer cohort, but I want to apply the results to a cohort from the UK (this is the best option as there was no available UK data). Initially I thought the relative mortality would be a good option, but have lately been thinking I should use the excess mortality instead, because given a similar distribution of cause of the death, the absolute impact of cancer is likely the same in the UK as in the US regardless of the base mortality risk. Does anyone have any thoughts on which measure is the better for conversion to a different population than the one under study? ","Creater_id":125040,"Start_date":"2016-08-17 01:26:05","Question_id":230217,"Tags":["survival","hazard","relative-risk","mortality"],"Answer_count":1,"Last_activity":"2016-08-17 02:35:36","Link":"http://stats.stackexchange.com/questions/230217/excess-mortality-versus-relative-mortality","Creator_reputation":11}
{"_id":{"$oid":"5837a57ca05283111e4d4b36"},"View_count":38,"Display_name":"ching","Question_score":0,"Question_content":"i wanted to build a prediction model.Since my data had some missing data, I imputed data with the MICE algorithm.After that I wanted to do a regression with Random Forest.Now I'm kinda stuck because:I wanted to do Multiple Imputation with MICE because I wanted to show consideration for the variance of the missing variables in my model.So I imputed 5 data sets with MICE.If i wanted to do a glm, I would build 5 models(for each imputed data set) and then pool them together. (Meaning in the end i have 1 model and the variance of my parameters will be higher)Now what I want to do is to build a random forest. But I just can't find any strategies for this. Since RF doesn't have parameter estimations, I can't pool them together...Has anyone worked on this before? or any advices what I should do?Best wishes and thank you in advance!I really appreciate any help and answersChing","Creater_id":84119,"Start_date":"2016-08-06 05:14:02","Question_id":228553,"Tags":["regression","random-forest","missing-data","multiple-imputation","mice"],"Answer_count":1,"Last_activity":"2016-08-17 02:34:47","Link":"http://stats.stackexchange.com/questions/228553/perform-random-forest-after-multiple-imputation-with-mice","Creator_reputation":57}
{"_id":{"$oid":"5837a57ca05283111e4d4b38"},"View_count":25,"Display_name":"e-lena","Question_score":1,"Question_content":"I am comparing the percentage of 5 areas with regards to the magnitude of poverty, income distribution, proportion of women etc. Two of the five areas are independent cities and the three other are provinces(one province has two cities, the other has one city and the other is a rural province with no city) and all are part of one administrative region.Usually I compute the percentage of one sector [(example:(woman-headed household) /(total household of a place)] and compare the percentage across the five areas without using an inferential statistics. The comparison is based solely on the percentages.The two independent cities always yielded different percentages(either lower or higher) compared to the three provinces.The inherent difference characteristics between urban and rural areas probably played the part.But  the size of  each two cities is only half of each three provinces. Given that the gathering of data is scientific and unbiased, does the different sizes of population/sample affect the comparison?","Creater_id":127079,"Start_date":"2016-08-17 02:21:03","Question_id":230229,"Tags":["proportion","descriptive-statistics","percentage"],"Answer_count":0,"Last_activity":"2016-08-17 02:21:03","Link":"http://stats.stackexchange.com/questions/230229/comparing-percentages-with-unequal-sample-size","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4b3a"},"View_count":18,"Display_name":"user798719","Question_score":1,"Question_content":"I'm really bad at math.  The context is regularized linear regression, specifically gradient descent.As you can see, there's a regularization term lambda above in purple added to cost function J(theta). J(theta) is the cost. We're trying to minimize J(theta).  So we keep iterating 1000 times or so, each time with the cost function decreasing.  We stop when the cost stops decreasing, which is the minimum of the cost function.  Then we use whatever theta values were set to on the iteration that gave us a cost function of 0.  All I see is that adding the purple regularization term will only serve to increase the cost and make it converge slower, and require more iterations.  But would not the value of theta that minimizes the J(theta) always be the same no matter how many iterations it took?I also do not see how adding a regularization term will decrease the parameter coefficients.  Adding increases their magnitude, no?Or simply put, how is adding that purple term penalizing added features, mathematically speaking? How does regularization shrink parameter coefficients?Similary in the picture below--how does adding 1000 * theta3 making the parameter smaller?  It seems to make it 1000 times larger:Thank you.","Creater_id":72495,"Start_date":"2016-08-17 01:41:54","Question_id":230221,"Tags":["regression","regularization"],"Answer_count":0,"Last_activity":"2016-08-17 02:16:58","Link":"http://stats.stackexchange.com/questions/230221/regularization-how-does-adding-a-regularization-term-shrink-the-coefficients","Creator_reputation":121}
{"_id":{"$oid":"5837a57ca05283111e4d4b3c"},"View_count":38,"Display_name":"Mark Mitchell","Question_score":2,"Question_content":"This question stems from research carried out by Attanasio et al. 2015 : \"Human Capital Development and Parental Investment in India\", but i will use general terms here to make the question clearer.Say i have assumed the following relationship between two \"sets\" of variables: y = \\mu + \\lambda x + \\varepsilon where  and  are vectors of random variables and  is a matrix of parameters. It is assumed that the joint distribution of  is a mixture of two multivariate Normals, \\tau f_A(x) + (1-\\tau)f_B(x)with mean zero. It is further assumed that \\varepsilon \\sim N(0, \\Sigma_{\\varepsilon}) and E[{x}{\\varepsilon '}] = 0My question is then; under these assumptions, how would on go about deriving the distribution of y? Based on the assumptions made, the authors demean the  variables so as to be  left with \\tilde{y} = \\lambda x + \\varepsilon and arrive at the following expression for the distribution of y: p(y) = \\tau \\int  g(\\tilde{y} -\\lambda x)f_A(x)dx + (1-\\tau)  \\int  g(\\tilde{y} -\\lambda x)f_B(x)dx I can't quite see how this solution is arrived at.I would think of using the rule that the PDF of the sum of two random variables is the convolution of the their densities, alongside the fact that when you convolve a density with a mixture you get a mixture of the convolutions, to arrive at something of the form p(\\tilde{y}) = \\tau \\int  g(\\tilde{y} -\\lambda x)f_A(\\lambda x)dx + (1-\\tau)  \\int  g(\\tilde{y} -x)f_B(\\lambda x)dx What is troubling me is the treatment of the 's in this setting. Any comments, pointers or solutions would be greatly appreciated. Thank you in advance.","Creater_id":122240,"Start_date":"2016-08-16 06:58:46","Question_id":230102,"Tags":["normal-distribution","econometrics","gaussian-mixture","convolution"],"Answer_count":0,"Last_activity":"2016-08-17 02:11:52","Link":"http://stats.stackexchange.com/questions/230102/how-to-calculate-the-sum-of-a-function-of-a-gaussian-mixture-and-a-gaussian-va","Creator_reputation":28}
{"_id":{"$oid":"5837a57ca05283111e4d4b3e"},"View_count":136,"Display_name":"Jack Twain","Question_score":1,"Question_content":"I'm looking for a paper that includes a proof that simply training a Restricted Boltzmann Machine and then using the latent features as input to a logistic regression classifier is a correct thing and makes sense.The thing is that I know that training deep belief networks is done in a different way than the described procedure I mentioned above. Since the training involves pretraining, fine-tuning and then using a Softmax function. However, this tutorial from scikit-learn is merely using the procedure I described: http://scikit-learn.org/stable/auto_examples/plot_rbm_logistic_classification.htmlI'm looking for a paper that justifies the procedure in the tutorial. Or at least that justifies that the latent features of a Restricted Boltzmann Machine are better than the original features.","Creater_id":27779,"Start_date":"2015-01-05 18:59:01","Question_id":131361,"Tags":["machine-learning","data-mining","scikit-learn","deep-learning","deep-belief-networks"],"Answer_count":1,"Last_activity":"2016-08-17 02:11:40","Link":"http://stats.stackexchange.com/questions/131361/a-paper-that-proves-using-the-latent-features-of-rbm-as-input-to-logistic-regres","Creator_reputation":1842}
{"_id":{"$oid":"5837a57ca05283111e4d4b40"},"View_count":50,"Display_name":"Gaurav","Question_score":1,"Question_content":"Based on my limited understanding, there are in general two reasons why feature selection is used:Reducing the number of features, to reduce overfitting and improve the generalisation of models.To gain a better understanding of the features and their relationship to the response variables.In most of the articles about feature selection focus is on reducing noise and to find a balance between bias and variance. My question is - Are there any cases where higher noise(not so important features) in model will actually be helpful?","Creater_id":127857,"Start_date":"2016-08-16 22:22:40","Question_id":230210,"Tags":["machine-learning","feature-selection"],"Answer_count":1,"Last_activity":"2016-08-17 01:50:11","Link":"http://stats.stackexchange.com/questions/230210/feature-selection-for-machine-learning","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4b42"},"View_count":15,"Display_name":"Arnaud Roussel","Question_score":1,"Question_content":"I try to perform 2 models after multiple imputation  with MICE (package mice in R) : a logisitic regression and a Cox regression models with random effect. I'd like to get the global pooled effect of the categorical covariates (with more than 2 classes).R gives the pooled coefficients, p values and 95%CI for each class of the categorical covariates. How can I get the overall coefficients, p values and 95%IC of the categorical covariates? I tried to use drop1 that gives the p values of each imputed data sets but I can't pool the results.Thank you for your helpArnaud","Creater_id":127948,"Start_date":"2016-08-17 01:50:05","Question_id":230222,"Tags":["categorical-data","mice","combining-p-values"],"Answer_count":0,"Last_activity":"2016-08-17 01:50:05","Link":"http://stats.stackexchange.com/questions/230222/how-to-get-the-global-effect-of-a-categorical-variable-with-mice","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4b44"},"View_count":70,"Display_name":"stackoverflowuser2010","Question_score":2,"Question_content":"I am trying to reconcile different definitions of the soft-margin SVM cost / loss function in primal form. There is a \"max()\" operator that I do not understand.I learned about SVM many years ago from the undergraduate-level textbook \"Introduction to Data Mining\" by Tan, Steinbach, and Kumar, 2006. It describes the soft-margin primal form SVM cost function in Chapter 5, p. 267-268. Note that there is no mention of a max() operator.  This can be done by introducing positive-valued slack variables ()  into the constraints of the optimization problem. ...  The modified objective function is given by the following equation:        where  and  are user-specified parameters representing the penalty  of misclassifying the training instances. For the remainder of this  section, we assume  = 1 to simplify the problem. The parameter  can be  chosen based on the model's performance on the validation set.     It follows that the Lagrangian for this constrained optimization  problem can be written as follows:        where the first two terms are the objective function to be minimized,  the third term represents the inequality constraints associated with  the slack variables, and the last term is the result of the  non-negativity requirements on the values of the 's.So that was from a 2006 textbook. Now (in 2016), I started reading more recent material on SVM. In the Stanford class for image recognition, the soft-margin primal form is stated in a much different way:   Relation to Binary Support Vector Machine. You may be coming to this  class with previous experience with Binary Support Vector Machines,  where the loss for the i-th example can be written as:    Similarly, on Wikipedia's article on SVM, the loss function is given as:  Where is this \"max\" function coming from? Is it contained in the first two formulas in the \"Introduction to Data Mining\" version? How do I reconcile the old and new formulations? Is that textbook simply out-of-date?","Creater_id":29072,"Start_date":"2016-05-20 12:06:14","Question_id":213687,"Tags":["machine-learning","classification","svm","data-mining","loss-functions"],"Answer_count":1,"Last_activity":"2016-08-17 01:49:10","Link":"http://stats.stackexchange.com/questions/213687/svm-cost-function-old-and-new-definitions","Creator_reputation":390}
{"_id":{"$oid":"5837a57ca05283111e4d4b46"},"View_count":8,"Display_name":"Black Dagger","Question_score":2,"Question_content":"I film the take-off of flies. I have two sets of data. The first set are normal controls. And the second set of flies are physically perturbed. I wish to see the difference in the take of behaviour of these two sets of flies.Now, the way I film these flies is that I release a bunch of them together and I take videos. I had 3 filming session for controls. And I have 10 videos from these three filming sessions. Since I did not mark the flies, I have no way knowing how many distinct flies I can get from each session. So thus I have 10 videos from 3 days or 10 flights from n=3 flies.The process is similar for perturbed flies as well. 10 flights from n=3 flies.I clearly cannot say n=10 flies. That will be a pseudo replication.So how do I compare these two data sets?Can I assume that the 10 observations in these datasets are mutually independent, even though some observations come from the same fly? And the probably run some kind of ANOVA.Or should I just dump this dataset and collect n=10 flights for n=10 flies?","Creater_id":81233,"Start_date":"2016-08-17 01:34:05","Question_id":230220,"Tags":["anova","replication"],"Answer_count":0,"Last_activity":"2016-08-17 01:34:05","Link":"http://stats.stackexchange.com/questions/230220/statistical-test-for-pseudo-replicates","Creator_reputation":111}
{"_id":{"$oid":"5837a57ca05283111e4d4b48"},"View_count":31,"Display_name":"Simon Nusinovici","Question_score":1,"Question_content":"I am using quantile regression to estimate the effect of a categorical variable AG_SEP2 on a response outcome ScoreGSA according to the distribution of this outcome variable, while adjusting on other variables.quantreg \u0026lt;- rq(ScoreGSA ~ AG_SEP2 + Sexe + Jumeau + ZS_POIDS_NAIS_Cat + CMU + CSPTOT, method=\"fn\", c(0.2,0.5,0.8), data=SepGSA)a \u0026lt;- summary(quantreg, se=\"boot\", R=500, cov=TRUE)AG_SEP2 is a 4-class categorical variable, so I get the following output:     Coefficients:                          tau= 0.2      tau= 0.5      tau= 0.8(Intercept)           4.800000e+01  5.400000e+01  5.800000e+01AG_SEP2AG3234_SEP1    9.907373e-11  1.000000e+00  1.000000e+00AG_SEP2AG2431_SEP0   -2.000000e+00 -2.000000e+00 -1.000000e+00AG_SEP2AG2431_SEP1   -7.000000e+00 -5.000000e+00 -2.000000e+00...Now, I would like to test whether two coefficients within a class of AG_SEP2 variable are different or not using Wald test. For example, within the class \"AG2431_SEP1\", does -7 at tau=0.2 is different from -2 at tau=0.8 ? To do so, I use the following formula: W = (beta1 - beta2) / (var(beta1) + var(beta2) - 2*cov(beta1,beta2))My question is: I can get the covariance between coefficients for a given tau with the following code:a[[1]]$covbut how can I get the covariance between coefficients within different tau, for example between tau=0.2 and tau =0.8?Thanks for your help. ","Creater_id":91247,"Start_date":"2016-08-17 01:33:11","Question_id":230219,"Tags":["covariance-matrix","quantile-regression"],"Answer_count":0,"Last_activity":"2016-08-17 01:33:11","Link":"http://stats.stackexchange.com/questions/230219/variance-covariance-matrix-of-coefficients-in-quantile-regression","Creator_reputation":11}
{"_id":{"$oid":"5837a57ca05283111e4d4b4a"},"View_count":218,"Display_name":"cartonn","Question_score":2,"Question_content":"What are some ways that people know to plot a function of three variables, in a way that is relatively easy to understand on paper?Although this could be generalized to pretty much any scenario, I'm specifically looking at a process where I can control pressure, power, and temperature. My output is a single number. I'd be interested to know what kind of answers people come up with.My closest attempt:One could use a regular 3D plot, where two axes represent two of the three independent variables, and the third one represents the output. Then color can be used to indicate the third independent variable. But this plot gets cluttered with information, especially when the input space is sampled well.","Creater_id":28107,"Start_date":"2013-07-16 14:05:07","Question_id":64521,"Tags":["data-visualization"],"Answer_count":3,"Last_activity":"2016-08-17 01:11:03","Link":"http://stats.stackexchange.com/questions/64521/plotting-functions-of-three-variables","Creator_reputation":111}
{"_id":{"$oid":"5837a57ca05283111e4d4b4c"},"View_count":25,"Display_name":"Willie J. C.","Question_score":1,"Question_content":"my interest is to test for the effect of covariate on the failure of water pipe  using length, width and age of pipe as my covariate. from cox model: h(t,x)=h(t)exp(xB) where x=column vector of covariate, B=coefficient of the regression parameter. But my covariate length and width are constant e.g 300mm and 100m, and i have 10yrs failure history i.e. data point is 10, how can i construct a column vector from this covariates as to enable me use the Cox model? ","Creater_id":127871,"Start_date":"2016-08-16 09:45:30","Question_id":230137,"Tags":["survival","cox-model"],"Answer_count":0,"Last_activity":"2016-08-17 00:41:58","Link":"http://stats.stackexchange.com/questions/230137/how-do-i-construct-a-vector-of-constant-covariate-lenght-width-when-using-cox","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4b4e"},"View_count":393,"Display_name":"itsSLO","Question_score":0,"Question_content":"For homework, I was given data to create/train a predictor that uses lasso regression. I create the predictor and train it using the lasso python library from scikit learn.So now I have this predictor that when given input can predict the output.The second questions was to \"Extend your predictor to report the confidence interval of the prediction by using the bootstrapping method.\"I've looked around and found examples of people doing this for the mean and other things.But I am completely lost on how I'm suppose to do it for a prediction. I am trying to use the scikit-bootstrap library.The course staff is being extremely unresponsive, so any help is appreciated. Thank you.","Creater_id":37842,"Start_date":"2015-11-23 14:31:41","Question_id":183230,"Tags":["regression","machine-learning","self-study","confidence-interval","bootstrap"],"Answer_count":1,"Last_activity":"2016-08-17 00:11:39","Link":"http://stats.stackexchange.com/questions/183230/bootstrapping-confidence-interval-from-a-regression-prediction","Creator_reputation":48}
{"_id":{"$oid":"5837a57ca05283111e4d4b50"},"View_count":84,"Display_name":"PRP","Question_score":2,"Question_content":"I want to generate an  matrix where in each element is a processor parameter. Each cell has a value (process parameter) associated with it which can be modeled as a Gaussian random variable with fixed mean and standard deviation.Also there is a spatial co-relation coefficient between each of the cells based on distance given by :\\rho = e^{-\\alpha\\sqrt{(i-k)^2 + (j-l)^2}} \\forall i,j,k,l  \\in [1,N]How do I write a program to generate  a random matrix with the following properties ?","Creater_id":127939,"Start_date":"2016-08-16 22:58:15","Question_id":230212,"Tags":["dataset","random-variable"],"Answer_count":1,"Last_activity":"2016-08-16 23:51:55","Link":"http://stats.stackexchange.com/questions/230212/generate-random-matrix-with-elements-having-spatial-co-relation","Creator_reputation":113}
{"_id":{"$oid":"5837a57ca05283111e4d4b52"},"View_count":29,"Display_name":"user127912","Question_score":1,"Question_content":"help me, I need to make a forecast with the ARMAX model, including as predictors exogenous variable-rate newspapers (2pi * t) / frequency ... The code is as follows ...database \"ultimo2meses.csv\"- link https://www.dropbox.com/s/0w9im0pr1wozdvd/ultimo2meses.csv?dl=0comunas\u0026lt;-read.csv(\"ultimo2meses.csv\",sep=\",\",dec=\".\",header=TRUE)comunasdate, format = \"%Y-%m-%d %H:%M\", \"GMT\"))o33mes\u0026lt;-comunastemperaturao32014 \u0026lt;- ts(o33mes, start=c(2014, 10),frequency=365.25*24)plot(o32014)#****VARIABLES EXOGENAS******acf(o32014)t\u0026lt;-c(1:2208)sen2= 6.74266*sin((2*pi*t)/12) ; cos2=-1.03929*cos((2*pi*t)/12)sen3= -7.50755*sin((2*pi*t)/24) ; cos3=-9.16253*cos((2*pi*t)/24)sen4= -1.49818*sin((2*pi*t)/8768) ; cos4= 3.59926*cos((2*pi*t)/8768)sen5= -3.84711*sin((2*pi*t)/24.06587) ; cos5=-4.82285*cos((2*pi*t)/24.06587)sen6= -4.87610*sin((2*pi*t)/23.93449) ; cos6=-3.36913*cos((2*pi*t)/23.93449)exog=data.frame(tem3meses,sen2,cos2,sen3,cos3,sen4,cos4,sen5,cos5,sen6,cos6)armax\u0026lt;-arima(log(o32014),order=c(4,0,4), xreg=exog)fit1.preds \u0026lt;- predict(armax, n.ahead = 25, newxreg = exog[(2208:2233), ])\"Error\"How to apply predict command in this case.","Creater_id":null,"Start_date":"2016-08-16 15:50:10","Question_id":230178,"Tags":["r"],"Answer_count":0,"Last_activity":"2016-08-16 22:14:10","Link":"http://stats.stackexchange.com/questions/230178/how-to-do-forecast-with-armax-models","Creator_reputation":null}
{"_id":{"$oid":"5837a57ca05283111e4d4b54"},"View_count":71,"Display_name":"Phil","Question_score":1,"Question_content":"Is there a way to establish the size parameter of a negative binomial distribution if I know the mean  and the quantile high density interval estimates (say the bottom 2.5% and the upper 97.5%)? I'd greatly appreciate it if it could be shown how to establish it in R.EDIT: An example to clarify: Suppose I know that  = 50 (red line shown below), and that 95% of the distribution is between 31 and 69 (blue lines shown below). Is there a way that I can determine the size parameter in order to run the rnbinom function to replicate the distribution shown in green?","Creater_id":87002,"Start_date":"2016-08-10 18:39:40","Question_id":229291,"Tags":["distributions","negative-binomial"],"Answer_count":1,"Last_activity":"2016-08-16 21:45:32","Link":"http://stats.stackexchange.com/questions/229291/calculating-the-parameters-of-a-negative-binomial-distribution-given-a-mean-and","Creator_reputation":102}
{"_id":{"$oid":"5837a57ca05283111e4d4b56"},"View_count":330,"Display_name":"Blou91","Question_score":2,"Question_content":"I have an experiment with one factor and five levels. The response is proportion of conversion (ex: 1/11, 1/8...). Doing a power analysis:power.anova.test(5, between.var =.0004, within.var =.14, n = 20)I'm estimating between.var by simply doing var(1/11, 1/8, etc.) and estimating within.var by doing var(1/11). Is that right?If my process is correct:I'm finding that with an ability to only have sample sizes of around ~20 doing a one way ANOVA doesn't tell me much aka power is extremely low.What options do I have for my experiment? Is the Kruskal Wallis test more effective with lower sample sizes or do I just need to figure out how to increase my sample size. Are there any other tests I can do?","Creater_id":59609,"Start_date":"2015-05-14 11:40:25","Question_id":152363,"Tags":["anova","experiment-design","power-analysis","kruskal-wallis"],"Answer_count":2,"Last_activity":"2016-08-16 21:42:04","Link":"http://stats.stackexchange.com/questions/152363/using-kruskal-wallis-vs-one-way-anova-with-small-sample-size","Creator_reputation":56}
{"_id":{"$oid":"5837a57ca05283111e4d4b58"},"View_count":80,"Display_name":"Buck Shlegeris","Question_score":2,"Question_content":"I have a normal distribution with mean 0 and variance 1. I draw twice from it, and get the values  and . What's ?More generally, for what  is there a closed form solution to ?","Creater_id":107796,"Start_date":"2016-08-16 17:38:06","Question_id":230186,"Tags":["normal-distribution","expected-value"],"Answer_count":1,"Last_activity":"2016-08-16 21:19:22","Link":"http://stats.stackexchange.com/questions/230186/average-squared-distance-between-two-samples-of-a-normal-distribution","Creator_reputation":81}
{"_id":{"$oid":"5837a57ca05283111e4d4b5a"},"View_count":28,"Display_name":"Kim","Question_score":0,"Question_content":"A process has a binary result, success or fail. The success chance is 0% until after 199 consecutive fails, at which point, the next trial always succeeds (100% chance). Thus the average probability of success is 1/200.Is this process considered to have varying probability of success or dependence or both?Edit: On hindsight, this process is unlikely to be considered fixed probability of success as p1 ... p199 all have a probability of 0 where as p200 has a probability of 100%. Although it would still be nice to have a confirmation from someone more experienced.Now I am curious of an example of a binomial distribution with fixed probability but positive dependence / covariance.Context:A game's item drop system is designed as such: A counter starts at 0. Every trial increases the counter count by 1. No item is dropped (fail) until the counter count is at 200, at which point an item is dropped (success). The counter is then reset to 0.Research:From statisticshowto.com I have determined that the events must happen in a particular order: 199 failures followed by 1 success. Therefore this process must be considered dependent.However, I am unsure if this is also considered to have varying probability of 0% and 100%, or that because the average probability of success is always 1/200, it is considered to have a fixed success chance.This question is related to the answer I received from my other post:Process with Binary output, how to prove that it is a binomial experiment with fixed or varying probability of success?","Creater_id":127845,"Start_date":"2016-08-16 17:07:30","Question_id":230184,"Tags":["probability","binomial","random-variable"],"Answer_count":0,"Last_activity":"2016-08-16 20:46:08","Link":"http://stats.stackexchange.com/questions/230184/is-this-binary-proccess-considered-to-have-varying-probability-of-success-or-dep","Creator_reputation":41}
{"_id":{"$oid":"5837a57ca05283111e4d4b5c"},"View_count":35,"Display_name":"Yoshihiro Maeda","Question_score":1,"Question_content":"I want to analyze time series variation using cross sectional data.In other words, I want to use cross sectional data like longitudinal data.My data is observed at the same point of time and the data contains these variables.3 objective variablesmany explanatory variablesagegenderquestionnaire data(about 30 variables, likert scale)I think the important point is how to handle \"age\" but I don't have any ideas.Do you know how to analyze growth or time series variation using this cross sectional data?If you know any knowledge or suggestion, would you please advise me how to analyze?Thanks.","Creater_id":115494,"Start_date":"2016-08-16 20:41:39","Question_id":230203,"Tags":["panel-data","graphical-model","cross-section","growth-model"],"Answer_count":0,"Last_activity":"2016-08-16 20:41:39","Link":"http://stats.stackexchange.com/questions/230203/how-to-analyze-time-series-variation-using-cross-sectional-data","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4b5e"},"View_count":35,"Display_name":"addikted","Question_score":3,"Question_content":"If the data for dependent variable is not normally  distributed and when the data is transformed the skewness becomes less than 1 but the Shapiro Wilk test says its not normally distributed so what other remedies are available?Thanks.","Creater_id":127221,"Start_date":"2016-08-16 10:20:20","Question_id":230141,"Tags":["normal-distribution"],"Answer_count":1,"Last_activity":"2016-08-16 19:49:40","Link":"http://stats.stackexchange.com/questions/230141/transforming-non-normal-data","Creator_reputation":16}
{"_id":{"$oid":"5837a57ca05283111e4d4b60"},"View_count":58,"Display_name":"Felix Zhao","Question_score":1,"Question_content":"I was trying to build a random forest model for a dataset in Kaggle, i always doing machine learning with caret package, the dataset has 1.5 million + rows and 46 variables with no missing values (about 150 mb in size), 40+ variables are categorical and the outcome is the response i am trying to predict and it is binary. After some pre-processing with dplyr, I started working on building model with caret package, but i got this error message when i was trying to run the \"train\" function:\"Error: cannot allocate vector of size 153.1 Gb\" Here is my code:## load packagesrequire(tidyr)require(dplyr)require(readr)require(ggplot2)require(ggthemes)require(caret)require(parallel)require(doParallel)## prepare for parallel processingn_Cores \u0026lt;- detectCores()n_Cluster \u0026lt;- makeCluster(n_Cores)registerDoParallel(n_Cluster)## import orginal datasetspeople_Dt \u0026lt;- read_csv(\"people.csv\",col_names = TRUE)activity_Train \u0026lt;- read_csv(\"act_train.csv\",col_names = TRUE)### join two sets together and remove variables not to be usedfirst_Try \u0026lt;- people_Dt%\u0026gt;%   left_join(activity_Train,by=\"people_id\")%\u0026gt;%   select(-ends_with(\"y\"))%\u0026gt;%   filter(!is.na(outcome))## try with random forestin_Tr \u0026lt;- createDataPartition(first_Try$outcome,p=0.75,list=FALSE)rf_Train \u0026lt;- firt_Try[in_Tr,]rf_Test \u0026lt;- firt_Try[-in_Tr,]## set model cross validation parametersmodel_Control \u0026lt;- trainControl(method = \"repeatedcv\",repeats=2,number=2,allowParallel = TRUE)rf_RedHat \u0026lt;- train(outcome~.,               data=rf_Train,               method=\"rf\",               tuneLength=10,               importance=TRUE,               trControl=model_Control)My computer is a fairly powerful machine with E3 processors and 32GB RAM. I have two questions:1. Where did i get a vector that is as large as 150GB? Is it because some codes I wrote?2. I cannot get a machine with that big ram, is there any workarouds to solve the issue that i can move on with my model building process?","Creater_id":97736,"Start_date":"2016-08-16 19:15:07","Question_id":230195,"Tags":["r","random-forest","caret"],"Answer_count":0,"Last_activity":"2016-08-16 19:15:07","Link":"http://stats.stackexchange.com/questions/230195/random-forest-with-caret-package-error-cannot-allocate-vector-of-size-153-1-gb","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4b62"},"View_count":33,"Display_name":"pionpi_","Question_score":1,"Question_content":"Let  be an  matrix. We obtain a matrix  by removing the mean from each column of .Let . Principal components are . What is the interpretation of ?","Creater_id":127807,"Start_date":"2016-08-15 21:46:00","Question_id":230031,"Tags":["pca","centering"],"Answer_count":0,"Last_activity":"2016-08-16 18:36:00","Link":"http://stats.stackexchange.com/questions/230031/applying-pc-rotation-obtained-from-centered-data-to-un-centered-data","Creator_reputation":6}
{"_id":{"$oid":"5837a57ca05283111e4d4b64"},"View_count":5527,"Display_name":"user39531","Question_score":1,"Question_content":"There are four different types of mobile phones (same brand) that are used as stimuli in an experiment.In the first example, if \"mobile phone\" is the independent variablein the study, which one  of the two is appropriate? (i) mobile phoneis the independent variable with four levels or (ii) this study has 4independent variables (since there are 4 different mobile phones)The same webpage is modified into two versions: one with a blue background, and the other version with an orange background. In the second example, if webpage is the independent variable, can wesay that the study has one independent variable with 2 levels or does ithave two independent variables?Kindly clarify.","Creater_id":39531,"Start_date":"2014-04-14 18:06:20","Question_id":93797,"Tags":["anova"],"Answer_count":2,"Last_activity":"2016-08-16 18:29:43","Link":"http://stats.stackexchange.com/questions/93797/levels-or-independent-variables-in-anova","Creator_reputation":60}
{"_id":{"$oid":"5837a57ca05283111e4d4b66"},"View_count":26,"Display_name":"da4l","Question_score":2,"Question_content":"I have got myself confused as to what method will best tell me whether the results from two measuring devices are producing the same results.Basically the subject will wear two devices while performing an activity and we are testing whether the results between the two devices are the same or not.I am not too familiar with the coefficient of variation (CV) often used in sports science research, but could this potentially be used to test whether the CV's produced from each device are similar or not?","Creater_id":127923,"Start_date":"2016-08-16 17:45:52","Question_id":230187,"Tags":["variance","coefficient-of-variation"],"Answer_count":1,"Last_activity":"2016-08-16 18:01:26","Link":"http://stats.stackexchange.com/questions/230187/what-is-the-most-meaningful-method-to-test-whether-two-versions-of-the-same-meas","Creator_reputation":11}
{"_id":{"$oid":"5837a57ca05283111e4d4b68"},"View_count":111,"Display_name":"Mr K","Question_score":0,"Question_content":"A simple question. I know in theory, it is possible to calculate standard deviation for two numbers. I am wondering if it is plausible to do that. My objective is to compare two arbitrary time series data for the same phenomenon and plot mean and standard deviation as error bars for every time point. I know that you could compare the two time series by taking Pearson correlation and such, but I want to compare how much the absolute values were in agreement at every time point. Any insights will be appreciated.Update:  Thank you for the answers. Let us forget about the time series. It is an unnecessary complication. My question is more fundamental. I am doing a biological experiment to measure a biologically relevant quantity, say concentration of a chemical in my cells. Ideally, I would do 3 or 5 or some number of replicates of my experiment to get an estimate of mean and standard deviation. But due to time limitation, complexity of my experiment and costs involved, I can only do two replicates. Now, I end up with two estimates of concentration. No one questioned me when I took the mean of these two quantities. But people were uncomfortable when I calculated the standard deviation. I could understand their concern but I want to get more insights into why it is ok or not ok to take standard deviation in this case? If it is not ok, what are my options?","Creater_id":127910,"Start_date":"2016-08-16 14:55:44","Question_id":230171,"Tags":["standard-deviation"],"Answer_count":2,"Last_activity":"2016-08-16 17:36:50","Link":"http://stats.stackexchange.com/questions/230171/is-it-meaningful-to-calculate-standard-deviation-of-two-numbers","Creator_reputation":3}
{"_id":{"$oid":"5837a57ca05283111e4d4b6a"},"View_count":22,"Display_name":"KevinKim","Question_score":1,"Question_content":"Let  be the variable representing the treatment. Let  be the variable representing the attributes of subject. For example,  is a new medicine that will make people grow taller and  is gender.In order to test the treatment on the response variable , I could do the following 2 experiment.I do randomized assignment to my sample, and run the following regression . As I can observe , so I also add  in the regression.I block the attributes  by first divide my sample into 2 groups based on the value of , then within each subgroup, I do randomized assignment. Then I pool the data together and run the following regression My question is: these 2 regressions look the same, what is the difference of these 2 design? what do I gain by running one extra experiment in the 2nd method with blocking? (I think theoretically speaking, this randomized assignment will make  and any attributes of subjects  independent right?)For example, in the first method, my treatment group () could end up with , where  means Female and  means Male. My control group () is  (for each subject, I flip a coin to determine treatment or control group). Then I run the regression with both  and However, if I were using the second blocking method, I would first group Male together and group female together, then I do randomization within group. I could end up with in the Male group  and . And within Female Group  and . So this is essentially the same as in the Method 1 and this happens with same probability as happened in method 1. And all the data (observation 's) will be the same as collected from method 1.So there should be no difference between these 2 methods","Creater_id":66461,"Start_date":"2016-08-16 12:02:26","Question_id":230156,"Tags":["experiment-design","blocking"],"Answer_count":0,"Last_activity":"2016-08-16 17:02:08","Link":"http://stats.stackexchange.com/questions/230156/blocking-effect-in-experiment","Creator_reputation":1625}
{"_id":{"$oid":"5837a57ca05283111e4d4b6c"},"View_count":1735,"Display_name":"nimcap","Question_score":8,"Question_content":"I am having difficulties to select the right way to visualize data. Let's say we have bookstores that sells books, and every book has at least one category.For a bookstore, if we count all the categories of books, we acquire a histogram that shows the number of books that falls into a specific category for that bookstore.I want to visualize the bookstore behavior, I want to see if they favor a category over other categories. I don't want to see if they are favoring sci-fi all together, but I want to see if they are treating every category equally or not.I have ~1M bookstores.I have thought of 4 methods:Sample the data, show only 500 bookstore's histograms. Show them in 5 separate pages using 10x10 grid. Example of a 4x4 grid:Same as #1. But this time sort x axis values according to their count desc, so if there is a favoring it will be seen easily.Imagine putting the histograms in #2 together like a deck and showing them in 3D. Something like this: Instead of using third axis suing color to represent colors, so using a heatmap (2D histogram): If generally bookstores prefer some categories to others it will be displayed as a nice gradient from left to right.Do you have any other visualization ideas/tools to represent multiple histograms?","Creater_id":760,"Start_date":"2010-08-05 03:03:34","Question_id":1289,"Tags":["data-visualization","pca","histogram"],"Answer_count":2,"Last_activity":"2016-08-16 16:51:15","Link":"http://stats.stackexchange.com/questions/1289/visualizing-multiple-histograms","Creator_reputation":208}
{"_id":{"$oid":"5837a57ca05283111e4d4b6e"},"View_count":110,"Display_name":"julian.marr","Question_score":3,"Question_content":"Suppose I have one data point,  and I estimate the Epanechnikov kernel using bandwith . The formula is then: for I use the code: density(x=0, bw=1, kernel=c(\"epanechnikov\"))Yet, I get that instead of getting the parabola from  to , it has a larger domain. Why is this? ","Creater_id":96388,"Start_date":"2016-08-15 17:33:19","Question_id":230012,"Tags":["r","kernel-smoothing","density-estimation"],"Answer_count":1,"Last_activity":"2016-08-16 16:39:35","Link":"http://stats.stackexchange.com/questions/230012/epanechnikov-in-r","Creator_reputation":33}
{"_id":{"$oid":"5837a57ca05283111e4d4b70"},"View_count":154,"Display_name":"Jacobadtr","Question_score":5,"Question_content":"I am using a non linear least squares method to fit an analytical function to some experimental data. I have to provide some initial guess values to the algorithm, so I am trying to figure out how to do this automatically (rather than by eye, which is what I have been doing).This is some simulated data, created by adding normally distributed random noise to the analytical functionI am trying reliably detect the position of this step change in the data. I have had some limited success by calculating the mean variance in the data points and looking for points in the data that differ significantly from this value, but this approach seems very limited by the signal to noise ratio.I am hoping for some direction on what I need to look into to solve my problem, as I don't know much statistics at all.Thank you!-Edit paste bin link to xy datahttp://pastebin.com/QTawFex3","Creater_id":127724,"Start_date":"2016-08-15 06:36:43","Question_id":229914,"Tags":["time-series","variance"],"Answer_count":4,"Last_activity":"2016-08-16 16:19:37","Link":"http://stats.stackexchange.com/questions/229914/step-change-detection","Creator_reputation":128}
{"_id":{"$oid":"5837a57ca05283111e4d4b72"},"View_count":33,"Display_name":"Syrtis Major","Question_score":2,"Question_content":"To be brief, my question is what could we do when we have an estimator with a known bias.I want to estimate the parameter  in a distribution model. I select an estimator  (e.g. maximum likelihood method), unfortunately according to the training data , I find this estimator is biased , where a is a constant, e.g. . To correct this bias, I’d like to use  as a new unbiased estimator, does this make sense? Is there any terminology for this reduction?And what if we have , where a is constant, then make a correction ?","Creater_id":95627,"Start_date":"2016-08-10 23:57:58","Question_id":229309,"Tags":["estimation","bias","bias-correction"],"Answer_count":0,"Last_activity":"2016-08-16 16:11:53","Link":"http://stats.stackexchange.com/questions/229309/bias-correction-for-estimator-with-known-bias","Creator_reputation":131}
{"_id":{"$oid":"5837a57ca05283111e4d4b74"},"View_count":92,"Display_name":"lacerbi","Question_score":2,"Question_content":"I recently started using Pareto smoothed importance sampling leave-one-out cross-validation (PSIS-LOO), described in these papers:Vehtari, A., \u0026amp; Gelman, A. (2015). Pareto smoothed importance sampling. arXiv preprint (link).Vehtari, A., Gelman, A., \u0026amp; Gabry, J. (2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. arXiv preprint (link)This represents a very enticing approach to out-of-sample model evaluation as it allows to perform LOO-CV with a single MCMC run, and it is allegedly better than existing information criteria such as WAIC.PSIS-LOO has a diagnostics to tell you whether the approximation is reliable, namely given by the estimated exponents  of the Pareto distributions fitted to the tails of the empirical distributions of importance weigths (one weight per data point). In short, if an estimated weight , bad things can happen.Sadly, I found that in my application of this method to my problem, for the majority of models of interest I find that a large fraction of the . Unsurprisingly, some of the reported LOO log-likelihoods were quite obviously nonsensical (compared to other datasets). As a double-check, I performed a traditional (and time consuming) 10-fold cross-validation, finding that indeed in the above case PSIS-LOO was giving awfully wrong results (on the upside, results were in very good agreement with 10-fold CV for the models in which all ). For the record, I am using the MATLAB implementation of PSIS-LOO by Aki Vehtari.Maybe I am just very unlucky in that my current and first problem in which I apply this method is \"difficult\" for PSIS-LOO, but I suspect that this case might be relatively common. For cases such as mine,  the Vehtary, Gelman \u0026amp; Gabry paper simply says:  Even if the PSIS estimate has a finite variance, when , the  user should consider sampling directly from  for the  problematic , use -fold cross-validation, or use a more robust  model.These are obvious but not really ideal solutions as they are all time consuming or require additional fiddling (I appreciate that MCMC and model evaluation are all about fiddling, but the less the better).Is there any general method that we can apply beforehand to try and prevent PSIS-LOO from failing? I have a few tentative ideas, but I wonder if there is already an empirical solution that people have been adopting.","Creater_id":80479,"Start_date":"2016-03-02 10:13:19","Question_id":199572,"Tags":["machine-learning","cross-validation","mcmc","pareto-distribution","importance-sampling"],"Answer_count":1,"Last_activity":"2016-08-16 15:28:22","Link":"http://stats.stackexchange.com/questions/199572/preventing-pareto-smoothed-importance-sampling-psis-loo-from-failing","Creator_reputation":1737}
{"_id":{"$oid":"5837a57ca05283111e4d4b76"},"View_count":19,"Display_name":"mlinegar","Question_score":0,"Question_content":"I've been trying to automatically find low periods in some data that I have. The data is structured as hourly observations across a period of two years. Thus far I've experimented with a number of approaches (in particular, Twitter's AnomalyDetection package), without much success. My current approach is to first fit a distribution to each hour, and then to use a copula to combine those distributions into a joint, daily, distribution. However, no matter what distributions I pick, I wind up with the same problem - the daily data all appear to be very unlikely to occur (see the final graph in the code below). I have a number of questions (how to select the right copula, whether this approach is even valid, whether another approach might be better suited to this - in particular, Renato suggested here that I try a Generalized Additive Model), but mostly would like to know what (if anything) I'm doing wrong in the code below.Thank you for your help! Any thoughts or suggestions are much appreciated.   # first load the data     library(repmis)    dropFile \u0026lt;- \"https://dl.dropboxusercontent.com/u/428109976/testData.csv\"    testData \u0026lt;- repmis::source_data(dropFile, sep = \",\", header = TRUE, stringsAsFactors = FALSE)    testDatatime \u0026lt;- lubridate::mdy_hm(testDatatime))){      thishour \u0026lt;- which(hour(testDataobs, \"norm\", start = c(1,1))      par(mfrow = c(2, 2),oma=c(0,0,2,0))      plot.legend \u0026lt;- \"Normal\"      denscomp(list(fw), legendtext = plot.legend)      qqcomp(list(fw), legendtext = plot.legend)      cdfcomp(list(fw), legendtext = plot.legend)      ppcomp(list(fw), legendtext = plot.legend)      title(paste(\"For Hour\", i), outer = TRUE)    }    # clearly a normal isn't the way to go - better would probably be a weibull or GPD, but this serves as a good example    # maybe a truncated distribution    # from here we'll contruct a joint distribution using copulas    library(copula)    # now capture the distributions fitted to each hour    groupDist \u0026lt;- function(x, time.colname=\"time\", data.colname=\"obs\", distribution, timeFUN = hour, startlist = NULL){      timeFUN \u0026lt;- match.fun(timeFUN)      distList \u0026lt;- list()      for (i in unique(timeFUN(x[,time.colname]))){        timeiter \u0026lt;- which(timeFUN(x[,time.colname])==i)        fg \u0026lt;- fitdist(x[timeiter, data.colname], distribution, start = startlist)        distList[[i + 1]] \u0026lt;- fg      }      distList    }    fitList \u0026lt;- groupDist(testData, distribution = \"norm\", startlist = c(1,1))    #     param_list \u0026lt;- list()    for (i in seq_along(fitList)){      param_list[[i]] \u0026lt;- list(mean = as.numeric(fitList[[i]]estimate[2]))    }    # now let's reorder the data so that it fits well into copula functions    testDatatime)    testDatatime)    testData$time \u0026lt;- NULL    library(tidyr)    wide_data \u0026lt;- spread(testData, hour, obs)    # for now we'll remove all rows with missing data    wide_data \u0026lt;- wide_data[complete.cases(wide_data[, names(wide_data) != \"time\"]),]    # this data doesn't fit in [0,1] as needed for `copula`, so we use pobs()    pData \u0026lt;- pobs(wide_data[, names(wide_data) != \"time\"])    # now we want to fit a copula to the data    # I'm not sure how to select which copula to use, but I suspect I need an Archimedean    # as dim \u0026gt; 2    # let's arbitrarily choose a Frank copula for now    library(copula)    frank_model \u0026lt;- archmCopula(\"frank\", dim = ncol(pData))    fit.f \u0026lt;- fitCopula(frank_model, pData, method = 'ml')    coef(fit.f)    library(copula)    frank_model \u0026lt;- archmCopula(\"frank\", dim = ncol(pData))    fit.f \u0026lt;- fitCopula(frank_model, pData, method = 'ml')    coef(fit.f)    # creating our joint distribution given the marginal distributions and our copula    myMvd.frank \u0026lt;- mvdc(copula = frankCopula(param = as.numeric(coef(fit.f)), dim = ncol(pData)),                         margins = rep(\"norm\", ncol(pData)), paramMargins = param_list)    # finally, we have the CDF of the joint distribution    cdf_frank \u0026lt;- pMvdc(pData, myMvd.frank)    # let's plot to see what things look like    library(ggplot2)    # why are so many of the days so unlikely? this seems wrong    # I've seen this no matter my selectin of distribution and (archimedean) copula    # so far I've tried fitting truncated normal, weibull, truncated lognormal, truncated logis,    # gamma and exponential distributions but nothing seems to work    ggplot(as.data.frame(cdf_frank), aes(cdf_frank, ..density..)) +      stat_bin(bins = 100) +      ggtitle(\"CDF of Frank Copula over all daily observations\")","Creater_id":86065,"Start_date":"2016-08-16 15:23:40","Question_id":230175,"Tags":["r","distributions","fitting","joint-distribution","copula"],"Answer_count":0,"Last_activity":"2016-08-16 15:23:40","Link":"http://stats.stackexchange.com/questions/230175/using-copulas-to-fit-hourly-observations-to-daily-data","Creator_reputation":15}
{"_id":{"$oid":"5837a57ca05283111e4d4b78"},"View_count":176,"Display_name":"Lena","Question_score":3,"Question_content":"My data set includes 400 records. Each record comprises values for the binary outcome variable  and 12 categorical predictor variables , most of which are binary too. The records originate from 10 different studies, of which one is much larger than the remaining ones (it contributes nearly three-third of the records, while each of the remaining studies contributes 4 to 30 records). My plan is to fit a logistic regression model. Since I cannot rule out that records from the same study are correlated, I consider using GEEs instead of ordinary logistic regression modeling. The records from the same study would form the clusters. As working correlation structure I would use 'exchangable'. However, I am still unsure if GEEs are a suitable approach in my case, which is distinguished by the low number of clusters (viz., ten) the large differences in cluster size. Are there any advices/experiences on this? I found in the literature that actually 40, 50 or even more clusters would be needed (opinions seem to diverge) in order to get reliable 'sandwich' errors for the model parameter estimates. For smaller numbers of clusters, some correction would be needed. However, I neither found much on how to precisely apply this correction, nor on the second issue, if differences in cluster size affects the results of GEEs. I would highly appreciate any advice.","Creater_id":37559,"Start_date":"2014-01-20 08:37:55","Question_id":82836,"Tags":["gee"],"Answer_count":1,"Last_activity":"2016-08-16 15:07:09","Link":"http://stats.stackexchange.com/questions/82836/gees-in-case-of-a-small-number-of-clusters-with-strongly-heterogeneous-cluster-s","Creator_reputation":16}
{"_id":{"$oid":"5837a57ca05283111e4d4b7a"},"View_count":14,"Display_name":"tintinthong","Question_score":1,"Question_content":"It is known that a two-factorial design is balanced iff each combination of the level of the two factors have the same replication. ie A has 2 levels, B has 2 levels then the replication for treatment A1B1,A2B2,A1B2,A2B1 are all the same. But, how is balanced considered inside an ANCOVA design? For example, if I have a model with a factor A of 2 levels and one continuous covariate,x. Does it mean that the design is balanced if A1 and A2 have the same replication(ignoring the continuous covariate)?","Creater_id":121671,"Start_date":"2016-08-16 15:03:14","Question_id":230172,"Tags":["experiment-design","ancova"],"Answer_count":0,"Last_activity":"2016-08-16 15:03:14","Link":"http://stats.stackexchange.com/questions/230172/what-is-the-definition-of-balance-for-ancova-designs","Creator_reputation":117}
{"_id":{"$oid":"5837a57ca05283111e4d4b7c"},"View_count":5031,"Display_name":"Washington S. Silva","Question_score":54,"Question_content":"Why continue to teach and use hypothesis testing (with all its difficult concepts and which are among the most statistical sins) for problems where there is an interval estimator (confidence, bootstrap, credibility or whatever)? What is the best explanation (if any) to be given to students? Only tradition? The views will be very welcome.","Creater_id":523,"Start_date":"2011-02-07 11:05:50","Question_id":6966,"Tags":["hypothesis-testing","teaching","interval"],"Answer_count":8,"Last_activity":"2016-08-16 14:51:59","Link":"http://stats.stackexchange.com/questions/6966/why-continue-to-teach-and-use-hypothesis-testing","Creator_reputation":451}
{"_id":{"$oid":"5837a57ca05283111e4d4b7e"},"View_count":32,"Display_name":"Jessie","Question_score":0,"Question_content":"During the Gibbs sampling, every step it will sample the topic label for a given word in a given doc based on the join probability (excluding the word in the target document): 1) How much the i-th doc likes a particular topic ; 2) how much the topic  likes the word . So the joint probability is given by , where  is the length of the i-th document,  is the count of word  in topic , and  and  are the smoothing parameters (e.g. the hyperparameter for prior Dirichlet Distribution. However, when the length of every document reduced to 1, then the first term in the joint probability is going to be . Not sure if the output of the model is still valid. The log-likelihood is still convergent over iterations.","Creater_id":127904,"Start_date":"2016-08-16 14:49:12","Question_id":230168,"Tags":["machine-learning","text-mining","topic-models"],"Answer_count":0,"Last_activity":"2016-08-16 14:49:12","Link":"http://stats.stackexchange.com/questions/230168/what-will-happen-in-lda-model-if-the-length-of-document-reduce-to-a-single-word","Creator_reputation":4}
{"_id":{"$oid":"5837a57da05283111e4d4be8"},"View_count":28,"Display_name":"Peter234","Question_score":2,"Question_content":"I'm programming a neural network for MNIST-Recognition. My net has a pretty good performance, with accuracy \u003e 98% on test set. But the training is very slow. So I thought it would be faster if I scale the data set to the range [-0.9, 0.9] (I use tanh as activation function).The net trains faster with scaled data but the accuracy is now only 97%. I don't know why the performance is worse with scaled data, can you explain that to me?","Creater_id":127709,"Start_date":"2016-08-16 13:58:31","Question_id":230166,"Tags":["neural-networks","python","backpropagation"],"Answer_count":0,"Last_activity":"2016-08-16 14:29:58","Link":"http://stats.stackexchange.com/questions/230166/scale-mnist-data-to-0-9-0-9","Creator_reputation":80}
{"_id":{"$oid":"5837a57da05283111e4d4bea"},"View_count":20,"Display_name":"Ytsen de Boer","Question_score":1,"Question_content":"My goalTo demonstrate the importance of choosing the right degrees of freedom while evaluating a fit to data using a  test.My approachMy approach is to often repeat an experiment, in which:a small number of draws  is done from a normal distribution evaluate  evaluate  Note uses the knowledge of the real mean and variance, leaving  degrees of freedom. uses the data itself to evaluate estimates of  and , using up 2 degrees of freedom, leaving .My expectationWhen I plot the distributions, I expect to see two different  distributions, one with  degrees of freedom and another with .My resultMy questionWhy do I not see a nice  but rather this very narrow distribution around ?My codeimport numpy as npimport matplotlib.pyplot as pltplt.ion()# Plot many pointsexperiment_count = 100000# Number of points to draw (data points)draw_count = 7# Parameters of underlying distributionmu = 5sd = 2chi2_nu = np.zeros(experiment_count)chi2_nu_min_2 = np.zeros(experiment_count)for i in range(experiment_count):    # Generate nu random numbers (data points)    ran = np.random.normal(mu, sd, draw_count)    # Normalise (not using restrictions) ...    ran_nu = (ran - mu) / sd    # ... and calculate chi2    chi2_nu[i] = np.sum(ran_nu * ran_nu)    # Normalise (using 2 degrees freedom) ...    mu_i = np.mean(ran)    sd_i = np.std(ran)    ran_nu_min_2 = (ran - mu_i) / sd_i    # ... and calculate chi2    chi2_nu_min_2[i] = np.sum(ran_nu_min_2 * ran_nu_min_2)# Now plot both distributionsplt.figure()# Expected a chi2 distribution with degrees of freedom# (nu) equal to draw_counthist = plt.hist(chi2_nu, 50, label=r'')# Expected a chi2 distribution with degrees of freedom# (nu) equal to draw_count - 2, since we used up 2 degrees# of freedom in calculating the mean and variance.hist = plt.hist(chi2_nu_min_2, 50, label=r'')plt.legend()# Why is the second plot so heavily peaked around# the draw count, is it me or numpy?import ipdbipdb.set_trace()EditThis post truly began as a statistical question, because I was not sure whether the discrepancy was with my code or with my math. I sought to first find out if it was with the latter.Let me know what I can do to make this post more agreeable with this platform.","Creater_id":109818,"Start_date":"2016-08-12 02:26:52","Question_id":229487,"Tags":["chi-squared","python","goodness-of-fit","numpy"],"Answer_count":0,"Last_activity":"2016-08-16 14:10:44","Link":"http://stats.stackexchange.com/questions/229487/generating-chi2-distribution-numpy-puzzle","Creator_reputation":146}
{"_id":{"$oid":"5837a57da05283111e4d4bec"},"View_count":5031,"Display_name":"levesque","Question_score":6,"Question_content":"The statistics book I am reading recommends omega squared to measure the effects of my experiments. I have already proven using a split plot design (mix of within-subjects and between-subjects design) that my within-subjects factors are statistically significant with p\u0026lt;0.001 and F=17.Now I'm looking to see how big is the difference... is there an implementation of omega squared somewhere for R (or python? I know... one can dream ;) Searching on the internet for R-related stuff is a pain the *, I don't know how I manage to find stuff with C.thanks!","Creater_id":1320,"Start_date":"2010-09-21 21:38:22","Question_id":2962,"Tags":["r","anova","effect-size","split-plot"],"Answer_count":4,"Last_activity":"2016-08-16 14:05:11","Link":"http://stats.stackexchange.com/questions/2962/omega-squared-for-measure-of-effect-in-r","Creator_reputation":380}
{"_id":{"$oid":"5837a57da05283111e4d4bfc"},"View_count":49,"Display_name":"achompas","Question_score":2,"Question_content":"I'm playing around with implementing an MCMC sampler for predicting arrival counts for the next future time interval, using information about previous arrival times.I'm using a simple Gamma/Poisson distribution: \\lambda_{t+1} ~ | ~ y_{t} \\sim Gamma(y_{t}, \\theta) y_{t+1} \\sim Poisson(\\lambda_{t+1})where  (since I am only conditioning on the previously-observed arrival count).Here's a simple algorithm describing my approach.for arriving_thing in arriving_things:    ## hyperparam k = mean of the last two arrival counts    k = avg(arriving_thing.arrival_count[-2:])    theta = 1    count_hats = []    for i in iterations:        # update k, theta if we have count estimates, otherwise use prior        if count_hats:            k += sum(count_hats)            theta = theta / (len(count_hats) * theta + 1.0)        my_lambda = draw_from_gamma(k, theta)        count_hat = draw_from_poisson(my_lambda)        count_hats.append(count_hat)# toss out the warm up drawsreturn count_hats[warm_up:]I have several concerns about this approach:Is this a convoluted approach to a simple problem? I'm aware that I could use empirical Bayes estimation instead of sampling from the posterior, but my distribution might become complicated in future versions.On that note, should I update  and  with each newly-drawn count estimate? I always get mixed up with this when working with MCMC sampling.What are your thoughts on averaging two previous arrival counts for each arriving thing to obtain hyperparameters for that arriving thing?","Creater_id":25660,"Start_date":"2016-08-12 18:18:17","Question_id":229631,"Tags":["bayesian","poisson","mcmc","gamma-distribution"],"Answer_count":0,"Last_activity":"2016-08-16 14:05:07","Link":"http://stats.stackexchange.com/questions/229631/building-a-gamma-poisson-sampler-to-predict-arrival-counts","Creator_reputation":98}
{"_id":{"$oid":"5837a57da05283111e4d4bfe"},"View_count":21,"Display_name":"Tarek Soukieh","Question_score":1,"Question_content":"Our success measure (satisfaction) have dropped in July compared to June. I want to understand the factors that affected the drop. Since I could not create a flag that identify records that contributed to the drop vs. the one that did not, I came up with another way. I developed a regression model for June and another separate model for July, and used the change in each variable coefficient to explain the drop in outcome.I have good sample size in each month (more than 20k), and I used the same variables in both months models.1- Is that a good approach? 2- All variables were significant in both months except one, it was significant in one month but not significant in the other month, how can I interpret that? Can I say the variable improved and it is now significant in impacting our success measure?Thanks.","Creater_id":79085,"Start_date":"2016-08-16 13:25:28","Question_id":230164,"Tags":["regression","regression-coefficients","operations-research"],"Answer_count":0,"Last_activity":"2016-08-16 13:25:28","Link":"http://stats.stackexchange.com/questions/230164/comparing-coefficients-to-understand-the-change-in-impact-of-a-variable","Creator_reputation":27}
{"_id":{"$oid":"5837a57da05283111e4d4c00"},"View_count":144,"Display_name":"nouse","Question_score":0,"Question_content":"I was asked to check a small dataset (3 replicates for 5 treatments, each) for significant outcome differences.I give an example:The variance within replicates in the data cannot be assumed equal, so i thought a pairwise t-test is not appropriate. For another dataset (60 replicates per treatment), i used glht() from the multcomp-package in R, but i was told the sample size of my current data set is too small for it too work.The question is, are there even tests that are suitable for nonparametric data with large within-replicate-variance and small sample sizes?If so, is it possible to get a pairwise comparison chart output? I thought that agricolae's kruskal() comes in very handy, but i have no idea if kruskal is very good for my problem.Any alternatives (if there are any) are much welcome.","Creater_id":53848,"Start_date":"2015-06-19 06:19:40","Question_id":157742,"Tags":["hypothesis-testing","nonparametric","multiple-comparisons","small-sample"],"Answer_count":2,"Last_activity":"2016-08-16 13:24:10","Link":"http://stats.stackexchange.com/questions/157742/pairwise-non-parametric-test-for-small-sample-sizes","Creator_reputation":124}
{"_id":{"$oid":"5837a57da05283111e4d4c0e"},"View_count":47,"Display_name":"Rali","Question_score":2,"Question_content":"In many textbooks or journal papers, I encounter the term 'statistical process'. Sometimes it's referred to more specifically; i.e. AR process, random walk process, spatial process, ... etc. For a long time, I interpreted the word 'process' as random variable. Is my interpretation correct? What does the word 'process' mean/refer to?","Creater_id":127896,"Start_date":"2016-08-16 11:49:09","Question_id":230152,"Tags":["time-series","terminology","stochastic-processes"],"Answer_count":1,"Last_activity":"2016-08-16 13:05:36","Link":"http://stats.stackexchange.com/questions/230152/what-does-the-term-statistical-process-mean","Creator_reputation":11}
{"_id":{"$oid":"5837a57da05283111e4d4c1b"},"View_count":38,"Display_name":"B_Miner","Question_score":3,"Question_content":"From the paper  Equivalence and Noninferiority Testing in Regression    Models and Repeated-Measures Designs the following graphic is taken.I am interested in a non-inferiority test between two proportions (difference), say  and  where T is the new treatment and S is the standard. I am willing to accept that the new treatment is not more than 0.05 worse than the standard. I also have a variable I need to control for and thus want to use regression. Call this variable .So, I believe according to this graphic that if I fit a logistic regression model : where Treatment is coded 1 for treatment and 0 for standard, will an exponentiation of the lower side of the confidence interval for  , if larger than -0.05 conclude that treatment is non-inferior to the standard?","Creater_id":2040,"Start_date":"2016-08-16 12:55:58","Question_id":230162,"Tags":["regression","statistical-significance","clinical-trials","non-inferiority"],"Answer_count":0,"Last_activity":"2016-08-16 12:55:58","Link":"http://stats.stackexchange.com/questions/230162/noninferiority-using-logistic-regression","Creator_reputation":1098}
{"_id":{"$oid":"5837a57da05283111e4d4c1d"},"View_count":21,"Display_name":"Eren","Question_score":1,"Question_content":"I was wondering, is there a rule of thumb for the number of observations used for the estimation of a GARCH model. Second, is more data always (or most of the time) better (in the sense that it leads to more accurate estimation of the parameters)?","Creater_id":98417,"Start_date":"2016-08-16 12:34:39","Question_id":230158,"Tags":["estimation","garch"],"Answer_count":1,"Last_activity":"2016-08-16 12:36:27","Link":"http://stats.stackexchange.com/questions/230158/amount-of-observations-used-to-estimate-garch-model","Creator_reputation":44}
{"_id":{"$oid":"5837a57da05283111e4d4c2a"},"View_count":563,"Display_name":"hxd1011","Question_score":10,"Question_content":"In linear regression (squared loss), using matrix we have a very concise notation for the objective \\text{minimize}~~ \\|Ax-b\\|^2Where  is the data matrix,  is the coefficients, and  is the response.Is there similar a matrix notation for logistic regression objective? All the notations I have seen cannot get rid of the sum, over all data points (something like ).EDIT: thanks for joceratops and AdamO's great answer. Their answer helped me to realize that another reason linear regression have a more concise notation is because the definition of the norm, which encapsulate the square and the sum or . But in logistic loss, there is not such  definition, which makes notation a little bit more complicated. ","Creater_id":113777,"Start_date":"2016-08-09 09:05:14","Question_id":229014,"Tags":["regression","logistic","linear-model","notation"],"Answer_count":2,"Last_activity":"2016-08-16 12:27:02","Link":"http://stats.stackexchange.com/questions/229014/matrix-notation-for-logistic-regression","Creator_reputation":4443}
{"_id":{"$oid":"5837a57da05283111e4d4c38"},"View_count":39,"Display_name":"Nikolay","Question_score":1,"Question_content":"Suppose we have a dataset where the majority of the features are categorical. Do tree-based methods (such as decision trees, random forests and boosting) generally outperform other classification models on such a dataset?I am aware of this question that discusses some of the potential computational advantages of using tree-based methods with categorical variables. My question is about accuracy/precision and not  concerned with computational time.","Creater_id":127607,"Start_date":"2016-08-14 09:44:52","Question_id":229801,"Tags":["classification","categorical-data","random-forest","cart","boosting"],"Answer_count":0,"Last_activity":"2016-08-16 12:11:23","Link":"http://stats.stackexchange.com/questions/229801/tree-based-methods-performance-on-datasets-with-many-categorical-variables","Creator_reputation":6}
{"_id":{"$oid":"5837a57da05283111e4d4c3a"},"View_count":343,"Display_name":"ManuParra","Question_score":0,"Question_content":"I'm using decision trees in R, with    library(\"party\")    ctree.dt \u0026lt;- ctree(Pasillo_Ataque_2T ~ Dist_Col_1T + Dist_1T_2T, data=dt)This produces:It's correct, but I'd like ctree to start with the root node Dist_Col_1T, not Dist_1T_2T like in the image.In SPSS, I can force start a tree with a specific node variable.","Creater_id":79303,"Start_date":"2015-06-09 00:59:01","Question_id":156097,"Tags":["r","classification","cart"],"Answer_count":1,"Last_activity":"2016-08-16 11:59:09","Link":"http://stats.stackexchange.com/questions/156097/force-start-with-specific-root-node-using-decision-trees-in-r","Creator_reputation":103}
{"_id":{"$oid":"5837a57da05283111e4d4c46"},"View_count":22,"Display_name":"Massinissa","Question_score":1,"Question_content":"In [1] an algorithm based on statistical modeling of the frequency bins of a signal in order to determine if it contains speech or not. There is two hypothesis H0 (speech absent) and H1 (speech present) under which the probability density function (pdf) is calculated for each frequency bin.The Likelihood Ratios of these pdf's are then computed and the geometric mean is compared to a certain threshold .I wanted to know how to set the threshold  to decide whether or not rejecting the null hypothesis. I looked into type I error but I am not sure if it is the right direction.Reference:[1] Sohn, Jongseo, Nam Soo Kim, and Wonyong Sung. \"A statistical model-based voice activity detection.\" IEEE signal processing letters 6.1 (1999): 1-3.","Creater_id":125219,"Start_date":"2016-08-04 08:13:58","Question_id":230154,"Tags":["untagged"],"Answer_count":0,"Last_activity":"2016-08-16 11:57:42","Link":"http://stats.stackexchange.com/questions/230154/setting-threshold-in-sohn-al-1999-voice-activity-detection-algorithm","Creator_reputation":6}
{"_id":{"$oid":"5837a57da05283111e4d4c48"},"View_count":66,"Display_name":"DP78","Question_score":0,"Question_content":"I have a question about the role of the MAPE in the ARIMA model optimization.For a daily time series I have found that the best model (using the Box-Jenkins approach) is an ARIMA(7,0,7)(0,0,0). If I check the ACF and PACF of the model residuals, I see that there is no more information I can extract; see the right panel of the picture below.However, the MAPE value is as high as 18209.16. To me this sounds too high according to its definition.Have I missed something? Can I accept this ARIMA model even if it has such a high MAPE?By the way, I am getting similar results if I use auto.arima; MAPE is always big.","Creater_id":80951,"Start_date":"2016-08-15 03:57:20","Question_id":229892,"Tags":["time-series","arima","mape"],"Answer_count":2,"Last_activity":"2016-08-16 11:53:11","Link":"http://stats.stackexchange.com/questions/229892/arima-model-with-huge-mape-value","Creator_reputation":27}
{"_id":{"$oid":"5837a57da05283111e4d4c56"},"View_count":519,"Display_name":"Wintermute","Question_score":4,"Question_content":"Is there really any difference between the jackknife and leave one out cross validation? The procedure seems identical am I missing something?","Creater_id":36049,"Start_date":"2015-03-30 07:27:33","Question_id":144064,"Tags":["cross-validation","jackknife"],"Answer_count":2,"Last_activity":"2016-08-16 11:36:53","Link":"http://stats.stackexchange.com/questions/144064/jackknife-vs-loocv","Creator_reputation":311}
{"_id":{"$oid":"5837a57da05283111e4d4c64"},"View_count":37,"Display_name":"Shivi Bhatia","Question_score":1,"Question_content":"I am working to reduce the # of predictor variables from the model using woe and   iv values. For this i am using riv package. However i am having a hard time   installing this package:  install_github(\"riv\",\"tomasgreif\")install.packages(\"DBI\",dependencies=TRUE)The error i receive is :Username parameter is deprecated. Please use tomasgreif/riv   The other approach i have to use the library(InformationValue). I have this as below:  WOE(X=SFDC1survey)WOETable(X=SFDC1survey)IV(X=SFDC1survey)This package assists me achieve what i am looking for but here i need toadd one independent variable at a time to see whether it is predictive ornot. Is there a way i can add all variables at a go or have to add oneby one.For the above package (riv) i have seen an example which helps to take the   entire data range and  predictive the power of each predictor. The link for the   same is:https://www.r-bloggers.com/r-credit-scoring-woe-information-value-in-woe-package/Thanks, Shivi","Creater_id":79611,"Start_date":"2016-08-16 11:36:00","Question_id":230149,"Tags":["r","mathematical-statistics","predictive-models"],"Answer_count":0,"Last_activity":"2016-08-16 11:36:00","Link":"http://stats.stackexchange.com/questions/230149/error-in-riv-package","Creator_reputation":13}
{"_id":{"$oid":"5837a57da05283111e4d4c66"},"View_count":87739,"Display_name":"Shane","Question_score":278,"Question_content":"Last year, I read a blog post from Brendan O'Connor entitled \"Statistics vs. Machine Learning, fight!\" that discussed some of the differences between the two fields.  Andrew Gelman responded favorably to this:Simon Blomberg:   From R's fortunes  package: To paraphrase provocatively,  'machine learning is statistics minus  any checking of models and  assumptions'.  -- Brian D. Ripley (about the difference between machine learning  and statistics) useR! 2004, Vienna  (May 2004) :-) Season's Greetings!Andrew Gelman:  In that case, maybe we should get rid  of checking of models and assumptions  more often. Then maybe we'd be able to  solve some of the problems that the  machine learning people can solve but  we can't!There was also the \"Statistical Modeling: The Two Cultures\" paper by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the predictive accuracy of models.Has the statistics field changed over the last decade in response to these critiques?  Do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?","Creater_id":5,"Start_date":"2010-07-19 12:14:44","Question_id":6,"Tags":["machine-learning"],"Answer_count":19,"Last_activity":"2016-08-16 11:12:10","Link":"http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning","Creator_reputation":7937}
{"_id":{"$oid":"5837a57da05283111e4d4c85"},"View_count":17,"Display_name":"HiHO","Question_score":1,"Question_content":"Suppose i have a sample  where  and Suppose further that we know that  is composed (in a way unknown to us) of two different populations.Some questions:1) If we know that in each such populations  is a linear function of , can we expect any reasonable outcome by fitting linear models?2) If no extra data is given about the generation of the sample. What kind of predictive modeling strategy can be applied to this data?","Creater_id":127888,"Start_date":"2016-08-16 10:46:25","Question_id":230147,"Tags":["mixed-model","predictive-models"],"Answer_count":0,"Last_activity":"2016-08-16 10:46:25","Link":"http://stats.stackexchange.com/questions/230147/sub-samples-of-different-populations","Creator_reputation":6}
{"_id":{"$oid":"5837a57da05283111e4d4c87"},"View_count":9,"Display_name":"godspeed","Question_score":1,"Question_content":"I have 30 subjects with 5 observations each. Is my sample size 30 or 150 and would it be sufficient to satisfy the asymptotic assumptions of regression parameters for GEEs","Creater_id":103245,"Start_date":"2016-08-16 10:33:54","Question_id":230145,"Tags":["repeated-measures","sample-size","gee"],"Answer_count":0,"Last_activity":"2016-08-16 10:33:54","Link":"http://stats.stackexchange.com/questions/230145/sufficient-sample-size-for-longitudinal-data-analysis","Creator_reputation":133}
{"_id":{"$oid":"5837a57da05283111e4d4c89"},"View_count":80,"Display_name":"ts_highbury","Question_score":0,"Question_content":"Could someone help me with a simple derivation of why OLS is biased in the presence of reverse causality and how to derive the direction of the bias?","Creater_id":101137,"Start_date":"2016-08-16 01:46:01","Question_id":230055,"Tags":["econometrics","least-squares","endogeneity"],"Answer_count":0,"Last_activity":"2016-08-16 10:30:58","Link":"http://stats.stackexchange.com/questions/230055/derivation-of-simultaneity-bias","Creator_reputation":45}
{"_id":{"$oid":"5837a57da05283111e4d4c8b"},"View_count":5809,"Display_name":"user28","Question_score":11,"Question_content":"The 'fundamental' idea of statistics for estimating parameters is maximum likelihood. I am wondering what is the corresponding idea in machine learning.Qn 1. Would it be fair to say that the 'fundamental' idea in machine learning for estimating parameters is: 'Loss Functions'[Note: It is my impression that machine learning algorithms often optimize a loss function and hence the above question.]Qn 2: Is there any literature that attempts to bridge the gap between statistics and machine learning?[Note: Perhaps, by way of relating loss functions to maximum likelihood. (e.g., OLS is equivalent to maximum likelihood for normally distributed errors etc)]","Creater_id":null,"Start_date":"2010-07-28 04:31:59","Question_id":886,"Tags":["machine-learning","maximum-likelihood","loss-functions"],"Answer_count":6,"Last_activity":"2016-08-16 10:22:15","Link":"http://stats.stackexchange.com/questions/886/what-is-the-fundamental-idea-of-machine-learning-for-estimating-parameters","Creator_reputation":null}
{"_id":{"$oid":"5837a57da05283111e4d4c9d"},"View_count":119,"Display_name":"Dark_Knight","Question_score":1,"Question_content":"I have some ordinal data based on a population survey. My  consist of age groups  years . On the , I have proportion of certain population. And I have these kind of plots (data) for 6 different categories  . I have already constructed  frequency tables (and made histograms accordingly) but I have to analyse this data more. I need to test if my data follows a normal distribution (which I know will turn out to be not normal as it is ordianl). Since  calculating mean doesn't make much sense so I was wondering what other parameters and statistics that could be calculated. It would also be helpful if any suggestions regarding software tools is given (especially most feasible to ordinal set of data) . ","Creater_id":106322,"Start_date":"2016-08-16 09:58:50","Question_id":230138,"Tags":["normal-distribution","ordinal","likert","summary-statistics","demography"],"Answer_count":0,"Last_activity":"2016-08-16 10:18:25","Link":"http://stats.stackexchange.com/questions/230138/statistics-for-ordinal-data","Creator_reputation":158}
{"_id":{"$oid":"5837a57da05283111e4d4c9f"},"View_count":96,"Display_name":"user127849","Question_score":3,"Question_content":"I see that it has something to do with the variance and one has 12 on the numerator and the other denominator, but why is this the same 12 and how do they become inverse?Equations:  Kruskal Wallis: The first equation under 3 hereH = \\frac{12}{N(N+1)} \\sum_{i=1}^g n_i\\bigg(\\bar r_{i\\cdot}-\\frac{N-1}{2} \\bigg)^2Wilcoxon: The one for the variance here\\begin{align}z \u0026amp;= \\frac{R - \\mu_R}{\\sigma_R}  \\\\[5pt]\u0026amp;\\text{where}  \\\\[5pt]\\mu_R \u0026amp;= \\frac{n_1(n_1 + n_2 + 1)}{2}  \\\\[8pt]\\sigma_R \u0026amp;= \\sqrt{\\frac{n_1n_2(n_1 + n_2 + 1)}{12}}\\end{align}","Creater_id":127849,"Start_date":"2016-08-16 05:24:28","Question_id":230089,"Tags":["wilcoxon","kruskal-wallis"],"Answer_count":1,"Last_activity":"2016-08-16 09:54:19","Link":"http://stats.stackexchange.com/questions/230089/where-does-the-number-12-come-from-and-why-is-it-the-same-12-in-wilcoxon-and-kru","Creator_reputation":16}
{"_id":{"$oid":"5837a57da05283111e4d4cac"},"View_count":37,"Display_name":"user1536464","Question_score":0,"Question_content":"I am running a model where I regress the change in a party's vote share on a party-level policy variable, a country-level variable that counts the number of effective parties in a political system, and an interaction of the two. Change in vote share ranges from -25 to 25, the policy variable ranges from -.28 to .28 and the number of effective parties ranges from 2 to 5.  I run pooled OLS with  parties,  elections and party-level fixed effects with the following model:Where  are party dummies. I am interested in the marginal effects of the policy variable conditional on the number of parties. When calculating the marginal effects of a one-standard deviation increase in  conditional on different levels of , I find nonsensically large effects. For example, the model is telling me that increasing  by one standard deviation when  results in an increase of 50 percentage points (a party going from 0% of the vote in election 1 to 50% in election 2 or from 25% to 75%).  These effects are much too large and I do not observe these types of swings in my data. I would greatly appreciate any help in figuring out what is causing this. I checked the scaling of the variables and everything seems to look good. Should I use a different type of model? Or should I be using a different quantity to examine the marginal effects? ","Creater_id":106263,"Start_date":"2016-08-15 17:26:44","Question_id":230010,"Tags":["regression","econometrics","interpretation","fixed-effects-model"],"Answer_count":0,"Last_activity":"2016-08-16 09:49:45","Link":"http://stats.stackexchange.com/questions/230010/substantive-effects-too-large-in-linear-regression","Creator_reputation":16}
{"_id":{"$oid":"5837a57da05283111e4d4cae"},"View_count":31,"Display_name":"user2543622","Question_score":1,"Question_content":"I have a very strong server. It has 240 GB ram and 10 cores. I want to run Market Basket analysis on my data. The data is very huge 564 million rows and the data is in below format. The data has 3 columns. The 3rd column can be ignored for time being. I read the data in rstudio  basket_id    product_description   basket_gross_sale_total 1          POT PIE-CHICKEN      6.400000 1          BEEF-VARIETY-STEW    6.400000 2          CARROTS              4.00000 2          CORN                 4.0000 2          ALMOND               4.00000 3          POTATOES             6.00000library(arules)i \u0026lt;- split(basket0basket_id)I ran above 2 commands to load the arules package. The split command is taking a lot of time \u003e12 hours and it hasnt finished yet. I also noticed that R is using around 11% of my RAM and 11% of my CPU.How can I make this step faster? Can I run this step on all cores using most of the ram?My R info as belowMicrosoft R Open 3.3.0Default CRAN mirror snapshot taken on 2016-06-01The enhanced R distribution from Microsoft","Creater_id":29065,"Start_date":"2016-08-16 09:34:34","Question_id":230135,"Tags":["r"],"Answer_count":0,"Last_activity":"2016-08-16 09:34:34","Link":"http://stats.stackexchange.com/questions/230135/market-basket-analysis-microsoft-r-open-3-3-0-using-all-pc-cores-and-ram","Creator_reputation":161}
{"_id":{"$oid":"5837a57da05283111e4d4cb0"},"View_count":26,"Display_name":"Bill Kavvas","Question_score":1,"Question_content":"I am working on a problem of trying to classify materials using image data. I have tried several classifiers and have ended up that an SVM with a non-linear kernel gives me the best results up to now.My problem is that when I create a training/testing set for e.g. 10 materials, everything works fine. When I am trying to create a training/testing set for e.g. 25 materials, classification accuracy drops dramatically. Could you propose any solution to this problem?Thanks!","Creater_id":125500,"Start_date":"2016-08-16 09:02:23","Question_id":230127,"Tags":["svm","accuracy"],"Answer_count":1,"Last_activity":"2016-08-16 09:12:07","Link":"http://stats.stackexchange.com/questions/230127/scaling-up-an-svm-classifier","Creator_reputation":18}
{"_id":{"$oid":"5837a57da05283111e4d4cbd"},"View_count":14,"Display_name":"Data_D","Question_score":1,"Question_content":"I have some data with predictors that work out the value of changes applied to an ID. Each TYPE is a predictor for the CHANGE applied to an ID.Below is an example of the data:ID  CHANGE   TYPE    VALUE   STATEA   RA       1       1.14    POSA   RA       2       0.96    POSA   RA       3       0.78    POSA   RA       4       0.84    POSA   RA       5       0.92    POSB   SR       1       1       POSB   SR       2       0.82    POSB   SR       3       -0.012  NEGB   SR       4       -0.036  NEGB   SR       5       -0.02   NEGC   GW       1       -0.86   NEGC   GW       2       -0.77   NEGC   GW       3       -0.82   NEGC   GW       4       -0.62   NEGC   GW       5       -0.74   NEGIs there a measure of a distance/spread I could apply to each ID? For example, C has all predictions very close to each other so we can be confident in the prediction but B's predictions are spread out so we are less confident.","Creater_id":93518,"Start_date":"2016-08-16 07:06:49","Question_id":230103,"Tags":["similarities"],"Answer_count":1,"Last_activity":"2016-08-16 09:03:32","Link":"http://stats.stackexchange.com/questions/230103/is-there-a-measure-i-could-apply-to-measure-spread-of-predictions","Creator_reputation":6}
{"_id":{"$oid":"5837a57da05283111e4d4cca"},"View_count":46,"Display_name":"snowneji","Question_score":1,"Question_content":"I'm trying to find the maximum output for a quite complex function in R. The input of the function requires 4 parameters: x1, x2, x3, x4, the output is a single value.The constraints are : 5 \u0026lt; x1 \u0026lt; 150 \u0026lt; x2 \u0026lt; 890 \u0026lt; x3 \u0026lt; 890 \u0026lt; x4 \u0026lt; 89x3 \u0026lt; x4Now what I did is quite dumb and slow: run all the possible combinations through the function and find the maximum value, it works but takes 3-4 hours to \"search\" the maximum output of the function.I believe there is a better way to optimize this. I tried run linear regression using the output against all input variables but can't see any good explanation(R-squared \u0026lt; 0.1). I manually change each parameter to see the output but the changes of the output is quite unpredictable to me. I tried using symbolic regression to find a formula for the function so that I can find the cost function later but not working. I also checked some optimization functions in R such as optim(c(5,1,1,2),findVar,lower = c(5,1,1,2), upper = c(15,89,89,89)) but looks like they were just not working as well (Error Message:Your panel, as described by unit.variable and time.variable, is unbalanced. Balance it and run again.). I'm a newbie to optimization problems so any good advices will be really appreciated !","Creater_id":127869,"Start_date":"2016-08-16 08:47:57","Question_id":230123,"Tags":["r","optimization"],"Answer_count":0,"Last_activity":"2016-08-16 08:47:57","Link":"http://stats.stackexchange.com/questions/230123/r-optimization-for-a-complex-function","Creator_reputation":6}
{"_id":{"$oid":"5837a57da05283111e4d4ccc"},"View_count":20,"Display_name":"Theoden","Question_score":1,"Question_content":"Imagine three back to back directed nodes.A -\u003e B -\u003e CKnowing B, A and C are ought to become independant. Imagine B to be binary A \u0026amp; C have positive correlation. After I correct for B, for B=0, A \u0026amp; C become independent while for B=1 they become negatively correlated.The network is learned by bnlearn in R, I;m showing a branch which is Markov Blanketed from the rest I'm confused about the remaining correlation. Is it a sign that the network is not trained well or what?Best   ","Creater_id":60655,"Start_date":"2016-08-16 08:42:04","Question_id":230122,"Tags":["conditional-probability","bayesian-network","networks"],"Answer_count":0,"Last_activity":"2016-08-16 08:42:04","Link":"http://stats.stackexchange.com/questions/230122/how-to-explain-conditional-independence-between-two-nodes-in-a-bayesian-network","Creator_reputation":107}
{"_id":{"$oid":"5837a57da05283111e4d4cce"},"View_count":83,"Display_name":"M.R.Karimi","Question_score":4,"Question_content":"Suppose that for two discrete random variables  and , we know their conditional distributions. NamelyX_1~|~X_2 = x_2 \\sim \\mathrm{Poisson}(\\lambda_1 + ax_2),X_2~|~X_1 = x_1 \\sim \\mathrm{Poisson}(\\lambda_2 + bx_1).We want to calculate their joint distribution, .My own idea is to divide the equations above and find  and then sum over all values of  to find . But for this, I have to compute a very bad series. Do you have any idea for this problem?P.S. The series I have to compute is of form\\sum_{n=0}^\\infty \\frac{c^n}{n!(a+nb)^k},which I hardly believe to have a closed form.","Creater_id":127708,"Start_date":"2016-08-15 05:00:05","Question_id":229901,"Tags":["probability","self-study","distributions","poisson","conditional-probability"],"Answer_count":1,"Last_activity":"2016-08-16 08:27:17","Link":"http://stats.stackexchange.com/questions/229901/finding-the-joint-distribution-from-poisson-conditionals","Creator_reputation":21}
{"_id":{"$oid":"5837a57da05283111e4d4cdb"},"View_count":60,"Display_name":"Stats9415","Question_score":1,"Question_content":"I really need help to understand how the function interact.gbm (gbm package) can compute Friedman's H statistic to assess the relative strengh of interaction effects.In this Friedman's paper http://arxiv.org/pdf/0811.1679.pdf (section 8.1, page 20), we can see how Friedman computes this statistic.The question is why does he consider partial dependance function as additive ? It's totally not the case for me : the range of a partial dependance function F_j is the same as the range of the predictive function F.Thank you !","Creater_id":120087,"Start_date":"2016-08-16 08:07:59","Question_id":230119,"Tags":["interaction","boosting","partial-plot"],"Answer_count":0,"Last_activity":"2016-08-16 08:07:59","Link":"http://stats.stackexchange.com/questions/230119/interact-gbm-friedmans-h-statistic","Creator_reputation":11}
{"_id":{"$oid":"5837a57da05283111e4d4cdd"},"View_count":128,"Display_name":"AshNTU","Question_score":2,"Question_content":"I have X, Y and Z co-ordinate of the movement patterns of a person for 30 days over some known physical layout. This is unevenly spaced time-series data with maximum frequency of 2Hz while in motion. It is known that a person can depicts one of the four patterns of locomotion i.e. lapping, pacing, random and direct. It has been defined as:Lapping: Locomotion that has a circular path (closed loop). Pacing: Back and forth locomotion between two end points.Random: Locomotion along a haphazard path from source to destination.Direct: Locomotion along a somewhat straight path from source to destination.(Note: Source and destination are start and end of an episode. Episode is a smaller simpler navigation path which can have one of the pattern ) My aim is to find the pattern in an episode. What features I should extract from these dataset and which ML classification algorithm will be best suitable to deal with these types of problem.I have split the movement into smaller simpler episodes where each episodes can be any of the four patters. Right now, I am using some heuristics to identify these patterns but I  feel that ML can be very good to predict these patterns.Thanks in advance :)","Creater_id":81157,"Start_date":"2015-07-01 03:24:19","Question_id":159441,"Tags":["time-series","classification"],"Answer_count":1,"Last_activity":"2016-08-16 08:07:04","Link":"http://stats.stackexchange.com/questions/159441/identification-of-navigation-pattern-lapping-pacing-random-and-direct-from-x","Creator_reputation":11}
{"_id":{"$oid":"5837a57da05283111e4d4cea"},"View_count":23,"Display_name":"user32849","Question_score":3,"Question_content":"Assume a set of random variables  which are i.i.d. according to some unknown distribution . All we know is , and . What, if anything, can we say about , e.g. bounds on its expectation etc.?","Creater_id":45618,"Start_date":"2016-08-16 08:06:41","Question_id":230118,"Tags":["order-statistics"],"Answer_count":0,"Last_activity":"2016-08-16 08:06:41","Link":"http://stats.stackexchange.com/questions/230118/expected-maximum-of-absolute-centered-iid-rv","Creator_reputation":36}
{"_id":{"$oid":"5837a57da05283111e4d4cec"},"View_count":574,"Display_name":"Alexander","Question_score":2,"Question_content":"BackgroundI am writing a systematic review and meta-analysis of the association of exposure to X with the outcome Y. I have identified ten studies that report on this subject. This will be a meta-analysis of the published literature, not of individual participant data. I want to report the overall mean age and standard deviation of all patients in all of the studies together. The studies all have different sample sizes. How should I do this?Possibilities(1) I imagine that I cannot just add up the reported mean values and divide by ten. This would give smaller studies an undue weight in contributing to the overall mean.(2) I thought perhaps that I could just do a simple weighted average, using the sample size of each study as the weight. Is this appropriate? Can I also just do a weighted average of the reported standard deviations and ranges? If not, how would I compute those?(3) Another thought I had was actually taking each of the ten reported mean weights along with their standard deviations and actually plugging them into a meta-analysis program to get a \"pooled weight\" with a fixed effects model. But perhaps this would be a strange approach?Any advice on which technique to use? Is there another possibility that I'm not thinking of? Thanks!","Creater_id":8724,"Start_date":"2014-04-22 18:10:55","Question_id":94791,"Tags":["mean","meta-analysis","summary-statistics","epidemiology"],"Answer_count":1,"Last_activity":"2016-08-16 08:03:07","Link":"http://stats.stackexchange.com/questions/94791/estimating-an-overall-summary-statistic-mean-for-a-value-reported-by-ten-publi","Creator_reputation":1100}
{"_id":{"$oid":"5837a57da05283111e4d4cf7"},"View_count":127,"Display_name":"gusdadjdk123","Question_score":2,"Question_content":"I'm trying to understand the SIR algorithm. In order to do so it seemed best to look at some existing r code (see below), taken from http://hedibert.org/wp-content/uploads/2013/12/example-iii.R.txt.# SIR: Uniform proposalN      = 10000m      = 2000draw1  = runif(N,-10,10)w      = dnorm(draw1)/0.05ind    = sample(1:N,size=m,replace=TRUE,prob=w)draws1 = draw1[ind]length(unique(draws1))/mI don't really understand any of the steps taken after computing w. Why not just take the mean of w to approximate the integral? Why resample?Also, what is the last step about: Why calculate how many unique draws you have?","Creater_id":127158,"Start_date":"2016-08-09 10:41:05","Question_id":229036,"Tags":["monte-carlo","resampling","importance-sampling"],"Answer_count":1,"Last_activity":"2016-08-16 08:02:37","Link":"http://stats.stackexchange.com/questions/229036/sampling-importance-resampling-why-resample","Creator_reputation":11}
{"_id":{"$oid":"5837a57da05283111e4d4d04"},"View_count":25,"Display_name":"Bill Kavvas","Question_score":1,"Question_content":"Currently I am handling hyperspectal imaging data, where for each pixel, I have the response at 424 spectral bands. Not all of these bands contain useful information. Could you please propose some algorithms, able to perform feature selection for that problem?Thank you!","Creater_id":125500,"Start_date":"2016-08-16 08:01:48","Question_id":230117,"Tags":["feature-selection"],"Answer_count":0,"Last_activity":"2016-08-16 08:01:48","Link":"http://stats.stackexchange.com/questions/230117/finding-the-most-important-features","Creator_reputation":18}
{"_id":{"$oid":"5837a57da05283111e4d4d06"},"View_count":178,"Display_name":"user3403745","Question_score":2,"Question_content":"I'm trying to do a meta analysis across a bunch of bird playback studies. Thanks to help on a previous post here I'm fairly certain I need to use inverse variance weighting (maybe with the 'meta' package in R).These playback studies use different response measures with some overlap. Some studies use 10 response measures while others use one. I have the means and standard errors for each study's response measures. I want to look at if, across birds, these response measures increase along with another variable that I have for each bird.I am unsure where to go from here. It doesn't seem appropriate to average the means and standard errors within each study. I'm also not incredibly familiar with the 'meta' package and can't find resources online detailing how to conduct a meta analysis with data like this. What should be my next step?","Creater_id":94946,"Start_date":"2016-02-03 09:24:03","Question_id":193853,"Tags":["r","meta-analysis"],"Answer_count":3,"Last_activity":"2016-08-16 07:54:43","Link":"http://stats.stackexchange.com/questions/193853/meta-analysis-across-studies-with-multiple-response-measures","Creator_reputation":20}
{"_id":{"$oid":"5837a57da05283111e4d4d15"},"View_count":39,"Display_name":"dkoehn","Question_score":1,"Question_content":"Geometrically the SVM tries to classify each data point rightly, while maximize the margin . In the linear seperable case this can be formulated as \\begin{align}\\max_{\\gamma,b \\in \\mathbb{R}, w \\in \\mathbb{R}^d} \\gamma \\ \\ \\textit{s.t.} \\ \\ \\gamma - y_i \\dfrac{w^T x_i + b}{||w||}  \u0026amp;\\leq 0 \\ \\ \\forall i = 1,\\dots,n \\label{eq:hardsvm}\\end{align} Is there a simple and mathematical exact way to show that the unequality constraint of this problem is not convex? ","Creater_id":103147,"Start_date":"2016-08-16 07:40:29","Question_id":230114,"Tags":["machine-learning","svm","proof","theory","convex"],"Answer_count":0,"Last_activity":"2016-08-16 07:54:30","Link":"http://stats.stackexchange.com/questions/230114/how-to-show-non-convexity-of-the-geometrical-motivated-svm-optimization-problem","Creator_reputation":33}
{"_id":{"$oid":"5837a57da05283111e4d4d17"},"View_count":37,"Display_name":"AVoelp","Question_score":0,"Question_content":"I am having a statistical problem that falls into two parts:(!) I have several literature reports on studies that have investigated the effect of drug \"A\" on a particular laboratory parameter. Some articles report the mean and SD values at baseline and treatment end while others report mean and SD intra-individual change between baseline and end of treatment. How can I determine the average change between baseline and treatment end across all studies?(2) Experiments like those in (1) have been reported not only for drug \"A\", but also for \"B\", \"C\", and \"D\" (of course, the laboratory measure of interest is always the same). For each drug, I only have some value(s) for average change over time, but there is neither a head-to-head comparison nor an indirect comparison between them. Is there anything more that I can do to compare the effects of the drugs on the lab measure except to present average change and some measure of variability side by side?Thank you,Andreas ","Creater_id":102978,"Start_date":"2016-02-04 01:13:47","Question_id":193967,"Tags":["meta-analysis"],"Answer_count":1,"Last_activity":"2016-08-16 07:52:07","Link":"http://stats.stackexchange.com/questions/193967/averaging-and-comparing-mean-values-across-studies","Creator_reputation":1}
{"_id":{"$oid":"5837a57da05283111e4d4d23"},"View_count":53,"Display_name":"fede_luppi","Question_score":0,"Question_content":"First of all, I am sorry if this question sounds very basic, but I am new in meta-analysis and statistics and I don’t know how to execute what I have in mind. I have a dataset of 60 experiments with two different types of measurements (biomass and temperature) under control and increased CO2 concentration (the treatment).For biomass: I have calculated the effects of CO2 on biomass as escalc(measure=“ROM”, m1i=elev.biomass.mean, m2i=control.biomass.mean, sd1i=elev.SD, sd2i=control.SD, data=data)For temperature: I have calculated the effects of CO2 on temperature asescalc(measure=“ROM”, m1i=elev.temp.mean, m2i=control.temp.mean, sd1i=elev.SD, sd2i=control.SD, data=data)Therefore I have to meta-analysis with their respective effect size (ESbiomass, EStemp) and variance (VARbiomass, VARtemp). Next, I want to analyse the relationship between the CO2 effects on biomass and the CO2 effects on temperature to determine if there is a negative relationship between them (i.e. are the effects of CO2 on temperature negative when the effects of CO2 on biomass are positive?).Of course I could use lm (biomass.yi ~ temperature.yi) but I want to use user defined weights for each experiment weightsTime.My question is: i) How can I analyse the relationship biomass.yi ~ temperature.yi while taking into account the weights of the experiments in the context of a meta-analysis?; ii) How can I represent the relationship in a graph showing point sizes based on the weights and the model fit?","Creater_id":31979,"Start_date":"2016-05-23 06:23:00","Question_id":214107,"Tags":["r","regression","meta-analysis","meta-regression"],"Answer_count":0,"Last_activity":"2016-08-16 07:50:11","Link":"http://stats.stackexchange.com/questions/214107/linear-regression-from-two-meta-analysis-outcomes-in-r-metafor","Creator_reputation":71}
{"_id":{"$oid":"5837a57da05283111e4d4d25"},"View_count":1348,"Display_name":"Sue","Question_score":2,"Question_content":"Using an independent samples t test, I am comparing the performance of 2 groups on an activity, based on the percentage of items correct (the percentage of total items correct, as well as percentage of items correct for four sub-domains of the activity).  The n of one group is 23, and the n of the other is 34.Due to a ceiling effect, my data are non-normally distributed for both groups. At the recommendation of a statistician/professor, to correct for this problem I bootstrapped the t tests in SPSS using 95% bias corrected confidence intervals based on 1,000 replications.  The smaller group has a larger variance, so I am interpreting the analyses using Welch's correction (\"equal variances not assumed\"). The SPSS output for the bootstrapped t tests produces a table with some p values left blank under the Sig. (2-tailed) column...Why is this? And what do I report instead of these missing bootstrapped p values? I just want to be consistent in how I explain which results were significant.  All of the Lower and Upper BCa 95% confidence intervals ARE listed in the table, but I'm not sure how to succinctly communicate significance using only the confidence intervals (Though I know that if the interval does not include 0 then it can be concluded that the difference between the 2 groups is significant).   ","Creater_id":42784,"Start_date":"2014-04-06 10:03:46","Question_id":92734,"Tags":["spss","t-test","bootstrap","reporting"],"Answer_count":2,"Last_activity":"2016-08-16 07:28:09","Link":"http://stats.stackexchange.com/questions/92734/bootstrapped-t-test-in-spss-why-are-some-p-value-cells-blank","Creator_reputation":50}
{"_id":{"$oid":"5837a57da05283111e4d4d33"},"View_count":47,"Display_name":"MOJO","Question_score":0,"Question_content":"I have a question regarding GARCH(1,2)  estimation. I used Matlab econometrics toolbox to estimate a GARCH(1,2) model. A likelihood ratio test suggested that it would be a better fit than GARCH(1,1), GARCH(2,1) or GARCH(2,2). Now my b1 coefficient for the first lag is equal to zero. Is this possible?My code:pd4 = fitdist(res3,'tLocationScale');v4= ceil(pd4.ParameterValues(1,3));tdist4=struct('Name','t','DoF',v4);Mdl4=garch('Offset',0,'GARCHLags',2,'ARCHLags',1,'Distribution',tdist4);[EstMdl4,EstParamCov4,logL4,info4]=estimate(Mdl4,res4);","Creater_id":81795,"Start_date":"2015-07-23 10:28:36","Question_id":162888,"Tags":["matlab","garch"],"Answer_count":1,"Last_activity":"2016-08-16 07:10:31","Link":"http://stats.stackexchange.com/questions/162888/garch1-2-with-beta-1-0","Creator_reputation":1}
{"_id":{"$oid":"5837a57da05283111e4d4d40"},"View_count":48,"Display_name":"Jan Modus","Question_score":0,"Question_content":"I have a simple question. I'm doing a regression with countries (346 countries). I have a variable that measures level of previous conflict. I rescaled this variable in a variable that goes from 0.0 (minimum) to 1.0 (maximum) (mean 0.16 and SD = 0.23). I found that I need a quadratic and a cubic term of this variable.My dependent variable is attitudes towards abortion at the national level and it goes from 0 to 9.Now I found that my intercept is 1.74, my main term (conflict) is -7.00, my quadratic term is 17.32 and my cubic term is -10.15.How do I interpret this? These are raw polynomials.","Creater_id":29893,"Start_date":"2016-08-16 03:43:27","Question_id":230070,"Tags":["regression","polynomial"],"Answer_count":0,"Last_activity":"2016-08-16 07:09:38","Link":"http://stats.stackexchange.com/questions/230070/interpreting-results-of-cubic-regression","Creator_reputation":10}
{"_id":{"$oid":"5837a57da05283111e4d4d42"},"View_count":1886,"Display_name":"VitoshKa","Question_score":13,"Question_content":"Random walk Metropolis-Hasitings with symmetric proposal  has the property that the acceptance probability P(accept\\ y) = \\min\\{1, f(y)/f(x)\\} does not depend on proposal . Does that mean that I can change the  as a function of previous performance of the chain, without affecting the markovianity of the chain?Of particular interest to me is the adjustment of the scaling of Normal proposal as a function of acceptance rate. Would also greatly  appreciate if someone can point out to the adaptation algorithms used in practice for this type of problem.Many thanks.[edit: Starting with the references given by robertsy and wok I found the following references on MH adaptive algorithms:Andrieu, Christophe, and Éric Moulines. 2006.On the Ergodicity Properties of Some Adaptive MCMC Algorithms. The Annals of Applied Probability 16, no. 3: 1462-1505. http://www.jstor.org/stable/25442804.  Andrieu, Christophe, and Johannes Thoms.2008. A tutorial on adaptive MCMC. Statistics and Computing 18, no. 4 (12): 343-373. doi:10.1007/s11222-008-9110-y. http://www.springerlink.com/content/979087678366r78v/.  Atchadé, Y., G. Fort, E. Moulines, and P. Priouret. 2009.Adaptive Markov Chain Monte Carlo: Theory and Methods. Preprint.  Atchadé, Yves. 2010.Limit theorems for some adaptive MCMC algorithms with subgeometric kernels. Bernoulli 16, no. 1 (February): 116-154. doi:10.3150/09-BEJ199.   http://projecteuclid.org/DPubS?verb=Display\u0026amp;version=1.0\u0026amp;service=UI\u0026amp;handle=euclid.bj/1265984706\u0026amp;page=record. Cappé, O., S. J Godsill, and E. Moulines. 2007.An overview of existing methods and recent advances in sequential Monte Carlo. Proceedings of the IEEE 95, no. 5: 899-924.  Giordani, Paolo. 2010.Adaptive Independent Metropolis–Hastings by Fast Estimation of Mixtures of Normals. Journal of Computational and Graphical Statistics 19, no. 2 (6): 243-259. doi:10.1198/jcgs.2009.07174. http://pubs.amstat.org/doi/abs/10.1198/jcgs.2009.07174.  Latuszynski, Krzysztof, Gareth O Roberts, and Jeffrey S Rosenthal. 2011.Adaptive Gibbs samplers and related MCMC methods. 1101.5838 (January 30). http://arxiv.org/abs/1101.5838.Pasarica, C., and A. Gelman. 2009.Adaptively scaling the Metropolis algorithm using expected squared jumped distance. Statistica Sinica.  Roberts, Gareth O. 2009.Examples of Adaptive MCMC. Journal of Computational and Graphical Statistics 18, no. 2 (6): 349-367. doi:10.1198/jcgs.2009.06134. http://pubs.amstat.org/doi/abs/10.1198/jcgs.2009.06134.  ]","Creater_id":1542,"Start_date":"2011-02-16 03:20:43","Question_id":7286,"Tags":["mcmc","metropolis-hastings"],"Answer_count":4,"Last_activity":"2016-08-16 07:07:59","Link":"http://stats.stackexchange.com/questions/7286/can-i-change-the-proposal-distribution-in-random-walk-mh-mcmc-without-affecting","Creator_reputation":167}
{"_id":{"$oid":"5837a57da05283111e4d4d52"},"View_count":96,"Display_name":"sandyp","Question_score":1,"Question_content":"My question is around the conceptual difference between Holt-Winters and ARIMA. As far as I understand, Holt-Winters is a special case of ARIMA. But when is one algorithm preferred over the other? Perhaps Holt-Winters is incremental and therefore serves as an inline (faster) algorithm?Looking forward to some insight here.","Creater_id":53955,"Start_date":"2016-08-11 09:52:42","Question_id":229384,"Tags":["time-series","arima","exponential-smoothing"],"Answer_count":2,"Last_activity":"2016-08-16 06:45:48","Link":"http://stats.stackexchange.com/questions/229384/use-holt-winters-or-arima","Creator_reputation":173}
{"_id":{"$oid":"5837a57da05283111e4d4d60"},"View_count":190,"Display_name":"user1320502","Question_score":3,"Question_content":"Imagine we have a data set with a y, x z and group1 and group2.We want to do this model library(lme4)model \u0026lt;- glmer(y ~ x + z + (1|group1/group2), data = data)We want to check colinearity between x and z, so we calculate variance inflation factors with (taken from here):vif.mer \u0026lt;- function (fit) {## adapted from rms::vifv \u0026lt;- vcov(fit)nam \u0026lt;- names(fixef(fit))## exclude interceptsns \u0026lt;- sum(1 * (nam == \"Intercept\" | nam == \"(Intercept)\"))if (ns \u0026gt; 0) {v \u0026lt;- v[-(1:ns), -(1:ns), drop = FALSE]nam \u0026lt;- nam[-(1:ns)] }d \u0026lt;- diag(v)^0.5v \u0026lt;- diag(solve(v/(d %o% d)))names(v) \u0026lt;- nam v }vif.mer(model)Lets say it is \u0026lt;2, however when we do individual gifs for each group  e.g.vif(data[datagroup2 == 2 ,]).....vif(data[data$group2 == 20 ,])We get gifs what range from 1.1 to over 10.How do we interpret this? Should we be concerned about colinearity, it seems strange to get such variable results?","Creater_id":19744,"Start_date":"2016-03-11 10:24:08","Question_id":201205,"Tags":["mixed-model","covariance","lme4","glmer"],"Answer_count":1,"Last_activity":"2016-08-16 06:31:32","Link":"http://stats.stackexchange.com/questions/201205/variance-inflation-factors-vif-mer-verus-group-factor-level-vif","Creator_reputation":292}
{"_id":{"$oid":"5837a57da05283111e4d4d6d"},"View_count":77,"Display_name":"Pardis","Question_score":2,"Question_content":"According to Thomas Schreiber in the paper \"Measuring Information Transfer\", we need to calculate the Mutual Information between time series  and itself with a delay of  and let's call the delayed form .What I understand is that if ,  will be . Therefore, their lengths are not the same. How can I compute the mutual information between these two time series? Do I have to pre-zero-pad  or something like that?Any help or reference in appreciated.","Creater_id":127848,"Start_date":"2016-08-16 05:18:10","Question_id":230088,"Tags":["time-series","information-theory","mutual-information"],"Answer_count":1,"Last_activity":"2016-08-16 06:23:34","Link":"http://stats.stackexchange.com/questions/230088/mutual-information-between-a-time-series-and-its-delayed-form","Creator_reputation":13}
{"_id":{"$oid":"5837a57da05283111e4d4d7a"},"View_count":23,"Display_name":"sisaman","Question_score":1,"Question_content":"We can represent subclass of linear time invariant (LTI) systems with State Space Representation:\\dot X = AX + BU,Y = CX + DU.Also, nonlinear systems are formulated with generalized State Space Representation where the functions could be nonlinear:\\dot X=f(X,U,t),Y = g(X,U,t). The question is, what is their representation power? Can we formulate any system in State Space Model? ","Creater_id":116252,"Start_date":"2016-08-16 04:53:04","Question_id":230086,"Tags":["state-space-models"],"Answer_count":0,"Last_activity":"2016-08-16 06:20:27","Link":"http://stats.stackexchange.com/questions/230086/representation-power-of-state-space-models","Creator_reputation":28}
{"_id":{"$oid":"5837a57da05283111e4d4d7c"},"View_count":133,"Display_name":"Ereck","Question_score":5,"Question_content":"By using singular value decomposition (SVD), I noticed from the derivation that ridge regression shrinks the coefficients by factor , where  is the diagonal matrix of the matrix . Moreover, as the penalty term  increases, the amount of shrinkage increases. But, what about LASSO regression? Unlike ridge regression, LASSO regression shrinks some of the coefficients to zero. My question: Is there a way to show, in some mathematical fashion, that LASSO regression shrinks some of the coefficients to zero as the notation above does for ridge regression?Using the two predictor case would make it easy to understand. Could you please provide mathematical lines?EDITKnight \u0026amp; Fu (2000) show that  if and only if .How does that occur?References:Knight, Keith, and Wenjiang Fu. \"Asymptotics for lasso-type estimators.\" Annals of Statistics (2000): 1356-1378.","Creater_id":123446,"Start_date":"2016-07-23 20:05:26","Question_id":225319,"Tags":["lasso","regularization","ridge-regression"],"Answer_count":1,"Last_activity":"2016-08-16 05:49:33","Link":"http://stats.stackexchange.com/questions/225319/is-there-a-mathematical-expression-that-shows-how-lasso-shrinks-coefficients-in","Creator_reputation":62}
{"_id":{"$oid":"5837a57da05283111e4d4d89"},"View_count":53,"Display_name":"Lee","Question_score":0,"Question_content":"In the project, I was asked to determine whether correctly predicts the height for boys.To do this, I've collected 50 male students' height and their parents'.However, I am not sure how to run my hypothesis test. I used confidence interval to classify Tall, Average and Short with 90 percent of the sample are classified as average height.(Actually I'm not sure about the percentage too, is 90% a good choice?) Here the problem comes. Now, I have 2 tests in my mind that can be used, chi-square goodness-of-fit test, and binomial test. By using chi-squared test, I'm going to test whether the formula correctly predicts the category a person falls into(eg. Short,Average).If I use binomial test, I am going to test whether the probability of success prediction, , and a prediction is considered as success if the predicted height differs from the actual height within 2.5cm.But, I don't know which are better. Or, can anyone suggest a better test?","Creater_id":127813,"Start_date":"2016-08-15 23:35:08","Question_id":230042,"Tags":["hypothesis-testing","sampling"],"Answer_count":2,"Last_activity":"2016-08-16 05:28:58","Link":"http://stats.stackexchange.com/questions/230042/testing-a-hypothesis-about-the-size-of-coefficients-in-a-linear-relationship","Creator_reputation":1}
{"_id":{"$oid":"5837a57da05283111e4d4d97"},"View_count":67,"Display_name":"Ole Tange","Question_score":6,"Question_content":"I develop GNU Parallel. For each release I would like to test whether the version I am about to release has a memory leak.I can measure the memory usage. Unfortunately the usage varies slightly given the same input. The memory usage is: base + leak. The base varies slightly and there is an expected leak of 1 bit per job. What I want to detect is if there is a bigger leak than the expected leak.For one release the base is between, say, 13050 and 13150 kB and for another release it can be between 15000 and 15500 kB. They are more or less normally distributed.It is very easy to see if there is a memory leak: Just compare runs of 1000 jobs to 1000000 jobs and if there is a leak of 1 byte/job then the usage will increase by 1000000 bytes which is bigger than the range of the base memory usage.But running 1000000 jobs will take around 10000 seconds (1 job takes around 0.01 sec to run).  So is there a faster way in which I can determine if there is likely a bigger than expected leak?I am thinking of running 10 runs with 1000 jobs and 10 runs with 2000 jobs and comparing the distributions of memory usage of these. If the distributions are significantly different then there is a leak.  How do I do that?Intuitively I can see that you need fewer runs and fewer jobs if the leak is big (say, 500 bytes per job) than if the leak is small (say, 2 bytes per job), and thus it should be possible to stop much earlier.  How do I figure out how many runs and how many jobs I should run to get a 99% certainty?(I am a novice at statistics, R and Python, and advanced in Perl. So a solution will have to show real code, not just refer to some statistical methods; also code that gives a \"yes/no\" is preferred over \"eyeballing it\").((Ideally I want 2 pieces of code: One that I can tell \"You have 3 minutes to run. After that I want to know how certain you are about a memleak of 1 byte, 10 bytes, 100 bytes, and 1000 bytes\", and the second version that show me the certainties continously while it is running more runs to gather more datapoints.))","Creater_id":53309,"Start_date":"2016-08-15 12:24:45","Question_id":229977,"Tags":["statistical-significance","normal-distribution","non-inferiority"],"Answer_count":1,"Last_activity":"2016-08-16 05:27:28","Link":"http://stats.stackexchange.com/questions/229977/determine-if-two-distributions-are-the-same","Creator_reputation":131}
{"_id":{"$oid":"5837a57da05283111e4d4da4"},"View_count":39,"Display_name":"denominator","Question_score":1,"Question_content":"I came across Pearson’s companion site of Murray, M. P. (2005). Econometrics: A modern introduction. Pearson Higher Education.While skimming through the related lecture slides here http://wps.aw.com/wps/media/objects/2387/2445250/PPTs/ch18lectr26.ppt, there is something I don’t get.Slide #26-7 of the aformentioned powerpoint presentation correctly states that . Given this assumption I do not understand why  as stated on slide #26-5: \"The trending variable changes by a random amount each period\". I think  should drop out of the equation if the general assumption  holds. Notice that slide #26-7 also states that  is a random variable with . So, if  holds in general, why is  still included in  above?","Creater_id":81449,"Start_date":"2016-08-15 02:21:08","Question_id":229879,"Tags":["stochastic-processes","unit-root","random-walk"],"Answer_count":1,"Last_activity":"2016-08-16 04:52:15","Link":"http://stats.stackexchange.com/questions/229879/random-walk-with-drift-why-is-the-change-in-a-trending-variable-also-a-function","Creator_reputation":71}
{"_id":{"$oid":"5837a57da05283111e4d4daf"},"View_count":30,"Display_name":"Petreius","Question_score":1,"Question_content":"Let C be a Gaussian or Student copula and  empirical margins.I know how to draw from C and get Imagine that I know . I was wondering how I can draw from C conditional on . I don't expect a closed form formula as I have no expression for  but how would you 'numerically' generate a serie of conditional draws from ?","Creater_id":116873,"Start_date":"2016-08-16 04:50:45","Question_id":230083,"Tags":["copula"],"Answer_count":0,"Last_activity":"2016-08-16 04:50:45","Link":"http://stats.stackexchange.com/questions/230083/conditional-copula-distribution","Creator_reputation":18}
{"_id":{"$oid":"5837a57da05283111e4d4db1"},"View_count":41,"Display_name":"ilanman","Question_score":1,"Question_content":"I built a Bayesian A/B testing tool - i.e. one which models A and B having posteriors  where  are updated every iteration.After T iterations, I compare the posterior distributions of A and B and determine which one is larger, lets say, using the mean.How can I incorporate a third (or fourth...) group?Originally I wanted to model it as a Dirichlet, giving each group some  that is updated each iteration. And then after T iterations select the group with the largest . But I think that it's more correct to keep each group A, B, C as a separate Beta (since they are theoretically independent) and just compare the posteriors of their respective distributions. It seems naive to me, but also intuitive. Can someone who has done this please opine?","Creater_id":73527,"Start_date":"2016-06-02 13:23:00","Question_id":216027,"Tags":["bayesian","beta-distribution","dirichlet-distribution","ab-test"],"Answer_count":1,"Last_activity":"2016-08-16 04:45:46","Link":"http://stats.stackexchange.com/questions/216027/bayesian-a-b-c-testing","Creator_reputation":1006}
{"_id":{"$oid":"5837a57da05283111e4d4dbe"},"View_count":16,"Display_name":"qed","Question_score":1,"Question_content":"I have results from the same regression on two datasets, namely t statistics and standard errors , what method would be appropriate for a meta-analysis?","Creater_id":7787,"Start_date":"2016-08-15 07:37:18","Question_id":229926,"Tags":["meta-analysis"],"Answer_count":1,"Last_activity":"2016-08-16 04:30:23","Link":"http://stats.stackexchange.com/questions/229926/meta-analysis-using-known-t-statistics-and-standard-errors","Creator_reputation":673}
{"_id":{"$oid":"5837a57da05283111e4d4dcb"},"View_count":28,"Display_name":"Plissken","Question_score":1,"Question_content":"I have estimated a time-varying parameter vector auto-regressive model (TVP-VAR) as in Primiceri. (2005). Time Varying Structural Vector Autoregressions andMonetary Policy. I then proceed to compute impulse responses as I would for a constant parameter model i.e. I compute the impulse responses for each time period but when calculating them I disregard the fact that the coefficients change (see Koop and Korobilis. (2009). Bayesian Multivariate Time Series Methods for Empirical Macroeconomics , p. 321, footnote 3 where they mention this and show the IRF's). Note that for each period one calculates the distribution of impulse responses for each period and then takes the median or similar together with credible interval to present in a graph.I now want to compare the impulse responses for, say 1981:Q1 and 2011Q:3 to see if they differ.My question is how to go about doing this. Should I:(a)Calculate the median impulse responses from the distribution of impulse responses for 1981:Q1 and 2011Q:3 (Median impulse response for 1981:  and median impulse response for 2011: ) together with credible intervals. Then simply calculate:  and the same for thecredible intervals and then plot these.(b)Calculate the difference between the distributions of impulse responses () and then calculate the median and credible intervals for  and plot these?I can add a graph for both approaches here in case that helps. I think that approach (b) is correct but would like some reassurance on this.","Creater_id":45326,"Start_date":"2016-08-16 04:24:26","Question_id":230076,"Tags":["time-series","bayesian","var","impulse-response"],"Answer_count":0,"Last_activity":"2016-08-16 04:29:37","Link":"http://stats.stackexchange.com/questions/230076/difference-in-impulse-responses-bayesian-analysis","Creator_reputation":839}
{"_id":{"$oid":"5837a57da05283111e4d4dcd"},"View_count":35,"Display_name":"PhilippPro","Question_score":1,"Question_content":"There are several evaluation measures for multilabel classification. In the mlr package tutorial you can see some of them: http://mlr-org.github.io/mlr-tutorial/devel/html/measures/index.html and even more in this paper https://journal.r-project.org/archive/2015-2/charte-charte.pdf (e.g. AUC for multilabel) with implementations of them in R.In this paper there are also described some more measures:http://people.oregonstate.edu/~sorowerm/pdf/Qual-Multilabel-Shahed-CompleteVersion.pdfSome take into account dependencies, others do not. What do you think is the best evaluation metrics (in which case)?I heard that the hamming loss can be optimized best by using the binary relevance method...","Creater_id":96979,"Start_date":"2016-08-16 04:29:08","Question_id":230077,"Tags":["machine-learning","classification","measurement","multilabel"],"Answer_count":0,"Last_activity":"2016-08-16 04:29:08","Link":"http://stats.stackexchange.com/questions/230077/what-measures-should-one-use-to-evaluate-a-multilabel-classification","Creator_reputation":129}
{"_id":{"$oid":"5837a57da05283111e4d4dcf"},"View_count":26,"Display_name":"Diogo Santos","Question_score":2,"Question_content":"Variable  is a random variable with know  and . Variable  is also a random variable with know  and  and  and  are independent.What is the variance and expected value of ?The expected value looks simple, but the variance not... I found this post, and from there the only problem is to get , which I have no idea how to calculate...","Creater_id":27069,"Start_date":"2016-08-16 03:03:55","Question_id":230062,"Tags":["variance","random-variable"],"Answer_count":1,"Last_activity":"2016-08-16 03:52:17","Link":"http://stats.stackexchange.com/questions/230062/variance-of-frequency","Creator_reputation":108}
{"_id":{"$oid":"5837a57da05283111e4d4ddc"},"View_count":37,"Display_name":"lanselibai","Question_score":1,"Question_content":"In my experiment, I prepared 6 identical samples in the beginning. At Time 1, 3 samples were sacrificed and analysed; at Time 2, the rest 3 samples were sacrificed and analysed. So I got the data as below. The slope (-0.5243) is obtained as the rate. Can I ask how to caculate the error bar of the slope? Do I need to do this experiment e.g. three times, and get three slopes to draw the error bar?I do not think I can simply get three slopes from these 6 data points. Because I can get a slope either from \"Time2_repeat1 - Time1_repeat1\" or \"Time2_repeat2 - Time1_repeat1\". Is there a particular definition for the slope, e.g. \"average slope\", \"individual slope\"?Time    repeat1 repeat2 repeat3 average STDEV.S1   0.8628  0.8675  0.8661  0.8655  0.0024 2   0.3445  0.3343  0.3448  0.3412  0.0060 Thank you Glen_b. I have attached it as below according to your words.Can I ask, 1) I use \"VAR.S\" for the VAR, is that correct?2) When you say \"standard error\", do you mean \"standard error of the mean\"? If I can calculate the Standard Error of the Mean (SEM) by:SEM = STDEV.S/sqrt(count(n))Thank you.","Creater_id":96542,"Start_date":"2016-08-14 04:10:43","Question_id":229756,"Tags":["standard-error","excel"],"Answer_count":1,"Last_activity":"2016-08-16 03:50:02","Link":"http://stats.stackexchange.com/questions/229756/how-to-calculate-the-error-bar-e-g-stdev-s-for-a-slope-based-on-independent-s","Creator_reputation":70}
{"_id":{"$oid":"5837a57da05283111e4d4de9"},"View_count":1134,"Display_name":"Ham","Question_score":6,"Question_content":"I have performed a Cochran's Q test for a within-subjects experimental design with 3 conditions and 36 participants with a dichotomous dependent variable.I found a (just) statistically significant effect ( = 6.00, df = 2, p = 0.04979) and would like to also report the effect size, but haven't been able to find any information as to which effect measure to use and how to calculate it.Any pointers would be gratefully received - the domain is human factors (psychology).","Creater_id":4258,"Start_date":"2011-04-22 04:44:46","Question_id":9867,"Tags":["categorical-data","repeated-measures","effect-size","cochran-q"],"Answer_count":2,"Last_activity":"2016-08-16 02:46:43","Link":"http://stats.stackexchange.com/questions/9867/effect-size-of-cochrans-q","Creator_reputation":131}
{"_id":{"$oid":"5837a57da05283111e4d4df6"},"View_count":3620,"Display_name":"vonjd","Question_score":18,"Question_content":"I am having difficulties understanding the underlying logic in setting the null hypothesis. In this answer the obviously generally accepted proposition is stated that the null hypothesis is the hypothesis that there will be no effect, everything stays the same, i.e. nothing new under the sun, so to speak.The alternative hypothesis is then what you try to prove, that e.g. a new drug delivers on its promises.Now coming form science theory and general logic we know that we can only falsify propositions, we cannot prove something (no number of white swans can prove that all swans are white but one black swan can disprove it). This is why we try to disprove the null hypothesis, which is not equivalent to proving the alternative hypothesis - and this is where my skepticism starts - I will give an easy example:Let's say I want to find out what kind of animal is behind a curtain. Unfortunately I cannot directly observe the animal but I have a test which gives me the number of legs of this animal. Now I have the following logical reasoning:  If the animal is a dog then it will have 4 legs.If I conduct the test and find out that it has 4 legs this is no proof that it is a dog (it can be a horse, a rhino or any other 4-legged animal). But if I find out that it has not 4 legs this is a definite proof that it can not be a dog (assuming a healthy animal).Translated into drug effectiveness I want to find out if the drug behind the curtain is effective. The only thing I will get is a number that gives me the effect. If the effect is positive, nothing is proved (4 legs). If there is no effect, I disprove the effectiveness of the drug.Saying all this I think - contrary to common wisdom - the only valid null hypothesis must be  The drug is effective (i.e.: if the drug is effective you will see an effect).because this is the only thing that I can disprove - up to the next round where I try to be more specific and so on. So it is the null hypothesis that states the effect and the alternative hypothesis is the default (no effect).Why is it that statistical tests seem to have it backwards?P.S.: You cannot even negate the above hypothesis to get a valid equivalent hypothesis, so you cannot say \"The drug is not effective\" as a null hypothesis because the only logically equivalent form would be \"if you see no effect the drug will not be effective\" which brings you nowhere because now the conclusion is what you want to find out!P.P.S.: Just for clarification after reading the answers so far: If you accept scientific theory, that you can only falsify statements but not prove them, the only thing that is logically consistent is choosing the null hypothesis as the new theory - which can then be falsified. Because if you falsify the status quo you are left empty handed (the status quo is disproved but the new theory far from being proved!). And if you fail to falsify it you are in no better position either.","Creater_id":230,"Start_date":"2011-08-03 03:22:32","Question_id":13797,"Tags":["hypothesis-testing","philosophical"],"Answer_count":10,"Last_activity":"2016-08-16 02:33:04","Link":"http://stats.stackexchange.com/questions/13797/which-one-is-the-null-hypothesis-conflict-between-science-theory-logic-and-sta","Creator_reputation":2256}
{"_id":{"$oid":"5837a57da05283111e4d4e0c"},"View_count":47,"Display_name":"Oleg","Question_score":0,"Question_content":"When speaking about regression,Why is the  ?  is the correlation coefficient . I can't really see it. ","Creater_id":108326,"Start_date":"2016-08-15 02:43:38","Question_id":229882,"Tags":["regression","correlation","standard-error"],"Answer_count":1,"Last_activity":"2016-08-16 02:22:05","Link":"http://stats.stackexchange.com/questions/229882/making-sense-out-of-the-standard-error-formula","Creator_reputation":43}
{"_id":{"$oid":"5837a57da05283111e4d4e19"},"View_count":48,"Display_name":"user133586","Question_score":7,"Question_content":"I am running a multiple regression model and looking to use AIC and BIC to select models. However I notice that both measures do not consider the number of variables we can choose from but only consider the number of variables chosen. If I have many many variables to choose from, chances are I will find something highly correlated with what I am trying to model, just by luck. Is there a measure that considers how many variables we can choose from? ","Creater_id":54689,"Start_date":"2016-08-15 13:31:36","Question_id":229987,"Tags":["multiple-regression","aic","bic"],"Answer_count":1,"Last_activity":"2016-08-16 00:57:41","Link":"http://stats.stackexchange.com/questions/229987/an-information-criterion-that-considers-how-many-variables-we-can-choose-from","Creator_reputation":53}
{"_id":{"$oid":"5837a57da05283111e4d4e26"},"View_count":29,"Display_name":"vonjd","Question_score":0,"Question_content":"Starting from a (possibly multi-classed) confusion matrix comprising the tallied values of a prediction vector and the actual values I calculate the percent error rate reduction like so:\\frac{accuracy(prediction) - accuracy(baserate)}{1 - accuracy(baserate)}while  is just the maximum of the proportions of the classes in actual.So in case that the accuracy of the prediction is at least equal to the accuracy of the base rate and at most equal to this number will be between  and .I want to test if this number is statistically significant and give a confidence interval. My idea is to use a one-sided binomial test to see if there really is an effect, so I take the number of correctly classified examples as the number of successes and the number of all examples as the number of trails and set the alternative hypothesis as \"greater than\" the base rate - in R:binom.test(correct_class, N, p = base.rt, alternative = \"greater\")My questionI have come as far as stated above. My question is if this does make any sense and how to proceed. Is the given p-value correct? How to calculate the confidence interval (just take the given confidence interval and put it into the above formula?) Or am I missing something fundamental here?","Creater_id":230,"Start_date":"2016-08-16 00:54:15","Question_id":230050,"Tags":["hypothesis-testing","statistical-significance","confidence-interval","accuracy","confusion-matrix"],"Answer_count":0,"Last_activity":"2016-08-16 00:54:15","Link":"http://stats.stackexchange.com/questions/230050/statistical-test-confidence-interval-p-value-for-percent-error-rate-reduction","Creator_reputation":2256}
{"_id":{"$oid":"5837a57da05283111e4d4e28"},"View_count":15,"Display_name":"SKM","Question_score":1,"Question_content":"Question is based on the document Neighborhood Preserving Projections (NPP): ANovel Linear Dimension Reduction MethodSection 4.2 presents a method to find an estimator for a matrix  in the model :  where   is the lower dimensional representation of the high dimensional data point , and  represents the number of data points. The obective function is Queston 1: The weights  are also unknown. So, there must be a way to estimate  as well. How to estimate ? Question 2: I am not aware of the method which the Authors have used to solve this objective function that leads to Eq(20) and from Eq(20) how does one get the expression for .","Creater_id":21160,"Start_date":"2016-08-16 00:18:46","Question_id":230046,"Tags":["dimensionality-reduction","parameter-optimization"],"Answer_count":0,"Last_activity":"2016-08-16 00:18:46","Link":"http://stats.stackexchange.com/questions/230046/unable-to-understand-locally-linear-embedding-technique-for-dimensionality-reduc","Creator_reputation":99}
{"_id":{"$oid":"5837a57da05283111e4d4e2a"},"View_count":94,"Display_name":"Kodiologist","Question_score":5,"Question_content":"Here's xkcd #1210:I'm interested in the tooltip, which reads:  In retrospect, it's weird that as a kid I thought completely random outbursts made me seem interesting, given that from an information theory point of view, lexical white noise is just about the opposite of interesting by definition.What information-theoretic concept is Randall Munroe (the cartoonist) thinking of when he says \"lexical white noise\" and \"interesting\"? And is his claim correct?","Creater_id":14076,"Start_date":"2016-08-13 17:14:29","Question_id":229719,"Tags":["information-theory"],"Answer_count":1,"Last_activity":"2016-08-16 00:10:06","Link":"http://stats.stackexchange.com/questions/229719/is-it-true-that-from-an-information-theory-point-of-view-lexical-white-noise-i","Creator_reputation":8337}
{"_id":{"$oid":"5837a57da05283111e4d4e2d"},"View_count":20,"Display_name":"Asterix","Question_score":2,"Question_content":"Suppose I have two random variables  over some finite set and a function  such thatIntuitively, I would think that if I now consider the function  I should haveHowever, a simple counterexample shows this is not the case. If we choose  to be uniformly distributed over  and f(X,Y) = (-1)^{X+Y}then  and  both have the same distribution: they are uniformly distributed over However, if I take , then  does not have the same distribution as . For example, we have that  , but What is going on in this counter-example that breaks the intuition? Is it that  is not a well-defined function (it has multiple complex branches)?  Is it because  is discontinuous for ? Or is the intuition just not true?","Creater_id":53814,"Start_date":"2016-08-15 13:54:55","Question_id":229991,"Tags":["conditional-probability"],"Answer_count":1,"Last_activity":"2016-08-15 23:49:27","Link":"http://stats.stackexchange.com/questions/229991/functions-of-random-variables-and-conditional-probability","Creator_reputation":50}
{"_id":{"$oid":"5837a57da05283111e4d4e3a"},"View_count":1577,"Display_name":"user974514","Question_score":1,"Question_content":"I have a question concerning the RTextTools package. When I was reading textbooks and wikipedia I imagined that text classification is all about defining a classificator. In RTextTools you define learning and testing documents in one command. For exampledata \u0026lt;- data[sample(1:3100,size=100,replace=FALSE),]matrix \u0026lt;- create_matrix(cbind(dataSubject), language=\"english\",removeNumbers=TRUE, stemWords=FALSE, weighting=weightTfIdf)corpus \u0026lt;- create_corpus(matrix,data$Topic.Code,trainSize=1:75, testSize=76:100,virgin=FALSE)models \u0026lt;- train_models(corpus, algorithms=c(\"MAXENT\",\"SVM\"))We create a corpus from one dataset, where like first 75 documents is for learning and the rest is for training.But when it comes to applying text classification theories for practical use, you want already predefined text classificator to be applied for different datasets. For now the only solution i managed to find is to train with the same documents and then use different test data. But it sounds very unconfortable when it comes to large texts. So my question is how to create predefined classificator and then apply it to different datasets with R? It would be preferable to use algorithms from RTextTools. ","Creater_id":7138,"Start_date":"2011-11-25 05:20:56","Question_id":18935,"Tags":["r","classification","text-mining"],"Answer_count":1,"Last_activity":"2016-08-15 23:33:56","Link":"http://stats.stackexchange.com/questions/18935/obtaining-final-classifier-in-rtexttools","Creator_reputation":283}
{"_id":{"$oid":"5837a57da05283111e4d4e47"},"View_count":59,"Display_name":"Atte Juvonen","Question_score":5,"Question_content":"My question relates to regularization in linear regression and logistic regression. I'm currently doing week 3 of Andrew Ng's Machine Learning course on Coursera. I understand how overfitting can be a common problem and I have some intuition for how regularization can reduce overfitting. My question is can we improve our models by regularizing different parameters in different ways?Example:Let's say we're trying to fit w0 + w1 * x1 + w2 * x2 + w3 * x3 + w4 * x4. This question is about why we penalize for high w1 values in the same way that penalize for high w2 values.If we know nothing about how our features (x1,x2,x3,x4) were constructed, it makes sense to treat them all in the same way when we do regularization: a high w1 value should yield as much \"penalty\" as a high w3 value.But let's say we have additional information: let's say we only had 2 features originally: x1 and x2. A line was underfitting our training set and we wanted a more squiggly shaped decision boundary, so we constructed x3 = (x1)^2 and x4 = (x2)^3. Now we can have more complex models, but the more complex they get, the more we risk overfitting our model to the training data. So we want to strike a balance between minimizing the cost function and minimizing our model complexity. Well, the parameters that represent higher exponentials (x3,x4) are drasticly increasing the complexity of our model. So shouldn't we penalize more for high w3,w4 values than we penalize for high w1,w2 values?","Creater_id":101702,"Start_date":"2016-08-15 17:36:15","Question_id":230013,"Tags":["regression","machine-learning","regularization","overfitting"],"Answer_count":2,"Last_activity":"2016-08-15 22:33:33","Link":"http://stats.stackexchange.com/questions/230013/why-regularize-all-parameters-in-the-same-way","Creator_reputation":68}
{"_id":{"$oid":"5837a57da05283111e4d4e55"},"View_count":35,"Display_name":"Dzamo Norton","Question_score":1,"Question_content":"I was thinking a bit about uplift (in this sense) and arrived at some basic inequalities relating (a version of) it to lift which look like this\\frac{|S|}{|C|}l(S) \\leq \\frac{u(S)}{u(C)} \\leq \\frac{|C|}{|S|}where  is a target set,  is the \"traditional\" lift and  is the uplift.  The writeup is hardly complicated but feels just a little long for pasting in here so I've linked to it below.  Does the result look correct?  Should I be sending it to anyone (perhaps to go into an \"introduction to uplift modelling\" chapter in someone's textbook, or something) or is it elementary enough that I should treat as having been for my own edification and just leave it in my Github?If this is not the right kind of question for here I do apologise.http://nbviewer.jupyter.org/gist/DzamoNorton/f6408d092781b4a84bbb76966531f591","Creater_id":6990,"Start_date":"2016-08-15 10:10:32","Question_id":229954,"Tags":["classification","proof","theory"],"Answer_count":0,"Last_activity":"2016-08-15 22:02:25","Link":"http://stats.stackexchange.com/questions/229954/basic-inequalities-relating-lift-and-uplift","Creator_reputation":46}
{"_id":{"$oid":"5837a57da05283111e4d4e57"},"View_count":27,"Display_name":"user3269","Question_score":1,"Question_content":"With respect to support vector machine, there are multiple types of kernels, such as The Gaussian Kernel, The homogeneous kernel, The linear kernel,etc. Are there any ways to calculate the exact formula of feature map for a given kernel function?","Creater_id":3269,"Start_date":"2016-08-15 21:25:21","Question_id":230029,"Tags":["machine-learning","svm","kernel-trick","libsvm"],"Answer_count":0,"Last_activity":"2016-08-15 21:25:21","Link":"http://stats.stackexchange.com/questions/230029/how-to-get-feature-map-for-a-given-kernel-function","Creator_reputation":1194}
{"_id":{"$oid":"5837a57da05283111e4d4e59"},"View_count":22,"Display_name":"paragbaxi","Question_score":1,"Question_content":"Example question:What colors do you like (choose at least 1)?BlackWhiteGreyExample data:Black = 90%White = 50%Grey = 10%How can I visualize this and demonstrate that some people that chose Black also chose White?","Creater_id":127802,"Start_date":"2016-08-15 21:12:23","Question_id":230028,"Tags":["data-visualization"],"Answer_count":0,"Last_activity":"2016-08-15 21:12:23","Link":"http://stats.stackexchange.com/questions/230028/visualize-question-that-allows-for-multiple-choices-sum-over-100","Creator_reputation":106}
{"_id":{"$oid":"5837a57da05283111e4d4e5b"},"View_count":27,"Display_name":"user_012314112","Question_score":0,"Question_content":"I have a custom algorithm that uses kernel function to estimate distances among individuals, I would like to tune the kernel parameters (like sigma) with the grid search, but I don't know which range would be suitable. Is there any way to get an approximation of an interval that contains the optimal then apply the grid search ? My case is different to SVM since I don't have the parameter C.","Creater_id":60023,"Start_date":"2016-08-15 07:50:26","Question_id":229928,"Tags":["machine-learning","cross-validation","kernel-trick","hyperparameter"],"Answer_count":0,"Last_activity":"2016-08-15 21:05:43","Link":"http://stats.stackexchange.com/questions/229928/range-of-values-for-kernel-tuning-parameter","Creator_reputation":21}
{"_id":{"$oid":"5837a57da05283111e4d4e5d"},"View_count":23,"Display_name":"Simon","Question_score":1,"Question_content":"Say I have data collected from an independent multinomial sampling scheme that violates Cochran's cell expecteds condition. Instead of Pearson's  test of independence, can I use the Bayesian method?If so, I would like to make sure that I am making the right interpretations. The raceDolls example in the BayesFactor package was useful, but I want to extend this to an nx5 table and make sure I understand the output. Here's an example:set.seed(21)#Create fake data. Yes, I could just create a table with those probabilities, but this was more fun.df \u0026lt;- data.frame(subjectType = sample(LETTERS[1:5], 150, replace=TRUE, prob=c(0.15, 0.1, 0.09, 0.22, 0.44)))dfsubjectType == \"A\", \"c(0.05, 0, 0, 0.09, 0.86)\",               ifelse(dfsubjectType == \"C\", \"c(0, 0.23, 0, 0, 0.77)\",                             ifelse(dfsubjectType == \"E\", \"c(0.24, 0.14, 0.02, 0.05, 0.55)\",NA)))))splitDf \u0026lt;- split(df, dfprobs[1], \")\")))))df \u0026lt;- do.call(\"rbind\", splitDf)tabDf \u0026lt;- table(dfsubjectType)#Testschisq.test(tabDf)#fisher.test(tabDf) # common alternative. Assumes fixed totalsfriedman.test(tabDf) # another test that's used as an alternativelibrary(BayesFactor)contingencyTableBF(tabDf, sampleType = \"indepMulti\", fixedMargin = \"cols\")The output shows , so it 14 times more likely under the null hypothesis of independence than the alternative hypothesis.","Creater_id":67446,"Start_date":"2016-08-14 23:06:34","Question_id":229863,"Tags":["bayesian","chi-squared"],"Answer_count":0,"Last_activity":"2016-08-15 20:25:40","Link":"http://stats.stackexchange.com/questions/229863/chi-square-test-alternatives-when-cell-expecteds-condition-violated-bayesian-me","Creator_reputation":56}
{"_id":{"$oid":"5837a57da05283111e4d4e5f"},"View_count":18,"Display_name":"kayle","Question_score":1,"Question_content":"I'm brand new to machine learning - let me know if I should correct terminology or extract anything out into separate questions.I'm attempting to create a board evaluation function for Othello. Each input position consists of 64 squares which can be one of {black, white, empty}. The output is the predicted score of the game which ranges from [-64, 64]. 3^64 is too many positions to calculate, so I first need to select the best features.My understanding is that other top Othello engines use linear regression with ~25 different board feature patterns as an input.The most important 8-square feature patterns are the 4 board edges which each have 3^8=6561 possible configurations. 10-square feature patterns have 3^10=59049 positions and 12-square feature patterns have 531441 positions. I'd expect the larger feature patterns to be more accurate, but also be more expensive to store in memory and train.I can store an Othello position in 16 bytes + 1 byte for the calculated score. So the scale I'm looking at is roughly ~100 million training positions at a time and I'd like to perform the analysis locally on a single machine.When a board is mostly full, I can easily generate millions of solved training positions, but it gets exponentially more expensive when there are more turns remaining in the game. Note that an Othello position has 8 symmetries (4 rotations + 4 mirrored rotations). Also many feature-patterns will be correlated if they share common squares (e.g. each corner square is in two different edge patterns and a diagonal pattern). Should I try to exploit this or exercise any particular cautions?After starting with a model based on the edge feature patterns, how should I try incorporating the next most important feature patterns?Most other programs I've looked at seem to only use a linear model for combining feature pattern terms. However, it seems like it'd be much more accurate to go one step further and combine related feature patterns. For example, two strong edge configurations that share a common corner should be worth more than simply adding their individual contributions. My gut tells me that the final evaluation function should:Calculate the output value for each of the input feature patternsCombine related feature pattern outputs to predict a final score for the entire boardWhat methods should I look at for combining feature pattern terms?How should I initialize each of the feature pattern configurations from my initial set of training positions. Just average the scores?Is there a general rule-of-thumb for how many training position samples I should have for each possible feature pattern?See Too many models to choose from for a related unanswered question.","Creater_id":127789,"Start_date":"2016-08-15 20:08:16","Question_id":230025,"Tags":["regression"],"Answer_count":0,"Last_activity":"2016-08-15 20:08:16","Link":"http://stats.stackexchange.com/questions/230025/how-to-create-best-board-evaluation-function-from-training-positions","Creator_reputation":6}
{"_id":{"$oid":"5837a57da05283111e4d4e61"},"View_count":3931,"Display_name":"Calvin","Question_score":2,"Question_content":"I have run a 1 way ANOVA and carried out some post hocs to see which direction the results go in. If I report the direction of the findings - e.g. this group was higher than the other, which statistic do I quote? I have carried out Games-Howell post hoc and all it seems to come with is an associated p value, is this all that I report? ","Creater_id":7661,"Start_date":"2011-11-28 10:28:56","Question_id":19073,"Tags":["anova","post-hoc"],"Answer_count":2,"Last_activity":"2016-08-15 19:51:03","Link":"http://stats.stackexchange.com/questions/19073/how-to-report-games-howell-post-hoc-tests-following-anova","Creator_reputation":36}
{"_id":{"$oid":"5837a57da05283111e4d4e6f"},"View_count":8931,"Display_name":"Archaeopteryx","Question_score":22,"Question_content":"I've read that the t-test is \"reasonably robust\" when the distributions of the samples depart from normality. Of course, it's the sampling distribution of the differences that are important. I have data for two groups. One of the groups is highly skewed on the dependent variable. The sample size is quite small for both groups (n=33 in one and 45 in the other). Should I assume that, under these conditions, my t-test will be robust to violations of the normality assumption?","Creater_id":14464,"Start_date":"2012-10-08 17:29:25","Question_id":38967,"Tags":["t-test","assumptions","normality","robust"],"Answer_count":5,"Last_activity":"2016-08-15 19:01:31","Link":"http://stats.stackexchange.com/questions/38967/how-robust-is-the-independent-samples-t-test-when-the-distributions-of-the-sampl","Creator_reputation":225}
{"_id":{"$oid":"5837a57da05283111e4d4e80"},"View_count":30,"Display_name":"Roger","Question_score":1,"Question_content":"I have an independent variable (that is a group of 5; could be 1 or 2 or 3 or 4 or 5) and a dependent variable (that is categorical;  could be either Yes or No). I've run a chi-squred test and found significance. How can I run pairwise comparisons between the individual groups (group 1 vs. group 2, group 1 vs. 3) to get the significance between groups. In other words, I am looking for something similar to ANOVA's post hoc Tukey's, except for chi-squared. ","Creater_id":127793,"Start_date":"2016-08-15 18:24:22","Question_id":230018,"Tags":["spss","chi-squared","multiple-comparisons","post-hoc"],"Answer_count":0,"Last_activity":"2016-08-15 18:54:51","Link":"http://stats.stackexchange.com/questions/230018/performing-pairwise-comparisons-for-chi-squred-test-in-spss","Creator_reputation":6}
{"_id":{"$oid":"5837a57da05283111e4d4e82"},"View_count":25,"Display_name":"tzipy","Question_score":1,"Question_content":"My response variable is a combining of two variables which I would like to rescale with different weights. ratio:0.5/0.5; 0.25/0.75; 0.1/0.9.Now I would like to test which is the best fit.The three models have overdispersion so I used quasipoisson regression.Following former instructions in this matter, I used the F test, but it didn't work because of the different response variable. I got this massage:    anova(glm0.5_0.5,glm0.75_0.25,glm0.9_0.1, test = \"F\")        Model: quasipoisson, link: logResponse: datadep0.75_0.25\", \"data$dep0.9_0.1\")’ removed because response differs from model 1what should I do?","Creater_id":74669,"Start_date":"2016-08-15 14:34:40","Question_id":229997,"Tags":["r","regression","goodness-of-fit","quasi-likelihood"],"Answer_count":0,"Last_activity":"2016-08-15 18:43:54","Link":"http://stats.stackexchange.com/questions/229997/compare-quasi-poisson-models-with-different-response-variables","Creator_reputation":167}
{"_id":{"$oid":"5837a57da05283111e4d4e84"},"View_count":56,"Display_name":"confused mathematician","Question_score":0,"Question_content":"I am posting to request help understanding a part of a paper [1] on clustering analysis.  I should mention that I am new to clustering analysisIn [1] the authors use the Earth Mover's Distance (EMD) to cluster a set of approximately 50,000 signatures.They do this in combination with a Complete Linkage Clustering (CLC) algorithm.  In subsection 5.3, at the top of page 642 they state: \"in our experiments, we used complete linkage hierarchical clustering [14] given Earth Mover Distances\".I understand the EMD algorithm and I understand the CLC algorithm, but I cannot see how they are combining the two.One way that I thought to do this would be to run the CLC algorithm, and compute the EMD from the original distribution to each level of the  hierarchy, to see which level is the closest.  This does not seem to be what they are doing.  Earlier in Subsection 5.3 on page 641, the authors write  \"We can then use EMD to compute distances from cluster centers to the entire data set, allowing for both out-lier detection and cluster assignment.\".So somehow, they seem to be using the EMD in the CLC algorithm for clustering assignment. How could they be doing this, since the EMD is for calculating the difference between two distributions, not clusters within the same distribution.I would be grateful for any help anyone could offer. Reference[1] Applegate, David, et al. \"Unsupervised clustering of multidimensional distributions using earth mover distance.\" Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2011. ","Creater_id":127753,"Start_date":"2016-08-15 09:57:16","Question_id":229950,"Tags":["multivariate-analysis","hierarchical-clustering"],"Answer_count":1,"Last_activity":"2016-08-15 18:27:35","Link":"http://stats.stackexchange.com/questions/229950/using-the-earth-movers-distance-in-combination-with-complete-linkage-clustering","Creator_reputation":3}
{"_id":{"$oid":"5837a57da05283111e4d4e91"},"View_count":15,"Display_name":"Peter Pan","Question_score":0,"Question_content":"The arithmetic and geometric averages provide a summary of historical performance. They are both unique in calculation and contain biases (as stated by my lecturer).My question is where exactly are these underlying biases with respect to calculation and under which situation (with example) would a statistician prefer to use the arithmetic average over the geometric and vise versa.Thank you","Creater_id":126796,"Start_date":"2016-08-15 17:57:10","Question_id":230016,"Tags":["arithmetic","geometric-mean"],"Answer_count":0,"Last_activity":"2016-08-15 17:57:10","Link":"http://stats.stackexchange.com/questions/230016/arithmetic-and-geometric-means","Creator_reputation":125}
{"_id":{"$oid":"5837a57da05283111e4d4e93"},"View_count":152,"Display_name":"mlinegar","Question_score":1,"Question_content":"I have been working to fit a normal distribution to data that is truncated to only be zero or greater. Given my data, which I have at the bottom, I previously used the following code: library(fitdistrplus)library(truncnorm)fitdist(testData, \"truncnorm\", start = list(a = 0, mean = 0.8, sd = 0.9))Which, of course, won't work for a number of reasons, not least of which being that the mle estimator provides increasingly negative estimates as a tends towards zero. I previously got some very helpful information about fitting a normal distribution to this data here, where two basic options were presented:I might either use a low, negative value for a, and try out a number of different valuesfitdist(testData, \"truncnorm\", fix.arg=list(a=-.15),        start = list(mean = mean(testData), sd = sd(testData)))or I might set lower bounds for the parametersfitdist(testData, \"truncnorm\", fix.arg=list(a=0),        start = list(mean = mean(testData), sd = sd(testData)),        optim.method=\"L-BFGS-B\", lower=c(0, 0))Either way, though, it seems that some information is being lost - in the first case, because a isn't being truncated at zero, and in the second because the parameters have arbitrary lower bounds - I'm only concerned with achieving a good fit of the data, not with having positive a positive mean for the distribution. Given that mle estimators tend negative as a goes to zero, would it be better to use non-mle estimation? Does it make sense to have negative values of a if the data itself can't be negative?This question applies more generally as well, as I have been using the truncdist package to try to fit Weibull, Log Normal, and Logistic distributions as well (for the Weibull, of course, there is no need to truncate at zero).Finally, here's the data:testData \u0026lt;- c(3.2725167726, 0.1501345235, 1.5784128343, 1.218953218, 1.1895520932,               2.659871271, 2.8200152609, 0.0497193249, 0.0430677458, 1.6035277181,               0.2003910167, 0.4982836845, 0.9867184303, 3.4082793339, 1.6083770189,               2.9140912221, 0.6486576911, 0.335227878, 0.5088426851, 2.0395797721,               1.5216239237, 2.6116576364, 0.1081283479, 0.4791143698, 0.6388625172,               0.261194346, 0.2300098384, 0.6421213993, 0.2671907741, 0.1388568942,               0.479645736, 0.0726750815, 0.2058983462, 1.0936704833, 0.2874115077,               0.1151566887, 0.0129750118, 0.152288794, 0.1508512023, 0.176000366,               0.2499423442, 0.8463027325, 0.0456045486, 0.7689214668, 0.9332181529,               0.0290242892, 0.0441181842, 0.0759601229, 0.0767983979, 0.1348839304)","Creater_id":86065,"Start_date":"2016-08-12 17:03:39","Question_id":229624,"Tags":["r","fitting","truncation","truncated-normal"],"Answer_count":1,"Last_activity":"2016-08-15 17:29:30","Link":"http://stats.stackexchange.com/questions/229624/fitting-truncated-distributions-using-fitdistrplus-with-a-lower-bound-of-zero","Creator_reputation":15}
{"_id":{"$oid":"5837a57da05283111e4d4ea0"},"View_count":32,"Display_name":"arun","Question_score":-1,"Question_content":"I am just curious about the running output of R caret train function.I am doing a grid search for a random forest model. Here is the code:predCount \u0026lt;- ncol(dfTrain) - 1rfGrid \u0026lt;- expand.grid(  #mtry = round( predCount * c(0.75, 0.5, 0.33, 0.1) )  mtry = round( predCount * c(0.33) ))rfTrControl \u0026lt;- trainControl(  method = \"cv\",  number = 5,  verboseIter = TRUE,  returnData = FALSE,  allowParallel = TRUE)rfTrain \u0026lt;- train(  x = as.matrix(dfTrain[, ! names(dfTrain) %in% c(\"totalRet\")]),   y = dfTrain$totalRet,  trControl = rfTrControl,  tuneGrid = rfGrid,  method = \"rf\",  ntree = 1500)Here is the output:+ Fold1: mtry=10 - Fold1: mtry=10 + Fold2: mtry=10 - Fold2: mtry=10 + Fold3: mtry=10 - Fold3: mtry=10 + Fold4: mtry=10 - Fold4: mtry=10 + Fold5: mtry=10 - Fold5: mtry=10 Aggregating results...Why are the folds printed twice? What do the + and - before each fold denote?","Creater_id":108868,"Start_date":"2016-08-15 15:23:38","Question_id":230006,"Tags":["r","caret"],"Answer_count":1,"Last_activity":"2016-08-15 17:22:04","Link":"http://stats.stackexchange.com/questions/230006/why-does-caret-print-each-fold-twice","Creator_reputation":108}
{"_id":{"$oid":"5837a57da05283111e4d4ead"},"View_count":59,"Display_name":"hermitian","Question_score":3,"Question_content":"I have a series of trials such that  takes only binary outcome. There are in total  trials. Denote, . For all  . However there is correlation, between successive trials, ie . Given , how can I find the distributions for ?Thank you very much.","Creater_id":127736,"Start_date":"2016-08-15 08:04:02","Question_id":229931,"Tags":["correlation","binomial"],"Answer_count":1,"Last_activity":"2016-08-15 16:48:29","Link":"http://stats.stackexchange.com/questions/229931/correlated-binomial-distribution","Creator_reputation":35}
{"_id":{"$oid":"5837a57ea05283111e4d4eb9"},"View_count":44,"Display_name":"Charlie Glez","Question_score":1,"Question_content":"I'm trying to explain Y (a count variable), in terms of X, Xsquared, and Size, with random effects for Industry/Firm, by year. My hypothesis is that there is a positive relationship between Y and X, and that this relationship has a inverted-U shape (thus the Xsquared term). I normally do this with normal regression models, but I want to know if it would mean the same in a GLMER one (I'm new with these models)Do my results, here below, support my inverted-U hypothesis, or something else? Any tips on how to show this inverted-U effect graphically in r? Lastly, how can I interpret the random effects part? My GLMER model looks like this:model \u0026lt;- glmer(Y ~ Year + X + Xsquared + Size + (1 + Year|Industry/Firm), data = mydata, family = poisson)Results:Random effects: Groups      Name        Variance    Std.Dev.  Corr  Firm:Industry (Intercept) 1.787e+04 1.337e+02                   Year          4.434e-03 6.659e-02 -1.00 Industry      (Intercept) 7.749e-01 8.803e-01                   Year          1.923e-07 4.385e-04 -1.00Number of obs: 436, groups:  Firm:Industry, 109; Industry, 37Fixed effects:             Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept) 51.697639   9.178129   5.633 1.77e-08 ***Year        -0.027132   0.004535  -5.983 2.19e-09 ***X            1.322702   0.334092   3.959 7.52e-05 ***Xsquared    -0.277335   0.141129  -1.965   0.0494 *  Size         0.321026   0.043825   7.325 2.39e-13 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Correlation of Fixed Effects:         (Intr) Year   DOI    DOI2  Year     -0.999                     DOI       0.004 -0.019              DOI2     -0.009  0.020 -0.953       LNAssets -0.162  0.119  0.003 -0.007Updated results after changing Year to cYearRandom effects: Groups        Name        Variance  Std.Dev. Corr Firm:Industry (Intercept) 5.840e-01 0.764205                    cYear       5.546e-03 0.074474 0.38 Industry      (Intercept) 8.243e-05 0.009079                    cYear       2.011e-05 0.004485 0.54Number of obs: 436, groups:  Firm:Industry, 109; Industry, 37Fixed effects:            Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept) -2.87753    0.47970  -5.999 1.99e-09 ***cYear       -0.02303    0.02778  -0.829    0.407    X            1.32358    0.33632   3.935 8.30e-05 ***Xsquared    -0.27628    0.14217  -1.943    0.052 .  FirmSize     0.31789    0.04989   6.371 1.87e-10 ***","Creater_id":41784,"Start_date":"2016-08-15 10:37:03","Question_id":229959,"Tags":["poisson-regression","glmer"],"Answer_count":2,"Last_activity":"2016-08-15 16:02:33","Link":"http://stats.stackexchange.com/questions/229959/how-to-interpret-squared-term-in-glmer-model","Creator_reputation":101}
{"_id":{"$oid":"5837a57ea05283111e4d4ec7"},"View_count":35,"Display_name":"user918967","Question_score":1,"Question_content":"I have some clinical diagnostic data from three sites that I have been looking at performance against a reference standard using the McNemar test.How would I set up the analysis to investigate if there are any differences between the three sites?  My data (in R) look like:myData \u0026lt;- read.table(header=TRUE, text='Site ReferenceStandard NewDiagnostic CountA R R 47A S R 2A R S 0A S S 387B R R 3B S R 1B R S 7B S S 161C R R 13C S R 0C R S 0C S S 108')ct \u0026lt;- xtabs(Count ~ ReferenceStandard + NewDiagnostic + Site, data=myData)ftable(ct)                                Site   A   B   CReferenceStandard NewDiagnostic                 R                 R                   47   3  13                  S                    0   7   0S                 R                    2   1   0                  S                  387 161 108Do I use the Cochran–Mantel–Haenszel?","Creater_id":44618,"Start_date":"2016-08-15 11:26:11","Question_id":229969,"Tags":["interaction","mcnemar-test"],"Answer_count":1,"Last_activity":"2016-08-15 15:46:23","Link":"http://stats.stackexchange.com/questions/229969/site-interaction-with-mcnemar-tests","Creator_reputation":141}
{"_id":{"$oid":"5837a57ea05283111e4d4ed4"},"View_count":214,"Display_name":"Steve G. Jones","Question_score":20,"Question_content":"Having included an quantile regression model in a paper, the reviewers want me to include adjusted  in the paper. I have calculated the pseudo-s (from Koenker and Machado's 1999 JASA paper) for the three quantiles of interest for my study.However, I have never heard of an adjusted  for quantile regression and wouldn't know how to calculate it. I am asking you for either of the following:preferably: a formula or approach on how to meaningfully calculate an adjusted  for quantile regression.alternatively: convincing arguments to provide to the reviewers as to why there isn't such a thing as an adjusted  in quantile regression. ","Creater_id":121926,"Start_date":"2016-07-04 07:52:35","Question_id":222084,"Tags":["goodness-of-fit","r-squared","quantile-regression"],"Answer_count":2,"Last_activity":"2016-08-15 15:29:15","Link":"http://stats.stackexchange.com/questions/222084/is-there-such-a-thing-as-an-adjusted-r2-for-a-quantile-regression-model","Creator_reputation":113}
{"_id":{"$oid":"5837a57ea05283111e4d4ee2"},"View_count":86,"Display_name":"Gino Strato","Question_score":5,"Question_content":"In Pattern Recognition and Machine Learning Bishop writes about Bayes networks:  For practical applications of probabilistic models, it will typically  be the highernumbered variables corresponding to terminal nodes of the  graph that represent the observations, with lower-numbered nodes  corresponding to latent variables.  The primary role of the latent  variables is to allow a complicated distribution over the observed  variables to be represented in terms of a model constructed from  simpler (typically exponential family) conditional distributions.And after a few lines:    The hidden variables in a probabilistic model need not, however, have  any explicit physical interpretation but may be introduced simply to  allow a more complex joint distribution to be constructed from simpler  components.What do you think he means by this type of hidden variables (with no physical interpretation)?What can be a simple example of this?I thought about mixture of gaussians, but they don’t correspond to a situation where the variables we are interested are highernumbered.","Creater_id":20359,"Start_date":"2013-02-01 05:16:54","Question_id":49061,"Tags":["latent-variable","bayesian-network"],"Answer_count":2,"Last_activity":"2016-08-15 15:28:31","Link":"http://stats.stackexchange.com/questions/49061/latent-variables-in-bayes-nets-with-no-physical-interpretation","Creator_reputation":51}
{"_id":{"$oid":"5837a57ea05283111e4d4ef0"},"View_count":87,"Display_name":"user22478","Question_score":1,"Question_content":"I know that this kind of discussion might be both a silly...very silly and a hot issue simultaneously...but I thought it would be nice to raise it : Which types of linear AND non-linear regression could be recommended when you have a small sample size (e.g. 50-70 records) and you want to predict a continuous outcome from 5-7 predictors (both categorical and continuous). Can k-fold cross-validated stepwise linear regression OR Partial Least Regression be a remedy to this problem? Are boosted trees OR Neural networks just insane even to think about them? Your responses are GREATLY WELCOMED and VERY MUCH APPRECIATED. THANK YOU!","Creater_id":22478,"Start_date":"2013-05-09 13:01:23","Question_id":58596,"Tags":["regression","sample-size"],"Answer_count":1,"Last_activity":"2016-08-15 15:13:21","Link":"http://stats.stackexchange.com/questions/58596/types-of-regression-that-might-work-on-small-samples","Creator_reputation":60}
{"_id":{"$oid":"5837a57ea05283111e4d4efd"},"View_count":44,"Display_name":"Econstat","Question_score":2,"Question_content":"I have a variable whose values range between 1 for strongly disagree and 5 strongly agree. I want to test the hypothesis if the respondents (managers) agree on the new implemented procedure. So I want to check if the mean is 3.5, but the data is not normal so I cannot use a one-sample t-test. What can I use in this case?","Creater_id":127780,"Start_date":"2016-08-15 14:28:21","Question_id":229995,"Tags":["hypothesis-testing","nonparametric"],"Answer_count":1,"Last_activity":"2016-08-15 15:12:51","Link":"http://stats.stackexchange.com/questions/229995/alternative-to-one-sample-t-test-when-data-is-not-normal","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d4f0a"},"View_count":1368,"Display_name":"Elena Wimmerwuchs","Question_score":2,"Question_content":"Both can be used for an assessment of model accuracy, but what is the difference? formula for coefficient of determination, or R²:with:  SSres= sum (yi - fi)²   and    SStot = sum (yi - ymean)²y = observed values (for model evaluation), f = modelled/predicted values.source:wikipediaformula for Nash-Sutcliffe efficiency: source: journal article \"Model evaluation guidelines for systematic quantification of accuracy in watershed simulations\", Moriasi et al. 2007Do I miss something here or are the formulas identical? Sorry for my expression.","Creater_id":89925,"Start_date":"2015-12-09 06:02:47","Question_id":185898,"Tags":["model","accuracy","coefficient","model-evaluation"],"Answer_count":2,"Last_activity":"2016-08-15 15:12:06","Link":"http://stats.stackexchange.com/questions/185898/difference-between-nash-sutcliffe-efficiency-and-coefficient-of-determination","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d4f17"},"View_count":11,"Display_name":"Sav","Question_score":2,"Question_content":"Anyone know how to find a sample size estimate for nested ancova? G*power only does Ancova. I do not need t done for me (unless you want too :)) but how to calculate the est. sample size. OrHow does the nested aspect of the ANCOVA change the sample size needed once it becomes nested. Thanks!","Creater_id":127784,"Start_date":"2016-08-15 15:09:20","Question_id":230003,"Tags":["ancova","nested"],"Answer_count":0,"Last_activity":"2016-08-15 15:09:20","Link":"http://stats.stackexchange.com/questions/230003/nested-ancova-sample-size-est","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d4f19"},"View_count":23,"Display_name":"FKG","Question_score":2,"Question_content":"I am a bit puzzled. Assume an international forum where governments, NGOs and companies vote either in favor or against a particular agreement/treaty. I want to analyze the determinants of the voting behavior of each actor. What is puzzling in this case is that the number of governments, NGOs and companies is not the same (although NGOs and companies officially represent countries but are independent from how their governments vote). So for example, for the year 2000, 50 governments were selected to this forum, 45 NGOs and 25 companies. One way of analyzing their voting behavior is to generate a sample for each actor and analyze it separately. Are there any other ways to analyze this? I am going to use their voting behavior as my dependent variable, so combining them into one (government + NGO or government + company) does not work as their number differ.  ","Creater_id":53160,"Start_date":"2016-08-15 14:51:15","Question_id":230001,"Tags":["regression","sample-size"],"Answer_count":0,"Last_activity":"2016-08-15 14:51:15","Link":"http://stats.stackexchange.com/questions/230001/how-to-handle-a-sample-when-the-size-differs","Creator_reputation":50}
{"_id":{"$oid":"5837a57ea05283111e4d4f1b"},"View_count":107,"Display_name":"user08041991","Question_score":6,"Question_content":"A reviewer of mine is asking for a reason why I have used unweighted data, instead of weighted data. I have discussed the issue with a statistician and his response was along the lines of   If you have independent observations and you take the overall mean, its variance is always smaller than the variance from a weighted mean as the estimator. ... So confidence intervals will be widened!I have since found the following question on this website, and from my understanding, they suggest that the variance should be the same.So can someone, please, with a more statistically gifted mind than mine, please confirm the response from the statistician and explain in layman terms the theory, or  with a worked example.","Creater_id":94093,"Start_date":"2016-08-14 07:33:41","Question_id":229781,"Tags":["variance","weighted-mean","weighted-data"],"Answer_count":2,"Last_activity":"2016-08-15 14:36:41","Link":"http://stats.stackexchange.com/questions/229781/variance-of-weighted-mean-greater-than-unweighted-mean","Creator_reputation":50}
{"_id":{"$oid":"5837a57ea05283111e4d4f29"},"View_count":17,"Display_name":"slips","Question_score":2,"Question_content":"I just have a quick question on using sampling techniques in survival analysis.I am currently working on a project where we are trying to predict which patients should get a specific treatment. I have done KM curves, and a Cox analysis, all seeming to confirm what we have previously shown and are trying to nail down.We are also trying to use a Multi-task Logistic Regression technique called patient specific survival prediction (PSSP: http://pssp.srv.ualberta.ca/)We currently only have a small dataset (~200 patients, with 50 events). We are currently running PSSP on the dataset with 13 features (recorded medical data).Due to the small number of events to censored data about 1:4, my classification results were not what I had expected. So I wanted to try some techniques to help balance the classes.I looked into techniques to either subsample the larger class or boost the rarer class with more examples can be used. I did this using a Synthetic Minority Oversampling Technique (SMOTE) and ENN algorithms implemented in python.Using these sampling algorithms improved my performance as I had predicted they would now that the classes were closer to balanced at the cutoff points we looked at.So the Question is: Is it feasible to use upsampling and downsampling techniques with survival data in order to balance the classes for prediction algorithms, or are we inherently biasing our classifier due to the synthetically generated patients with events. And If we are biasing the classifier, how can we tell?First time posting to a stack exchange site, please let me know.Thanks.","Creater_id":127773,"Start_date":"2016-08-15 14:05:49","Question_id":229993,"Tags":["sampling","survival"],"Answer_count":0,"Last_activity":"2016-08-15 14:05:49","Link":"http://stats.stackexchange.com/questions/229993/use-of-super-sub-sampling-techniques-in-survival-analysis","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d4f2b"},"View_count":11,"Display_name":"CMUEngineer","Question_score":1,"Question_content":"Currently, I am attempting to use the G-Means algorithm to attempt to solve the ill-defined image segmentation problem as it is a very algorithmically cheap solution to the ill-defined high-dimensional clustering problem. The specifics about this particular implementation I'm using can be found at this blog post on The Data Science Lab. I'm finding that for if I use a small maximum K value (it's easy to end up with a ridiculous amount of clusters for image data), the algorithm tends to select K=2 as the optimal clustering, when from qualitative analysis of the image it would seem like there are 3-4 feature clusters. Looking at the math again, there seems to be an unwanted local minimum for the gap statistic at K=2, which would affect clustering if the maximum K is small enough. For the purpose of illustrating the algorithm to the reader, I will define the G-Means algorithm's gap statistic by the code I am using. The gap statistic function is defined as such:def get_alpha(k,ndim,previous_alpha):    if k\u0026lt;=2:        alpha = 1-(3/(4*ndim))    else:        alpha =  previous_alpha + ((1-previous_alpha)/6)    return alpha def gap_statistic(inertia,previous_inertia,k,ndim,previous_alpha):    alph = alpha(k,ndim,previous_alpha)    gap = inertia/(alph*previous_inertia)    return alpha, gapThe G-Means algorithm iterates over K from K=1 to some hyper-parameter K=Max(K), calculating the gap statistic for each K except for K=1 (since the inertia of the previous K is undefined for K=1). It stores the alpha values from the gap_statistic calculation for the next iteration. The inertia is easily obtained from sci-kit learn's implementation of the KMeans algorithm. It seems to me that making alpha an increasing function, along with the fact that the change in inertia from K=1 to K=2 will intuitively be incredibly large for reasonably clustered data, leads to an unwanted local minima in the gap statistic at K=2. Is this intuition wrong? Is there a proof for this minimum (based upon the definition of inertia), and what would be a way to implement the gap statistic for a low Maximum K and not have this local minimum affect the clustering results?Thank you!","Creater_id":124469,"Start_date":"2016-08-15 13:57:49","Question_id":229992,"Tags":["clustering","algorithms","k-means"],"Answer_count":0,"Last_activity":"2016-08-15 13:57:49","Link":"http://stats.stackexchange.com/questions/229992/does-a-local-minima-exists-at-k-2-in-the-gap-statistic-for-the-g-means-algorithm","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d4f2d"},"View_count":39,"Display_name":"Dello","Question_score":2,"Question_content":"I have a  matrix  containing  realisations of a -variate random vector. I compute Mahalanobis distances to assess multivariate normality of the random vector. Since the sample covariance matrix is singular, I use the Matlab function CorrelationMatrix by D. Sun to compute the \"nearest\" positive definite correlation matrix as followscorm = CorrelationMatrix(corr(X),ones(size(X,1),1),1.0e-5,1.0e-6).Though Matlab is able to compute the inverse of , the  diagonal entries of  are all equal to each other, and all non-diagonal entries are also equal. If I exclude  columns of , distances become unequal. Does anybody know why? Many thanks!","Creater_id":127771,"Start_date":"2016-08-15 13:39:54","Question_id":229988,"Tags":["matrix","linear-algebra","multivariate-normal"],"Answer_count":0,"Last_activity":"2016-08-15 13:39:54","Link":"http://stats.stackexchange.com/questions/229988/mahalanobis-distances-all-equal-for-a-low-rank-data-matrix","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d4f2f"},"View_count":39,"Display_name":"Marta_ro","Question_score":3,"Question_content":"I am applying a GLM (using SPSS v. 18) using negative binomial with log link structure for analyzing the effects of different predictors in crash frequency. From what I understand model form isHowever one of the independent variables is an exposure variable, and its effect is different from other independent variables in a way that model form should be:, where  is the exposure variable and  are the other explanatory variablesMy question is: how to model this using SPSS?","Creater_id":127747,"Start_date":"2016-08-15 09:10:01","Question_id":229942,"Tags":["generalized-linear-model","negative-binomial"],"Answer_count":1,"Last_activity":"2016-08-15 13:18:00","Link":"http://stats.stackexchange.com/questions/229942/help-with-syntax-for-glm-negative-binomial-in-spss","Creator_reputation":16}
{"_id":{"$oid":"5837a57ea05283111e4d4f3c"},"View_count":28,"Display_name":"shruthi","Question_score":2,"Question_content":"I want to compare consent rate for 2 tests on the same population. For example in a sample of 60 people, 55 gave consent to test A, 45 gave consent to test B, then consent rate for test A would be 91.7% and for test B would be 75%. How to know whether they are statistically significant?","Creater_id":127769,"Start_date":"2016-08-15 13:04:25","Question_id":229984,"Tags":["hypothesis-testing"],"Answer_count":0,"Last_activity":"2016-08-15 13:04:25","Link":"http://stats.stackexchange.com/questions/229984/compare-two-percentages-from-same-population","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d4f3e"},"View_count":59,"Display_name":"Eduardo Gonzatti","Question_score":2,"Question_content":"I know this probably is a very newbie question, but I haven't been able to find anything about it elsewhere. I'm running some OLS regressions on high frequency data of stocks in order to model some imbalances, all done in R and checked with Excel. But, in my tests,  I'm constantly seeing that the residuals of the regressions yield far better results than the regular dependent variable (Y) would when used in the OPOSITE of the original logic. residue \u0026lt; lower threshold (negative int), then, BUY regular meta code would be: \\hat y \u0026gt; upper threshold (positive int), then, BUY I tried this because of a mistake in the beginning of my coding and later found out it was a far better predictor in the out of sample analysis I ran. So I kept up testing and this kept showing up in the results. The possible explanation I thought about this is that, if the residue is large enough, I should expect that the contrary of it to happen next. Like a mean reversal logic of the error. If the error is large enough, it will probably return to the mean,  so I use it as the \"- (Y hat)\" Is there some kind of explanation to this, is this some kind of statistical technique that I just don't know? Or I'm am I just delusional and this makes no sense at all, using the residuals  as a predictor for my model? Thank in advance EDIT : What is the expected correlation between residual and the dependent variable? talks about this kind of comparison between residuals and Y.. Not much value in it. I'll post the other scatter plots of residuals against fitted values and Y against fitted values. ","Creater_id":127682,"Start_date":"2016-08-14 18:10:56","Question_id":229846,"Tags":["multiple-regression","forecasting","least-squares","prediction","residual-analysis"],"Answer_count":1,"Last_activity":"2016-08-15 13:00:35","Link":"http://stats.stackexchange.com/questions/229846/can-i-use-the-residuals-of-a-multiple-linear-regression-instead-of-the-predicted","Creator_reputation":13}
{"_id":{"$oid":"5837a57ea05283111e4d4f4b"},"View_count":27,"Display_name":"b_pcakes","Question_score":1,"Question_content":"I know that normally, there are ways to deal with missing features (the most basic of which is to just take the mean or mode (for categorical features) of the feature over all samples). How is this dealt with in online learning? Of course the previous mentioned approach can also possibly be used with a few changes, but I'm wondering whether there is a more standard approach to dealing with this?Online learning: https://en.wikipedia.org/wiki/Online_machine_learningHere is an example scenario: The features of the dataset are survey results, let's say the survey is questions about whether a particular survey taker likes certain bands (for example \"Do you like Radiohead\"), and suppose I want to be able to predict whether the surveytaker will like the band Nine Inch Nails. In online learning, how it would work is that every time I receive a new survey submission, I would output a prediction based on my current model, and then I would ask the user if the prediction is correct, and then based on their response, I would update my model based on that new sample. The point is that, in this scenario I would not be able to easily use imputation techniques that rely on information from the rest of the samples (such as mean/mode), because every time I get a new sample I would need to recalculate it over the existing samples.","Creater_id":124743,"Start_date":"2016-08-14 13:47:31","Question_id":229821,"Tags":["missing-data","online"],"Answer_count":0,"Last_activity":"2016-08-15 12:58:43","Link":"http://stats.stackexchange.com/questions/229821/how-to-deal-with-missing-features-in-online-learning","Creator_reputation":128}
{"_id":{"$oid":"5837a57ea05283111e4d4f4d"},"View_count":4835,"Display_name":"Surya Dewangga","Question_score":7,"Question_content":"I want to create a code for plotting ACF and PACF from time-series data. Just like this generated plot from minitab (below).I have tried to search the formula, but I still don't understand it well. Would you mind telling me the formula and how to use it please? What is the horizontal red line on ACF and PACF plot above ? What is the formula ?Thank You,","Creater_id":63794,"Start_date":"2014-12-14 06:21:55","Question_id":129052,"Tags":["correlation","data-visualization","autocorrelation","partial-correlation"],"Answer_count":3,"Last_activity":"2016-08-15 12:54:47","Link":"http://stats.stackexchange.com/questions/129052/acf-and-pacf-formula","Creator_reputation":36}
{"_id":{"$oid":"5837a57ea05283111e4d4f5c"},"View_count":191,"Display_name":"user22119","Question_score":4,"Question_content":"When should I use weights when performing a logistic regression? The weights I'm referring to are sampling weights from a survey? Or should I just use the unweighted data?","Creater_id":22119,"Start_date":"2014-02-25 02:34:39","Question_id":87779,"Tags":["logistic","weighted-regression","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-15 12:51:42","Link":"http://stats.stackexchange.com/questions/87779/are-sampling-weights-necessary-in-logistic-regression","Creator_reputation":264}
{"_id":{"$oid":"5837a57ea05283111e4d4f69"},"View_count":55,"Display_name":"jkff","Question_score":4,"Question_content":"Consider the problem of taking a weighted sample of size  from a stream of unknown but finite size  in a single pass.Reservoir sampling solves this by assigning each item from the stream with a key , where  is a random number in 0..1 and  is the item's weight, and maintaining the set of  items with the largest key.When a new item arrives, we compute its key and insert it into the set. Let us say that the item is skipped (never enters the set) if its computed key is smaller than the smallest key in the set, otherwise let us say that the item is competitive (it replaces some previous item in the set). Some implementations of reservoir sampling may use this as an optimization to avoid fully populating the sample datum in case it is not competitive.I'm curious, what is the expected number of competitive items (i.e. number of replacements), relative to  (stream size) and  (sample size)? I was not able to find the answer to this question in any of the descriptions of reservoir sampling I found on the internet.","Creater_id":37243,"Start_date":"2015-01-12 09:49:03","Question_id":133132,"Tags":["sampling","algorithms","asymptotics","weighted-sampling","survey-sampling"],"Answer_count":1,"Last_activity":"2016-08-15 12:50:32","Link":"http://stats.stackexchange.com/questions/133132/expected-number-of-replacements-during-weighted-reservoir-sampling","Creator_reputation":156}
{"_id":{"$oid":"5837a57ea05283111e4d4f76"},"View_count":57,"Display_name":"Justin","Question_score":6,"Question_content":"Let's say that you were wanting to model how many times someone had to take a certain test before passing (depending on a range of predictors like practice, mock tests taken, classes attended, etc.). Let's also say that most people pass after the first attempt, but that others have to take the test several times, and that the distribution looks Poisson-ish. If you were to model the dependent variable as the number of tests taken, your minimum count would be 1. On the other hand, if you wanted to model the dependent variable as the number of resits needed, the minimum count would be 0. Both of these seem to me to a reasonable thing to do, and the latter is just the former minus 1, i.e. is shifted.   It also seems, conceptually like this difference (tests~x1+x2+x3... vs. resits~x1+x2+x3... or tests-1~x1+x2+x3...) shouldn't really affect your ultimate conclusions: if practice decreases the number of tests, it should also decrease the number of resits, and it seems like it should do so to a similar extent. My questions are: What is the practical effect on the model parameters of using (a) shifted dependent variable (resits) rather than (b) the unshifted one (tests)? For instance, would you generally expect the parameter to be overestimated if using resits, or underestimated? If either, would you generally expect the difference to be substantial or minor? Or would this all depend so much on the particular data set that there's no way to tell? That is, is the conceptual similarity between tests and resits misleading, in as far as it makes me think I should get similar results for both.What is the practical effect on the model parameters of using:(a) a zero-truncated model - e.g., in R, I'd specify:vglm(tests~x, data, family=pospoisson())  and(b) a left-shifted model - e.g. in R, glm(resits~x, family=poisson)?There is a discussion of shifting vs. truncation here but this discussion doesn't specifically address things like the model parameters and significance. It also focuses on right-shifting rather than left-shifting. I have tried the various options above on my data and it turned out that the basic Poisson (y~x, fam=poisson) had a lower estimate for the predictor than the zero-truncated (y~x, fam=pospoisson), which in turn had a lower estimate than the left-shifted model (y-1~x, fam=poisson). Bootstrapped confidence intervals suggest that these differences are not significant, though. However, doing this hasn't told me whether I can expect this to hold generally, i.e., whether the conceptual similarity between tests and resits should typically translate into similar models. In my case, left-shifting resulted in a higher parameter than the zero-truncation model, but is that generally the case? In my case, the parameters weren't significantly different, but is that generally the case? I realize that someone might be able to derive an answer to all this from first principals, mathematically, but I don't have the mathematical background to do so. I'm asking this as a prelude to another question, here. For reasons that I'll explain in that post, I have to left-shift my response variable and I'm wanting to know whether this is in principle problematic (in which case I've just been lucky that the model parameters are quite similar).*Edit: My data is not in the form of test-counts vs resit-counts. I'm just using these as illustrations because the conceptual similarity between tests and resits is fairly obvious. So my question is not about what regression someone should use for such variables, but is rather about what the effect of shifting vs truncating is on model parameters - would you expect a trivial difference, a significant difference, or there's no way to tell without data? Since people suggested negative binomials below, though, I'm happy to accept answers to this question concerning either Poisson or negative binomial models.","Creater_id":127487,"Start_date":"2016-08-12 07:31:23","Question_id":229547,"Tags":["r","poisson","regression-coefficients","negative-binomial"],"Answer_count":2,"Last_activity":"2016-08-15 12:42:40","Link":"http://stats.stackexchange.com/questions/229547/positive-poisson-regression-what-is-the-effect-on-the-model-of-shifting-vs-trun","Creator_reputation":86}
{"_id":{"$oid":"5837a57ea05283111e4d4f84"},"View_count":37,"Display_name":"Kristi P","Question_score":2,"Question_content":"I am doing power analyses in SAS using PROC GLMPOWER to solve for a total sample size (N Total). I have clustered data (Ward method), to account for variability. There are 4 clusters of varying sample sizes (16, 38, 6, 6) and they are split evenly between test and control groups, i.e. half of each cluster will be in the test group and half will be in the control group. When running the power analyses, my N total increases greatly (from 32 to 132) when I include weights for the clusters vs when I don't. I find that when I use weights reflecting relative proportion rather than actual sample size, I get different N Totals, which seems incorrect. Is weighting clusters in power analysis necessary when using clustered data? What is the correct weighting method (sample size vs relative)?","Creater_id":66693,"Start_date":"2015-01-15 15:22:34","Question_id":133622,"Tags":["power-analysis","weighted-data","cluster-sample","weights"],"Answer_count":0,"Last_activity":"2016-08-15 12:39:21","Link":"http://stats.stackexchange.com/questions/133622/cluster-weights-in-proc-glmpower","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d4f86"},"View_count":321,"Display_name":"mark step","Question_score":6,"Question_content":"I have a (small) population from which I wish to sample. I assign probabilities proportional to . I enumerate the possible samples and then determine the probability of each sample occurring based on the product of the probabilities for each  in the sample. I add up the probabilities for the samples that contain the  and I believe (incorrectly?) that under the assumption of independence (i.e. with replacement sampling) this gives me the inclusion probability for . I look at the inclusion probabilities returned by the inclusionprobabilities function in the sampling package and I get a different answer. I do not understand why, is someone able to explain?library(survey)library(sampling)library(gtools)set.seed(123)y \u0026lt;- c(1190,26751,68570,34536)p \u0026lt;- y/sum(y)df \u0026lt;- data.frame(permutations(n=length(y), r=2, v=1:length(y), repeats.allowed = T))dfX1] * p[dfX1  | 1 == dfp)pik \u0026lt;- inclusionprobabilities(y, 2)data.frame(pik=pik,name=1:length(y))Update:Thanks both @whuber and @StasK. It is clear that the inclusion probabilities reflect sampling without replacement. However, I am uncertain what the inclusion probabilities returned by inclusionprobabilities are. They seem to be calculated as:n \\frac{y_i}{\\sum_{i=1}^{N} y_i}and have an adjustment to ensure that no probability is greater than 1 and also that the sum of the probabilities corresponds to the sample size.If I assume that my population is  such that the probabilities of selection are ,  and  and then I take a sample of 2, I calculate the inclusion probabilities to be ,  and  respectively. Clearly, these are not what is returned by inclusionprobabilities and so my question now is have I calculated the inclusion probabilities incorrectly or is the inclusionprobabilities  function returning something that represents the inclusion probabilities but isn't actually the inclusion probabilities?myn \u0026lt;- 2a \u0026lt;- c(1,2,3)p \u0026lt;- myn * a/sum(a); p[1] 0.3333333 0.6666667 1.0000000inclusionprobabilities(a, myn)[1] 0.3333333 0.6666667 1.0000000Thanks.","Creater_id":24865,"Start_date":"2015-01-23 02:05:43","Question_id":134624,"Tags":["r","sampling","survey-sampling"],"Answer_count":1,"Last_activity":"2016-08-15 12:37:38","Link":"http://stats.stackexchange.com/questions/134624/difference-between-calculated-inclusion-probability-and-what-is-returned-by-samp","Creator_reputation":363}
{"_id":{"$oid":"5837a57ea05283111e4d4f93"},"View_count":28,"Display_name":"CreamStat","Question_score":0,"Question_content":"I have a data base of schools per district and the number of teachers per school. I want to select a sample where between 1 an 10 schools are selected in each district. Once a school is selected, the number of teachers selected should be proportional to the total number of teachers in the sample.As we see, the districts are stratums, and the schools too.EDIT:  The maximum error of the proportion sholud be 0.035 and a 95% intervale confidence. I'm new to probabilistic sampling. Is there any good tutorial or some advice to select the sample?","Creater_id":37311,"Start_date":"2015-02-09 16:54:00","Question_id":137008,"Tags":["sampling","stratification","cluster-sample","survey-sampling"],"Answer_count":0,"Last_activity":"2016-08-15 12:08:19","Link":"http://stats.stackexchange.com/questions/137008/ideas-for-sampling-design","Creator_reputation":145}
{"_id":{"$oid":"5837a57ea05283111e4d4f95"},"View_count":212,"Display_name":"fmark","Question_score":3,"Question_content":"I'm analysing a data set to which I do not have full access but am allowed to submit a limited number stata commands. These data used a complex survey design which is obscured for privacy reasons. However, a series of 60 replicate weight variables have been created for weighting with the svr suite of commands.Is it possible to perform multilevel logistic regression with the svr suite? If so, what commands should I use? I'm fairly new to stata, but need to use it due to the privacy controls around these data.","Creater_id":179,"Start_date":"2015-04-11 17:59:16","Question_id":145930,"Tags":["stata","multilevel-analysis","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-15 12:06:32","Link":"http://stats.stackexchange.com/questions/145930/multilevel-regression-modelling-with-replicate-weights-in-stata","Creator_reputation":2040}
{"_id":{"$oid":"5837a57ea05283111e4d4fa2"},"View_count":531,"Display_name":"Fabr\u0026#237;cio Fialho","Question_score":1,"Question_content":"I am analyzing a dataset that has a variable for post-stratification weights. As this is a complex survey, the plan is to use the R survey package. I have been reading its documentation and feel like able to set a survey design correctly. So far, so good. That said, one aspect is still not clear for me.Lumley says that survey assumes weights are sampling weights -- i.e. 1/(prop of selection for that observation):  Survey designs are specified using the svydesign function. The main arguments to the the function are id to specify sampling units (PSUs and optionally later stages), strata to specify strata, weights to specify sampling weights, and fpc to specify finite population size corrections. These arguments should be given as formulas, referring to columns in a data frame given as the data argument. (http://r-survey.r-forge.r-project.org/survey/example-design.html)My dataset does not include a variable for sampling weights. Its weight is a post-stratification weight accounting for probability of selection, unit non-responses, and post-stratifies the sample to match the age and gender joint distribution. The post-stratification weight is rescaled to sample size -- there are 1,000 observations so sum(poststratification.wt)=1,000, ranging from ~0.9 to ~5.5. I have closely inspected the data and the info available does not allow me to estimate the probability weights from the scratch.So my question is: Am I safe, or roughly safe, using the provided post-stratification weight in the svydesign(weights=) argument? If not, what should I do? (Running a 1,000 survey is out of my budget possibility, hehe).","Creater_id":59772,"Start_date":"2015-04-16 23:04:18","Question_id":146841,"Tags":["r","survey","stratification","survey-weights","survey-sampling"],"Answer_count":2,"Last_activity":"2016-08-15 11:48:36","Link":"http://stats.stackexchange.com/questions/146841/using-post-stratification-weights-in-r-survey-package","Creator_reputation":16}
{"_id":{"$oid":"5837a57ea05283111e4d4fb0"},"View_count":42,"Display_name":"hakaa","Question_score":2,"Question_content":"I have dataset with product prices observed during period of time. E.g.day_number | price 1          | 103          | 116          | 812         | 915         | 1220         | ??Price is recorded in data only on some days - it's missing for most of the days. If we assume that daily price changes are normally distributed with mean of zero and some standard deviation , how can we calculate (based on data) 95% prediction interval for price on day 20?I guess I need to first find most likely parameters for  and then use formula for distribution of sum of random variables and get the prediction interval from there?  ","Creater_id":113945,"Start_date":"2016-08-15 10:07:32","Question_id":229953,"Tags":["time-series","normal-distribution","missing-data","prediction-interval"],"Answer_count":1,"Last_activity":"2016-08-15 11:40:38","Link":"http://stats.stackexchange.com/questions/229953/95-prediction-interval-for-price-change","Creator_reputation":23}
{"_id":{"$oid":"5837a57ea05283111e4d4fbc"},"View_count":37,"Display_name":"hxd1011","Question_score":1,"Question_content":"Suppose I am fitting a logistic regression with intercept only, which is equivalent tho using the count to estimate the outcome probability and make prediction.Can I say following?  We are using prior only to make the prediction.I think some persons from CV corrected me, that the word prior has close relationship with Bayesian statistics, and the statement is not correct.If it is not accurate, what should I say if we only use the counts to estimate the outcome probability?","Creater_id":113777,"Start_date":"2016-08-15 11:23:31","Question_id":229968,"Tags":["logistic","bayesian","terminology"],"Answer_count":0,"Last_activity":"2016-08-15 11:23:31","Link":"http://stats.stackexchange.com/questions/229968/the-usage-of-word-prior-in-logistic-regression-with-intercept-only","Creator_reputation":4443}
{"_id":{"$oid":"5837a57ea05283111e4d4fbe"},"View_count":112,"Display_name":"VasoGene","Question_score":2,"Question_content":"I have the following boxplots of a quantitative trai by medication group. I would like to compare Med0 against all the other groups. Which statistical test in R would be appropriate in this case?Med0 are basically healthy individuals and med1-4 cases in different medications. All groups contain independent samples.Thanks","Creater_id":null,"Start_date":"2016-08-15 01:29:36","Question_id":229944,"Tags":["r","mean"],"Answer_count":2,"Last_activity":"2016-08-15 11:16:44","Link":"http://stats.stackexchange.com/questions/229944/comparison-of-means-in-r","Creator_reputation":null}
{"_id":{"$oid":"5837a57ea05283111e4d4fcb"},"View_count":197,"Display_name":"mangoengineer","Question_score":4,"Question_content":"I've been trying to improve the performance of my random forest model, and read the following paper on feature selection using random forest (see algorithm in section IV: Overfitting - A. Feature Selection):http://ftp.cs.nyu.edu/mishra/PUBLICATIONS/Heritage11.pdfMy understanding is, suppose there are 5 predictors: [A, B, C, D, E], the algorithm does the following:run_random_forest(data=[A, B, C, D, E], max_features=5) =\u003e OOB=0.5, least_important_feature = [B]delete [B] from the data filerun_random_forest(data=[A, C, D, E], max_features=4) =\u003e OOB=0.6,least_important_feature = [C]delete [C] from the data filerun_random_forest(data=[A, D, E], max_features=3) =\u003e OOB=0.5,least_important_feature = [A]Since OOB score in step 5 is smaller than OOB score in step 3, the \"optimal\"    max_features is 4run_random_forest(data=[A, B, C, D, E], max_features=4), and rankthe feature importance.Here I have 2 questions:1) Am I understanding the algorithm correctly?2) What happens after step 7? If the rank of feature importance after step 7 is D\u003eE\u003eC\u003eB\u003eA with max_features=4, do we then:delete feature [A] forever from the data file, and only train the random forest with run_random_forest(data=[B, C, D, E], max_features=4), and predict with [B, C, D, E]?or do we still keep feature [A] from the data file, and train the random forest with run_random_forest(data=[A, B, C, D, E], max_feature=4), and predict with [A, B, C, D, E]?Help is really appreciated. Thanks a lot in advance!Best Regards,mangoengineer","Creater_id":81772,"Start_date":"2015-07-16 00:31:25","Question_id":161733,"Tags":["feature-selection","random-forest","cart"],"Answer_count":1,"Last_activity":"2016-08-15 10:56:30","Link":"http://stats.stackexchange.com/questions/161733/how-does-feature-selection-work-in-random-forest","Creator_reputation":26}
{"_id":{"$oid":"5837a57ea05283111e4d4fd8"},"View_count":47,"Display_name":"sof_dff","Question_score":2,"Question_content":"I've recently came across a Wiki article about Concentration of Measure.At the very beginning of this article, in general setting we see the following:  Let  be a metric measure space, .Then,   \\alpha(\\epsilon):=\\sup \\{\\mu(X / A_{\\epsilon} )|\\mu(A)\u0026gt;1/2\\}In this case  is an extension of some set . So my question is what does mean in this context that ? Does it essentially mean the requirement for  is to occupy half of some volume?","Creater_id":46179,"Start_date":"2016-08-15 07:17:24","Question_id":229922,"Tags":["probability","measure-theory"],"Answer_count":0,"Last_activity":"2016-08-15 10:50:35","Link":"http://stats.stackexchange.com/questions/229922/understanding-basics-concepts-of-measure-concentration","Creator_reputation":133}
{"_id":{"$oid":"5837a57ea05283111e4d4fda"},"View_count":301,"Display_name":"coip","Question_score":4,"Question_content":"I would like to use an F-Test for Equality of Variances on a variable to compare two groups. Normally, this would be done with an sdtest command in Stata or a var.test command in R. However, the data come from a multi-stage, stratified random sample with disproportionate selection, and thus, need to be weighted. Both Stata and R have packages geared at running analyses with complex sampling weights. In Stata it is done via the svyset command and subsequent analyses are ran with the svy prefix (e.g. svy: mean). Likewise, it is done in R via the survey package, which also subsequently conducts analyses with a svy prefix (e.g. svymean).But, unless I have overlooked something, neither seem able to do an F-test for Equality of Variances (or a similar test) on weighted data. They both can do difference in means t-tests (R via svyttest) and Stata via postestimation commands.Is there a workaround for this? A different test, perhaps, or a manual override in which the weighted data are converted to appear unweighted? Or is it statistically dubious to test for the equality of variances between two groups using weighted data (given that weighting affects variances), and therefore the reason why such options do not exist in Stata or R? If that's the case, what should one do when he or she wants to compare the variances of two groups that were sampled disproportionately? Certainly running F-tests on unweighted data isn't a better option, and since many surveys feature disproportionate selection methods, there must be some viable solution other than simply saying \"you can't do that\", right? ","Creater_id":64572,"Start_date":"2015-04-25 17:54:40","Question_id":148314,"Tags":["r","t-test","stata","f-test","survey-weights"],"Answer_count":2,"Last_activity":"2016-08-15 10:47:11","Link":"http://stats.stackexchange.com/questions/148314/f-test-for-equality-of-variances-with-weighted-survey-data","Creator_reputation":142}
{"_id":{"$oid":"5837a57ea05283111e4d4fe8"},"View_count":143,"Display_name":"Ernest3.14","Question_score":3,"Question_content":"Hello. I'm working on a AP stats project,  but since what I'm doing is beyond the scope of what we do in class (i.e. learn how to use a calculator), I think it's safe to say this isn't a homework question.What I'm trying to accomplish is extract some statistics (e.g. word frequencies, sentence lengths) from texts by various authors (with the help of Project Gutenberg), and then attempt to match \"mystery\" texts to those authors based on the similarity of those statistics. From what I understand, I'm looking for a goodness-of-fit test.For word frequency tables, I think a chi-square GOF test is the way to go, since I am comparing an observed frequency distribution to a known one. However, I would like to combine this data (specifically the p-value?) with other statistics, such as average sentence length or dialogue-narration ratio, to get more accurate comparison. Ideally, I would like to be able to weight each statistic as well, since some (e.g. word frequencies) should count more than others (e.g. punctuation frequencies). What sort of test should I use for this scenario? I'm not sure if it's legal to weight the observations in a chi-square test, or if doing so would actually work. Also, the p-values would not be integers.Sidenote: When I asked my teacher whether I could simply multiply each category by a constant to weight them, he kinda just shrugged at me","Creater_id":78174,"Start_date":"2015-05-26 16:21:31","Question_id":154156,"Tags":["chi-squared","modeling","goodness-of-fit","text-mining","weighted-data"],"Answer_count":1,"Last_activity":"2016-08-15 10:41:37","Link":"http://stats.stackexchange.com/questions/154156/goodness-of-fit-test-to-use-for-weighted-data","Creator_reputation":16}
{"_id":{"$oid":"5837a57ea05283111e4d4ff5"},"View_count":103,"Display_name":"justin","Question_score":0,"Question_content":"I have survey data that includes a random sample of emergency rooms in the United States. Each observation has a sampling weight that allows me to estimate the  number of cases for a particular disease nationally. While I can get the standard error for this estimate, I also want to calculate the population rate, i.e. cases of disease per 100,000 people. I am using census data to calculate:    disease cases / (population for that demographic) * 100000But how can I also get the standard error for the population rate? I tried:  std err for case count / (population for that demographic) * 100000When using that formula, however, standard error grows when that population (based on census data) is small, even if the initial standard error for national case count is low. That doesn't seem like it makes sense and suggests I don't know what I'm doing.I would appreciate any insight. I am using Stata, but even knowing the theory behind this might help me figure it out.Here is just an example of an annual count:. svy: tab year, count se format(%9.0g) (running tabulate on estimation sample) Number of strata = 1 Number of obs = 11321 Number of PSUs = 62 Population size = 655770.73 Design df = 61 --------------------‌​-------------- year      | count    se ----------+---------‌​-------------- 2004      | 73282.14 14066.59 2005      | 68625.86 12901.92 ... 2010      | 90914.29 18704.43 2011      | 96551.95 20952.36 Total     | 655770.7","Creater_id":79327,"Start_date":"2015-06-09 06:16:36","Question_id":156138,"Tags":["stata","standard-error","survey-weights","standard"],"Answer_count":0,"Last_activity":"2016-08-15 10:36:43","Link":"http://stats.stackexchange.com/questions/156138/standard-errors-for-population-rates-based-on-survey-data","Creator_reputation":1}
{"_id":{"$oid":"5837a57ea05283111e4d4ff7"},"View_count":28,"Display_name":"Stephen Barnett","Question_score":0,"Question_content":"I have a a dataset where each item is a % above or below 100% (taking an individual item, dividing by the mean). In order to produce a rank I weighted each item by a % (summing to 100%) to provide a master weighting. The problem I have is that within each individual dataset, the % +/- is very different. E.g. the min/max for one set of data is 200%, and in another it's 20% (due to the similarity of the original data). Therefore when I apply my subjective weighting, those with extreme deviation from the mean are effectively a larger component of the master weighting. At the moment I am using Standard Deviation to sensitize my percentage weighting. I was wondering if there was a better way to essentially put each data item on a level playing fieldThanks a lot, in advance, for your helpStephen","Creater_id":79897,"Start_date":"2015-06-16 07:03:29","Question_id":157208,"Tags":["dataset","standard-deviation","normalization","weights"],"Answer_count":1,"Last_activity":"2016-08-15 10:27:14","Link":"http://stats.stackexchange.com/questions/157208/normalising-extreme-items-within-datasets","Creator_reputation":1}
{"_id":{"$oid":"5837a57ea05283111e4d5004"},"View_count":234,"Display_name":"Antoni Parellada","Question_score":2,"Question_content":"There are likely more than one serious misunderstandings in this question, but it is not meant to get the computations right, but rather to motivate the learning of time series with some focus in mind.In trying to understand the application of time series, it seems as though de-trending the data makes predicting future values implausible. For instance, the gtemp time series from the astsa package looks like this:The trend upward in the past decades needs to be factored in when plotting predicted future values.However, to evaluate the time series fluctuations the data need to be converted into a stationary time series. If I model it as an ARIMA process with differencing (I guess this is carried out because of the middle 1 in order = c(-, 1, -)) as in:require(tseries); require(astsa)fit = arima(gtemp, order = c(4, 1, 1))and then try to predict future values ( years), I miss the upward trend component:pred = predict(fit, n.ahead = 50)ts.plot(gtemp, predpred), log = \"y\", lty = c(1,3))rendering a plot that makes sense.","Creater_id":67822,"Start_date":"2016-08-15 09:43:19","Question_id":229948,"Tags":["r","time-series","data-visualization"],"Answer_count":0,"Last_activity":"2016-08-15 10:27:02","Link":"http://stats.stackexchange.com/questions/229948/plotting-predicted-values-in-arima-time-series-in-r","Creator_reputation":7682}
{"_id":{"$oid":"5837a57ea05283111e4d5006"},"View_count":368,"Display_name":"Max","Question_score":0,"Question_content":"I am using the boot function in R to get standard errors for several statistics (I am doing a oaxaca blinder decomposition). My data (EU-SILC) has sample weights (PB040) for every observation. My understanding is that I have to use those weights for every OLS regression or other package I run to correct for sampling errors of the survey.The thing is now that the boot package in R also can use sample weights.Should I use the same sample weights for the call of the boot function and for the calculations \"inside\" the boot function?My reason for this question is that since the boot function does a weighted draw from the original sample the function \"inside\" the boot-function should allready have a weighted sample. So using sample weights inside the boot function would be redundant or lead to a bias.This is my first posting to cross validated and I am pretty new to bootstraping and econometrics so I hope I asked the question concise enough.","Creater_id":80180,"Start_date":"2015-06-19 09:29:03","Question_id":157777,"Tags":["r","bootstrap","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-15 10:23:29","Link":"http://stats.stackexchange.com/questions/157777/r-do-i-have-to-use-sample-weights-for-calculations-inside-a-bootstrap-function","Creator_reputation":3}
{"_id":{"$oid":"5837a57ea05283111e4d5013"},"View_count":258,"Display_name":"Marquis de Carabas","Question_score":3,"Question_content":"BackgroundA colleague recently came to me with a problem.  He was tasked with comparing health service utilization indicators in two secondary datasets:The first dataset is a Demographic and Health Survey (DHS) dataset,which has around 9,000 observations and is a nationallyrepresentative sample of women 15-49 in Country X.  Individuals weresampled using stratified two-stage cluster sampling.  The strata forthis sample were the province of residence and whether the clusterwas in an urban area or rural area.The survey instrument for the second dataset was modeled after the DHS. The same sampling strategy used by the DHS Program was used here. It has around 2,000 observations and is a representative sample of women 15-49 in 3 of the 11 provinces in Country X.  As with the DHS, the strata were the province of residence and whether the cluster was in an urban or rural area.Both datasets contain sampling weights, but the scales are different.  For the first dataset, the party that processes the data generally multiplies the weight by 100,000 to preserve decimal places.  The documentation urges users to divide the weight variable that is shipped with the dataset by 100,000 before using.  For the second dataset, the party that processes the data did not transform the weight variable further, so that the variable could be used as-is.  ProblemThe \"scale\" of the weight--whether divided by 100,000 first or used as is--doesn't really matter when working within a single survey for point estimates of proportions, means, or parameters, as this kind of transformation only affects the \"effective\" number of observations (i.e. 1,000/1,000,000 is equivalent to 0.01/10).  What I am not sure about is whether the weights necessarily need to be re-scaled when the data are pooled.  The DHS documentation for sampling states that when pooling DHS datasets, the weights need to be \"de-normalized\" before using (ICF International, 2012, p. 28) by  multiplying the weight by the target population and dividing this by the number of completed cases (in other words, sum of the weights), for each survey, because the given sampling weights are country-time specific.  My inclination is that once the weights are de-normalized, it is not necessary to ensure that they are the same scale, as he is only interested in the differences in proportions between the two datasets.  Is this correct, or will having variables of different scales be a problem when doing regression?ReferenceICF International. 2012. Demographic and Health Survey Sampling andHousehold Listing Manual. Calverton, Maryland, USA: ICFInternational.http://dhsprogram.com/pubs/pdf/DHSM4/DHS6_Sampling_Manual_Sept2012_DHSM4.pdf","Creater_id":40230,"Start_date":"2015-07-15 15:34:13","Question_id":161684,"Tags":["survey","pooling","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-15 10:21:56","Link":"http://stats.stackexchange.com/questions/161684/pooling-data-from-two-different-samples-does-the-scale-of-the-sampling-weights","Creator_reputation":1018}
{"_id":{"$oid":"5837a57ea05283111e4d5020"},"View_count":41,"Display_name":"mathi164","Question_score":1,"Question_content":"I'm struggling with the data of an experiment in an online shop. Participants were presented with a picture of an item available in the shop and asked to browse trough the different categories and add the correct item into the shopping cart. The variable of interest is task completion time (i.e. how long it took participants to find the item and add it to the cart). I have four groups (different versions of the shop). As you can see in the boxplots, the distribution of completion times is right skewed and there are a bunch of outliers in all four groups. I have tried several different ways - and combination of these -  to account for the non-normal distribution and the outliers, which all lead to different results: Trimming of OutliersWinzorization (i.e. setting most extreme values to 95% percentile)Log-Transforming the data Eventually, I want to be able to perform an ANOVA. In your opinion, which method for the analysis of completion time data is the most appropriate and are there other ways, which I have not mentioned? ","Creater_id":53236,"Start_date":"2016-08-14 06:57:45","Question_id":229775,"Tags":["anova","normal-distribution","data-transformation"],"Answer_count":1,"Last_activity":"2016-08-15 10:03:43","Link":"http://stats.stackexchange.com/questions/229775/analysis-of-completion-time-data","Creator_reputation":13}
{"_id":{"$oid":"5837a57ea05283111e4d502d"},"View_count":29,"Display_name":"Lzydude","Question_score":1,"Question_content":"My main goal is to model some count data. The dataset is a time series dataset that if I were to perform linear regression, I would difference to make sure the data became stationary. However, if I want to use Poisson GLM, would I still difference it or try to make stationary? My main concern is if I were to apply a transformation, I wouldn't be modeling counts anymore, rendering Poisson irrelevant. In addition, differencing can lead to negative values, and Poisson is only for positive values.","Creater_id":127752,"Start_date":"2016-08-15 09:26:26","Question_id":229945,"Tags":["time-series","generalized-linear-model","poisson"],"Answer_count":1,"Last_activity":"2016-08-15 09:36:14","Link":"http://stats.stackexchange.com/questions/229945/poisson-glm-with-nonstationary-data","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d503a"},"View_count":91,"Display_name":"Peter Albertson","Question_score":0,"Question_content":"I calculated a linear mixed model using the packages lme4 and lsmeans with the lmer-function, where i have one dependent variable rv and the interacting factors treatment, time, age and race. I'm interested in the response variable change over time, that's why i use the lstrends-function. So far so good. The problem is, i have to square root the response variable in order to fit the model properly. But the pairs-function only gives out a response to the square root of the rv, hard to interpret!So i tried to back-transform the response variable after pairs:     model.lmer \u0026lt;- lmer(sqrt(rv) ~ treat*time*age*race + (1|individual), data=mydata)model.lst \u0026lt;- lstrends(model.lmer, ~treat | age*race , var = \"time\", type=\"response\")pairs(mouse.lst, type=\"response\")This obviously doesn't work, as stated by the package itself:# Transformed responsesqwarp.rg \u0026lt;- ref.grid(update(warp.lm, sqrt(breaks) ~ .))summary(sqwarp.rg)# Back-transformed results - compare with summary of 'warp.rg'summary(sqwarp.rg, type = \"response\")# But differences of sqrts can't be back-transformedsummary(pairs(sqwarp.rg, by = \"wool\"), type = \"response\")# We can do it via regridsqwarp.rg2 \u0026lt;- regrid(sqwarp.rg)summary(sqwarp.rg2)  # same as for sqwarp.rg with type = \"response\"pairs(sqwarp.rg2, by = \"wool\")Anybode got an idea how to solve this particular problem? Thanks in advance!edit1:It could look like the following code:summary(pairs(lsmeans(rg.regrid, ~ treat | race*age, trend=\"time\")), type=\"response\")The problem is, i can't alter the reference grid for lstrends, just for lsmeans, because the first argument in lstrends or lsmeans with trend=\"time\" requires the linear mixed effect model (model.lmer) intead of just the reference grid like in lsmeans, without the trend-argument... That's probably why i can't back-transform the data with edit2: This here sums up my problem pretty well:model.sqrt \u0026lt;- lmer(sqrt(rv) ~ time*treat*race*age, data=mydata)rg \u0026lt;- ref.grid(model.sqrt)rg.regrid \u0026lt;- regrid(rg)summary(pairs(lsmeans(rg.regrid, ~treat | race*age*time), type = \"response\"))works perfectly.summary(pairs(lsmeans(rg.regrid, ~treat | race*age, trend=\"time\"), type = \"response\"))Gives the following error:Error in summary(pairs(lsmeans(rg.regrid, ~treat | race * age, trend = \"time\"),  : error in evaluating the argument 'object' in selecting a method for function 'summary': Error in data[[var]] : subscript out of boundsHow to avoid the error and still be able to back-transform my data?edit3:model \u0026lt;- lme(sqrt(dv) ~ time*treat*race*age, random=~1|individual, data=mydata, weights=varPower(0.19, form = ~time|individual), method=\"ML\")lsms \u0026lt;- summary(pairs(model, ~treat | time*race*age, at=list(time=4))))estimateslose \u0026lt;- summary(pairs(lstrends(model, ~treat | race*age, var=\"time\")))$SEfor(i in 1:4){    eslo[i] \u0026lt;- 2 * lsms[i] * slome[i]    ese[i] \u0026lt;- abs(2*lsms[i]) * slose[i]    }i = 1 is race1 at age1; i = 2 is race1 at age2; i = 3 is race2 at age1; i = 4 is race2 at age2; slose: slope-SE from lstrends for sqrt(dv)-difference between treated and untreated group slome: slope from lstrends for sqrt(dv)-difference per time between treated and untreated groupeslo and ese: estimated slope and se for dv-difference per time between treated and untreated group","Creater_id":126632,"Start_date":"2016-08-09 03:58:57","Question_id":228958,"Tags":["r","lmer","back-transformation","test-for-trend"],"Answer_count":1,"Last_activity":"2016-08-15 09:35:12","Link":"http://stats.stackexchange.com/questions/228958/back-transforming-contrast-lstrends-results-in-r","Creator_reputation":8}
{"_id":{"$oid":"5837a57ea05283111e4d5046"},"View_count":412,"Display_name":"Thomas","Question_score":2,"Question_content":"I have several OLS models with robust s.e.'s that predict an outcome variable Y.   For instance:Model 1:Model 2:Model 3:I am interested in giving an average effect for  across Models 1-3 with an accompanied 95% CI.Can I just take the average of 's across Models 1-3 and the average of standard errors to construct my confidence interval?  What is this called?     ","Creater_id":null,"Start_date":"2010-12-16 23:31:41","Question_id":5586,"Tags":["regression","confidence-interval","linear-model","regression-coefficients","average"],"Answer_count":1,"Last_activity":"2016-08-15 09:27:59","Link":"http://stats.stackexchange.com/questions/5586/average-effect-of-coefficients-across-multiple-linear-models","Creator_reputation":null}
{"_id":{"$oid":"5837a57ea05283111e4d5053"},"View_count":20,"Display_name":"Marius","Question_score":1,"Question_content":"I am measuring the time between jumps from a jump process. However my observation window starts randomly somewhere in time. Therfore I do not know when the jump before the first observed one occurs. The same thing happens when my observation window ends. In order to have sufficient data I make these observations multiple times. Up till now I have just disregarded the first and last measured intervals.How can I include the 'incomplete' times in order to estimate at least the mean and variance, better yet the distribution of waiting times.","Creater_id":127748,"Start_date":"2016-08-15 09:07:01","Question_id":229940,"Tags":["censoring","interarrival-time"],"Answer_count":0,"Last_activity":"2016-08-15 09:15:36","Link":"http://stats.stackexchange.com/questions/229940/how-to-take-incomplete-waiting-times-into-account","Creator_reputation":106}
{"_id":{"$oid":"5837a57ea05283111e4d5055"},"View_count":458,"Display_name":"fabee","Question_score":7,"Question_content":"I frequently read that Bonferroni correction also works for dependent hypotheses. However, I don't think that is true and I have a counter example. Can somebody please tell me (a) where my mistake is or (b) whether I am correct on this.Setting up the counter exampleAssume we are testing two hypotheses. Let  is the firsthypothesis is false and  otherwise. Define  similarly.Let  be the p-values associated with the two hypothesesand let  denote the indicator function for the setspecified inside the brackets. For fixed  define\\begin{eqnarray*}P\\left(p_{1},p_{2}|H_{1}=0,H_{2}=0\\right) \u0026amp; = \u0026amp; \\frac{1}{2\\theta}[\\![0\\le p_{1}\\le\\theta]\\!]+\\frac{1}{2\\theta}[\\![0\\le p_{2}\\le\\theta]\\!]\\\\P\\left(p_{1},p_{2}|H_{1}=0,H_{2}=1\\right) \u0026amp; = \u0026amp; P\\left(p_{1},p_{2}|H_{1}=1,H_{2}=0\\right)\\\\ \u0026amp; = \u0026amp; \\frac{1}{\\left(1-\\theta\\right)^{2}}[\\![\\theta\\le p_{1}\\le1]\\!]\\cdot[\\![\\theta\\le p_{2}\\le1]\\!]\\end{eqnarray*}which are obviously probability densities over . Here is a plot of the two densitiesMarginalization yields\\begin{eqnarray*}P\\left(p_{1}|H_{1}=0,H_{2}=0\\right) \u0026amp; = \u0026amp; \\frac{1}{2\\theta}[\\![0\\le p_{1}\\le\\theta]\\!]+\\frac{1}{2}\\\\P\\left(p_{1}|H_{1}=0,H_{2}=1\\right) \u0026amp; = \u0026amp; \\frac{1}{\\left(1-\\theta\\right)}[\\![\\theta\\le p_{1}\\le1]\\!]\\end{eqnarray*}and similarly for .Furthermore, let\\begin{eqnarray*}P\\left(H_{2}=0|H_{1}=0\\right) \u0026amp; = \u0026amp; P\\left(H_{1}=0|H_{2}=0\\right)=\\frac{2\\theta}{1+\\theta}\\\\P\\left(H_{2}=1|H_{1}=0\\right) \u0026amp; = \u0026amp; P\\left(H_{1}=1|H_{2}=0\\right)=\\frac{1-\\theta}{1+\\theta}.\\end{eqnarray*}This implies that\\begin{eqnarray*}P\\left(p_{1}|H_{1}=0\\right) \u0026amp; = \u0026amp; \\sum_{h_{2}\\in\\{0,1\\}}P\\left(p_{1}|H_{1}=0,h_{2}\\right)P\\left(h_{2}|H_{1}=0\\right)\\\\ \u0026amp; = \u0026amp; \\frac{1}{2\\theta}[\\![0\\le p_{1}\\le\\theta]\\!]\\frac{2\\theta}{1+\\theta}+\\frac{1}{2}\\frac{2\\theta}{1+\\theta}+\\frac{1}{\\left(1-\\theta\\right)}[\\![\\theta\\le p_{1}\\le1]\\!]\\frac{1-\\theta}{1+\\theta}\\\\ \u0026amp; = \u0026amp; \\frac{1}{1+\\theta}[\\![0\\le p_{1}\\le\\theta]\\!]+\\frac{\\theta}{1+\\theta}+\\frac{1}{1+\\theta}[\\![\\theta\\le p_{1}\\le1]\\!]\\\\ \u0026amp; = \u0026amp; U\\left[0,1\\right]\\end{eqnarray*}is uniform as required for p-values under the Null hypothesis.The same holds true for  because of symmetry.To get the joint distribution  we compute\\begin{eqnarray*}P\\left(H_{2}=0|H_{1}=0\\right)P\\left(H_{1}=0\\right) \u0026amp; = \u0026amp; P\\left(H_{1}=0|H_{2}=0\\right)P\\left(H_{2}=0\\right)\\\\\\Leftrightarrow\\frac{2\\theta}{1+\\theta}P\\left(H_{1}=0\\right) \u0026amp; = \u0026amp; \\frac{2\\theta}{1+\\theta}P\\left(H_{2}=0\\right)\\\\\\Leftrightarrow P\\left(H_{1}=0\\right) \u0026amp; = \u0026amp; P\\left(H_{2}=0\\right):=q\\end{eqnarray*}Therefore, the joint distribution is given by \\begin{eqnarray*}P\\left(H_{1},H_{2}\\right) \u0026amp; = \u0026amp; \\begin{array}{ccc} \u0026amp; H_{2}=0 \u0026amp; H_{2}=1\\\\H_{1}=0 \u0026amp; \\frac{2\\theta}{1+\\theta}q \u0026amp; \\frac{1-\\theta}{1+\\theta}q\\\\H_{1}=1 \u0026amp; \\frac{1-\\theta}{1+\\theta}q \u0026amp; \\frac{1+\\theta-2q}{1+\\theta}\\end{array}\\end{eqnarray*}which means that . Why it is a counter exampleNow let  for the significance level of interest. The probability to get at least one false positive with the corrected significance level  given that bothhypotheses are false (i.e. ) is given by\\begin{eqnarray*}P\\left(\\left(p_{1}\\le\\frac{\\alpha}{2}\\right)\\vee\\left(p_{2}\\le\\frac{\\alpha}{2}\\right)|H_{1}=0,H_{2}=0\\right) \u0026amp; = \u0026amp; 1\\end{eqnarray*}because all values of  and  are lower than given that  and  by construction. The Bonferroni correction, however, would claim thatthe FWER is less than .","Creater_id":6000,"Start_date":"2013-12-19 03:43:02","Question_id":80112,"Tags":["hypothesis-testing","mathematical-statistics","multiple-comparisons","p-value","bonferroni"],"Answer_count":2,"Last_activity":"2016-08-15 09:08:38","Link":"http://stats.stackexchange.com/questions/80112/is-bonferroni-correction-too-anti-conservative-liberal-for-some-dependent-hypoth","Creator_reputation":1618}
{"_id":{"$oid":"5837a57ea05283111e4d5063"},"View_count":47,"Display_name":"PanPsych","Question_score":0,"Question_content":"I ran several lmms using lme4 and would like to report an effect size. Partial eta squared would be my preference but I don't know how to obtain this from my model output. Any suggestions?","Creater_id":109053,"Start_date":"2016-08-15 08:51:48","Question_id":229939,"Tags":["r","mixed-model","random-effects-model","effect-size"],"Answer_count":0,"Last_activity":"2016-08-15 08:51:48","Link":"http://stats.stackexchange.com/questions/229939/how-to-get-partial-eta-squared-for-individual-predictors-in-linear-mixed-model-i","Creator_reputation":119}
{"_id":{"$oid":"5837a57ea05283111e4d5065"},"View_count":33,"Display_name":"Andreea","Question_score":1,"Question_content":"Can stratified sampling be used to deal with the outliers that a data set might have? For the data set that I have, when trying to fit a statistical distribution and calculate a Value at Risk by using the Loss Distribution Approach, I find that the outliers( in my case defined based on the 3 standard deviation rule ) distort quite a lot the VAR values and the summary statistics. I read that in statistics one would eliminate the outliers but in this case if eliminated, we loose valuable information. Thus, I was wondering if stratified sampling could be used to deal with these outliers rather then eliminating them to make them more representative for the whole data set.For example, if I get k outliers for my data based on the 3 standard deviation method which I mentioned, I was thinking that I could get k samples from the stratified sampling method and replace the outliers with these new observations. The observations would result from separating the data set into k stratums and replacing the k outliers by the mean of each stratum. In this manner, instead of getting rid of them I could make the data set more representative and when fitting for example a GPD I could get a more realistic VaR value.","Creater_id":127722,"Start_date":"2016-08-15 06:00:48","Question_id":229908,"Tags":["distributions","sampling","outliers"],"Answer_count":0,"Last_activity":"2016-08-15 08:45:12","Link":"http://stats.stackexchange.com/questions/229908/stratified-sampling-in-a-data-set-with-outliers","Creator_reputation":6}
{"_id":{"$oid":"5837a57ea05283111e4d5067"},"View_count":169,"Display_name":"RTrain3K","Question_score":1,"Question_content":"Can the utilities or choice probabilities be weighted by population weights? Or must the weighting actually occur at the observational/individual level? I assume it must occur at the observational level, but I've seen people weight utilities. Additionally, if the weighting occurs at the observational level, will one still get accurate market share estimates when sub selecting the sample based on different attributes?  ","Creater_id":82576,"Start_date":"2015-07-17 06:19:47","Question_id":161945,"Tags":["econometrics","multinomial","logit","weighted-data","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-15 07:43:07","Link":"http://stats.stackexchange.com/questions/161945/exogenous-weighting-multinomial-logit-models","Creator_reputation":52}
{"_id":{"$oid":"5837a57ea05283111e4d5073"},"View_count":119,"Display_name":"Brydon Parker","Question_score":0,"Question_content":"From thinkstats2 the O'Reilly text. class cdf(object):    def ValueArray(self, ps):      \"\"\"Returns InverseCDF(p), the value that corresponds to probability p.      Args:          ps: NumPy array of numbers in the range [0, 1]      Returns:          NumPy array of values      \"\"\"      ps = np.asarray(ps)      if np.any(ps \u0026lt; 0) or np.any(ps \u0026gt; 1):          raise ValueError('Probability p must be in range [0, 1]')      index = np.searchsorted(self.ps, ps, side='left')      return self.xs[index]     def Sample(self, n):        \"\"\"Generates a random sample from this distribution.        n: int length of the sample        returns: NumPy array        \"\"\"        ps = np.random.random(n) # chooses n random terms in [0,1)        return self.ValueArray(ps)         # so here i choose an         # ps are defined as the % of getting \u0026lt;= a given term in the cdf        # ValueArray returns the val associated with a % \u0026lt;= desireddef ResampleRowsWeighted(df, column='finalwgt'):    weights = df[column] # giving us the desired weights   cdf = Cdf(dict(weights)) # creating a cdf object   indices = cdf.Sample(len(weights)) # returns list of vals   sample = df.loc[indices] # returns chosen rows   return sample why do we use a cdf instead of pdf?with a cdf were most likely to land in the biggest gap, the largest differencebetween 2 percentileswith a pmf we can use a series and add each perc together, and then picka random number in [0,1) where larger weights are more likely to get picked","Creater_id":82593,"Start_date":"2015-07-20 17:31:06","Question_id":162383,"Tags":["python","cdf","statsmodels"],"Answer_count":1,"Last_activity":"2016-08-15 07:42:02","Link":"http://stats.stackexchange.com/questions/162383/weighted-re-sampling-why-cdf-not-pdf-or-pmf","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d5080"},"View_count":117,"Display_name":"SpicyClubSauce","Question_score":1,"Question_content":"in unbalanced classification problems, I find myself using class_weigh = \"auto\" or similar parameters often, but I don't think I'm fully understanding what it's doing. I know that it's the industry practice to do so, but I don't have a deep understanding of it.If we have an unbalanced 90/10 dataset, does giving 9 times more weight to the minority class mean the same thing as oversampling the minority class? Or is giving higher weight completely different from oversampling? (also - in the case of oversampling in this particular 90/10 example, would we want to be sampling from the minority class 9x the normal amount?)If not, how do the ML algorithms take into account the heavier weight of the minority class? Is it automatically computed into the cost function they seek to minimize? Am I making any sense? (probably not, as I'm getting twisted myself).Any input would be greatly appreciated, thanks.","Creater_id":83853,"Start_date":"2015-07-31 23:52:27","Question_id":164227,"Tags":["classification","unbalanced-classes","weighted-sampling","weighted-data"],"Answer_count":0,"Last_activity":"2016-08-15 07:39:19","Link":"http://stats.stackexchange.com/questions/164227/for-classification-w-unbalanced-datasets-is-class-weighing-the-same-as-oversamp","Creator_reputation":125}
{"_id":{"$oid":"5837a57ea05283111e4d5082"},"View_count":25,"Display_name":"quirik","Question_score":0,"Question_content":"I am doing a multinomial logit analysis on a survey weighted sample of household data in R. I am trying do model if household owns, has access to mobile phone or has none. Since I am beginner in this I am having doubts if I should include the weights in the analysis? Is it OK to combine household and person level variables as independent variables?","Creater_id":43204,"Start_date":"2016-01-30 16:44:01","Question_id":193292,"Tags":["survey","multinomial","survey-weights"],"Answer_count":0,"Last_activity":"2016-08-15 07:36:05","Link":"http://stats.stackexchange.com/questions/193292/multinomial-logit-model-and-weighted-sample","Creator_reputation":113}
{"_id":{"$oid":"5837a57ea05283111e4d5084"},"View_count":77,"Display_name":"Lin Ma","Question_score":0,"Question_content":"Wondering if any general guidelines whether using Mean or Median is better to represent the statistics of underlying data? I think using Median is always better, especially better when standard deviation is large for a data set.Your advice is appreciated.","Creater_id":18254,"Start_date":"2016-08-12 10:14:01","Question_id":229576,"Tags":["mean","descriptive-statistics","median"],"Answer_count":1,"Last_activity":"2016-08-15 07:34:13","Link":"http://stats.stackexchange.com/questions/229576/using-mean-or-median","Creator_reputation":108}
{"_id":{"$oid":"5837a57ea05283111e4d5091"},"View_count":27,"Display_name":"Igor","Question_score":0,"Question_content":"Suppose an outcome depends on the intensity of a treatment intervention , where . Given intensity of treatment , the data generating process is Y_i = \\beta_0 + \\beta_1 \\pi + \\epsilon_i.I have experimental data from two treatment intensities,  and , and a control . Suppose 1/3 of treated individuals are assigned to T1, 1/6 are assigned to T2 and 1/2 are assigned to the pure control. I am interested in measuring the pooled treatment effect across both treatment intensities. Let . Y_i = \\beta_0 + \\beta_2 T_i + \\epsilon_iThe true effect corresponds to \\beta_2 = \\frac 2 3 \\beta_1  \\pi_1+ \\frac 1 3 \\beta_1 \\pi_2 and the minimum detectable effect for  at significance level  and power  isMDE=(t_{\\alpha} + t_{1-\\kappa}) 2 \\sqrt{ \\frac{\\sigma^2}{N}}where  is the variance of  and  is the total sample size.Now suppose I want to weight the data so that the observations with intensity  have double the weight of the  observations. This will yield a measure of the effect:\\beta_{2 weighted} = \\frac 1 2 \\beta_1  \\pi_1+ \\frac 1 2 \\beta_1 \\pi_2 How do I adjust the minimum detectable effect to account for the sample weights?","Creater_id":83947,"Start_date":"2015-08-02 13:22:39","Question_id":164409,"Tags":["power-analysis","weighted-sampling"],"Answer_count":1,"Last_activity":"2016-08-15 07:28:33","Link":"http://stats.stackexchange.com/questions/164409/power-analysis-weighted-data","Creator_reputation":1}
{"_id":{"$oid":"5837a57ea05283111e4d509d"},"View_count":32,"Display_name":"Parth Raghav","Question_score":2,"Question_content":"Currently, neural networks can be trained in a parallel fashion and I read a couple of very good research papers on it. I am trying to implement neural networks that can take concurrent input and compute the output.For example, rather than directly taking an input vector and activating the neural network to produce certain output , it should take a series of binary vectors that add up to the main vector.For example let there be a normal neural network  and a special kind of neural network  with initial state , an input vector , rather than providing this as an input, neural network should take binary vectors like  such that . Rather than directly passing the decimal input vector to the neural network I want to implement . With every input vector, the state of function changes from  such that allows orderless currying i.e. The only thing that changes the state is \"1\" i.e.  and And statement 2 said, any integer vector  could be decomposed in vectors How can this be implemented? Should I develop a FSM and Neural Network hybrid? Are there current alternatives for the same? I am afraid if I know all the important keywords, please feel free to edit the tagsKindly provide me some direction for the same.  I have completed significant parts of the project but this area is something I am currently working on. I hope the community could provide me support.Thanks in advance","Creater_id":110367,"Start_date":"2016-08-15 07:21:40","Question_id":229924,"Tags":["neural-networks","svm","recurrent-events","cooccurrence"],"Answer_count":0,"Last_activity":"2016-08-15 07:21:40","Link":"http://stats.stackexchange.com/questions/229924/concurrent-input-for-neural-network","Creator_reputation":28}
{"_id":{"$oid":"5837a57ea05283111e4d509f"},"View_count":20,"Display_name":"Erik Rasmussen","Question_score":0,"Question_content":"I work with forecasting sales of fashion products, and I am looking for a model to improve our demand planning. Based on production lead time, we often need to predict monthly demand up to 6 months in advance.I have previously read that ETS and Arima forecast models are best suited for short term forecasts. Is there a more common model for longer term forecasts, or a recommendation of a similar issue I could study?","Creater_id":83588,"Start_date":"2016-08-15 07:19:40","Question_id":229923,"Tags":["forecasting"],"Answer_count":0,"Last_activity":"2016-08-15 07:19:40","Link":"http://stats.stackexchange.com/questions/229923/forecast-model-for-long-time-horizons","Creator_reputation":28}
{"_id":{"$oid":"5837a57ea05283111e4d50a1"},"View_count":23223,"Display_name":"elmes","Question_score":16,"Question_content":"I have just heard, that it's a good idea to choose initial weights of a neural network from the range , where  is the number of inputs to a given neuron. It is assumed, that the sets are normalized - mean 0, variance 1 (don't know if this matters).Why is this a good idea?","Creater_id":16763,"Start_date":"2013-01-12 14:26:39","Question_id":47590,"Tags":["neural-networks","normalization"],"Answer_count":3,"Last_activity":"2016-08-15 07:16:47","Link":"http://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network","Creator_reputation":203}
{"_id":{"$oid":"5837a57ea05283111e4d50b0"},"View_count":47,"Display_name":"Melisa","Question_score":0,"Question_content":"I have 66 indicators of leadership, and I want to check which ones contribute the most to leadership effectiveness. I have performed EFA, resulting 8 constructs from leadership indicators.Should I perform CFA after that or can I move directly to multiple regression analysis?Thanks!","Creater_id":127725,"Start_date":"2016-08-15 06:32:48","Question_id":229912,"Tags":["multiple-regression","factor-analysis","confirmatory-factor"],"Answer_count":1,"Last_activity":"2016-08-15 07:00:50","Link":"http://stats.stackexchange.com/questions/229912/should-i-perform-cfa-after-the-efa-or-can-i-move-directly-to-multiple-regression","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d50bd"},"View_count":23,"Display_name":"Ashe","Question_score":0,"Question_content":"I'm working to power a study that determines the prevalence of a biological trait in a population of individuals.  This trait can take on a continuous range of values.  Everything I've found regarding powering prevalence assumes that the outcome of interest is binary, like the presence or absence of a disease.  What I have been unable to find is a good methodology for determining sample size when the outcome of interest in continuous.My question:  What methodology is appropriate to determine sample size for a study that determines prevalence of an outcome of unknown, but continuous, distribution?  I don't have a distinct hypothesis I'm testing for.  Rather, I want to sufficiently quantify the range and frequency of the outcome values.  As an example, BMI in the population of primiparous women looks like this:How would I consider powering a study that captures the main central peak between 18.50 and 24.99 and the long tail ≥30.00?  I wouldn't know a priori these values or the overall structure.\"Sufficiently\" can be defined as capturing the major structures of the distribution.BMI image taken from here.","Creater_id":60621,"Start_date":"2016-08-09 07:15:54","Question_id":228991,"Tags":["distributions","sample-size","power-analysis","continuous-data"],"Answer_count":0,"Last_activity":"2016-08-15 06:26:52","Link":"http://stats.stackexchange.com/questions/228991/how-to-power-for-prevalence-of-a-trait-that-can-take-on-a-range-of-scores","Creator_reputation":525}
{"_id":{"$oid":"5837a57ea05283111e4d50bf"},"View_count":86,"Display_name":"Matthew Drury","Question_score":4,"Question_content":"I've always subscribed to the folk wisdom that decreasing the learning rate in a gbm (gradient boosted tree model) does not hurt the out of sample performance of the model.  Today, I'm not so sure.I'm fitting models (minimizing sum of squared errors) to the boston housing dataset.  Here is a plot of error by number of trees on a 20 percent hold out testing data setIt's hard to see what's going on at the end, so here's a zoomed in version at the extremesIt seems that in this example, the learning rate of  is best, with the smaller learning rates performing worse on hold out data.How is this best explained? Is this an artifact of the small size of the boston data set?  I'm much more familiar with situations where I have hundreds of thousands or millions of data points.Should I start tuning the learning rate with a grid search (or some other meta-algorithm)?","Creater_id":74500,"Start_date":"2016-08-14 20:29:18","Question_id":229855,"Tags":["machine-learning","boosting","out-of-sample"],"Answer_count":1,"Last_activity":"2016-08-15 05:32:23","Link":"http://stats.stackexchange.com/questions/229855/how-can-a-smaller-learning-rate-hurt-the-performance-of-a-gbm","Creator_reputation":12732}
{"_id":{"$oid":"5837a57ea05283111e4d50cc"},"View_count":37,"Display_name":"Bajcz","Question_score":0,"Question_content":"I have read this article and those linked to it, but I am still having difficulties fitting a function of this form to data I have using the nls function in R. Invariably, I fail to get convergence regardless of what starting values I choose for A through C.Here is a plot of all six of the relationships I would ultimately like to fit:While VEG.STM3 and PROP.RIPE probably will conform to a curve of this shape fairly well, the others probably won't, and that's fine. I still have a justifiable reason for trying it.My questions are as follows:What are the parameters A, B, and C doing, mathematically, in a function of this form? A appears to be the Y asymptote, B seems to be a scalar of some kind, and C seems to control the rate of decay, but, beyond that, I can't seem to figure out what a reasonable range of values for each parameter should be, especially from one set of data to the next.Is there a way to \"linearize\" this problem so that lm can be used in place of nls? My understanding is that this function is equivalent to ln(Y) ~ ln(A) + ln(B) + CX, but I don't understand how to fit that equation any more than I understand how to fit this current curve. Is there any way to get nls to be less fussy and more robust to lousy starting guesses?Once I get nls to run successfully, how can I compare the fit of the model to one fit by simple linear regression? Would use of AICc be appropriate in that case? Here is some data to work with:PROP.RIPE = c(0.37, 0.223, 0.223, 0.224, 0.388, 0.413, 0.406, 0.422, 0.554, 0.453, 0.569, 0.511, 0.13, 0.166, 0.16, 0.216, 0.297, 0.344, 0.339, 0.292, 0.601, 0.535, 0.65, 0.535, 0.269, 0.238, 0.334, 0.272, 0.523, 0.358, 0.449, 0.393, 0.458, 0.426, 0.576, 0.468, 0.581, 0.579, 0.527, 0.568, 0.348, 0.313, 0.317, 0.267, 0.623, 0.527, 0.646, 0.589, 0.488, 0.444, 0.498, 0.449, 0.109, 0.103, 0.171, 0.153, 0.505, 0.343, 0.345, 0.213, 0.029, 0.011, 0.071, 0.013, 0.697, 0.604, 0.624, 0.639, 0.386, 0.508, 0.38, 0.471, 0.618, 0.488, 0.513, 0.485, 0.602, 0.597, 0.625, 0.495, 0.318, 0.457, 0.423, 0.547, 0.88, 0.949, 0.912, 0.771, 0.628, 0.635, 0.486, 0.567, 0.621, 0.549, 0.698, 0.709, 0.541, 0.563, 0.789, 0.692, 0.525, 0.395, 0.449, 0.597, 0.57, 0.487, 0.556, 0.546, 0.495, 0.617, 0.754, 0.71, 0.585, 0.719, 0.508, 0.536, 0.592, 0.472, 0.481, 0.658, 0.937, 0.853, 0.981, 0.887)NODES0 = c(124L, 362L, 198L, 343L, 152L, 193L, 98L, 167L, 148L, 284L, 113L, 137L, 227L, 323L, 156L, 362L, 166L, 327L, 137L, 312L, 166L, 350L, 97L, 222L, 182L, 456L, 143L, 277L, 172L, 272L, 110L, 184L, 138L, 288L, 102L, 124L, 236L, 280L, 159L, 127L, 104L, 176L, 93L, 167L, 178L, 400L, 126L, 248L, 189L, 336L, 181L, 304L, 245L, 283L, 151L, 327L, 116L, 179L, 144L, 177L, 397L, 642L, 322L, 443L, 125L, 249L, 100L, 144L, 56L, 22L, 23L, 17L, 252L, 387L, 184L, 308L, 115L, 267L, 82L, 157L, 223L, 226L, 79L, 73L, 101L, 139L, 104L, 60L, 200L, 164L, 66L, 49L, 173L, 204L, 64L, 107L, 435L, 215L, 51L, 129L, 392L, 550L, 174L, 178L, 276L, 204L, 98L, 74L, 421L, 303L, 126L, 150L, 168L, 195L, 77L, 75L, 72L, 142L, 59L, 47L, 391L, 479L, 109L, 111L)NODE.SUCCESS = c(1.05, 0.79, 0.86, 0.69, 0.85, 0.8, 0.77, 0.84, 0.53, 0.88, 0.88, 0.79, 0.85, 0.88, 1, 1.03, 0.95, 0.77, 0.92, 0.73, 0.98, 0.92, 0.97, 0.99, 0.88, 0.82, 0.84, 0.78, 0.63, 0.54, 0.54, 0.47, 1.09, 0.88, 0.95, 0.99, 0.96, 1.13, 0.91, 1.01, 0.81, 0.89, 0.99, 0.85, 0.95, 0.65, 0.87, 0.73, 0.64, 0.82, 0.82, 0.75, 0.93, 1.06, 0.94, 0.89, 0.65, 0.53, 0.6, 0.62, 0.91, 0.89, 0.93, 1.13, 0.81, 0.63, 0.75, 0.67, 0.93, 0.82, 0.7, 0.88, 0.8, 0.96, 0.9, 0.94, 0.58, 0.66, 0.65, 0.63, 0.66, 0.62, 0.66, 0.77, 0.51, 0.6, 0.47, 0.9, 0.69, 0.73, 0.59, 0.63, 0.97, 0.93, 0.95, 0.91, 0.81, 0.92, 0.88, 1.1, 0.56, 0.57, 0.44, 0.51, 0.85, 0.83, 0.96, 0.85, 1, 0.97, 0.94, 0.95, 0.61, 0.71, 0.73, 0.8, 1.06, 0.96, 0.9, 0.91, 0.45, 0.41, 1.3, 0.55)","Creater_id":61175,"Start_date":"2016-08-15 05:17:01","Question_id":229905,"Tags":["r","nonlinear-regression","curve-fitting","parameterization","nls"],"Answer_count":0,"Last_activity":"2016-08-15 05:17:01","Link":"http://stats.stackexchange.com/questions/229905/what-do-the-parameters-a-b-and-c-do-in-an-equation-of-the-form-y-a-becx","Creator_reputation":130}
{"_id":{"$oid":"5837a57ea05283111e4d50ce"},"View_count":23,"Display_name":"tak","Question_score":0,"Question_content":"I have one categorical predictor coded as 0 or 1 for a sample of about 290.Based on some research, I want to control for 4 covariates. One is continuous (age) and the rest are categorical (gender, ever having used certain drugs as either yes or no).My outcome is number of violent acts - this is what I'm having trouble with. According to a professor at my university this isn't a true continuous variable as it is an interval variable. This confused me as I've used interval variables as continuous in other tests. What type of test would I use if my outcome was continuous? Do I need to use a different one for my interval data?I am using SPSS to analyze my data. After finding base associations (using t-tests) between my predictor and outcome and covariates and outcome, I tried to go further.I did a univariate analysis of variance (under general linear models) with my predictor as a fixed factor. This seems logical as there's a section for covariates, but I honestly have no idea. Any guidance would be much appreciated.","Creater_id":127711,"Start_date":"2016-08-15 04:18:47","Question_id":229897,"Tags":["categorical-data","spss","continuous-data"],"Answer_count":1,"Last_activity":"2016-08-15 05:13:07","Link":"http://stats.stackexchange.com/questions/229897/what-test-should-i-use-for-my-data","Creator_reputation":3}
{"_id":{"$oid":"5837a57ea05283111e4d50db"},"View_count":46,"Display_name":"Jack Pierce-Brown","Question_score":2,"Question_content":"Let  be a stationary stochastic process with mean , variance  and correlation function . Let the integral of a stochastic process be:I = \\int_0^L X(t) \\, dtThe variance of  is given by (see StackExchange):\\text{Var}[I] =\\sigma^2 \\int_0^L \\int_0^L \\rho(t_1-t_2)\\,\\mathrm{dt_1\\,dt_2}Where . The variance of  is maximised when  is perfectly correlated i.e. :\\text{Var}[I_{corr}] = \\sigma^2 \\int_0^L \\int_0^L 1\\,\\mathrm{dt_1\\,dt_2} = \\sigma^2L^2The variance of  is minimised when  is uncorrelated (white noise process) i.e.  where  is the Dirac delta function. Can you evaluate the following integral?\\text{Var}[I_{uncorr}] = \\sigma^2 \\int_0^L \\int_0^L \\delta(t_1-t_2)\\,\\mathrm{dt_1\\,dt_2} = ","Creater_id":112088,"Start_date":"2016-08-15 02:04:57","Question_id":229877,"Tags":["probability","variance","stochastic-processes","integration"],"Answer_count":1,"Last_activity":"2016-08-15 04:29:43","Link":"http://stats.stackexchange.com/questions/229877/max-and-min-variance-of-the-integral-of-a-stationary-stochastic-process","Creator_reputation":91}
{"_id":{"$oid":"5837a57ea05283111e4d50dd"},"View_count":35,"Display_name":"Jeannie","Question_score":0,"Question_content":"I am trying to fit dshw for the double seasonal times series, and compare the results with tbats() , I used:  \u0026gt; dresid.train.dshw \u0026lt;- dshw(dresid.ts,h = 44)    Error in dshw(dresid.ts, h = 44) : dshw not suitable when data contain zeros or negative numbersIt seems that dshw can not deal with negative or zero values. Does anyone know how to fit dshw on negative data?Also, from a dshw example, the parameters (alpha, beta, gamma, omega, and phi) are specified in the model. I am wondering how those parameters are selected? May I also know how to get confidence interval for the dshw forecast? As I can always extract upper and lower from the forecast() of a tbats model, but it seems that I can not find those in the dshw. Thanks. ","Creater_id":114130,"Start_date":"2016-08-15 03:39:55","Question_id":229889,"Tags":["r","time-series","multiple-seasonalities"],"Answer_count":0,"Last_activity":"2016-08-15 04:18:36","Link":"http://stats.stackexchange.com/questions/229889/double-sesonal-holt-winters-methods-for-data-containing-zeros-or-negative-number","Creator_reputation":59}
{"_id":{"$oid":"5837a57ea05283111e4d50df"},"View_count":59,"Display_name":"hxd1011","Question_score":2,"Question_content":"Is there any other reasons beside numerical problems with finite precision system (ieee 754) ? If our computer can have infinite precision, do we still need log likelihood?","Creater_id":113777,"Start_date":"2016-08-14 15:33:38","Question_id":229831,"Tags":["probability","likelihood","computational-statistics"],"Answer_count":1,"Last_activity":"2016-08-15 04:12:16","Link":"http://stats.stackexchange.com/questions/229831/why-apply-log-to-likelihood","Creator_reputation":4443}
{"_id":{"$oid":"5837a57ea05283111e4d50e1"},"View_count":16,"Display_name":"Teresa","Question_score":0,"Question_content":"I am fitting GLMMs (using a binary variable as response variable and continuous variables as explanatory variables [family = binomial(link=\"logit\")]), and I have three possible random effects: -study areas (n=5)-individual ID (n=26)-individual ID nested in study areas (n=26, since no individual is present in two study areas).Variance levels (considering the same response variable and fixed effects, only changing random effects):-Study Areas: AIC      BIC   logLik deviance df.resid  17464.3  17590.2  -8717.1  17434.3    32655 Random effects: Groups     Name        Variance Std.Dev. Study.Area (Intercept) 3.24     1.8     Number of obs: 32670, groups:  Study.Area, 5-Individual ID: AIC      BIC   logLik deviance df.resid  17566.7  17692.7  -8768.4  17536.7    32655 Random effects: Groups Name        Variance Std.Dev. ID_1   (Intercept) 3.406    1.845   Number of obs: 32670, groups:  ID_1, 26-Individual ID nested in study areas:AIC      BIC   logLik deviance df.resid  17465.6  17599.9  -8716.8  17433.6    32654 Random effects: Groups          Name        Variance Std.Dev. ID_1:Study.Area (Intercept) 0.001396 0.03736  Study.Area      (Intercept) 3.254536 1.80403 Number of obs: 32670, groups:  ID_1:Study.Area, 26; Study.Area, 5My interpretation of this is: individual variation is low within study areas, but variation between study areas is higher. When considering only individual ID, variation is high because differences between study areas are \"included\" since there are individuals from all study areas.I believe it is more correct to include individual ID as random effect; it is more parsimonious than the nested random effect, it has the highest variance and using a random effect with five levels (study area) is not the best option, since they may not be enough. (However, the AIC values do not support this choice...)My questions are:1) If this interpretation is correct, should I report variance levels and standard deviations to justify my choice? 2)How does that work if I have an averaged model? 3) Are there any other methods to test variance of random effects in GLMMs? ","Creater_id":117281,"Start_date":"2016-08-15 04:07:52","Question_id":229893,"Tags":["variance","random-effects-model","glmm"],"Answer_count":0,"Last_activity":"2016-08-15 04:07:52","Link":"http://stats.stackexchange.com/questions/229893/what-is-correct-way-to-compare-random-effects-glmm","Creator_reputation":59}
{"_id":{"$oid":"5837a57ea05283111e4d50e3"},"View_count":35,"Display_name":"kanimbla","Question_score":0,"Question_content":"Consider 3 models (model1, model2, model3) with the following set of prediction errors:residuals_model1=c(-0.0422,-0.0198,-0.0167,-0.0350,-0.0084,-0.0387,0.0232,0.0239,-0.0104,-0.0174,0.0292,-0.0169,0.0456,-0.0068,0.0372,0.0557,0.0003,-0.0090)residuals_model2=c(0.0109,-0.0129,-0.0058,0.0192,0.0055,0.0288,-0.0285,-0.0254,0.0013,0.0089,-0.0317,0.0319,-0.0262,0.0140,-0.0156,-0.0404,0.0190,0.0308)residuals_model3=c(0.0189,0.0035,0.0025,0.0222,0.0004,0.0340,-0.0232,-0.0238,0.0145,0.0238,-0.0215,0.0243,-0.0379,0.0092,-0.0322,-0.0541,-0.0042,0.0111)Computing root mean squared forecast errors (RMSFE):rmsfe_model1=sqrt(mean(residuals_model1^2))rmsfe_model2=sqrt(mean(residuals_model2^2))rmsfe_model3=sqrt(mean(residuals_model3^2))Next, create a matrix with the relative RMSFE of models 2 \u0026amp; 3 versus model 1 and the relevant p-values from the Diebold-Mariano test using the forecast package:library(forecast)forecast_results=matrix(nrow=2,ncol=2)rownames(forecast_results)=c('model2 / model1','model3 / model1')colnames(forecast_results)=c('relative RMSFE','p-value (DM-Test)')forecast_results[1,1]=round(rmsfe_model2/rmsfe_model1,2)forecast_results[2,1]=round(rmsfe_model3/rmsfe_model1,2)forecast_results[1,2]=round(dm.test(residuals_model1,residuals_model2,alternative='two.sided',h=1,power=2)p.value,2)Print results:print(forecast_results)                relative RMSFE p-value (DM-Test)model2 / model1           0.79              0.10model3 / model1           0.85              0.04It can be seen that model 2 performs better in terms of relative RMSFE compared to model 3. However, the p-value from the DM-test shows a higher level of statistical significance for model 3 compared to model 2. Suppose you had to present this table of results and somebody would claim that a higher relative RMSFE for model 3 is inconsistent with a lower p-value for model 3. How would you explain this result?  ","Creater_id":79243,"Start_date":"2016-08-15 03:20:41","Question_id":229888,"Tags":["r","forecasting"],"Answer_count":0,"Last_activity":"2016-08-15 03:20:41","Link":"http://stats.stackexchange.com/questions/229888/diebold-mariano-test-in-r-how-to-rationalize-p-value","Creator_reputation":74}
{"_id":{"$oid":"5837a57ea05283111e4d50e5"},"View_count":18,"Display_name":"tomka","Question_score":0,"Question_content":"I am analyzing a stratified randomized trial. I wonder what is the best way to estimating and testing the treatment effect. The trial is stratified by three strata, 2 by 2 within each of 30 centers. Within each stratum-center combination, randomization is executed into treatment () and control (). The outcome variable is a proportion, say , where the null hypothesis H_0: \\pi_t-\\pi_c =0 is sought to be rejected.My intuition is that a proper analysis will estimate the conditional treatment effect in each stratum and then pool the conditional effects to a marginal (population averaged) effect. So let  denote the effect in stratum-center combination , estimated by \\hat{\\delta}_s = \\hat{\\pi}_{t,s}-\\hat{\\pi}_{c,s}  and we estimate the population treatment effect by \\hat{\\delta} = \\sum_{s=1}^S \\frac{N_s}{N} \\hat{\\delta}_swhere  is the number of strata and  is the number of units in stratum  and  the sample size of the trial. I would test for variance using bootstrap resampling from the data or by pooling the variance in a similar way.Is this an appropriate procedure and what are possibly better alternatives?","Creater_id":24515,"Start_date":"2016-08-15 03:06:07","Question_id":229887,"Tags":["hypothesis-testing","experiment-design","binary-data","treatment-effect","stratification"],"Answer_count":0,"Last_activity":"2016-08-15 03:11:18","Link":"http://stats.stackexchange.com/questions/229887/how-should-we-analyse-stratified-randomized-trials-with-binary-outcomes","Creator_reputation":1607}
{"_id":{"$oid":"5837a57ea05283111e4d50e7"},"View_count":45,"Display_name":"MiniQuark","Question_score":0,"Question_content":"For deep neural networks using ReLU neurons, the recommended connection weight initialization strategy is to pick a random uniform number between -r and +r with:Where fan-in and fan-out are the number of connections going in and out of the layer being initialized. This is called \"He initialization\" (paper).My question is: what's the recommended weights initialization strategy when using ELU neurons (paper)?Since ELUs look a lot like ReLUs, I'm tempted to use the same logic, but I'm not sure it's the optimal strategy.","Creater_id":109561,"Start_date":"2016-08-15 03:00:23","Question_id":229885,"Tags":["neural-networks","deep-learning","weights"],"Answer_count":1,"Last_activity":"2016-08-15 03:05:02","Link":"http://stats.stackexchange.com/questions/229885/whats-the-recommended-weight-initialization-strategy-when-using-the-elu-activat","Creator_reputation":429}
{"_id":{"$oid":"5837a57ea05283111e4d50e9"},"View_count":1001,"Display_name":"Arun","Question_score":4,"Question_content":"In a part of my research, I am fitting probability distribution models for the count data(and binomial data) using Poisson, Binomial, Negative Binomial and Beta Binomial models. I have few data sets in which the observed frequency and hence the expected modeled frequencies are too small(less than 5).. I want to compare the goodness of fits using a proper method. I know, Pearson's Chi-Square test will not be applicable for my purposes as it has some limitations on expected counts. Here is a similar situation which I also deal with..I am looking for an alternative Goodness of fit test measure which can be applied to this kind of situations.Any suggestion is greatly appreciated.. Thank you.","Creater_id":13342,"Start_date":"2013-03-29 11:26:09","Question_id":54674,"Tags":["chi-squared","goodness-of-fit"],"Answer_count":2,"Last_activity":"2016-08-15 02:31:09","Link":"http://stats.stackexchange.com/questions/54674/alternative-to-pearsons-chi-square-goodness-of-fit-test-when-expected-counts","Creator_reputation":51}
{"_id":{"$oid":"5837a57ea05283111e4d50eb"},"View_count":36297,"Display_name":"Strohmi","Question_score":22,"Question_content":"I am new to modeling with neural networks, but I managed to establish a neural network with all available data points that fits the observed data well. The neural network was done in R with the nnet package:require(nnet)      ##33.8 is the highest valuemynnet.fit \u0026lt;- nnet(DOC/33.80 ~ ., data = MyData, size = 6, decay = 0.1, maxit = 1000)      mynnet.predict \u0026lt;- predict(mynnet.fit)*33.80  mean((mynnet.predict - MyData$DOC)^2) ## mean squared error was 16.5      The data I am analyzing looks as follows, where the DOC is the variable that has to be modeled (there are about 17,000 observations):      Q  GW_level Temp   t_sum   DOC1 0.045    0.070 12.50     0.2 11.172 0.046    0.070 12.61     0.4 11.093 0.046    0.068 12.66     2.8 11.164 0.047    0.050 12.66     0.4 11.285 0.049    0.050 12.55     0.6 11.456 0.050    0.048 12.45     0.4 11.48Now, I have read that the model should be trained with 70% of the data points, and validated with the remaing 30% of the data points. How do I do this? Which functions do I have to use?I used the train function from the caret package to calculate the parameters for size and decay.require(caret)my.grid \u0026lt;- expand.grid(.decay = c(0.5, 0.1), .size = c(5, 6, 7))mynnetfit \u0026lt;- train(DOC/33.80 ~ ., data = MyData, method = \"nnet\", maxit = 100, tuneGrid = my.grid, trace = f)Any direct help or linkage to other websites/posts is greatly appreciated.","Creater_id":2063,"Start_date":"2012-01-25 13:21:27","Question_id":21717,"Tags":["r","neural-networks"],"Answer_count":1,"Last_activity":"2016-08-15 02:04:58","Link":"http://stats.stackexchange.com/questions/21717/how-to-train-and-validate-a-neural-network-model-in-r","Creator_reputation":390}
{"_id":{"$oid":"5837a57ea05283111e4d50ed"},"View_count":33,"Display_name":"user3821193","Question_score":1,"Question_content":"I'm reading the book \"Convex Optimization\" by Boyd and Vandenbherge. On the second paragraph of page 71, the authors seem to state that in order to check if the Hessian (H) is positve semidefinite (for a function f in R), this reduces to the second derivative of the function being positive for any x in the domain of f and for the domain of f to be an interval. However, am I right to assume that the case for R^n is more complex, that is, I need to do more work to proof that z^THz is positive for any z (likely using minors, or something else)?","Creater_id":123457,"Start_date":"2016-08-15 00:47:29","Question_id":229869,"Tags":["optimization","convex","hessian"],"Answer_count":0,"Last_activity":"2016-08-15 02:02:56","Link":"http://stats.stackexchange.com/questions/229869/positive-definiteness-of-hessians","Creator_reputation":26}
{"_id":{"$oid":"5837a57ea05283111e4d50ef"},"View_count":36,"Display_name":"Lin Ma","Question_score":0,"Question_content":"Here is the link and related parts where I am confused. Especially confused what is the relationship between p and h, are they the same thing?  In effect, the methods compute Qp, the estimate for the k-th q-quantile, where p = k/q, from a sample of size N by computing a real valued index h. When h is an integer, the h-th smallest of the N values, xh, is the quantile estimate. Otherwise a rounding or interpolation scheme is used to compute the quantile estimate from h, x⌊h⌋, and x⌈h⌉. (For notation, see floor and ceiling functions).https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample","Creater_id":18254,"Start_date":"2016-08-12 01:46:45","Question_id":229482,"Tags":["descriptive-statistics","quantiles"],"Answer_count":1,"Last_activity":"2016-08-15 01:53:47","Link":"http://stats.stackexchange.com/questions/229482/quantile-estimation-issue","Creator_reputation":108}
{"_id":{"$oid":"5837a57ea05283111e4d50f1"},"View_count":55,"Display_name":"llewmills","Question_score":1,"Question_content":"I have recently completed a study where I hypothesised a significant difference between two groups. I hypothesised that Group B would be report a significantly lower score on the outcome variable on average following the experimental manipulation than group A.The t-test testing this hypothesis was non-significant (p = .998). There was so little difference in the scores that I thought I might perform a post-hoc test for equivalence using a Bayes Factor to test the hypothesis of no difference against the alternative hypothesis of some difference. My question is should the alternative hypothesis I test the null hypothesis against be directional like my original hypothesis, or should it be bi-directional, testing the null hypothesis of no difference against the alternative of any difference (i.e. a difference in any direction, not just the original direction I hypothesised)?","Creater_id":79732,"Start_date":"2016-08-15 01:49:55","Question_id":229876,"Tags":["bayesian","bayes-factors"],"Answer_count":0,"Last_activity":"2016-08-15 01:49:55","Link":"http://stats.stackexchange.com/questions/229876/directional-vs-non-directional-alternative-hypothesis-in-bayesian-model-comparis","Creator_reputation":120}
{"_id":{"$oid":"5837a57ea05283111e4d50f3"},"View_count":86,"Display_name":"Lioba","Question_score":3,"Question_content":"I have a sample of 100 participants who have scores on 5 different variables (V1-V5). Some participants took part in a workshop, others did not. I am interested in investigating the influence of workshop participation and gender on the scores of variables. For gender and workshop participation significant differences where shown (see below). My data looks the following:  V1 V2 V3 V4 V5 gender workshop1  2  2  5  5  4   male      yes2  4  3  4  3  3 female      yes3  3  5  5  3  1 female      yes4  1  5  5  5  1 female      yes5  5  2  2  4  5 female       no6  5  4  5  1  7   male      yes...Here is what I did:MANOVA  library (car)DV \u0026lt;- as.matrix(x[, 1:5])output = lm(DV ~ gender*workshop, data = x, contrasts=list(gender = contr.sum, workshop = contr.sum))manova_out = Manova(output, type = \"III\")summary (manova_out, multivariate = T)Multivariate Tests: gender                 Df test stat approx F num Df den Df  Pr(\u0026gt;F)  Pillai            1 0.1336421 2.838336      5     92 0.01983 *Wilks             1 0.8663579 2.838336      5     92 0.01983 *Hotelling-Lawley  1 0.1542574 2.838336      5     92 0.01983 *Roy               1 0.1542574 2.838336      5     92 0.01983 *---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Multivariate Tests: workshop                 Df test stat approx F num Df den Df   Pr(\u0026gt;F)  Pillai            1 0.0967188 1.970179      5     92 0.090415 .Wilks             1 0.9032812 1.970179      5     92 0.090415 .Hotelling-Lawley  1 0.1070749 1.970179      5     92 0.090415 .Roy               1 0.1070749 1.970179      5     92 0.090415 .---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1Multivariate Tests: gender:workshop                 Df test stat  approx F num Df den Df  Pr(\u0026gt;F)Pillai            1 0.0242859 0.4579838      5     92 0.80649Wilks             1 0.9757141 0.4579838      5     92 0.80649Hotelling-Lawley  1 0.0248904 0.4579838      5     92 0.80649Roy               1 0.0248904 0.4579838      5     92 0.80649Workshop participation appears to have a significant effect on the scores of variables. My questions:Is this effect maybe just due to the larger amount of females in the sample? How can I make sure that my significant results for workshop participation are only due to workshop participation and not gender? Is there a way of controlling for gender?Would checking for an interaction effect between gender and workshop be useful? Is a MANOVA be the right method in that case? But what would the MANOVA results tell me?  And to what extend are my results influenced by the different amount of females/males and workshop participants/not workshop participants?For replicationMy datalibrary (dplyr)set.seed(2)n \u0026lt;- 100x \u0026lt;- replicate(5, sample(1:5, n, rep=T))colnames(x) \u0026lt;- paste0(\"V\", 1:5)x \u0026lt;- as.data.frame(x)xworkshop \u0026lt;- sample(c(\"yes\", \"no\"), n, rep=T)x[xworkshop == \"yes\", 1:5] + sample(0:1, 100, rep=T, prob=c(.9, .1))x[xgender == \"male\", 1:5] + sample(0:1, 100, rep=T, prob=c(.9, .1))xworkshop)xgender)","Creater_id":126743,"Start_date":"2016-08-05 07:20:07","Question_id":228448,"Tags":["r","hypothesis-testing","multivariate-analysis","manova"],"Answer_count":1,"Last_activity":"2016-08-15 01:22:22","Link":"http://stats.stackexchange.com/questions/228448/multivariate-differences-between-groups-controlling-for-one-factor-manova","Creator_reputation":16}
{"_id":{"$oid":"5837a57ea05283111e4d50f5"},"View_count":331,"Display_name":"Daniel B.L.","Question_score":1,"Question_content":"As the title suggests, I'm trying to test for the sign of Granger causality in a large VAR. For exposition, consider the following three-dimensional VAR:\\begin{align}\\vec y_t=\\vec c+\\sum_{\\ell=1}^pA_\\ell \\vec y_{t-\\ell},\\end{align}where . I want to determine the sign of the causal relationship . My first thought was to use\\begin{align}\\text{sign}\\left(\\sum_{\\ell=1}^p [A_\\ell]_{12}\\right)\\end{align}and call it a day, but this ignores the impact of  on the system.I now know that the correct way to infer the sign is to use the impulse response function, based on the following comment from Dave Giles's blog. As implied by his comment, however, if the IRF is not (weakly) positive or negative, then the sign of the causality likely depends on the time horizon. This suggests to me that perhaps I should be referring to the impulse transfer function to infer how the sign of causality depends on frequency.Is my thinking on this matter correct? And if so, how can I correctly use the impulse transfer function to determine how the sign of causality depends on frequency?","Creater_id":71666,"Start_date":"2015-03-21 10:13:02","Question_id":142777,"Tags":["var","granger-causality"],"Answer_count":1,"Last_activity":"2016-08-15 00:58:56","Link":"http://stats.stackexchange.com/questions/142777/how-can-we-determine-the-sign-of-granger-causality-in-a-2-dimensional-var","Creator_reputation":21}
{"_id":{"$oid":"5837a57ea05283111e4d50f7"},"View_count":14,"Display_name":"midi","Question_score":0,"Question_content":"The website thegrid.io generated quite some heat with the features that they are promising - dynamic content layout, keeping in mind design nuances.For the best suitable design and layout it seems they might probably have to learn from a large dataset. However I am not able to find any literature in this field. Could someone kindly point me to the relevant articles that throw light upon this task. To me it does not look trivial at all and therefore I am curious as to what the current state of the art is.","Creater_id":78378,"Start_date":"2016-08-15 00:58:16","Question_id":229871,"Tags":["machine-learning","recommender-system"],"Answer_count":0,"Last_activity":"2016-08-15 00:58:16","Link":"http://stats.stackexchange.com/questions/229871/automatic-layout-design","Creator_reputation":21}
{"_id":{"$oid":"5837a57ea05283111e4d50f9"},"View_count":225,"Display_name":"Nick Allen","Question_score":4,"Question_content":"I understand that imbalance or skew in the target variable within your training data can negatively impact effectiveness.  Does the same apply to the predictor/independent variables?y ~ B0 + B1*x1 + B2*x2Consider this simple example.  I am trying to predict y, a categorical variable, from two variables x1 and x2 which are also categorical variables.  If I have an imbalanced set of y values, this could be a bad thing.  What if I have an imbalanced set of x1 or x2?  Could the same issue apply?","Creater_id":32358,"Start_date":"2014-06-04 12:41:26","Question_id":101189,"Tags":["skewness"],"Answer_count":2,"Last_activity":"2016-08-14 23:58:44","Link":"http://stats.stackexchange.com/questions/101189/is-there-a-negative-impact-from-imbalance-skew-in-predictor-variables","Creator_reputation":121}
{"_id":{"$oid":"5837a57ea05283111e4d50fb"},"View_count":36,"Display_name":"Felix Zhao","Question_score":0,"Question_content":"I have trained a predictive model based on random forest algorithm, but it took two days to train the model and the results were quite good according to test dataset.The problem i am facing right now is that, every time i shut down my computer, if i need to make predictions based on new data, i have to run the model first, which made it very ineffective to make the predictions, so the more generalized question is the deployment of the model based on R.Is there any way that i could reuse the model without training the model first?","Creater_id":97736,"Start_date":"2016-08-14 23:43:16","Question_id":229865,"Tags":["r","model"],"Answer_count":1,"Last_activity":"2016-08-14 23:52:18","Link":"http://stats.stackexchange.com/questions/229865/r-predictive-models-reuse-without-running-the-model-everytime","Creator_reputation":6}
{"_id":{"$oid":"5837a57ea05283111e4d50fd"},"View_count":51,"Display_name":"user2939212","Question_score":2,"Question_content":"This is related to this question. I am concerned about the expected value of a function estimated using sampling. If the samples were obtained by a sampling method such as Gibbs sampling, and if I do the following estimation,  Can we say  ?I think we can arrive at this by. ","Creater_id":72507,"Start_date":"2016-08-13 22:01:07","Question_id":229734,"Tags":["self-study","sampling","mcmc","unbiased-estimator","gibbs"],"Answer_count":1,"Last_activity":"2016-08-14 22:40:16","Link":"http://stats.stackexchange.com/questions/229734/expected-value-of-an-estimate-done-using-gibbs-sampling","Creator_reputation":110}
{"_id":{"$oid":"5837a57ea05283111e4d50ff"},"View_count":49,"Display_name":"NoNa","Question_score":1,"Question_content":"I have gathered the following data sets, 24 readings for 7 days and it is as follows:mydata\u0026lt;-c(0.1083,0,0,0,0,0.0083,0.05,0.1667,0.6583,0.7917,1.0333,1.4333,0.9833,1.0417,0.9917,0.8833,1.15,0.77,1.2083,1.05,1.7833,1.3417,0.9583,1.025,0.175,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.0583,0.6833,0.8333,0.5917,1.4917,0.8333,1.4583,1.2583,0.8417,1.025,0.4,0,0.0083,0,0.0333,0.2,0.2833,1.2333,1.2083,0.8417,0.7333,0.7,0.7833,1.2833,1.2667,0.7583,1.125,0.7667,0.625,1.0833,0.675,0.6917,0.4,0.1417,0.375,0.0167,00,0.0583,0.0917,0.2167,0.6083,0.9083,1.35,1.3,1.425,1.6417,0.925,0.875,1.3583,0.7083,0.65,0.8917,1.83,1.575,0.8917,0.5917,0.375,0.0667,0.0333,0.0083,0,0.0083,0.1,0.4083,0.9833,1.1083,1.0667,1.1917,1.375,0.5333,0.975,1.3833,0.725,0.9417,1,1.0083,1.275,0.7833,1.1417,0.5917,0.2917,0.1,0,0,0,0.0333,0.233,0.3667,0.7167,0.6833,1.55,1.175,0.9417,1.025,1.425,1.275,1.0583,1.1583,0.65,0.9417,1.4583,1.3417,1.9083,1.5417,0.75,0.1583,0.0083,0,0,0.1833,0.075,0.4083,0.6667,1.1,0.6167,0.8167,1.2667,1.6333,1.4083,0.9,0.975,0.8,0.7667,1.1417,2.0917,1.1667,0.675,0.65,0.4833,0.525,0.025,0.075,0,0,0.025,0.2333,0.5833,1.0833,0.9917,0.775,0.9667,0.6167) and the other set is : 0.00550.00080.00010000.00010.00090.00050.00220.000100.01110.00040.00180.02560.00160.00010.02830.03450.00460.01440.07320.00140.0002000000000000000.00040.00030.00020.020.00030.00040.00050.00180.00310.000500000.0050.00720.00030.00040.00040.00330.00360.00250.02690.00050.00010.00020.00330.00890.00020.00030.0021000.0008000000.00020.00030.00150.00310.000200.00090.01760.00220.00030.01560.04460.000100.00870.01250.34030.000100000000.00440.00030.00270.00030.00510.00020.00090.00140.0110.00120.000900.0030.0010.00510.000700000000.00010.00470.00120.00070.03670.03110.00040.00020.00030.09970.000300.00990.01320.00060.00570.009900000000.00190.00460.00010.01070.0020.00160.00620.10110.22570.00010.00020.0070.14010.02780.00260.00550.00180.00110.00020.00040.00030.00030.00020.00060.00170.0090.00780.01540.01710.00210.00250.0069And I need to find the best fitting probability distribution with good graphs  and a step by step explanation of what each graph represent , so I can use them for further analysisThe data is the readings of traffic in a mobile network node and the values are measured in Erlang. which is typically a negative exponential distribution fit. but I need to test it anyways.the second set is the data traffic and they are measured in GB.therefore you can see that all readings are \u003e 0.","Creater_id":127673,"Start_date":"2016-08-14 13:36:20","Question_id":229820,"Tags":["distributions","fitting","exponential"],"Answer_count":1,"Last_activity":"2016-08-14 22:17:52","Link":"http://stats.stackexchange.com/questions/229820/does-my-data-fit-an-exponential-distribution-and-what-does-it-mean","Creator_reputation":6}
{"_id":{"$oid":"5837a57ea05283111e4d5101"},"View_count":133,"Display_name":"drstevok","Question_score":0,"Question_content":"I am trying to replicate the methods used in this paper by Paul Rosenbaum and colleagues (Near-Far matching). I have been studying the methods in the referenced papers by Michael Baoicchi.My work examines the (binary) outcome (dead/alive) a treatment where there is a 'health system' discouragement that acts as the instrument. The treatment is usually selected for the sickest patients, and I am hoping that the 'discouragement' instrument will allow me to evaluate the outcome without selection bias.I have posted a sample data set here and the code to githubI have replicated the matching steps, and now have a balanced discouraged group and an encouraged group. Summary of balance for matched data:         Means Treated Means Control SD Control Std. Mean Diff. eCDF Med eCDF Mean eCDF Maxdistance        0.2998        0.2996     0.0347          0.0051   0.0055    0.0047   0.0330age            70.1648       70.3901    16.3334         -0.0133   0.0110    0.0148   0.0495male            0.4945        0.5165     0.5011         -0.0438   0.0110    0.0110   0.0220illness        16.2363       16.1978     7.7234          0.0050   0.0110    0.0123   0.0604I can see that there appears to be an effect of discouragement on both the treatment and the outcome.\u0026gt; with(mdt.match, table(treat, discourage)) # treatment is 'discouraged'     discouragetreat   0   1    0 132 140    1  50  42\u0026gt; with(mdt.match, table(dead, discourage))  # mortality is lower when discouraged    discouragedead   0   1   0 106 120   1  76  62A simple t-test now shows me the difference by 'discouragement', but doesn't exploit the IV effect.First, reshape the data (one row per pair).\u0026gt; mdt \u0026lt;- data.table(+     z0=tdf[matchit.outmatch.matrix[,1], 'treat'],+     y0=tdf[matchit.outmatch.matrix), 'discourage'],+     d1=tdf[row.names(matchit.outmatch.matrix), 'dead'])\u0026gt; mdt[,id.pair := .I, by=.I]\u0026gt; head(mdt,10)    z0 d0 y0 z1 d1 y1 id.pair 1:  0  0  0  1  0  0       1 2:  0  1  1  1  0  0       2 3:  0  0  0  1  0  0       3 4:  0  0  0  1  0  0       4 5:  0  0  0  1  1  0       5 6:  0  0  0  1  0  1       6 7:  0  0  1  1  1  1       7 8:  0  1  1  1  0  1       8 9:  0  0  0  1  0  0       910:  0  0  0  1  0  0      10Now the paired t-test\u0026gt; with(mdt, t.test(y1, y0, paired=TRUE))    Paired t-testdata:  y1 and y0t = -1.552, df = 181, p-value = 0.1224alternative hypothesis: true difference in means is not equal to 095 percent confidence interval: -0.17471948  0.02087332sample estimates:mean of the differences             -0.07692308 What I can't get calculate is the effect ratio, and its confidence interval. Can anyone point me to a walk-through, or offer some simple guidance.","Creater_id":7746,"Start_date":"2015-07-28 16:21:39","Question_id":163652,"Tags":["r","instrumental-variables","matching","propensity-scores"],"Answer_count":0,"Last_activity":"2016-08-14 22:09:00","Link":"http://stats.stackexchange.com/questions/163652/instrumental-variable-analysis-after-propensity-matching","Creator_reputation":275}
{"_id":{"$oid":"5837a57ea05283111e4d5103"},"View_count":22,"Display_name":"DesirePRG","Question_score":0,"Question_content":"I have trouble in getting my head around with the concept of input standardization. I have several subquestions as follows.1) When we standardize input, does it mean we need to standardize the response variable as well? or only the predictor variables?2) Does this always improve the model? in terms of performance. 3) can there be instances where standardized model and unstandardized model have the same performance with the same data. ","Creater_id":53428,"Start_date":"2016-08-14 20:46:01","Question_id":229857,"Tags":["regression","machine-learning","data-preprocessing"],"Answer_count":0,"Last_activity":"2016-08-14 20:46:01","Link":"http://stats.stackexchange.com/questions/229857/what-does-input-standardizing-gain","Creator_reputation":123}
{"_id":{"$oid":"5837a57ea05283111e4d5105"},"View_count":39,"Display_name":"William","Question_score":1,"Question_content":"To the best of my knowledge, the posterior odds satisfies the equation: (\\text{posterior odds}) = (\\text{Bayes factor}) \\times (\\text{prior odds})  This is a simple consequence of Bayes' rule.The whole point of Bayesian inference when applied to model selection, or so I thought, was to use the information from the prior probabilities to get a more accurate estimate of what the correct answer is than the naive estimate one gets from the likelihoods, which is given by the Bayes factor.However, I recall having read several papers where the Bayes factors were reported as evidence of one model being more likely than the other.  Was the idea of the paper's authors to appeal to frequentists who would have considered it taboo to incorporate information from prior probabilities, and to show that their argument was (their results were) robust to such objections on methodological/philosophical grounds?     Would a Bayesian ever be more interested in the Bayes factor than the posterior odds?Note: I had these questions while reading the first chapter of James Stone's \"Bayes' Rule: A Tutorial Introduction to Bayesian Analysis\" and while thinking back to some papers I had read a while ago about influenza virus transmission. I can try to find the paper if that would help.Anyway I am a complete novice at this so I apologize in advance if this question is non-sensical.","Creater_id":113090,"Start_date":"2016-08-14 19:23:58","Question_id":229852,"Tags":["bayesian","odds-ratio"],"Answer_count":1,"Last_activity":"2016-08-14 20:44:08","Link":"http://stats.stackexchange.com/questions/229852/why-is-the-bayes-factor-sometimes-considered-more-important-than-the-posterior-o","Creator_reputation":2039}
{"_id":{"$oid":"5837a57ea05283111e4d5107"},"View_count":5684,"Display_name":"user164846","Question_score":7,"Question_content":"I have some data that I smooth using loess. I'd like to find the inflection points of the smoothed line. Is this possible? I'm sure someone has made a fancy method to solve this...I mean...after all, it's R! I'm fine with changing the smoothing function I use. I just used loess because that's what I was have used in the past. But any smoothing function is fine. I do realize that the inflection points will be dependent on the smoothing function I use. I'm okay with that. I'd like to get started by just having any smoothing function that can help spit out the inflection points. Here's the code I use:x = seq(1,15)y = c(4,5,6,5,5,6,7,8,7,7,6,6,7,8,9)plot(x,y,type=\"l\",ylim=c(3,10))lo \u0026lt;- loess(y~x)xl \u0026lt;- seq(min(x),max(x), (max(x) - min(x))/1000)out = predict(lo,xl)lines(xl, out, col='red', lwd=2)","Creater_id":5913,"Start_date":"2013-11-18 20:59:16","Question_id":76959,"Tags":["r","smoothing","loess"],"Answer_count":5,"Last_activity":"2016-08-14 20:19:41","Link":"http://stats.stackexchange.com/questions/76959/finding-inflection-points-in-r-from-smoothed-data","Creator_reputation":155}
{"_id":{"$oid":"5837a57ea05283111e4d5109"},"View_count":218,"Display_name":"hxd1011","Question_score":5,"Question_content":"I understand for squared loss, add a  to objective function will simplify many derivations, since the derivative of a square has a constant .Are we doing similar things to logistic loss? If not why, residual deviance is twice the negative log likelihood?Few lines of code to demo my question.fit=glm(vs~mpg+hp+wt,mtcars,family = binomial())p=fitvs# these two values are the samefit$deviance/2-sum(y*log(p)+(1-y)*log(1-p))","Creater_id":113777,"Start_date":"2016-08-14 14:46:48","Question_id":229824,"Tags":["r","logistic","generalized-linear-model","deviance"],"Answer_count":1,"Last_activity":"2016-08-14 19:49:42","Link":"http://stats.stackexchange.com/questions/229824/why-is-a-glms-residual-deviance-minus-twice-its-log-likelihood","Creator_reputation":4443}
{"_id":{"$oid":"5837a57ea05283111e4d510b"},"View_count":1042,"Display_name":"S\u0026#248;r\u0026#235;n","Question_score":7,"Question_content":"The expected value of a distribution  is the mean, that is the weighted average valueE[x]=\\int_{-\\infty}^{+\\infty} x \\, \\,  f(x) dxThe most likely value is the mode, that is  the most probable value.However do we expect somehow to see  a lot of times? Quoting from here:  If the outcomes  are not equally probable, then the simple average must be replaced with the weighted average, which takes into account the fact that some outcomes are more likely than the others. The intuition however remains the same: the expected value of  is what one expects to happen on average.I cannot understand what does \"happen on average\" means, does this mean that, for istance, taking a measure a lot of time I expect to see  more than other values of ? But isn't this the definition of mode?So how to interpret the statement? And what is the probabilistic meaning of ?I would also like to show an example where I do get confused. Studying  distribution I learned that the mode is , while , where  are the degrees of freedom of data.I heard at university that, when doing a  test after using Least Squares Method to fit a set of data,  I should expect to get  because \"that's what happens in general\".Did I misunderstand all of this or is the expected value somehow very probable? (Even if the most probable value is of course the mode)","Creater_id":114858,"Start_date":"2016-08-14 07:29:35","Question_id":229779,"Tags":["probability","distributions","chi-squared","expected-value","mode"],"Answer_count":4,"Last_activity":"2016-08-14 19:47:01","Link":"http://stats.stackexchange.com/questions/229779/expected-value-vs-most-probable-value-mode","Creator_reputation":162}
{"_id":{"$oid":"5837a57ea05283111e4d510d"},"View_count":60,"Display_name":"Toney Shields","Question_score":2,"Question_content":"Pretty much all of the test statistics fit a certain distribution. Is there a possibility that when doing my own hypothesis test, I could get a test statistic that doesn't fit any distribution?","Creater_id":109437,"Start_date":"2016-08-14 05:28:00","Question_id":229766,"Tags":["hypothesis-testing","distributions"],"Answer_count":2,"Last_activity":"2016-08-14 18:53:05","Link":"http://stats.stackexchange.com/questions/229766/does-every-test-statistic-fit-a-certain-distribution","Creator_reputation":197}
{"_id":{"$oid":"5837a57ea05283111e4d510f"},"View_count":140,"Display_name":"Joshua Onyango","Question_score":1,"Question_content":"I'm working with a large data set looking at climate variables to see whether they could be risk factor for a sheep disease in a country with dependent variable being disease count (cases) and also whether the factors could explain regional variations i.e why more cases in some regions. My problem is that the variation between independent variables seem too be too small making it harder to come up with best method of analysis. I have tried approach such as poisson but get error which likely not the best approach.Some suggested ANOVA which cant figure out how this would work. Any advise will be of great help. Here is part of the data plus results when I tried poisson (whole data as region just brings all results to - NA cames in likely because of small variation on the variables:    Region  Max temp    Min temp    total rain  Rain days   Cases        1   13.45       6.05        922.9       143.9       0        1   13.45       6.05        922.9       143.9       0        1   13.45       6.05        922.9       143.9       0        1   13.45       6.05        922.9       143.9       0        1   13.45       6.05        922.9       143.9       16        2   15.32       7.13        629.8       112.6       0        2   15.32       7.13        629.8       112.6       0        2   15.32       7.13        629.8       112.6       70        2   15.32       7.13        629.8       112.6       127        2   15.32       7.13        629.8       112.6       11        2   15.32       7.13        629.8       112.6       130        3   14.17       7.06        1068.8      155.7       0        3   14.17       7.06        1068.8      155.7       5        3   14.17       7.06        1068.8      155.7       0        3   14.17       7.06        1068.8      155.7       0        3   14.17       7.06        1068.8      155.7       35        3   14.17       7.06        1068.8      155.7       0        3   14.17       7.06        1068.8      155.7       0        4   15.41       7.02        453.7       89.9        0        4   15.41       7.02        453.7       89.9        130        4   15.41       7.02        453.7       89.9        98        4   15.41       7.02        453.7       89.9        20        4   15.41       7.02        453.7       89.9        565        4   15.41       7.02        453.7       89.9        0Call:glm(formula = Cases2011 ~ Winmaxtemp11 + Springmaxtemp11 + Summaxtemp11 +     Autummaxtem11 + Anumaxtemp11 + Winmintemp11 + Springmintemp11 +     Summintemp11 + Autmintem11 + Anumintemp11 + Winsundays11,     family = \"poisson\", data = orf)Deviance Residuals:     Min       1Q   Median       3Q      Max  -18.689  -10.242   -8.238   -1.635  124.064  Coefficients: (7 not defined because of singularities)                 Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)     -2.424380   0.453662  -5.344 9.09e-08 ***Winmaxtemp11    -0.009405   0.080438  -0.117    0.907    Springmaxtemp11 -4.570125   0.120415 -37.953  \u0026lt; 2e-16 ***Summaxtemp11     0.636944   0.121192   5.256 1.47e-07 ***Autummaxtem11    3.900808   0.067515  57.777  \u0026lt; 2e-16 ***Anumaxtemp11           NA         NA      NA       NA    Winmintemp11           NA         NA      NA       NA    Springmintemp11        NA         NA      NA       NA    Summintemp11           NA         NA      NA       NA    Autmintem11            NA         NA      NA       NA    Anumintemp11           NA         NA      NA       NA    Winsundays11           NA         NA      NA       NA  ","Creater_id":82976,"Start_date":"2015-11-20 12:25:30","Question_id":182798,"Tags":["logistic","correlation","multiple-regression","chi-squared","poisson-regression"],"Answer_count":1,"Last_activity":"2016-08-14 17:52:17","Link":"http://stats.stackexchange.com/questions/182798/statistical-approach-on-association-between-two-variables","Creator_reputation":16}
{"_id":{"$oid":"5837a57ea05283111e4d5111"},"View_count":46,"Display_name":"user93892","Question_score":0,"Question_content":"I am quite familiar with statistical significance, but do not very understand the clinical significance I consulted the wiki but still felt confused with this concept. Could anyone explain for me what is clinical signivicance and what is the difference between clinical significance and statistical significance? ","Creater_id":93892,"Start_date":"2016-08-14 15:17:20","Question_id":229828,"Tags":["statistical-significance","clinical-trials"],"Answer_count":1,"Last_activity":"2016-08-14 17:45:03","Link":"http://stats.stackexchange.com/questions/229828/what-is-the-difference-between-clinical-significance-and-statistical-significanc","Creator_reputation":58}
{"_id":{"$oid":"5837a57ea05283111e4d5113"},"View_count":25,"Display_name":"chris","Question_score":1,"Question_content":"I'm trying to measure the impact of teachers' performance on student outcomes. I'm not entirely sure how to do this however. As my dependent variables, I have the grades of the students, mapped onto a 1-10 scale. I have a number of independent variables, including aspects about the students. My concern is in how to incorporate the teachers into the regression. It would seem like the simplest way would be to introduce them as dummy variables, with a 1 representing they taught the student, and a 0 representing that they did not teach the student. This seems problematic though, as the dataset has around 1000 teachers, and there are 600,000 students in the dataset. It seems that this would be far too many variables to regress on at once. Do you have any advice in how I can incorporate the teachers' performance into my model?","Creater_id":71665,"Start_date":"2016-08-14 16:29:05","Question_id":229838,"Tags":["regression","categorical-data"],"Answer_count":1,"Last_activity":"2016-08-14 17:42:01","Link":"http://stats.stackexchange.com/questions/229838/not-sure-whether-including-1000-dummy-variables-is-the-best-method-or-whether-th","Creator_reputation":23}
{"_id":{"$oid":"5837a57ea05283111e4d5115"},"View_count":37,"Display_name":"Mathias","Question_score":0,"Question_content":"As a learning exercise to understand the algorithm, I am implementing gradient boosting for regression, based on the Wikipedia description. I think I get the gist of it, but I have been stumped by a question, which leads me to wonder if I am misunderstanding it.Here is my understanding:* at iteration n, I have a predictor F_n available,* compute the residuals r_n = Y - F_n(X)* fit a model f_n to the residuals, minimizing some cost function L* find gamma_n to minimize L(Y, F_n(X) + gamma_n * f_n(X))* iterate with F_n+1 = F_n + gamma_n * f_n  I have tried this out with L = SSR / sum of squares of prediction errors, fitting stumps as well as regression trees to minimize the same cost function. What puzzled me is that gamma always ended up being 1.0.Which leads to my question: why would gamma not be 1.0?I can think of 3 reasons:1. I am misunderstanding something in the algorithm,2. I happen to use the same cost function to fit my weak learners and the combination, which might not always be the case (I might want to, say, minimize manhattan distance overall, but fit weak learners using a different cost function),3. Gamma could be searched on a test set different from the training set, to avoid over-fitting (which is not explicit in the Wikipedia description).  The short version: am I missing something? Should I expect gamma = 1.0 if I use SSR as a cost? Any insight would be greatly appreciated, the nagging doubt that I am missing something is driving me nuts.","Creater_id":945,"Start_date":"2016-08-14 16:52:23","Question_id":229840,"Tags":["machine-learning","boosting"],"Answer_count":0,"Last_activity":"2016-08-14 16:52:23","Link":"http://stats.stackexchange.com/questions/229840/why-would-gamma-be-different-from-1-in-gradient-boosting","Creator_reputation":128}
{"_id":{"$oid":"5837a57ea05283111e4d5117"},"View_count":5767,"Display_name":"Bryce Thomas","Question_score":14,"Question_content":"I have some data to which I am trying to fit a trendline.  I believe the data to follow a power law, and so have plotted the data on log-log axes looking for a straight line.  This has resulted in an (almost) straight line and so in Excel I have added a trendline for a power law.  Being a stats newb, my question is, what is now the best way for me to go from \"well the line looks like it fits pretty well\" to \"numeric property  proves that this graph is fitted appropriately by a power law\"?  In Excel I can get an r-squared value, though given my limited knowledge of statistics, I don't even know whether this is actually appropriate under my specific circumstances.  I have included an image below showing the plot of the data I am working with in Excel.  I have a little bit of experience with R, so if my analysis is being limited by my tools, I am open to suggestions on how to go about improving it using R.","Creater_id":870,"Start_date":"2010-10-01 10:04:11","Question_id":3242,"Tags":["goodness-of-fit","power-law"],"Answer_count":3,"Last_activity":"2016-08-14 16:01:11","Link":"http://stats.stackexchange.com/questions/3242/how-to-measure-argue-the-goodness-of-fit-of-a-trendline-to-a-power-law","Creator_reputation":267}
{"_id":{"$oid":"5837a57ea05283111e4d5119"},"View_count":3922,"Display_name":"ching","Question_score":2,"Question_content":"i want to model a logistic regression with imbalanced data (9:1).i wanted to try the weights option in the glm function, but i'm not 100% sure what it does. maybe someone can help me out...let say my output variable is (0,0,0,0,0,0,0,0,0,1).now i want to give the \"1\" 10 times more weight.so i give the weights argument weights=c(1,1,1,1,1,1,1,1,1,1,1,10).when i do that, it will be considered in the calculation of the maximum likelihood. am i right? misclassification of \"1\" ist just 10 times worse then missclassify a \"0\".i hope u guys can help methanks!","Creater_id":84119,"Start_date":"2015-08-04 11:28:33","Question_id":164693,"Tags":["regression","logistic","classification","weighted-data","imbalanced"],"Answer_count":2,"Last_activity":"2016-08-14 15:45:04","Link":"http://stats.stackexchange.com/questions/164693/weights-in-glm-logistic-regression-imbalanced-data","Creator_reputation":57}
{"_id":{"$oid":"5837a57ea05283111e4d511b"},"View_count":28,"Display_name":"capybara","Question_score":1,"Question_content":"How would you interpret negative correlations between factors in factor analysis, and should they ever be negative? I did FA with oblique rotation (Oblimin) on 30 variables which were intended to measure the same dimension, and I extracted 4 (correlated) factors. All the variables were previously recoded in order to express the same side of the measured dimension. Yet, some of factors have negative correlations that I don't know how to interpret.Also, it may be important that some of the loadings from the pattern matrix were negative (which I also am not sure how to interpret). I would expect them all to be positive, since the items were recoded...(Don't know if it matters, but I am using SPSS statistics, and the extraction method was maximum likelihood.)I hope somebody more experienced in factor analysis can help me out. Thanks!","Creater_id":127679,"Start_date":"2016-08-14 15:42:41","Question_id":229832,"Tags":["correlation","interpretation","factor-analysis","factor-rotation"],"Answer_count":0,"Last_activity":"2016-08-14 15:42:41","Link":"http://stats.stackexchange.com/questions/229832/interpretation-of-negative-factor-correlations","Creator_reputation":6}
{"_id":{"$oid":"5837a57ea05283111e4d511d"},"View_count":29,"Display_name":"JonB","Question_score":1,"Question_content":"I'm working on a revision of a manuscript. I have a longitudinal cohort of adolescents that were followed throughout junior high school. One of the reviewers considers the attrition to be considerable, and asks me to reweight the sample to reflect the census on certain characteristics, so that the results can be compared to the general population that the sample aims to reflect.I understand the idea, but how do I go about to do this? I mean, say that I have a divorce rate (one of the characteristics suggested by the reviewers) of 10 percent in my sample and 15 percent in the relevant general population, how do I reweight my sample to reflect this? Is is it that simple that the 10 percent with divorced parents get a weight of 15/10 = 1.5 and the 90 percent with parents that are not divorced get 85/90 = 0.94, and then I just use this weight in the calculations? Do anyone have materials to good tutorials regarding this issue?","Creater_id":80706,"Start_date":"2015-09-21 01:39:08","Question_id":173444,"Tags":["survey-weights"],"Answer_count":1,"Last_activity":"2016-08-14 15:33:20","Link":"http://stats.stackexchange.com/questions/173444/reweighting-sample-to-reflect-census","Creator_reputation":1818}
{"_id":{"$oid":"5837a57ea05283111e4d511f"},"View_count":56,"Display_name":"Thomas Browne","Question_score":4,"Question_content":"I would like to remove the first principal component from a data set, but keep that data set in its original coordinates. I have taken a stab at this by taking PCA, zeroing the first PC, and then rotating back using the inverse of the eigvenvector matrix. Is that the most efficient way to do this?create a sample data set:set.seed(1234)xx \u0026lt;- rnorm(500)yy \u0026lt;- 0.5 * xx + rnorm(500, sd = 0.3)vec \u0026lt;- cbind(xx, yy)plot(vec, xlim = c(-4, 4), ylim = c(-4, 4))Take principal components and zero out the first PC:vv \u0026lt;- eigen(cov(vec))$vectorsnewvec \u0026lt;- vec %*% vvnewvec[, 1] \u0026lt;- 0Now rotate the new data set back to its original coordinates using the inverse of the PCA rotation matrix:rvec \u0026lt;- newvec %*% t(vv) # transpose of orthogonal matrix = inverse# plot new points in red and plot eigenvectors in greenpoints(rvec, col = \"red\")arrows(0, 0, vv[1,1], vv[2, 1], col = \"green3\", lwd = 2)arrows(0, 0, vv[1,2], vv[2, 2], col = \"green3\", lwd = 2)legend(\"topleft\", legend = c(\"original data\", \"data after extracting PC1\", \"eigenvectors of original data\"), fill = c(\"black\", \"red\", \"green3\"))As you can see it seems to agree with the eigenvector orientations in green. But is this the correct and/or best way? Can I avoid the intermediate matrix multiplications for example?","Creater_id":4705,"Start_date":"2016-08-04 04:28:18","Question_id":228240,"Tags":["r","pca","linear-algebra"],"Answer_count":0,"Last_activity":"2016-08-14 15:19:08","Link":"http://stats.stackexchange.com/questions/228240/how-do-i-remove-the-first-principal-component-from-a-data-set-while-keeping-it","Creator_reputation":298}
{"_id":{"$oid":"5837a57ea05283111e4d5121"},"View_count":25,"Display_name":"Adam M. Kuczynski","Question_score":1,"Question_content":"Quick question: when performing CFA with maximum likelihood estimation, does the assumption of multivariate normality apply to the items or to the latent factors? Initially I thought it applied to the items, but considering the way the arrow points (that factor predicting the item score), I am now rethinking that.Thanks for your help. ","Creater_id":127675,"Start_date":"2016-08-14 14:02:57","Question_id":229822,"Tags":["maximum-likelihood","factor-analysis","psychometrics","confirmatory-factor"],"Answer_count":0,"Last_activity":"2016-08-14 14:02:57","Link":"http://stats.stackexchange.com/questions/229822/multivariate-normality-in-confirmatory-factor-analysis-of-items-or-factors","Creator_reputation":6}
{"_id":{"$oid":"5837a57ea05283111e4d5123"},"View_count":183,"Display_name":"user4429703","Question_score":2,"Question_content":"I am trying to find the correlation between two continuous variables but the values of one of them are originally given in intervals ([1-5] ,[6-9],...). What is the best approach to deal with this? Should I take the mean of each interval and use Pearson's r or rather rank the intervals and use Spearman's Rho?","Creater_id":66098,"Start_date":"2015-01-07 10:13:58","Question_id":132577,"Tags":["correlation","confidence-interval","ordinal","pearson","spearman"],"Answer_count":2,"Last_activity":"2016-08-14 13:50:34","Link":"http://stats.stackexchange.com/questions/132577/correlation-choice-when-attribute-values-are-given-in-intervals","Creator_reputation":11}
{"_id":{"$oid":"5837a57ea05283111e4d5125"},"View_count":30,"Display_name":"luchonacho","Question_score":2,"Question_content":"Is there a formal method/test/proof that one can use to justify the use of sampling weights when using survey data? You could compare the coefficients in both regressions, the R2, etc. But is this enough?I ask this because there tests and estimation methods available for my software (Stata) is restricted when using weights. Notice that the use of weights many times means that the sample sizes differ among regressions (as some observations have weight equal to zero).","Creater_id":100369,"Start_date":"2016-08-14 11:02:00","Question_id":229806,"Tags":["weighted-regression","survey-weights","survey-sampling"],"Answer_count":1,"Last_activity":"2016-08-14 13:12:48","Link":"http://stats.stackexchange.com/questions/229806/formal-test-to-justify-the-use-of-sampling-weights-in-survey-data","Creator_reputation":584}
{"_id":{"$oid":"5837a57ea05283111e4d5127"},"View_count":27,"Display_name":"SriniShine","Question_score":0,"Question_content":"I’m working with an enzymatic reaction. Below is the enzymatic reaction (A –substrate, B - product and E - enzyme).The reaction is broken down into three reactions so it could be represented as a computational model and simulate.A + E -\u003e A|E A|E   -\u003e B + E A|E   -\u003e A + E The model is simulated using Marcie. Concentration of all the metabolites is recorded over time (time series data). I’m experimenting on how activity of each reaction affects the overall activity of the model. Activity of a reaction is increased by repeating the reaction in the model. For example to double the activity of a reaction, reaction is repeated twice in the model. It is decide to vary the activity of a reaction from 1 to 100. For each activity there would be separate model. As there are only 3 reactions if I vary each reaction’s activity from 1 to 100 there would be 1,000,000 models. It’s impractical to look at all the million models. The approach I’m doing now is take a random sample of 100 models, simulates them and cluster the models based on the metabolites concentration. However this approach is not giving me good results. Question: What methods could be used to identify the reaction or reactions which is crucial in determining the overall activity of the model?  I came across the terms sensitivity analysis and parameter scanning however I do not have the understanding how those could be applied to this scenario.  I'm using R for the analysis.","Creater_id":84181,"Start_date":"2016-08-14 13:01:00","Question_id":229818,"Tags":["r","bioinformatics","parameter-optimization","sensitivity-analysis"],"Answer_count":0,"Last_activity":"2016-08-14 13:01:00","Link":"http://stats.stackexchange.com/questions/229818/how-to-determine-the-importance-of-individual-reactions-for-the-overall-activity","Creator_reputation":18}
{"_id":{"$oid":"5837a57ea05283111e4d5129"},"View_count":298,"Display_name":"Z. Li","Question_score":3,"Question_content":"Consider the penalized linear regression problem:\\text{minimize}_\\beta \\,\\,(y-X\\beta)^T(y-X\\beta)+\\lambda \\sqrt{\\sum \\beta_i^2} Without the square root this problem becomes ridge regression. Note that this is not the LASSO problem which may be expressed as:\\text{minimize}_\\beta \\,\\,(y-X\\beta)^T(y-X\\beta)+\\lambda \\sum \\sqrt{ \\beta_i^2} This is also a special case of group LASSO when all coefficients are within one group. Is there a closed form solution to this problem?","Creater_id":91322,"Start_date":"2015-10-05 19:24:49","Question_id":175624,"Tags":["regression","penalized"],"Answer_count":1,"Last_activity":"2016-08-14 12:28:45","Link":"http://stats.stackexchange.com/questions/175624/is-there-a-closed-form-solution-for-l2-norm-regularized-linear-regression-not-r","Creator_reputation":19}
{"_id":{"$oid":"5837a57ea05283111e4d512b"},"View_count":45,"Display_name":"slazien","Question_score":3,"Question_content":"I'm going through Blitzstein's book Introduction to Probability and on p. 45 there is this exercise:Now, NOT using Bayes' Rule, . For the exercise, , since the comma means AND. This is then: . This is different from the  in the exercise. Why is that?","Creater_id":121401,"Start_date":"2016-08-14 11:50:22","Question_id":229810,"Tags":["probability","conditional-probability","combinatorics"],"Answer_count":1,"Last_activity":"2016-08-14 12:28:01","Link":"http://stats.stackexchange.com/questions/229810/conditional-probability-union-of-events","Creator_reputation":58}
{"_id":{"$oid":"5837a57ea05283111e4d512d"},"View_count":40,"Display_name":"Deepend","Question_score":-1,"Question_content":"I am having some trouble confirming that I am on the right track and I hope someone can help me out.I have a data set with 9 products each of which has received 8 treatments, including the control this makes a 9 Product x 9 Treatment matrix that was organised in a reduced Latin square. Participants in the experiment were required to provide user ratings for each of the 9 products, one each with a different treatment applied. Consequently each participant seen each product once and each treatment once. This means that every participant contributed to each of the mean user ratings below. I then worked out the mean user ratings for the control and each of the user ratings. Below is an example:             Control  T1      T2      T3     T4     T5     T6      T7     T8Mean Ratings 4.74     -1.77   -1.88   7.77   4.08   6.90   13.66   17.13  19.44I want to find out the level of effect of each treatment in standard deviations from the mean of the control, which received no treatment.The mean user ratings for the control and each of the treatments were taken from the same population and are selected at randomMy mean user ratings range from -100 to +100My data is normally or approximately normally distributed.  The sample size for each mean user ratings is \u003e 30 The Standard Deviation is known.Question: Is it possible using Z-tests to compare the the mean user rating of each of the treatments to the mean of the control? Is this the correct test?So the comparisons will be in the from of Control V T1 then Control V T2 etc.I did ask a similar question previously, but that focused on my efforts and what I have attempted so far. I wanted to ask this question separately to make sure I was on the right track before I keep going.This has been driving me slightly crazy for a few days not so any help would be seriously appreciated, thanks. ","Creater_id":39684,"Start_date":"2016-08-14 09:19:29","Question_id":229794,"Tags":["hypothesis-testing","mean","z-test","sample-mean"],"Answer_count":1,"Last_activity":"2016-08-14 12:18:53","Link":"http://stats.stackexchange.com/questions/229794/is-a-z-test-the-right-test-to-compare-the-mean-of-several-sub-groups-to-a-contr","Creator_reputation":53}
{"_id":{"$oid":"5837a57ea05283111e4d512f"},"View_count":42,"Display_name":"Eren","Question_score":1,"Question_content":"Both the GJR and the GARCH-specifications are used quite often in the finance literature. The GARCH is defined as: \\sigma^2_t = \\omega + \\alpha\\varepsilon^2_{t-1} + \\beta\\sigma^2_{t-1}and the GJR-GARCH reads as follows \\sigma^2_t = \\omega + (\\alpha+\\gamma \\mathbb{I}_{t-1})\\varepsilon^2_{t-1} + \\beta\\sigma^2_{t-1}where  is the indicator function: for  and   otherwise.According to research (Laurent et al. and Brownlees et al.) the GJR models generally perform better than the GARCH specification. Thus, including a leverage effect leads to enhanced forecasting performance.My question is: what are reasons for the GJR specification to perform better than the GARCH model. Also, it can occur that both models perform on par. What could be reasons for this observation of equal forecasting performance?Thank you very much.References:Brownlees, C. T., Engle, R. F., \u0026amp; Kelly, B. T. (2011). A practical guide to volatility forecasting through calm and storm. Available at SSRN 1502915.Laurent, S., Rombouts, J. V., \u0026amp; Violante, F. (2012). On the forecasting accuracy of multivariate GARCH models. Journal of Applied Econometrics, 27(6), 934-955.","Creater_id":98417,"Start_date":"2016-08-14 08:20:43","Question_id":229786,"Tags":["garch"],"Answer_count":1,"Last_activity":"2016-08-14 11:50:57","Link":"http://stats.stackexchange.com/questions/229786/garch-vs-gjr-garch","Creator_reputation":44}
{"_id":{"$oid":"5837a57ea05283111e4d5131"},"View_count":82,"Display_name":"hxd1011","Question_score":4,"Question_content":"I am getting different results (close but not exact the same) from R GLM and manual solving logistic regression optimization. Could anyone tell me where is the problem?BFGS does not converge?Numerical problem with finite precision?Thanks# logistic regression without interceptfit=glm(factor(vs) ~ hp+wt-1, mtcars, family=binomial())# manually write logistic loss and use BFGS to solvex=as.matrix(mtcars[,c(4,6)])y=ifelse(mtcars$vs==1,1,-1)lossLogistic \u0026lt;- function(w){  L=log(1+exp(-y*(x %*% w)))  return(sum(L))}opt=optim(c(1,1),lossLogistic, method=\"BFGS\")","Creater_id":113777,"Start_date":"2016-08-13 17:57:05","Question_id":229720,"Tags":["logistic","generalized-linear-model","optimization"],"Answer_count":1,"Last_activity":"2016-08-14 10:38:47","Link":"http://stats.stackexchange.com/questions/229720/r-using-glm-and-manual-solve-logistic-regression-have-different-close-but-not-e","Creator_reputation":4443}
{"_id":{"$oid":"5837a57ea05283111e4d5133"},"View_count":35,"Display_name":"Krombopulos Michael","Question_score":0,"Question_content":"Let´s say I have a set of 1500 variables and I want to train a neural network to classify subjects based on those 1500. My problem is I only have about 400 subjects to train the NN and only about 10 - 20 of those Variables are probably relevant. If I calculate an U-Test with p \u0026lt; 0.01 criteria about 35 variables show a significant difference. If I´m not mistaken about 15 of those should be result of random variance.Presumably complex and non linear relations between the relevant variables lead to the resulting dependent variable.My idea was to somehow make a rough preselection of variables before training the NN. But standard methods like PCA reduce the number to much.Do you experts have a good idea how to approach this kind of problem?","Creater_id":121161,"Start_date":"2016-08-14 04:51:44","Question_id":229758,"Tags":["neural-networks","feature-selection"],"Answer_count":0,"Last_activity":"2016-08-14 10:04:27","Link":"http://stats.stackexchange.com/questions/229758/variable-preselection","Creator_reputation":18}
{"_id":{"$oid":"5837a57ea05283111e4d5135"},"View_count":50,"Display_name":"WikiLogic","Question_score":1,"Question_content":"I posted this question on LessWrong.com where it was suggested i come here. I also posted there about an open source decision making web site I am working on called WikiLogic. The site has a 2 minute explanatory animation if you are interested. I wont repeat myself but the tl;dr is that it will follow the Wikipedia model of allowing everyone to collaborate on a giant connected database of arguments where previously established claims can be used as supporting evidence for new claims.The raw deduction element of it works fine and would be great in a perfect world where such a thing as absolute truths existed, however in reality we normally have to deal with claims that are just the most probable. My program allows opposing claims to be connected and then evidence to be gathered for each. The evidence will create a probability of it being correct and which ever is highest, gets marked as best answer. Principles such as Occams Razor are applied automatically as long list of claims used as evidence will be less likely as each claim will have its own likelihood which will dilute its strength.However, my only qualification in this area is my passion and I am hitting a wall with some basic questions. I am not sure if this is the correct place to get help with these. If not, please direct me somewhere else and I will remove the post.The arbitrarily chosen example claim I am working with is whether “Alexander the Great existed”. This has the useful properties of 1: an expected outcome (that he existed - although, perhaps my problem is that this is not the case!) and 2: it relies heavily on probability as there is little solid evidence.One popular claim is that coins were minted with his face on them. I want to use Bayes to find how likely a face appearing on a coin is for someone who existed. As I understand it, there should be 4 combinations:Existed; Had a coin mintedExisted; Did not have a coin mintedNo Existed; Had a coin mintedNo Existed; Did not have a coin mintedThe first issue is that there are infinite people who never existed and did not have a coin made. If I narrow it to historic figures who turned out not to exist and did not have a coin made it becomes possible but also becomes subjective as to whether someone actually thought they existed. For example, did people believe the Minotaur existed?Perhaps I should choose another filter instead of historic figure, like humans that existed. But picking and choosing the category is again so subjective. Someone may also argue that woman inequality back then was so great that the data should only look at men, as a woman’s chance of being portrayed on a coin was skewed in a way that isn’t applicable to men.I hope i have successfully communicated the problem i am grappling with and what i want to use it for. If not, please ask for clarifications. A friend in academia suggested that this touches on a problem with Bayes priors that has not been settled. If that is the case, is there any suggested resources for a novice with limited free time, to start to explore the issue? ","Creater_id":127658,"Start_date":"2016-08-14 08:07:10","Question_id":229784,"Tags":["bayesian"],"Answer_count":1,"Last_activity":"2016-08-14 10:00:06","Link":"http://stats.stackexchange.com/questions/229784/help-with-bayesian-priors","Creator_reputation":8}
{"_id":{"$oid":"5837a57ea05283111e4d5137"},"View_count":63,"Display_name":"Juraj","Question_score":0,"Question_content":"I performed bootstrap (10 000 samples) in boot package for R (multivariate model, binary logistic regression with Firth´s correction). One variable in the model has after bootstrap negative value in confidence interval, i.e. CI 95% = (-0,403, 1,103). Can someone explain please what does it mean and how to interpret it? The variable is birth weighth and the model should estimate predictors for mortality.  Thanks!","Creater_id":116404,"Start_date":"2016-08-14 07:35:22","Question_id":229782,"Tags":["bootstrap"],"Answer_count":1,"Last_activity":"2016-08-14 09:49:51","Link":"http://stats.stackexchange.com/questions/229782/negative-value-in-ci-after-bootstrap","Creator_reputation":13}
{"_id":{"$oid":"5837a57ea05283111e4d5139"},"View_count":42,"Display_name":"Dasha Murlina","Question_score":0,"Question_content":"I am trying to see what model performs best. I am considering the Hausman test to see which model is the most stable out of the three. However, I am not sure what exactly my null hypothesis should be when I test three models, and if I should compare RE against Pooled OLS and then RE against FE.Do you think that this is the correct way of testing?Also, is there another test or statistic that can be used in my case?","Creater_id":127642,"Start_date":"2016-08-14 09:17:21","Question_id":229793,"Tags":["regression","random-effects-model","fixed-effects-model","pooling","hausman"],"Answer_count":1,"Last_activity":"2016-08-14 09:35:30","Link":"http://stats.stackexchange.com/questions/229793/what-test-can-be-applied-to-fixed-effect-random-effect-and-pooled-ols-models","Creator_reputation":3}
{"_id":{"$oid":"5837a57ea05283111e4d513b"},"View_count":24,"Display_name":"bucky","Question_score":2,"Question_content":"I have a simple within subject design: I measure my variable (time point A), then do an intervention and measure the same variable afterwards once again (time point B) - my subjects are of course the same. I suggest that the variance decreases due to the intervention - Var(A)\u003eVar(B).My problem is, that I struggle to find an appropriate procedure to test if the change is significant or not. I can´t use the Levene-test or the F-Test, because they can only be used for independent variables (right?).So: Does anybody know a suitable test similar to the e.g. Levene-test for this kind of design? Or, can I simply use 2 confidence intervals?","Creater_id":127482,"Start_date":"2016-08-13 11:21:36","Question_id":229689,"Tags":["variance","repeated-measures","dependent"],"Answer_count":1,"Last_activity":"2016-08-14 08:44:44","Link":"http://stats.stackexchange.com/questions/229689/testing-for-changes-in-variance-following-an-intervention","Creator_reputation":13}
{"_id":{"$oid":"5837a57ea05283111e4d513d"},"View_count":3003,"Display_name":"user39538","Question_score":5,"Question_content":"I am looking to compare regression coefficients between two regression models. Each model has the same four independent variables: two predictors of interest (we'll call them A and B) and two control variables (C and D). The only difference between the two models is that they have different dependent variables: the first model is predicting DV1, while the second model is predicting DV2. All observations are from the same sample, so the regression coefficients are dependent. I believe that both A and B will more strongly predict DV1 than DV2. In other words, the regression coefficient for A predicting DV1 (controlling for B, C, and D) should be higher in magnitude than the regression coefficient for A predicting DV2 (controlling for B, C, and D). Similarly, the regression coefficient for B predicting DV1 (controlling for A, C, and D) should be higher in magnitude than the regression coefficient for B predicting DV2 (controlling for A, C, and D). Essentially, I want to test the difference between two dependent regression coefficients from two models that share all of the same IVs, but have different DVs. Is there a formal significance test I can use?","Creater_id":39538,"Start_date":"2014-02-04 14:30:24","Question_id":85445,"Tags":["regression"],"Answer_count":1,"Last_activity":"2016-08-14 08:43:46","Link":"http://stats.stackexchange.com/questions/85445/comparing-dependent-regression-coefficients-from-models-with-different-dependent","Creator_reputation":28}
{"_id":{"$oid":"5837a57ea05283111e4d513f"},"View_count":105,"Display_name":"Tom","Question_score":5,"Question_content":"The stochastic integral is defined asu_t = \\int_{t-1}^t e^{-\\kappa(t-s)}\\int_0^s e^{-c(s-r)} \\, dW(r) \\, ds.where  is a standard Brownian motion,  and  are both positive.I know this integral can be viewed asu_t = \\int_{t-1}^t e^{-\\kappa(t-s)}J_c(s) \\, ds,where , is an Ornstein-Uhlenbeck process. But how do we handle this double integral and use Ito's isometry to get the variance of it? Further, does this integral admit a Wold representation, that is,u_t = \\sum_{j = 0}^\\infty F_j \\varepsilon_{t-j},where .Here is my calculations, which may be wrong, I am not so sure...\\begin{align}Var(u_t) \u0026amp;= E(u_t^2) = E\\left(\\int_{t-1}^{t} \\int_{0}^{s}e^{-\\kappa(t-s)-c(s-r)}dW(r) ds\\right)^2 \\\\\u0026amp;= E\\left(\\int_{0}^{t} \\int_{r}^{t}e^{-\\kappa(t-s)-c(s-r)}ds dW(r) \\right)^2 \\\\\u0026amp;= \\int_{0}^{t} \\left(\\int_{r}^{t}e^{-\\kappa(t-s)-c(s-r)}ds\\right)^2 dr \\\\\u0026amp;= \\int_{0}^{t} e^{-2\\kappa t + 2cr} \\left(\\int_{r}^{t}e^{(\\kappa-c)s}ds\\right)^2 dr \\\\\u0026amp;= \\dfrac{1}{(\\kappa-c)^2}\\int_{0}^{t} e^{-2\\kappa t + 2cr} \\left(e^{(\\kappa-c)t}- e^{(\\kappa-c)r} \\right)^2 dr \\\\\u0026amp;= \\dfrac{1}{(\\kappa-c)^2}\\left(\\int_{0}^{t} e^{-2c(t-r)} dr + \\int_{0}^{t} e^{-2\\kappa(t-r)} dr - 2\\int_{0}^{t} e^{-(\\kappa+c)(t-r)} dr\\right)\\\\\u0026amp;= \\dfrac{1}{(\\kappa-c)^2}\\left(\\dfrac{1-e^{-2ct}}{2c} + \\dfrac{1-e^{-2\\kappa t}}{2\\kappa} - 2\\dfrac{1-e^{-(\\kappa+c)t}}{\\kappa+c}\\right)\\end{align}which indicates  exhibits heteroskedasiticity. But still, I don't know if it is weakly stationary.","Creater_id":119063,"Start_date":"2016-06-06 09:50:14","Question_id":217595,"Tags":["time-series","stochastic-processes","continuous-data"],"Answer_count":1,"Last_activity":"2016-08-14 07:24:21","Link":"http://stats.stackexchange.com/questions/217595/whats-the-variance-of-the-following-stochastic-integral-and-is-it-weakly-statio","Creator_reputation":28}
{"_id":{"$oid":"5837a57ea05283111e4d5141"},"View_count":43,"Display_name":"Deez Nuts","Question_score":1,"Question_content":"(Sourced from a paper)I don't understand why 'the inverse of the mortality m1 is the mean duration d of the disease'.","Creater_id":127627,"Start_date":"2016-08-14 00:54:08","Question_id":229739,"Tags":["epidemiology"],"Answer_count":1,"Last_activity":"2016-08-14 07:19:59","Link":"http://stats.stackexchange.com/questions/229739/derivation-of-the-relationship-between-prevalence-incidence-and-duration-of-a-d","Creator_reputation":6}
{"_id":{"$oid":"5837a57fa05283111e4d5156"},"View_count":84,"Display_name":"ahra","Question_score":-4,"Question_content":"If for a normal distribution  and if we have  how can then  ?","Creater_id":125486,"Start_date":"2016-08-14 05:22:28","Question_id":229764,"Tags":["self-study","distributions","normal-distribution","mathematical-statistics","variance"],"Answer_count":2,"Last_activity":"2016-08-14 06:45:29","Link":"http://stats.stackexchange.com/questions/229764/i-need-help-to-show-that-e-sum-x-sum-ex","Creator_reputation":149}
{"_id":{"$oid":"5837a57fa05283111e4d5164"},"View_count":131,"Display_name":"Gary Grey","Question_score":0,"Question_content":"This is how my training looks like(500, 300, 50)(500,)Minibatch Loss = 7.722980, Training Accuracy= 0.18400(500, 300, 50)(500,)Minibatch Loss = 20.557695, Training Accuracy= 0.35600(500, 300, 50)(500,)Minibatch Loss = 32.925579, Training Accuracy= 0.22800(500, 300, 50)(500,)Minibatch Loss = 34.841656, Training Accuracy= 0.22400(500, 300, 50)(500,)Minibatch Loss = 38.137703, Training Accuracy= 0.22400(500, 300, 50)(500,)Minibatch Loss = 22.291409, Training Accuracy= 0.22400(500, 300, 50)(500,)Minibatch Loss = 26.780132, Training Accuracy= 0.34800(500, 300, 50)(500,)Minibatch Loss = 27.132868, Training Accuracy= 0.34800(500, 300, 50)(500,)Minibatch Loss = 21.303114, Training Accuracy= 0.35800(500, 300, 50)(500,)Minibatch Loss = 20.854801, Training Accuracy= 0.31600(500, 300, 50)(500,)Minibatch Loss = 24.449608, Training Accuracy= 0.23000(500, 300, 50)(500,)Minibatch Loss = 29.198355, Training Accuracy= 0.19600(500, 300, 50)(500,)Minibatch Loss = 20.845459, Training Accuracy= 0.20000(500, 300, 50)(500,)Minibatch Loss = 17.757305, Training Accuracy= 0.23600(500, 300, 50)(500,)Minibatch Loss = 15.250696, Training Accuracy= 0.37000(500, 300, 50)(500,)Minibatch Loss = 15.362234, Training Accuracy= 0.37200(500, 300, 50)(500,)Minibatch Loss = 14.827072, Training Accuracy= 0.35000(500, 300, 50)(500,)Minibatch Loss = 20.541281, Training Accuracy= 0.31800(500, 300, 50)(500,)Minibatch Loss = 22.777840, Training Accuracy= 0.22400(500, 300, 50)(500,)Minibatch Loss = 15.121683, Training Accuracy= 0.22400(500, 300, 50)(500,)Minibatch Loss = 8.579925, Training Accuracy= 0.33400(500, 300, 50)(500,)Minibatch Loss = 9.990248, Training Accuracy= 0.31400Sometimes training loss increases and so does accuracy and I'm training my neural network with same single batch of size 500. I'm passing this same single batch every time and this is how my results look like. I think as I'm passing same single batch every time, loss should go down and training accuracy should increase. But it's not what is happening. What could go wrong?Here is my code:from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport tensorflow as tfimport numpy as npimport mathimport osimport nltkbatch = 500start = 0end = batchlearning_rate = 0.2num_classes = 8path = \"/home/indy/Downloads/aclImdb/train/pos\"time_steps = 300embedding = 50step = 10def get_embedding():    gfile_path = os.path.join(\"/home/indy/Downloads/glove.6B\", \"glove.6B.50d.txt\")    f = open(gfile_path,'r')    embeddings = {}    for line in f:        sp_value = line.split()        word = sp_value[0]        embedding = [float(value) for value in sp_value[1:]]        assert len(embedding) == 50        embeddings[word] = embedding    return embeddingsebd = get_embedding()def get_y(file_name):    y_value = file_name.split('_')    y_value = y_value[1].split('.')    if y_value[0] == '1':       return 0    elif y_value[0] == '2':         return 1    elif y_value[0] == '3':          return 2    elif y_value[0] == '4':          return 3    elif y_value[0] == '7':          return 4    elif y_value[0] == '8':          return 5    elif y_value[0] == '9':          return 6    elif y_value[0] == '10':          return 7 def get_x(path,file_name):    file_path = os.path.join(path,file_name)    x_value = open(file_path,'r')    for line in x_value:        x_value = line.replace(\"\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;\",\"\")         x_value = x_value.lower()    x_value = nltk.word_tokenize(x_value.decode('utf-8'))    padding = 300 - len(x_value)    if padding \u0026gt; 0:       p_value = ['pad' for i in range(padding)]       x_value = np.concatenate((x_value,p_value))    if padding \u0026lt; 0:       x_value = x_value[:300]    for i in x_value:        if ebd.get(i) == None:           ebd[i] = [float(np.random.normal(0.0,1.0)) for j in range(50)]    x_value = [ebd[value] for value in x_value]    assert len(x_value) == 300    return x_valuedef  batch_f(path):     directory = os.listdir(path)     y = [get_y(directory[i]) for i in range(len(directory))]     x = [get_x(path,directory[i]) for i in range(len(directory))]         return x , yx , y = batch_f(path)   def batch_size(start,end):    if start == 12500:       start = 0       end = 500    return x[:200] , y[:200]X = tf.placeholder(tf.float32, [200,time_steps,embedding])Y = tf.placeholder(tf.int32, [200])def build_nlp_model(x, _units,num_classes,num_of_filters):     x = tf.expand_dims(x,3)     filter_shape = [1, embedding, 1, num_of_filters]     conv_weights = tf.Variable(tf.truncated_normal(filter_shape, stddev = 1.0))     conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))     conv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")     relu = tf.nn.relu(conv + conv_biases)     pooling = tf.nn.max_pool(relu, [1, 1, 1, 1], strides=[1,1,1,1], padding=\"VALID\")     outputs_fed_lstm = pooling     x = tf.squeeze(outputs_fed_lstm)          x = tf.transpose(x, [1, 0, 2])     x = tf.reshape(x, [-1, num_of_filters])     x = tf.split(0, time_steps, x)     lstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)     # multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)     outputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)          weights = tf.Variable(tf.random_normal([_units,num_classes]))     biases  = tf.Variable(tf.random_normal([num_classes]))     logits = tf.matmul(outputs[-1], weights) + biases     return logitslogits = build_nlp_model(X,500,num_classes,1500)c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)loss = tf.reduce_mean(c_loss)global_step = tf.Variable(0, name=\"global_step\", trainable=False)decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)optimizer= tf.train.AdamOptimizer(decayed_learning_rate)minimize_loss = optimizer.minimize(loss, global_step=global_step)correct_predict = tf.nn.in_top_k(logits, Y, 1)accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))init = tf.initialize_all_variables()with tf.Session() as sess:     sess.run(init)     for i in range(2500):         x , y = batch_size(start,end)         print (np.array(x).shape)         print(np.array(y).shape)            sess.run(minimize_loss,feed_dict={X : x, Y : y})         step1 = sess.run(global_step)         cost = sess.run(loss,feed_dict = {X: x,Y: y})         accu = sess.run(accuracy,feed_dict = {X: x, Y: y})         print (\"Minibatch Loss = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))     print (\"Optimization Finished\")  ","Creater_id":124140,"Start_date":"2016-08-12 05:01:24","Question_id":229767,"Tags":["python","tensorflow","deep-learning","conv-neural-network"],"Answer_count":2,"Last_activity":"2016-08-14 06:25:25","Link":"http://stats.stackexchange.com/questions/229767/my-training-loss-is-increasing-and-my-training-accuracy-is-also-increasing-is-i","Creator_reputation":28}
{"_id":{"$oid":"5837a57fa05283111e4d5172"},"View_count":1875,"Display_name":"ankushg","Question_score":16,"Question_content":"I'm a high school student and I'm working on a computer programming project, but I don't have a lot of experience in statistics and modeling data beyond a high school statistics course so I'm kinda confused.Basically, I have a reasonably large list (assume it's large enough to meet the assumptions for any statistical tests or measures) of times that someone decided to print a document. Based on this list, I would like to construct a statistical model of some sort that will predict the most likely time for the next print job given all of the previous event times.I've already read this, but the responses don't exactly help out with what I have in mind for my project. I did some additional research and found that a Hidden Markov Model would likely allow me to do so accurately, but I can't find a link on how to generate a Hidden Markov Model using just a list of times. I also found that using a Kalman filter on the list may be useful but basically, I'd like to get some more information about it from someone who's actually used them and knows their limitations and requirements before just trying something and hoping it works.Thanks a bunch!","Creater_id":6592,"Start_date":"2011-09-30 13:26:34","Question_id":16302,"Tags":["probability","modeling","data-mining","predictive-models"],"Answer_count":2,"Last_activity":"2016-08-14 06:06:21","Link":"http://stats.stackexchange.com/questions/16302/how-to-predict-when-the-next-event-occurs-based-on-times-of-previous-events","Creator_reputation":181}
{"_id":{"$oid":"5837a57fa05283111e4d5180"},"View_count":250,"Display_name":"Rodrigo Remedio","Question_score":3,"Question_content":"Simply put, I'd like to know how the plm package in R calculates the residuals of a random-effect regression.I ask this because i'm getting some \"weird\" outputs. Let-me reproduce them here using the Grunfeld data for four firms, like Gujarati in his Basic Econometrics do:require(plm)require(foreign)Grunfeld\u0026lt;-read.dta(\"Data.dta\")Grunfeld\u0026lt;-pdata.frame(Grunfeld,index = c(\"id\",\"t\"))grun.re \u0026lt;- plm(Y~X2+X3,data=Grunfeld,model=\"random\",index=\"id\")#Means by idX2M\u0026lt;-tapply(Grunfeldid,FUN = mean)X3M\u0026lt;-tapply(Grunfeldid,FUN = mean)YM\u0026lt;-tapply(Grunfeldid,FUN = mean)#Random Effect: Fit the model and the calculate residuals \"by hand\"fit.re\u0026lt;-grun.recoefficients[2]*Grunfeldcoefficients[3]*GrunfeldY-fit.re)#Random Effect:head(cbind(grun.reresiduals   alphaRE       eRE        uRE calcResid.re1         99.395803 -169.9282 116.23154  -53.69666    -53.696662         18.023715 -169.9282  34.85946 -135.06874   -135.068743        -39.256625 -169.9282 -22.42089 -192.34909   -192.349084         -2.857048 -169.9282  13.97869 -155.94951   -155.949515        -28.334107 -169.9282 -11.49837 -181.42656   -181.426566          6.475226 -169.9282  23.31096 -146.61723   -146.61723In this table, uRE is the overall residual of the regression provided by Stata (which is identical to Gretl's) and calcResid.re is the manually calculated residuals from the fitted model. So, Stata, Gretl and I did the same. But what plm package do?We can se that calcResid.re and uRE are equals. But the residuals provided by the plm estimation (grun.re$residuals) completely differs.Here is a link to the dataset and results: https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0","Creater_id":83090,"Start_date":"2015-11-23 12:32:18","Question_id":183206,"Tags":["r","stata","panel-data","random-effects-model","plm"],"Answer_count":2,"Last_activity":"2016-08-14 05:18:09","Link":"http://stats.stackexchange.com/questions/183206/package-plm-random-effect-residuals","Creator_reputation":33}
{"_id":{"$oid":"5837a57fa05283111e4d518e"},"View_count":69,"Display_name":"john","Question_score":1,"Question_content":"I have 4 years electrical load data. I split the data into 3 years (75%) training data, 1 year for testing (25%). Also I have the temperature data for each day during the previous period. (The link to the dataset: here.) I want to make use of the temperature data to enhance the forecasting using argument xreg in arima function. Here is my code:mydata1\u0026lt;-read.csv(\"1st pape/kaggle_data.csv\");mydata\u0026lt;-ts(mydata1[,2],start = c(2004),frequency = 365)#split the data into trainData and test datatrainData = window(mydata, end=c(2007))testData = window(mydata, start=c(2007))temp\u0026lt;-ts(mydata1[,3],start = c(2004),frequency = 365)#split the temperature into trainData and test datatrainReg = window(temp, end=c(2007))testReg = window(temp, start=c(2007))Apply ARIMA model without using xreg:mod_arima \u0026lt;- auto.arima(trainData, ic='aicc', stepwise=FALSE)summary(mod_arima)Series: trainData ARIMA(1,0,3) with non-zero mean Coefficients:         ar1      ma1      ma2      ma3  intercept      0.9642  -0.2098  -0.2157  -0.1693  24008.122s.e.  0.0110   0.0322   0.0330   0.0325   1018.007sigma^2 estimated as 9318421:  log likelihood=-10347.38AIC=20706.75   AICc=20706.83   BIC=20736.75Training set error measures:                   ME     RMSE      MAE       MPE     MAPE      MASETraining set 6.102332 3045.638 2293.946 -1.519484 9.625694 0.5151126                    ACF1Training set 0.004483007plot(forecast(mod_arima)); lines(testData , col=\"red\", start= c(2007,1,1)); legend(\"topleft\", lty=1,col=c(4,2),legend=c(\"forecasted data\",\"real data\"))y \u0026lt;- msts(trainData, c(7,365)) # multiseasonal tsx \u0026lt;- msts(trainReg, c(7,365)) # multiseasonal tsfit \u0026lt;- auto.arima(y, xreg=(fourier(y, K=c(3,30))))fit_f \u0026lt;- forecast(fit, xreg= fourier(y, K=c(3,30), 365), 365)plot(fit_f)the red line is the actual data, while the blue is the foretasted data. The left plot is appeared before using fourier function, while the right after using it. Apply ARIMA model using xreg:mod_arima2 \u0026lt;- auto.arima(trainData ,xreg = trainReg, ic='aicc', stepwise=FALSE)summary(mod_arima2)Series: trainData ARIMA(1,0,3) with non-zero mean Coefficients:         ar1      ma1      ma2      ma3  intercept  trainReg      0.9709  -0.2403  -0.2108  -0.1609  29984.188  -88.3976s.e.  0.0094   0.0320   0.0330   0.0321   1468.108   13.1966sigma^2 estimated as 8955023:  log likelihood=-10325.13AIC=20664.26   AICc=20664.36   BIC=20699.26Training set error measures:                   ME     RMSE      MAE       MPE     MAPE      MASETraining set 6.030471 2984.292 2267.803 -1.464553 9.529988 0.5092422                    ACF1Training set 0.005526977plot(forecast(mod_arima2,xreg = testReg)); lines(testData , col=\"red\", start= c(2007,1,1)); legend(\"topleft\", lty=1,col=c(4,2), legend=c(\"forecasted data\",\"real data\"))l = (fourier(y, K=c(3,30)))z = cbind(l,x)fit2 \u0026lt;- auto.arima(y, xreg=z)fit_f2 \u0026lt;- forecast(fit, xreg= z, 365)plot(fit_f2)Questions:Did I use xreg correctly?If yes, why is the summary the same without using xreg?Why are the forecasts far away from the real data?","Creater_id":110051,"Start_date":"2016-08-13 18:01:30","Question_id":229721,"Tags":["r","time-series","forecasting","arima"],"Answer_count":0,"Last_activity":"2016-08-14 05:13:44","Link":"http://stats.stackexchange.com/questions/229721/poor-electrical-load-forecasts-from-auto-arima-why","Creator_reputation":106}
{"_id":{"$oid":"5837a57fa05283111e4d5190"},"View_count":47,"Display_name":"Tanya","Question_score":0,"Question_content":"What causes a low explained variance in a PCA?  I'm comparing three wine regions and have significantly higher samples of one of them.  Will this cause the low explained variance \u0026lt;50%?","Creater_id":127631,"Start_date":"2016-08-14 01:32:02","Question_id":229741,"Tags":["pca"],"Answer_count":2,"Last_activity":"2016-08-14 05:09:27","Link":"http://stats.stackexchange.com/questions/229741/pca-explained-variance-low","Creator_reputation":1}
{"_id":{"$oid":"5837a57fa05283111e4d519e"},"View_count":64,"Display_name":"Antoni Parellada","Question_score":1,"Question_content":"I am trying to duplicate a Wilcoxon signed-rank test example in this Wikipedia post using R.The data is as follows:after = c(125, 115, 130, 140, 140, 115, 140, 125, 140, 135)before = c(110, 122, 125, 120, 140, 124, 123, 137, 135, 145)sgn = sign(after-before)abs = abs(after - before)d = data.frame(after,before,sgn,abs)dmulti = drank(W=abs(sum(dbefore,dbefore and d$afterV = 18, p-value = 0.5936alternative hypothesis: true location shift is not equal to 0This (V=18) is different from the 9 value in the Wikipedia post.What am I missing?","Creater_id":67822,"Start_date":"2016-08-11 14:06:15","Question_id":229760,"Tags":["r"],"Answer_count":1,"Last_activity":"2016-08-14 05:03:27","Link":"http://stats.stackexchange.com/questions/229760/wilcoxon-signed-rank-test-in-r","Creator_reputation":7682}
{"_id":{"$oid":"5837a57fa05283111e4d51ab"},"View_count":8,"Display_name":"jason","Question_score":0,"Question_content":"I am trying to model a system using a simple quadratic function.I have theoretic reasons why this is the appropriate model of the physiological process. Assume the model is: X --\u0026gt; F(X) --\u0026gt; YI assume that Y = C + B*X + A*X*XLets suppose I have two sets of data collected from this system which have different ranges of X.One has X values from 10 to 90 and the other has X values from 30 to 80.I would like to estimate A, B and C so that they model the underlying system and the estimates are not specific to one data set or the other.How can I do this?I am testing by using a full dataset and fitting the quadratic, then using a subset of data and fitting the quadratic again and getting different parameter estimates of A, B and C. Is there some method (some orthogonalization/projection of X) where A, B and C are the same regardless of whether I use dataset one or two?","Creater_id":127406,"Start_date":"2016-08-11 14:06:58","Question_id":229759,"Tags":["r","modeling"],"Answer_count":0,"Last_activity":"2016-08-14 05:02:40","Link":"http://stats.stackexchange.com/questions/229759/model-fitting-to-part-of-the-data","Creator_reputation":31}
{"_id":{"$oid":"5837a57fa05283111e4d51ad"},"View_count":24,"Display_name":"beldaz","Question_score":0,"Question_content":"Sorry, my stats is rusty and I'm failing to find a quick answer to what is likely to be a very basic question.If I take n measurements of a property that has an inherent mean mu and standard deviation s, I would estimate mu from the sample mean, and quantify the uncertainty in this estimate with the standard error. But what about the standard deviation? (I guess this is sort of a population s.d., though of a potentially infinite population) Presumably I'd estimate it from the sample standard deviation, but what is the right way to quantify its uncertainty? Clearly a value based upon 5 values is less reliable than one using 500.","Creater_id":122888,"Start_date":"2016-08-14 03:20:38","Question_id":229752,"Tags":["standard-deviation","error"],"Answer_count":1,"Last_activity":"2016-08-14 04:20:48","Link":"http://stats.stackexchange.com/questions/229752/estimating-the-error-in-the-standard-deviation","Creator_reputation":103}
{"_id":{"$oid":"5837a57fa05283111e4d51ba"},"View_count":58,"Display_name":"blu","Question_score":3,"Question_content":"Let's say I'm comparing 60 different model hyperparameter value combinations using 10-fold cross-validation.  It's tempting to simply select the hyperparameter combination whose mean accuracy is highest across the folds.  However, should one make use of the standard deviation of the accuracies when deciding on the best hyperparameter combination?  If so, any particularly rule of thumb (e.g. go with the hyperparameter combination that has the highest mean accuracy amongst the better half in terms of standard deviation)","Creater_id":108829,"Start_date":"2016-08-11 18:32:35","Question_id":229450,"Tags":["cross-validation","standard-deviation","model-evaluation"],"Answer_count":2,"Last_activity":"2016-08-14 03:55:24","Link":"http://stats.stackexchange.com/questions/229450/is-there-a-rule-of-thumb-for-using-standard-deviation-of-k-fold-cross-validation","Creator_reputation":48}
{"_id":{"$oid":"5837a57fa05283111e4d51c8"},"View_count":6864,"Display_name":"user3378649","Question_score":7,"Question_content":"I am using confusion matrix to check the performance of my classifier. I am using Scikit-Learn, I am little bit confused. How can I interpret the result from  from sklearn.metrics import confusion_matrix\u0026gt;\u0026gt;\u0026gt; y_true = [2, 0, 2, 2, 0, 1]\u0026gt;\u0026gt;\u0026gt; y_pred = [0, 0, 2, 2, 0, 2]\u0026gt;\u0026gt;\u0026gt; confusion_matrix(y_true, y_pred)array([[2, 0, 0],       [0, 0, 1],       [1, 0, 2]])How can I take the decision whether this predicted values are good or no. ","Creater_id":43653,"Start_date":"2014-04-25 10:00:53","Question_id":95209,"Tags":["predictive-models","prediction","confusion-matrix"],"Answer_count":2,"Last_activity":"2016-08-14 03:43:23","Link":"http://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix","Creator_reputation":317}
{"_id":{"$oid":"5837a57fa05283111e4d51d6"},"View_count":24,"Display_name":"Alex","Question_score":0,"Question_content":"Given a cross validated generalised linear model with about 5-10 coefficients across 10 folds, is it appropriate to plot all the coefficients on one graph using multiple line charts? That is:Have the folds numbered 1 to 10 on the x-axis.Plot a line chart for each model covariate, so that the y-value is the model coefficient for that covariate on the respective fold.I would like to do this as it shows:How the model coefficients compare to each other in scale.The flatness of the line shows the stability of the coefficients to some extent.What I do not like is:There is no linear relationship between the folds so a line chart would not be usually suitable; in other cases like this a scatterplot or bar plot would be preferred.The horizontal width of the plot artificially affects the flatness of the line.Does anyone have any comments on this methodology or can recommend other ways of showing the cross validated coefficients?","Creater_id":22199,"Start_date":"2016-08-13 20:04:25","Question_id":229729,"Tags":["generalized-linear-model","data-visualization","cross-validation"],"Answer_count":1,"Last_activity":"2016-08-14 03:30:25","Link":"http://stats.stackexchange.com/questions/229729/plotting-cross-validated-regression-coefficients-across-folds","Creator_reputation":727}
{"_id":{"$oid":"5837a57fa05283111e4d51e3"},"View_count":38,"Display_name":"corey979","Question_score":0,"Question_content":"Having two models to describe the data, one computes for each of themAIC=2k-2L,where  is the max log-likelihood and  the number of parameters in the model. Then we choose the model with smaller  and evaluate its strength against the other model via .Is it really correct to use the same formulation of  for uni- and multivariate distributions? E.g., in a 1D setting one can compare a Gaussian and a mixture of 2 Gaussians, and in 2D the models can be a bivariate Gaussian and a mixture of two bivariate Gaussians. Why should the above formula for  be the same regardless of the dimensionality of the data? Shouldn't there be some way (e.g., an additional term) dependent on the number of dimensions?","Creater_id":72352,"Start_date":"2016-08-12 17:54:09","Question_id":229629,"Tags":["distributions","multivariate-analysis","model-selection","aic"],"Answer_count":1,"Last_activity":"2016-08-14 02:50:56","Link":"http://stats.stackexchange.com/questions/229629/why-the-formula-for-aic-is-the-same-for-uni-and-multivariate-distributions-mode","Creator_reputation":313}
{"_id":{"$oid":"5837a57fa05283111e4d51f0"},"View_count":21,"Display_name":"tucker.crowe","Question_score":1,"Question_content":"I recently estimated some OLS regressions with daily returns scaled by 100 as dependent variable (thus in percentage points). As I learned (and empirically confirmed), this scaling only scales coefficients by 100, but has no impact on statistical significance.I used the same, scaled returns to estimate a simple EGARCH(1,1) model and found that the scaling directly impacts the coefficient for the unconditional variance omega and the significance of all other coefficients. The results are thus vastly different when using scaled data, especially when it comes to the interpretation of significance.I used the SP500 data from the \"rugarch\" package in R and the ugarchfit function to produce following example:Here with normal returns:        Estimate  Std. Error    t value Pr(\u0026gt;|t|)  mu      0.000670    0.000211     3.17900 0.001478               ar1    -0.679036    0.017029   -39.87517 0.000000               ma1     0.701977    0.016065    43.69611 0.000000               omega  -0.269569    0.005428   -49.65921 0.000000               alpha1 -0.197466    0.025594    -7.71537 0.000000               alpha2  0.129236    0.005627    22.96744 0.000000               beta1   0.970782    0.000080 12106.51240 0.000000               gamma1 -0.009223    0.068496    -0.13465 0.892888               gamma2  0.124195    0.055994     2.21802 0.026553               shape   4.670759    0.848486     5.50481 0.000000                           LogLikelihood : 3204.702 And here with returns scaled by 100:        Estimate  Std. Error    t value Pr(\u0026gt;|t|)  mu      0.067050    0.021114   3.175639 0.001495  ar1    -0.679038    0.016626 -40.840967 0.000000  ma1     0.701978    0.016644  42.176242 0.000000  omega  -0.000460    0.006190  -0.074301 0.940771  alpha1 -0.197462    0.060240  -3.277926 0.001046  alpha2  0.129237    0.061149   2.113468 0.034561  beta1   0.970786    0.003998 242.815325 0.000000  gamma1 -0.009222    0.073792  -0.124977 0.900542  gamma2  0.124189    0.075641   1.641818 0.100628  shape   4.670628    0.881931   5.295909 0.000000  LogLikelihood : -1400.468 Look especially at omega and the significance of gamma2!  Does anybody know why this is the case?","Creater_id":126990,"Start_date":"2016-08-08 06:06:12","Question_id":228781,"Tags":["r","data-transformation","garch"],"Answer_count":1,"Last_activity":"2016-08-14 02:33:56","Link":"http://stats.stackexchange.com/questions/228781/why-is-egarch-sensitive-to-scale","Creator_reputation":6}
{"_id":{"$oid":"5837a57fa05283111e4d51fd"},"View_count":1029,"Display_name":"Masudur","Question_score":3,"Question_content":"I built a confusion matrix of three class. like, (a,b,c are the class)-   a   b   c \u0026lt;= predicteda   20  5   0b   7  18   0c   0   0  20Now I want to calculate precision, recall from this confusion matrix. In order to do that I need to find out true positive, true negative, false positive, and false negative.How can I find out the values of true positive, true negative, false positive, and false negative?","Creater_id":97256,"Start_date":"2015-12-21 00:59:24","Question_id":187724,"Tags":["confusion-matrix"],"Answer_count":2,"Last_activity":"2016-08-14 02:18:20","Link":"http://stats.stackexchange.com/questions/187724/how-to-find-true-positive-true-negative-false-positive-false-negative-from-a","Creator_reputation":36}
{"_id":{"$oid":"5837a57fa05283111e4d520a"},"View_count":87,"Display_name":"Nickel","Question_score":0,"Question_content":"I have one dimensional data. All data points are larger than 0. The median and mean are about 10 and 25 respectively. The distribution appears to be lognorm but with really high frequency around the median and fat tail, so lognorm does not fit well. Then I am thinking to use Kernel Density Estimation to describe the data. I tried different ways to find the best bandwidth. (Reference: https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/)R (reference rules)bw.SJ(data) bw.nrd(data)bw.nrd0(data)bw.ucv(data)All results are too small (smaller than 0.2) and the graph shows too many bumps, which makes it difficult to analyze.Python sklearn (cross validation) grid = GridSearchCV(KernelDensity(), {'bandwidth': np.linspace(0.1, 1.0, 30)}, cv=20)I ended up testing values of bandwidths from 0.01 to 50 and the best one was 20. Since 20 is too large, the graph is almost flat and does not fit the data at all.Do you have any ideas why these methods do not work well with my data? Could you tell me other methods to find better bandwidths?","Creater_id":95447,"Start_date":"2016-08-14 01:42:19","Question_id":229743,"Tags":["machine-learning","python","kernel-smoothing","smoothing"],"Answer_count":1,"Last_activity":"2016-08-14 02:12:28","Link":"http://stats.stackexchange.com/questions/229743/methods-to-find-the-best-bandwidth-for-kernel-density-estimation","Creator_reputation":15}
{"_id":{"$oid":"5837a57fa05283111e4d5217"},"View_count":542,"Display_name":"Sibbs Gambling","Question_score":4,"Question_content":"I have a set of objects . I have calculated the pairwise distances of all possible pairs. The distances are stored in a  matrix , with  being the distance between  and . Then it is natural to see  is a symmetric matrix.Now I wish to perform unsupervised clustering to these objects. After some searching, I find Spectral Clustering may be a good candidate, since it deals with such pairwise-distance cases.However, after carefully reading its description, I find it unsuitable in my case, as it requires the number of clusters as the input. Before clustering, I don't know the number of clusters. It has to be figured out by the algorithm while performing the clustering, like DBSCAN.Considering these, please suggest me some clustering methods that fit my case, whereThe pairwise distances are all available.The number of clusters is unknown.","Creater_id":26881,"Start_date":"2013-09-19 22:54:40","Question_id":70568,"Tags":["machine-learning","clustering"],"Answer_count":1,"Last_activity":"2016-08-14 01:20:39","Link":"http://stats.stackexchange.com/questions/70568/clustering-given-pairwise-distances-with-unknown-cluster-number","Creator_reputation":443}
{"_id":{"$oid":"5837a57fa05283111e4d5224"},"View_count":34,"Display_name":"Jaco","Question_score":0,"Question_content":"In R:qnorm(.075, mean=11,sd=8.58) == -1.35qnorm(.16, mean=11,sd=8.58) == 2.48It's also strange to me that the first line is negative, but I assume that it's a technical issue.Shouldn't the area under the pdf equal to .85 have a larger interval than the area equal to .68?","Creater_id":113507,"Start_date":"2016-08-13 19:31:17","Question_id":229726,"Tags":["r","confidence-interval","quantiles"],"Answer_count":2,"Last_activity":"2016-08-14 00:59:53","Link":"http://stats.stackexchange.com/questions/229726/why-are-these-quantile-values-increasing-when-the-confidence-percentage-is-incr","Creator_reputation":8}
{"_id":{"$oid":"5837a57fa05283111e4d5232"},"View_count":67,"Display_name":"user3282777","Question_score":2,"Question_content":"I was experimenting with clustering using kmeans on a large data set having mostly numerical variables (How Americans spend their time) in my class. A few of them (like education, marital status etc) that were categorical were transformed to dummy variables. As numerical variables (such as TV time, time spent with children, no of children etc) had varying value-range. It was necessary to scale and transform them.A question has arisen whether besides scaling should we also try to make skewed data symmetric by using, BoxCox, transformations. In, R, caret package has a nice and easy pre-processing facility and in one command can carry out all transformations and scaling.My question is: As the BoxCox transformation is a power-transformation, will it not change the shape of data distribution and we may get a different set of clusters than were originally existing in the dataset? While linear scaling of data may be OK, I am little skeptical about BoxCox transformations.I will be grateful for an answer.","Creater_id":78454,"Start_date":"2016-08-12 17:36:49","Question_id":229627,"Tags":["clustering","k-means"],"Answer_count":1,"Last_activity":"2016-08-14 00:37:44","Link":"http://stats.stackexchange.com/questions/229627/can-i-boxcox-transform-variables-before-k-means-clustering","Creator_reputation":121}
{"_id":{"$oid":"5837a57fa05283111e4d523f"},"View_count":165,"Display_name":"Pinocchio","Question_score":6,"Question_content":"I wanted to train a network with non-linearities that suffer from the vanishing (or exploding gradient problem though mainly vanishing). I know that the (current) standard way is to use batch normalization 1 [BN]1 or simply abandon the non-linearity and use ReLu Rectifier/ReLu units. I wanted two things:Stick with my non-linearity, so I don't want to abandon it and use the ReLu (i.e. no ReLu's allowed!). Re-parametrising the non-linearity is ok, say putting a multiplicative in front of it as in  for example.Ideally, I did not want to rely too much batch normalization (or at least if its used, it has to be used in a novel way other than how it was used in the original paper or generalize to many non-linearities). One of the reasons I wanted to avoid Batch Normalize is because it seems to only work for specific non-linearities. For example, for sigmoids, tanh but its unclear how they'd work for other non-linearities, say gaussians.The reason I have these constraints is because I'd like to deal with the problem of vanishing gradient or exploding gradients by taling the problem directly rather than hacking a solution that works only for specific non-linearities or just avoiding the problem by shoving in a ReLu.I was wondering, with those two constraints, what are alternative ways to deal with the vanishing gradient problem? (another non-linearity in consideration would be RBF gaussian kernel with euclidean norm pre-activation, sigmoid, tanh, etc)The possible (vague) ideas I had in mind would be:Have good initialization so that the saturating non-linearities don't start already saturated (saturated non-linearities result in gradients close to zero).For RBF, similarly, good init might be important because gaussians mostly have a large value close to 0 (i.e. when filters are similar to its activation or data). Thus, having them too big or too small has a similar vanishing gradient issues.I don't really know if this is too constraining but it would be nice if there was a different way to use batch normalization other than its traditional suggestion in the original paper (or maybe some BN idea that generalizes to a bigger set of non-linearities, currently it seems most of the research is to show it works for sigmoids as far as I know).Another idea could be to instead of having non-linearity  we have  where . If , then it means that the non-linearities are not multiplied backwards multiple times for each layer, so as to avoid to be \"vanished\" for earlier layers. It might make the learning rule unstable, so maybe some regularizer might be a good idea.An optimizer that intrinsically deals with the vanishing gradient (or at least updating each parameter differently). For example, if its a layer closer to the input, then the learning step should be larger. It would be nice for the learning algorithm to take this into account by itself so to deal with the vanishing gradient.If there are any suggestions on how to deal with vanishing gradient other than batch-norm or ReLu's I'd love to hear about them!It seems that vanishing gradient happens mainly because the non-linearities have the property that  and also because  and after multiplying it many times, it either explodes or vanish. Explicitly saying the problem might help solve it. The issue is that it causes lower layers to not update or hinders signal through the network. It would be nice to maintain this signal flowing through the network, during the forward and backward pass (and also during training, not only at initialization).1: Ioffe S. and Szegedy C. (2015),\"Batch Normalization: Accelerating Deep Network Training by ReducingInternal Covariate Shift\",Proceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015.Journal of Machine Learning Research: W\u0026amp;CP volume 37","Creater_id":37632,"Start_date":"2016-08-03 09:34:46","Question_id":227114,"Tags":["machine-learning","neural-networks","conv-neural-network","batch-normalization"],"Answer_count":2,"Last_activity":"2016-08-14 00:11:14","Link":"http://stats.stackexchange.com/questions/227114/are-there-any-ways-to-deal-with-the-vanishing-gradient-for-saturating-non-linear","Creator_reputation":790}
{"_id":{"$oid":"5837a57fa05283111e4d524d"},"View_count":35,"Display_name":"Felipe Almeida","Question_score":0,"Question_content":"I'm using scikit-learn to classify a bunch of documents with a Linear SVM classifier based on their TF*IDF term frequencies.The pipeline is this: ('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', svm.LinearSVC()),Now CountVectorizer has an ngram_range argument with which I can specify what types of n-grams I want to extract.What surprises me is that using just unigrams (ngram_range=(1,1)) yields a slightly better result than using unigrams and bigrams (ngram_range=(1,2)) as features.What may have caused this? Am I possibly overfitting the data?","Creater_id":95456,"Start_date":"2016-08-13 21:54:47","Question_id":229733,"Tags":["machine-learning","text-mining","scikit-learn","tf-idf"],"Answer_count":0,"Last_activity":"2016-08-13 21:54:47","Link":"http://stats.stackexchange.com/questions/229733/why-does-adding-bigram-features-make-my-results-in-text-classification-worse","Creator_reputation":106}
{"_id":{"$oid":"5837a57fa05283111e4d524f"},"View_count":34,"Display_name":"Thoughtcraft","Question_score":0,"Question_content":"Let's say I have results of a mesurement for a large number of treatments (p-value, beta, t-stat) on samples from a large number of different populations, and I want to compare how different the effect of a treatment was in different populations (of which there are 30), and then look at which treatments were most sensitive to population differences. Populations that have effects in opposite directions are a definite possibility, and would be of particular interest. Unfortunately, I don't have the raw data, so I can't just go back and do a test that includes population as a factor. So my thought is to get an estimate of this by taking beta divided by the p-value. By doing this, I can factor in both the absolute difference between beta coefficient values, and my certainty that the coefficient is significant. In other words, I would get the biggest difference in scores by maximizing the absolute difference between their beta-coefficient values and minimizing each of their p-values. Either increasing p-value or decreasing the absolute difference between beta-coefficients will lower the score.The problem that I am anticipating is that increasing p-value is not considered a valid way to claim increasing likelihood of the null hypothesis being true. I'm hoping that I can get away with it by thinking of p \u003e0.95 as \"significant non-significance\" as opposed to \"accepting the null\". Is there something wrong with how I am thinking about this? Would this give me any different information than just looking at the effect size (t-stat)?I've looked at this thread: Why are 0.05 \u0026lt; p \u0026lt; 0.95 results called false positives?and this: https://www.reddit.com/r/askscience/comments/2yiqjq/why_can_we_not_accept_the_null_hypothesis_if_p095/","Creater_id":127614,"Start_date":"2016-08-13 18:44:26","Question_id":229723,"Tags":["hypothesis-testing","anova","p-value","effect-size","group-differences"],"Answer_count":1,"Last_activity":"2016-08-13 21:36:58","Link":"http://stats.stackexchange.com/questions/229723/division-of-beta-coefficient-by-p-value","Creator_reputation":1}
{"_id":{"$oid":"5837a57fa05283111e4d525c"},"View_count":115,"Display_name":"Dendrobates","Question_score":1,"Question_content":"I'm working on segmentation/clustering and trying to use Gaussian Mixture Modelling for Model-Based Clustering. I'm using the R package Mclust in order to come up with the best fit for my data.All data is transformed to a uniform distribution with mean zero, standard deviation one (I know, not Gaussian) and the variables included are chosen based on earlier attempts using k-means, where the given variables seemed to be discriminating. Of course, k-means comes with some drawbacks (lack of statistical foundation, no control of cross-correlation etc,), and that's the reason I want to use Model-Based Clustering (or latent class analysis, with the package poLCA).When using mclustBIC, many of the possible BICs are actually NA. I tried to reduce the dimension of the data, but this didn't improve the output. For example the VEV is only calculated for nr clusters 1:3, while it looks like it could improve for more clusters (see plot below).Someone who experienced similar problems? And can someone help me into the right direction for finding the best model, using mclust? I would like to calculate other BICs with a higher number of clusters.Help would be appreciated!","Creater_id":99304,"Start_date":"2016-04-26 03:38:35","Question_id":209364,"Tags":["r","clustering","segmentation"],"Answer_count":1,"Last_activity":"2016-08-13 20:30:13","Link":"http://stats.stackexchange.com/questions/209364/r-clustering-using-mclust-bic-are-often-na","Creator_reputation":21}
{"_id":{"$oid":"5837a57fa05283111e4d5269"},"View_count":29,"Display_name":"Buck Shlegeris","Question_score":1,"Question_content":"My friend and I need to make some calculations involving probability distributions over extremely wide ranges of values.For example, I want to be able to take a bunch of random variables with lognormal PDFs, add and multiply them together, then use this as a likelihood function in a Bayesian update of a Pareto distribution prior, and take the mean of the resulting posterior distribution.My distributions often have significant probability mass over 50 orders of magnitude. So I can't just approximate everything as log-normal distributions. My friend has currently implemented this with buckets on a log scale, with about 4 buckets per order of magnitude. This is somewhat slow and we haven't proven any error bounds on this approach. I feel like it's quite foolish to try to write statistical computation software as an amateur.Is there an existing library that implements this kind of functionality?","Creater_id":107796,"Start_date":"2016-08-10 16:20:32","Question_id":229279,"Tags":["computational-statistics","fat-tails"],"Answer_count":1,"Last_activity":"2016-08-13 18:54:25","Link":"http://stats.stackexchange.com/questions/229279/software-for-computing-with-extremely-wide-probability-distributions","Creator_reputation":81}
{"_id":{"$oid":"5837a57fa05283111e4d5276"},"View_count":77,"Display_name":"zergylord","Question_score":4,"Question_content":"I'm drawing samples from a distribution to train a machine learning classifier (training it via mini-batches of 32 samples at a time). It's just a toy dataset, so I know that the samples are coming from a multivariate Gaussian with an identity co-variance matrix. Since the quantile function (inverse CDF) is tractable for such a distribution, I could evenly space the samples across the domain (0,1) of this function (e.g. points of the unit square in the 2D case) and presumably get a mini-batch of sample the encompass the entire distribution. Obviously I'd have to add a random offset to this grid of samples such that every mini-batch isn't identical, but otherwise it seems like this should speed up learning  alot. My question is: is this approach reasonable or is there something I'm missing? And if it is, then is there name for it or literature around it?Thanks! ","Creater_id":10320,"Start_date":"2015-11-13 18:12:34","Question_id":181705,"Tags":["machine-learning","distributions","normal-distribution","sampling","weighted-sampling"],"Answer_count":1,"Last_activity":"2016-08-13 18:19:17","Link":"http://stats.stackexchange.com/questions/181705/representative-sampling-from-a-distribution","Creator_reputation":212}
{"_id":{"$oid":"5837a57fa05283111e4d5283"},"View_count":13,"Display_name":"Akinkunle Allen","Question_score":0,"Question_content":"Suppose a company T has a set of fixed customers N and set of products P. For each product, there are competing products produced by other companies that the customer might buy. Once a customer takes up a product (either from Company T or their competitors), they don't usually change, they stick with that product.Company T has been collecting monthly market data for 4 years and have calculated the market share they have for each month for 4 years for each product. For each of their product, they also know the percentage of their customers N that are using that product for each month.Now, the analysis I want to do is to is, for a new product company T is planning to launch,   How much market share will the new product reach if X% of customers have already switched to a competitor product.Please, I am confused as to how to approach this problem? What methods can I explore and use for this analysis?","Creater_id":127229,"Start_date":"2016-08-13 17:02:31","Question_id":229717,"Tags":["forecasting"],"Answer_count":0,"Last_activity":"2016-08-13 17:02:31","Link":"http://stats.stackexchange.com/questions/229717/forecasting-market-share","Creator_reputation":101}
{"_id":{"$oid":"5837a57fa05283111e4d5285"},"View_count":37,"Display_name":"machinelearningguy","Question_score":1,"Question_content":"Suppose we want to model whether somebody makes 50,000\u0026lt; \\ based on various predictor variables. These variables can be either continuous or categorical. If a categorical variable has many levels, is there any way to account for this without having to create dummy variables? For example, in a random forest or logistic regression, it could take a long time for these algorithms to run if you include a lot of dummy variables.","Creater_id":127610,"Start_date":"2016-08-13 15:53:31","Question_id":229714,"Tags":["machine-learning"],"Answer_count":1,"Last_activity":"2016-08-13 16:55:54","Link":"http://stats.stackexchange.com/questions/229714/how-do-you-deal-with-a-categorical-variable-that-has-multiple-levels","Creator_reputation":16}
{"_id":{"$oid":"5837a57fa05283111e4d5292"},"View_count":74,"Display_name":"user259047","Question_score":2,"Question_content":"Does the following qualify as a univariate regression?y=b_0+b_1x+b_2x^2+\\epsilonI fully comprehend the implications of adding regressors and need no background information - a \"yes\" or \"no\" will do :-)","Creater_id":127605,"Start_date":"2016-08-13 13:33:36","Question_id":229700,"Tags":["regression","definition","multivariate-regression","univariate"],"Answer_count":2,"Last_activity":"2016-08-13 16:49:14","Link":"http://stats.stackexchange.com/questions/229700/defining-a-univariate-regression","Creator_reputation":25}
{"_id":{"$oid":"5837a57fa05283111e4d52a0"},"View_count":94,"Display_name":"Mud Warrior","Question_score":1,"Question_content":"I built a generalized linear mixed-effects model (GLMM) using glmer function from the lme4 package in r to model species richness around aquaculture sites based on significant explanatory variables using Zuur et al. (2009) Mixed Effects Models and Extensions in Ecology with R. The dataset used to build the model has ~ 1000 samples and my best model is:Mod1 \u0026lt;- glmer(Richness ~ Distance + Depth + Substrate + Beggiatoa +         Distance*Beggiatoa + (1|Site/transect), family = poisson, data = mydata)Now I have a full data set collected at different sites and I want to assess how this model performs on the new data set.I never assessed model performance on a new data set before and my first approach would be to compare R^2 values between data sets and p-values of each explanatory variables. However, I’m sure there are much better options. What technique would you use to assess model performance on a new data set?","Creater_id":108922,"Start_date":"2016-08-11 08:53:25","Question_id":229370,"Tags":["r","lme4","glmm","validation","glmer"],"Answer_count":1,"Last_activity":"2016-08-13 16:17:41","Link":"http://stats.stackexchange.com/questions/229370/how-to-assess-glmm-performance-on-a-new-data-set","Creator_reputation":290}
{"_id":{"$oid":"5837a57fa05283111e4d52ac"},"View_count":105,"Display_name":"kap","Question_score":1,"Question_content":"SMOTE is a popular method to generate synthetic examples of the minority class in an unbalanced-class data set.I am trying out SMOTE in the \"unbalanced\" package in R. I am generating a simple simulate data but SMOTE seems to fail on it. Not sure what the problem is. library(unbalanced)set.seed(1)X \u0026lt;- matrix(rnorm(1000), ncol = 2)X[1:50,] \u0026lt;- X[1:50,]+5Y \u0026lt;- as.factor(c(rep(1,50), rep(0,450)))smoted \u0026lt;- ubSMOTE(X,Y,k=1)#WARNING: NAs generated by SMOTE removed dim(smotedy)#  0   1 #200 150","Creater_id":8757,"Start_date":"2016-08-13 11:32:10","Question_id":229695,"Tags":["r","machine-learning","unbalanced-classes","synthetic-data"],"Answer_count":0,"Last_activity":"2016-08-13 16:11:08","Link":"http://stats.stackexchange.com/questions/229695/smote-using-unbalanced-package-in-r-fails-on-simple-simulated-data","Creator_reputation":16}
{"_id":{"$oid":"5837a57fa05283111e4d52ae"},"View_count":16,"Display_name":"Toney Shields","Question_score":1,"Question_content":"I have certain statistic  for a likelihood ratio test, and I want to know how its the distribution behave (its form), so I did a simulation: 1- I created a loop and I simulated my data, and placed the values of  in a vector  (number of values calculated is let's say 100). 2- I created another loop contaning the first one that calculate the mean of each vector calculated in 1, and I put them into another vector  (of length 500). So my code (in Matlab) is something like:for j=1:500     for i=1:100         simulate data         Y(i)= value of T      end     V(j)=mean(Y)endMy question is: To know how the distribution of  behaves, should I plot the histogram of  or ?","Creater_id":109437,"Start_date":"2016-08-13 15:37:55","Question_id":229711,"Tags":["distributions","matlab"],"Answer_count":1,"Last_activity":"2016-08-13 15:49:32","Link":"http://stats.stackexchange.com/questions/229711/the-form-of-the-distribution-of-a-test-statistic","Creator_reputation":197}
{"_id":{"$oid":"5837a57fa05283111e4d52bb"},"View_count":36,"Display_name":"Arault","Question_score":0,"Question_content":"I was wondering how the distribution specified in a GLM changes the coefficients. If I have understood the process, when you fit a GLM, let's say , most of the software and R packages will use a Newton Raphson algorithm to fit the . We have , where f is the gradient function. So in our case,  (i.e. log likelihood/) and , the hessian matrix. Again, correct me if I'm wrong, but the  matrix is approximated by the Fisherinformation, so , where  is diagonal with ith elemnt equals to :\\frac{w_i}{\\phi V(\\mu_i)(g'(\\mu_i))^2}+\\frac{V(\\mu_i)g''(\\mu_i)+V'(\\mu_i)g'(\\mu_i)}{\\phi V(\\mu_i)^2(g'(\\mu_i))^3}Here my first question, how do you estimate the variance of  ?Then,When I want to calculate the , I find :\\frac{\\partial l}{\\partial \\beta_j}=\\sum_{i=1}^n \\frac{y_i-\\mu_i}{\\phi}*\\frac{W_i}{(g^{-1})'(\\eta_i)}*x_{ij}Where  is the log likelihood,  the scale parameter,  the estimated average, and  the weight matrix.My second question is, that all thoses elements doesn't not seem to be directly linked to the specified distribution. The only thing that looks important is the link function choosen (). So I tried to launch two models on R to see if the coefficients are the same, and obviously not... for instance this codedata(\"UScrime\")model1=glm(formula=Prob~M+So+Ed,data = UScrime,family=gaussian(link=log))model2=glm(formula=Prob~M+So+Ed,data = UScrime,family=Gamma(link=log))I have two different set of coefficients.Thanks for you answers","Creater_id":123067,"Start_date":"2016-08-06 05:49:51","Question_id":228557,"Tags":["generalized-linear-model"],"Answer_count":1,"Last_activity":"2016-08-13 15:41:59","Link":"http://stats.stackexchange.com/questions/228557/understanding-the-glm-coefficients-calculation","Creator_reputation":41}
{"_id":{"$oid":"5837a57fa05283111e4d52c8"},"View_count":287,"Display_name":"S\u0026#248;r\u0026#235;n","Question_score":3,"Question_content":"I'm trying to understand better the difference between  covariance and correlation, besides the fact that the correlation coefficient is a dimensional and has values between  and . One unclear point is the following: the correlation coefficient can highlight only a linear relation between two variables, but does  covariance only work for linear relations too?Supposing that the relation is not linear, is the covariance not zero? Is it bigger than in the case in which the relation is linear? Cauchy inequality says|\\sigma_{xy}|\u0026lt;\\sigma_{x} \\sigma_{y}Which seems to say that the covariance is maximum when there is a perfect linear relation.  So I do not get what does happen if the relation is not linear, and, if covariance is not a suitable parameter in that cas I don't get the reason for that, since simply looking at the definition:\\sigma_{xy}=\\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{N}it looks like that this should be non zero for any kind of relation (like parabolic).So can this be a difference between the use of covariance and correlation coefficient? (The first is useful for any kind of relation, the second one only for linear ones).","Creater_id":114858,"Start_date":"2016-08-13 07:25:40","Question_id":229667,"Tags":["regression","correlation","covariance","non-independent","linear"],"Answer_count":2,"Last_activity":"2016-08-13 14:55:30","Link":"http://stats.stackexchange.com/questions/229667/difference-between-correlation-and-covariance-is-covariance-only-useful-if-the","Creator_reputation":162}
{"_id":{"$oid":"5837a57fa05283111e4d52d6"},"View_count":15,"Display_name":"Marko","Question_score":1,"Question_content":"I am aware of the rationale behind taking the exponentiated coefficients in a proportional hazards model as representing change in hazard per unit change in the corresponding predictor. This also implies that it should be possible to obtain a hazard greater than 1. However, with the inverse of the cloglog link function being , it doesn't seem possible for the hazard to actually exceed 1. What am I missing?Thank you.","Creater_id":113478,"Start_date":"2016-08-12 17:36:23","Question_id":229626,"Tags":["regression-coefficients","proportional-hazards","discrete-time"],"Answer_count":1,"Last_activity":"2016-08-13 14:22:30","Link":"http://stats.stackexchange.com/questions/229626/coefficients-in-discrete-proportional-hazards-model","Creator_reputation":49}
{"_id":{"$oid":"5837a57fa05283111e4d52e3"},"View_count":33,"Display_name":"user697110","Question_score":1,"Question_content":"I am doing signal analysis for the fist time and am using the implementation to found in the stats package to obtain the spectral density of a time series, so I can know which frequencies of the signal have the most power.Now although I understand the general propose of the analysis, (the plot meets the shape it was supposed to have in my dataset), I don't understand the scales obtrained through the use of Spectrum(). For example when I do the spectrum(lh) example, it gives me in the Frequency axis a scale of 0 to 0.5 if I say the time seris has a frequency 1. Could anybody explain to a new guy how do I obtain the values in the frequency and spectrum Axis?","Creater_id":52182,"Start_date":"2016-08-12 06:06:36","Question_id":229530,"Tags":["r","signal-processing","spectral-analysis"],"Answer_count":1,"Last_activity":"2016-08-13 14:07:14","Link":"http://stats.stackexchange.com/questions/229530/spectral-density-of-a-time-series-r-implementation-explanation","Creator_reputation":30}
{"_id":{"$oid":"5837a57fa05283111e4d52f0"},"View_count":63,"Display_name":"n17n","Question_score":2,"Question_content":"In a football manager game, there are many players as in this screenshot:My y is the average rating of a player, my sample size N is big enough, andx1,x2,x3,...,x30 are the attributes like finishing, acceleration and pace (which can take values from 1 to 20). No attribute has a negative effect.Can I use regression or another technique to find parameters such as:y=f(x1)+f(x2)+...+f(x30)In other words, after sampling N players, I want to know which attribute has the most effect in average rating, and which the least, and a scale to know how much more important is one attribute compared to the least important one.","Creater_id":127600,"Start_date":"2016-08-13 11:28:21","Question_id":229692,"Tags":["regression","model","rating"],"Answer_count":1,"Last_activity":"2016-08-13 14:02:50","Link":"http://stats.stackexchange.com/questions/229692/good-way-to-model-y-fx1fx2-fx30","Creator_reputation":11}
{"_id":{"$oid":"5837a57fa05283111e4d52fd"},"View_count":21,"Display_name":"Shivi Bhatia","Question_score":1,"Question_content":"In a follow up to one of the business problem i have discussed here,for one of the logistic regression model, i have 19 predictor variables of     which around 8 are categorical with multiple categories.   Instead of including all in the model where one could correlate to another,i am thinking of using a chi square test of independence at a given p value.With this i would come to realization if there is any correlation among the     categorical predictors.  If so i would include the correlated variables one by one in the modeland would used the significant one.I have researched on this on SE where they have suggested to use chi-square but unsure if my approach to drop these correlated var is justified.Please advice. ","Creater_id":79611,"Start_date":"2016-08-13 13:42:09","Question_id":229702,"Tags":["r","statistical-significance","descriptive-statistics"],"Answer_count":0,"Last_activity":"2016-08-13 13:42:09","Link":"http://stats.stackexchange.com/questions/229702/correlation-in-categorical-predictors","Creator_reputation":13}
{"_id":{"$oid":"5837a57fa05283111e4d52ff"},"View_count":22,"Display_name":"redcalx","Question_score":2,"Question_content":"In Restricted Boltzmann Machines for Collaborative Filtering Restricted Boltzmann Machines (RBMs) are applied to the Netflix Prize data set. An obvious next step might be to use stacks of RBMs (i.e. Deep Belief Nets) on the same problem, and indeed this is discussed briefly in section 8.2 of the RBM paper. Has this ever been tried? I'm suspecting that the data in question is not amenable to further levels of abstraction, but I would be interested in others thoughts on that.","Creater_id":7789,"Start_date":"2016-08-13 13:40:04","Question_id":229701,"Tags":["deep-belief-networks","rbm","netflix-prize"],"Answer_count":0,"Last_activity":"2016-08-13 13:40:04","Link":"http://stats.stackexchange.com/questions/229701/deep-belief-net-applied-to-netflix-prize","Creator_reputation":393}
{"_id":{"$oid":"5837a57fa05283111e4d5301"},"View_count":11,"Display_name":"Devin","Question_score":2,"Question_content":"Hopefully this is a simple question. I am seeking a way to compare the effect size of a factor with multiple levels under two different conditions. For example, consider a situation where the per acre yield of 5 different cultivars (varieties) of corn are significantly different. However, I expect the magnitude of these differences to be reduced under drought conditions. I set up an experiment (with a fully balanced design and well replicated) where I include all 5 varieties and simulate drought or non-drought conditions. I can perform an ANOVA and find that variety, drought and their interaction are all highly significant predictors of yield. But, I would like to test whether the magnitude of the variation in yield among cultivars is different under drought or non-drought conditions. I can compare the variance among cultivars under the two treatments and see that it is substantially lower under drought, but I am not sure the best way to test this statistically. Any help would be greatly appreciated. ","Creater_id":127603,"Start_date":"2016-08-13 13:09:52","Question_id":229699,"Tags":["effect-size"],"Answer_count":0,"Last_activity":"2016-08-13 13:09:52","Link":"http://stats.stackexchange.com/questions/229699/comparing-effect-sizes-of-a-multi-level-predictor","Creator_reputation":11}
{"_id":{"$oid":"5837a57fa05283111e4d5303"},"View_count":23,"Display_name":"Calculon","Question_score":2,"Question_content":"I have a panel data of stock returns and I want to estimate the covariance matrix of these returns throughout time. I also want to use exponential smoothing. The scheme is as follows.\\hat{\\Sigma}_t = \\lambda\\hat{\\Sigma}_{t-1} + (1-\\lambda)(r_t - \\hat{\\mu}_t)(r_t - \\hat{\\mu}_t)^{\\top} \\tag{1}Here  and  are my estimates of the covariance matrix of returns and the mean return at time .In Chapter 10 of Tsay \"Analysis of Financial Time Series\" the author describes the existing literature on this and gives the following example. He has two index returns, one from Hong Kong and one from Japan. He considers the univariate case first, that is he estimates the variances of the two indices separately. To this end he fits a GARCH(1,1) model to both these index returns.The estimated conditional variance models are as follows.\\sigma_{1,t}^2 = 0.038 + 0.855\\sigma_{1,t-1}^2 + 0.143(r_t - 0.109)^2\\sigma_{2,t}^2 = 0.044 + 0.861\\sigma_{2,t-1}^2 + 0.127(r_t - 0.109)^2I am guessing that  and  also happen to be the optimal smoothing parameters. Can someone confirm this?In both cases the coefficients (excluding the constant) sum up to , which is awfully close to . This is very convenient since this is what we need for exponential smoothing, i.e. the weights of the previous estimate and the current data have to sum up to . I find it very hard to believe that this is some coincidence. Can someone please explain the link between fitting a GARCH model and the optimal exponential smoothing parameter? Furthermore, can someone maybe explain what the author may have done to find those estimates that fit right into the exponential smoothing framework? What would the author have done if the fitted GARCH model had parameters  and ?","Creater_id":49764,"Start_date":"2016-08-13 10:32:17","Question_id":229685,"Tags":["garch","exponential-smoothing"],"Answer_count":1,"Last_activity":"2016-08-13 12:58:47","Link":"http://stats.stackexchange.com/questions/229685/exponential-smoothing-versus-garch1-1-for-conditional-variance","Creator_reputation":145}
{"_id":{"$oid":"5837a57fa05283111e4d5310"},"View_count":170,"Display_name":"Matt","Question_score":3,"Question_content":"This question is in the context of time-delay estimation. Say I have a stationary Gaussian stochastic process , and I know its autocorrelation function . To do time-delay estimation, I'm computing a windowed cross correlation between  and a delayed version of it. In other words,\rg_1 = g(x-D) \\\\\r\\phi(\\tau) = \\int_{-T/2}^{T/2} g(x) g_1(x + \\tau)\rand I'm going to determine the delay by finding the maximum of . My question is, is it possible to get an expression for the probability distribution of ? ","Creater_id":9544,"Start_date":"2012-03-01 12:53:11","Question_id":23962,"Tags":["distributions","stochastic-processes","cross-correlation"],"Answer_count":1,"Last_activity":"2016-08-13 12:56:23","Link":"http://stats.stackexchange.com/questions/23962/probability-distribution-of-windowed-cross-correlation","Creator_reputation":116}
{"_id":{"$oid":"5837a57fa05283111e4d531d"},"View_count":24,"Display_name":"harlekin","Question_score":3,"Question_content":"I am stuck with interpreting the information given in the following exercise:Due to a recent increase in the price of tomatoes a pub landlord is looking to reduce the quantity of tomatoes he buys, but wants to minimize the risk of running out of it on any one day. Data collected over the last 20 days has shown that on average 8 customers order tomatoes and that the average number of orders per day is 25. The standard deviation of the data collected was 1.2. Calculate the probability that the landlord will run out of tomatoes if he buys enough for 10 portions per day.My first interpretation: I would model this using the random variable  number of tomatoes ordered.  is normally distributed with mean  and standard deviation . Then from the data I have an estimate  for  and  for , and I know that\\frac{\\bar X - \\mu}{S/\\sqrt{20}} \\backsim t_{19} But I am not sure this helps me to find the required probability  and also I haven't used the information that  meals are ordered per day on average.My second interpretation: Let  be the proportion of meals with tomatoes ordered per day. Then the data gives us an estimate  and we know that \\frac{p - \\pi}{\\sqrt{\\frac{\\pi(1 - \\pi)}{20}}} \\backsim N(0,1)but again I don't think this is what I should look for as it would not be useful in calculating the required probability.My third interpretation: Let  be the number of meals with tomatoes ordered per day. Then  from which we can compute the probability  as required. However, here I don't think the random variable accurately reflects the given information, in particular treating the number of observations as repeated attempts sounds weird, and again I haven't used given information (namely the sample standard deviation).I am quite confused and not sure how to progress -- all these interpretations seem to be not entirely correct. Any hint would be very useful, thanks a lot for your help !!","Creater_id":98411,"Start_date":"2016-08-13 06:37:44","Question_id":229662,"Tags":["self-study","distributions","basic-concepts"],"Answer_count":1,"Last_activity":"2016-08-13 11:34:34","Link":"http://stats.stackexchange.com/questions/229662/extracting-information-correctly-from-text","Creator_reputation":128}
{"_id":{"$oid":"5837a57fa05283111e4d532a"},"View_count":27,"Display_name":"atomsmasher","Question_score":1,"Question_content":"Say I have some data, with 400 features, I want to use these features to predict a continuous response variable and the data meet the assumptions to run a linear regression model. So I want to reduce my feature space, and I use lasso. Taking the nonzero coefficients I now notice that my feature space has been drastically reduced to 10 dimensions (10 features). But now I wonder, how do I make sure that the features that survive the lasso regression explain \u003e= 90% of the variance of the original data set? ","Creater_id":117380,"Start_date":"2016-08-13 09:24:04","Question_id":229681,"Tags":["regression","variance","lasso","dimensionality-reduction"],"Answer_count":1,"Last_activity":"2016-08-13 11:30:39","Link":"http://stats.stackexchange.com/questions/229681/making-sure-that-the-features-retained-after-dim-reduction-explain-90-of-the","Creator_reputation":35}
{"_id":{"$oid":"5837a57fa05283111e4d5337"},"View_count":37,"Display_name":"Sambit Nandi","Question_score":2,"Question_content":"I am trying to develop a model that classifies text documents to ~50 classes. Would appreciate any help regarding what should my approach be to make it happen?I have a training set of ~5000 documents. My initial approach is to use a random forest on the term document matrix -Few questions/challenges:The matrix is really sparseHow to finally define accuracyshould I create binary flags for each class and develop multiple models for each class?Should I do a K-Means clustering on the data prior to the RF andthen run RF in each cluster to improve accuracy?Any help is really appreciated.","Creater_id":127598,"Start_date":"2016-08-13 11:29:04","Question_id":229693,"Tags":["machine-learning","classification","modeling","text-mining","multi-class"],"Answer_count":0,"Last_activity":"2016-08-13 11:29:04","Link":"http://stats.stackexchange.com/questions/229693/multi-class-classification-of-text","Creator_reputation":11}
{"_id":{"$oid":"5837a57fa05283111e4d5339"},"View_count":46,"Display_name":"marcodena","Question_score":1,"Question_content":"I am trying to fit the number of crimes in a city with some enviromental variables (aka my features). I'm using a Poisson/Negative Binomial model since I have count data.The problems are:selecting the features which REALLY fit (I have sometimes big collinearity)select the smallest subset of significative features (p-values)understand the relative importance between them to fit the number of crimesPossible solutions to the problems:Variance inflation factor (VIF). Is it ok? Sometimes interactions could be ignored. I mean: what if I have to test whenever two variabiles are important, but collinear? (e.g. population density and employment density)Stepwise (backward/foward/with Cross-Validation) steps to reduce the AIC. However, this does not mean that the resulting variables are significative (p-values). Should I care at all about the features' p-values? Standardize the input data, then looking at the Beta coefficient to see the importance of them. Is it really the best way?Should I use something else?Keep in mind that my primary goal is not the . I want to see and to be able to explain how some variables could explain the crime.","Creater_id":48622,"Start_date":"2016-08-13 08:15:49","Question_id":229672,"Tags":["regression","cross-validation","multicollinearity","aic","vif"],"Answer_count":1,"Last_activity":"2016-08-13 11:23:11","Link":"http://stats.stackexchange.com/questions/229672/exploratory-data-analysis-what-features-are-important","Creator_reputation":267}
{"_id":{"$oid":"5837a57fa05283111e4d5346"},"View_count":31,"Display_name":"Matteo Guidarini","Question_score":1,"Question_content":"I have this function to maximize: funcToOpt \u0026lt;- function (theta1,theta2,theta3){  inner \u0026lt;- rowSums((wb + (theta1*X1+theta2*X2+theta3*X3) / Nt) * r)    innerU \u0026lt;- u(inner, gamma)                                     # u() is another function   return (sum(innerU) / T)}fbb \u0026lt;- function(x) funcToOpt(x[1],x[2],x[3])max \u0026lt;- optim(c(0,0,0),fbb,control=list(fnscale=-1))Where: wb,X1,X2,X3,r are TxN matrices and Nt a vector with length of NI wanna just that (wb + (theta1*X1+theta2*X2+theta3*X3) / Nt)can't assume negative values. Is there someone who can help me? Thanks!","Creater_id":123764,"Start_date":"2016-08-13 11:15:15","Question_id":229688,"Tags":["r","bounds","function"],"Answer_count":0,"Last_activity":"2016-08-13 11:15:15","Link":"http://stats.stackexchange.com/questions/229688/how-to-bound-a-value-in-a-function-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a57fa05283111e4d5348"},"View_count":17,"Display_name":"jason","Question_score":1,"Question_content":"I am trying to model a system using a simple quadratic function.I have theoretic reasons why this is the appropriate model of the physiological process.Assume the model is: X --\u003e F(X) --\u003e YI assume that Y = C + BX + AX*XLets suppose I have two sets of data collected from this system which have different ranges of X.One has X values from 10 to 90 and the other has X values from 30 to 80.I would like to estimate A, B and C so that they model the underlying system and the estimates are not specific to one data set or the other.How can I do this?I am testing by using a full dataset and fitting the quadratic, then using a subset of data and fitting the quadratic again and getting different parameter estimates of A, B and C.Is there some method (some orthogonalization/projection of X) where A, B and C are the same regardless of whether I use dataset one or two?Thank you.","Creater_id":127406,"Start_date":"2016-08-13 06:46:33","Question_id":229664,"Tags":["r","modeling"],"Answer_count":1,"Last_activity":"2016-08-13 10:59:21","Link":"http://stats.stackexchange.com/questions/229664/estimating-the-model-from-part-of-the-data","Creator_reputation":31}
{"_id":{"$oid":"5837a57fa05283111e4d5355"},"View_count":31,"Display_name":"Adrian","Question_score":1,"Question_content":"Suppose I want to compare groups A and B on 5 variables: age, education level, blood pressure, brain volume, and glucose level. Given that the assumptions are met, I will use the 2 sample t-test to make 5 comparisons. In this case, do I have to correct for multiple testing? What if I wanted to compare the groups on 10 variables? Or 20? Under what condition should I correct for multiple testing?","Creater_id":45971,"Start_date":"2016-08-13 09:05:57","Question_id":229677,"Tags":["multiple-comparisons","bonferroni"],"Answer_count":1,"Last_activity":"2016-08-13 10:51:36","Link":"http://stats.stackexchange.com/questions/229677/when-should-i-correct-for-the-multiple-testing-problem","Creator_reputation":356}
{"_id":{"$oid":"5837a57fa05283111e4d5362"},"View_count":1870,"Display_name":"user3671","Question_score":4,"Question_content":"Is there a way to easily create factors based on quantiles of selected variables in a dataframe? Say, in a datatset D, I have variables V1 to V10, which are all numeric. I would like to create dummies for V7 to V10 based on their respective quantiles.","Creater_id":3671,"Start_date":"2011-03-11 04:42:59","Question_id":8151,"Tags":["r","categorical-data"],"Answer_count":2,"Last_activity":"2016-08-13 10:27:56","Link":"http://stats.stackexchange.com/questions/8151/quantile-transformation-in-r","Creator_reputation":514}
{"_id":{"$oid":"5837a57fa05283111e4d5370"},"View_count":19,"Display_name":"user792000","Question_score":1,"Question_content":"I'd love to hear some experts' advice on a survey I'm working on.Let's assume a survey being conducted on a sample of community organizations. A public database (the sample frame, but almost a census) of over 2,000  institutions was used to produce a stratified sample of 755 organizations. Later on, during the pre-test, researchers found that not all the addresses/telephone provided were actually valid for contacting sampled organizations, some duplicates also were identified on the original database. The company then replaced the problematic units withing each stratum through resampling. However, after the survey gets the road, other units were found with similar issues in such a way that from the 755 original draw, only 720 organizations were actually \"valid\". Now, with a stratified sample designed to accommodate 755 elements, but with possibly 720 clearly the weight will not match as expected, so what are the possible ways one has to deal with this? I'd really appreciate in-depth comments accompanied with literature indications.","Creater_id":60188,"Start_date":"2015-10-06 14:07:40","Question_id":175765,"Tags":["survey","survey-weights","survey-sampling"],"Answer_count":0,"Last_activity":"2016-08-13 10:23:19","Link":"http://stats.stackexchange.com/questions/175765/re-weighting-a-survey-after-resampling-with-replacement-been-already-used","Creator_reputation":120}
{"_id":{"$oid":"5837a57fa05283111e4d5372"},"View_count":58,"Display_name":"CakeNRun","Question_score":2,"Question_content":"a company that conducts face to face surveys have described their sampling procedure as follows:  We use a unique and rigorous sampling method - a form of random  location sampling, using a control method applied to field region and  sub-region over a robust number of sample points (typically 170-180)  to ensure we get a good geographical spread. We then set our  interviewer quotas for gender, age, working status and tenure to  ensure our sample is nationally representative - we use the CACI ACORN  geo-demographic system in the selection process.Would this be a random or a probability sample? Or is it a non-probability quota sample? Or a combination of both? (is that possible?)","Creater_id":70243,"Start_date":"2015-10-23 02:24:13","Question_id":178313,"Tags":["sample","survey-sampling"],"Answer_count":1,"Last_activity":"2016-08-13 10:21:05","Link":"http://stats.stackexchange.com/questions/178313/is-this-considered-a-random-probability-sample","Creator_reputation":16}
{"_id":{"$oid":"5837a57fa05283111e4d537f"},"View_count":24,"Display_name":"E.Brogdon","Question_score":2,"Question_content":"I am trying to compare financial investment of 2 groups, however, my data is not normally distributed AND has unequal variance between the two groups. I thought of using the Mann-Whitney on SPSS but I read that to use the Mann-Whitney data must have equal variance, is there any exception to this rule? I have decent sample sizes with my smallest sample size being 85, would this large sample size allow me to use a t-test with unequal variance? Any help would be appreciated.","Creater_id":127414,"Start_date":"2016-08-11 16:10:47","Question_id":229439,"Tags":["t-test","nonparametric","mann-whitney-u-test"],"Answer_count":1,"Last_activity":"2016-08-13 10:08:07","Link":"http://stats.stackexchange.com/questions/229439/comparisons-on-nonparametic-unequal-variance-data","Creator_reputation":13}
{"_id":{"$oid":"5837a57fa05283111e4d538c"},"View_count":19,"Display_name":"R Noob","Question_score":1,"Question_content":"I am trying to compare 4 different (but correlated) parameters (biomarkers of a disease with continuous values) to identify which parameters would detect more deteriorating subjects and in a shorter period. My data consists of more than 100 subjects, with measurements every 6 months for 2-6 years. Subjects have different follow-up lengths and the data is right-censored.I am using a time-to-event approach. To define the events (significant change from baseline, first study measurement), I am using 95% repeatability estimates of each parameter. If the difference between a follow-up measurement and the baseline is more than the expected 95% repeatability, that is being considered an event. To compare the Kaplan-Meyer survival curves between the parameters, I intend to use pair-wise log-rank or Cox regression comparisons. (At the moment, I am not concerned to use pair-wise comparisons between 4 parameters)I see two issues with this approach:1 - The descriptions of log-rank or Cox regression that I find always describe comparisons between two groups. I imagine that comparing two parameters in the same subjects may violate some assumptions, given their correlation (I don't know how strong the correlation is). Reading this \"Which model should I use for Cox proportional hazards with paired data?\" and this \"How to conduct conditional Cox regression for matched case-control study?\", I am planning to use a frailty random coefficient for the subject level. Would it be adequate and sufficient?2 - My event definition gives a 5% chance of false-positive, i.e. considering a amount of change in a parameter significant (and therefore an event), when it occurred by chance due to measurement variability. Subjects with longer follow-up and more measurements would have a higher chance of having any false-positive during the study. How can I account for this? I am aware of familywise error rate and false discovery rate procedures, but I am not sure if and how they apply in a time-to-event approach.I would imagine this scenario happen frequently in biomedical research, yet I didn't find clear solutions searching Google, Pubmed or this forum. I would appreciate any solutions or guidance about where to look for them. Thanks!","Creater_id":67509,"Start_date":"2016-08-13 10:04:54","Question_id":229683,"Tags":["survival","panel-data","cox-model","false-discovery-rate","frailty"],"Answer_count":0,"Last_activity":"2016-08-13 10:04:54","Link":"http://stats.stackexchange.com/questions/229683/comparison-of-longitudinal-change-in-several-parameters-in-same-individuals-vio","Creator_reputation":11}
{"_id":{"$oid":"5837a57fa05283111e4d538e"},"View_count":343,"Display_name":"Steve Powell","Question_score":5,"Question_content":"Analyzing educational datasets we have samples of children from samples of class in samples of schools - we have sampling weights, so I use the survey package e.g. to do a linear model. But this kind of design also requires looking at the mixed effects. But this isn’t possible using the survey package. I can do this in nlme – but then I don't know how to account for the weighting. I guess I could use the sample weights as predictors in nlme regressions but I don’t think that is correct.It seems that this kind of design (in fact any stratified survey sample which includes nested levels) needs analysing from both perspectives – (survey weights and mixed effects) at once – but the packages of choice for each of these perspectives, survey and nlme, each don’t seem to have slots for the other perspective.Can someone put me on the right track, or suggest another package which does both at the same time?","Creater_id":16546,"Start_date":"2012-11-05 06:10:59","Question_id":41917,"Tags":["r","mixed-model","multilevel-analysis","sample","survey-sampling"],"Answer_count":1,"Last_activity":"2016-08-13 10:04:20","Link":"http://stats.stackexchange.com/questions/41917/multi-stage-sampling-together-with-hierarchical-mixed-effects-models-which-r-p","Creator_reputation":26}
{"_id":{"$oid":"5837a57fa05283111e4d539b"},"View_count":43,"Display_name":"RayVelcoro","Question_score":5,"Question_content":"I have a dataset that I want to run propensity score analysis on. Using package TWANG in R, I plan to compute the propensity score and use it as IPTW. The variables that I put into the model are those I believe are confounded with treatment selectionThe question that I have is what to do about a confounder that is already balanced at baseline. Should I include it in my propensity score model? What good/harm does it do?For example, in my data (3 treatments). Surg 0/1 is something I believe  is confounded with treatment selection. I want to control for it, but it seems to already be balanced between all three groups.     tmt1 tmt2      var mean1 mean2 pop.sd std.eff.sz   p      ks ks.pval 15     1    2   surg:0 0.147 0.136  0.342      0.034 0.668 0.012   0.668         16     1    2   surg:1 0.853 0.864  0.342      0.034 0.668 0.012   0.668         33     1    3   surg:0 0.147 0.135  0.342      0.035 0.711 0.012   0.711         34     1    3   surg:1 0.853 0.865  0.342      0.035 0.711 0.012   0.711         51     2    3   surg:0 0.136 0.135  0.342      0.000 0.998 0.000   0.998         52     2    3   surg:1 0.864 0.865  0.342      0.000 0.998 0.000   0.998 ","Creater_id":84140,"Start_date":"2015-11-12 13:07:41","Question_id":181505,"Tags":["model-selection","propensity-scores","confounding","weighted-data"],"Answer_count":1,"Last_activity":"2016-08-13 09:58:02","Link":"http://stats.stackexchange.com/questions/181505/including-already-balanced-confounders-in-propensity-score-model","Creator_reputation":501}
{"_id":{"$oid":"5837a57fa05283111e4d53a8"},"View_count":28,"Display_name":"naginata","Question_score":2,"Question_content":"I need to compare results of one nation on several variables with the mean result of other nations that are participants in same research. Normally, the test would be a chi-squared or t-test (depending on variable compared). But the sample sizes of other nations vary, with some countries having 3 or 4 times more participants than others. I am aware that direct comparison would thus not be with the mean results of other nations, but mean results of all other participants. What can be done to get the relevant comparison with the mean results of other nations? ","Creater_id":127581,"Start_date":"2016-08-13 07:12:22","Question_id":229666,"Tags":["chi-squared","mean","unbalanced-classes"],"Answer_count":0,"Last_activity":"2016-08-13 09:52:51","Link":"http://stats.stackexchange.com/questions/229666/comparing-results-of-one-group-to-mean-result-of-several-groups-with-unequal-sam","Creator_reputation":11}
{"_id":{"$oid":"5837a57fa05283111e4d53aa"},"View_count":24,"Display_name":"LLOLLIPOPPI","Question_score":0,"Question_content":"I have 330 cases, 66 percent from one site and 34 percent from another. I'm looking to make comparisons between the two sites. Do I need to weight the data given that two thirds of the cases come from one site?","Creater_id":95161,"Start_date":"2015-11-15 09:11:56","Question_id":181857,"Tags":["experiment-design"],"Answer_count":0,"Last_activity":"2016-08-13 09:51:50","Link":"http://stats.stackexchange.com/questions/181857/data-weighting-scheme","Creator_reputation":1}
{"_id":{"$oid":"5837a57fa05283111e4d53ac"},"View_count":14,"Display_name":"Coding Mash","Question_score":1,"Question_content":"I am working on a research project in which I am using inverted index for terms and documents in one of the techniques. I am doing algorithmic analysis for all the methods so that I can compare and contrast them on asymptotic time bounds. For inverted indexes, I need to find out the average number of documents containing any term. I have looked at the relevant literature but could not find anything relevant. Heap's Law is present but it tells us something else.Do you have some idea on how can I get this value, apart from doing empirical evaluation on some datasets? Or if someone has done the algorithmic analysis for inverted indexes before that I can cite.","Creater_id":49617,"Start_date":"2016-08-13 09:47:50","Question_id":229682,"Tags":["algorithms","information-retrieval"],"Answer_count":0,"Last_activity":"2016-08-13 09:47:50","Link":"http://stats.stackexchange.com/questions/229682/average-number-of-documents-containing-a-term","Creator_reputation":58}
{"_id":{"$oid":"5837a57fa05283111e4d53ae"},"View_count":41,"Display_name":"Jef Van Alsenoy","Question_score":1,"Question_content":"Consider a model where:Y (response variable): continuous and both positive and negativeX1 (explanatory variable): continuous data, but with many zero valuesX2 (explanatory variable): binomially distributed data [0,100,1], but with many zero valuesIn planning a good approach I assume that:the many zeros for X1 (the continuous variable) will not be a problem as long as the residuals of my model are roughly normally distributed.However, I have no idea how to handle X2. I cannot use binomial regression because my Y is both negative and positive. Could somebody point me in the right direction on how to handle this variable?","Creater_id":123928,"Start_date":"2016-08-13 04:14:01","Question_id":229650,"Tags":["regression","binomial","zero-inflation"],"Answer_count":1,"Last_activity":"2016-08-13 08:16:26","Link":"http://stats.stackexchange.com/questions/229650/regression-with-zero-inflated-continuous-variable-zero-inflated-binomial-variab","Creator_reputation":26}
{"_id":{"$oid":"5837a57fa05283111e4d53ba"},"View_count":227,"Display_name":"fcop","Question_score":6,"Question_content":"I am wondering why it is said that multiple testing corrections are ''arbitrary'' and that they are based on a incoherent philosophy that  the veracity of one statement depends on which other hypotheses are entertainedsee e.g. answers and comments to What\u0026#39;s wrong with Bonferroni adjustments? and in particular the discussion between @FrankHarrell and @Bonferroni.  Let us (for simplicity and for the ease of the exposition) assume that we have two (independent) normal populations, independent and with known standard deviations but unknown means. Let (just as an example) say that these standard deviations are resp. . Joint testAssume we want to test the hypothesis  versus  at a significance level of  (the symbol  meaning 'and' while  means 'or'). We also have a random outcome  from the first population and  from the second population.  if  is true then the first random variable  and the second one  as we assumed independence it holds that the random variable  is  with . We can use this  as a test statistic and we will accept  if, for the observed outcomes  and  it holds that . In other words the acceptance region for this test is an ellipse centered at  and we have a density mass of  ''on top'' of this ellipse. Multiple testsWith multiple testing we will do two independent tests and ''adjust'' the significance level.  So we will perform two independent tests  versus  and a second test  versus  but with an adjusted significance level  that is such that  or  or  or  which yields . In this case we will accept  and  (and both together are equivalent to our ''original'' ) whenever  and So we conclude that, with multiple testing, the acceptance region for  has become a rectangle with center  and with a probability mass of  on top of it. ConclusionSo we find that, for a joint () test the geometrical shape of the acceptance region is an ellipse, while with multiple testing it is a rectangle.  The density mass ''on top'' of the acceptance region is in both cases 0.95. QuestionsSo what is then the problem with multiple testing ?  If there exists such a problem, then (see supra) the same problem should exist for joint tests or not ? The reason can not be that we prefer ellipses over rectangles does it ?","Creater_id":83346,"Start_date":"2016-08-10 07:32:05","Question_id":229193,"Tags":["hypothesis-testing","multiple-comparisons","bonferroni"],"Answer_count":3,"Last_activity":"2016-08-13 08:12:54","Link":"http://stats.stackexchange.com/questions/229193/whats-wrong-with-multiple-testing-correction-compared-to-joint-tests","Creator_reputation":3038}
{"_id":{"$oid":"5837a57fa05283111e4d53c9"},"View_count":10,"Display_name":"Moose","Question_score":0,"Question_content":"This question could apply to any many-features problem, but I am specifically interested in gene expression microarray data. I have a gene of interest that is differentially expressed between cases and controls. I would like to find genes with a similar expression profile. The final goal would be an enrichment test to see what these similar genes may have in common. My initial thought was to simply correlate all other genes with my gene of interest, and select an arbitrary number of the most correlated ones. However, one could also use a distance based approach. For example, here.Is there a theoretical or practical reason to select one approach over the other? ","Creater_id":32377,"Start_date":"2016-08-13 08:10:22","Question_id":229670,"Tags":["correlation","feature-selection","distance","genetics"],"Answer_count":0,"Last_activity":"2016-08-13 08:10:22","Link":"http://stats.stackexchange.com/questions/229670/finding-features-with-a-similar-pattern-as-a-feature-of-interest-e-g-genes","Creator_reputation":643}
{"_id":{"$oid":"5837a57fa05283111e4d53cb"},"View_count":73,"Display_name":"MiniQuark","Question_score":1,"Question_content":"Xavier initialization seems to be used quite widely now to initialize connection weights in neural networks, especially deep ones (see What are good initial weights in a neural network?).The original paper by Xavier Glorot and Yoshua Bengio suggests initializing weights using a Uniform distribution between  and  with  (where  and  are the number of connections going in and out of the layer we are initializing), in order to ensure that the variance is equal to . This helps ensure that the variance of the outputs is roughly equal to the variance of the inputs to avoid the vanishing/exploding gradients problem.Some libraries (such as Lasagne) seem to offer the option to use the Normal distribution instead, with 0 mean and the same variance.Is there any reason to prefer the Uniform distribution over the Normal distribution (or the reverse)? Some examples in TensorFlow's tutorials also use a truncated Normal distribution.My guess is that the uniform distribution guarantees that no weights will be large (and so does the truncated Normal distribution). Or perhaps it just doesn't change much at all.Any idea?","Creater_id":109561,"Start_date":"2016-08-13 08:07:28","Question_id":229669,"Tags":["normal-distribution","neural-networks","uniform","weights"],"Answer_count":0,"Last_activity":"2016-08-13 08:07:28","Link":"http://stats.stackexchange.com/questions/229669/when-should-i-use-the-normal-distribution-or-the-uniform-distribution-when-using","Creator_reputation":429}
{"_id":{"$oid":"5837a57fa05283111e4d53cd"},"View_count":286,"Display_name":"Max","Question_score":-1,"Question_content":"How can I calculate the Area Under Curve for a classifier of a polynomial label in Rapidminer? I could only find a performance operator for binomial labels that provides the AUC value. ","Creater_id":40556,"Start_date":"2014-11-18 02:11:24","Question_id":124496,"Tags":["classification","roc","auc","rapidminer"],"Answer_count":1,"Last_activity":"2016-08-13 07:34:36","Link":"http://stats.stackexchange.com/questions/124496/roc-auc-for-polynomial-labels","Creator_reputation":56}
{"_id":{"$oid":"5837a57fa05283111e4d53da"},"View_count":56,"Display_name":"ahra","Question_score":3,"Question_content":"Starting from  I am trying to algebraically show that it is equal to  using the fact that the variance of the sum equals to the sum of variances. I start by \\operatorname{Var}(\\overline{x})=\\operatorname{Var}\\left(\\frac1N\\sum_{i=1}^Nx_i\\right)then\\frac1N\\sum_{i=1}^N \\left(\\frac1N \\sum_{i=1}^Nx_i-\\mu\\right)_i^2 = \\frac1N \\sum_{i=1}^N \\left[\\frac1{N^2} \\left(\\sum_{i=1}^N x_i\\right)^2-\\frac{2\\mu}{N}\\sum_{i=1}^N x_i+\\mu^2 \\right]_iwhich becomes\\frac1N\\sum_{i=1}^N \\left[\\overline{x}^2-2\\mu \\overline{x}+\\mu^2\\right]_i = \\frac1N \\sum_{i=1}^N(\\overline{x}-\\mu)_i^2=\\frac{\\sigma^2}{N}","Creater_id":125486,"Start_date":"2016-08-13 04:51:17","Question_id":229653,"Tags":["variance"],"Answer_count":2,"Last_activity":"2016-08-13 07:25:57","Link":"http://stats.stackexchange.com/questions/229653/is-this-proof-of-operatornamevar-overlinex-frac-sigma2n-correct","Creator_reputation":149}
{"_id":{"$oid":"5837a57fa05283111e4d53e8"},"View_count":1110,"Display_name":"goro","Question_score":17,"Question_content":"I read the following paper: Perneger (1998) What's wrong with Bonferroni adjustments.The author summarized by  saying that Bonferroni adjustment have, at best, limited applications in biomedical research and should not be used when assessing evidence about specific hypothesis:  Summary points:      Adjusting statistical significance for the number of tests that have been performed on study data—the Bonferroni method—creates more problems than it solves  The Bonferroni method is concerned with the general null hypothesis (that all null hypotheses are true simultaneously), which is rarely of interest or use to researchers  The main weakness is that the interpretation of a finding depends on the number of other tests performed  The likelihood of type II errors is also increased, so that truly important differences are deemed non-significant  Simply describing what tests of significance have been performed, and why, is generally the best way of dealing with multiple comparisons  I have the following data set and I want to do multiple testing correction BUT I am unable to decide for the best method in this case.I want to know if it is imperative to do this kind of correction for all the data sets that contain lists of means and what is the best method for the correction in this case?","Creater_id":58684,"Start_date":"2014-10-16 13:43:51","Question_id":120362,"Tags":["hypothesis-testing","multiple-comparisons","bonferroni"],"Answer_count":7,"Last_activity":"2016-08-13 07:11:39","Link":"http://stats.stackexchange.com/questions/120362/whats-wrong-with-bonferroni-adjustments","Creator_reputation":367}
{"_id":{"$oid":"5837a57fa05283111e4d53fb"},"View_count":18,"Display_name":"giogix","Question_score":0,"Question_content":"My dataset has 3 features and 21 rows.The Output Y have 11 distinct values in an ordinal scale from 1 to 10.The 3 features are real numbers, in a range between 150 to 350.More precisely the output is the answer given by 21 patients to a question.The question is made after an experiment where a few bio-signals are measured.I'm trying to build a model to forecast the level of sickness (I don't have to be specific about the kind of sickness), given 3 different bio-signal measurements. The answer of the question is used as the output for the model.I'm considering to use the ordered logit method for this task.Questions:How can I know if my dataset meet the proportional odds assumptions?Some of the number in the questionnaire have never been used as an answer (for example nobody answered 4, 6 or 10). Is this by itself going to make the proportional odds assumption not valid?If the proportional odds assumption is not met, would is have sense to reduce the output from 11 distinct values to 3 or 4, grouping the answer which are \"close\" between each other?Final question/consideration:I don't understand the sense of the proportional odds assumption. In my case I'm measuring bio-signal of patients who almost always will declare a level of sickness higher than 4 or 5. The proportional odds assumption says that at least the half of the patients should be in the group of people who declared a level equal to 0, since  right?The previous formula comes from the Wikipedia's page of the ordered logit (Ordered_logit), but over there they consider  as the number of members in the population i. Does this fit also in my case? Or I should interpret  as the probability of answering i? (which would be may be the same)","Creater_id":124345,"Start_date":"2016-08-13 06:31:13","Question_id":229661,"Tags":["ordinal","biostatistics","odds-ratio","odds","ordered-logit"],"Answer_count":0,"Last_activity":"2016-08-13 06:31:13","Link":"http://stats.stackexchange.com/questions/229661/ordered-logit-with-missing-output-values-does-it-meet-the-proportional-odds-ass","Creator_reputation":108}
{"_id":{"$oid":"5837a57fa05283111e4d53fd"},"View_count":10,"Display_name":"R.Hes","Question_score":0,"Question_content":"I was reading a book about repeated measures analysis. While discussing the math and concept behind the repeated measures analysis for correlated data (gee), I came across the equation below. (See the image, please!) My question is not how it is related to gee; what I need to know is a sense or an imagination of this equation which is an extention of normal distribution. ","Creater_id":120570,"Start_date":"2016-08-13 06:26:40","Question_id":229660,"Tags":["regression","normal-distribution","gee","multivariate-normal"],"Answer_count":0,"Last_activity":"2016-08-13 06:26:40","Link":"http://stats.stackexchange.com/questions/229660/one-question-about-repeated-measures","Creator_reputation":36}
{"_id":{"$oid":"5837a57fa05283111e4d53ff"},"View_count":21,"Display_name":"user249613","Question_score":1,"Question_content":"I am looking for an option pricer in Excel or R package that can price a payoff that could be some formula on the underlying prices and interest rate. This means the option payoff could depend on multiple underlyings. An example of the payoffs could be, when we have let us say, three underlyings for a American Binary Down and in option:Here,  is the strike price or the price where the binary option becomes on.  is the rate of interest. It can be assumed constant if simple or can be any other interest rate process.  are the prices processes for the different underlyings. They are standard geometric brownian motions.  is the time to expiration of the option.  is the time when the price level is breached and the option becomes on.Please let me know if any such macro or addin is available for Excel or any package in R that can do this.If my question is not clear or if you need more details, please let me know.Thanks in advance,","Creater_id":80273,"Start_date":"2016-08-12 18:59:13","Question_id":229633,"Tags":["r","stochastic-processes","brownian"],"Answer_count":0,"Last_activity":"2016-08-13 06:16:23","Link":"http://stats.stackexchange.com/questions/229633/option-pricing-macro-addin-in-excel-or-r-function-to-capture-arbitrary-payoff-fo","Creator_reputation":161}
{"_id":{"$oid":"5837a57fa05283111e4d5401"},"View_count":30,"Display_name":"dreamon","Question_score":0,"Question_content":"When calculating impulse response functions for a VAR/VEC model in software, one will typically get the response of one endogenous variable to an orthogonalized shock to another variable, the size of the shock being one standard deviation of the impulse variable. What would be the simplest way of scaling the impulse response so that it represents the reaction of the response variable to a different shock size, e.g. a one unit change in the impulse variable?","Creater_id":61257,"Start_date":"2016-08-13 04:49:12","Question_id":229652,"Tags":["var","vecm","impulse-response"],"Answer_count":0,"Last_activity":"2016-08-13 04:49:12","Link":"http://stats.stackexchange.com/questions/229652/how-to-scale-var-vecm-impulse-responses","Creator_reputation":44}
{"_id":{"$oid":"5837a57fa05283111e4d5403"},"View_count":293,"Display_name":"LisaR","Question_score":1,"Question_content":"I have a question about what the difference is in how Stata and R compute ANOVAs. I have run exactly the same ANOVA in both softwares, but curiously get a different F-statistics for one of the predictors. I´m not too familiar with Stata, but as far as I understood it, I do a Type 2 SS ANOVA for both.  To understand my output, this is my model:Outcome variable is a continuous variable called vertrauen (=trust)predictor 1 is a 2-level factor called trustee in R and Goodguy in Statapredictor 2 is also a 2 level factor called Group in R and uw in Stata.This is the R output:   \u003em2-lm(vertrauen~trustee*Group,data=RTG.UWD.short.50)\u003e Anova(m2,type=\"2\")\u003eAnova Table (Type II tests)\u003eResponse: vertrauen\u003e              Sum Sq Df F value    Pr(\u003eF)      \u003etrustee       2.4928  1 24.5497    1.367e-05 ***  \u003eGroup         0.0030  1  0.0292    0.8651      \u003etrustee:Group 0.1137  1  1.1200    0.2963      \u003eResiduals     4.0617 40                        \u003e  \u003eSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  \u003eThis is the Stata output:. anova vertrauen uw Goodguy uw#Goodguy                         Number of obs =         44    R-squared     =  0.3912                         Root MSE      =    .318658    Adj R-squared =  0.3455                  Source | Partial SS         df         MS        F    Prob\u003eF              -----------+----------------------------------------------------                   Model |  2.6095358          3   .86984526      8.57  0.0002                         |                      uw |  .00296733          1   .00296733      0.03  0.8651                 Goodguy |  1.2981586          1   1.2981586     12.78  0.0009              uw#Goodguy |  .11373073          1   .11373073      1.12  0.2963                         |                Residual |  4.0617062         40   .10154266                -----------+----------------------------------------------------                   Total |   6.671242         43   .15514516    As you can see, the F-statistics for the Group (UW) main effect and for the Group (UW) x trustee (Goodguy) interaction are the same, but for the trustee (Goodguy) main effect they differ. In R it´s almost twice as high as in Stata. I tried to change the order of the predictor and the reference levels, but that didn´t change my R output.  Does anyone know what causes the difference in the F-statistic here? I´m really puzzled about it. I expected it to be the same.  Here is the Stata output without the interaction:  . anova vertrauen uw Goodguy                         Number of obs =         44    R-squared     =  0.3741                         Root MSE      =    .319124    Adj R-squared =  0.3436                  Source | Partial SS         df         MS        F    Prob\u003eF              -----------+----------------------------------------------------                   Model |   2.495805          2   1.2479025     12.25  0.0001                         |                      uw |  .00296733          1   .00296733      0.03  0.8653                 Goodguy |  2.4928377          1   2.4928377     24.48  0.0000                         |                Residual |   4.175437         41   .10183993                -----------+----------------------------------------------------                   Total |   6.671242         43   .15514516    And here is the R output without the interaction:\u003e m2.4-lm(vertrauen~trustee+Group,data=RTG.UWD.short.50)\u003e Anova(m2.4)Anova Table (Type II tests)Response: vertrauen          Sum Sq Df F value    Pr(\u003eF)    trustee   2.4928  1 24.4780 1.328e-05 ***Group     0.0030  1  0.0291    0.8653    Residuals 4.1754 41                      ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u003e   It´s the same, thus it has to do something with how the two softwares incorporate the interaction term.  I also tried to manually compute the interaction term and found something interesting:  Here is the R output:RTG.UWD.short.50trustee)*as.numeric(RTG.UWD.short.50vertrauen)RTG.UWD.short.50Group)RTG.UWD.short.50trustee)RTG.UWD.short.50$trustee       n missing  unique      44       0       2 bad (22, 50%), good (22, 50%) and from Stata:. sum vertrauen uw Goodguy    Variable |        Obs        Mean    Std. Dev.       Min        Max-------------+---------------------------------------------------------   vertrauen |         44    .5045969    .3938847    .000998          1          uw |         44    .2272727    .4239151          0          1     Goodguy |         44          .5    .5057805          0          1","Creater_id":115326,"Start_date":"2016-05-11 09:34:54","Question_id":212068,"Tags":["r","anova","stata","f-statistic"],"Answer_count":1,"Last_activity":"2016-08-13 03:34:32","Link":"http://stats.stackexchange.com/questions/212068/f-test-differences-stata-and-r","Creator_reputation":6}
{"_id":{"$oid":"5837a57fa05283111e4d5410"},"View_count":34,"Display_name":"broncoAbierto","Question_score":2,"Question_content":"I want to prove that a mixture of conjugate priors is itself conjugate. It does not look difficult, but I'm still a bit unsure when manipulating probabilities, especially in a Bayesian context. Is this proof correct?Consider the following prior. p(\\theta) = \\sum_k p(z=k)p(\\theta | z=k)The posterior after observing  isp(\\theta | D) = \\frac{p(\\theta, D)}{p(D)}By the law of total probability\\frac{p(\\theta, D)}{p(D)} = \\frac{\\sum_k p(z=k)p(\\theta ,D |z=k)}{p(D)}which by the chain rule of probability equals \\frac{\\sum_k p(z=k) p(D |z=k) p(\\theta | D,z=k)}{p(D)}Since p(\\theta | D) = \\sum_k p(z=k|D) p(\\theta | D, z=k) \\mbox{ Q.E.D.}","Creater_id":66341,"Start_date":"2016-08-13 03:23:44","Question_id":229648,"Tags":["machine-learning","bayesian","mixture","conjugate-prior"],"Answer_count":0,"Last_activity":"2016-08-13 03:34:25","Link":"http://stats.stackexchange.com/questions/229648/a-mixture-of-conjugate-priors-is-conjugate","Creator_reputation":244}
{"_id":{"$oid":"5837a57fa05283111e4d5412"},"View_count":1761,"Display_name":"amoeba","Question_score":35,"Question_content":"Principal component analysis (PCA) can be used for dimensionality reduction. After such dimensionality reduction is performed, how can one approximately reconstruct the original variables/features from a small number of principal components?Alternatively, how can one remove or discard several principal components from the data?In other words, how to reverse PCA?Given that PCA is closely related to singular value decomposition (SVD), the same question can be asked as follows: how to reverse SVD?","Creater_id":28666,"Start_date":"2016-08-09 16:52:47","Question_id":229092,"Tags":["pca","dimensionality-reduction","svd"],"Answer_count":1,"Last_activity":"2016-08-13 03:30:19","Link":"http://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com","Creator_reputation":29995}
{"_id":{"$oid":"5837a57fa05283111e4d541f"},"View_count":96681,"Display_name":"mariana soffer","Question_score":72,"Question_content":"Is there any GUI for R that makes it easier for a beginner to start learning and programming in that language?","Creater_id":1808,"Start_date":"2010-12-09 06:49:10","Question_id":5292,"Tags":["r","gui"],"Answer_count":14,"Last_activity":"2016-08-13 02:35:40","Link":"http://stats.stackexchange.com/questions/5292/good-gui-for-r-suitable-for-a-beginner-wanting-to-learn-programming-in-r","Creator_reputation":887}
{"_id":{"$oid":"5837a57fa05283111e4d5438"},"View_count":1819,"Display_name":"Jay khan","Question_score":0,"Question_content":"I am trying to build a linear regression model for my data which has following variables. [1] \"Productcode\"   \"Category\"    \"Month\"   \"Mode.of.operations\" \"sales\"   \"profit.margin\"      [7] \"Name\"         \"Packaging.content\"  \"Specifications\"     \"Unit       \"Origin\"   Now as some of my variables are non numeric e.g Origin has values which are names of cities and countries, Mode of operation has values (joint venture, reseller, distributor).  In non numeric variable i dont know how to represent it in my linear regression model. One way I can think is to assign numeric values to these variables e.g (Joint venture =1 , Reseller = 2 and Distribution =3) but then it won't be right because it implies Distribution is better or 3 times higher than Joint venture. Can anyone guide me how to solve this problem in R. ","Creater_id":87577,"Start_date":"2015-08-17 22:47:47","Question_id":167600,"Tags":["r","regression","categorical-data"],"Answer_count":2,"Last_activity":"2016-08-13 01:43:40","Link":"http://stats.stackexchange.com/questions/167600/linear-regression-on-non-numeric-variables-in-r","Creator_reputation":101}
{"_id":{"$oid":"5837a57fa05283111e4d5446"},"View_count":129,"Display_name":"iuppiter","Question_score":1,"Question_content":"It seems like I can learn the parameters just fine and find the posterior probabilities for the training data but I have no clue on how to make new predictions on new data. The problem in particular comes from the transition probabilities changing on covariates so it's not trivial to write code to predict new data.The standard approach is to define the (dependent) mixture and fit the model:mod \u0026lt;- depmix(EventTime ~ 1, data = data[1:40320,], nstates = 2, family=multinomial(\"identity\"), transition = ~ Count, instart = runif(2))fm \u0026lt;- fit(mod, emcontrol=em.control(classification=\"soft\", maxit = 60))What we have above should function similarly to a binary HMM as it is trying to classify whether an event occurred as a 1/0 dependent variable moving through the sequence. The transition covariate is a frequency count variable that should directly affect the transition probabilities of the states which should thereafter control the emission probabilities of the 1/0 dependent variable.It is possible to get the parameters of the model and set the parameters to another new model. However, there is no clear method of prediction, even though there should be somewhere in the guts of the library. modNew \u0026lt;- depmix(EventTime~1,data=data2,transition=~Count,nstates=2,family=multinomial(\"identity\"))modNew \u0026lt;- setpars(modNew,getpars(fm))Note, in the documentation it says it is possible to run the viterbi algorithm to generate states for new data. However, this is not particularly useful to me and it seems to perfectly fit the data suggesting it still learns to fit the new data.probs2 \u0026lt;- viterbi(modNew)Note, I am new to this topic. This stage of implementation is difficult for me, yet it somehow seems like a basic part of an analysis.","Creater_id":101091,"Start_date":"2016-08-12 21:57:47","Question_id":229638,"Tags":["r","hidden-markov-model","mixture"],"Answer_count":0,"Last_activity":"2016-08-13 01:30:55","Link":"http://stats.stackexchange.com/questions/229638/how-to-predict-state-probabilities-or-states-for-new-data-with-depmixs4-package","Creator_reputation":51}
{"_id":{"$oid":"5837a57fa05283111e4d5448"},"View_count":17,"Display_name":"GeeDeeJay","Question_score":0,"Question_content":"I'm very new to statistics and am trying to learn how to analyse the risk of a therapeutic side effect occurring given different data sources.For example, I'm trying to calculate the risk of a shared side effect occurring in one individual if two medicines (Medicine A and Medicine B), given they share the same side effects.What makes it a little more complicated is that there is varying data in the literature for the side effects. For example, there are 2 studies for the risk of side effects occurring in Medicine A: Study 1: 3% (N = 500)Study 2: 4.5% (N = 1000)And for Medicine B:Study 1: 4.8% (N=100)Study 2: 7.2% (N=150)The side effect probability is also dose-dependent, which makes it even more complicated.As I said, I'm new to statistics, so any pointer in the right direction would be greatly appreciated!","Creater_id":127099,"Start_date":"2016-08-09 03:12:22","Question_id":228947,"Tags":["mathematical-statistics","average","weighted-mean","weighted-data"],"Answer_count":1,"Last_activity":"2016-08-13 01:20:40","Link":"http://stats.stackexchange.com/questions/228947/weighted-averages-with-conditional-probabilities","Creator_reputation":1}
{"_id":{"$oid":"5837a57fa05283111e4d5455"},"View_count":68,"Display_name":"SanjanaS801","Question_score":1,"Question_content":"I have a dataset with unbalanced classes. Three classes make up about 60% of the data. Also, I have different test splits that cause an imbalance. For e.g:Train set:label_1 ... label_nTest set:label_1, label_3, label_9 This means that even though I have only 3 labels in my test set, it could potentially be predicted as 1 of n labels. So when I use sklearn.metrics.precision_recall_fscore_support, I get a matrix with a lot of zeros.My problem is that I need to get an average F-score across all classes, rather than a per-class value. However, just taking an average of the matrix returned from the above sklearn function will always be a very low value since there are so many zeros. On the other hand, taking an average over non-zero values does not make sense to me either since the total number of potential predictions should be the total number of classes.Is there a good way to take an average in this case? I've tried using the micro, macro and weighted average options but I am not sure which one is right.Could anyone please help me with this?","Creater_id":127554,"Start_date":"2016-08-12 18:10:26","Question_id":229630,"Tags":["machine-learning","scikit-learn","average","unbalanced-classes"],"Answer_count":1,"Last_activity":"2016-08-13 00:27:52","Link":"http://stats.stackexchange.com/questions/229630/best-way-to-average-f-score-with-unbalanced-classes","Creator_reputation":6}
{"_id":{"$oid":"5837a57fa05283111e4d5461"},"View_count":20,"Display_name":"user93892","Question_score":0,"Question_content":"I implemented the Kruskal-Wallis rank test and obtained a significant result. Next I need to do some post-hoc analysis, and I use the package PMCMR posthoc.kruskal.dunn.test(x=x, g=m, p.adjust.method=\"bonferroni\")I want to calculate the family-wise confidence intervals, but don't know how to do it. TukeyHSD() seems perfect, but it is only applied for aov fit. Does anyone have an idea for that?","Creater_id":93892,"Start_date":"2016-08-12 17:47:13","Question_id":229628,"Tags":["confidence-interval","kruskal-wallis","familywise-error"],"Answer_count":0,"Last_activity":"2016-08-12 23:02:08","Link":"http://stats.stackexchange.com/questions/229628/method-to-perform-pairwise-comparisons-after-kruskal-wallis-rank-test","Creator_reputation":58}
{"_id":{"$oid":"5837a57fa05283111e4d5463"},"View_count":42,"Display_name":"DP78","Question_score":1,"Question_content":"I have a question on a ACF/PACF result reported below.Using a daily time series composed by 953 values I have the following result plotting the acf and pacf results.My questions are:how can I interpret the fact that in the ACF there are a kind of pick at lag 7, 14, 21 ,ecc ? It seems drop down only at 600th laghow can I interpret the PACF result due to this stange behavior ?","Creater_id":80951,"Start_date":"2016-08-12 10:20:51","Question_id":229577,"Tags":["time-series","data-visualization","modeling","autocorrelation","autoregressive"],"Answer_count":1,"Last_activity":"2016-08-12 23:00:04","Link":"http://stats.stackexchange.com/questions/229577/acf-and-pacf-strange-behavior","Creator_reputation":27}
{"_id":{"$oid":"5837a57fa05283111e4d5470"},"View_count":128,"Display_name":"Steven L. Johnson","Question_score":2,"Question_content":"Looking at the National Weather Service forecast for daytime precipitation tomorrow, it shows a 70% chance of rain (\"showers likely\"). I interpret this as meaning that on average it will rain 7 out of 10 days with a similar forecast -- leaving 3 days of 10 with no rain.Looking at the hourly breakdown of the same forecast it shows a 50% to 70% of rain for each daytime hour (a 12 hour period). Simplifying to 50%... if I flipped a coin at the beginning of each hour, the odds of a heads are the same as the odds for rain (50%). Taking this further, that means that \"no rain today\" is as likely as \"12 heads in a row\" ... a far, far, smaller probability than the 30% implied by the 70% forecast for the day.How can I interpret the hourly forecast percentage to be consistent with the daily forecast?","Creater_id":4677,"Start_date":"2016-04-27 15:55:33","Question_id":209699,"Tags":["probability","forecasting"],"Answer_count":1,"Last_activity":"2016-08-12 22:55:45","Link":"http://stats.stackexchange.com/questions/209699/how-to-interpret-daily-vs-hourly-chance-of-rain-precipitation-forecasts","Creator_reputation":93}
{"_id":{"$oid":"5837a57fa05283111e4d547d"},"View_count":98,"Display_name":"Tal Galili","Question_score":2,"Question_content":"Let us have some response variable y which can only get 0 or 1, and some predictor space in  (for illustration let's say  and that we have  two explanatory variables, each one between -1 to 1).I can intuitively say that if the rule of the tree is only y = 1 if X1 \u003e 0 (and 0 otherwise), then the model is not very complex. However, if I say that in order for y to be equal to 1 I need that X1 \u003e 0 \u0026amp; X2 \u003e 0, than this model \"feels\" to be more \"complex\". Also, if y = 1 only when observations in  are within the radius of 1/2 (i.e. a circle that within it y = 1 and outside of it y = 0), then it is clear I will need infinite number of linear splits in order to create a model that knows when y is 0 and when it is 1. It would also seem to me to be a more \"complex\" model than the one where y = 1 only when observations in  are within the radius of 1/10000 (since then most of the observations would just be 0).Which type of measure(s) could help here?Thanks","Creater_id":253,"Start_date":"2016-08-11 09:19:03","Question_id":229378,"Tags":["classification","model-selection","cart","degrees-of-freedom"],"Answer_count":2,"Last_activity":"2016-08-12 22:04:15","Link":"http://stats.stackexchange.com/questions/229378/how-to-measure-model-complexity-in-the-context-of-classification-decision-trees","Creator_reputation":7686}
{"_id":{"$oid":"5837a57fa05283111e4d548b"},"View_count":548,"Display_name":"KAE","Question_score":4,"Question_content":"How can I take the cumulative sum of a 2D distribution, starting from the highest value in the distribution?Background: my 2D distribution is the probability of different combinations of wave heights and wave periods based on measurements made at an ocean site. Assume the distribution shows that the most common wave (highest probability, 1%) has a height of 3 meters and a period of 8 seconds. This combination represents the \"peak\" of a \"mountain\" in the distribution, i.e.  adjacent combinations of wave height/period occur often but less frequently (say 0.1%), and combinations further \"downhill\" occur even less frequently (say 0.01%). Non-zero probability is spread out over many bins of wave height and wave period so it still totals to 1.I want to be able to identify the sea states that occur 10% of the time, i.e. make a contour that encloses the height/period combinations that together add up to a probability of 10%. In other words, I can look at the contour and say \"10% of the time, wave heights are between 1 and 4 meters and periods are between 6 and 10 seconds\". I believe I have to contour the cumulative sum of the 2D distribution starting at the \"peak\" and working \"downhill\". What is confusing is that downhill extends in 2 directions since I am summing over 2 variables, height and period.Any advice is helpful since I am having trouble wrapping my mind around this.","Creater_id":18578,"Start_date":"2014-08-15 08:03:19","Question_id":112015,"Tags":["distributions"],"Answer_count":1,"Last_activity":"2016-08-12 21:28:55","Link":"http://stats.stackexchange.com/questions/112015/how-to-take-cumulative-sum-of-a-2d-distribution","Creator_reputation":208}
{"_id":{"$oid":"5837a57fa05283111e4d5498"},"View_count":17,"Display_name":"Lane","Question_score":0,"Question_content":"I would like to see if a change in scores is predictive of a health outcome Y. However, I'd also like to control for the baseline score. Is the following valid?Y = c+\\beta_1(X_{t=1}-X_{t=0})  + \\beta_2X_{t=0} + \\epsilonI know that the model can be rewritten as:Y=c+\\beta_1X_{t=1}+(\\beta_2-\\beta_1)X_{t=0} + \\epsilonCan I still interpret  as the effect of a 1 unit change in X on Y?","Creater_id":127549,"Start_date":"2016-08-12 16:31:08","Question_id":229616,"Tags":["regression","panel-data","time-varying-covariate"],"Answer_count":1,"Last_activity":"2016-08-12 16:39:54","Link":"http://stats.stackexchange.com/questions/229616/is-it-valid-to-interpret-the-coefficient-on-a-difference-score-while-controlling","Creator_reputation":1}
{"_id":{"$oid":"5837a57fa05283111e4d54a4"},"View_count":206,"Display_name":"BigData","Question_score":0,"Question_content":"I am trying to mine association rules from my transaction dataset and I have questions regarding the support, confidence and lift of a rule.Assume we have rule like {X} -\u003e {Y}I know that support is P(XY), confidence is P(XY)/P(X) and lift is P(XY)/P(X)P(Y), where the lift is a measurement of independence of X and Y (1 represents independent)However, I just don't know how to interpret rules with these indicators. I have rules with high support, high confidence and low lift, is that a good rule ?Since high confidence represents strong association and high support represents how convincing their association are. So high confidence + high support = good rule and we can ignore lift?If I am going to order / rank my rules and pick, let say the best 10 to examine, which indicator should be chosen as the ranking variable?","Creater_id":125543,"Start_date":"2016-08-12 05:23:50","Question_id":229523,"Tags":["data-mining","association-rules"],"Answer_count":1,"Last_activity":"2016-08-12 16:19:25","Link":"http://stats.stackexchange.com/questions/229523/association-rules-support-confidence-and-lift","Creator_reputation":17}
{"_id":{"$oid":"5837a57fa05283111e4d54b0"},"View_count":25,"Display_name":"John","Question_score":2,"Question_content":"I have a finite space of 'data' (rows) that is fairly large (3 billion) and I want to check how many 'Positives' there are in this data set. However, checking the data is time consuming from a computational standpoint. If I have reason to believe that the data is i.i.d and each row has an equal likelihood of being a 'Positive' how can I calculate the sample size required to get a reasonable estimate on the TOTAL NUMBER (not just proportion in a Binomial Distribution) of 'Positives' in the data?Things I have tried / other caveats:I have looked into Binomial sample size calculation, but I'm not sure how this extends to the case where I am just looking for counts. I am guessing that when I multiply the estimated probability by the 3 billion, that it will distort the confidence intervals as it adds more variance.I have looked into doing a Poisson sample size calculation but can't find many resources online that I can understand.Do I have to assume that data is i.i.d in the distribution of 'Positives'? Intuitively I can understand why it wouldn't matter, since a large enough sample would factor in an aggregate probability of success.","Creater_id":127545,"Start_date":"2016-08-12 15:31:15","Question_id":229608,"Tags":["statistical-significance","confidence-interval","sampling","binomial","poisson"],"Answer_count":1,"Last_activity":"2016-08-12 16:11:04","Link":"http://stats.stackexchange.com/questions/229608/sample-size-calculation-for-poisson-binomial-type-data","Creator_reputation":11}
{"_id":{"$oid":"5837a57fa05283111e4d54bd"},"View_count":30,"Display_name":"siby","Question_score":1,"Question_content":"Is it correct to classify fuzzy logic under Artificial Intelligence i.e. can fuzzy logic be considered a concept under the purview of of AI. If not how can we classify fuzzy logic.","Creater_id":127022,"Start_date":"2016-08-12 15:28:33","Question_id":229607,"Tags":["terminology","artificial-intelligence","fuzzy"],"Answer_count":1,"Last_activity":"2016-08-12 15:53:39","Link":"http://stats.stackexchange.com/questions/229607/fuzzy-logic-and-artificial-intelligence","Creator_reputation":72}
{"_id":{"$oid":"5837a57fa05283111e4d54ca"},"View_count":41,"Display_name":"iacobus","Question_score":1,"Question_content":"This isn't a problem with correlation between predictors - I have two models, each considers only one of the variables. That is the only difference between the models.  I'm estimating the probability of an diagnosis given some confounders and a measure of monthly temperature. I have two possible temperature definitions I'm considering: monthly average temperature and monthly average high temperature. I don't expect the response to temperature to be linear, so I broke average temperature into 5 degree bins with bottom and top coding at \u0026lt; 40 and \u003e 90. I did the same with average high temperature but shifted the bins slightly with bottom and top coding \u0026lt; 50 and \u003e 100. I estimate the first logistic model event ~ age + sex + ... + mean_temp_groupand get the response I'd expect from my theorized process. However, I'd prefer to report the results using mean high temperature since average temperature is misleadingly low (average temp of 70, for instance, is pretty warm with highs in the 80s but people think \"70 degrees? That's wonderful!\"). So I estimate the same model but instead replace mean_temp_group with mean_high_group:event ~ age + sex + ... + mean_high_groupand the results don't match either my theory or what I saw with mean_temp_group. That seems weird given how similar the two variables are. The average and average high variables have a correlation coefficient of 0.9939. In essence the average high is the average plus a constant (on average, 9.4 degrees with a standard deviation of 2.1). At first I assumed this was a problem with the code, so I re-pulled the data (still have the same problem and the data extraction seems to be accurate). I also took the model with mean_temp_group and edited the formula in place to read mean_high_group lest I omitted/included a different variable between the models (I didn't). I assume it has something to do with the binning or something along those lines - any ideas? I'm very confused by two variables that basically appear to be an additive shift of each other giving very different results. ","Creater_id":73659,"Start_date":"2016-08-12 13:18:22","Question_id":229598,"Tags":["r","regression","generalized-linear-model"],"Answer_count":1,"Last_activity":"2016-08-12 15:52:35","Link":"http://stats.stackexchange.com/questions/229598/two-models-estimated-using-two-different-but-strongly-related-predictors-r-0","Creator_reputation":238}
{"_id":{"$oid":"5837a57fa05283111e4d54d7"},"View_count":40,"Display_name":"Chuck Haas","Question_score":2,"Question_content":"I am running k-fold cross validation on a complex neural network model.  I often can get the situation where on a particular k-fold, the SSE (sum of squared prediction errors) for the reduced data set plus the SSE for the predicted subset is \u0026lt; the SSE for the original full data set.My sense is that something is wrong.  Am I correct or not?Example:full data set SSE = 100fit to 90% of data = 75predicted SSE of remaining 10% = 15","Creater_id":127532,"Start_date":"2016-08-12 12:13:23","Question_id":229592,"Tags":["neural-networks","cross-validation"],"Answer_count":1,"Last_activity":"2016-08-12 15:50:56","Link":"http://stats.stackexchange.com/questions/229592/sse-for-subset-much-better-than-full-data","Creator_reputation":31}
{"_id":{"$oid":"5837a57fa05283111e4d54e4"},"View_count":38,"Display_name":"Benjamin","Question_score":3,"Question_content":"Let the random variables  and . Take the samples  and  for some natural numbers  and . Define the sample mean , and the sample variance , and define  and  similarly.Consider the -test statistic for testing : T = \\frac{\\left( \\bar{x} - \\bar{y} \\right) - \\delta}{\\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}}.Using the test statistic , I wish to \"prove\" that . Of course, this can't be done directly since  is continuous over .However, I've been given the information that there's some tolerance allowed so that I'm only trying to prove that  for some given positive . Therefore I wish to test the hypothesis \\begin{cases} H_0 : | \\mu_1 - \\mu_2 | \u0026lt; \\epsilon \\\\ H_1: | \\mu_1 - \\mu_2 | \\geq \\epsilon \\end{cases} At the end of the day, I want to be able to state that the null hypothesis  is true with some confidence. To do this, if I fail to reject , I will state that \"I am % confident that the mean difference is within the given tolerance of zero\", where  is the true negative rate.There are problems here, though. I'm not sure how to perform the -test when there's a composite null (unless I choose the -value very conservatively by taking the largest -value over the null), and, again, I'm not sure how to most reasonably evaluate the true negative rate over the composite alternative.What should I do?","Creater_id":59059,"Start_date":"2016-08-12 15:45:58","Question_id":229609,"Tags":["hypothesis-testing","t-test"],"Answer_count":0,"Last_activity":"2016-08-12 15:45:58","Link":"http://stats.stackexchange.com/questions/229609/accept-null-in-t-test","Creator_reputation":135}
{"_id":{"$oid":"5837a57fa05283111e4d54e6"},"View_count":12,"Display_name":"R.M.","Question_score":0,"Question_content":"From a certain point of view, a standard linear regression finds a model which minimizes the Pearson correlation between the predicted value of the response variable and the true value of the response variable. Are there any (standard) modifications to the procedure which instead minimize a rank-order correlation (e.g. the Spearman rho or Kendall tau)? Or more generally, are there approaches to generalized linear models where the functional form of the link function is unknown/unspecified?I have a situation where I think a generalized linear model is likely to be appropriate ... except that I don't know what the functional form for the link function would be. I can, however, specify the constraint that the link function will be monotonic. Hence the hope that I can fit a (generalized) linear model to rank ordering (which will just enforce monotonicity) rather than a standard fit (which will also attempt to match absolute magnitudes of the response variable).An approach which can be regularized (as in lasso or ridge) would be much preferred.Note: My situation is distinct from that for ordinal regression. My response variable is a continuously-valued real function, rather than a small set of discrete integer levels as is the case for ordinal regression.","Creater_id":69382,"Start_date":"2016-08-02 13:56:49","Question_id":226955,"Tags":["generalized-linear-model","rank-correlation"],"Answer_count":1,"Last_activity":"2016-08-12 15:16:50","Link":"http://stats.stackexchange.com/questions/226955/can-rank-ordering-of-response-variables-be-used-with-generalized-linear-models","Creator_reputation":356}
{"_id":{"$oid":"5837a57fa05283111e4d54f3"},"View_count":5371,"Display_name":"geotheory","Question_score":12,"Question_content":"I'm working in R through an excellent PCA tutorial by Lindsay I Smith and am getting stuck in the last stage.  The R script below takes us up to the stage (on p.19) where the original data is being reconstructed from the (singular in this case) Principal Component, which should yield a straight line plot along the PCA1 axis (given that the data only has 2 dimensions, the second of which is being intentionally dropped).d = data.frame(x=c(2.5,0.5,2.2,1.9,3.1,2.3,2.0,1.0,1.5,1.1),               y=c(2.4,0.7,2.9,2.2,3.0,2.7,1.6,1.1,1.6,0.9))# mean-adjusted values dx - mean(dy_adj = dy)# calculate covariance matrix and eigenvectors/values(cm = cov(d[,1:2]))#### outputs ##############          x         y# x 0.6165556 0.6154444# y 0.6154444 0.7165556##########################(e = eigen(cm))##### outputs ############### vectors#          [,1]       [,2]# [1,] 0.6778734 -0.7351787# [2,] 0.7351787  0.6778734############################ principal component vector slopess1 = evectors[2,1] # PC1s2 = evectors[2,2] # PC2plot(dy_adj, asp=T, pch=16, xlab='x', ylab='y')abline(a=0, b=s1, col='red')abline(a=0, b=s2)# PCA data = rowFeatureVector (transposed eigenvectors) * RowDataAdjust (mean adjusted, also transposed)feat_vec = t(e$vectors)row_data_adj = t(d[,3:4])final_data = data.frame(t(feat_vec %*% row_data_adj)) # ?matmult for detailsnames(final_data) = c('x','y')#### outputs ################ final_data#              x           y# 1   0.82797019 -0.17511531# 2  -1.77758033  0.14285723# 3   0.99219749  0.38437499# 4   0.27421042  0.13041721# 5   1.67580142 -0.20949846# 6   0.91294910  0.17528244# 7  -0.09910944 -0.34982470# 8  -1.14457216  0.04641726# 9  -0.43804614  0.01776463# 10 -1.22382056 -0.16267529############################# final_data[[1]] = -final_data[[1]] # for some reason the x-axis data is negative the tutorial's resultplot(final_data, asp=T, xlab='PCA 1', ylab='PCA 2', pch=16)This is as far as I've got, and all OK so far.  But I can't figure out how the data is obtained for the final plot - the variance attributable to PCA 1 - which Smith plots as:This is what I've tried (which ignores adding the original means):trans_data = final_datatrans_data[,2] = 0row_orig_data = t(t(feat_vec[1,]) %*% t(trans_data))plot(row_orig_data, asp=T, pch=16).. and got an erronous:.. because I've lost a data dimension somehow in the matrix multiplication.  I'd be very grateful for an idea what's going wrong here.* Edit *I wonder if this is the right formula:row_orig_data = t(t(feat_vec) %*% t(trans_data))plot(row_orig_data, asp=T, pch=16, cex=.5)abline(a=0, b=s1, col='red')But I'm a little confused if so because (a) I understand the rowVectorFeature needs to be reduced to the desired dimensionality (the eigenvector for PCA1), and (b) it doesn't line up with the PCA1 abline:Any views much appreciated.","Creater_id":13849,"Start_date":"2014-03-17 12:21:23","Question_id":90331,"Tags":["r","pca"],"Answer_count":3,"Last_activity":"2016-08-12 15:14:59","Link":"http://stats.stackexchange.com/questions/90331/step-by-step-implementation-of-pca-in-r-using-lindsay-smiths-tutorial","Creator_reputation":203}
{"_id":{"$oid":"5837a57fa05283111e4d5502"},"View_count":1692,"Display_name":"Serendipity","Question_score":3,"Question_content":"I've performed PCA on face images dataset and I'm not sure how can I use the most informative principal components to show the \"reduced\" image.The original image is 96*96 pixels (96*96 = 9216) and I use a sample of 70 images here (70 rows and 9216 column). We get 70 principal components (min{num of samples, num of features}=70).  How can I re-construct a 96x96 image in order to show the eigenfaces? I want to show my students how the eigenvectors \"predict\" the real data.The dataset I'm using can be downloaded here.The code:install.packages(\"foreach\")file ='C:\\\\I\\\\Love\\\\Data Science\\\\face.training.csv'data_all = read.csv(file , stringsAsFactors=F)dim(data_all) #7049   31# use only 70 first imagesdata = data_all[1:70,]names(data)str(data)# extract the images dataim.train \u0026lt;- dataImage = NULL# each image is a vector of 96*96 pixels (96*96 = 9216).library(foreach)im.train \u0026lt;- foreach(im = im.train, .combine=rbind) %dopar% {  as.integer(unlist(strsplit(im, \" \")))}# im.train is a matrix of pixels 70x9216# show picture number 2im \u0026lt;- matrix(data=rev(im.train[2,]), nrow=96, ncol=96)image(1:96, 1:96, im, col=gray((0:255)/255))# Apply PCApca \u0026lt;- prcomp(im.train,                 center = TRUE,                 scale. = TRUE) ## using correlation matrix# There are in general min(n − 1, p) informative principal components in a data set with n observations and p variables. Hence, pcasdev# A numeric matrix which provides the data for the principal components analysispcax)# The print method returns the standard deviation of each of the PCs, # and their rotation (or loadings), which are the coefficients of the linear combinations of the continuous variables.print(pca)#The summary method describe the importance of the PCs.summary(pca)#The first row describe again the standard deviation associated with each PC. #The second row shows the proportion of the variance in the data explained by each component #while the third row describe the cumulative proportion of explained variance. # plot method returns a plot of the variances (y-axis) associated with the PCs (x-axis). # useful to decide how many PCs to retain for further analysis. plot(pca, type = \"l\")","Creater_id":60522,"Start_date":"2014-12-10 07:44:08","Question_id":127502,"Tags":["r","pca","image-processing"],"Answer_count":1,"Last_activity":"2016-08-12 15:11:55","Link":"http://stats.stackexchange.com/questions/127502/how-to-reconstruct-an-image-after-performing-pca-on-face-image-dataset-eigenfac","Creator_reputation":87}
{"_id":{"$oid":"5837a57fa05283111e4d550f"},"View_count":96,"Display_name":"Lisa","Question_score":2,"Question_content":"I'm unsure how to calculate the effect size after applying a Wilcoxon rank-sum test. I'm using scipy.stats.ranksums, where the outputs are z, p. Looking at the implementation of the test, it seems z is assumed to be normally distributed - could I just calculate Cohen's d as discussed here?I'm confused since Wikipedia offers alternative effect sizes for these non-parametric tests, but then also, the article states that the Wilcoxon rank-sum and the Mann-Whitney-U tests are the same, while scipy clearly implements them quite differently. Unfortunately, I cannot apply the MWU test since some of my comparisons involve samples with zero variance which leads to errors with MWU but not rank-sums.","Creater_id":39507,"Start_date":"2016-04-19 03:18:37","Question_id":208139,"Tags":["effect-size","wilcoxon","scipy"],"Answer_count":1,"Last_activity":"2016-08-12 15:04:47","Link":"http://stats.stackexchange.com/questions/208139/effect-size-for-wilcoxon-rank-sum-test","Creator_reputation":202}
{"_id":{"$oid":"5837a57fa05283111e4d551c"},"View_count":29,"Display_name":"kap","Question_score":0,"Question_content":"I am using Gaussian Mixture Models (GMM) to fit a small data set with ~60 observations and 4 dimensions. This data was generated from the raw data with 14 dimensions after retaining principal components with eigen values \u003e= 1. I am using the mclust package with a small modification that allows to calculate AICc (following Burnham and Anderson) for each fit. It calculates the best fitting model by searching over a given number of clusters (e.g. G=1:10).The question is how to set the upper limit on the number of clusters to search for? Here is the plot for my data when using the model \"EEE\" in mclust. The AICc drops dramatically at k=11. This is the best fit across all the models provided in mclust. This does not \"look\" right but it will be better to have a good justification to limit the search space.","Creater_id":8757,"Start_date":"2016-08-05 11:57:29","Question_id":228490,"Tags":["r","clustering","gaussian-mixture"],"Answer_count":1,"Last_activity":"2016-08-12 14:57:48","Link":"http://stats.stackexchange.com/questions/228490/upper-limit-on-number-of-clusters-in-gmm","Creator_reputation":16}
{"_id":{"$oid":"5837a580a05283111e4d5529"},"View_count":6514,"Display_name":"learner","Question_score":0,"Question_content":"I am facing two problems while using caret package in R. I am reproducing an example below:library(mlbench)library(caret)set.seed(998)data(Sonar)   #Random data, just for illustration purposeSonar= Sonar[, 1:6] #Selected first 6 columsn only for showing an example. I am assuming V6 to be response.head(Sonar)inTraining \u0026lt;- createDataPartition(Sonar$V6, p = 0.75, list = FALSE)training \u0026lt;- Sonar[inTraining, ]testing \u0026lt;- Sonar[-inTraining, ]modelFit \u0026lt;- train( V6~.,data=training, method=\"rpart\" )  varImp(modelFit)a. How to extract top three (3) variables from varImp output? I tried to order the variables but for any reason, its not working for me. b. Also, why the following code doesn't work for \"randomForest\"?modelFit \u0026lt;- train( V6~.,data=training, method=\"rf\" )  varImp(modelFit)\u0026gt; varImp(modelFit) Rerun with Debug Error in varImp[, \"%IncMSE\"] : subscript out of bounds ","Creater_id":31017,"Start_date":"2014-08-28 12:15:04","Question_id":113618,"Tags":["r","caret"],"Answer_count":3,"Last_activity":"2016-08-12 14:39:52","Link":"http://stats.stackexchange.com/questions/113618/caret-package-in-r-get-top-variable-of-importance","Creator_reputation":220}
{"_id":{"$oid":"5837a580a05283111e4d5538"},"View_count":25,"Display_name":"Batool","Question_score":0,"Question_content":"I am trying to evaluate the integrity of a model using various accuracy parameters. I am trying to predict the retirement events in a given year at a company. The data set is highly imbalanced as we have very few retirement events each year. In the following example, minority class '1' (also the negative class) indicates retirement events. Here are the 2 models that were run using R and the results were obtained using ConfusionMatrix function in R:Model A:Adaptive boosting ApproachModel B:Random Forest Approach (additionally, I changed the response variable to all \"Voluntary Terminations\" rather than just \"retirements\" so as to have less imbalance in data, and used a segment of the entire dataset)QUESTIONS:Even though both models have different response variables, I am trying to figure out which model is more reliable in its prediction. Type II error (False Negative) is more costly. We want to do a good job in rightfully predicting retirements, however, Type I error (False Positives) should not be greater than the True Negatives.Model A has higher accuracy, sensitivity and specificity than Model B. Does that make it's prediction performance better than Model B? If I look at the Negative Prediction value, Model A is doing a very poor job. That is to say, if Model A predicts retirements, only 9.8% of those will be true retirements. So does that make me reject Model A and go with Model B instead? If so, in machine learning, why do we lay such heavy emphasis on balanced accuracy, sensitivity, specificity and F scores? Non of these measures tell us much about the Positive and Negative Prediction values.","Creater_id":53333,"Start_date":"2016-08-12 11:28:36","Question_id":229588,"Tags":["accuracy","sensitivity","specificity","imbalanced","f-statistic"],"Answer_count":0,"Last_activity":"2016-08-12 14:13:30","Link":"http://stats.stackexchange.com/questions/229588/probing-models-accuracy-in-imbalanced-dataset-using-negative-prediction-values","Creator_reputation":20}
{"_id":{"$oid":"5837a580a05283111e4d553a"},"View_count":48,"Display_name":"Ereck","Question_score":1,"Question_content":"I am not sure about some parts of the following plots:-- I do not understand what does the expression on x-axis means (first figure) and how to interpret that.-- What is the bars and the points in figure 2 and 3 refer to.Could you please help me ?Thanks,","Creater_id":123446,"Start_date":"2016-08-12 12:22:37","Question_id":229593,"Tags":["cross-validation","lasso","lars"],"Answer_count":1,"Last_activity":"2016-08-12 13:45:54","Link":"http://stats.stackexchange.com/questions/229593/cross-validation-and-lasso-plot","Creator_reputation":62}
{"_id":{"$oid":"5837a580a05283111e4d5547"},"View_count":23,"Display_name":"Quickbeam2k1","Question_score":0,"Question_content":"Hey there I have a marketing campaign (catalogues to customers) and want to conduct two experiments simultaneously.For the first experiment I want to try two different layouts for the catalogue.Let's call the experiment C with + refers to new layout and - refers to standard layout. My target here might be the average profit or response rate.At the same time, I want to test two postal service companies. Let's call this experiment PC and + refers to the delivery of the mailing by a new postal service company and - to the delivery by the company that is already in use (this company can ship to every customer). Here, I am interested in the percentage of succesfully delivered mailings or response rate after a certain, short period of time.So, of course I could split my customer base randomly into 4 groups and conduct the experiments completely separately. Particularly, I am not considered in side effects.Unfortunately, it is required, that roughly half of the customers shall take part in the layout test (C). Additionally, the test for PC shall also be conducted on as many customers as possible.Here, one could use an orthogonal setup likeC | PC------+ | ++ | -- | +- | -right?So where is the blocking?There are two restrictions I need to consider/block:Firstly, the new postal service company can only address roughly 70% of the customers, (this depends essentially on the region where those customers live in). Let's call this blocking variable P where + refers to customers that the new postal delivery company can ship to and - where the company can'tSecondly, there are two customer groups that receive catalogues of different length. The customers with that receive longer catalogues are considered to be \"better\" than the other catalogues. Let's call this blocking variable A, where + refers to the prime customers with longer catalogues, and - to the regular customers.So, let us assume for the moment that P is not an issue (i.e. all customers can be adressed by both companies) and we block by variable A.Then, an orthogonal design looks like thisA | C | PC----------+ | + | ++ | + | -+ | - | ++ | - | -- | + | +- | + | -- | - | +- | - | -right?So the question is, how could I incorporate P?For the test of PC, I am, by construction, restricted to the set where P = + (the set where both companies can ship to, remember that the old company can ship to every customer). If P = - I a test of PC does not make sense!Hence, the layout given above, restricted to the customers where P=+ is, right?However, when testing for C this approach leaves some customers not randomly out.However, the left out customers will be associated with PC= - (they get their catalogues by the old postal service company). If I assign to those customers the treatment C randomly, I have also the second layout, only that the group sizes are shifted in comparison to the PC-test.Would such a procedure be valid? If not, is there any setup to conduct both experiments simultaneously?I apologize for the lengthy question. I hope the intention is clear now?","Creater_id":71975,"Start_date":"2016-08-11 03:43:03","Question_id":229328,"Tags":["experiment-design"],"Answer_count":0,"Last_activity":"2016-08-12 13:19:49","Link":"http://stats.stackexchange.com/questions/229328/two-simultaneous-experiments-with-blocking","Creator_reputation":188}
{"_id":{"$oid":"5837a580a05283111e4d5549"},"View_count":53,"Display_name":"ssm","Question_score":0,"Question_content":"I have 13 features in a classification task and I use Random Forest, L1 logistic regression and L2 logistic regression for as separate classifiers and would like to compare their performance. Although they have similar performances, when I look at the feature importance from Random Forest and logistic regression (based on coefficients), they have slight difference, particularly the best feature. I am only interested in the best 3 feature and in all the 3 classifiers, these 3 are the same, but the first (best of the best) is different. Can you explain me if this is not undesirable, and if possible explain why this can happen. Thank you.   ","Creater_id":38393,"Start_date":"2016-03-24 15:21:31","Question_id":203565,"Tags":["logistic","random-forest","lasso","regularization","ridge-regression"],"Answer_count":1,"Last_activity":"2016-08-12 13:00:26","Link":"http://stats.stackexchange.com/questions/203565/difference-of-feature-importance-from-random-forest-and-regularized-logistic-reg","Creator_reputation":30}
{"_id":{"$oid":"5837a580a05283111e4d5556"},"View_count":37,"Display_name":"Justin","Question_score":0,"Question_content":"My dataset (with the dependent variable being a count variable) has two features that are different from a standard Poisson regression. I haven't been able to find a more advanced model that accounts for both irregularities, but have found alternatives that can deal with one or the other irregularity. I'll outline the situation below, but my questions are:Given the features below, is there some hybrid model I could use that simultaneously accounts for both issues?If not, is my strategy below reasonable in the circumstances?I have an experiment where people are given a cue word and have to respond with another word that they think other people will pick too. I.e. they have to try coordinate. The dependent count variable is simply the number of people picking each word. The minimum value of this variable, then, is 1. However, while some people were able to coordinate well, many more were not. This means there are a lot of data points at y=1. A zero-truncated poisson (e.g. in R, vglm(y~x,family=pospoisson)) explicitly models the fact that the minimum count is 1 rather than 0 but doesn't address over-inflation of this minimum count. On the other hand, a hurdle model (e.g. in R hurdle(y~x,dist=poisson)) explicitly models inflated minimum counts, but it expects a minimum count of 0 rather than 1, so to use it for my data, I'd have to use hurdle(y-1~x), so it doesn't address the fact that I have a positive poisson distribution. This doesn't seem conceptually bothersome to me, since a count of 1 (only one person picked the word) means there has been a failure to coordinate. I don't know of a model that can address both issues simultaneously. I have asked another question here about whether shifting y-1 so that I can use the hurdle model is in principle problematic, or similar enough for this to be informative. My strategy so far has been to do all three (regular poisson, zero-truncated poisson, hurdle model with shifted values), get boot strapped confidence intervals for the model parameters to show that they're not significantly different, and conclude that my predictor consistently has a significant effect that is similar across these models, and that over-inflation and having a positive poisson distribution thus do not undermine the results of the basic model. Is that a reasonable strategy, or can you recommend something better?The dependent count variable ranges from 1 to 18 (20 people were given each cue word, but there were no cases where all 20 gave the same response), with most of the weight at 1. There is one predictor, which ranges continuously from 0 to 1. For extremely low values of that predictor, I expect no coordination (y=1 meaning only one person generated the response). For low values of the predictor, I expect small amounts of coordination, and more people should coordinate over a response as the value of the predictor increases. ","Creater_id":127487,"Start_date":"2016-08-12 08:14:04","Question_id":229555,"Tags":["regression","poisson","zero-inflation"],"Answer_count":0,"Last_activity":"2016-08-12 12:23:12","Link":"http://stats.stackexchange.com/questions/229555/poisson-regression-hurdle-models-and-zero-truncated-models","Creator_reputation":86}
{"_id":{"$oid":"5837a580a05283111e4d5558"},"View_count":39,"Display_name":"sfortney","Question_score":1,"Question_content":"I am looking for a reasonable estimate of how many total connections (weights) there are in a given neural net. My prior would be that if it is a vanilla ANN with 3 layers of 1200 neurons apiece it would be: (1200^2)*2 . Since each neuron on each layer connects to every other neuron there are 1200^2 between each layer. On a 3 layer ANN there are 2 \"in-between\" layers. Is this correct? Also, how would this change for RNN's with self-directed loops (and LSTM RNN's)?","Creater_id":127530,"Start_date":"2016-08-12 11:50:14","Question_id":229591,"Tags":["neural-networks","lstm","rnn","weights"],"Answer_count":1,"Last_activity":"2016-08-12 12:23:04","Link":"http://stats.stackexchange.com/questions/229591/counting-the-number-of-total-connections-weights-in-a-deep-neural-net","Creator_reputation":108}
{"_id":{"$oid":"5837a580a05283111e4d5565"},"View_count":61,"Display_name":"Sean001","Question_score":0,"Question_content":"I have a data set with three columns, say, ,  and  of which  is the dependent variable (on  and ). I need to know how to use any software (I have MATLAB) to develop a model in the form:\\hat Y=f(X_1,X_2)=\\frac {1+aX_1+bX_2}{1+cX_1+dX_2}where ,,,and  are constant coefficients.Can someone help me with a step-by-step guide? ","Creater_id":126601,"Start_date":"2016-08-12 10:01:59","Question_id":229572,"Tags":["regression","nonlinear"],"Answer_count":1,"Last_activity":"2016-08-12 11:53:47","Link":"http://stats.stackexchange.com/questions/229572/how-do-i-use-nonlinear-regression-to-fit-my-data-into-a-particular-function","Creator_reputation":1}
{"_id":{"$oid":"5837a580a05283111e4d5572"},"View_count":30,"Display_name":"Allen Lu","Question_score":1,"Question_content":"Say I am predicting swimming times. I get a group of 20 to make predictions on the fastest to slowest swimmers from 1 to 6. I then ask a single expert in swimming to make the same predictions. Is there a way to test statistical significance between the group's predictions versus the expert's?\"significant\" would mean if I were to select a random person from the group there's a very low percent he would predict in the same order as the expert.ex.We rank 1 to 6 the swimmers who received the highest to lowest average predictions (10 people predict swimmer 1 to get first, 10 predict swimmer 1 to get second, no one predicts swimmer to get other; his average is then 1.5, average of the predictions. We do this for every swimmer and rank the first 6 according to average). Now we have our top 6 from the group, how would I determine significance (as defined above) of an expert's prediction on the same swimmers.","Creater_id":127434,"Start_date":"2016-08-11 21:11:16","Question_id":229459,"Tags":["hypothesis-testing","statistical-significance"],"Answer_count":1,"Last_activity":"2016-08-12 11:50:03","Link":"http://stats.stackexchange.com/questions/229459/comparing-group-and-expert-result","Creator_reputation":6}
{"_id":{"$oid":"5837a580a05283111e4d557f"},"View_count":104,"Display_name":"Jake","Question_score":2,"Question_content":"I'm in the beginnings of following along with the Coursera machine learning course, and I just did univariate linear regression. My regression line/output looks good and the cost function decreased, but was still extremely high at the end of iterating (J(theta) = 2058715091.21221 at the final iteration). Is this an issue if everything looks right and it seems to asymptote around there or should it really be going to zero? Here are the plots:If there's not really a general answer and it depends upon specifics I'll make an edit and post all the code. For a general overview, the data is (house sqft, house price) with ranges (852-4478),(179900-699900). I normalized the house sqft input between [0,1], set the learning rate to 1 and the number of iterations to 100. I tried with a smaller learning rate and higher iterations and it doesn't seem to help. Any comments or suggestions are greatly appreciated, thanks. ","Creater_id":127191,"Start_date":"2016-08-09 19:44:44","Question_id":229102,"Tags":["regression","machine-learning","matlab"],"Answer_count":1,"Last_activity":"2016-08-12 11:49:35","Link":"http://stats.stackexchange.com/questions/229102/is-a-very-high-cost-funtion-value-a-problem-by-itself","Creator_reputation":113}
{"_id":{"$oid":"5837a580a05283111e4d558c"},"View_count":17920,"Display_name":"Legend","Question_score":69,"Question_content":"It is a known fact that median is resistant to outliers. If that is the case, when and why would we use the mean in the first place? One thing I can think of perhaps is to understand the presence of outliers i.e. if the median is far from the mean, then the distribution is skewed and perhaps the data needs to be examined to decide what is to be done with the outliers. Are there any other uses?","Creater_id":2164,"Start_date":"2011-08-13 00:50:53","Question_id":14210,"Tags":["mean","median","basic-concepts"],"Answer_count":8,"Last_activity":"2016-08-12 11:43:40","Link":"http://stats.stackexchange.com/questions/14210/if-mean-is-so-sensitive-why-use-it-in-the-first-place","Creator_reputation":1317}
{"_id":{"$oid":"5837a580a05283111e4d55a0"},"View_count":434,"Display_name":"Strohmi","Question_score":5,"Question_content":"I am about to do a laboratory experiment in the scientific field of soil ecology and hydrology. Beforehand I want to make sure not to make any crucial mistakes, and therefore I would appreciate any hints and comments from your side. The main issue is how to deal with high natural variability (~25%) and a rather small sample size (total max=20).Short description of the experiment:  We will put soil cores in cylinders and keep them under different moisture scenarios. Three different kind of Carbon forms will be measured for about one month.We want to know if the variables change within groups and between groups.The experimental design that was proposed as follows:There will be 2 different soil types, 2 different treatments + one control, and each treatment should be replicated three times. The total number of cylinders is thus 18 cylinders. The variability in field measurements can be as high as 25% within one group. Due to pracitcal reasons there cannot be more than 20 cylinders in total.My questions:  Would it make more sense to have only one treatment and one control, but each one is replicated five times?   Will I be able to draw any reliable conclusions from this kind of experiment under these conditions?  How should I set the parameters to make some power calculations using e.g. G*Power 3? Which test should I choose? What should I set the effect size to and what should the numbers for df be?  How should I analyze the data after the experiment is done? Should I use ANOVA? Can I use a mixed effect model?","Creater_id":2063,"Start_date":"2010-11-19 07:48:02","Question_id":4729,"Tags":["anova","mixed-model","experiment-design","power-analysis","degrees-of-freedom"],"Answer_count":1,"Last_activity":"2016-08-12 10:32:40","Link":"http://stats.stackexchange.com/questions/4729/how-to-setup-a-laboratory-experiment-in-ecological-research-under-high-natural-v","Creator_reputation":390}
{"_id":{"$oid":"5837a580a05283111e4d55ad"},"View_count":46,"Display_name":"Andrew Yoak","Question_score":1,"Question_content":"I am attempting to help a colleague with data analysis and I'm a bit swamped as to what is appropriate. We are trying create a formula to explain why children in various provinces have stunted growth and we predict it is due to Vitamin A deficiency and damage from Diarrhea events (Simplified of course). I have proportional data for nine provinces on:the percentage of stunted children  the percent that have received Vitamin A recentlythe percent that have recently had diarrhea.Example such as: Province 1   ---- 14.5% Stunted-----75.4% Vitamin A intake -----14% Diarrhea     Since the data is not normal and also inherently bounded by 0 and 100%, I assume I should not be using linear regression. This website suggests using a probit or two-limit tobit model, but I've found suggestions to use GLMM, ancovas, or logistic regression.I use normally use SPSS but can use R, JMP if needed. Any suggestions would be greatly appreciated. ","Creater_id":121424,"Start_date":"2016-07-29 09:19:40","Question_id":226326,"Tags":["regression","correlation","multiple-regression","generalized-linear-model","proportion"],"Answer_count":1,"Last_activity":"2016-08-12 10:13:56","Link":"http://stats.stackexchange.com/questions/226326/proportion-percentage-regression-analysis-methods","Creator_reputation":108}
{"_id":{"$oid":"5837a580a05283111e4d55ba"},"View_count":585,"Display_name":"landroni","Question_score":1,"Question_content":"What is the baseline level for a factor-by-factor interaction term in multiple regression? Consider this example from Fox 2003. In the regression below, these two variables are categorical: year={1997,..,2002} and colour={black,white}. require(effects)require(lmtest)Arrestsyear)arrests.mod \u0026lt;- glm(released ~ employed + citizen + checks                         + colour*year + colour*age,                         family=binomial, data=Arrests)Which yields: \u0026gt; coeftest(arrests.mod)z test of coefficients:                       Estimate Std. Error  z value  Pr(\u0026gt;|z|)    (Intercept)           0.3444334  0.3100749   1.1108 0.2666514    employedYes           0.7350645  0.0847701   8.6713 \u0026lt; 2.2e-16 ***citizenYes            0.5859841  0.1137717   5.1505 2.598e-07 ***checks               -0.3666425  0.0260322 -14.0842 \u0026lt; 2.2e-16 ***colourWhite           1.2125167  0.3497751   3.4666 0.0005272 ***year1998             -0.4311794  0.2603589  -1.6561 0.0977023 .  year1999             -0.0944343  0.2615447  -0.3611 0.7180519    year2000             -0.0108975  0.2592073  -0.0420 0.9664655    year2001              0.2430630  0.2630151   0.9241 0.3554129    year2002              0.2129549  0.3532786   0.6028 0.5466444    age                   0.0287279  0.0086191   3.3330 0.0008590 ***colourWhite:year1998  0.6519565  0.3134898   2.0797 0.0375555 *  colourWhite:year1999  0.1559504  0.3070430   0.5079 0.6115161    colourWhite:year2000  0.2957537  0.3062034   0.9659 0.3341076    colourWhite:year2001 -0.3805413  0.3040538  -1.2516 0.2107305    colourWhite:year2002 -0.6173178  0.4192551  -1.4724 0.1409086    colourWhite:age      -0.0373729  0.0102003  -3.6639 0.0002484 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1In the table above, I interested in identifying the baseline level for the factor by factor interaction term... For instance, group colourWhite:year1998 is compared to which other group? Is colourWhite:year1997 the baseline level, or perhaps colourBlack:year1997? ","Creater_id":36515,"Start_date":"2015-04-18 09:22:27","Question_id":147083,"Tags":["r","regression","categorical-data","interaction","interpretation"],"Answer_count":1,"Last_activity":"2016-08-12 10:03:26","Link":"http://stats.stackexchange.com/questions/147083/what-is-the-baseline-level-in-a-factor-by-factor-interaction","Creator_reputation":463}
{"_id":{"$oid":"5837a580a05283111e4d55c7"},"View_count":41,"Display_name":"J. Krauel","Question_score":1,"Question_content":"I want to know if a proportion increased significantly after an event, and then find out if that holds true over multiple events.  I'm using a paired t-test to measure this with continuous data (e.g. mass), but I don't think I can use that to test for a proportion. For example, I'm interested in the proportion of identified insect species (range 14-56) that is migratory (range 2-15) in 94 almost-daily samples of insect DNA extracted from bat feces, examined before and after 39 cold fronts that occurred during the sampling period.  The insects migrate on favorable winds after cold fronts, and I'm testing whether bats eat more migratory insects directly after a cold front than on other sampling days.  Here's a sample of the data.structure(list(migr = c(5, 6, 2, 6, 8, 7, 10, 4, 7, 9, 8, 10, 8, 13), spp = c(26, 31, 26, 30, 35, 44, 35, 32, 43, 30, 38, 39, 49, 49), front = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0)), class = \"data.frame\", row.names = c(NA, -14L), .Names = c(\"migr\", \"spp\", \"front\"))I'm using R, and there does not seem to be a function that does what I need.  I can compare two proportions (prop.test), but not a series of proportions a la paired t-test.  I looked at the function pairwise.prop.test but it does a subtly different test, comparing multiple percentages instead of multiple pairs of percentages. It seems odd that I would need to use meta-analysis tools when I have the original data.If I must use meta-analysis tools to aggregate p-values, how do I determine the appropriate function to use?  Thanks to the link to the metap package, but there are many options (including the sum of logs method which was suggested to me earlier).","Creater_id":127176,"Start_date":"2016-08-11 14:45:25","Question_id":229433,"Tags":["proportion","meta-analysis","paired-comparisons","combining-p-values"],"Answer_count":1,"Last_activity":"2016-08-12 09:59:56","Link":"http://stats.stackexchange.com/questions/229433/testing-multiple-comparisons-of-paired-proportions","Creator_reputation":6}
{"_id":{"$oid":"5837a580a05283111e4d55d4"},"View_count":76,"Display_name":"mkoala","Question_score":0,"Question_content":"I am having a hard time trying to understand the mathematical principle behind XGBoost. What really bothers me is the definition of the model complexity:  \\Omega(f) = \\gamma T + \\frac12 \\lambda \\sum_{j=1}^T w_j^2Can someone tell me what  and  stand for? ","Creater_id":108895,"Start_date":"2016-08-12 08:23:09","Question_id":229557,"Tags":["machine-learning","predictive-models","boosting","xgboost"],"Answer_count":1,"Last_activity":"2016-08-12 09:49:21","Link":"http://stats.stackexchange.com/questions/229557/definition-of-model-complexity-in-xgboost","Creator_reputation":3}
{"_id":{"$oid":"5837a580a05283111e4d55e1"},"View_count":49,"Display_name":"Jack Pierce-Brown","Question_score":4,"Question_content":"Let  and  be identically distributed correlated lognormal random variables:X_1,X_2 \\sim \\ln \\mathcal{N}(\\mu_X,\\sigma_X^2)(such that their logs are bivariate normal).The correlation between  and  is given by:\\text{corr}(X_1,X_2) = \\rho_XLet  and  be the reciprical of these random variables:Y_1 = \\frac{1}{X_1}, Y_2 = \\frac{1}{X_2}Then  and  will also be lognormally distributed according to:Y_1,Y_2 \\sim \\ln \\mathcal{N}(-\\mu_X,\\sigma_X^2)The correlation between them is given by :\\text{corr}(Y_1,Y_2) = \\rho_YCan an expression for  be written in terms of ? ","Creater_id":112088,"Start_date":"2016-08-02 03:30:12","Question_id":226844,"Tags":["correlation","lognormal"],"Answer_count":1,"Last_activity":"2016-08-12 09:33:28","Link":"http://stats.stackexchange.com/questions/226844/correlation-between-the-reciprocal-of-lognormal-random-variables","Creator_reputation":91}
{"_id":{"$oid":"5837a580a05283111e4d55ee"},"View_count":21,"Display_name":"blu","Question_score":1,"Question_content":"The shorter and more general version of this question:If tuning a model via cross-validation (within training set) produces worse results on the test set than my previous default/baseline model, do I stick with the tuned model (to avoid over-fitting on the training data) or do I go back to the baseline (even though it seems like I'm overfitting by doing so)?Long more detailed scenario:Let's say I have an 80/20 train/test split set.  Then I build a model with default model hyperparameters and obtain an F1 score of 0.35.Then I use cross-validation on the train set to identify the best hyperparameters and build a new model on all the training data using those hyperparameters found optimal per the cross-validation.  However, when I evaluate this \"optimal\" tuned model on the test set I get an F1 score of 0.23.In such cases, should I stick to the default hyperparameters that had produced the higher F1 score on the test set or stick with the tuned model since it was tuned using cross-validation? In case such variation in numbers is unlikely - I guess then I'm wondering whether there may be some other factor at play, such as too small a dataset (e.g. around total 1000 datapoints total with fewer than 200 of them being in the class of interest) or imbalanced fold partitioning.","Creater_id":108829,"Start_date":"2016-08-11 17:44:35","Question_id":229445,"Tags":["cross-validation","parameterization"],"Answer_count":1,"Last_activity":"2016-08-12 09:31:58","Link":"http://stats.stackexchange.com/questions/229445/do-i-stick-with-the-tuned-model-parameters-even-if-they-produce-worse-test-score","Creator_reputation":48}
{"_id":{"$oid":"5837a580a05283111e4d55fa"},"View_count":247,"Display_name":"Bex","Question_score":1,"Question_content":"Are there any specific families of probability distribution which are not exchangeable by construction?I was thinking that the Hyper Geometric distribution would not be since it models random sampling without replacement, but I'm not sure?","Creater_id":117108,"Start_date":"2016-08-12 05:54:45","Question_id":229528,"Tags":["probability","distributions","hypergeometric","exchangeability"],"Answer_count":2,"Last_activity":"2016-08-12 09:19:11","Link":"http://stats.stackexchange.com/questions/229528/which-probability-distributions-are-not-exchangeable","Creator_reputation":8}
{"_id":{"$oid":"5837a580a05283111e4d5608"},"View_count":382,"Display_name":"Diogo Santos","Question_score":9,"Question_content":"Given a random variable , what is the mean and variance of  ? I look at the Inverse Gamma Distribution, but the mean and variance are only defined for  and  respectively... ","Creater_id":27069,"Start_date":"2016-08-12 07:05:33","Question_id":229543,"Tags":["variance","mean","exponential"],"Answer_count":3,"Last_activity":"2016-08-12 09:14:35","Link":"http://stats.stackexchange.com/questions/229543/mean-of-inverse-exponential-distribution","Creator_reputation":108}
{"_id":{"$oid":"5837a580a05283111e4d5617"},"View_count":2396,"Display_name":"Jim Blum","Question_score":7,"Question_content":"I do not think that this is a difficult question, but I guess someone needs experience to answer it. It is a question that is asked a lot here, but I did not found any answer that explains the reasons of choosing an appropriate ML algorithm.So, let's suppose we have a set of data. And let's suppose I want to do clustering (This could be classification or regression if I also had labels or values or my training set data).What should I consider before choosing an appropriate algorithm? Or I just choose algorithms in random? In addition how I choose any data preprocessing that can be applied at my data? I mean are there any rules of the format \"IF feature X has property Z THEN do Y\"?In addition are there any other things except preprocessing and choosing my data that I miss and you want to advice me about them?For example, lets suppose that I want to do clustering. Is saying \"I will apply k means at that problem\" the best approach? What can improve my performance?I will chose as best answer the answer that is much more justified and explains everything that someone should consider.","Creater_id":35243,"Start_date":"2014-02-19 04:16:25","Question_id":87135,"Tags":["regression","machine-learning","classification","clustering"],"Answer_count":2,"Last_activity":"2016-08-12 08:51:59","Link":"http://stats.stackexchange.com/questions/87135/selecting-an-appropriate-machine-learning-algorithm","Creator_reputation":249}
{"_id":{"$oid":"5837a580a05283111e4d5624"},"View_count":455,"Display_name":"Tim","Question_score":17,"Question_content":"What would be the best way to sample from Cantor distribution? It only has cdf and we can't invert it.","Creater_id":35989,"Start_date":"2016-08-12 08:17:15","Question_id":229556,"Tags":["distributions","simulation","random-generation"],"Answer_count":1,"Last_activity":"2016-08-12 08:42:10","Link":"http://stats.stackexchange.com/questions/229556/how-to-sample-from-cantor-distribution","Creator_reputation":25400}
{"_id":{"$oid":"5837a580a05283111e4d5631"},"View_count":96,"Display_name":"zlqs1985","Question_score":1,"Question_content":"When the first stage is linear, like the case of 2sls, the test of weak IV is straight forward, method like Stock and Yogo (2005) test are available in standard package command, like ivreg2 in Stata. But when I specify something nonlinear in the first stage (like a ordered probit model), the situation become complicated. Can I find some counterpart test for the nonlinear case, or I can specify a \"linear\" specification as the baseline (which I believe is problematic). Any suggestion is warmly welcome and literatures are desired, thanks in advance.","Creater_id":66930,"Start_date":"2015-07-19 08:34:00","Question_id":162176,"Tags":["instrumental-variables","2sls"],"Answer_count":0,"Last_activity":"2016-08-12 08:27:41","Link":"http://stats.stackexchange.com/questions/162176/test-on-weak-iv-when-first-stage-is-nonlinear","Creator_reputation":45}
{"_id":{"$oid":"5837a580a05283111e4d5633"},"View_count":77,"Display_name":"Andrew J.","Question_score":2,"Question_content":"I am medical student with only a very basic background in CNN's and TensorFlow, so I appreciate any advice.I implemented a CNN with ~20K black and white images that are 32x32 pixels. I chose to partition my dataset into ~19K images as training data and ~1K images as testing data. Using Tensorflow and TFLearn, I classified my images into one of three classes and plotted accuracy over time with TensorBoard.The resulting accuracy graph looks like this:My questions are:Why does the accuracy suddenly spike down randomly into the mid 90 percents, then shoot back up?How can I tell if I am \"overfitting\" my data set?Is my breakdown of training vs. testing data optimal? (i.e. I chose to use 5% of my images to test and 95% of my images to train. Is there a better combination?)","Creater_id":127313,"Start_date":"2016-08-10 21:19:18","Question_id":229301,"Tags":["machine-learning","conv-neural-network","tensorflow"],"Answer_count":1,"Last_activity":"2016-08-12 08:27:01","Link":"http://stats.stackexchange.com/questions/229301/why-does-the-accuracy-of-my-cnn-periodically-spike-down","Creator_reputation":11}
{"_id":{"$oid":"5837a580a05283111e4d563f"},"View_count":69,"Display_name":"Raj","Question_score":5,"Question_content":"I have a bunch of plots that look like the following:Eyeballing this, it's clear that we have a bunch of values that follow a pattern which we can predict from a linear model, but then a couple that are \"outliers\". What are some accurate ways we can identify these outliers?My thoughtsOne way to go about this is to create a LM to fit this plot, and then look for points that are some threshold away from that LM. However, we see that there are a bunch of outliers that also follow a linear pattern; if there were enough of these, it could skew the LM so that it would be much more difficult to identify real outliers. Another idea is to assess the mean and SD of this dataset, and if a point is far enough away (determined by z-score) we say it's an outlier. Just a few thoughts; any ideas or thoughts would be helpful.","Creater_id":127488,"Start_date":"2016-08-12 06:49:08","Question_id":229537,"Tags":["dataset","prediction","outliers"],"Answer_count":1,"Last_activity":"2016-08-12 08:20:06","Link":"http://stats.stackexchange.com/questions/229537/how-to-guess-when-a-number-is-far-away-from-expected-value","Creator_reputation":26}
{"_id":{"$oid":"5837a580a05283111e4d564c"},"View_count":40,"Display_name":"SimonB","Question_score":0,"Question_content":"I have two variables x and y which are correlated but I want to know if this correlation is real or is due to a spurious correlation. Therefore, I wanted to calculate the spurious coefficient between two variables.Below the dataframe I am using. df\u0026lt;- structure(list(A = c(1262.24986565392, 225.207155478226, 102.610153421648, 454.086587867539, 560, 391.49284084118, -518.729500712384, 11.5525291214872, 120.713583319183, 4.11763982519915, 84.6405661457684, 17.1155716306239, 62.8432272937437, 111.374956877204, 15.2104309141481, -220.613145695315, 157.129576861289, 44.1257786633602, 46.8326830295943, -146.719591499443, 1.8368979105227, -4.10548956954153, -108.258462657337, 90.3369144331664, 184.272826701352, 453.871128587052, 167.142700318449, 296.910724735508, 250.031364376555, 369.886702498286, 369.886702498286, 268.903338304488, 463.207100768923, 575.085017045649, 836.138520270266, -45.4886732113082, 571.946372566243, 618.930183215125, -266.612732616486, -3.24800625447339, 372.091088309901, 613.897540961003, 82.1480000555981, 151.060152757273, 404.007940533033, 218.563716648002, -46.9675149230141, 653.144593625536, 333.76992898155, 300.979565786038, 249.269569331618, 155.404971522881, 201.195892312115, 100, 466.727302447427, 552.762670615377, 595.145436977735, 187.970225354856, 481.359543363331, 513.738444643561, 229.153300818488, 685.018688042004, 413.278539420697, 308.198167469197, -638.973101716489, 321.395064735785, 960.206859404842, 449.611617737741, 642.711487830187, 629.214319321327, -163.439559314225, 214, 59.1716887350485, -75.4854116312228, -186.42650026083, 66.3441410785308, 657.172059780536, 595.091753601562, 367.15020653978, 187.113088187529, 268.681242669467, 239.81099676255, 91.8453621773515, 756.036955245771, -9.02147635902511, 69.3275392616401, -70.3684077725047, 167.431721580711, 216.223144340989, 208.74650923241, 86.3613471518824, 368.347069109092, 609.207890124193, 141.97570288631, 651.627135872608, 146.356796323322, 230.428160804673, 423.078742577004, 278.735641213587, 170.880363314238, 914.470944693661, 171.275307008387, 828.099953922805, 11.0289920627591, 43.8593843849764, 59.0674157321143, 86.863138092478, 180.822916703924, 205.283659537323, 569.757759056969, 79.3789962158771, 269.496854757409, 306.179549410374, 857.566411400051, 282.235120381209, 358.242634316906, 156.666666666667, 10, 440, 360, 270, 247, 100), B = c(2437.9937774539, 2107.89290260772, 1363.31318485845, 1752.60637397666, 3630, 2232.28817154825, 588.296003962209, 1342.57124635804, 1059.82335092546, 690.949743975086, 814.79050584964, 529.028059860284, 588.180042402702, 669.167532135227, 423.183787499297, 230.279491515383, 1051.74875971674, 784.200597481914, 572.337961145761, 321.051727265343, 649.798119157386, 365.984901298172, 128.198042456082, 535.519377609133, 1302.43845038345, 1682.68272444244, 1493.99830379654, 1493.02795584358, 693.315179047553, 1481.55208933962, 1485.30116796424, 1491.67772108177, 1460.49230002798, 1820.51414117636, 1725.89186816063, 1354.17276381073, 1610.47457482679, 1530.12119655125, 510.59605475131, 1136.89096303926, 1804.51988710744, 1979.96216355234, 1529.43757814094, 1946.8578510831, 1432.53358200192, 1113.40698076565, 541.382959078834, 1832.423523796, 1599.72128022698, 1931.46187551168, 1240.19112022885, 3708.5771597392, 649.624245001876, 710, 1131.78323406311, 1606.54794243226, 909.329436211996, 1121.63122592973, 1346.19773759902, 1465.14862184832, 1548.44958677415, 1448.24548082734, 2037.63473260907, 1087.48954802548, 613.15010408836, 1546.44900958125, 2474.01780851682, 1660.36455411885, 1965.4129422307, 1842.15117161721, 1503.07789248641, 594, 704.123482171384, 1155.4254257937, 461.156280127354, 674.107352191582, 1749.16955647711, 2111.6101945499, 1102.14509913209, 1968.73145147786, 524.398260936374, 512.986586335697, 238.455805805541, 2245.51732066832, 1033.72743610183, 933.174328979291, 369.047471772705, 1504.53748132585, 1458.14613814657, 1399.06313519658, 1258.28910464796, 1233.05517321796, 1616.02410411663, 852.500007182098, 1063.20067437459, 791.712743086042, 872.886731202831, 1719.12478481041, 1301.34322502363, 2023.90410973958, 2787.31611419407, 843.164187698849, 1774.97197922982, 944.642496684275, 413.518131878841, 283.074539450953, 341.028475512401, 1712.39834046969, 2704.42955517343, 2435.33415092662, 1171.87561623209, 1294.49672738818, 1335.61126811194, 1478.83877500477, 1174.31703938687, 3499.21653687954, 1313.33333333333, 1430, 2710, 1634, 911, 727, 777), C = c(1175.74381220341, 1882.68574563662, 1260.85722545895, 1298.51972555347, 3070, 1840.79532907903, 1107.02550385165, 1331.01871782169, 939.109786485322, 686.832104034332, 730.149855332798, 511.91248805076, 525.336794896857, 557.79264668798, 407.973335866428, 450.892636924867, 894.619184653042, 740.074818417721, 525.505276597609, 467.77131837895, 647.961245282087, 370.090391202229, 236.456506340317, 445.182462833822, 1120.44176453054, 1230.61835588675, 1326.85560327338, 1198.6568645152, 443.283813476628, 1111.66538761358, 1115.41446532535, 1222.77416813525, 997.285213679075, 1245.42909570783, 893.082955101036, 1399.66144205342, 1038.38051471859, 911.191017536282, 777.208798903513, 1140.13896284997, 1432.42869251009, 1366.06462557614, 1447.28957333144, 1795.4655185584, 1028.52563957497, 894.843264635463, 588.35047468619, 1183.49288836641, 1296.30028343201, 1735.94063444643, 991.238841821129, 3562.56429613961, 448.428351869542, 610, 665.055927230345, 1053.78527054687, 314.183998964151, 933.432498527425, 864.838195415214, 956.926154464483, 1319.29628365513, 763.226798327631, 1624.85551252428, 779.291378117398, 1252.12321834266, 1225.05394333415, 1500.52133205833, 1176.48935586512, 1322.70148577541, 1212.93685369224, 1666.00941412118, 380, 644.951793942513, 1211.83562788933, 647.582784344451, 709.713355378362, 1112.14420013216, 1516.51844120026, 734.994892628422, 1781.61846411228, 255.717019842959, 273.175588674374, 146.610444564718, 1489.48036754131, 1042.74891297817, 865.253583592176, 439.415880262852, 1335.98203414381, 1241.92299326395, 1190.31662442061, 1171.92775708586, 864.708101771772, 1008.60616259775, 710.52430604064, 411.57354164564, 645.355948520824, 642.115285318451, 1296.04600696104, 1021.85666603483, 1853.02374902181, 1872.84517205134, 673.37871074714, 946.871954278031, 936.808985005459, 369.658747287397, 224.007124872257, 254.165337413084, 1532.20489517041, 2499.14589381218, 1865.57639188568, 1092.49662436018, 1026.95263245429, 1029.43167272283, 621.272360969873, 892.081852362163, 3140.97389888763, 1156.66666666667, 1420, 2270, 1274, 641, 480, 677)), .Names = c(\"A\", \"B\", \"C\"), row.names = c(NA, -123L), class = \"data.frame\")Basically, A= B+ C and I want to know if the relationship between A and B is spurious or not. I have been adapting the Pearson (1896) spurious coefficient defined as such: rsc = sd(x) / [sd(x)^2 + sd(y)^2]^(1/2)where A = x + y and B = xHowever, I am not sure if I am doing it correctly and if this is the best approach. Thanks for your help.","Creater_id":91034,"Start_date":"2016-08-10 07:43:50","Question_id":229195,"Tags":["correlation","pearson"],"Answer_count":0,"Last_activity":"2016-08-12 08:03:20","Link":"http://stats.stackexchange.com/questions/229195/how-to-calculate-spurious-coefficient-between-two-variables","Creator_reputation":105}
{"_id":{"$oid":"5837a580a05283111e4d564e"},"View_count":23,"Display_name":"Raymond Augestin","Question_score":0,"Question_content":"i have done mediation analysis according to the standards of preacher and hayes. My c' happens to be less than c with bootstrap at -.063 and no 0 in between the confidence intervals, but in the path c', the p value did not become non-significant. According to B\u0026amp;K this can be assumed as partial mediation, but what does preacher and hayes have to say for this? Does preacher and hayes support partial mediation? And citation will be much appreciated. Thank you.","Creater_id":127206,"Start_date":"2016-08-10 01:20:28","Question_id":229136,"Tags":["spss","mediation"],"Answer_count":1,"Last_activity":"2016-08-12 07:54:09","Link":"http://stats.stackexchange.com/questions/229136/mediation-analysis-doubt-on-standards-proposed-by-preacher-and-hayes","Creator_reputation":1}
{"_id":{"$oid":"5837a580a05283111e4d565b"},"View_count":33,"Display_name":"dan","Question_score":1,"Question_content":"I have data from these set of experiments:In each experiment I infect a neuron with a rabies virus. The virus climbs backwards across the axon of the infected cell and jumps across the synapse to the inputs of that cell. In these input cells it well express a marker gene.The purpose of this is to trace the input cells to the cells I'm infecting (target cells).Therefore in each experiment I obtain counts of all the infected input cells. What I'd like to estimate is the effect of each target cell on these counts, relative to the grand mean.I was thinking that a multinomial regression model is appropriate in this case, where the contrasts are set to the contr.sumoption.However, I'd like some help interpreting the coefficients estimated by the model.If I simulate very simple data like this:set.seed(1)probs \u0026lt;- c(0.1,0.1,0.4,0.2,0.2)mat.col \u0026lt;- rmultinom(1, as.integer(runif(1,50,150)), probs)mat \u0026lt;- matrix(rep(mat.col,3),ncol=3)inputs \u0026lt;- LETTERS[1:5]targets \u0026lt;- letters[1:3]df \u0026lt;- data.frame(input = c(unlist(apply(mat,2,function(x) rep(inputs ,x)))),target = rep(targets ,apply(mat,2,sum)))where the counts produced by the two targets are identical, after fitting the model:library(foreign)library(nnet)library(reshape2)dfinput,levels=inputs)dftarget,levels=targets)fit \u0026lt;- multinom(input ~ target, data = df,contrasts = list(target = \"contr.sum\"))I get:summary(fit)$coefficients  (Intercept)  target.exon1  target.exon2B   0.6931475 -2.682397e-08 -2.682397e-08C   1.5040775  1.502057e-07  1.502057e-07D   0.9162909 -5.340499e-08 -5.340499e-08E   1.0116010 -6.342754e-08 -6.342754e-08I thought that since inputs C-E are all equal to or above the grand mean, their coefficients should be positive. So my question is why are they negative?","Creater_id":66552,"Start_date":"2016-08-11 22:55:39","Question_id":229465,"Tags":["regression","generalized-linear-model","multinomial"],"Answer_count":0,"Last_activity":"2016-08-12 07:51:14","Link":"http://stats.stackexchange.com/questions/229465/understanding-multinomial-regression","Creator_reputation":148}
{"_id":{"$oid":"5837a580a05283111e4d565d"},"View_count":53,"Display_name":"Julian Karls","Question_score":3,"Question_content":"I am currently working on a problem where I try to explain within subject variance in an outcome using multiple other variables. In R setting up the model looks like thislmFull\u0026lt;-lmer(outcome ~ (1|subject) + pred1 + pred2 +pred3 ... pred40)The problem is that there is some mild missingness in each predictor variables. Thus, using listwise deletion (the default setting of lmer) many cases are lost, since due to the number of predictors chances are high that at least one is missing.I have googled this problem but can only find examples claiming that this is not an issue in mixed modelling since it uses the long format. This, however, is only true if there are few predictors.One obvious solution would be to use FIML. However, some of my predictors are ordinal and thus not normally distributed. Setting up a valid joint distribution for all my predictors will be very cumbersome.One quick and dirty solution might be to impute the values with the within subject means.Any thoughts? Is there a recommended approach?","Creater_id":30495,"Start_date":"2016-08-11 12:03:36","Question_id":229410,"Tags":["mixed-model","missing-data"],"Answer_count":1,"Last_activity":"2016-08-12 07:38:57","Link":"http://stats.stackexchange.com/questions/229410/missing-data-mixed-effects-modelling-for-repeated-measures","Creator_reputation":316}
{"_id":{"$oid":"5837a580a05283111e4d5669"},"View_count":152,"Display_name":"user127390","Question_score":5,"Question_content":"From what I understand about Maximum Likelihood Estimation, by observing a set of data, we guess a distribution family and then find the parameters for that distribution that will maximize the probability of observing the data we have observed. (Am  I right?) So prior to using MLE I do need to have a way to guess a distribution family that the data follow. How can I make this guess?","Creater_id":127390,"Start_date":"2016-08-11 11:37:33","Question_id":229396,"Tags":["distributions","maximum-likelihood"],"Answer_count":4,"Last_activity":"2016-08-12 07:34:38","Link":"http://stats.stackexchange.com/questions/229396/do-i-need-to-guess-a-distribution-to-use-mle","Creator_reputation":26}
{"_id":{"$oid":"5837a580a05283111e4d5679"},"View_count":348,"Display_name":"Riko","Question_score":3,"Question_content":"Let a, b be real numbers randomly selected independently and uniformly from the range of (0,1).What is P(a \u0026lt; b)?The problem here is that a can be equal to b, so is P(a \u0026lt; b)  ≈   0.5 or P(a \u0026lt; b)  →   0.5 formally correct? Or anything else?What I'm looking for here is a correct formal way to write this probability down.","Creater_id":127465,"Start_date":"2016-08-12 03:16:53","Question_id":229499,"Tags":["probability","uniform"],"Answer_count":3,"Last_activity":"2016-08-12 07:28:47","Link":"http://stats.stackexchange.com/questions/229499/given-x-y-sim-i-i-u0-1-what-is-pxy","Creator_reputation":18}
{"_id":{"$oid":"5837a580a05283111e4d5688"},"View_count":20,"Display_name":"J. R. C.","Question_score":0,"Question_content":"Consider the following stochastic process. It is partly a Wiener process with drift coefficient  and diffusion coefficient :X(t+dt)=X(t)+N(\\mu dt,dt)But it has also a certain probability of \"dying\" that depends on  and . More concretely, it has the following instantaneous rate of \"death\":\\begin{align}f(x,t)dt \\equiv \u0026amp; \\text{probability, given } X(t)=x, \\text{that the process }\\\\ \u0026amp; \\text{will die at some instant between  and }\\end{align}With I would like to know whether it is possible to obtain the probability density   (with  and ) that the process is \"alive\" at  through a Fokker-Planck-like equation. If the answer is positive, I would like to know how to obtain this equation (good advice to solve it numerically would be awesome). If it is negative, I would appreciate suggestions about how would be the most efficient way to proceed, if possible avoiding path integral evaluation methods.The ultimate purpose is to apply maximum likelihood to a data set that consists in the observed times when the process \"ended\" (re-starting everytime from ). So the trajectories  are never observed. The way I think about it is as a Hidden Markov Model where  is the latent variable and the observed variable is a binary variable that tells you if the process has ended or not.","Creater_id":127493,"Start_date":"2016-08-12 07:20:42","Question_id":229545,"Tags":["time-series","maximum-likelihood","stochastic-processes","markov-process","hidden-markov-model"],"Answer_count":0,"Last_activity":"2016-08-12 07:20:42","Link":"http://stats.stackexchange.com/questions/229545/probability-distribution-for-interrelated-wiener-and-inhomogeneous-poisson-proce","Creator_reputation":21}
{"_id":{"$oid":"5837a580a05283111e4d568a"},"View_count":45,"Display_name":"Marcos Alex","Question_score":0,"Question_content":"I am new on the subject... I am computing AIC in R:fitM1 = lm(dataBM)logM1 = logLik(fitM1)aicM1 = 2*(n+1) - 2*logM1First I was very concerned with negative AIC values, but what concerns me now is the high differences among AIC values of different models. By comparing some models with different input parameters to a benchmark model, I have got very discrepant (negative) AIC values. For instance, for one example the AIC values for the models are:-105885.1, -105121.2, -109740.6, -117007, -105858.8, -108601.9, -108856.9With such values set it is not even possible to compute the Akaike weight (wi).Could it be correct?","Creater_id":127484,"Start_date":"2016-08-12 06:27:06","Question_id":229533,"Tags":["r","aic","model-averaging"],"Answer_count":1,"Last_activity":"2016-08-12 07:08:59","Link":"http://stats.stackexchange.com/questions/229533/extremely-high-aic-differences-between-models","Creator_reputation":101}
{"_id":{"$oid":"5837a580a05283111e4d5697"},"View_count":49,"Display_name":"Jef Van Alsenoy","Question_score":2,"Question_content":"Suppose two samples X and Z are dependent and of equal length. I see two approaches to investigate the possible difference in means:1) Use a paired t-test if the normality assumption is met for each sample.2) Subtract the samples from each other and apply a one-sample t-test to test whether the difference (X-Z) differs from 0. Is there a difference in the way the test statistics are calculated? This thread  (Paired difference t-test vs independent two sample t-test to assess means difference) with an excellent answer suggests there is not. I confess, I often give in to using the test that suits me best for reporting. When both samples approximate normality I use a paired t-test, when they do not but the difference (X-Y) does I use a one-sample t-test. Is the latter approach wrong? In the proposed should situation should I always favour option (1) over option (2)?","Creater_id":123928,"Start_date":"2016-08-12 02:42:34","Question_id":229491,"Tags":["t-test","normality","paired-data"],"Answer_count":1,"Last_activity":"2016-08-12 07:01:35","Link":"http://stats.stackexchange.com/questions/229491/is-there-a-difference-between-a-paired-two-sample-t-test-and-one-sample-t-test-o","Creator_reputation":26}
{"_id":{"$oid":"5837a580a05283111e4d56a4"},"View_count":816,"Display_name":"hxd1011","Question_score":14,"Question_content":"For vector norm, the L2 norm or \"Euclidean distance\" is the widely used and intuitive definition. But why \"most used\" or \"default\" norm definition for a matrix is spectral norm, but not Frobenius norm (which is similar to L2 norm for vectors)? Does that have something to do with iterative algorithms / matrix powers (if the spectral radius is smaller than 1, then the algorithm will converge)? It is always arguable for the words like \"most used\" ,\"default\". The word \"default\" mentioned above is coming from the default return type in Matlab function norm. In R the default norm for matrix is L1 norm. Both of the are \"unnatural\" to me (for a matrix, it seems more \"natural\" to do  like in vector). (Thanks for @usεr11852 and @whuber's comments and sorry for the confusion.)May be expand the usage of the matrix norm would help me to understand more?","Creater_id":113777,"Start_date":"2016-08-11 06:56:38","Question_id":229354,"Tags":["matrix","linear-algebra"],"Answer_count":3,"Last_activity":"2016-08-12 06:53:20","Link":"http://stats.stackexchange.com/questions/229354/why-the-default-matrix-norm-is-spectral-norm-and-not-frobenius-norm","Creator_reputation":4441}
{"_id":{"$oid":"5837a580a05283111e4d56b2"},"View_count":92,"Display_name":"Wudanao","Question_score":5,"Question_content":"I've come across the following theorem while studying MCMC. It seems to suggest that the sample mean taken from the MCMC – the posterior marginal expectation – should be normally distributed, using just :  Theorem 4.4. (A Central Limit Theorem). For a Harris recurrent, -invariant Markov chain, and a function  satisfying enough regularity conditions,  \\sqrt{t} \\left[ \\frac1t \\sum_{i=1}^t \\phi(X_t) - \\int_{\\mathbb X} \\, \\phi(x) \\pi(x) \\, dx \\right] \\mathop{\\longrightarrow}\\limits_{t \\to \\infty}^D \\mathcal N(0, \\sigma^2(\\phi))  where  \\sigma^2(\\phi) = \\mathbb V[\\phi(X_1)] + 2 \\sum_{k=2}^\\infty \\mathbb{Cov}[\\phi(X_1), \\phi(X_k)].  The variance and covariance in the expression above are with respect to the distribution  of the Markov chain in its stationary regime.I am wondering is there some extension of this result when dimension of the parameter space is larger than 1? So in this case .","Creater_id":80926,"Start_date":"2015-10-16 14:33:42","Question_id":177317,"Tags":["bayesian","mcmc","monte-carlo","markov-process","posterior"],"Answer_count":1,"Last_activity":"2016-08-12 06:38:50","Link":"http://stats.stackexchange.com/questions/177317/are-the-mean-of-samples-taken-from-metropolis-hastings-mcmc-normally-distributed","Creator_reputation":174}
{"_id":{"$oid":"5837a580a05283111e4d56bf"},"View_count":79,"Display_name":"JeanDrayton","Question_score":1,"Question_content":"I am working through the examples in Andy Field’s Discovering Statistics with R. I am stuck on the last Task (Task 3) of the Smart Alex exercises for Chapt 8 logistic regression (tasks are here: https://studysites.uk.sagepub.com/dsur/study/DSUR%20Smart%20Alex-Labcoat%20Leni-Self%20Test%20Answers/DSUR%20Chapter%2008%20Web%20Material.pdf, data are here: https://studysites.uk.sagepub.com/dsur/study/articles.htm. The example starts on page 25 of the Pdf).  The example looks at predicting the probability of condom use based on several predictor variables (output for the coefficients and odds ratios pasted below). What I’m stuck on is the interpretation of the coefficients and odds ratio for a predictor variable that is categorical and has three levels (the predictor in question is previous condom use, labelled “previous” in the example). There are three categories of previous condom use (\"No condom\", \"Condom used\", \"First Time with Partner\"). In the model summary, there are coefficients for 1)\"PreviousCondomUsed\" and 2)\"PreviousFirst Time with Partner\". I would interpret these parameters as 1) \"PreviousCondomUsed\": the difference in the coefficients and odds ratio between \"Condom Used\" and the reference category \"No condom\", and 2)\"PreviousFirst Time with Partner\": the difference in the coefficients and odds ratio between \"First Time with Partner\" and the reference category \"No Condom\". However, the answers to Task 3 (in the Pdf) explain the \"PreviousCondomUsed” coefficient, for example, as comparing group \"CondomUsed\" with the other two groups. I don’t think this is correct, I would have thought that this parameter was comparing group \"CondomUsed\" with the reference group \"NoCondom\". Am I correct in assuming that when there are more than 2 levels of a categorical predictor variable in logistic regression, a level is chosen as the baseline level, and then pairwise comparisons are made between each level of the predictor and the baseline? In relation to the example, am I correct in assuming that the parameters \"PreviousCondomUsed\" and \"PreviousFirst Time with Partner\" are comparing level \"CondomUsed\" with level \"NoCondom\", and comparing \"First Time with Partner\" with \"NoCondom\" respectively?    Coefficients:                             Estimate Std. Error z value Pr(\u0026gt;|z|)        (Intercept)                     -4.959739   1.146497  -4.326 1.52e-05 ***    genderFemale                     0.002656   0.572823   0.005  0.99630        safety                          -0.482460   0.236033  -2.044  0.04095 *      perceive                         0.949088   0.236972   4.005 6.20e-05 ***    selfcon                          0.347626   0.126842   2.741  0.00613 **     previousCondom used              1.087196   0.551952   1.970  0.04887 *      previousFirst Time with partner -0.016615   1.399907  -0.012  0.99053        sexexp                           0.180423   0.111586   1.617  0.10590    The odds ratios are:                                    exp.mod2.coefficients.    (Intercept)                                0.007014758    genderFemale                               1.002659308    safety                                     0.617263292    perceive                                   2.583353254    selfcon                                    1.415702224    previousCondom used                        2.965946499    previousFirst Time with partner            0.983522066    sexexp                                     1.197724363","Creater_id":127429,"Start_date":"2016-08-11 20:18:34","Question_id":229456,"Tags":["regression","self-study","logistic","regression-coefficients","odds-ratio"],"Answer_count":1,"Last_activity":"2016-08-12 06:38:28","Link":"http://stats.stackexchange.com/questions/229456/logistic-regression-with-multi-level-categorical-predictors","Creator_reputation":51}
{"_id":{"$oid":"5837a580a05283111e4d56cc"},"View_count":36,"Display_name":"user1058210","Question_score":0,"Question_content":"Context:  Let's say I have two datasets from 2000 and 2010.  Each dataset consists of an example for each area of a city along with variables A, B and C, which are all numerical variables which describe how the area performs in some measure.  Each area also has a score, variable D.  Imagine we are trying to build a model which predicts which areas will have the highest variable D in dataset 2010 based on variables A-C in dataset 2000.Eventually we want this model to be used on variables A-C in dataset 2010 to predict variable D in the year 2020.  The problem is that variables A-C will most likely change over time e.g. one variable could be crime rate which falls over time in general.  This makes it hard to model because boundaries, rules etc which are found by the model will be using absolute values of these variables.  For example, in 2000 a good area may be one with crime rate below 40%, but in 2010 this may be average and instead a good area is one with crime rate less than 30%.  Fortunately, theory says that it is not the actual values of variables A-C which matter, but rather how areas perform relative to each other.For example, let's say that in 2000 for variable A:Area 1:  50%Area 2:  40%Area 3:  60%In 2010 for variable A, we have:Area 1:  40%Area 2:  35%Area 3:  45%We want to turn those scores in values where only the other areas are concerned, so 2000 could become:Area 1:  0.5Area 2:  0Area 3:  1Then 2010:Area 1:  0.5Area 2:  0Area 3:  1In reality though, the distributions aren't that neat.Problem:  How do we model the data to make the resulting model most valid for future decades?Attempts so far:  I have considered the following:Zero-Mean and Unit Variance Scaling:  Scaling each dataset separately means that values above 0 will be above the mean in both situations but a reading e.g. 0.4 in 2000 could signify different things in different decades because the variance gets scaled to be 1 overall.  Also the mean is affected by outliers.Zero-Median and Unit Variance Scaling:  Similar to above, but more robust against outliers.Min-max scaling:  Set the minimum at 0 and the maximum at 1 for each variable and scale the data for each dataset to that range.  The problem here is that this is affected by outliers and it also does not consider the distribution - althought I may have to assume that it just stays the same.","Creater_id":99445,"Start_date":"2016-08-12 06:30:21","Question_id":229534,"Tags":["distributions","normalization","standardization","optimal-scaling"],"Answer_count":0,"Last_activity":"2016-08-12 06:30:21","Link":"http://stats.stackexchange.com/questions/229534/standardising-these-datasets-so-that-they-are-comparable","Creator_reputation":121}
{"_id":{"$oid":"5837a580a05283111e4d56ce"},"View_count":12,"Display_name":"ceilylou","Question_score":1,"Question_content":"I am using discriminant analysis to determine whether a company is likely to commit earnings fraud.  I have a 100% correct classification in the original analysis and a 46.7% correct classification in the cross-validation. (I'm using SPSS for the analysis. I'm using the \"leave one out\" option of cross-validation.)Is the fact that the results are so different a problem?  Am I doing something wrong?  ","Creater_id":120260,"Start_date":"2016-08-12 06:03:12","Question_id":229529,"Tags":["cross-validation","multivariate-analysis","discriminant"],"Answer_count":0,"Last_activity":"2016-08-12 06:11:36","Link":"http://stats.stackexchange.com/questions/229529/original-analysis-results-versus-cross-validated-results-are-dramatically-differ","Creator_reputation":6}
{"_id":{"$oid":"5837a580a05283111e4d56d0"},"View_count":9,"Display_name":"Akis","Question_score":0,"Question_content":"I am looking online for an analytical derivation of the hierarchical logistic regression model with a CAR prior (it doesnt matter which) and I really cannot find anything. Does anyone know any book, paper or supplementary material that describes all the steps of the MCMC? An alternative could be any online code (preferably in R). Thanks for your consideration.Best, Akis","Creater_id":106636,"Start_date":"2016-08-12 06:08:47","Question_id":229531,"Tags":["hierarchical-bayesian","car"],"Answer_count":0,"Last_activity":"2016-08-12 06:08:47","Link":"http://stats.stackexchange.com/questions/229531/car-models-and-code","Creator_reputation":26}
{"_id":{"$oid":"5837a580a05283111e4d56d2"},"View_count":31,"Display_name":"wildetudor","Question_score":0,"Question_content":"If one wants to see how much variance there is in the responses that various subjects give to a single rating measure, why are measures any more sophisticated than a mere box plot, or error bar with SEMs needed? What do measures such as the Inter-rater reliability tell us, above and beyond simple descriptive statistics of the response distribution? Am I right that the difference lies in the assumption we make about the \"true\" value of the thing being rated, in other words that if we use the Inter-rater Reliability we assume that \"ideal\" subjects would agree and their scores would come close to an objectively-true/measurable value, whereas if using e.g. a box plot, that asuumption need not be made?Is a high inter-rater reliability not simply (qualitatively) equivalent to a low standard deviation/error of the distribution? ","Creater_id":41307,"Start_date":"2016-08-12 03:58:49","Question_id":229506,"Tags":["distributions","inter-rater"],"Answer_count":1,"Last_activity":"2016-08-12 05:43:00","Link":"http://stats.stackexchange.com/questions/229506/inter-rater-reliability-vs-mere-distribution-statistics-e-g-box-plots","Creator_reputation":266}
{"_id":{"$oid":"5837a580a05283111e4d56de"},"View_count":17,"Display_name":"Crops","Question_score":0,"Question_content":"Consider the following models - model1 and model2.library(lme4)library(agridat)data(john.alpha)dat \u0026lt;- john.alpha# `rep`, `block` and `gen` are categorical variables (factors)str(dat)'data.frame':   72 obs. of  5 variables:  rep  : Factor w/ 3 levels \"R1\",\"R2\",\"R3\": 1 1 1 1 1 1 1 1 1 1 ...  gen  : Factor w/ 24 levels \"G01\",\"G02\",\"G03\",..: 11 4 5 22 21 10 20 2 23 14 ... $ yield: num  4.12 4.45 5.88 4.58 4.65 ...# modelsmodel1 \u0026lt;- lmer(yield ~ 1 + rep + (1|gen) + (1|rep:block), dat)model2 \u0026lt;- lmer(yield ~ 1 + rep + gen + (1|rep:block), dat)As rep and gen are factors, vcov() gives the variances of the differences in the expected value from the first level.vcov(model1)3 x 3 Matrix of class \"dpoMatrix\"            (Intercept)       repR2       repR3(Intercept)  0.02105802 -0.01510377 -0.01510377repR2       -0.01510377  0.03020754  0.01510377repR3       -0.01510377  0.01510377  0.03020754vcov(model2)26 x 26 Matrix of class \"dpoMatrix\"            (Intercept)         repR2         repR3        genG02        genG03        genG04        genG05(Intercept)  0.04748541 -1.387503e-02 -1.387503e-02 -3.623006e-02 -3.594567e-02 -3.594567e-02 -3.317053e-02repR2       -0.01387503  2.775005e-02  1.387503e-02 -1.723344e-17 -1.742046e-17 -1.742099e-17 -1.607552e-17repR3       -0.01387503  1.387503e-02  2.775005e-02 -1.657859e-17 -1.709303e-17 -1.709409e-17 -1.577335e-17genG02      -0.03623006 -1.723344e-17 -1.657859e-17  7.246011e-02  3.623006e-02  3.617117e-02  3.343504e-02genG03      -0.03594567 -1.742046e-17 -1.709303e-17  3.623006e-02  7.189134e-02  3.566128e-02  3.551049e-02genG04      -0.03594567 -1.742099e-17 -1.709409e-17  3.617117e-02  3.566128e-02  7.189134e-02  3.571109e-02genG05      -0.03317053 -1.607552e-17 -1.577335e-17  3.343504e-02  3.551049e-02  3.571109e-02  6.630786e-02genG06      -0.03596555 -1.743436e-17 -1.711101e-17  3.577827e-02  3.571109e-02  3.830550e-02  3.292263e-02genG07      -0.03315373 -1.607158e-17 -1.577377e-17  3.295262e-02  3.589269e-02  3.313738e-02  3.293607e-02genG08      -0.03596202 -1.740363e-17 -1.705130e-17  3.898537e-02  3.875395e-02  3.828164e-02  3.320368e-02genG09      -0.03340814 -1.616739e-17 -1.583978e-17  3.625000e-02  3.342494e-02  3.310418e-02  3.298899e-02genG10      -0.03362605 -1.627089e-17 -1.593920e-17  3.670237e-02  3.360970e-02  3.318702e-02  3.338845e-02genG11      -0.03320671 -1.609684e-17 -1.579813e-17  3.305207e-02  3.272121e-02  3.599864e-02  3.300232e-02genG12      -0.03600456 -1.744900e-17 -1.712104e-17  3.628894e-02  3.572017e-02  3.572017e-02  3.297496e-02genG13      -0.03641734 -1.762400e-17 -1.726723e-17  3.947686e-02  3.895789e-02  3.616288e-02  3.337442e-02genG14      -0.03360572 -1.628857e-17 -1.598459e-17  3.363564e-02  3.358583e-02  3.638085e-02  3.081398e-02genG15      -0.03340511 -1.616789e-17 -1.584227e-17  3.602945e-02  3.618024e-02  3.293344e-02  3.061337e-02genG16      -0.03623006 -1.756087e-17 -1.723344e-17  3.623006e-02  3.617117e-02  3.623006e-02  3.298325e-02genG17      -0.03337493 -1.617715e-17 -1.587571e-17  3.335536e-02  3.290597e-02  3.593240e-02  3.059671e-02genG18      -0.03643117 -1.763482e-17 -1.728204e-17  3.903518e-02  3.599214e-02  3.917013e-02  3.318708e-02genG19      -0.03338819 -1.618507e-17 -1.588500e-17  3.320363e-02  3.595894e-02  3.340499e-02  3.059671e-02genG20      -0.03592887 -1.738908e-17 -1.703857e-17  3.878400e-02  3.848634e-02  3.846640e-02  3.592590e-02genG21      -0.03620013 -1.752083e-17 -1.716815e-17  3.902507e-02  3.572846e-02  3.618024e-02  3.339931e-02genG22      -0.03641462 -1.762483e-17 -1.727023e-17  3.923655e-02  3.611066e-02  3.898537e-02  3.592590e-02genG23      -0.03624963 -1.756763e-17 -1.723731e-17  3.655082e-02  3.878716e-02  3.578067e-02  3.298899e-02genG24      -0.03638470 -1.763340e-17 -1.730215e-17  3.665273e-02  3.870431e-02  3.589919e-02  3.330560e-02                   genG06        genG07        genG08        genG09        genG10        genG11        genG12(Intercept) -3.596555e-02 -3.315373e-02 -3.596202e-02 -3.340814e-02 -3.362605e-02 -3.320671e-02 -3.600456e-02repR2       -1.743436e-17 -1.607158e-17 -1.740363e-17 -1.616739e-17 -1.627089e-17 -1.609684e-17 -1.744900e-17repR3       -1.711101e-17 -1.577377e-17 -1.705130e-17 -1.583978e-17 -1.593920e-17 -1.579813e-17 -1.712104e-17genG02       3.577827e-02  3.295262e-02  3.898537e-02  3.625000e-02  3.670237e-02  3.305207e-02  3.628894e-02genG03       3.571109e-02  3.589269e-02  3.875395e-02  3.342494e-02  3.360970e-02  3.272121e-02  3.572017e-02genG04       3.830550e-02  3.313738e-02  3.828164e-02  3.310418e-02  3.318702e-02  3.599864e-02  3.572017e-02genG05       3.292263e-02  3.293607e-02  3.320368e-02  3.298899e-02  3.338845e-02  3.300232e-02  3.297496e-02genG06       7.189789e-02  3.571115e-02  3.567794e-02  3.571443e-02  3.317698e-02  3.324339e-02  3.856500e-02genG07       3.571115e-02  6.630747e-02  3.317008e-02  3.059632e-02  3.054973e-02  3.041477e-02  3.277359e-02genG08       3.567794e-02  3.317008e-02  7.192404e-02  3.576444e-02  3.364240e-02  3.293867e-02  3.553540e-02genG09       3.571443e-02  3.059632e-02  3.576444e-02  6.678306e-02  3.386365e-02  3.041472e-02  3.603776e-02genG10       3.317698e-02  3.054973e-02  3.364240e-02  3.386365e-02  6.725210e-02  3.082820e-02  3.642389e-02genG11       3.324339e-02  3.041477e-02  3.293867e-02  3.041472e-02  3.082820e-02  6.641342e-02  3.558522e-02genG12       3.856500e-02  3.277359e-02  3.553540e-02  3.603776e-02  3.642389e-02  3.558522e-02  7.200911e-02genG13       3.616944e-02  3.315645e-02  3.900443e-02  3.386345e-02  3.665519e-02  3.337442e-02  3.881618e-02genG14       3.360899e-02  3.083058e-02  3.615960e-02  3.058268e-02  3.098214e-02  3.342423e-02  3.319293e-02genG15       3.339931e-02  3.318392e-02  3.597893e-02  3.363974e-02  3.110229e-02  3.019719e-02  3.344411e-02genG16       3.902507e-02  3.575774e-02  3.580738e-02  3.322357e-02  3.342494e-02  3.347475e-02  3.628894e-02genG17       3.616698e-02  3.312355e-02  3.291961e-02  3.081171e-02  3.080085e-02  3.341110e-02  3.597135e-02genG18       3.900853e-02  3.335485e-02  3.638863e-02  3.342197e-02  3.382716e-02  3.369221e-02  3.647371e-02genG19       3.576407e-02  3.337139e-02  3.594510e-02  3.058288e-02  3.059691e-02  3.298919e-02  3.314312e-02genG20       3.546325e-02  3.547689e-02  3.872035e-02  3.337473e-02  3.614981e-02  3.317003e-02  3.551880e-02genG21       3.620341e-02  3.310423e-02  3.574752e-02  3.365951e-02  3.645125e-02  3.599870e-02  3.903415e-02genG22       3.641816e-02  3.315102e-02  3.617652e-02  3.360931e-02  3.407512e-02  3.621622e-02  3.649031e-02genG23       3.626643e-02  3.320323e-02  3.624609e-02  3.369549e-02  3.626996e-02  3.303900e-02  3.886599e-02genG24       3.897532e-02  3.353387e-02  3.611666e-02  3.638772e-02  3.406508e-02  3.336135e-02  3.923552e-02                   genG13        genG14        genG15        genG16        genG17        genG18        genG19(Intercept) -3.641734e-02 -3.360572e-02 -3.340511e-02 -3.623006e-02 -3.337493e-02 -3.643117e-02 -3.338819e-02repR2       -1.762400e-17 -1.628857e-17 -1.616789e-17 -1.756087e-17 -1.617715e-17 -1.763482e-17 -1.618507e-17repR3       -1.726723e-17 -1.598459e-17 -1.584227e-17 -1.723344e-17 -1.587571e-17 -1.728204e-17 -1.588500e-17genG02       3.947686e-02  3.363564e-02  3.602945e-02  3.623006e-02  3.335536e-02  3.903518e-02  3.320363e-02genG03       3.895789e-02  3.358583e-02  3.618024e-02  3.617117e-02  3.290597e-02  3.599214e-02  3.595894e-02genG04       3.616288e-02  3.638085e-02  3.293344e-02  3.623006e-02  3.593240e-02  3.917013e-02  3.340499e-02genG05       3.337442e-02  3.081398e-02  3.061337e-02  3.298325e-02  3.059671e-02  3.318708e-02  3.059671e-02genG06       3.616944e-02  3.360899e-02  3.339931e-02  3.902507e-02  3.616698e-02  3.900853e-02  3.576407e-02genG07       3.315645e-02  3.083058e-02  3.318392e-02  3.575774e-02  3.312355e-02  3.335485e-02  3.337139e-02genG08       3.900443e-02  3.615960e-02  3.597893e-02  3.580738e-02  3.291961e-02  3.638863e-02  3.594510e-02genG09       3.386345e-02  3.058268e-02  3.363974e-02  3.322357e-02  3.081171e-02  3.342197e-02  3.058288e-02genG10       3.665519e-02  3.098214e-02  3.110229e-02  3.342494e-02  3.080085e-02  3.382716e-02  3.059691e-02genG11       3.337442e-02  3.342423e-02  3.019719e-02  3.347475e-02  3.341110e-02  3.369221e-02  3.298919e-02genG12       3.881618e-02  3.319293e-02  3.344411e-02  3.628894e-02  3.597135e-02  3.647371e-02  3.314312e-02genG13       7.280147e-02  3.405171e-02  3.386017e-02  3.668184e-02  3.336109e-02  3.944037e-02  3.614627e-02genG14       3.405171e-02  6.717822e-02  3.079737e-02  3.643066e-02  3.101862e-02  3.666196e-02  3.382041e-02genG15       3.386017e-02  3.079737e-02  6.677701e-02  3.343504e-02  3.312078e-02  3.358665e-02  3.084455e-02genG16       3.668184e-02  3.643066e-02  3.343504e-02  7.246011e-02  3.367612e-02  3.950749e-02  3.621012e-02genG17       3.336109e-02  3.101862e-02  3.312078e-02  3.367612e-02  6.671664e-02  3.620038e-02  3.080085e-02genG18       3.944037e-02  3.666196e-02  3.358665e-02  3.950749e-02  3.620038e-02  7.286234e-02  3.385381e-02genG19       3.614627e-02  3.382041e-02  3.084455e-02  3.621012e-02  3.080085e-02  3.385381e-02  6.674318e-02genG20       3.639745e-02  3.594491e-02  3.337196e-02  3.590930e-02  3.309034e-02  3.615991e-02  3.335479e-02genG21       3.665519e-02  3.359239e-02  3.339179e-02  3.882447e-02  3.598538e-02  3.670243e-02  3.315715e-02genG22       3.924228e-02  3.407159e-02  3.338856e-02  3.925649e-02  3.382728e-02  3.967526e-02  3.358937e-02genG23       3.949346e-02  3.619609e-02  3.606506e-02  3.910476e-02  3.341110e-02  3.927569e-02  3.366647e-02genG24       3.687316e-02  3.357579e-02  3.638167e-02  3.940804e-02  3.379407e-02  3.687020e-02  3.357276e-02                   genG20        genG21        genG22        genG23        genG24(Intercept) -3.592887e-02 -3.620013e-02 -3.641462e-02 -3.624963e-02 -3.638470e-02repR2       -1.738908e-17 -1.752083e-17 -1.762483e-17 -1.756763e-17 -1.763340e-17repR3       -1.703857e-17 -1.716815e-17 -1.727023e-17 -1.723731e-17 -1.730215e-17genG02       3.878400e-02  3.902507e-02  3.923655e-02  3.655082e-02  3.665273e-02genG03       3.848634e-02  3.572846e-02  3.611066e-02  3.878716e-02  3.870431e-02genG04       3.846640e-02  3.618024e-02  3.898537e-02  3.578067e-02  3.589919e-02genG05       3.592590e-02  3.339931e-02  3.592590e-02  3.298899e-02  3.330560e-02genG06       3.546325e-02  3.620341e-02  3.641816e-02  3.626643e-02  3.897532e-02genG07       3.547689e-02  3.310423e-02  3.315102e-02  3.320323e-02  3.353387e-02genG08       3.872035e-02  3.574752e-02  3.617652e-02  3.624609e-02  3.611666e-02genG09       3.337473e-02  3.365951e-02  3.360931e-02  3.369549e-02  3.638772e-02genG10       3.614981e-02  3.645125e-02  3.407512e-02  3.626996e-02  3.406508e-02genG11       3.317003e-02  3.599870e-02  3.621622e-02  3.303900e-02  3.336135e-02genG12       3.551880e-02  3.903415e-02  3.649031e-02  3.886599e-02  3.923552e-02genG13       3.639745e-02  3.665519e-02  3.924228e-02  3.949346e-02  3.687316e-02genG14       3.594491e-02  3.359239e-02  3.407159e-02  3.619609e-02  3.357579e-02genG15       3.337196e-02  3.339179e-02  3.338856e-02  3.606506e-02  3.638167e-02genG16       3.590930e-02  3.882447e-02  3.925649e-02  3.910476e-02  3.940804e-02genG17       3.309034e-02  3.598538e-02  3.382728e-02  3.341110e-02  3.379407e-02genG18       3.615991e-02  3.670243e-02  3.967526e-02  3.927569e-02  3.687020e-02genG19       3.335479e-02  3.315715e-02  3.358937e-02  3.366647e-02  3.357276e-02genG20       7.182453e-02  3.846974e-02  3.637215e-02  3.596504e-02  3.589623e-02genG21       3.846974e-02  7.236705e-02  3.944125e-02  3.648774e-02  3.919663e-02genG22       3.637215e-02  3.944125e-02  7.279604e-02  3.670198e-02  3.919360e-02genG23       3.596504e-02  3.648774e-02  3.670198e-02  7.246605e-02  3.946379e-02genG24       3.589623e-02  3.919663e-02  3.919360e-02  3.946379e-02  7.276939e-02How to get the variance-covariances of the value at each level instead of that for deviation from base level (first level) ?For example the variance-covariance matrix of repR1, repR2, repR3 (3x3 matrix) and genG01 to genG24 (24x24 matrix).","Creater_id":37661,"Start_date":"2016-08-03 21:58:24","Question_id":229525,"Tags":["r","lme4"],"Answer_count":1,"Last_activity":"2016-08-12 05:29:32","Link":"http://stats.stackexchange.com/questions/229525/variance-covariance-matrix-of-the-value-at-each-level-instead-of-that-for-deviat","Creator_reputation":202}
{"_id":{"$oid":"5837a580a05283111e4d56eb"},"View_count":2446,"Display_name":"James","Question_score":3,"Question_content":"I have a set of data from each of the two sites I am measuring for plant species diversity. I plan on using the Simpson's diversity index (SDI), which combines species richness (number of different species) with the number of each individual to form a number between 0 and 1. The mean for the SDI value for each site will be calculated from the different samples at each site. What statistical test should I use for comparing the values for the two sites? I've heard that a t-test may be useful but I'm not sure.","Creater_id":73467,"Start_date":"2015-04-13 12:35:42","Question_id":146188,"Tags":["t-test","ecology"],"Answer_count":2,"Last_activity":"2016-08-12 05:23:25","Link":"http://stats.stackexchange.com/questions/146188/statistical-test-to-perform-on-species-diversity-simpsons-diversity-index","Creator_reputation":16}
{"_id":{"$oid":"5837a580a05283111e4d56f7"},"View_count":57,"Display_name":"Lukasz","Question_score":0,"Question_content":"How should I interpret the reachability plot returned by the optics method from the dbscan package in R.Is there a simple rule of thumb to determine the value of parameters xi or eps_cl to get meaningful clusters in the corresponding methods based on the plot?result \u0026lt;- optics_cut(result, eps_cl = 4e-04)result \u0026lt;- opticsXi(result, xi = 0.005)","Creater_id":126816,"Start_date":"2016-08-12 05:20:05","Question_id":229522,"Tags":["r","clustering"],"Answer_count":0,"Last_activity":"2016-08-12 05:20:05","Link":"http://stats.stackexchange.com/questions/229522/how-to-interpret-the-reachability-plot-of-the-optics-algorithm","Creator_reputation":104}
{"_id":{"$oid":"5837a580a05283111e4d56f9"},"View_count":1168,"Display_name":"Alec","Question_score":3,"Question_content":"I'm trying to understand the derivation of Expected Prediction Error, as described in The  Elements of Statistical Learning. Specifically, it says:      By conditioning on X, we can write EPE as:    And, a footnote for this says:  Conditioning here amounts to factoring the joint density  where  and splitting up the bivariate integral accordingly.I'm confused on the following:Is  equivalent to  and is  equivalent to ? I'm not used to seeing  used without an argument surrounded by parentheses.I don't understand why .What is meant by \"splitting up the bivariate integral accordingly\" in the footnote?Besides Wikipedia, what are some resources I should be looking at so that I don't get confused by these concepts?","Creater_id":2403,"Start_date":"2012-01-13 22:56:45","Question_id":21086,"Tags":["expected-value"],"Answer_count":2,"Last_activity":"2016-08-12 05:11:42","Link":"http://stats.stackexchange.com/questions/21086/factoring-expected-prediction-error","Creator_reputation":665}
{"_id":{"$oid":"5837a580a05283111e4d5707"},"View_count":27,"Display_name":"Arno","Question_score":0,"Question_content":"Let's say I want to model the distribution of race results of an individual athlete, say on the 100 meter dash. What class of distribution would fit this data well? Although the distribution of race results might look normal, it will clearly be skewed, since it is much harder to improve on your personal best than it is to have a really bad result because you stumble or get hurt. Therefore the distribution will have a long tail towards the slow end.Is there any literature on modelling such data?","Creater_id":127455,"Start_date":"2016-08-12 02:42:09","Question_id":229490,"Tags":["distributions","games","density-estimation"],"Answer_count":1,"Last_activity":"2016-08-12 05:09:28","Link":"http://stats.stackexchange.com/questions/229490/modelling-the-distribution-of-race-results-in-sports","Creator_reputation":1}
{"_id":{"$oid":"5837a580a05283111e4d5713"},"View_count":19,"Display_name":"Juan Pablo Soto Quir\u0026#243;s","Question_score":2,"Question_content":"Let's define two random variables , . Also, . We know\\Sigma_{xx}=\\left(  \\begin{array}{cc}    \\sigma_1^2 \u0026amp; \\rho\\sigma_1\\sigma_2 \\\\    \\rho\\sigma_1\\sigma_2 \u0026amp; \\sigma_2 ^2\\\\  \\end{array}\\right),where  is the correlation between  and .What happen with  when  and  have other distribution? (For example Uniform, Exponential, Gamma, ChiSquared, etc.)Is there any reference using other type of distributions to obtain ?","Creater_id":127318,"Start_date":"2016-08-11 18:35:15","Question_id":229451,"Tags":["random-variable","covariance-matrix"],"Answer_count":1,"Last_activity":"2016-08-12 05:08:27","Link":"http://stats.stackexchange.com/questions/229451/bivariate-continuous-disfribution-and-covariance-matrix","Creator_reputation":11}
{"_id":{"$oid":"5837a580a05283111e4d571f"},"View_count":202,"Display_name":"StubbornAtom","Question_score":2,"Question_content":"I want to know if there are any examples of real-life applications of the Laplace and Cauchy density functions. How do they differ in their applications?This related post, however, does not answer my question.","Creater_id":119261,"Start_date":"2016-08-12 00:41:56","Question_id":229474,"Tags":["distributions","pdf","laplace-distribution"],"Answer_count":2,"Last_activity":"2016-08-12 05:03:57","Link":"http://stats.stackexchange.com/questions/229474/practical-applications-of-the-laplace-and-cauchy-distributions","Creator_reputation":140}
{"_id":{"$oid":"5837a580a05283111e4d572d"},"View_count":112,"Display_name":"PRYM","Question_score":0,"Question_content":"In a data set, there is a variable previous_contact which captures how many days have passed since last contact was made. It takes values like 0,1,2 ... 50. And for records when no contact was made earlier, it is NULL.I am trying to build a logistic regression and previous_contact is an independent variable in the model. How to capture the NULL in the variable?One idea which I had was to code NULL as a large number like 9999. It will mean if the person was not contacted before, it is equivalent to him being contacted a very long time back.I was wondering if there is a better way to do this, because won't the result change if we code NULL as 9999 or 99999 or something else?Frequency of NA is ~95% in the data","Creater_id":70633,"Start_date":"2016-08-06 23:28:27","Question_id":228622,"Tags":["logistic","dataset","missing-data"],"Answer_count":2,"Last_activity":"2016-08-12 05:03:42","Link":"http://stats.stackexchange.com/questions/228622/handling-null-value-in-logistic-regression","Creator_reputation":101}
{"_id":{"$oid":"5837a580a05283111e4d573b"},"View_count":21,"Display_name":"Francisco Vargas","Question_score":0,"Question_content":"The problem can be generalized in the following way lets say I have the following pairs error log  and a small specific error signal/message that is what I care of  s.t.  and this error varies message widely depending on what  however the dataset containing the pairs  call it  is very big and in general these bug error messages  have patterns that are moderately easy to pick up.I want to train a modlel that given a log input  spits out a substring  which is the important error message  within . Alternatively returning the start and end of the error  is also an option ( a better option use case wise actually).The approach so far is to binarize  so all the characters in  which are not  become  and the ones which are become  creating the label sequence for  namely .Post to this just train a standard RNN/LSTM structure to learn how to match the sequences  and  within .Does this seem reasonable ? are there any pervious papers on this or better approaches ?","Creater_id":99470,"Start_date":"2016-08-12 05:00:15","Question_id":229514,"Tags":["neural-networks","deep-learning","lstm","sequence-analysis","rnn"],"Answer_count":0,"Last_activity":"2016-08-12 05:00:15","Link":"http://stats.stackexchange.com/questions/229514/using-an-rnn-to-find-bugs-in-massive-error-logs","Creator_reputation":108}
{"_id":{"$oid":"5837a580a05283111e4d573d"},"View_count":210,"Display_name":"Ryan","Question_score":5,"Question_content":"In my machine learning class, we have learned about how LASSO regression is very good at performing feature selection, since it makes use of  regularization.My question: do people normally use the LASSO model just for doing feature selection (and then proceed to dump those features into a different machine learning model), or do they typically use LASSO to perform both the feature selection and the actual regression?For example, suppose that you want to do ridge regression, but you believe that many of your features are not very good. Would it be wise to run LASSO, take only the features that are not near-zeroed out by the algorithm, and then use only those in dumping your data into a ridge regression model? This way, you get the benefit of  regularization for performing feature selection, but also the benefit of  regularization for reducing overfitting. (I know that this basically amounts to Elastic Net Regression, but it seems like you don't need to have both the  and  terms in the final regression objective function.)Aside from regression, is this a wise strategy when performing classification tasks (using SVMs, neural networks, random forests, etc.)?","Creater_id":115090,"Start_date":"2016-05-09 15:48:11","Question_id":211710,"Tags":["feature-selection","lasso","regression-strategies"],"Answer_count":2,"Last_activity":"2016-08-12 04:59:11","Link":"http://stats.stackexchange.com/questions/211710/using-lasso-only-for-feature-selection","Creator_reputation":26}
{"_id":{"$oid":"5837a580a05283111e4d574b"},"View_count":38,"Display_name":"104078","Question_score":1,"Question_content":"if we have  variables, the number of scatter plots we have is :.Why is this so?Is there any one who can explain this formula?","Creater_id":127459,"Start_date":"2016-08-12 02:55:44","Question_id":229494,"Tags":["combinatorics"],"Answer_count":2,"Last_activity":"2016-08-12 04:53:16","Link":"http://stats.stackexchange.com/questions/229494/how-to-calculate-the-number-of-scatter-plots","Creator_reputation":111}
{"_id":{"$oid":"5837a580a05283111e4d5759"},"View_count":56,"Display_name":"Tim","Question_score":6,"Question_content":"Some time ago Xi'an asked What is the equivalent for cdfs of MCMC for pdfs? The naive answer would be to use \"approximate\" Metropolis algorithm in form  Given     1. generate     2. take  X^{(t+1)} = \\begin{cases}   Y \u0026amp; \\text{ with probability } \u0026amp; \\min\\left(  \\frac{F(Y+\\varepsilon) - F(Y-\\varepsilon)}{F(x^{(t)}+\\varepsilon) - F(x^{(t)}-\\varepsilon)} , 1 \\right)\\\\x^{(t)} \u0026amp; \\text{ otherwise.} \\end{cases} where  is a target CDF and  is some small constant. This enables us to use Metropolis algorithm with CDF's.The question is: is there any reason why this may actually be a bad idea?","Creater_id":35989,"Start_date":"2016-07-24 12:44:15","Question_id":225389,"Tags":["simulation","mcmc","cdf","metropolis-hastings"],"Answer_count":1,"Last_activity":"2016-08-12 04:34:21","Link":"http://stats.stackexchange.com/questions/225389/approximate-metropolis-algorithm-does-it-make-sense","Creator_reputation":25400}
{"_id":{"$oid":"5837a580a05283111e4d5766"},"View_count":47,"Display_name":"Ravi","Question_score":0,"Question_content":"I have the following dataframe \u0026gt; str(kmeans)'data.frame':   29271 obs. of  3 variables:  y: num  -1.127 1.195 0.622 3.138 -1.382 ... $ z: num  -0.607 -2.354 1.121 0.318 1.958 ...x, y and z are basically PC1, PC2 and PC3 components after PCA of the original dataset. The original dataset consisted of 10 features. I have performed K-means on kmeans dataset. Below is the graph How do I extract the original features from these clusters. For example if I identify cluster 1 then what are the values of corresponding attributes of the original 10 features not of the principal components.","Creater_id":81470,"Start_date":"2016-06-24 04:18:29","Question_id":220449,"Tags":["r","pca","k-means"],"Answer_count":0,"Last_activity":"2016-08-12 04:34:06","Link":"http://stats.stackexchange.com/questions/220449/how-do-i-predict-original-features-from-pca-components","Creator_reputation":101}
{"_id":{"$oid":"5837a580a05283111e4d5768"},"View_count":33,"Display_name":"Tyler Durden","Question_score":4,"Question_content":"Using data from US vital statistics I created a simulation that was based on the number of children a woman has over her lifetime as follows (out of 10000):0   41001   58002   82003   93804   97805   99006   99607   99858   99959   999810  10000So for example, out of 10000 women 4100 will have no children, 1700 will have 1 child, 2400 will have 2 children and so on. However, when I run the simulation, the average number of children per woman is 1.28, a lot lower than expected. According to real demographic data, women in the US have an average of 1.87 children over their lifetime.To make the value come out as it should (around 1.87) I had to use radically different values:{ 3600, 4800, 6400, 7680, 8780, 9900, 9960, 9985, 9995, 9998, 10000 }Where am I going wrong?In this table why is the 15-50 rate 1.29, but when broken out, the aggregated rate is 1.9?","Creater_id":73875,"Start_date":"2016-08-11 18:32:17","Question_id":229449,"Tags":["demography"],"Answer_count":0,"Last_activity":"2016-08-12 04:30:59","Link":"http://stats.stackexchange.com/questions/229449/my-fertility-simulation-is-not-matching-reality","Creator_reputation":128}
{"_id":{"$oid":"5837a580a05283111e4d576a"},"View_count":126,"Display_name":"chetak","Question_score":1,"Question_content":"I have a data matrix  that has  samples (rows) described by  variables (columns) . I do a SVD to reduce the  dimensions to just 3 dimensions. I understand that the  coordinates (i.e., the SVD values) are calculated from the eigenvectors of .My question is, if I pick an arbitrary point in the SVD space (i.e. a value for SVD1, SVD2, SVD3, not necessarily in the data), is there a convenient way to translate that back to a set of the original variables (i.e., )?","Creater_id":124135,"Start_date":"2016-08-03 09:03:17","Question_id":227104,"Tags":["pca","dimensionality-reduction","svd"],"Answer_count":1,"Last_activity":"2016-08-12 04:13:07","Link":"http://stats.stackexchange.com/questions/227104/reversing-svd-back-to-the-original-variables","Creator_reputation":14}
{"_id":{"$oid":"5837a580a05283111e4d5777"},"View_count":55,"Display_name":"Wudanao","Question_score":1,"Question_content":"Say I want to model the income from people's age, and we know the effect of age is not continuous, despite it being a continuous variable, and an obvious thing to do is to categorize the age variable. So far people have been doing this by breaking at the quantiles or something, but is there a \"best way\" (by whatever sensible criteria) to automatically determine the best breakpoint？","Creater_id":80926,"Start_date":"2016-08-12 01:04:10","Question_id":229476,"Tags":["r","regression","machine-learning","data-mining"],"Answer_count":1,"Last_activity":"2016-08-12 03:59:25","Link":"http://stats.stackexchange.com/questions/229476/is-there-a-way-to-automatically-identify-the-best-categorization-of-a-continuous","Creator_reputation":174}
{"_id":{"$oid":"5837a580a05283111e4d5784"},"View_count":15,"Display_name":"Martin","Question_score":0,"Question_content":"There are methods to measure the performance (or goodness of fit) between two time series (e.g. obs and sim), for example KGE, NSE, RMSE, correlation, , , among others, most of them provided by the package HydroGOF.By changing parameters of the simulation sim, I am trying to find the best parameter, , to get the best fit between sim and obs.However, the curves will be fitted equally across the values. I want to reach better fits in periods of low measurement uncertainty (in my case: high values) ! I.e., the performance measure should focus on best fits where the observational uncertainty is lowest.Given a time series with columns t, sim, obs, obs_error, what is the best way to calculate the goodness of fit with regards to the uncertainty? (assuming obs_error is the standard deviation of a gaussian error distribution). ","Creater_id":127467,"Start_date":"2016-08-12 03:45:59","Question_id":229505,"Tags":["goodness-of-fit","measurement-error","uncertainty","parameter-optimization","measure-theory"],"Answer_count":0,"Last_activity":"2016-08-12 03:45:59","Link":"http://stats.stackexchange.com/questions/229505/time-series-performance-measure-including-uncertainty-r","Creator_reputation":101}
{"_id":{"$oid":"5837a580a05283111e4d5786"},"View_count":60,"Display_name":"user77005","Question_score":0,"Question_content":"Say there are users and they view articles and click (or not click) on articles. I represent the -th user as , a  vector and -th article as , a vector. The reward (click) of user  on article  is .Now say there is a tensor indicator . In matrix terms . is the -th feature of . is the -th feature of . And  represents the affinity of these two features.I want to use logistic regression to determine W. So lets say, Is it correct to say that the gradient of the cost function is  If the above it correct, is it correct to say the gradient descent would bew_{a,b} = w_{a,b}-\\alpha[w_{a.b}-\\Sigma_{i,j}r_{i,j}x_{i,b}z_{j,a}(1-p(r_{ij}|s_{ij}))]I have written the following python code to generate this. Is it correct or is there a better way to do this?if __name__ == '__main__':       data_matrix =np.random.rand(8,6)   print gradient_decent((3,2))def gradient_decent(dimension, iterations=10):   weights = np.ones(dimension)   alpha = 0.001   for k in range(iterations):       gradient_error = weights_by_user_article_features(weights)       estimated_weights = np.subtract(weights,gradient_error)       weights = np.subtract(weights,estimated_weights)   return weightsdef weights_by_user_article_features(original_weights):    weights = np.zeros(original_weights.shape)    for j in range(0, 3):  # number of user features        for k in range(3, 5):  # number of article features            for i in range(0, 8):  # number of records                sigmoid_tensor = tensor_indicator(original_weights,np_data_matrix[i][:]) * np_data_matrix[i][5]                predicted_reward = sigmoid(sigmoid_tensor)                reward_error = 1 - predicted_reward                error = np_data_matrix[i][j] * np_data_matrix[i][k] * np_data_matrix[i][5] * reward_error                               weights[j][k - 3] = weights[j][k - 3] + error               return weightsThis question is related to this paperhttp://www.gatsby.ucl.ac.uk/~chuwei/paper/isp781-chu.pdf. ","Creater_id":100669,"Start_date":"2016-08-10 04:07:06","Question_id":229157,"Tags":["logistic","python","gradient-descent","theano","tensor"],"Answer_count":0,"Last_activity":"2016-08-12 03:36:22","Link":"http://stats.stackexchange.com/questions/229157/logistic-tensor-regression","Creator_reputation":1}
{"_id":{"$oid":"5837a580a05283111e4d5788"},"View_count":6,"Display_name":"Reza_Research","Question_score":0,"Question_content":"I want to use an arbitrary classifier for classification of some data. There exists two labels (+ and -).I have got a bunch of features, but I want to select some of them as inputs of my classifier.Would anyone give me a quick and easy hint on how to select my features (which combination of features)?Thank you very much :)","Creater_id":126608,"Start_date":"2016-08-12 03:35:52","Question_id":229504,"Tags":["feature-selection"],"Answer_count":0,"Last_activity":"2016-08-12 03:35:52","Link":"http://stats.stackexchange.com/questions/229504/how-to-select-the-most-suitable-features-for-my-classifier","Creator_reputation":48}
{"_id":{"$oid":"5837a580a05283111e4d578a"},"View_count":109,"Display_name":"Diogo Santos","Question_score":3,"Question_content":"After reading a bit on the wiki, I though I could find the variance of my variable, but I'm getting confused...Let's assume we have two random variables,  and  which are uncorrelated.Now, what is the variance of:My problem is, from this formula I get:However, the initial definition of D can be rewritten as:which in turns givesMaybe I'm seeing wrong, but the two variance formulas are different","Creater_id":27069,"Start_date":"2016-08-12 03:29:08","Question_id":229502,"Tags":["variance","random-variable"],"Answer_count":1,"Last_activity":"2016-08-12 03:34:36","Link":"http://stats.stackexchange.com/questions/229502/variance-on-operations-of-random-variables","Creator_reputation":108}
{"_id":{"$oid":"5837a580a05283111e4d5797"},"View_count":189,"Display_name":"Valentina","Question_score":0,"Question_content":"I am working on a project and I need some suggestions. I have a data set with 600 songs and for each song we have 60 numerical features (linked to the rhythm and the timbre of the sound). Moreover one or more emotions chosen out of 6 (happy,amazed,angry,sad,quiet,relaxing) are associated to each song. This means that, for each song, we have a vector of 60 numerical features and a binary vector of 6 labels (with one or more ones) and our objective is to be able to classify the songs with respect to the emotions associated starting from their physical features. Do you have any suggestion about multi-label classification algorithms (or others) that we could use? Are there some useful libraries on R?","Creater_id":82354,"Start_date":"2015-07-14 23:49:14","Question_id":161552,"Tags":["r","machine-learning","classification","multilabel"],"Answer_count":3,"Last_activity":"2016-08-12 03:25:13","Link":"http://stats.stackexchange.com/questions/161552/multi-label-classification","Creator_reputation":8}
{"_id":{"$oid":"5837a580a05283111e4d57a6"},"View_count":28,"Display_name":"Pedestrian","Question_score":1,"Question_content":"Diebold \u0026amp; Mariano suggest using Newey-West standard errors to correct for autocorrelation and heteroscedasticity in the error terms when comparing forecast accuracy.However, if I understand correctly, they also assume dependency only for (k-1) day ahead forecast errors.As such, am I correct in assuming that I do not need to use Newey-West standard errors when looking at 1-day ahead forecasts?","Creater_id":122805,"Start_date":"2016-08-12 01:52:23","Question_id":229483,"Tags":["hypothesis-testing","forecasting","autocorrelation"],"Answer_count":1,"Last_activity":"2016-08-12 02:32:05","Link":"http://stats.stackexchange.com/questions/229483/diebold-mariano-test-error-correction-for-1-day-ahead-forecasts","Creator_reputation":16}
{"_id":{"$oid":"5837a580a05283111e4d57b3"},"View_count":110,"Display_name":"Harper","Question_score":-1,"Question_content":"Statistical significance indicates that the difference between 2 variables is real and not due to chance. It doesn’t indicate that the difference is large.  But isn’t saying that a difference is real is also implying that the difference is large enough to be real?","Creater_id":82133,"Start_date":"2016-08-11 23:21:01","Question_id":229466,"Tags":["statistical-significance","effect-size"],"Answer_count":2,"Last_activity":"2016-08-12 02:31:26","Link":"http://stats.stackexchange.com/questions/229466/statistical-significance-paradox","Creator_reputation":31}
{"_id":{"$oid":"5837a580a05283111e4d57c1"},"View_count":15,"Display_name":"Blain Waan","Question_score":0,"Question_content":"Suppose I have estimated the survival probabilities  from  strata. Now I want to get an unbiased estimate of population survival probability  using these stratum-specific estimates. Then what should be the estimator? Should it be ? If I want to adjust for under or over representation in the sample, then what should be the estimator (suppose I know the population and sample proportions)? Thanks for your kind guidance.   ","Creater_id":12603,"Start_date":"2016-08-11 23:53:22","Question_id":229471,"Tags":["estimation","unbiased-estimator","estimators","stratification","point-estimation"],"Answer_count":0,"Last_activity":"2016-08-12 02:14:59","Link":"http://stats.stackexchange.com/questions/229471/how-to-obtain-an-unbiased-post-stratified-estimator-for-survival-probabilities","Creator_reputation":1293}
{"_id":{"$oid":"5837a580a05283111e4d57c3"},"View_count":46,"Display_name":"STC","Question_score":2,"Question_content":"I refer to 'Thirteen Ways to Look at the Correlation Coefficient' byRodgers \u0026amp; Nicewander (1988): http://www.stat.berkeley.edu/~rabbee/correlation.pdfI refer specifically to 2 of the 13 ways of looking at the correlation coefficient, namely: No. 6 \"CORRELATION AS THE MEAN CROSSPRODUCTOF STANDARDIZED VARIABLES\" and No. 8 \"CORRELATION AS A FUNCTION OF THE ANGLE BETWEEN THE TWO VARIABLE VECTORS\". My issue is that No. 6 does NOT appear to be to be a crossproduct (i.e. a vector product between vectors resulting in a pseudovector). It appears to me to be a sum of products of Z-scores. I don't dispute that it is a computationally correct version of the coefficient, just that it is not a crossproduct.Similarly, No. 8 DOES appear to be a product between vectors, namely a normalised dot (or inner) product resulting in the cosine of the angle between vectors. However, it is not referred to in that way even though it would appear to be correct to do so.I am mystified as to where vector terminology is used in the first case, apparently incorrectly, whilst it is not used in the second case when it apparently would have been correct to do so.Could somebody please explain or am I just missing something?","Creater_id":127438,"Start_date":"2016-08-12 01:13:10","Question_id":229478,"Tags":["correlation","coefficient"],"Answer_count":1,"Last_activity":"2016-08-12 02:11:46","Link":"http://stats.stackexchange.com/questions/229478/correlation-coefficient-dot-and-cross-products","Creator_reputation":11}
{"_id":{"$oid":"5837a580a05283111e4d57d0"},"View_count":11,"Display_name":"maple","Question_score":0,"Question_content":"I have a set, suppose , I can calculate the correlation coefficient of each element in a matrix , I want to divide the set into some sub set, Is there any algorithm can help me todo this?","Creater_id":62208,"Start_date":"2016-08-12 02:08:30","Question_id":229485,"Tags":["correlation","clustering"],"Answer_count":0,"Last_activity":"2016-08-12 02:08:30","Link":"http://stats.stackexchange.com/questions/229485/how-to-group-a-set-with-every-elementss-correlation-coefficient","Creator_reputation":136}
{"_id":{"$oid":"5837a580a05283111e4d57d2"},"View_count":121,"Display_name":"Yavor Paunov","Question_score":1,"Question_content":"I am struggling to interpret the results of a binomial logistic regression I did. The experiment has 4 conditions, in each condition all participants receive different version of treatment. DVs (1 per condition)=DE01,DE02,DE03,DE04, all binary (1 - participants take a spec. decision, 0 - don't)Predictors: FTFinal (continuous, a freedom threat scale)SRFinal (continuous, situational reactance scale)TRFinal (continuous, trait reactance scale)SVO_Type(binary, egoists=1, altruists=0)After running these binomial (logit) models, model_soc_inf\u0026lt;- glm(mydataDE01~FTFinal+SRFinal+TRFinal,                     family=binomial(link='logit'),data=mydata)summary(model_soc_inf)model_pers_inf \u0026lt;- glm(mydataDE02~FTFinal+SRFinal+TRFinal,                       family=binomial(link='logit'),data=mydata)model_pers_inf2 \u0026lt;- glm(mydataDE03~FTFinal+SRFinal+TRFinal+SVO_Type,                     family=binomial(link='logit'),data=mydata)model_soc_uninf1\u0026lt;-glm(mydataDE04~FTFinal+SRFinal+TRFinal+SVO_Type,                      family=binomial(link='logit'),data=mydata)model_pers_uninf1\u0026lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,                       family=binomial(link='logit'),data=mydata)summary(model_pers_uninf)I ended up with the following. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:model1\u0026lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)summary(model1)model2\u0026lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)summary(model2)model3\u0026lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)summary(model3)interaction\u0026lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(  link = \"logit\"),data = mydata) As for mediation, I followed  this mediation procedure for logistic regression, but found no mediation. To sum up:There is some relationship between SVO_Type and FTFinal, but I have no clue what.Predicting DE02 from SVO_Type only is not significant.Predicting DE02 from FTFinal is not significantPutitng those two in the regression makes them both significant predictors.Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the \"mediate\" package in R, only to discover that there is a \"treatment\" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. I apologize for this external way of uploading the dataset, it is way too complicated to reproduce here (I am a noob).Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.","Creater_id":98517,"Start_date":"2016-08-12 01:11:19","Question_id":229477,"Tags":["r","regression","logistic","interaction"],"Answer_count":0,"Last_activity":"2016-08-12 01:11:19","Link":"http://stats.stackexchange.com/questions/229477/interactions-in-binomial-logistic-regression","Creator_reputation":28}
{"_id":{"$oid":"5837a580a05283111e4d57d4"},"View_count":29,"Display_name":"Statsquestions1","Question_score":1,"Question_content":"My outcome variable includes six categories but I'm not sure whether they count as \"fully ordered\". The categories measure the extent of added value of a certain intervention: 1 (major added value), 2 (considerable added value), 3 (minor added value), 4 (non-quantifiable added value), 5 (no added value), 6 (less value).My concern is, however, that category 4 (non-quantifiable) is defined as showing that there is added value but that it cannot be quantified because of a lack of data, i.e. the benefit could be either major, considerable or minor. It doesn't mean that category 4 is smaller than category 3 for example. Would it still be possible to use an ordered logit model given that one of the six categories doesn't really fit in the order? Or can the ordered logit model be discarded even without having tested for proportional odds? Thank you very much in advance!","Creater_id":127342,"Start_date":"2016-08-11 05:53:56","Question_id":229345,"Tags":["logistic","logit","ordered-logit"],"Answer_count":0,"Last_activity":"2016-08-12 00:41:29","Link":"http://stats.stackexchange.com/questions/229345/ordered-logit-appropriate-given-my-dependent-variable","Creator_reputation":6}
{"_id":{"$oid":"5837a580a05283111e4d57d6"},"View_count":200,"Display_name":"Desmarais","Question_score":15,"Question_content":"Is there any distribution for two i.i.d. random variables  where the joint distribution of  is uniform over support [0,1]?","Creater_id":127336,"Start_date":"2016-08-11 04:44:14","Question_id":229335,"Tags":["distributions","random-variable"],"Answer_count":2,"Last_activity":"2016-08-12 00:18:43","Link":"http://stats.stackexchange.com/questions/229335/for-i-i-d-random-varianbles-x-y-can-x-y-be-uniform-0-1","Creator_reputation":76}
{"_id":{"$oid":"5837a580a05283111e4d57e4"},"View_count":20,"Display_name":"beetroot","Question_score":1,"Question_content":"This is a follow-up to my previous question.I have 11 plots with plants growing on them. On each plot the same 15 plants are measured for some variable each year (for nine years). My aim is to find out if:in each year there are significant differences of the measured variable between plotson each plot there are significant differences of the measured variable between yearsSo in total there are 1485 observations (11 plots x 15 plants on each plot x 9 years). Robert Long suggested to use a linear model and lsmeans, so here's what I did so far in R and my concerns.dat:  year   id plot  val1 2000 A_01    A 0.592 2000 A_02    A 0.723 2000 A_03    A 0.684 2000 A_04    A 1.265 2000 A_05    A 0.676 2000 A_06    A 0.68...     year   id plot  val1480 2008 K_10    K 0.361481 2008 K_11    K 0.811482 2008 K_12    K 0.391483 2008 K_13    K 0.201484 2008 K_14    K 0.581485 2008 K_15    K 0.96mod \u0026lt;- lm(val ~ year*plot, data = dat)\u0026gt; anova(mod)Analysis of Variance TableResponse: val            Df Sum Sq Mean Sq  F value    Pr(\u0026gt;F)    year         8 76.612  9.5765 140.0725 \u0026lt; 2.2e-16 ***plot        10  7.976  0.7976  11.6663 \u0026lt; 2.2e-16 ***year:plot   80 38.995  0.4874   7.1295 \u0026lt; 2.2e-16 ***Residuals 1380 94.349  0.0684                       ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1So both year and plot as well as their interaction are significant, and in order to find contrasts for differences between plots for each year I guess I can do:library(lsmeans)mod.grid \u0026lt;- ref.grid(mod)cld(lsmeans(mod.grid, ~ plot | year))year = 2000: plot    lsmean         SE   df   lower.CL  upper.CL .group  A    0.7306667 0.06751221 1380 0.59822901 0.8631043  1      C    0.7906667 0.06751221 1380 0.65822901 0.9231043  1      H    0.8273333 0.06751221 1380 0.69489568 0.9597710  12     K    0.8780000 0.06751221 1380 0.74556234 1.0104377  123    D    0.9146667 0.06751221 1380 0.78222901 1.0471043  1234   B    0.9753333 0.06751221 1380 0.84289568 1.1077710  1234   G    1.1180000 0.06751221 1380 0.98556234 1.2504377   2345  J    1.1220000 0.06751221 1380 0.98956234 1.2544377   2345  E    1.1593333 0.06751221 1380 1.02689568 1.2917710    345  I    1.2080000 0.06751221 1380 1.07556234 1.3404377     45  F    1.3606667 0.06751221 1380 1.22822901 1.4931043      5 ...This is - I think - exactly the output I am looking for. However, the residual plots show that.... there is some heteroskedasticity which is probably too severe to be ignored?So I read in this answer by Glen_b that    If sample sizes are equal, you don't have a problem. ANOVA is quite  robust to different variances if the n's are equal.So that may mean that for the model itself I'm ok not to worry (I hope) - but what about lsmeans? I couldn't find any information on whether heteroskedasticity might be problematic there and simply don't understand the concept (and stats in general) well enough to find an answer myself.","Creater_id":45065,"Start_date":"2016-08-12 00:09:39","Question_id":229472,"Tags":["anova","linear-model","residuals","lsmeans"],"Answer_count":0,"Last_activity":"2016-08-12 00:09:39","Link":"http://stats.stackexchange.com/questions/229472/is-heteroskedasticity-a-problem-when-calculating-lsmeans","Creator_reputation":53}
{"_id":{"$oid":"5837a580a05283111e4d57e6"},"View_count":1266,"Display_name":"aardvarkk","Question_score":2,"Question_content":"I'm working on implementing a logistic regression algorithm in code. It's based this link. Unfortunately, the paper doesn't talk about weighting the individual examples . I think the relevant log likelihood function will look something like this:L(\\vec{w}) = \\sum_{i=1}^n \\log{g(y_i z_i)} r_ias opposed to what's in the paper:L(\\vec{w}) = \\sum_{i=1}^n \\log{g(y_i z_i)}where  and  is the instance weight for the given instance . Also,  in this case, and  is the sigmoid function so . This is discussed in the link. Unfortunately, my math skills are not solid enough to be able to solve for the first and second partial derivatives, which are required to perform the optimization. Without the instance weights I'd like to add, the derivatives are:\\frac{\\partial{L}}{\\partial{w_{k}}} = \\sum_{i=1}^{n}y_{i}x_{ik}g(-y_{i}z_{i})\\frac{\\partial^{2}{L}}{\\partial{w_{j}}\\partial{w_{k}}} = -\\sum_{i=1}^{n}x_{ij}x_{ik}g(y_{i}z_{i})g(-y_{i}z_{i})How do these translate with the new  instance weight involved? Thanks!","Creater_id":7549,"Start_date":"2012-11-30 08:26:49","Question_id":44776,"Tags":["regression","machine-learning","logistic"],"Answer_count":1,"Last_activity":"2016-08-11 23:40:28","Link":"http://stats.stackexchange.com/questions/44776/logistic-regression-with-weighted-instances","Creator_reputation":135}
{"_id":{"$oid":"5837a580a05283111e4d57f3"},"View_count":129,"Display_name":"Catherine","Question_score":1,"Question_content":"If I have determined a curvilinear relationship between my dichotomous y and continuous x, what should I do before running a logit regression? Should I log transform my x variable, etc?","Creater_id":24636,"Start_date":"2013-04-21 10:30:39","Question_id":56767,"Tags":["regression","correlation","logistic"],"Answer_count":2,"Last_activity":"2016-08-11 21:45:11","Link":"http://stats.stackexchange.com/questions/56767/what-to-do-with-curvilinear-relationships","Creator_reputation":6}
{"_id":{"$oid":"5837a580a05283111e4d5801"},"View_count":52,"Display_name":"Demenyi Norbert","Question_score":1,"Question_content":"I understand well perceptron so put accent only on kernel but I am not familiar with matemathic expressions so please give me an numerical example and a guide on kernel.For example:My hyperplane of perceptron is x1*w1+x2*w2+x3*w3+b=0; The RBF kernel formula: k(x,z) = exp((-|x-z|^2)/2*variance^2) where takes action the radial basis function kernel here? Is x an input and what is z variable here? Or of what I have to calculate variance if it is variance in the formula? Somewhere I have understood so that I have to plug this formula in perceptron  decision function x1*w1+x2*w2+x3*w3+b=0; but how does it look look like If I plug in?I would like to ask a numerical example to avoid confusion.","Creater_id":127424,"Start_date":"2016-08-11 21:42:08","Question_id":229463,"Tags":["machine-learning","kernel-trick","libsvm"],"Answer_count":0,"Last_activity":"2016-08-11 21:42:08","Link":"http://stats.stackexchange.com/questions/229463/can-you-give-me-a-short-step-by-step-numerical-example-of-radial-basis-function","Creator_reputation":16}
{"_id":{"$oid":"5837a580a05283111e4d5803"},"View_count":203,"Display_name":"Vincent","Question_score":2,"Question_content":"The question is as the title says: what is the difference between A/B Testing and Randomized Control Trials?","Creater_id":66179,"Start_date":"2015-05-04 19:52:23","Question_id":149774,"Tags":["experiment-design","terminology","medicine"],"Answer_count":1,"Last_activity":"2016-08-11 21:33:29","Link":"http://stats.stackexchange.com/questions/149774/what-is-the-difference-between-a-b-testing-and-randomized-control-trials","Creator_reputation":130}
{"_id":{"$oid":"5837a580a05283111e4d5810"},"View_count":14,"Display_name":"CommTask","Question_score":0,"Question_content":"I am considering a linear regression model . where . 's are independent copies of random error  with mean 0 and variance .We have the  hat matrix given by  H = X^T (X^TX)^{-1} XI am trying to show .I know that h_{ij} = X_i^T (X^T X)^{-1} X_j and that  because it is idempotent. This gives me  but I don't know how to prove . Some help would be appreciated!","Creater_id":111092,"Start_date":"2016-08-11 21:26:29","Question_id":229461,"Tags":["multiple-regression","matrix","leverage","inequality"],"Answer_count":0,"Last_activity":"2016-08-11 21:26:29","Link":"http://stats.stackexchange.com/questions/229461/linear-regression-question-on-idempotent-matrix-and-leverage-points","Creator_reputation":26}
{"_id":{"$oid":"5837a580a05283111e4d5812"},"View_count":19,"Display_name":"Matthias","Question_score":0,"Question_content":"I have a unbalanced data set and use Cohen's kappa and AUC as performance measure. Without down sampling the Kappa value is around 0.85, with random down sampling it is 0.95. and with a house-made focused down sampling it is approx 0.75.Which data would you use to train the classifier? The AUC for the three cases are 0.998 without downsampling, 0.998 with random downsampling and 0.921 with house-made downsamplingI'm suspicious that the random down sampling increases the kappa value. The full data set looks like thisWhen I randomly downsampl it looks like thisAnd with a house made sort of focused downsampling it looks like this","Creater_id":121353,"Start_date":"2016-08-05 02:17:29","Question_id":228406,"Tags":["auc","kappa","down-sample"],"Answer_count":1,"Last_activity":"2016-08-11 20:27:39","Link":"http://stats.stackexchange.com/questions/228406/kappa-and-downsampling-selection-of-data-set","Creator_reputation":50}
{"_id":{"$oid":"5837a580a05283111e4d581e"},"View_count":1298,"Display_name":"user2350622","Question_score":5,"Question_content":"I am trying to derive the expression for the variance of  in simple linear regression. I substitute  for , but in the intermediate steps the covariance term  comes up and I don't know how to deal with it. Any help would be appreciated!","Creater_id":43273,"Start_date":"2014-04-07 21:17:31","Question_id":92964,"Tags":["regression","self-study","mathematical-statistics","variance","covariance"],"Answer_count":2,"Last_activity":"2016-08-11 19:02:12","Link":"http://stats.stackexchange.com/questions/92964/covariance-term-in-simple-linear-regression","Creator_reputation":39}
{"_id":{"$oid":"5837a580a05283111e4d582c"},"View_count":201,"Display_name":"Captain Murphy","Question_score":6,"Question_content":"First time posting!I'm trying to create a logit estimator using a looping simulation, where the loop detects the number of correct prediction (my code is below).  Is it possible to change the shock in the distribution (defined as the standard deviation in the rnorm distribution) after a certain number of correct predictions?  I'm trying it out with the f variable initialized below, but with little success.  I was thinking that the f variable could change to 2 after a say 20 correct predictions, for example.  The code below works -- with the hashes hiding a bit of working code -- but the results will not vary based on shock (the standard deviation is constant).Thanks!x\u0026lt;-1:7y\u0026lt;-c(0,0,0,1,0,1,1)n=2000bin1\u0026lt;-rep(NA,n)bin2\u0026lt;-rep(NA,n)right\u0026lt;-NULLb0\u0026lt;-rnorm(1,-4,.1)b1\u0026lt;-rnorm(1,1,.1) n=1000ti=0f\u0026lt;-1iter\u0026lt;-0bin1\u0026lt;-NULLbin2\u0026lt;-NULLright=-1for (i in 1:n) {nright\u0026lt;-NULLnb0\u0026lt;-b0 + rnorm(1,0,1/f)nb1\u0026lt;- b1+ rnorm(1,0,1/f)predict\u0026lt;-((1/(1+exp(- nb0- nb1*x))))for (j in 1:7) {ifelse ( y[j]==1,nright[j]\u0026lt;-predict[j],nright[j]\u0026lt;-1-predict[j])     }nright \u0026lt;- prod(nright)if (nright\u0026gt;right) b0 \u0026lt;-nb0if (nright\u0026gt;right) b1\u0026lt;- nb1bin1[i]\u0026lt;-b0bin2[i]\u0026lt;-b1#   ifelse ( nright \u0026gt; right, iter\u0026lt;-0, iter\u0026lt;-iter+1)#   if (iter \u0026gt; 50)  f\u0026lt;- f/2#   if (f\u0026lt;.05) stop(\"Done\")if (nright\u0026gt;right) right\u0026lt;-nrightti\u0026lt;-ti+1}   f   ti  b0b1","Creater_id":7188,"Start_date":"2011-11-02 14:27:32","Question_id":17864,"Tags":["r","standard-deviation","predictive-models","simulation"],"Answer_count":1,"Last_activity":"2016-08-11 18:18:18","Link":"http://stats.stackexchange.com/questions/17864/creating-an-estimator-with-varying-shock-levels-sd-in-r","Creator_reputation":186}
{"_id":{"$oid":"5837a580a05283111e4d5839"},"View_count":12067,"Display_name":"Adam SA","Question_score":11,"Question_content":"I am hoping you can give me some suggestions. I am teaching in a very diverse (made of minority groups) college and the students are mostly Psychology majors. Most students are fresh from high school but some of them are older returning students above 40. Most of the students have motivational problems and aversion to math. But I am still looking for a book that covers the basic curriculum: from descriptive to sampling and testing all the way to ANOVA, and all in the context of experimental methods. The department requires me to use SPSS in class, but I like the idea of building the analysis in a spreadsheet such as excel.p.s. the other teachers use a book that I don't like because of the extensive reliance on computational formulae. I find using these computational formulas - rather than the more intuitive and computationally intensive formula that is consistent with the rational and basic algorithm- unintuitive, unnecessary and confusing. This is the book I refer to Essentials of Statistics for the Behavioral Sciences, 7th Edition Frederick J Gravetter State University of New York, Brockport Larry B. Wallnau State University of New York, Brockport ISBN-10: 049581220XThank you for reading!","Creater_id":1084,"Start_date":"2013-01-23 07:29:18","Question_id":48347,"Tags":["references","teaching"],"Answer_count":8,"Last_activity":"2016-08-11 18:03:41","Link":"http://stats.stackexchange.com/questions/48347/any-suggestions-for-a-good-undergraduate-introductory-textbook-to-statistics","Creator_reputation":291}
{"_id":{"$oid":"5837a580a05283111e4d584d"},"View_count":30,"Display_name":"Arsl\u0026#225;n","Question_score":0,"Question_content":"I have been building several time series models over the past few week, using ARIMA, ANN and others, surprisingly Bayesian Time series or BATS in R, seems to give me good results. I tried to look up its working but there is not much explanation available. I'm guessing it works on the principle of Markov Chains, but not sure completely. Any advice?","Creater_id":106704,"Start_date":"2016-08-11 17:56:25","Question_id":229446,"Tags":["time-series","bayesian","hidden-markov-model"],"Answer_count":0,"Last_activity":"2016-08-11 17:56:25","Link":"http://stats.stackexchange.com/questions/229446/how-do-bayesian-time-series-models-work","Creator_reputation":27}
{"_id":{"$oid":"5837a580a05283111e4d584f"},"View_count":51,"Display_name":"Achyuta Aich","Question_score":0,"Question_content":"Since a MLP can implement any function. I have the following code, using which I am trying to implement the AND function. But what I find that on running the program multiple times, I end up getting different predicted values. Why is this happening ? Also how does one determine which type of activation function has to be provided at different layers ?from sknn.mlp import Regressor,Layer,Classifierimport numpy as np   X_train = np.array([[0,0],[0,1],[1,0],[1,1]])y_train = np.array([0,0,0,1])nn = Classifier(layers=[Layer(\"Softmax\", units=2)],learning_rate=0.001,n_iter=25)nn.fit(X_train, y_train)X_example = np.array([[0,0],[0,1],[1,0],[1,1]])y_example = nn.predict(X_example)print (y_example)","Creater_id":89178,"Start_date":"2016-03-18 10:24:59","Question_id":202399,"Tags":["machine-learning","neural-networks","python","deep-learning","scikit-learn"],"Answer_count":1,"Last_activity":"2016-08-11 17:14:44","Link":"http://stats.stackexchange.com/questions/202399/different-predictions-on-multiple-run-of-the-same-algorithm-scikit-neural-networ","Creator_reputation":18}
{"_id":{"$oid":"5837a580a05283111e4d585c"},"View_count":12,"Display_name":"Konstantin","Question_score":0,"Question_content":"Many people upload and/or share files on internet. How can one measure the success of such a file? For example we can look at the download count. The more download the more success. However we can't compare two different file according to this value, because for example one of the files were uploaded and shared much earlier than the other file, so there were much more time for the downloaders to download it. Then we can consider and calculate the daily or maybe weekly download count, dividing the download count by the elapsed time in days or in week. This could be a good measurement, but unfortunately freshly shared files are downloaded much more frequently then older files, so when making such a list sorted by the daily download count fresh files always tend to be at the first places of the list.What other approach should one follow to calculate a success measurement unit which compensate for the time of availability of the shared files.","Creater_id":127413,"Start_date":"2016-08-11 16:10:49","Question_id":229440,"Tags":["dataset","measurement"],"Answer_count":0,"Last_activity":"2016-08-11 16:10:49","Link":"http://stats.stackexchange.com/questions/229440/how-can-i-measure-the-success-of-an-uploaded-and-shared-file","Creator_reputation":101}
{"_id":{"$oid":"5837a580a05283111e4d585e"},"View_count":25,"Display_name":"haff","Question_score":0,"Question_content":"I have a couple 2x2 contingency tables with which I'm testing for difference between groups with the statement 'I play video games' (either yes or no). The variables I'm using are gender (female vs. male) and race (minority vs. non-minority). Just for clarity, my research questions are,\"Is whether or not a person plays video games dependent on gender?\" and \"Is whether or not a person plays video games dependent on minority status?\"I elected to use a chi-square test for these two questions since all cells have fairly large frequencies (nothing less than 5). Here's the problem: I also want to test video game usage (binary - yes or no, just like before) with academic standing which has three categories: underclassman, upperclassman, and graduate student. This is a 2x3 contingency table, and in looking at the raw frequencies there are clear differences between groups 1 and 3, but the chi-square test on all three groups does not detect this. Partitioning the table and solely comparing groups 1 and 3 indeed results in a very small p-value.So to get around this, I thought I'd use Kruskal-Wallis as an omnibus test (since it will detect differences if at least two groups are different) and do individual post hoc (kruskal wallis?) tests to find the significant results. But is Kruskal Wallis appropriate with binary data? Will it be difficult to defend why I used a different test on gender and race than I did on academic standing? Would it be better to just do chi-square on all combinations (a total of 3) of my academic standing groups? Does any of this even matter?","Creater_id":126943,"Start_date":"2016-08-11 15:54:43","Question_id":229437,"Tags":["categorical-data","chi-squared","binary-data","kruskal-wallis"],"Answer_count":0,"Last_activity":"2016-08-11 15:54:43","Link":"http://stats.stackexchange.com/questions/229437/three-chi-square-tests-or-one-kruskal-wallis-plus-post-hoc-tests","Creator_reputation":106}
{"_id":{"$oid":"5837a580a05283111e4d5860"},"View_count":20,"Display_name":"Jackson","Question_score":1,"Question_content":"I'm creating an Android app which can use a variety of classification formula, and while I have normal Softmax done correctly, I keep having an issue with the Softmax Neural Network. After about 10 iterations, all of the dot values on the forward pass become negatives, so their values are stored as 0, causing the resulting layers of h1, h2, and scores to become 0+bias. This means that every result is the same, regardless of input. I feel like somewhere there is a typo in my code, but I've repeatedly compared and contrasted it with my partner's code (which is written for iOS) and can't find any differences. Hopefully someone here can tell me where I've gone wrong.public List gradient(List weights, double[] X, int Y, int D, int K, double L, int nh){        int length = Dnh + nh + nhnh + nh + nh*K + K;        List grad = new ArrayList(length);    List\u0026lt;Double\u0026gt; W01 = new ArrayList\u0026lt;Double\u0026gt;(D*nh);    List\u0026lt;Double\u0026gt; b1 = new ArrayList\u0026lt;Double\u0026gt;(nh);    List\u0026lt;Double\u0026gt; W12 = new ArrayList\u0026lt;Double\u0026gt;(nh*nh);    List\u0026lt;Double\u0026gt; b2 = new ArrayList\u0026lt;Double\u0026gt;(nh);    List\u0026lt;Double\u0026gt; W23 = new ArrayList\u0026lt;Double\u0026gt;(nh*K);    List\u0026lt;Double\u0026gt; b3 = new ArrayList\u0026lt;Double\u0026gt;(K);    //Parse Parameters    int count = 0;    int end = count + D*nh;    while(count \u0026lt; end){        W01.add(weights.get(count));        count++;    }    end = count + nh;    while(count \u0026lt; end){        b1.add(weights.get(count));        count++;    }    end = count + nh*nh;    while(count \u0026lt; end){        W12.add(weights.get(count));        count++;    }    end = count + nh;    while(count \u0026lt; end){        b2.add(weights.get(count));        count++;    }    end = count + nh*K;    while(count \u0026lt; end){        W23.add(weights.get(count));        count++;    }    end = count + K;    while(count \u0026lt; end){        b3.add(weights.get(count));        count++;    }    //Forward Pass    double dot;    double sum;    List\u0026lt;Double\u0026gt; h1 = new ArrayList\u0026lt;Double\u0026gt;(nh);    for(int i = 0; i \u0026lt; nh; i++){        dot = 0;        for(int j = 0; j \u0026lt; D; j++){            dot += X[j]*W01.get(i + j*(nh));        }        sum = dot + b1.get(i);        if(sum \u0026gt; 0) {            h1.add(sum);        }        else{            h1.add(0.0);        }    }    List\u0026lt;Double\u0026gt; h2 = new ArrayList\u0026lt;Double\u0026gt;(nh);    for(int i = 0; i \u0026lt; nh; i++){        dot = 0;        for(int j = 0; j \u0026lt; nh; j++){            dot += h1.get(j)*W12.get(i + j*(nh));        }        sum = dot + b2.get(i);        if(sum \u0026gt; 0) {            h2.add(sum);        }        else{            h2.add(0.0);        }    }    List\u0026lt;Double\u0026gt; scores = new ArrayList\u0026lt;Double\u0026gt;(K);    for(int i = 0; i \u0026lt; K; i++){        dot = 0;        for(int j = 0; j \u0026lt; nh; j++){            dot += h2.get(j)*W23.get(i + j*(K));        }        sum = dot + b3.get(i);        scores.add(sum);    }    //scoreMax used to prevent overflow    double scoreMax = 0;    double score;    for(int i = 0; i \u0026lt; K; i++){        score = scores.get(i);        if(score \u0026gt; scoreMax){            scoreMax = score;        }    }    //denom = Σ(i:k) exp(Θ_i · X)    double denom = 0;    for(int i = 0; i \u0026lt; K; i++){        denom += Math.exp(scores.get(i) - scoreMax);    }    List\u0026lt;Double\u0026gt; probs = new ArrayList\u0026lt;Double\u0026gt;(K);    for(int i = 0; i \u0026lt; K; i++) {        //prob_i = exp(Θ_i · X)/denom        probs.add(Math.exp(scores.get(i) - scoreMax) / denom);    }    //Backward Pass    List\u0026lt;Double\u0026gt; dProbs = new ArrayList\u0026lt;Double\u0026gt;(K);    for(int i = 0; i \u0026lt; K; i++){        if(i == Y){            dProbs.add(probs.get(i) - 1);        }        else{            dProbs.add(probs.get(i));        }    }    List\u0026lt;Double\u0026gt; dW01 = new ArrayList\u0026lt;Double\u0026gt;(D*nh);    List\u0026lt;Double\u0026gt; db1 = new ArrayList\u0026lt;Double\u0026gt;(nh);    List\u0026lt;Double\u0026gt; dh1 = new ArrayList\u0026lt;Double\u0026gt;(nh);    List\u0026lt;Double\u0026gt; dW12 = new ArrayList\u0026lt;Double\u0026gt;(nh*nh);    List\u0026lt;Double\u0026gt; db2 = new ArrayList\u0026lt;Double\u0026gt;(nh);    List\u0026lt;Double\u0026gt; dh2 = new ArrayList\u0026lt;Double\u0026gt;(nh);    List\u0026lt;Double\u0026gt; dW23 = new ArrayList\u0026lt;Double\u0026gt;(nh*K);    List\u0026lt;Double\u0026gt; db3 = new ArrayList\u0026lt;Double\u0026gt;(K);    for(int i = 0; i \u0026lt; nh; i++){        for(int j = 0; j \u0026lt; K; j++){            dW23.add(dProbs.get(j)*h2.get(i) + L*W23.get(j*nh + i));        }    }    for(int i = 0; i \u0026lt; K; i++){        db3.add(dProbs.get(i));    }    for(int i = 0; i \u0026lt; nh; i++){        dot = 0;        for(int j = 0; j \u0026lt; K; j++){            dot += dProbs.get(j) * W23.get(j + i*K);        }        if(h2.get(i) \u0026gt; 0){            dh2.add(dot);        }        else{            dh2.add(0.0);        }    }    for(int i = 0; i \u0026lt; nh; i++){        for(int j = 0; j \u0026lt; nh; j++){            dW12.add(dh2.get(j)*h1.get(i) + L*W12.get(j*nh + i));        }    }    for(int i = 0; i \u0026lt; nh; i++){        db2.add(dh2.get(i));    }    for(int i = 0; i \u0026lt; nh; i++){        dot = 0;        for(int j = 0; j \u0026lt; nh; j++){            dot += dh2.get(j) * W12.get(j + i*(nh));        }        if(h1.get(i) \u0026gt; 0){            dh1.add(dot);        }        else{            dh1.add(0.0);        }    }    for(int i = 0; i \u0026lt; D; i++){        for(int j = 0; j \u0026lt; nh; j++){            dW01.add(dh1.get(j)*X[i] + L*W01.get(j*D + i));        }    }    for(int i = 0; i \u0026lt; nh; i++){        db1.add(dh1.get(i));    }    grad.addAll(dW01);    grad.addAll(db1);    grad.addAll(dW12);    grad.addAll(db2);    grad.addAll(dW23);    grad.addAll(db3);    return grad;}","Creater_id":127412,"Start_date":"2016-08-11 15:48:32","Question_id":229436,"Tags":["neural-networks","gradient-descent","backpropagation","java","softmax"],"Answer_count":0,"Last_activity":"2016-08-11 15:48:32","Link":"http://stats.stackexchange.com/questions/229436/neural-network-probabilities-converging-to-biases","Creator_reputation":6}
{"_id":{"$oid":"5837a580a05283111e4d5862"},"View_count":69,"Display_name":"Creatron","Question_score":2,"Question_content":"Going through this online class, the notes specify the following. (In the highlights):I understand how we get a maximum likelihood interpretation. What I do not get is:Why does using the Frobenious norm of the weight matrix  as a regularizer, lead to the interpretation of the weight matrix  having a prior as being sampled from a gaussian? How is this arrived at? And what if the regularization term was something other than the frobenius norm?(And to that last point, does this mean that every element in the matrix  is sampled from a gaussian?)Thank you.","Creater_id":27158,"Start_date":"2016-08-11 12:32:51","Question_id":229415,"Tags":["machine-learning","normal-distribution","maximum-likelihood","regularization","posterior"],"Answer_count":1,"Last_activity":"2016-08-11 15:29:50","Link":"http://stats.stackexchange.com/questions/229415/why-is-regularization-interpreted-as-a-gaussian-prior-on-my-weights","Creator_reputation":452}
{"_id":{"$oid":"5837a580a05283111e4d586f"},"View_count":66,"Display_name":"feynmanisabro","Question_score":1,"Question_content":"I'm trying to run PCA on sample covariance matrices of various sizes (ranging between 20 x 20 to 4000 x 4000). Assume the data follows a joint multivariate normal distribution. While derivations are great, I'm asking from an applied perspective. Bonus points for easy-to-understand papers and packages in R that help with the following:Is there a way to test the statistical significance of the eigenvalues? Or assign a probability of a given eigenvalue occurring?How does the relative magnitude of the eigenvalues change as we scale up the size of the covariance matrix? E.g. Let's say the first 3 principal components explain ~80% of the variance for a 20x20 matrix (not sure if this is true in practice). If we were to scale this up to a 4000x4000 matrix, would we still expect to see ~80% of the variance explained by the first 3 PCs?Assume the population covariance matrix is not diagonal and is of full rank. Thanks!","Creater_id":127394,"Start_date":"2016-08-11 11:49:32","Question_id":229404,"Tags":["pca","dimensionality-reduction","covariance-matrix","eigenvalues","wishart"],"Answer_count":1,"Last_activity":"2016-08-11 15:14:33","Link":"http://stats.stackexchange.com/questions/229404/covariance-matrix-eigenvalue-distribution-relation-to-size","Creator_reputation":6}
{"_id":{"$oid":"5837a580a05283111e4d587c"},"View_count":687,"Display_name":"Hacking Bear","Question_score":1,"Question_content":"I have had some success training my deep neural network (with ReLU hidden units) by first normalizing the features of my data set to zero-mean-unit-variance. Each sample of my data set has 600+ values. Majority of them are continuous but a handful or columns are symbolic / discrete (represent by fixed range integers). Currently I just normalize all columns to zero-mean-unit-variance the same way. Should I only normalize the continuous value columns? Are there better data normalization strategies?","Creater_id":78081,"Start_date":"2015-06-14 15:22:51","Question_id":156971,"Tags":["neural-networks","normalization","continuous-data","discrete-data","deep-learning"],"Answer_count":1,"Last_activity":"2016-08-11 14:49:10","Link":"http://stats.stackexchange.com/questions/156971/how-to-normalize-mixed-continuous-discrete-features-for-dnn","Creator_reputation":125}
{"_id":{"$oid":"5837a580a05283111e4d5888"},"View_count":11,"Display_name":"Omar Shehab","Question_score":0,"Question_content":"I am going through Quantum Boltzmann Machine by Amin et al. In Secton IV, the authors give an example. I am trying to understand how the training data set was generated for the example. Let me quote the relevant section.  In this Section we describe a few toy examples illustrating the ideas  described in the previous sections. In all examples studied, the  training data was generated as a mixture of  factorized  distributions (modes), each peaked around a random point. Every mode   is constructed by randomly selecting a center point   with  and  using Bernoulli distribution:  , where   is the probability of qubit  being aligned with , and   is the Hamming distance between  and . The average  probability distribution over  such modes gives our data  distribution      P^{data}_v = \\frac{1}{M} \\sum^M_{k=1} p^{N-d^k_v}(1-p)^{d^k_v}     In all our examples, we choose  and .My questions:What is a factorized distribution?What is a mixture in statistics?Is the center point a set of  numbers each from the binary ?Is it a multivariate data set?","Creater_id":27686,"Start_date":"2016-08-11 13:53:02","Question_id":229428,"Tags":["distributions","dataset","bernoulli-distribution","centering","mode"],"Answer_count":0,"Last_activity":"2016-08-11 13:53:02","Link":"http://stats.stackexchange.com/questions/229428/understanding-the-generation-of-a-dataset","Creator_reputation":140}
{"_id":{"$oid":"5837a580a05283111e4d588a"},"View_count":92,"Display_name":"akinn","Question_score":1,"Question_content":"I have fitted a SARIMA model to my time series. The diagnostics of the residuals are all good (ACF, PACF, ...), i.e. it seems they behave like white noise.But when I plot the normal qq-plot, they seem to be asymmetrically distributed. I tried doing a Student's  qq-plot and the errors seems to be corrected. My only problem is that now, I do not know how to interpret this result. What does it mean for the residuals to follow a Student's  distribution?","Creater_id":116448,"Start_date":"2016-05-22 02:13:21","Question_id":213873,"Tags":["time-series","distributions","arima","residuals"],"Answer_count":2,"Last_activity":"2016-08-11 13:45:18","Link":"http://stats.stackexchange.com/questions/213873/residuals-of-sarima-follow-students-t-distribution-implications","Creator_reputation":106}
{"_id":{"$oid":"5837a580a05283111e4d5898"},"View_count":1513,"Display_name":"Patrick","Question_score":33,"Question_content":"Can someone please offer a nice succinct explanation why it is not a good idea to teach students that a p-value is the prob(their findings are due to [random] chance).  My understanding is that a p-value is the prob(getting more extreme data | null hypothesis is true).    My real interest is what is the harm of telling them it is the former (aside from the fact it is simply not so).","Creater_id":6072,"Start_date":"2011-10-12 19:55:04","Question_id":16939,"Tags":["p-value","randomness","education"],"Answer_count":7,"Last_activity":"2016-08-11 13:36:45","Link":"http://stats.stackexchange.com/questions/16939/why-is-it-bad-to-teach-students-that-p-values-are-the-probability-that-findings","Creator_reputation":590}
{"_id":{"$oid":"5837a580a05283111e4d58ab"},"View_count":21,"Display_name":"Ye Tian","Question_score":0,"Question_content":"I am reviewing a logistic regression model describing customer default. Out of around 36M observations, around 50K are defaults (=1). Since it takes a very long time to fit logistic regressions on such a large dataset (even with SGD), the original model builder performed variable selection on a subset of the 36M observations. The subset was selected by Keeping all of the 50K defaultsRandomly selecting 5M from the remaining non-defaults (=0)Combining the observations from steps 1 and 2 aboveThe model builder then selected 8 variables from a total of 250 possible predictors using the subset.  I don't disagree that using a subset is time efficient. However, I believe that more statistical tests should be performed in order to ensure that a subset selected this way is representative of the full sample. So far the only addition to the procedure that I can recommend is to perform KS tests (if they don't take forever given sample size) to compare the distributions of all selected predictors of 1) the total dataset and of 2) the sub-dataset.Any insights to this problem is greatly appreciated.","Creater_id":82624,"Start_date":"2016-08-11 13:30:14","Question_id":229426,"Tags":["feature-selection"],"Answer_count":0,"Last_activity":"2016-08-11 13:30:14","Link":"http://stats.stackexchange.com/questions/229426/variable-selection-using-a-subset-of-data","Creator_reputation":329}
{"_id":{"$oid":"5837a580a05283111e4d58ad"},"View_count":14,"Display_name":"JohnAndrews","Question_score":0,"Question_content":"Let's say we have a(n) (positive) outcome rate which we try to optimize. Features affects this outcome.In a test, we find that users that used a specific feature X have a higher positive outcome rate than users that have not used this feature.What is then the meaning of uplift regarding to the positive outcome rate? Concretely, how would one measure what the uplift in the positive outcome rate would be if all new users started using feature X?Example:Nb users with positive outcome: 10Nb users with positive outcome with feature X: 8Nb users with positive outcome without feature X: 2Nb new users with feature X and thus a positive outcome: 3What is then the uplift in the positive outcome if all new users would use feature X? ","Creater_id":97958,"Start_date":"2016-08-11 13:29:21","Question_id":229425,"Tags":["hypothesis-testing","predictive-models","feature-selection","marketing"],"Answer_count":0,"Last_activity":"2016-08-11 13:29:21","Link":"http://stats.stackexchange.com/questions/229425/understanding-uplift-in-a-b-testing","Creator_reputation":180}
{"_id":{"$oid":"5837a580a05283111e4d58af"},"View_count":12,"Display_name":"areyoujokingme","Question_score":0,"Question_content":"I've been playing around with the NetworkAnalyzer functions in Cytoscape, and the same functions in igraph in R, and most of these functions are based on single node metrics that measure the importance of a single node in various ways (see list below). Node Important Metrics- Centrality measures (PageRank, Degree, Eigenvector, Betweenness)- Clustering coefficient- Histogram of shortest path length between a given node and all other nodes in networkMy question: What measures/statistics exist that would allow me to characterize the importance of a subset of nodes (that is, a subset with size greater than 1 node), using the full network as a background? I'd like, ideally, a probability (p-value), to help interpret the result. I'd like to use R or Cytoscape, ideally.If the statistic you have in mind does not output a probability, please explain how I can interpret this subset's statistic, without enumerating all possible subsets in the background network?","Creater_id":54883,"Start_date":"2016-08-11 13:27:00","Question_id":229424,"Tags":["networks","subset","igraph"],"Answer_count":0,"Last_activity":"2016-08-11 13:27:00","Link":"http://stats.stackexchange.com/questions/229424/statistics-to-characterize-subset-amidst-background-network","Creator_reputation":119}
{"_id":{"$oid":"5837a580a05283111e4d58b1"},"View_count":56464,"Display_name":"Random","Question_score":87,"Question_content":"What are the main differences between performing principal component analysis (PCA) on the correlation matrix and on the covariance matrix? Do they give the same results?","Creater_id":17,"Start_date":"2010-07-19 12:39:08","Question_id":53,"Tags":["correlation","pca","covariance","covariance-matrix","correlation-matrix"],"Answer_count":6,"Last_activity":"2016-08-11 13:25:17","Link":"http://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance","Creator_reputation":580}
{"_id":{"$oid":"5837a580a05283111e4d58c3"},"View_count":5,"Display_name":"Brock","Question_score":0,"Question_content":"I have a randomized control trial (RCT) population where we have applied a behavioural intervention to the treatment population.  I am able to measure the (small) effect of the intervention in the treatment population, but we believe only a handful of individuals in this population are acting due to our intervention.  Although the effect size in the treatment group is tiny, we believe the effect size on the actors is fairly large.Our goal is to find segments of the population that will allow us to identify the group of individuals who have the largest effect size.  There are various uncontrolled variables in this trial, which means that the randomized treatment and control need to be preserved in order to ensure statistical significance of our measurement.  In addition to this, resolving the effect size is difficult due to its size, so the number of segments we use must be minimal and roughly equal sized (the one with most of the actors could be smaller, since the resolution depends on the average effect size within the group and the number of individuals in the group).Our attempts at this so far have been to cluster the population along various socio-economic and physical dimensions with some success.  I am happy continuing this method as it provides us with some insight on why people are acting; however, knowledge of the most likely actors can be very helpful in other projects and therefore I want to make sure I am doing everything possible to extract this information.  My team is trying some other indirect approaches, but I can't help but think that there must be a way to achieve this automagically.Ideally I would develop an algorithm that recursively segments the population to reveal the smallest group with the largest effect (while still being resolvable statistically).  The difficulty is that anything I do must preserve RCT nature of the treatment and control populations.","Creater_id":94236,"Start_date":"2016-08-11 12:57:19","Question_id":229418,"Tags":["clustering","treatment-effect","segmentation","cluster-sample"],"Answer_count":0,"Last_activity":"2016-08-11 12:57:19","Link":"http://stats.stackexchange.com/questions/229418/segmenting-a-randomized-control-to-find-the-groups-with-largest-effect-size","Creator_reputation":11}
{"_id":{"$oid":"5837a580a05283111e4d58c5"},"View_count":1262,"Display_name":"norbertk","Question_score":2,"Question_content":"[UPDATE]Terms I used:- Weighted average: weighted arithmetic mean- \"Unweighted\" average: arithmetic meanI went through some of the forums here and looked for explanations online but I couldn't figure out the answer for this:If percentages represent probabilities, what does weighted and \"unweighted\" average of a list of values represent?Example:There are two shops that sell vegetables and two that don't sell vegetables. All of them ask their customers for feedback on their shopping. They say either that they were happy or they were not. Let's assume that every customers answers.Veg Shop 1 has 10 customers and 5 are happy. (50% average)Veg Shop 2 has 20 customers and 8 are happy. (40% average)Non-Veg Shop 1 has 15 customers and 5 are happy. (33% average)Non-Veg Shop 2 has 25 customers and 10 are happy. (40% average)Weighted average of Veg Shops is 43% (13 / 30), unweighted average is 45%.Weighted average of Non-Veg Shops is 37.5% (15 / 40), unweighted average is 36.5%.I know that with weighted average distribution matters and I feel that it implies that there is correlation between the values aggregated. However, in this example, if I count weighted average I accept that there is correlation, but I discard the correlation between Veg and Non-Veg shops.I can't really phrase the probabilities, but it feels that unweighted average is an answer to \"my expected performance (based on customer feedback) among other shops of the same type\", and weighted average is an answer to \"the chance of a customer being happy, when going to any of the shops of the same type\".It is still not really clear for me, though. Does anybody have some clearer ideas on what the probabilities would mean?Also, is it possible at all to assign meaningful values to both weighted and unweighted average on the same dataset?Thanks,Norbert","Creater_id":109679,"Start_date":"2016-03-24 00:42:28","Question_id":203420,"Tags":["probability","average","weighted-mean"],"Answer_count":2,"Last_activity":"2016-08-11 12:34:11","Link":"http://stats.stackexchange.com/questions/203420/weighted-average-vs-unweighted-average-in-probability","Creator_reputation":11}
{"_id":{"$oid":"5837a580a05283111e4d58d3"},"View_count":89,"Display_name":"Steven Stewart-Gallus","Question_score":2,"Question_content":"The most common kind of deviation is the standard deviation. \\text{Sd}(x) = \\sqrt{\\text{Mean}((x - \\text{Mean}(x))^2)}The standard deviation is very similar to the mean absolute deviance or \\text{MAD}(x) = \\text{Mean}(|x - \\text{Mean}(x)|)but is often simpler to calculate or obeys nice algebraic properties.But there are a lot of other measures of variance. For example, there is the most common absolute deviance from the mean value: .It is not clear to me why I should prefer the standard deviation over other kinds of measures of dispersion. I suppose the simplest answer is that this measure of dispersion is the most highly studied and well known and using other methods of dispersion will confuse people you try to communicate with.I guess I would have to use these kind of alternative measures for pathological distributions like the Cauchy distribution though.","Creater_id":111109,"Start_date":"2016-08-09 12:44:06","Question_id":229059,"Tags":["standard-deviation"],"Answer_count":3,"Last_activity":"2016-08-11 12:28:25","Link":"http://stats.stackexchange.com/questions/229059/why-should-i-prefer-the-standard-deviation-over-other-measures-of-variance","Creator_reputation":166}
{"_id":{"$oid":"5837a580a05283111e4d58e2"},"View_count":9,"Display_name":"German Demidov","Question_score":0,"Question_content":"I have a data that can be crazy distributed, but1) it can be divided into components with quite good separation2) one component is Gaussian and we know its mean.Other components can be not normal and probably small (so they can be called \"outliers\"), but number of data points in other components can be \u003e\u003e50%.Is there a way to extract only one central component with known mean? I was thinking about iterative increasing the range around the known mean and testing goodness-of-fit. The other approach is to estimate density with Parzen's window and construct tangent lines using the density around the known mean, so my tangent lines will show exactly ± under the assumption that central component is normal.The method I am looking for can be not exact, but give a good approximation and work reasonably fast (not 1 minute for 1000 of points). Do you know if there are such algorithms?","Creater_id":100673,"Start_date":"2016-08-11 11:42:48","Question_id":229400,"Tags":["normal-distribution","fitting"],"Answer_count":0,"Last_activity":"2016-08-11 11:49:26","Link":"http://stats.stackexchange.com/questions/229400/extraction-of-the-normal-component-with-the-known-mean","Creator_reputation":344}
{"_id":{"$oid":"5837a580a05283111e4d58e4"},"View_count":28,"Display_name":"Theoden","Question_score":0,"Question_content":"For example in count data modeling, imagine modeling count data generated from dispersed negative binomial using Poisson distribution. The poisson regression fits the significant covariates and they turn out to be strongly significant. Yet the goodness of fit test fails, because the underlying distribution is wrongly assumed. Without assuming knowledge of the negative binomial underneath, how can I proceed with the Poisson regression, what are the limitations or is it even sound to proceed when the GOF test fails?","Creater_id":60655,"Start_date":"2016-08-09 10:54:12","Question_id":229037,"Tags":["regression","confidence-interval","goodness-of-fit","count-data"],"Answer_count":1,"Last_activity":"2016-08-11 11:48:39","Link":"http://stats.stackexchange.com/questions/229037/if-the-goodness-of-fit-test-fails-how-should-i-treat-the-fitted-linear-model","Creator_reputation":107}
{"_id":{"$oid":"5837a580a05283111e4d58f1"},"View_count":20,"Display_name":"Kelvin","Question_score":0,"Question_content":"What is the best (most reliable and robust) test to measure deviations from tiny expected proportions (e.g., p0 \u0026lt; 10^-6) in a huge sample?Binomial? Poisson? Negative binomial? Something else?How do these tests differ?  What are their limitations?And how to calculate power/sample size for each test?","Creater_id":109510,"Start_date":"2016-03-29 10:13:25","Question_id":204391,"Tags":["binomial","poisson","negative-binomial"],"Answer_count":1,"Last_activity":"2016-08-11 11:47:26","Link":"http://stats.stackexchange.com/questions/204391/what-is-the-best-test-to-measure-deviations-from-tiny-proportions","Creator_reputation":398}
{"_id":{"$oid":"5837a580a05283111e4d58f3"},"View_count":133,"Display_name":"nfmcclure","Question_score":4,"Question_content":"I'm currently trying to verify the probabilities and outcomes for three card poker.  The game is essentially the same as 5 card poker, but with three cards.  The game is played against the dealer.If we assume one standard deck, I can get the probabilities for each outcome of dealing three cards. (See bottom of post for explanation of a standard deck and 3-card poker hands).Total Possible Outcomes:C(52,3)=22,100Straight Flush: (# of suits) * (# of start ranks)4*12=48Three of a Kind: (Choose a rank) * (Choose three suits)C(13,1)*C(4,3)=52Straight: (Choose a starting rank) * (Choose Three Suits) - (# Straight Flushes)C(12,1)*C(4,1)^{3}-48=720Flush: (Choose a suit) * (Choose 3 ranks) - (# Straight Flushes)C(4,1) * C(13,3) - 48 = 1,096Pair: (Choose a rank) * (Choose 2 suits) * (Choose a 3rd card)C(13,1)*C(4,2)*48=3,744Ace High or less:22,100-\\sum(3,744+1,096+720+52+48)=16,440Admittedly, I took the lazy way out on the last one.  I just found out that these counts and probabilities are on the \"Wizard of Odds\" site:http://wizardofodds.com/games/three-card-poker/This is great, and my counts are verified.But, I also want to count the total possible outcomes when considering a player AND the dealer.Where:Total Possible Outcomes: (# of ways to get 6 from 52) * (# of ways to get 3 player cards from 6)C(52,6)*C(6,3)=407,170,400Here is where I start to run into problems.  The wizard of odds site has the outcomes broken out as follows:  Hand                  Outcome              Count---------------------------------------------------------| Straight Flush      | Player Wins        | 617,044    |---------------------------------------------------------| Straight Flush      | Player Ties        | 144        |---------------------------------------------------------| Straight Flush      | Player Loses       | 968        |---------------------------------------------------------| Straight Flush      | Dealer Not Qualify | 266,196    |---------------------------------------------------------| Three of a Kind     | Player Wins        | 665,776    |---------------------------------------------------------| Three of a Kind     | Player Ties        | 0*         |---------------------------------------------------------| Three of a Kind     | Player Loses       | 3,312      |---------------------------------------------------------| Three of a Kind     | Dealer Not Qualify | 288,960    |---------------------------------------------------------...*Note: Obviously with only one deck, you cannot tie with the dealer on a three of a kind.I'm mostly drawing a blank here. How are these calculated.  I think I can do the easiest one though.  144 possible ties with a straight flush.  This is just:Tie with a Straight Flush: (# of ways to get a straight flush) * (# of suits left (3))48*3=144Player Loses with a Straight Flush: Here's my attempt. In order to lose while having a straight flush, the dealer must get a straight flush with a higher starting rank. So if you are dealt a A-2-3, the dealer must have a straight flush that starts with a 2 or higher.  There are 4*11=44 ways a dealer can have that.  Since there are 4 ways you can get the A-2-3, the total ways you can lose would be 4*4*11 = 176.  If we extend this for all possible straight flushes:4*4*(11+10+9+8+7+6+5+4+3+2+1+0)=1,056\\neq968EDIT (Thanks for @soakley for pointing this out)For the above, I'm over counting the situation where the dealers flush overlaps with the player's suit.  When the straight flush overlaps, the dealer only has three suits to choose from. Since that can only happen twice, we reduce the count by 2 and add 3*2=6 to each sum. Making the calculation slightly more complicated:2+4\\sum_{i=0}^{9}(4i + 6)=4*((4*9+6)+(4*8+6)+(4*7+6)+(4*6+6)+(4*5+6)+(4*4+6)+(4*3+6)+(4*2+6)+(4*1+6)+(4*0+6)+2) = 968The (4*0+6) term is when the player has a 10-J-Q hand, and the dealer can only win by having 3 suits for J-Q-K and 3 suits for Q-K-A = 6 choices.Player Wins with a Straight Flush:  Ok last question, I won't keep updating this with each outcome, I promise. Here is my strategy for this outcome: 1) The player has a straight flush, and the dealer has less than a straight flush (or = +) 2) the player has a straight flush and the dealer has a lesser straight flush. Any strategy pointers on this would be appreciated.My final question: Thanks to @soakley, and to writing out larger and larger formulas, I'm getting bogged down. Would the best strategy be to write a program that loops through all hands and counts the outcomes? This isn't for a class, but more for a generic presentation about probabilities for this game. Is a simulation acceptable for such purposes?Please let me know if I was unclear about anything, or if you need more information.  Any pointers would be greatly appreciated.Edit to include definitionsStandard Deck: Consists of 52 cards. Each Card has a rank and suit. There are 4 suits (Hearts, Diamonds, Clubs, and Spades). There are 13 ranks (in order: A, 2, 3, ..., 9, 10, Jack, Queen, King). One small caveat is that Ace can stand for value 1 OR the value above the King (14).Standard 3 card poker rules: You are to make the best hand out of 3 cards. If two hands are equivalent, the tie is broken by highest rank.  It is possible for ties to occur if all the ranks are equivalent.Possible hand types (in order from lowest worth to highest worth):\"High Card\": The hand does not contain any of the below special hands, and is usually said with the highest of the three card ranks. E.g. \"10, 6, 4\" is called \"Ten high\" and beats a nine-high hand or lower.\"Pair\": Exactly two of the three cards have the same rank without having any of the below hands.\"Flush\": All three of the cards have the same suit without having any of the below hands.\"Straight\": All three of the cards have consecutive ranks. E.g. \"5, 6, 7\" or \"A, 2, 3\" or \"Q, K, A\". You cannot have a wrap-around straight (i.e., the hand \"K-A-2\" is not a straight).\"Three of a Kind\": All three of the cards have the same rank. E.g. \"5, 5, 5\".\"Straight Flush\": The three cards make a straight and a flush. E.g. \"5H, 6H, 7H\".  It is also worth mentioning the highest hand, called a \"Royal Straight Flush\" which is a Q, K, A in the same suit (note: there are only 4 possible royal flushes).","Creater_id":36947,"Start_date":"2015-10-07 19:18:39","Question_id":175976,"Tags":["probability","poker"],"Answer_count":1,"Last_activity":"2016-08-11 11:23:48","Link":"http://stats.stackexchange.com/questions/175976/count-outcomes-in-three-card-poker","Creator_reputation":121}
{"_id":{"$oid":"5837a580a05283111e4d58f5"},"View_count":16,"Display_name":"wildetudor","Question_score":0,"Question_content":"I know the recommendation to report effect sizes is often accompanied by that to also report confidence intervals. However I'm not sure if the two refer to the same variables.If I want to ascertain the statistical significance and effect size of the difference in some variable X between population A and population B, I might report a t- \u0026amp; a p-value and an effect size measure (e.g. Cohen's d). The CIs would presumably apply to the population estimates (or is it more meaningful for it to apply to the effect size itself?).But then would the CIs (at 95%) not simply be mean+-1.96SEM, i.e. be graphically identical to an error bar (assuming it defined by the 95%-CI?)? If so, then I don't understand how reporting CIs complements reporting effect sizes, since essentially both could, at a first approximation, be eye-balled from the plot with error bars. Or?","Creater_id":41307,"Start_date":"2016-08-11 11:15:56","Question_id":229391,"Tags":["confidence-interval","effect-size"],"Answer_count":0,"Last_activity":"2016-08-11 11:15:56","Link":"http://stats.stackexchange.com/questions/229391/how-do-confidence-intervals-complement-effect-size","Creator_reputation":266}
{"_id":{"$oid":"5837a581a05283111e4d58f7"},"View_count":14,"Display_name":"Sus20200","Question_score":0,"Question_content":"  If we have an ordinary renewal process  in which s are the inter-renewal times and the number of renewals in time  is , we have the following relation  \\frac{N(t)}{t}\\xrightarrow{a.s}\\lambda  where  is the rate of the renewal process and . Now, assume that . How can we prove the following?  \\frac{N(t)}{\\sqrt{t}}\\xrightarrow{a.s}1My attemp: If we assume  is the  time of arrival, then we know . Therefore, \\frac{\\sqrt{t}N(t)}{S_{N(t)+1}}\u0026lt;\\frac{N(t)}{\\sqrt{t}}\u0026lt;\\frac{\\sqrt{t} N(t)}{S_{N(t)}}I don't know how to continue from here.","Creater_id":96063,"Start_date":"2016-08-10 08:00:50","Question_id":229200,"Tags":["probability","stochastic-processes","spatial","convergence","asymptotics"],"Answer_count":0,"Last_activity":"2016-08-11 11:04:48","Link":"http://stats.stackexchange.com/questions/229200/asymptotic-relation-for-renewal-process","Creator_reputation":183}
{"_id":{"$oid":"5837a581a05283111e4d58f9"},"View_count":45,"Display_name":"user2675516","Question_score":0,"Question_content":"Is it possible to model using Bayesian Network (probabilistic graphical model) if you have no idea at all of the interaction of the variables in the data? From my reading, I find that Bayesian network is very intuitive and aligns well with human reasoning. As such, I think that it should have a high performance. However, if one has no knowledge of the dataset, say a dataset for a hospital patients, how can one decide on the interaction between the nodes in the network? Or is this a big flaw of BN?","Creater_id":79151,"Start_date":"2016-08-11 07:31:23","Question_id":229360,"Tags":["bayesian-network","probabilistic-programming"],"Answer_count":1,"Last_activity":"2016-08-11 10:38:54","Link":"http://stats.stackexchange.com/questions/229360/is-it-possible-to-model-using-bayesian-network-probabilistic-graphical-model-i","Creator_reputation":53}
{"_id":{"$oid":"5837a581a05283111e4d58fb"},"View_count":24,"Display_name":"badim","Question_score":0,"Question_content":"When using linear regression we often check:  if the p-values for the estimated coefficients are below some threshold (say 0.1)if there is high correlation between any pair of variablesif VIF of any variable is highif the distribution of model residuals is Gaussian and i.i.d. (no auto-correlation, no heteroscedasticity)Now consider a problem where we have a large number of explanatory variables and we are searching for a subset of the variables that would give a good model. We can do an exhaustive search (or step-wise regression) across subsets of available variables and use model selection criteria such as adjusted-R2, BIC, AIC, cross validation error, etc. Could any of the above also be used as a model selection criterion? That is, would it make sense to search for a model with small p-values, small correlations and VIFs and Gaussian iid residuals?edit: the question is not about what is the standard approach for model selection in the above context, which can be found in standard textbooks and has already been answered in other questions. It rather asks what are the dangers and/or benefits (if any) of using statistics such as p-values, VIF, residual normality and auto-correlation tests for model selection purposes.","Creater_id":127289,"Start_date":"2016-08-10 14:13:17","Question_id":229268,"Tags":["regression","feature-selection","least-squares","model-selection"],"Answer_count":0,"Last_activity":"2016-08-11 10:35:25","Link":"http://stats.stackexchange.com/questions/229268/variable-selection-for-linear-regression","Creator_reputation":21}
{"_id":{"$oid":"5837a581a05283111e4d58fd"},"View_count":116,"Display_name":"RustyStatistician","Question_score":1,"Question_content":"I am curious to know if there are methods that exists for sequential modeling of binary outputs? Let me give an example to help further clarify the question: I have a problem where I have binary outcomes and some covariate information that I want to model using some sort of statistical model for binary outputs (think logistic regression type models).  My setup is the following, I start with some initial set of data, say of size , at time  and for each subsequent time point  I run an experiment and then receive an additional output which is binary.  Now, in my situation, I get binary outcomes, however, in the beginning I am almost surely guaranteed to have all of my  outcomes equal to 0 (in my case say the binary outcome is either 0 or 1).  Furthermore as I collect more data, I may not see an outcome of a 1 for some time. So in this situation logistic regression becomes infeasible because I can't fit the model if I have only observed one type outcome. So now that the problem setup has some context, is there a sequential method that is appropriate for modeling this type of data scheme? Or is there a suggestion of how this kind of data should be handled?  Ultimately at each iteration of the algorithm I want to be able to fit a model, then use that model to predict something, and based on that prediction I then get a new data outcome so being able to build a model that predicts well is also tantamount. ","Creater_id":98920,"Start_date":"2016-04-18 10:53:38","Question_id":208026,"Tags":["logistic","classification","predictive-models","binary-data","sequential-analysis"],"Answer_count":1,"Last_activity":"2016-08-11 10:10:16","Link":"http://stats.stackexchange.com/questions/208026/sequential-classification-methods","Creator_reputation":126}
{"_id":{"$oid":"5837a581a05283111e4d58ff"},"View_count":86,"Display_name":"Adrien W.","Question_score":3,"Question_content":"I want to compute the Kullback-Leibler divergence (KL) of two Gaussians, the first with mean of 1 and the second -1, where both have the same variance say, 1.In MATLAB, the distributions are:   y1 = normpdf([-10:0.1:10], -1, 1)y2 = normpdf([-10:0.1:10],  1, 1)The code I used to compute the KL is:  KL = sum(Apdf .* (log2(Apdf)-log2(Bpdf))) Are these the inputs I should use for the KL? The result I got is 28, shouldn't it be 2?","Creater_id":127352,"Start_date":"2016-08-11 07:36:02","Question_id":229361,"Tags":["matlab","kullback-leibler"],"Answer_count":2,"Last_activity":"2016-08-11 09:53:46","Link":"http://stats.stackexchange.com/questions/229361/kullback-leibler-divergence-and-probability-distribution-function-in-matlab","Creator_reputation":18}
{"_id":{"$oid":"5837a581a05283111e4d5901"},"View_count":149,"Display_name":"Tim","Question_score":4,"Question_content":"Beta distribution is related to binomial being also the distribution for order statistics. Probability mass function of binomial distribution is f(k) = {n \\choose k} p^k (1-p) ^{n-k} \\tag{1} Probability density function of beta distribution is g(p) = \\frac{1}{\\mathrm{B}(\\alpha, \\beta)} p^{\\alpha-1} (1-p)^{\\beta-1} \\tag{2} we can rewrite  in (1) as \\frac{1}{(n+1) \\mathrm{B}(k+1, n-k+1)} if we substitute  and  then (1) becomes \\frac{1}{(n+1) \\mathrm{B}(\\alpha, \\beta)} p^{\\alpha-1} (1-p)^{\\beta-1} So basically, beta is a distribution of  proportions in  trials where average proportion is denoted as  \\frac{1}{\\mathrm{B}(n\\mu+1, n(1-\\mu)+1)} p^{n\\mu} (1-p)^{n(1-\\mu)} \\tag{3} Are you familiar with any references or examples of such usage of beta? Most literature on statistic analysis with proportions (that I found) seems to describe only binomial distribution and beta-binomial Bayesian model rather than dealing directly with beta.","Creater_id":35989,"Start_date":"2016-02-05 03:40:40","Question_id":194182,"Tags":["references","binomial","proportion","beta-distribution"],"Answer_count":2,"Last_activity":"2016-08-11 09:51:02","Link":"http://stats.stackexchange.com/questions/194182/beta-as-distribution-of-proportions-or-as-continuous-binomial","Creator_reputation":25400}
{"_id":{"$oid":"5837a581a05283111e4d5903"},"View_count":87,"Display_name":"SwingingStrawberry","Question_score":0,"Question_content":"This is probably ridiculously simple but because I have to include the word 'negative' in my searches I'm constantly finding only posts and websites that discuss negative correlation which isn't what I'm stuck on.Essentially, I have two variables and want to explore their correlation. As one is ordinal I'll be using Spearman's rank order correlation. The other variable was originally test scores (0-100) but they were transformed (I don't know the procedure involved; the data is supplied from a large dataset) to account for the fact that some test items were harder than others. The transformation created logits (don't understand this) where the scores are now ranging from -3 to +2.What I'm not sure about is whether I can run a correlation using one variable that includes both positive and negative values?","Creater_id":116661,"Start_date":"2016-07-26 15:23:29","Question_id":225795,"Tags":["correlation"],"Answer_count":1,"Last_activity":"2016-08-11 09:49:56","Link":"http://stats.stackexchange.com/questions/225795/correlation-when-one-variable-has-both-positive-and-negative-values","Creator_reputation":32}
{"_id":{"$oid":"5837a581a05283111e4d5905"},"View_count":18,"Display_name":"osazuwa","Question_score":0,"Question_content":"I've looked at some other questions on bootstrap significance testing:Non-parametric bootstrap p-values vs confidence intervalsComputing p-value using bootstrap with Rp-value vs. confidence interval obtained in Bootstrapping...and mine is a bit different.  I think my problem might be specific to cases where the parameters under the null are on the boundary of the parameter space.  Suppose you do a case-resampling bootstrap, where you resample observations with replacement, fit a model, and get a bootstrap distribution for the parameter you are trying to estimate.Now suppose the parameter and your estimator are strictly positive.  For example, REML estimates of variance of random effects in random/mixed effect models, or estimates of information gain in classification trees.  I have found often that often the canonical null hypotheses in these cases is is that the parameter = 0.  Example: Random Effects ModelingI'll use a random effects model as a specific example.  A parametric bootstrap to test the hypothesis that the variance of the random effect is 0 might go as follows:Fit the model with the random effect using maximum likelihood.Do the same for the model without the random effect.Calculate a likelihood test statisticRepeat steps 1:3 using a response simulated with resampled errors. Do this n times, obtaining a bootstrap distribution of likelihood test statisticsCalculate the bootstrap p-value of the original test-statistic using the bootstrap distribution.Now suppose you want to use a REML estimator.  The model with and without the random effect are not comparable using the likelihood test.  So why not try a case-resampling bootstrap?  In a case-resampling bootstrap, one would obtain a bootstrap distribution of variance estimates for that random effect using data resampled with replacement.  But then what?  If we were trying to do inference on a regression coefficient, one might conclude in favor of the null that a regression coefficient is 0 if the bootstrap confidence interval contains 0. But this is never the case for an estimator that is \u003e= 0. How does one evaluate the significance of the estimate in this case?","Creater_id":29336,"Start_date":"2016-08-11 09:00:38","Question_id":229372,"Tags":["hypothesis-testing","bootstrap","regression-coefficients","reml"],"Answer_count":1,"Last_activity":"2016-08-11 09:48:27","Link":"http://stats.stackexchange.com/questions/229372/how-does-case-resampling-bootstrap-work-for-positive-value-estimators","Creator_reputation":190}
{"_id":{"$oid":"5837a581a05283111e4d5907"},"View_count":29,"Display_name":"Programmer2134","Question_score":1,"Question_content":"Assume we have a neural network with stochastic gradient descent used for backpropagation, and therefore each element in the training set is used once to calculate the error, and then to adjust the weights (assume each element in the training set is used only once).Assume we're doing a simple regression without regularization, and with 1 or 2 hidden layers.I can't quite understand why such an algorithm doesn't, through learning at training point N, \"undo\" learning that it did with training point N-1, N-2, etc.Is such an algorithm even supposed to converge to the optimal parameters? My intuition would say that it would \"hops back and forth\", because it doesn't take into account all the data all at once.","Creater_id":127096,"Start_date":"2016-08-11 09:15:34","Question_id":229376,"Tags":["gradient-descent","backpropagation"],"Answer_count":0,"Last_activity":"2016-08-11 09:15:34","Link":"http://stats.stackexchange.com/questions/229376/how-does-a-neural-network-with-stochastic-backpropagation-make-sure-it-doesnt","Creator_reputation":143}
{"_id":{"$oid":"5837a581a05283111e4d5909"},"View_count":56,"Display_name":"fungs","Question_score":4,"Question_content":"What is the best way to determine an exponent  of a vector of probabilies  so that the sum of the power-transformed vector equals one? is a large sample and contains values between zero and one. Let's assume that there is at least one value different from zero and at most one value at one. I'm looking for a fast computational method to find . I'm also interested in approximate, numerical approaches.","Creater_id":48258,"Start_date":"2016-08-10 09:47:21","Question_id":229224,"Tags":["probability","power","transform"],"Answer_count":2,"Last_activity":"2016-08-11 09:10:45","Link":"http://stats.stackexchange.com/questions/229224/find-power-transform-to-normalize-vector-to-unity","Creator_reputation":44}
{"_id":{"$oid":"5837a581a05283111e4d590b"},"View_count":136,"Display_name":"user35792","Question_score":0,"Question_content":"I have 5 different computational tasks. I executed both tasks on two laptops named LA and LB. Out of 5 tasks, 4 tasks were computed in less time on LB, while 1 task took less time on LA. I want to measure, that the achieved performance gain is significant or not. Below is my data example:           LA        LBTask1      2.3       4.1 (mean values of 10 executions)Task2      5.6       2.3Task3      10.5      3.4Task4      15.2      4.6Task5      11.3      3.1I have the following questions:1. Which test should I apply to find the significance?2. Can I apply that test on mean values rather than real 10 execution values?3. Which parameters (Alpha, P, t) of the test must be presented in table and discussed, as I cannot present/discuss all parameters due to space limitation.Thank you","Creater_id":35792,"Start_date":"2013-12-06 14:41:19","Question_id":78810,"Tags":["statistical-significance"],"Answer_count":2,"Last_activity":"2016-08-11 08:16:24","Link":"http://stats.stackexchange.com/questions/78810/test-to-measure-the-significane-of-performance-enhancement","Creator_reputation":1}
{"_id":{"$oid":"5837a581a05283111e4d590d"},"View_count":2458,"Display_name":"swiecki","Question_score":22,"Question_content":"I have a question about something that my statistics teacher said about the following problem:There are two hospitals named Mercy and Hope in your town. You must choose one of these in which to undergo an operation. You decide to base your decision on the success of their surgical teams. Fortunately, under the new health plan, the hospitals give data on the success of their operations, broken down into five broad categories of operations. Suppose you get the following data for the two hospitals:Mercy HospitalType         A    B      C    D      E    AllOperations  359  1836   299   2086  149  4729Successful  292  1449   179   434   13   2366Hope Hospital Type          A   B  C   D   E   AllOperations   88 514 222 86  45   955Successful   70 391 113 12  2    588You notice that, in all types of operations, Mercy has a higher success rate than Hope, yet Hope has the highest overall success rate. Which hospital would you choose and why (choose two answers)?A) Mercy; since I would go in for a specific operation, I want the hospital that has the best success rate for that operation.B) Hope; since they do fewer operations in all categories, they are not \"operation-happy\" like Mercy.C) Hope; this is an example of Simpson's paradox and we should always chose the \"obvious\" conclusion.D) Mercy; looking at column E, Mercy clearly does more difficult surgeries and so is probably a better hospital.E) Hope; it has the better overall success rate.F) Mercy; this is an example of Simpson's paradox and we should always chose the opposite of the \"obvious\" conclusion.My question isn't even about the occurrence of Simpson's paradox in this situation. My question is simply about the fact that my professor insists that A) and D) are the right answers instead of A) and F). He says,   \"Because the success rate is so low for Type E surgeries,we can  conclude that they are difficult and not just uncommon.  Hence, Mercy  probably has better equipment/doctors when compared to Hope.\"I don't understand how he could imply on a statistical basis that he can tell that Mercy does \"more difficult surgeries\". It is obvious that Mercy has better success rate at type E surgeries, but why does that mean they do \"more difficult surgeries\". I think I am being screwed over by the wording of this problem and the professor isn't budging. Can someone please explain why I am wrong or how I can explain this to the professor?","Creater_id":8803,"Start_date":"2012-01-28 14:13:16","Question_id":21896,"Tags":["self-study","confounding","simpsons-paradox"],"Answer_count":3,"Last_activity":"2016-08-11 08:12:56","Link":"http://stats.stackexchange.com/questions/21896/basic-simpsons-paradox","Creator_reputation":213}
{"_id":{"$oid":"5837a581a05283111e4d590f"},"View_count":42,"Display_name":"Bill Kavvas","Question_score":1,"Question_content":"I want to create a classification model, derived from real spectral measurements. My data is in the form of samples x features. The number of features is 424 and the number of samples is in the order of tens of thousands for each specimen. By using e.g. a non linear SVM classifier, I can see that some specimens are differentiated very well and some others are confused one with each other. By looking at the raw data, the misclassified ones are indeed very similar. Is there any official metric that can tell if two datasets are separable or not? I would like to know this to see if there is any point in trying furtherly to improve the classification results or not.Thank you a lot in advance!","Creater_id":125500,"Start_date":"2016-08-11 05:12:40","Question_id":229340,"Tags":["classification","separability"],"Answer_count":1,"Last_activity":"2016-08-11 07:56:09","Link":"http://stats.stackexchange.com/questions/229340/how-can-i-prove-if-2-datasets-are-separable-or-not","Creator_reputation":18}
{"_id":{"$oid":"5837a581a05283111e4d5911"},"View_count":60,"Display_name":"Mads Friis","Question_score":1,"Question_content":"How can I make a plot in R of the region, where it must fulfill\\mathbb{E}\\big[\\log(x Z^2_t+y)\\big]\u0026lt;0,where .Thanks!","Creater_id":127337,"Start_date":"2016-08-11 04:50:57","Question_id":229337,"Tags":["r","data-visualization"],"Answer_count":1,"Last_activity":"2016-08-11 07:54:56","Link":"http://stats.stackexchange.com/questions/229337/visualizing-a-region-on-x-y-plane-in-r","Creator_reputation":13}
{"_id":{"$oid":"5837a581a05283111e4d5913"},"View_count":16,"Display_name":"wildetudor","Question_score":0,"Question_content":"Say that, for N subjects, I have measures for two variables X and Y, and I compute a Pearson's R for each subject. I then take these subject-wise R values and do various other stats on them, such as computing group-level confidence intervals, cross-validations etc. My question is: does the p-value that each R-value \"came\" with need to be taken into consideration in these further analyses (i.e. only the significant R's 'survive'), or is it OK to disregard these p-values and just do the stats on the R-values without worrying about their within-subject significance?","Creater_id":41307,"Start_date":"2016-08-11 07:53:07","Question_id":229363,"Tags":["correlation","statistical-significance"],"Answer_count":0,"Last_activity":"2016-08-11 07:53:07","Link":"http://stats.stackexchange.com/questions/229363/need-to-take-into-account-r-values-p-values","Creator_reputation":266}
{"_id":{"$oid":"5837a581a05283111e4d5915"},"View_count":33,"Display_name":"Jonny Lomond","Question_score":0,"Question_content":"Two GLMs of the same distribution are fit to the same set of data, using Maximum Likelihood Estimation (MLE) to fit the regression parameters . The scale/dispersion parameter for each model is then separately estimated (using the same estimation, but not necessarily MLE).Is it appropriate to calculate the AIC for each model, using the different scale parameters for each model, and use the difference to choose between them?My motivation for this question is that for nested models , the differences in estimated scale parameters can make the calculated log likelihood of  larger than . My intuition for AIC is that the log likelihood measures how well we fit the data, and this is penalized by model complexity measured as 2 times the number of parameters. But that intuition appears to be wrong in this case, since the log likelihood of the more complicated model decreased all by itself, owing to the change in scale parameter.As an aside (and potentially an answer), I assume the situation I mention could not happen if the scale parameter were also fit with MLE. Is it that such comparisons are valid only when the scale parameter is fit with MLE?","Creater_id":85938,"Start_date":"2016-08-11 07:51:46","Question_id":229362,"Tags":["generalized-linear-model","model-selection","aic"],"Answer_count":0,"Last_activity":"2016-08-11 07:51:46","Link":"http://stats.stackexchange.com/questions/229362/can-we-use-aic-to-compare-two-glms-when-the-scale-parameter-is-estimated-separat","Creator_reputation":36}
{"_id":{"$oid":"5837a581a05283111e4d5917"},"View_count":183,"Display_name":"Danib90","Question_score":1,"Question_content":"I have been model tuning using caret, but then re-running the model using the gbm package.  It is my understanding that the caret package uses gbm and the output should be the same.  However, just a quick test run using data(iris) shows a discrepancy in model of about 5% using RMSE and R^2 as the evaluation metric.I want to find optimal model performance using caret but re-run in gbm to make use of the partial dependency plots. Code below for reproducibility. My questions would be:1) Why am I seeing a difference between these two packages even though they should be the same (I understand that they are stochastic but 5% is somewhat a large difference, especially when I am not using such a nice dataset as iris for my modeling).2) Are there any advantages or disadvantages to using both packages - if so, which ones?3) Unrelated: Using the iris dataset the optimal interaction.depth is 5 however it is higher than what I've read should be the maximum using floor(sqrt(ncol(iris))) which would be 2.  Is this a strict rule of thumb or is it quite flexible?library(caret)library(gbm)library(hydroGOF)library(Metrics)data(iris)# Using caretcaretGrid \u0026lt;- expand.grid(interaction.depth=c(1, 3, 5), n.trees = (0:50)*50,                   shrinkage=c(0.01, 0.001),                   n.minobsinnode=10)metric \u0026lt;- \"RMSE\"trainControl \u0026lt;- trainControl(method=\"cv\", number=10)set.seed(99)gbm.caret \u0026lt;- train(Sepal.Length ~ ., data=iris, distribution=\"gaussian\", method=\"gbm\",              trControl=trainControl, verbose=FALSE,               tuneGrid=caretGrid, metric=metric, bag.fraction=0.75)                  print(gbm.caret)# caret determines the optimal model to be at n.tress=700, interaction.depth=5, shrinkage=0.01# and n.minobsinnode=10# RMSE = 0.3247354# R^2 = 0.8604# Using GBMset.seed(99)gbm.gbm \u0026lt;- gbm(Sepal.Length ~ ., data=iris, distribution=\"gaussian\", n.trees=700, interaction.depth=5,           n.minobsinnode=10, shrinkage=0.01, bag.fraction=0.75, cv.folds=10, verbose=FALSE)best.iter \u0026lt;- gbm.perf(gbm.gbm, method=\"cv\")print(best.iter)# Here the optimal n.trees = 540train.predict \u0026lt;- predict.gbm(object=gbm.gbm, newdata=iris, 700)print(rmse(irisfit, iris$Sepal.Length)^2print(R2)# R^2 = 0.9178`","Creater_id":115462,"Start_date":"2016-08-11 07:14:31","Question_id":229356,"Tags":["r","caret","gbm"],"Answer_count":0,"Last_activity":"2016-08-11 07:29:11","Link":"http://stats.stackexchange.com/questions/229356/gbm-package-vs-caret-using-gbm","Creator_reputation":28}
{"_id":{"$oid":"5837a581a05283111e4d5919"},"View_count":11,"Display_name":"gerrit","Question_score":0,"Question_content":"I have a  matrix of predictors / independent variables and a  matrix of predictands / dependent variables.  I have uncertainty estimates for each predictor and each predictand.  I use orthogonal distance regression (ODR, with scipys Python interface to ODRPACK) to estimate my predictands one column at the time (regressing  as independent and  as dependent), but my estimate of β (estimated parameter values) suffers from strong collinearities between the input vectors, i.e....which in turn leads to my model being poorly generalised to independent testing data.  For physical reasons¹, I would expect that all but one factors of β are close to zero, and my initial guess for β is\\beta_k = \\cases{%0 \\quad k \\neq i\\\\1 \\quad k = i}and I would expect the estimated  to be close to it.In practice,  and  is in the order of .How can I mitigate the problems caused by strong collinearities between the input vectors?  Can I penalise large deviations from expected coefficients or otherwise regularise input data?  Alternately, can I incorporate my errors-in-variables into methods better suited for collinear predictors, such as PLSR?  I am looking for a practical solution that is reasonably straightforward to implement using Python and available libraries.¹ Physically,I am comparing two N-channel radiometers with small, poorly known differences between corresponding channels, leading to small but nonzero expected differences in radiance.  My regression aims to get a better understanding of the differences.  Uncertainty estimates are essential to the project.","Creater_id":12615,"Start_date":"2016-08-10 06:46:12","Question_id":229178,"Tags":["multiple-regression","multicollinearity","errors-in-variables","total-least-squares","deming-regression"],"Answer_count":0,"Last_activity":"2016-08-11 07:28:18","Link":"http://stats.stackexchange.com/questions/229178/multiple-errors-in-variables-regression-with-collinearities","Creator_reputation":870}
{"_id":{"$oid":"5837a581a05283111e4d591b"},"View_count":34,"Display_name":"dontloo","Question_score":1,"Question_content":"I've been using CNN for facial recognition tasks, first I train a CNN for classification, and I use the trained CNN to extract features from images and do verification (tell whether two pictures are of the same person).However I found if I subtract the mean from the data (same mean used in both training and verification), though the classification result stays basically the same, the verification result will actually drop by more than 2% on average.Is this some known fact that normalizing (image) data actually worsens CNN performance? or is it just coincidence? BTW I used batch normalization, and Euclidean distance for verification.","Creater_id":95569,"Start_date":"2016-08-10 20:58:36","Question_id":229300,"Tags":["machine-learning","neural-networks","deep-learning","normalization","conv-neural-network"],"Answer_count":0,"Last_activity":"2016-08-11 07:21:21","Link":"http://stats.stackexchange.com/questions/229300/normalizing-data-worsens-the-performance-of-cnn","Creator_reputation":2782}
{"_id":{"$oid":"5837a581a05283111e4d591d"},"View_count":26,"Display_name":"user127332","Question_score":4,"Question_content":"I've been given a stack of demographic data to look over by the small nonprofit I work for, though as the original electronic files have been misplaced by a predecessor I'm having to reverse-engineer it from a strangely-formatted hardcopy. The data is currently of the form;00001Name Address 1 PostcodeEtc00002NameAddress 1 PostcodeEtcwith each individual a paragraph and each line a field.While I've managed to scan and OCR the text into Notepad++ as well as strip out the printer's markings and other non-data, I'm struggling to get it transformed into excel (ie each para from the txt forms a new excel row, with lines within each para seperated horizontally into adjacent cells). There are enough entries that copy-pasting them across line by line is infeasible.Could anyone suggest a better way of doing this?","Creater_id":127332,"Start_date":"2016-08-11 03:35:36","Question_id":229326,"Tags":["excel"],"Answer_count":1,"Last_activity":"2016-08-11 07:18:35","Link":"http://stats.stackexchange.com/questions/229326/recovering-data-from-a-poorly-formatted-source","Creator_reputation":21}
{"_id":{"$oid":"5837a581a05283111e4d591f"},"View_count":81,"Display_name":"DJack","Question_score":3,"Question_content":"I have a multiple regression model similar to:On the other hand, I have different values of two parameters  and  that gives me different values of  (with another model).So for each pair of parameters , I can fit my multiple regression model ( and  remain constant) and get a . The result is a matrix of  for each  with a single maximum for the optimal pair of parameters. Based on this output, I was wondering how to get the standard deviation of the optimal  and  (if possible). Should I use a hessian matrix? If yes, how?","Creater_id":24771,"Start_date":"2016-08-08 00:35:05","Question_id":228739,"Tags":["regression","multiple-regression","optimization","r-squared","parameter-optimization"],"Answer_count":0,"Last_activity":"2016-08-11 07:16:12","Link":"http://stats.stackexchange.com/questions/228739/standard-deviation-of-optimal-parameters-based-on-r2-matrix","Creator_reputation":140}
{"_id":{"$oid":"5837a581a05283111e4d5921"},"View_count":281,"Display_name":"Simeon","Question_score":7,"Question_content":"Given the following experiment, what is the correct statistical method to answer the question below:A participant is shown pictures consecutively and is required to respond whether she saw an object or a face after each picture. In each trial (picture presentation) the presented picture (either 1 of 210 individual faces or 1 of 210 individual objects) is superimposed with a certain amount of random noise (between 5% and 98%). The presented picture in each trial is rather small, so each trial has also a background. The background can either be black, a large object or a large face. The individual pictures are matched, meaning each individual picture is presented 3 times in total, 1 time with a black background, 1 time with a large object as background, and 1 time with a large face as background. The amount of random noise superimposed on an individual picture is kept constant over the 3 different background conditions. The object in the large object background does not change and is not included in one of the 210 individual object pictures presented. Similarly, the face in the large face background does not change and is not included in one of the 210 individual face pictures presented. No noise is added to any of the backgrounds.The question I'd like to answer is whether the perception of either faces, objects or both significantly differs between the 3 different background conditions. See question 5 down below for more details on the question I'd like to answerSo in the end, I have a data table, looking like this:+-------------+-------------+-------------+-------------+-------------+-------------+| Participant |  Category   |  Pic ID     | Noise level |  Background |  Response*  |+-------------+-------------+-------------+-------------+-------------+-------------+|  1          |  0          |  1          |   5%        |  1          |  0          ||  1          |  0          |  1          |   5%        |  2          |  0          ||  1          |  0          |  1          |   5%        |  3          |  0          ||             |             |             |             |             |             ||  1          |  0          |  2          |  24%        |  1          |  0          ||  1          |  0          |  2          |  24%        |  2          |  1          ||  1          |  0          |  2          |  24%        |  3          |  0          ||             |             |             |             |             |             ||  1          |  0          |  3          |  80%        |  1          |  1          ||  1          |  0          |  3          |  80%        |  2          |  0          ||  1          |  0          |  3          |  80%        |  3          |  1          ||             |             |             |             |             |             ||  ..         |  ..         |  ..         |  ..         |  ..         |  ..         |+-------------+-------------+-------------+-------------+-------------+-------------+|  1          |  1          |  211        |  12%        |  1          |  1          ||  1          |  1          |  211        |  12%        |  2          |  1          ||  1          |  1          |  211        |  12%        |  3          |  1          ||             |             |             |             |             |             ||  1          |  1          |  212        |  20%        |  1          |  1          ||  1          |  1          |  212        |  20%        |  2          |  0          ||  1          |  1          |  212        |  20%        |  3          |  1          ||             |             |             |             |             |             ||  1          |  1          |  213        |  75%        |  1          |  0          ||  1          |  1          |  213        |  75%        |  2          |  0          ||  1          |  1          |  213        |  75%        |  3          |  1          ||             |             |             |             |             |             ||  ..         |  ..         |  ..         |  ..         |  ..         |  ..         |+-------------+-------------+-------------+-------------+-------------+-------------+where Category is face (0) or object (1) and Response is also face (0) or object (1). The participant's response is the dependent variable. Dichotomous with an underlying continuum. Since every participant is measured in all 3 background conditions, it is a dependent design. Since, for one individual picture, I keep the noise constant over the 3 background conditions, it is somehow paired or matched.First I thought about calculating the biserial correlations and comparing them based on the t-statistic but then I saw logistic regression which seemed to fit my data structure better. But I still feel that the matched samples and dependent design should be incorporated in the analysis somehow. So when I searched for that, Conditional Logistic Regression popped up.The problem is, in Conditional Logistic Regression, the matching is done on the dependent variable. They usually match a 1 in the dependent variable with one or more 0 samples. I didn't match on the dependent variable but on the independent variables (same pictures with same noise level in each background condition). So I don't think I can use  Conditional Logistic Regression for this data but I couldn't find anything else that fits.Could someone with more experience in statistics explain to me what the correct way is to answer the above question whether the perception of either faces, objects or both significantly differs between the 3 different background conditions.Thank you for your help. [Experimental procedure]The experiment has 1260 trials in total. Made up of 210 individual faces and 210 individual objects presented 3 times each (one time with each of the 3 different backgrounds). Trial order is randomized with the constraint that in the first, the second and the last block of 420 trials, each background is presented exactly 140 times and each individual object and each individual face is presented exactly once. Most but not all of the  different individual faces and objects have a different amount of noise added to them, but the noise for an individual face or object is kept contant over the 3 different background conditions it is presented.[Questions \u0026amp; Answers]1. How many participants? There are 5 participants in total.2. Are there any bounds on noise? The noise is discretized in 0.5% steps and in the range [5%, 98%]. The noise is randomly drawn from a noise vector (without replacement) and assigned to a picture. This vector includes a noise distribution (210 entries for each category) that does not include every possible value between 5% and 98% in 0.5% steps but skips some of those values and includes some other values up to 3 times (answer to question 3). This ensures that each participant experiences the same noise levels (though not likely for the same pictures since the noise levels are randomly assigned to the individual pictures at the beginning of the experiment) and that there is a good coverage over the whole range but the focus is on noise levels near the threshold at which (for our setup) the participants can recognize the picture in about 50% of the time. This threshold was found in a preliminary study with other participants using the same pictures presented on a black background. Thus the black background is the default background in this experiment.3. Is it possible that two or more pictures could be presented with the same level of noise? Yes, this will happen several times and include up to 3 individual pictures for the same level of noise, but not more than 3.4. Can you confirm you are not interested in the association of noise with the response? This question is hard to answer for me. It is to be expected that the effect of different backgrounds is (if at all present) most prominent if the pictures are harder to see i.e. there is more noise present. So I want to consider the noise in the analysis, but I don't necessarily require the analysis to tell me anything about the association of the noise with the response. I'm only interested in detecting any kind of difference between the background conditions with as much power as possible. At first I wanted to fit 2 psychometric curves for each of the 3 different background conditions (probability to respond with the respective category vs noise level) and then compare the shifts of the psychometric fits to check for differences in the background conditions. However, a bootstrapping analysis revealed that the variance of the fitting procedure is too large to be able to detect shifts in the range I am expecting them to be. So I assume information about the association of noise with the response might decrease the power of other kinds of analyses as well. If this is the case, I don't need it.5. What do you mean by 'perception' and 'both'. What do you actually want to know? By 'perception of [category]' I don't mean percent correct but '[category] responses'. The assumption I have (and I'd like to test that) is that a face-background would influence a participant to respond with face BUT an object-background would NOT influence a participant to respond with object (this assumption probably doesn't make any sense to you as a reader but that's what I need to test). What I mean with 'both' is that should it be the case that a face-background influences a participant to respond with face AND an object-background influences a participant to respond with object, my assumption that only the face-background has an effect on perception would be false. The different noise levels were included because the chances to influence perception towards one of the categories should be greater when the pictures are harder to see/recognize. So if there is a background dependent effect on perception for any of the categories it is unlikely to show in the e.g. 5% - 20% noise range but rather in the higher noise range. Please let me know if you need further information.","Creater_id":125496,"Start_date":"2016-08-03 05:25:36","Question_id":227053,"Tags":["hypothesis-testing","logistic","multiple-comparisons","paired-data"],"Answer_count":2,"Last_activity":"2016-08-11 06:56:06","Link":"http://stats.stackexchange.com/questions/227053/what-is-the-correct-analysis-for-this-type-of-question-conditional-logistic-re","Creator_reputation":43}
{"_id":{"$oid":"5837a581a05283111e4d5923"},"View_count":27,"Display_name":"PaoloH","Question_score":1,"Question_content":"Sorry if I'm unclear, english is not my mother tongue and i studied barely any statistics. So i don't know anything about the usual way of summarizing that kind of problem.Let's say that i study three populations that are divided according to a single variable all i have is the three plots of their behaviour with in the x-axis a linear timeline and in the \"y-axis\" some ratio which i'm studying.Basically, this is kind of similar to a survival anaysis except that i can't use the usual models because i'm not studying the ratio of survivors, i have the data to do that but instead I study the ratio of days that are lived after the x-th day.(For instance if i have only 2 individuals one that lasted 2 days and the other one that lasted 8 days, and i'm looking at the 5-th day on my graph i will put (8-5)/(2+8)=3/10)I want to know if the 3 populations have statistically speaking a real difference in their behaviour (Is there any type of commonly used condition to tell wether a difference is significant or not ?). What is the method to do so? To make it as clear and as simple as possible :I'm in a internship, i have this plot which is about sick leaves, the percentages i'm refering to are the one i tried to explain before, those two plots represent my whole population divided on whether or not they have a specific type of contract:I've been asked to determine wether there is 'a statistical diffrence' between those two populations, if there is I need to make two tables if there isn't I make one.(By the way i don't even know if this request makes any sense since I know almost nothing about statistics, so if it's completely absurd just tell me it isn't feasible to answer this question) I'm supposed to divide my population by considering many variables, the type of contract is one of them.If u want to know what the data looks like, it's a dataframe, with on each line a single sick leave with various variables describing it, including its duration in days.So I just want to know is there a basic(or not) way to tell if there is a statistical difference ?Thank you in advance.","Creater_id":127240,"Start_date":"2016-08-10 07:09:14","Question_id":229186,"Tags":["time-series","survival","multiple-comparisons"],"Answer_count":0,"Last_activity":"2016-08-11 06:55:07","Link":"http://stats.stackexchange.com/questions/229186/compare-populations-behaviour-according-to-a-single-variable","Creator_reputation":58}
{"_id":{"$oid":"5837a581a05283111e4d5925"},"View_count":48,"Display_name":"user08041991","Question_score":1,"Question_content":"Taking the data available from the survival() package, we can develop a survival curve and 95% CI, assuming an underlining weibull distibution.  library(survival)data(lung)head(lung)#create a Surv objects \u0026lt;- with(lung,Surv(time,status))#plot weibull survival curves, per sex,sWei \u0026lt;- survreg(s ~ as.factor(sex), dist='weibull', data=lung)#probs \u0026lt;- seq(0.001,0.999,0.001)pred \u0026lt;- predict(sWei,newdata=list(sex=1),type=\"uquantile\",p=probs,se.fit=TRUE)val \u0026lt;- cbind(predfit-1.96*predfit+1.96*predweigthsa \u0026lt;- with(lung, ifelse(time \u0026lt; 400, 1000,                                   ifelse(time \u0026lt; 7000, 500, 300)))We can continue to create a new model using the same methodolgy and plot#plot weibull survival curves, per sex,sWei \u0026lt;- survreg(s ~ as.factor(sex), weights = weigthsa, dist='weibull', data=lung)#pred \u0026lt;- predict(sWei,newdata=list(sex=1),type=\"uquantile\",p=probs,se.fit=TRUE)val \u0026lt;- cbind(predfit-1.96*predfit+1.96*pred$se.fit)val \u0026lt;- exp(val)    # convert from log scale to original scale#----------------------------------------------------------------------------------lines(val[,1],probs,col=\"red\",lty=1, lwd = 2)lines(val[,2],probs,col=\"red\",lty=2, lwd = 1)lines(val[,3],probs,col=\"red\",lty=2, lwd = 1)#----------------------------------------------------------------------------------So my question is, are we simply allowed to do this?, given that we see that the width of the CI's for the second model become very small.Do we violate any assumptions of survival analysis when upscaling the model to a larger population?I ask so, because I similiar question was asked on this linkWhere the answer was given       As with any statistical test that uses a null hypothesis, the p-value    for the phtest is dependent on the sample size. These tests were not    developed for such large datasets. In population-based survival    analyses violations of PH assumptions are universal, just as linearity    assumptions are.  But i have found no other supporting literature","Creater_id":94093,"Start_date":"2016-08-11 05:25:23","Question_id":229342,"Tags":["r","confidence-interval","survival","population","weibull"],"Answer_count":1,"Last_activity":"2016-08-11 06:38:33","Link":"http://stats.stackexchange.com/questions/229342/survival-analysis-with-large-populations-upscaling-sample-to-population","Creator_reputation":50}
{"_id":{"$oid":"5837a581a05283111e4d5927"},"View_count":72,"Display_name":"Mike Onder","Question_score":1,"Question_content":"Summary: Can I use non-population information (e.g. representative proportions from another survey) when calculating sampling weights? If so how might one account for the sampling error?I'm calculating standard errors for a set of mean estimates (cohorts are age, gender and health state).I can calculate sampling weights using my data and population age, gender information from the census. There is no census information on health state.I have non-census health status information from a 'large' survey* (from the same population) which also has age-gender information. Presently I assume that the 'large' survey* point estimates for relative proportions of health states (for a given age gender cohort) are sufficiently precise to stand in for the population proportions which I do not observe. I use these alongside census age/gender information to derive population cohort counts and calculate sampling weights as usual. Then I use Stata's svy package for se's via Taylor linearization.I'm uncomfortable with the stand-in of the the 'large' survey* proportions in place of true population proportions as the imprecision of these is not accounted for in my standard error estimation.I think the influence of these is small, but how acceptable would you say the above approach is? Are there better ways of going about it?When I say sampling weights I use the following simple formula: (Npopulation of a given age, gender, health cohort)/Npopulation)/(nsample  of a given age, gender, health cohort/nsample)The larger survey  is the National Survey of Mental Health and Welbeing (Australia), which has a stratified, multistage, design. The smaller survey is a simple random sample.","Creater_id":80447,"Start_date":"2015-11-09 00:06:07","Question_id":180840,"Tags":["sampling","survey","weighted-mean","survey-weights","survey-sampling"],"Answer_count":2,"Last_activity":"2016-08-11 06:21:57","Link":"http://stats.stackexchange.com/questions/180840/permissible-to-use-survey-data-in-generating-sampling-weights","Creator_reputation":11}
{"_id":{"$oid":"5837a581a05283111e4d5929"},"View_count":30,"Display_name":"Janina Steinert","Question_score":1,"Question_content":"I am using SEM and my measurement model includes around 15 items that are all rated on a 0-10 scale but highly skewed towards zero. I am wondering whether there is a way to estimate the measurement model with poisson corrections for zero inflation in R? Or would you assume that it should be sufficient to just run a generalised structural equation model that relaxes the normality assumption? ","Creater_id":112219,"Start_date":"2016-08-09 23:47:42","Question_id":229124,"Tags":["sem","measurement","zero-inflation","lavaan"],"Answer_count":0,"Last_activity":"2016-08-11 06:18:16","Link":"http://stats.stackexchange.com/questions/229124/sem-with-zero-inflated-indicators","Creator_reputation":16}
{"_id":{"$oid":"5837a581a05283111e4d592b"},"View_count":154,"Display_name":"Richard Hardy","Question_score":4,"Question_content":"An embarrassingly simple question -- but it seems it has not been asked on Cross Validated before:What is the definition of a regression model? Also a support question,What is not a regression model?With regards to the latter, I am interested in tricky examples where the answer is not immediately obvious. For example, What about latent variable models (latent dependent variable; latent independent variable; both)? What about deterministic models (containing no error term)?What about ARIMA model? What about GARCH model (which is latent dependent variable+latent independent variable, deterministic model)?","Creater_id":53690,"Start_date":"2015-09-22 07:38:09","Question_id":173660,"Tags":["regression","linear-model","model","terminology","definition"],"Answer_count":3,"Last_activity":"2016-08-11 06:16:37","Link":"http://stats.stackexchange.com/questions/173660/definition-and-delimitation-of-regression-model","Creator_reputation":12982}
{"_id":{"$oid":"5837a581a05283111e4d592d"},"View_count":95,"Display_name":"Mario GS","Question_score":0,"Question_content":"I was wondering if you can help me clarifying some concepts (if it is possible providing references to papers or books) that I will write in the form of ideas rather than questions. Consider this situation:   we draw a stratified random sampling method, first by  randomizing the selection of regions within a country and then randomizing the selection of households within the regions. But, we collect data at the individual level.Idea 1: If I don't have any information about the population, I can only assume that if the selection was drawn at random (each observation had the same probability of being selected) and if the sample size is big enough, It will be \"representative\" of a certain population parameter(s). Idea 2 If I have information about the population, I can test statistically whether or not the sample is \"representative\" of a certain population parameter. Idea 3 If the sample resulted not representative, do not necessarily means that the selection was not at random.  Rather, that it is not representative of a certain population parameter and it will produce a biased estimator(s). For instance, if I randomly select  individuals and collect information about two parameters  and  , i.e, age and gender. With information about the population ,  and , we could compute some weights  and  that will give us a rough indicator of the representativeness of our sample. It is possible that none, one or both estimators  and  resulted in bias. I can imagine two scenarios to explain the source of the bias, one is that the sampler purposely decided to bias the sample, which will make the selection process deterministic instead of random. If it is not the case, and the selection was done by change, I will consider that the sample was not big enough and that the estimator(s) could be overrepresented or underrepresented. This does not mean that the selection process had no uncertainty (it was drawn by choice rather than chance), nor independence between observations or that they didn't come from the same uniform distribution. But, rather that we need more information in order to get closer to the distribution of the population. I guess what I want to propose is that random sampling can still produce bias estimator(s) if the sample size is small. Idea 4 The method of weights that can only make sense \"after sampling\", when I know that the sample is not representative according to the information that I have about the population. In other  words, it is not a weakness of the stratified sampling method in itself, but rather a method that can be used in order to correct for bias in the sample.   Thanks a lot!","Creater_id":77318,"Start_date":"2015-12-01 18:24:57","Question_id":184563,"Tags":["sampling","bias","randomness","representative","survey-sampling"],"Answer_count":1,"Last_activity":"2016-08-11 06:08:55","Link":"http://stats.stackexchange.com/questions/184563/conceptual-definition-between-randomness-representativeness-and-bias-in-samplin","Creator_reputation":11}
{"_id":{"$oid":"5837a581a05283111e4d592f"},"View_count":33,"Display_name":"dixi","Question_score":0,"Question_content":"In order to reduce the sample size, would you rather have a larger margin of error or decrease the confidence interval?For example:current sample size is calculated from 95% CI and 5% margin of error.to reduce it, do you prefer to reduce the confidence interval? that is, calculate it with 90% CI and 5% margin of error, or do you prefer to increase the margin of error? that is, calculate it with 95% CI and 8% margin of error?","Creater_id":3379,"Start_date":"2016-08-11 06:05:56","Question_id":229347,"Tags":["sample-size","survey"],"Answer_count":0,"Last_activity":"2016-08-11 06:05:56","Link":"http://stats.stackexchange.com/questions/229347/sample-size-calculation-how-to-reduce-the-number-of-samples","Creator_reputation":36}
{"_id":{"$oid":"5837a581a05283111e4d5931"},"View_count":235,"Display_name":"Michael Kaiser","Question_score":3,"Question_content":"I'm working with data from a clustered sample where observations have a certain sampling weight (pweight). There are two ways to obtain the correct point estimates: I) using reg yvar xvar [pw = pweight] or ii) using svyset[pw = pweight] and then svy : reg yvar xvar These return identical point estimates (as they should). However, once one wants to introduce cluster-robust standard errors, the \"manual\" approach and the svyset approach return slightly different results. What I mean by \"manual\" is a command of the form: reg yvar xvar [pw = pweight], cluster(clustervar) as opposed to: svyset clustervar [pw = pweight] and then svy : reg yvar xvar. Here is a little code example to illustrate this with some numbers: sysuse auto set seed 92122 *a variable containing random integers from 1 thru 4 designating fake clusters  gen mycluster = ceil(4*uniform()) *random probability weights as the inverse of some random sampling probability gen mypw = 1/uniform() *run the \"manual\" regression reg price mpg weight [pw = mypw], cluster(mycluster) *using svy design svyset mycluster [pw = mypw] svy : reg price mpg weightThe standard errors are very close to one another but not identical (mpg is 72.48 and 71.48 and weight has 0.969 and 0.956). Stata calls the ones from the svyset-regression \"Linearized\" so I suppose that's where the difference comes from - potentially a Taylor expansion? Could somebody point me towards the precise (mathematical) difference? Are the patterns, i.e. one is always larger than the other?I previously posted this question on Stackoverflow but it was deemed more appropriate here. ","Creater_id":100728,"Start_date":"2016-01-14 12:58:53","Question_id":190741,"Tags":["stata","standard-error","survey","clustered-standard-errors"],"Answer_count":1,"Last_activity":"2016-08-11 06:03:56","Link":"http://stats.stackexchange.com/questions/190741/cluster-robust-se-in-stata-when-using-a-survey-design","Creator_reputation":18}
{"_id":{"$oid":"5837a581a05283111e4d5933"},"View_count":146,"Display_name":"Figaro","Question_score":1,"Question_content":"I have survey data that needs to be weighted, and to help me with this task, I have access to the full joint distributions of the variables I want to use. As I understand it, I should use the postStratify() function in the survey package for this task, but I must admit that I am quite lost when it comes to the required syntax.I have tried to use marginal distributions and the rake() function, this is quite straight-forward:http://www.r-bloggers.com/survey-computing-your-own-post-stratification-weights-in-r/But how do I incorporate all my joint distributions into the postStratify() function? The manual only lists a simple example and is a bit vague, to me at least. I have full joint distributions for 5 different variables.Moreover, if want to use my weighted result with a function that does not take weight as an input, how should I go about and \"duplicate\" my rows in the final data? When I tried the rake() function I could only trim weights to an interval with a min value of 0.87. I was thinking I could round all weights to integers.","Creater_id":3401,"Start_date":"2015-03-09 12:15:33","Question_id":141032,"Tags":["r","survey","stratification","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-11 06:01:01","Link":"http://stats.stackexchange.com/questions/141032/post-stratification-weights-and-the-survey-package-in-r","Creator_reputation":296}
{"_id":{"$oid":"5837a581a05283111e4d5935"},"View_count":368,"Display_name":"Andrew","Question_score":5,"Question_content":"What justifies the usage of a variable for post-stratification?I am working with a constituent survey of a non-profit's constituent with 2500 responses out of a much larger sample and even larger population.  I have many variables about the target population, which are all active constituents.  In literature I've read, it's common to use demographic variables (age, gender, and race, for example), but in my experience with this data, demographics have relatively high data quality errors and weak correlation to non-response error, while behavioral data (for example, donation history) are recorded reliably and correlate better to non-response.  I assume demographics are common because many surveys try to get a nationally representative sample, and the government publishes demographic information for this population.  Because I have them, is there anything wrong with using the behavioral variables instead of, or in addition to, the demographics? Is there a practical empirical method to choose variables?If the suggestion is to use behavioral variables in addition to demographics, how would I detect or prevent overfitting when raking weights with many variables?","Creater_id":8053,"Start_date":"2012-10-23 11:27:56","Question_id":41057,"Tags":["survey","non-response","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-11 05:59:19","Link":"http://stats.stackexchange.com/questions/41057/variables-for-post-stratification-weights","Creator_reputation":421}
{"_id":{"$oid":"5837a581a05283111e4d5937"},"View_count":37,"Display_name":"user2340706","Question_score":2,"Question_content":"I have a survey methodology question. I am preparing a report that contains some demographic information. My supervisor instructed that I use the NCHS bridged-race population estimates when reporting the state population. However, he has also instructed that I use the ACS IPUMS to calculate rates on various indicators such as poverty and language isolation. Then, I am to apply that rate to the NCHS population estimate; the product of the ACS IPUMS rate and the NCHS population estimate is the \"adjusted true estimate\" according to my supervisor. Since then, I have found that what he is instructing me to do is called post-stratification. I am, however, curious as to whether this is sensible practice given the methodology used to calculate NCHS bridged-race population estimates (http://www.census.gov/popest/methodology/2013-natstcopr-meth.pdf). Is my supervisor's proposed post-stratification method correct or poor form of survey analysis methodology?","Creater_id":56387,"Start_date":"2014-10-16 22:27:24","Question_id":120408,"Tags":["survey","methodology","survey-weights"],"Answer_count":1,"Last_activity":"2016-08-11 05:57:56","Link":"http://stats.stackexchange.com/questions/120408/post-stratification-of-acs-survey","Creator_reputation":16}
{"_id":{"$oid":"5837a581a05283111e4d5939"},"View_count":22,"Display_name":"James","Question_score":0,"Question_content":"I've got a dataset with patients (n=50) with 10 readings each (so overall, n=500) who suffered syncope (1 or 0), and 2 continuous predictors (rate and doppler).I'm trying to see if 1 predictor is more effective than another. I'm currently synthesising data, then creating a model for each using glm(syncope~predictor,family=\"binomial\"), and then using an ANOVA on these. The code is as follows.n_pts \u0026lt;- 50n_reads_per_pt \u0026lt;- 10intercept = log(0.2)gradient = 2.5x \u0026lt;- rnorm(n_pts*n_reads_per_pt,mean=0,sd=1)x_doppler \u0026lt;- xx_rate \u0026lt;- x + (rnorm(n_pts*n_reads_per_pt)) #Add a second random factor to make rate a less good predictory \u0026lt;- intercept + gradient*xp \u0026lt;- exp(y)/(1+exp(y))tmp \u0026lt;- runif(n_pts*n_reads_per_pt)syncope \u0026lt;- (tmp \u0026lt; p)glm_rate \u0026lt;- glm(syncope~x_rate,family=\"binomial\")glm_doppler \u0026lt;- glm(syncope~x_doppler,family=\"binomial\")anova(glm_rate, glm_doppler,test=\"Chisq\")The problem is the output of the ANOVA is:\u0026gt; anova(glm_rate, glm_doppler,test=\"Chisq\")Analysis of Deviance TableModel 1: syncope ~ x_rateModel 2: syncope ~ x_doppler  Resid. Df Resid. Dev Df Deviance Pr(\u0026gt;Chi)1       498     585.72                     2       498     386.71  0   199.01  Note no p value. I assume this is because I have 0 Df left?How would you recommend I then compare such a dataset, where 2 continuous predictors are being compared to assess one binary outcome?","Creater_id":73691,"Start_date":"2016-08-11 05:31:15","Question_id":229343,"Tags":["r","logistic","anova","generalized-linear-model","logit"],"Answer_count":0,"Last_activity":"2016-08-11 05:31:15","Link":"http://stats.stackexchange.com/questions/229343/testing-for-a-difference-between-two-glm-models-using-anova-issues-with-shared","Creator_reputation":57}
{"_id":{"$oid":"5837a581a05283111e4d593b"},"View_count":43,"Display_name":"John","Question_score":1,"Question_content":"We want to apply PCA to monitor our process testing data. After we plot the density of the testing data and do normality tests, we found them to be definitely not normal, but still symmetrical and long tailed.What distribution can these be?If they are not normal, is PCA still be applicable? If not, what other monitoring technique could be used? We have too many of those testing data, so eyeballing them 1 by 1 is definitely not a good choice.shapiro test  ARM_Z  RAMP_Z  DISC_Z RAMP_Z8  ARM_Z1  ARM_Z2  0.0000  0.0032  0.0000  0.0724  0.0246  0.0000 adf test  ARM_Z  RAMP_Z  DISC_Z RAMP_Z8  ARM_Z1  ARM_Z2    0.01    0.01    0.01    0.01    0.01    0.01 jarque-bera test  ARM_Z.X-squared  RAMP_Z.X-squared  DISC_Z.X-squared RAMP_Z8.X-squared  ARM_Z1.X-squared  ARM_Z2.X-squared            0.0000            0.4580            0.0003            0.8591            0.0029            0.0000 They are more peaked than corresponding fitted normal distributions.","Creater_id":71752,"Start_date":"2016-08-11 01:58:51","Question_id":229320,"Tags":["r","pca","normality"],"Answer_count":1,"Last_activity":"2016-08-11 05:20:25","Link":"http://stats.stackexchange.com/questions/229320/applying-pca-to-data-that-fail-a-normality-test","Creator_reputation":178}
{"_id":{"$oid":"5837a581a05283111e4d593d"},"View_count":42,"Display_name":"Joshua Onyango","Question_score":2,"Question_content":"I have a data set looking into whether a farm experienced a livestock disease or not in the year 2011 and 2012  and if several factors could be predictors for the livestock disease.The independent variables were also collected for both years though some variables did not change e.g Thistles remained the same for both years.I am looking for an appropriate method that will allow statistical comparison between the two years rather than treating analysis as two separate sets of analyses (i.e not to treating 2011 and 2012 as two separate data set)Whilst trying to do the analysis I have created dependent variable as farm having the disease or not between year 2011 and 2012(Orf.Yes.No2011.2012)against the dependent variables using logistic regression:I'm just wondering whether I doing the right thing or what could be the best statistical approach which will allow for statistical comparison between the two years? Any help will be very much appreciatedHere is the R output and sample of dataset: \u0026gt; mod=glm(Orf.Yes.No2011.2012~F2011+ F2012+as.factor(Breed)+                                   D2011+D2012,family=binomial, data=orf)      summary(mod)    Call:    glm(formula = Orf.Yes.No2011.2012 ~ F2011 + F2012 + as.factor(Breed) +         D2011 + D2012, family = binomial, data = orf)    Deviance Residuals:        Min      1Q  Median      3Q     Max      -1.862  -1.293   1.023   1.065   1.318      Coefficients:                        Estimate Std. Error z value Pr(\u0026gt;|z|)      (Intercept)        0.3917290  0.1626769   2.408    0.016 *    F2011              0.0003269  0.0002782   1.175    0.240      F2012             -0.0003596  0.0002786  -1.291    0.197      as.factor(Breed)2  0.0558285  0.1489246   0.375    0.708      D2011             -0.0311978  0.0272068  -1.147    0.252      D2012              0.0226963  0.0274981   0.825    0.409      Small sample data set:    F2011   F2012   Breed   Orf.Yes.No2011  Orf.Yes.No2012  Orf.Yes.No2011.2012    155     150     1       0               0               0    740     760     2       0               1               1    1000    850     1       0               0               0    1630    1520    1       1               1               1    0       460     1       0               0               0    1300    1335    1       0               1               1    450     450     1       0               0               0    390     730     1       1               0               1    390     380     2       0               0               0    600     600     2       0               0               0","Creater_id":82976,"Start_date":"2016-08-11 04:50:47","Question_id":229336,"Tags":["r","regression","multiple-regression","chi-squared","multivariate-analysis"],"Answer_count":0,"Last_activity":"2016-08-11 04:59:23","Link":"http://stats.stackexchange.com/questions/229336/appropriate-method-that-will-allow-statistical-comparison","Creator_reputation":16}
{"_id":{"$oid":"5837a581a05283111e4d593f"},"View_count":163,"Display_name":"Regis A. Ely","Question_score":0,"Question_content":"There are many methods to evaluate prediction models based in prediction errors, such as MSE, MAE, MAPE, WMAE, etc. These methods are usually used in data prediction competitions, where one is given a set of data used to discover potentially predictive relationships (training set), and must apply her predictions in another set of data used to assess the strength of her model (test data).Some of these methods have gaps that can be explored. I can think of two cases:The Mean Absolute Error (MAE) does not take into account the relative size of the error, so I guess conservative predictions may not be a good idea if this is the evalutation measure.I am not sure how, but weighted measures such as WMAE may be explored since one may pay more attention in predicting values that are more important for the evaluation method.Is my line of thought right? Can you elaborate more on that? What are other gaps that can be explored in the most common prediction evaluation measures and how one may explore this gaps? Is there a measure that you consider most appropriate for these data competitions?","Creater_id":101354,"Start_date":"2016-01-24 12:12:34","Question_id":192257,"Tags":["time-series","forecasting","data-mining","prediction","model-evaluation"],"Answer_count":2,"Last_activity":"2016-08-11 04:27:20","Link":"http://stats.stackexchange.com/questions/192257/gaps-of-methods-to-evaluate-prediction-accuracy","Creator_reputation":466}
{"_id":{"$oid":"5837a581a05283111e4d5941"},"View_count":27,"Display_name":"NSAA","Question_score":0,"Question_content":"How do we actually write the VAR-GARCH model? I don't really understand it.Here is the VAR() process: x_t = \\phi_0 + \\sum_{i=1}^m \\phi_i x_{t-i} + \\epsilon_t; and here is the GARCH(p,q):\\begin{eqnarray*} Var[\\boldsymbol{x_t}|\\boldsymbol{I}_{t-1}] \u0026amp;=\u0026amp; Var[\\boldsymbol{\\epsilon_t}|I_t] \\\\\\sigma_t^2 \u0026amp;=\u0026amp; \\alpha_0 + \\sum_{l=1}^{p} \\alpha_l \\sigma^2_{t-l} + \\sum_{m=1}^{q} \\beta_m x^2_{t-m} \\\\\\end{eqnarray*}As I understand, the variance is modeled as GARCH and we use VAR to model the conditional mean of the variables. How do we actually write the complete equation of VAR()-GARCH()?","Creater_id":39522,"Start_date":"2016-08-11 03:16:50","Question_id":229325,"Tags":["time-series","garch","var"],"Answer_count":1,"Last_activity":"2016-08-11 04:13:20","Link":"http://stats.stackexchange.com/questions/229325/equations-for-var-model-with-garch-errors","Creator_reputation":3}
{"_id":{"$oid":"5837a581a05283111e4d5943"},"View_count":24,"Display_name":"Ali Turab Lotia","Question_score":1,"Question_content":"We are trying to fit a GLM that estimates the number of insurance claims made in a year using 6 independent variables.The count variables are based on time frames (exposure) so to make it a fair comparison, we divide the count by exposure and call it frequency which gives an estimate of the counts you would expect in year.The count variable follows a poisson distribution and exposure is a uniform distribution between 0 and 1.The residuals vs. fits plot for frequency looks like:The problem is that a GLM modeling frequency with a poisson family is returning warnings as it is modeling a count variable and not getting integer values (as count/exposure often returns float values). Rounding the frequency response variable is not an option as it leads to other conceptual problems.Should I ignore the warnings and use the fitted model?","Creater_id":124010,"Start_date":"2016-08-11 03:40:59","Question_id":229327,"Tags":["distributions","poisson-regression","residual-analysis"],"Answer_count":0,"Last_activity":"2016-08-11 04:06:55","Link":"http://stats.stackexchange.com/questions/229327/modeling-a-poisson-distributed-count-variable-divided-by-uniformly-distributed-t","Creator_reputation":69}
{"_id":{"$oid":"5837a581a05283111e4d5945"},"View_count":53,"Display_name":"tintinthong","Question_score":1,"Question_content":"I understand  is also known as the effects vector(Q in the QR decomposition). I know that using this one can compute the extra fit sums of squares of the model or obtain the total sum of squares by squaring the vector and summing up accordingly. This question comes very close to what I am asking but the answers do not give the answer which I think is right. What does it  mean? What does  mean? In R they give a definition   For a linear model fitted by lm or aov, the effects are the uncorrelated single-degree-of-freedom values obtained by projecting the data onto the successive orthogonal subspacesBut, this I still do not get this on an intuitive level. This is certainly not  . How can  be a projection when it is just a number? ","Creater_id":121671,"Start_date":"2016-08-10 16:48:58","Question_id":229282,"Tags":["regression","least-squares"],"Answer_count":1,"Last_activity":"2016-08-11 02:51:49","Link":"http://stats.stackexchange.com/questions/229282/what-is-qty-in-linear-least-squares","Creator_reputation":117}
{"_id":{"$oid":"5837a581a05283111e4d5947"},"View_count":16,"Display_name":"godspeed","Question_score":0,"Question_content":"I have a retrospective dataset that has records of the medications a patient was administered to prevent recurrence of heart attack. The goal is to determine if a particular drug or combination of drugs is better at preventing (or prolonging) the recurrence of heart attack. My plan is to create an indicator variable for each of the drugs taken and include all possible two way interaction between the drugs in a cox regression model treating the problem as a survival analysis problem. Would this be an appropriate approach to answering the question? I'm thinking of using the interaction terms to model the effect of drug pairs. Should I create indicator variables for all pair of drug combination in the data instead and why?","Creater_id":103245,"Start_date":"2016-08-11 02:38:29","Question_id":229322,"Tags":["survival","modeling","cox-model","interaction-variable"],"Answer_count":0,"Last_activity":"2016-08-11 02:38:29","Link":"http://stats.stackexchange.com/questions/229322/statistical-design-retrospective-analysis-of-effectiveness-of-medication","Creator_reputation":133}
{"_id":{"$oid":"5837a581a05283111e4d5949"},"View_count":89,"Display_name":"abc","Question_score":3,"Question_content":"In this question: Different definitions of the cross entropy loss function, two different definitions of cross-entropy cost function are proposed:  C = -\\frac{1}{n} \\sum_x \\sum_j(y_j \\ln a_{j}^{L}) and C = -\\frac{1}{n} \\sum_x \\sum_j (y_j \\ln a_{j}^{L} + (1-y_j) \\ln(1-a_{j}^{L})).The analysis in the answer to the question I referred to shows that for binary classification (j=2), given that  and  is a one-hot vector, it holds that:  C = -\\frac{1}{n} \\sum_x \\sum_{j=1}^2 (y_j \\ln a_j) = -\\frac{1}{n}\\sum_x y_1\\ln a_1 + y_2 \\ln a_2 = \\\\ -\\frac{1}{n} \\sum_x y_1 \\ln a_1 + (1 - y_1) \\ln (1 - a_1).However, I don't see how this analysis shows that the 2 definitions are equivalent given the assumptions, because in the second definition, if we take , it yields: C = -\\frac{1}{n} \\sum_x [y_1 \\ln a_1^L + (1 - y_1) \\ln (1 - a_1^L) + y_2 \\ln a_2^L + (1 - y_2) \\ln (1 - a_2^L)].Moreover, I would like to show that the definitions are equivalent for any number of output neurons, not just for 2 neurons.","Creater_id":127005,"Start_date":"2016-08-08 09:02:55","Question_id":228813,"Tags":["machine-learning","neural-networks","loss-functions","cross-entropy"],"Answer_count":1,"Last_activity":"2016-08-11 02:24:57","Link":"http://stats.stackexchange.com/questions/228813/different-definitions-of-cross-entropy-loss-function-not-equivalent","Creator_reputation":16}
{"_id":{"$oid":"5837a581a05283111e4d594b"},"View_count":45,"Display_name":"jjwkdl","Question_score":0,"Question_content":"I have D-dimensional data with K components. How many parameters if I use a model with full covariance matrices? and How many if I use diaogonal covariance matrices?","Creater_id":127305,"Start_date":"2016-08-10 19:03:12","Question_id":229293,"Tags":["machine-learning","expectation-maximization","gaussian-mixture"],"Answer_count":1,"Last_activity":"2016-08-11 01:59:45","Link":"http://stats.stackexchange.com/questions/229293/the-number-of-parameters-in-gaussian-mixture-model","Creator_reputation":3}
{"_id":{"$oid":"5837a581a05283111e4d594d"},"View_count":42,"Display_name":"david","Question_score":0,"Question_content":"I was following the examples provided by Forecasting: principles and practice. I have modified a little bit the Holt-Winters' example: aust \u0026lt;- window(austourists,start=2009)\u0026gt; fit1 \u0026lt;- hw(aust,seasonal=\"additive\")Error in ets(x, \"AAA\", alpha = alpha, beta = beta, gamma = gamma, damped = damped,  :   You've got to be joking. I need more data!\u0026gt; fit2 \u0026lt;- hw(aust,seasonal=\"multiplicative\")Error in ets(x, \"MAM\", alpha = alpha, beta = beta, gamma = gamma, damped = damped,  :   You've got to be joking. I need more data!\u0026gt;aust         Qtr1     Qtr2     Qtr3     Qtr42009 55.55857 33.85092 42.07638 45.642292010 59.76678 35.19188 44.31974 47.91374Of course I understand that the problem is that the time series is too short. But with two previous periods of time the formula has not enough. Reviewing the formula I don't know why it doesn't work with two periods of time.","Creater_id":31036,"Start_date":"2016-08-11 01:24:00","Question_id":229316,"Tags":["time-series","forecasting","exponential-smoothing"],"Answer_count":1,"Last_activity":"2016-08-11 01:49:33","Link":"http://stats.stackexchange.com/questions/229316/holt-winters-seasonal-method-doesnt-work-in-a-small-sample","Creator_reputation":9}
{"_id":{"$oid":"5837a581a05283111e4d594f"},"View_count":45,"Display_name":"Ken S.","Question_score":1,"Question_content":"I am running a fixed effects model. My data consists of multiple variables for two time points for about 180 countries. My aim is to assess the effect that variable iv2 has on dv1, while controlling for other variables such as iv1. There is something odd though and I have the strong suspicion it has something to do with the two time points.Question:When I run the fixed effects regression, the predicted values as well as the residuals have the same absolute value, but the opposite sign. The outcome puzzles me and I cannot wrap my head around it. Obviously, the residuals now violate the regression assumption of independent errors (serial correlation). Why does the model show this behaviour? Does this compromise the analysis of the effectiv2 has on dv1?I would appreciate a formal mathematical explanation, but an intuitive explanation as to why this behaviour occurs in a fixed effects model (and not in random effects, for example) will also do the trick.Here is a reproducible example in R:# load the data:df \u0026lt;- structure(list(country = c(1, 1, 2, 2, 3, 3, 4, 4, 5, 5), year = c(2010,          2015, 2010, 2015, 2010, 2015, 2010, 2015, 2010, 2015), dv1 = c(28.61,          31.13, 38.87, 39.46, 68.42, 70.39, 79.36, 80.55, 70.14, 71.48         ), iv1 = c(-20.68, 0, NA, NA, -19.41, -18.73, 24.98, 25.23, 21.23,          -21.06), iv2 = c(-4.23, NA, NA, NA, -4.92, -4.22, 9.19, 9.37,          4.15, -3.92)), .Names = c(\"country\", \"year\", \"dv1\", \"iv1\", \"iv2\"         ), row.names = c(2L, 3L, 5L, 6L, 8L, 9L, 11L, 12L, 14L, 15L),class =\"data.frame\")# load the plm packagelibrary(plm)# Run the fixed effects regressionregoutput \u0026lt;- plm(dv1 ~ iv1 + iv2, data = df,                  model = \"within\", index = c(\"country\", \"year\"))# The predicted values and residuals:    predict(regoutput)    # [1]  0.0000000 -1.0660019  1.0660019 -0.2458495  0.2458495 -0.6692384  0.6692384    regoutput$residuals    # [1] -2.987628e-15  8.100187e-02 -8.100187e-02 -3.491505e-01  3.491505e-01 -7.615597e-04  7.615597e-04","Creater_id":115012,"Start_date":"2016-08-10 14:41:52","Question_id":229272,"Tags":["regression","panel-data","fixed-effects-model"],"Answer_count":1,"Last_activity":"2016-08-11 01:45:03","Link":"http://stats.stackexchange.com/questions/229272/why-does-my-fixed-effects-return-symmetric-predicted-values","Creator_reputation":133}
{"_id":{"$oid":"5837a581a05283111e4d5951"},"View_count":41,"Display_name":"Daniel Yefimov","Question_score":0,"Question_content":"Suppose that the X’s are a sample from F and the Y ’s a sample from G, andconsider estimating, as a measure of the effect of the treatment,π = P(X \u0026lt; Y )where X and Y are independently distributed with distribution functions F and G,respectively. The value π is the probability that an observation from the distributionF is smaller than an independent observation from the distribution G.An estimate of π canbe obtained by comparing all n values of X to all m values of Y and calculating the proportion of the comparisons for which X was less than Y :π = \\frac{1}{mn} \\sum_{i=1}^n \\sum_{i=1}^m Z_{ij} ,where  Z_{ij} = \\begin{cases}1,  \u0026amp; \\text{if  } \\\\0, \u0026amp; \\text{otherwise}\\end{cases}To see the relationship of πˆ to the rank sum introduced earlier, we will find it convenient to work with V_{ij} = \\begin{cases}1,  \u0026amp; \\text{if  } \\\\0, \u0026amp; \\text{otherwise}\\end{cases}Clearly,\\sum_{i=1}^m Z_{ij} = \\sum_{i=1}^m V_{ij}since the Vi j are just a reordering of the  . Also, = (number of X’s that are less than )+ (number of X’s that are less than )+ · · ·+ (number of X’s that are less than ))....All step above i have understood. But i can't understand, how do we get the result belowIf the rank of  in the combined sample is denoted by  , then the number of X’sless than  is  − 1, the number of X’s less than  is  − 2, etc. Therefor:\\sum_{i=1}^m V_{ij} = (R_{y1} − 1) + (R_{y2} − 2) + ... + (R_{ym} − m)Can you please explain me the last step? What does it exaclty mean and how did we got this? ","Creater_id":108018,"Start_date":"2016-08-11 01:41:08","Question_id":229319,"Tags":["mann-whitney-u-test","proof"],"Answer_count":0,"Last_activity":"2016-08-11 01:41:08","Link":"http://stats.stackexchange.com/questions/229319/explanation-of-derivationthe-mann-whitney-statistic","Creator_reputation":267}
{"_id":{"$oid":"5837a581a05283111e4d5953"},"View_count":70,"Display_name":"BobbyJohnsonOG","Question_score":0,"Question_content":"Let's say I have time series data.'person':['A','A','A','B','B','B','C,'C','C']'weight':[120, 123, 135, 140, 150, 151, 120, 120, 121]'height':[5, 5, 5, 6, 6, 6, 4.5, 4.5, 4.5]'running_time':[60,61,63,34,50,55, 60, 70, 80]'week':[1, 2, 3, 1, 2, 3, 1, 2, 3}Let's assume the dataset is much larger than that, of course. Let's assume I want to generate a model that will use person, weight, height, and week to predict running time (this is just an example, let's forget about other better ways to do this).For a train test split or cross-validation, I could completely randomly split the data, where some of person A's measurements will be in train, and some in test. Or I could randomly split based on people. In other words, 70% of people go into train, 30% into test.What would be the best way to do this?","Creater_id":125252,"Start_date":"2016-08-10 12:47:01","Question_id":229255,"Tags":["time-series","forecasting","cross-validation","predictive-models","train"],"Answer_count":0,"Last_activity":"2016-08-10 22:25:02","Link":"http://stats.stackexchange.com/questions/229255/train-test-split-and-cv-with-time-series-data","Creator_reputation":34}
{"_id":{"$oid":"5837a581a05283111e4d5955"},"View_count":131,"Display_name":"Ricardo UES","Question_score":0,"Question_content":"What is the best approach to forecasting yearly climate data, for example minimum temperature in a specific area? I have 24 meteorological stations and the data of minimum temperature (there are other climate indicators) are from 1970. I need to generate forecasts for 2015. I know exponential smoothing models but I don't know ARIMA models. I think ARIMA models are not suitable because yearly data does not have seasonality. Can I discard the use of ARIMA models?","Creater_id":82662,"Start_date":"2015-12-02 09:12:49","Question_id":184686,"Tags":["time-series","forecasting"],"Answer_count":1,"Last_activity":"2016-08-10 21:24:30","Link":"http://stats.stackexchange.com/questions/184686/forecasting-yearly-climate-data","Creator_reputation":34}
{"_id":{"$oid":"5837a581a05283111e4d5957"},"View_count":5773,"Display_name":"csgillespie","Question_score":93,"Question_content":"I recently asked a question regarding general principles around reviewing statistics in papers. What I would now like to ask, is what particularly irritates you when reviewing a paper, i.e. what's the best way to really annoy a statistical referee!One example per answer, please.","Creater_id":8,"Start_date":"2010-10-20 12:09:31","Question_id":3814,"Tags":["references","referee"],"Answer_count":19,"Last_activity":"2016-08-10 20:52:04","Link":"http://stats.stackexchange.com/questions/3814/how-to-annoy-a-statistical-referee","Creator_reputation":7991}
{"_id":{"$oid":"5837a581a05283111e4d5959"},"View_count":52,"Display_name":"O.rka","Question_score":0,"Question_content":"One thing I'm trying to do is look at the accuracy of a model by seeing if it can predict a certain number of categories correctly (in this example 10).  The algorithm is simple, if 100% accurate, then do this but this can't be done for continuous values for obvious reasons.  I know that rmse is a very useful way to assess the quality of regression model but how could I use this same logic to evaluate a regression model?  How do I know what to use for the cutoff?  I'm sure it's dependent on the data but how could I know what to look for? Below, I gave an example of how it works for a classifier, but I could really use some help in extending this to regression models predicting continuous data. The # Continuous prediction section is pseudo code that won't run but everything up until that section should run. from sklearn.linear_model import LogisticRegression, Lassofrom sklearn.datasets import *from sklearn.cross_validation import LeavePOutfrom sklearn import metrics# Categorical prediction    # Load datairis = load_iris()X, y = iris.data, iris.target    # Leave pair out indicieslpo = LeavePOut(n=X.shape[0], p=10)    # Number of cross-validationsN = 1000    # Iterate through modelsgood_models = list()Mod_clf = LogisticRegression(random_state=0)idx = 0for tr_idx, te_idx in lpo:    Mod_clf.fit(X[tr_idx,:], y[tr_idx])    accuracy = metrics.accuracy_score(y[te_idx], Mod_clf.predict(X[te_idx,:]))    if accuracy == 1.0:        good_models.append(tr_idx)    if idx == N:        break    idx += 1# How many were good? len(good_models)# 952# Continuous prediction    # Load databoston = load_boston()X, y = boston.data, boston.target    # Leave pair out indicieslpo = LeavePOut(n=X.shape[0], p=10)    # Number of cross-validationsN = 1000    # Iterate through modelsgood_models = list()Mod_regr = Lasso(random_state=0)idx = 0for tr_idx, te_idx in lpo:    Mod_regr.fit(X[tr_idx,:], y[tr_idx])    # Can't do this obviously    # accuracy = metrics.accuracy_score(y[te_idx], Mod_regr.predict(X[te_idx,:]))    # ValueError: continuous is not supported    # Root Mean Squared Error    rmse = np.sqrt(metrics.mean_squared_error(y[te_idx], Mod_regr.predict(X[te_idx,:])))    # For the first 10    # 5.25500009824    # 5.32108583557    # 5.41359559392    # 5.2581109734    # 5.31248941431    # 5.38128598652    # 5.32969937204    # 5.25701858098    # 5.26163919423    # 5.25943780064    # 5.33894556871#     if (How to make a cutoff?):#         good_models.append(tr_idx)    if idx == N:        break    idx += 1","Creater_id":92493,"Start_date":"2016-08-10 20:47:00","Question_id":229299,"Tags":["regression","machine-learning","mathematical-statistics","cross-validation","model"],"Answer_count":0,"Last_activity":"2016-08-10 20:47:00","Link":"http://stats.stackexchange.com/questions/229299/how-to-choose-cutoff-for-accuracy-in-regression-models-scikit-learn-python-3","Creator_reputation":238}
{"_id":{"$oid":"5837a581a05283111e4d595b"},"View_count":66,"Display_name":"amirgh","Question_score":5,"Question_content":"I am looking for a curve fitting method that provides upper and lower bounds confidence intervals for the fitted curve which is illustrated as figure below. Assume that the red solid curve is the fitted curve that could be obtained by ordinary curve fitting tools. However, such curve somehow predicts the expected values. I am looking for the dashed curves that are upper and lower bounds for the fitted curve. Note that my data have heteroscedasticity properties. Any help is much appreciated.","Creater_id":127050,"Start_date":"2016-08-08 14:47:41","Question_id":228872,"Tags":["confidence-interval","stochastic-processes","heteroscedasticity","curve-fitting"],"Answer_count":1,"Last_activity":"2016-08-10 20:36:34","Link":"http://stats.stackexchange.com/questions/228872/looking-for-a-stochastic-curve-fitting-method","Creator_reputation":28}
{"_id":{"$oid":"5837a581a05283111e4d595d"},"View_count":48,"Display_name":"KirkDCO","Question_score":0,"Question_content":"I'm working with a survival model to evaluate its accuracy on a new dataset that was not involved in training or initial evaluation.  This dataset is a truly independent dataset.  Within this dataset, most of the events occur within just under 2 years (\u003e90%), but we have additional observations that occur beyond that 2 year timepoint.  Additionally, the median time of follow up is just under 2 years.At this stage, I have chosen to calculate a 2-year risk score from the model for this new dataset, but there have been questions regarding how to properly identify those observations for which an event happened soon after the 2 year mark.  Below are the two main approaches that I've tried to present in an unbiased manner.One argument is that events are events, and given their close proximity to the 2-year mark, they should be considered events for all further evaluations as their 2-year risk scores are likely higher than any non-events observations within the study.  The alternative argument is that those observations for which events occurred after the risk score time should be counted as non-events regardless of their proximity to the calculation date.  In other words, if an event occurred at 2 years and one day, that's still a non-event at 2 years and should be considered as such.A third alternative consists of having three groups:  non-events, events within 2-years, and events after 2-years.Another suggestion has been to calculate the risk score for the maximum follow up time for an event observation, in this case 2.4 years thus covering all possible event times.  The argument against this is that some of the censored (and thus non-event observations) may have had events that we do not know about, potentially altering the non-event group's risk scores.I would appreciate any suggestions regarding how to approach this problem.Some extra details following the response by Todd:The original model is an AFT model and the original data had a 5-year follow up time.  I'm using this model now in a new dataset which has a shorter follow up time overall, but is similar in composition to the original dataset.  This is where the 2-year value for risk prediction is coming from, and it is simply a parameter within the AFT model.For validation we have taken various approaches including test of calibration including:  Hossmer-Lemeshow test using quantiles of risk scores compared to actual event rate within those quantiles; and most relevant to this questions, a discrimination test (discrimination slope?) comparing the distribution of risk scores for those observations considered non-events to those considered events.","Creater_id":36206,"Start_date":"2016-08-08 15:25:24","Question_id":228881,"Tags":["survival"],"Answer_count":1,"Last_activity":"2016-08-10 20:26:11","Link":"http://stats.stackexchange.com/questions/228881/evaluation-of-survival-analysis-time-to-event-calculations-and-cutoffs","Creator_reputation":95}
{"_id":{"$oid":"5837a581a05283111e4d595f"},"View_count":51,"Display_name":"Cofeinnie Bonda","Question_score":2,"Question_content":"I have daily return data for SPX over 50 years. And I calculate the mean return by just taking arithmetic average. I want to test the hypothesis whether the mean is 0.Can I use the t statistic, which is (mean-0)/sample varirance, to test whether the mean return is 0? If not, what statistic should I use? Thanks.","Creater_id":127292,"Start_date":"2016-08-10 14:18:22","Question_id":229270,"Tags":["hypothesis-testing"],"Answer_count":2,"Last_activity":"2016-08-10 19:48:19","Link":"http://stats.stackexchange.com/questions/229270/how-can-i-test-whether-the-mean-return-of-stock-indices-is-0","Creator_reputation":13}
{"_id":{"$oid":"5837a581a05283111e4d5961"},"View_count":345,"Display_name":"stefan","Question_score":3,"Question_content":"Is it possible to slowly turn a bimodal distribution into a normal distribution slowly by shifting some parameter K?The reason I want to do this is because I want to conduct a scenario analysis and the distribution looks like it ranges from being two humps to one central hump(bell curve)Thanks!","Creater_id":61795,"Start_date":"2014-12-12 15:12:15","Question_id":128912,"Tags":["distributions","normal-distribution"],"Answer_count":1,"Last_activity":"2016-08-10 19:46:04","Link":"http://stats.stackexchange.com/questions/128912/possible-to-morph-a-bimodal-distribution-into-a-normal-distribution-slowly","Creator_reputation":18}
{"_id":{"$oid":"5837a581a05283111e4d5963"},"View_count":23,"Display_name":"Mayur Mane","Question_score":1,"Question_content":"I have two values those are affinity % and population %. Now audience percentage is having more importance compare to affinity percentage. Below table is an example I can give:If I sort from largest to lowest then score for highest audience percentage should come first. Would appreciate if you can share suggestion please. ","Creater_id":127173,"Start_date":"2016-08-09 12:21:21","Question_id":229055,"Tags":["ranking"],"Answer_count":0,"Last_activity":"2016-08-10 19:42:14","Link":"http://stats.stackexchange.com/questions/229055/how-to-find-a-particular-score-from-two-different-values-which-are-carrying-diff","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d5965"},"View_count":53,"Display_name":"Kelvin","Question_score":0,"Question_content":"I have 2 normally distributed random variable  and , which are combined to give the weighted distribution  as follows:f_H = p * f_1(x) + (1-p) * f_0(x),where  has pdf  and  and  have pdfs  and  respectively.The mean value  of the combined distribution  is:Now, what is the standard deviation  of ?Is there a simple formula for , in terms of ?","Creater_id":109510,"Start_date":"2016-04-02 09:55:33","Question_id":205126,"Tags":["normal-distribution","standard-deviation","gaussian-mixture","weighted-mean","finite-mixture-model"],"Answer_count":2,"Last_activity":"2016-08-10 19:35:18","Link":"http://stats.stackexchange.com/questions/205126/standard-deviation-for-weighted-sum-of-normal-distributions","Creator_reputation":398}
{"_id":{"$oid":"5837a581a05283111e4d5967"},"View_count":38,"Display_name":"rikkitikkitumbo","Question_score":2,"Question_content":"If I randomly generate a number between 1 and 10.... 10 times, and then total all the numbers, what will the standard deviation of that total be?I'm pretty sure the mean of the total will be 55.5, but what about the average distance from that mean? ","Creater_id":70613,"Start_date":"2016-08-10 18:08:05","Question_id":229287,"Tags":["probability","standard-deviation","uniform"],"Answer_count":1,"Last_activity":"2016-08-10 19:26:44","Link":"http://stats.stackexchange.com/questions/229287/standard-deviation-of-the-sum-of-a-discrete-uniform","Creator_reputation":111}
{"_id":{"$oid":"5837a581a05283111e4d5969"},"View_count":34,"Display_name":"You_got_it","Question_score":0,"Question_content":"As the question says (I understand that Weka calls features attributes). For example, here is part of an output where the number of variables (cut off) is different from the number of features/attributes :Instances:    100Attributes:   599          [list of attributes omitted]Test mode:    10-fold cross-validation=== Classifier model (full training set) ===CostSensitiveClassifier using reweighted training instancesweka.classifiers.functions.Logistic -R 1.0E-8 -M -1Classifier ModelLogistic Regression with ridge parameter of 1.0E-8  Coefficients...                                    ClassVariable                                  man=============================================good                                   0.8888left                                   0.8793","Creater_id":61899,"Start_date":"2016-08-09 09:23:59","Question_id":229019,"Tags":["regression","machine-learning","logistic","feature-selection","weka"],"Answer_count":1,"Last_activity":"2016-08-10 18:39:06","Link":"http://stats.stackexchange.com/questions/229019/what-is-the-difference-between-attributes-features-and-variables-in-wekas-log","Creator_reputation":52}
{"_id":{"$oid":"5837a581a05283111e4d596b"},"View_count":26,"Display_name":"Thomas Moore","Question_score":0,"Question_content":"My professor said that percent change between two data points in a nonlinear data set doesn't make sense, as, it can be shown that the percent change between two adjacent points in a time series is proportional to the slope. I.e, , since  for two adjacent points. Is this correct?","Creater_id":109091,"Start_date":"2016-08-10 17:35:34","Question_id":229285,"Tags":["time-series","econometrics","computational-statistics"],"Answer_count":1,"Last_activity":"2016-08-10 18:14:08","Link":"http://stats.stackexchange.com/questions/229285/percent-change-for-nonlinear-data","Creator_reputation":21}
{"_id":{"$oid":"5837a581a05283111e4d596d"},"View_count":19,"Display_name":"madsthaks","Question_score":0,"Question_content":"So, I did do a search already and came across this response but, his explanation went over my head a bit. The research i've done online hasn't been any more helpful. I've used this code,m3 \u0026lt;- glm(daysabs ~ math + prog, family = \"poisson\", data = dat)X2 \u0026lt;- 2 * (logLik(m1) - logLik(m3))to find out whether or not the Poisson model is more applicable but i'm not sure what to do if the value of X2 is close to 0. Also, in the link, he mentions, Linearity: The model is still linear in the parameters (i.e. the linear predictor is Xβ), but the expected responseis not linearly related to them (unless you use the identity link function!).but I'm not sure what that means. Should I be looking at the linearity between Y vs. X? or the Residuals of the regression model vs. X?Please advise.","Creater_id":125449,"Start_date":"2016-08-10 18:05:56","Question_id":229286,"Tags":["r","regression","assumptions","negative-binomial"],"Answer_count":0,"Last_activity":"2016-08-10 18:05:56","Link":"http://stats.stackexchange.com/questions/229286/in-laymans-terms-what-are-the-assumptions-for-negative-binomial-regression-mod","Creator_reputation":45}
{"_id":{"$oid":"5837a581a05283111e4d596f"},"View_count":785,"Display_name":"Christin","Question_score":11,"Question_content":"When reading about how to approximate the distribution of the sample mean I came across the nonparametric bootstrap method. Apparently one can approximate the distribution of  by the distribution of , where  denotes the sample mean of the bootstrap sample. My question then is: Do I need the centering? What for? Couldn't I just approximate  by ?","Creater_id":14890,"Start_date":"2012-10-12 08:39:48","Question_id":39297,"Tags":["distributions","bootstrap","resampling","centering"],"Answer_count":1,"Last_activity":"2016-08-10 17:29:29","Link":"http://stats.stackexchange.com/questions/39297/is-centering-needed-when-bootstrapping-the-sample-mean","Creator_reputation":59}
{"_id":{"$oid":"5837a581a05283111e4d5971"},"View_count":47,"Display_name":"Tom Chen","Question_score":3,"Question_content":"Theory of MLE's state that, for , that  (assuming some regularity conditions).Now suppose  for all  and  for fixed . Compute  (that is, the MLE based on the subsample ). Then calculate . Is it also true that ? Are there conditions to guarantee this is true?","Creater_id":117159,"Start_date":"2016-08-10 11:16:16","Question_id":229234,"Tags":["maximum-likelihood","asymptotics","sample-mean"],"Answer_count":1,"Last_activity":"2016-08-10 17:03:55","Link":"http://stats.stackexchange.com/questions/229234/sample-average-of-fixed-subsample-mles","Creator_reputation":16}
{"_id":{"$oid":"5837a581a05283111e4d5973"},"View_count":17327,"Display_name":"Curious2learn","Question_score":39,"Question_content":"I want to browse a .rda file (R dataset). I know about the View(datasetname) command. The default R.app that comes for Mac does not have a very good browser for data (it opens a window in X11). I like the RStudio data browser that opens with the View command. However, it shows only 1000 rows and omits the remaining. (UPDATE: RStudio viewer now shows all rows) Is there a good browser that will show all rows in the data set and that you like/use.","Creater_id":4820,"Start_date":"2011-06-03 21:45:39","Question_id":11551,"Tags":["r"],"Answer_count":6,"Last_activity":"2016-08-10 16:43:59","Link":"http://stats.stackexchange.com/questions/11551/is-there-a-good-browser-viewer-to-see-an-r-dataset-rda-file","Creator_reputation":395}
{"_id":{"$oid":"5837a581a05283111e4d5975"},"View_count":56,"Display_name":"Kodiologist","Question_score":3,"Question_content":"The median of a vector  is a scalar  minimizing the mean of . Analogously, when quantile regression is used to estimate medians, it tries to minimize the mean of the absolute residuals.But suppose we consider the median of the absolute deviations rather than their mean (or sum). The median of  need not minimize the median of . Building a regression model that tries to minimize the median of the absolute residuals has a certain intuitive appeal, in that median absolute error has a natural interpretation as a distance around true values that predictions are as likely as not to fall within.This leads me to wonder:Is there a name for the value  that minimizes the median of ? What about a regression model minimizing the median absolute residual?Are there any better algorithms for calculating this value than using a generic function-minimizing routine like R's optim? How about algorithms for fitting this sort of regression model?","Creater_id":14076,"Start_date":"2016-08-07 15:39:59","Question_id":228700,"Tags":["regression","terminology","robust","median"],"Answer_count":1,"Last_activity":"2016-08-10 16:31:13","Link":"http://stats.stackexchange.com/questions/228700/minimizing-the-median-absolute-deviation-or-median-absolute-error","Creator_reputation":8337}
{"_id":{"$oid":"5837a581a05283111e4d5977"},"View_count":42,"Display_name":"Programmer2134","Question_score":3,"Question_content":"I am studying machine learning and I am confused by one of the derivations in our textbook. I have two questions:what makes equation 3.10 a \"exponential of a quadratic function of w\"? How is the distribution of p(w) derived here?Here is the relevant excerpt from my textbook:  We begin our discussion of the Bayesian treatment of linear regression  by introducing a prior probability distribution over the model  parameters w. [...] First note that the likelihood  function p(t|w) defined by (3.10) is the exponential of a quadratic  function of w. The corresponding conjugate prior is therefore given by  a Gaussian distribution of the form                (equation 3.10):           ","Creater_id":127096,"Start_date":"2016-08-09 07:11:06","Question_id":228989,"Tags":["bayesian","likelihood","quadratic-form"],"Answer_count":1,"Last_activity":"2016-08-10 16:20:30","Link":"http://stats.stackexchange.com/questions/228989/what-makes-this-equation-exponential-of-quadratic-and-how-is-this-distributio","Creator_reputation":143}
{"_id":{"$oid":"5837a581a05283111e4d5979"},"View_count":19,"Display_name":"Xian  Wang","Question_score":0,"Question_content":"I have a R code that do the multi-regression based on PCA. But I do not understand how the code calculate the coefficient of every variable. Here is the code. setwd(\"C:/Users/Mei/Desktop\")data=read.csv(file=\"Data.csv\",header=T)attach(data)names(data)data_SOS=data[, c(\"pre120102\",\"pre030405\",\"MDT120102\",\"MNT120102\",\"MDT030405\",\"MNT030405\",\"Sta_day\")]data_SOS_1=data_SOS[, c(\"pre120102\",\"pre030405\",\"MDT120102\",\"MNT120102\",\"MDT030405\",\"MNT030405\")]names(data_SOS_1)=c(\"P120102\",\"P030405\",\"DT120102\",\"NT120102\",\"DT030405\",\"NT030405\")pcal=princomp(data_SOS_1, scores=TRUE,cor=TRUE)summary(pcal)loadings(pcal)plot(pcal)data_SOS_2=data_SOS[,\"Sta_day\"]data_SOS_2=as.data.frame(data_SOS_2)#SELECT TWO COMPONENTSpre=predict(pcal)data_SOS_2Z2=pre[,2]#START REGRESSION, ACCORDING TO P-VALUUE, JUST SELECT THE FIRST COMPONENTlm_sub_data_SOS=lm(data_SOS_2~Z1,data=data_SOS_2)summary(lm_sub_data_SOS)#calculate the coeeficients of every variable, I do not understand this part**beta=coef(lm_sub_data_SOS)A=loadings(pcal)x.bar=pcalscalecoef=(beta[2]*A[,1])/x.sdbeta0=beta[1]-sum(x.bar*coef)c(beta0,coef)**# I am not sure about this results","Creater_id":89088,"Start_date":"2016-08-10 14:58:27","Question_id":229277,"Tags":["regression"],"Answer_count":0,"Last_activity":"2016-08-10 16:06:03","Link":"http://stats.stackexchange.com/questions/229277/regression-based-on-pca","Creator_reputation":1}
{"_id":{"$oid":"5837a581a05283111e4d597b"},"View_count":174,"Display_name":"Frederik","Question_score":3,"Question_content":"I'm performing a factor analysis and I have for a variable a Kaiser-Meyer-Olkin (KMO) measurement of .710 and a communality of .136. I recall that we are recommended to delete variables with a low KMO statistic (\u0026lt;=0.5) or with a low communality. In this case, I am not sure how to deal with this particular variable.","Creater_id":127278,"Start_date":"2016-08-10 11:50:39","Question_id":229244,"Tags":["factor-analysis"],"Answer_count":1,"Last_activity":"2016-08-10 16:03:10","Link":"http://stats.stackexchange.com/questions/229244/high-kmo-but-low-communality-in-factor-analysis","Creator_reputation":18}
{"_id":{"$oid":"5837a581a05283111e4d597d"},"View_count":615,"Display_name":"David Dao","Question_score":4,"Question_content":"I have  labeled images, each with 224x224 pixels and 5 different image channels. What is the best way to train a CNN architecture using this data when  is small (less than 2000)? Is it possible to consider each channel as a separate image or is it better to input all  channels into the input layer at the same time? ","Creater_id":98120,"Start_date":"2016-03-04 17:17:07","Question_id":200038,"Tags":["deep-learning","image-processing","conv-neural-network","train"],"Answer_count":2,"Last_activity":"2016-08-10 15:48:11","Link":"http://stats.stackexchange.com/questions/200038/how-to-train-convolutional-neural-networks-with-multi-channel-images","Creator_reputation":270}
{"_id":{"$oid":"5837a581a05283111e4d597f"},"View_count":106,"Display_name":"Ajay Choudhary","Question_score":1,"Question_content":"I'm doing the following competition on Kagglehttps://www.kaggle.com/c/street-view-getting-started-with-julia/details/knn-tutorialIts data set consists of bmp images of size 20x20 and we are required to use KNN as the algorithm. Now since KNN works with Euclidean Distances, I'm not able to figure out how should I give images as an input to KNN? How should I select the features and convert the images into vector representation?","Creater_id":103185,"Start_date":"2016-02-26 17:39:30","Question_id":198838,"Tags":["data-transformation","image-processing","k-nearest-neighbour"],"Answer_count":1,"Last_activity":"2016-08-10 15:15:39","Link":"http://stats.stackexchange.com/questions/198838/help-with-kaggle-data-set","Creator_reputation":29}
{"_id":{"$oid":"5837a581a05283111e4d5981"},"View_count":311,"Display_name":"Roman","Question_score":2,"Question_content":"There are different machine learning algorithms (neural networks, decision trees, support vector machines and so on). Usually we assume that there is a mapping from vectors to nominal or numeric values and we try to use the above mentioned algorithms to learn this unknown mapping (so, we speak about classification and regression).Now assume that we need to use images as input. It means that we need to learn a model to classify images or to assign numeric value to each image. What an intermediate step should we do in order to be able to use images as inputs in a meaningful way?Of course we can easily represent an image as a vector (read, green and blue values of each pixel). However, in this case we loose information about vicinity of pixels. For example the pixels that are just one or two steps from each other could be very distant in the vector. Another problem is that we get a very large input vectors (number of components is hight x width x 3). So, we can have a problem in which we use 1000 observations with input vectors having 1 000 000 components.So, to summarize, what are the techniques for transforming images into meaningful inputs (features vectors in the machine learning language)? Or, in other words, how to apply the machine learning techniques in case when images are used as input.","Creater_id":2407,"Start_date":"2015-05-04 06:10:27","Question_id":149644,"Tags":["regression","machine-learning","image-processing"],"Answer_count":1,"Last_activity":"2016-08-10 15:15:14","Link":"http://stats.stackexchange.com/questions/149644/how-to-use-images-as-input-in-machine-learning","Creator_reputation":69}
{"_id":{"$oid":"5837a581a05283111e4d5983"},"View_count":39,"Display_name":"Austin Przybysz","Question_score":1,"Question_content":"I am interested in recreating (for cost and data sharing discrepancies) a solution that machine learning solution companies like ABBYY have done. I would like to be able to take purchase order PDFs and convert the OCR text extraction to line item data. Several features would need to be extracted and a .csv should look something like:(PO ID Number,Product/service ID: UPC/SKU, Product/ service description,Manufacturer/vendor, Etc.)To clarify my question:1) Do I need to train a model for each feature I want to extract or are multivariate ML models for this?2) Is there a documented workflow for such a task?3) There are thousands of PO PDF's that could have many different vendors thus the format may not be the same.I plan to use R to for this, but would eventually like to move into Microsoft Azure ML where I just use R in the pre-processing stack and run the algorithm/ deployed model with Azure so that either the client's or our dev team may find use in an application.","Creater_id":127290,"Start_date":"2016-08-10 13:56:26","Question_id":229265,"Tags":["machine-learning","rocr"],"Answer_count":1,"Last_activity":"2016-08-10 15:02:11","Link":"http://stats.stackexchange.com/questions/229265/work-flow-for-multinomial-text-extraction-of-ocr-data","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d5985"},"View_count":984,"Display_name":"Darian","Question_score":3,"Question_content":"Using all possible subsets we consider the adjusted , Akaike's Information Criterion (AIC), corrected AIC (), and Bayesian Information Criterion. The model with the highest adjusted ), and lowest AIC, AICc and BIC is usually the best model.When doing stepwise subsets we use backward elimination and forward selection. Based on the criterion we choose we either add predictor variables to reduce the criterion (forward selection) or subtract predictor variables to reduce the criterion (backward elimination).My question is why do we sometimes encounter different models with the two approaches? Is it because in stepwise subsets we only focus on minimizing one criterion? Also does this mean using all possible subsets provides a better model?","Creater_id":60521,"Start_date":"2014-11-20 01:46:26","Question_id":124800,"Tags":["multiple-regression","model-selection"],"Answer_count":1,"Last_activity":"2016-08-10 14:53:55","Link":"http://stats.stackexchange.com/questions/124800/variable-selection-for-multiple-linear-regression","Creator_reputation":31}
{"_id":{"$oid":"5837a581a05283111e4d5987"},"View_count":51,"Display_name":"Marissa","Question_score":1,"Question_content":"So I am working on this regression and I am doing it for multiple levels of Females. I have done one regression for Females 18-34 and got this outputF18\u0026lt;-read.csv(\"C:/Users/marissa.ferguson/Desktop/Unrated/F18-34.csv\", header = T, sep = \",\", na.strings = \"?\")female\u0026lt;-na.omit(F18)set.seed(1000)train.size\u0026lt;-0.8train.index\u0026lt;- sample.int(length(subfemaleDemoMedianRtg)*train.size))train.sample\u0026lt;-subfemale[train.index,]test.sample\u0026lt;-subfemale[-train.index,]\u0026gt; Overall\u0026lt;-lm(DemoMedianRtg~ DP+Subscribers+Tier+SubRange+Male.+Female.+Avg.Age+NewTier+NewSubs+Avg.Income, data=train.sample)\u0026gt; summary(Overall)Call:lm(formula = DemoMedianRtg ~ DP + Subscribers + Tier + SubRange + Male. + Female. + Avg.Age + NewTier + NewSubs + Avg.Income, data = train.sample)Residuals:  Min        1Q    Median        3Q       Max -0.038269 -0.014386  0.003568  0.012190  0.029538 Coefficients: (8 not defined because of singularities)              Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)      2.258e+00  5.495e-01   4.108 0.000375 ***DPEarly Fringe   8.178e-03  1.129e-02   0.724 0.475665    DPEarly Morning  1.012e-02  1.739e-02   0.582 0.566011    DPLate Fringe    7.304e-02  1.643e-02   4.445 0.000157 ***DPOvernight      3.327e-02  1.968e-02   1.691 0.103281    DPPrimeTime      1.999e-02  1.188e-02   1.682 0.105005    DPWeekend        3.971e-02  1.054e-02   3.769 0.000895 ***Subscribers     -2.142e-05  5.280e-06  -4.057 0.000428 ***TierTier2       -9.673e-01  2.090e-01  -4.627 9.79e-05 ***TierTier3       -1.341e+00  3.010e-01  -4.457 0.000152 ***SubRange40-50K          NA         NA      NA       NA    SubRange60-70K          NA         NA      NA       NA    SubRange80-90K  -3.722e-01  9.075e-02  -4.101 0.000382 ***SubRange90-100K -1.958e-01  3.001e-02  -6.524 7.82e-07 ***Male.            9.721e-02  7.822e-02   1.243 0.225490    Female.                 NA         NA      NA       NA    Avg.Age                 NA         NA      NA       NA    NewTierTier2            NA         NA      NA       NA    NewTierTier3            NA         NA      NA       NA    NewSubs                 NA         NA      NA       NA    Avg.Income              NA         NA      NA       NA  As you can see Female. is one level. When I do it for another level of females I get this outputF2554\u0026lt;-read.csv(\"C:/Users/marissa.ferguson/Desktop/Unrated/F25-54.csv\", header = T, sep = \",\", na.strings = \"?\") female\u0026lt;-na.omit(F2554)set.seed(1000)train.size\u0026lt;-0.8train.index\u0026lt;- sample.int(length(subfemaleDemoMedianRtg)*train.size))train.sample\u0026lt;-subfemale[train.index,]test.sample\u0026lt;-subfemale[-train.index,]Overall\u0026lt;-lm(DemoMedianRtg~ Qtr+DP+Subscribers+Tier+SubRange+Male.+Female.+Avg.Age+NewTier+NewSubs+Avg.Income, data=train.sample)Coefficients: (39 not defined because of singularities)               Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)      -2.022e+00  7.314e-01  -2.765 0.005911 ** Qtr2015 Q4        3.252e-02  1.602e-02   2.030 0.042898 *  Qtr2016 Q1        1.529e-02  1.721e-02   0.888 0.374835    Qtr2016 Q2        1.265e-02  2.170e-02   0.583 0.560224    DPEarly Fringe    1.724e-02  1.842e-02   0.936 0.349910    DPEarly Morning  -2.050e-03  3.006e-02  -0.068 0.945656    DPLate Fringe     4.026e-02  2.070e-02   1.946 0.052270 .  DPOvernight      -2.201e-02  2.603e-02  -0.846 0.398193    DPPrimeTime       8.661e-02  1.909e-02   4.538 7.14e-06 ***DPWeekend         4.809e-02  2.040e-02   2.358 0.018760 *  Subscribers       1.638e-05  6.596e-06   2.483 0.013360 *  TierTier2        -1.314e-01  1.155e-01  -1.138 0.255800    TierTier3         6.884e-01  3.691e-01   1.865 0.062803 .  SubRange110-120K -3.832e-01  1.126e-01  -3.402 0.000722 ***SubRange30-40K    8.673e-01  2.440e-01   3.555 0.000414 ***SubRange40-50K    1.229e-01  8.418e-02   1.460 0.144792    SubRange50-60K           NA         NA      NA       NA    SubRange60-70K    7.460e-01  2.604e-01   2.865 0.004347 ** SubRange70-80K   -9.732e-02  1.917e-01  -0.508 0.611830    SubRange80-90K    2.315e-01  1.211e-01   1.912 0.056398 .  SubRange90-100K   8.464e-02  6.387e-02   1.325 0.185729    Male.0.24        -2.775e-01  6.075e-02  -4.567 6.25e-06 ***Male.0.25        -5.249e-02  4.436e-02  -1.183 0.237277    Male.0.29        -7.252e-02  6.412e-02  -1.131 0.258537    Male.0.31         1.320e-01  5.029e-02   2.624 0.008956 ** Male.0.33         7.011e-02  1.235e-01   0.568 0.570611    Male.0.34        -1.765e-01  6.584e-02  -2.682 0.007572 ** Male.0.35         1.708e-02  7.568e-02   0.226 0.821518    Male.0.36        -9.714e-02  9.060e-02  -1.072 0.284170    Male.0.37        -2.328e-01  8.924e-02  -2.609 0.009369 ** Male.0.39                NA         NA      NA       NA    Male.0.4         -9.214e-02  8.566e-02  -1.076 0.282642    Male.0.41        -1.629e-01  6.081e-02  -2.679 0.007631 ** Male.0.42        -2.339e-01  5.697e-02  -4.106 4.71e-05 ***Male.0.44        -2.548e-01  6.245e-02  -4.079 5.26e-05 ***Male.0.45        -9.951e-02  8.220e-02  -1.211 0.226632    Male.0.46        -7.510e-02  1.079e-01  -0.696 0.486864    Male.0.47        -4.412e-02  5.739e-02  -0.769 0.442357    Male.0.48        -2.330e-01  6.124e-02  -3.805 0.000159 ***Male.0.5         -2.787e-01  7.532e-02  -3.700 0.000240 ***Male.0.51        -2.241e-01  6.402e-02  -3.501 0.000506 ***Male.0.52        -1.690e-01  5.705e-02  -2.962 0.003203 ** Male.0.53        -1.254e-01  8.424e-02  -1.489 0.137125    Male.0.55        -2.398e-01  5.648e-02  -4.246 2.60e-05 ***Male.0.56        -3.289e-01  9.453e-02  -3.479 0.000548 ***Male.0.57        -1.017e-01  8.750e-02  -1.162 0.245633    Male.0.58        -3.471e-01  8.574e-02  -4.048 6.00e-05 ***Male.0.59        -3.855e-01  1.067e-01  -3.612 0.000335 ***Male.0.6          6.813e-01  1.378e-01   4.943 1.06e-06 ***Male.0.62        -2.782e-01  7.264e-02  -3.830 0.000144 ***Male.0.63         4.929e-01  1.042e-01   4.730 2.94e-06 ***Male.0.64                NA         NA      NA       NA    Male.0.65         4.818e-01  8.043e-02   5.990 4.04e-09 ***Male.0.66        -4.060e-01  1.459e-01  -2.783 0.005591 ** Male.0.71        -2.394e-03  1.239e-01  -0.019 0.984588    Male.0.76        -1.261e-01  9.176e-02  -1.374 0.169912    Male.n/a         -1.728e-01  6.243e-02  -2.768 0.005846 ** Female.0.29              NA         NA      NA       NA    Female.0.34              NA         NA      NA       NA    Female.0.35              NA         NA      NA       NA    Female.0.36              NA         NA      NA       NA    Female.0.37              NA         NA      NA       NA    Female.0.38              NA         NA      NA       NA    Female.0.4               NA         NA      NA       NA    Female.0.41              NA         NA      NA       NA    Female.0.42              NA         NA      NA       NA    Female.0.43              NA         NA      NA       NA    Female.0.44              NA         NA      NA       NA    Female.0.45              NA         NA      NA       NA    Female.0.47              NA         NA      NA       NA    Female.0.48              NA         NA      NA       NA    Female.0.49              NA         NA      NA       NA    Female.0.5               NA         NA      NA       NA    Female.0.52              NA         NA      NA       NA    Female.0.53              NA         NA      NA       NA    Female.0.54              NA         NA      NA       NA    Female.0.55              NA         NA      NA       NA    Female.0.56              NA         NA      NA       NA    Female.0.58              NA         NA      NA       NA    Female.0.59              NA         NA      NA       NA    Female.0.6               NA         NA      NA       NA    Female.0.61              NA         NA      NA       NA    Female.0.63              NA         NA      NA       NA    Female.0.65              NA         NA      NA       NA    Female.0.66              NA         NA      NA       NA    Female.0.67              NA         NA      NA       NA    Female.0.69              NA         NA      NA       NA    Female.0.71              NA         NA      NA       NA    Female.0.75              NA         NA      NA       NA    Female.0.76              NA         NA      NA       NA    Female.0.77              NA         NA      NA       NA    Female.n/a               NA         NA      NA       NA    Avg.Age          -3.740e-03  1.244e-03  -3.006 0.002785 ** NewTierTier2      2.158e-01  1.048e-01   2.059 0.040012 *  NewTierTier3      4.357e-01  1.294e-01   3.367 0.000819 ***NewTierTier4             NA         NA      NA       NA    NewSubs           1.056e-05  2.873e-06   3.677 0.000262 ***Avg.Income       -3.272e-06  1.437e-06  -2.277 0.023193 *  ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1I am confused on why Female. and Male. are both different when I have copied and pasted the code and only changed the csv I am using for them. Any help would be appreciated. Is there a way to keep it at one variable? Or are the multiple levels necessary?","Creater_id":127268,"Start_date":"2016-08-10 11:17:07","Question_id":229235,"Tags":["r","categorical-data"],"Answer_count":2,"Last_activity":"2016-08-10 14:53:54","Link":"http://stats.stackexchange.com/questions/229235/why-is-r-changing-my-categorical-variable-into-one-variable-for-the-regression","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d5989"},"View_count":74,"Display_name":"Pinocchio","Question_score":1,"Question_content":"I was trying to figure out the number of units/features that one would get after doing a 1D convolution. Assume the condition of the question namely: k convolution filters of size f with an image of size D and stride sthen, how many features does a 1D convolution generate?To figure this out I drew a couple of examples. First for  simplest . It seems that for this special case its just how many times one can slide the filter until it reaches the end of the vector/signal/image in . It seems to be that for 1 filter we have D - f+1features. So in total features x number_if_filters = (D - f + 1)kHowever, when the stride is as big as a filter , then it seems that the answer is just how many times the filter fits in the input image (not that  seems really weird so I thought  was the largest stride that made sense). In this case we would have for 1 filter: \\left \\lfloor {\\frac{D}{f}} \\right \\rfloor  features. So in total features x number_if_filters = \\left \\lfloor {\\frac{D}{f}} \\right \\rfloorkhowever, I was having a hard time coming up with a general formula with  as a part of the equation (and obviously some conditions when the formula holds). Can anyone provide some guidance?","Creater_id":37632,"Start_date":"2016-08-10 09:17:12","Question_id":229217,"Tags":["machine-learning","neural-networks","conv-neural-network"],"Answer_count":1,"Last_activity":"2016-08-10 14:26:40","Link":"http://stats.stackexchange.com/questions/229217/given-k-convolution-filters-of-size-f-with-an-image-of-size-d-and-stride-s-how","Creator_reputation":790}
{"_id":{"$oid":"5837a581a05283111e4d598b"},"View_count":68,"Display_name":"Deepend","Question_score":2,"Question_content":"I am trying to compare user ratings of various products, the majority of which come in several standard versions. However some of the products do not come in certain versions meaning that my data set has several N/A values.The data is continuous interval type data with a range of -100 to +100My question is, when should these values be replaced by 0?            V1      V2      V3      V4      V5      V6      V7      V8      V9Product 1   2.63    -5.12   -0.41   5.29    9.89    4.16    14.73   9.06    -7.80Product 2   0.60    0.94    4.47    N/A     0.12    21.47   N/A     -4.63   1.29Product 3   5.53    -16.20  -19.56  N/A     2.24    N/A     15.07   -3.47   -6.93With N/A values included, excel tells me the average user rating given to each product is Product 1 = 3.60Product 2 = 3.47Product 3 = -3.33However if I replace the values with 0 then it changes the scores: Product 1 = 3.60Product 2 = 2.70Product 3 = -2.59I am sure others have dealt with this question before but I am not sure what to do. In future I want to undertake t-tests or z-tests on the data.Some ResearchTo be honest apart from a little bit here and some questions on Research Gate I cant find a lot on this topic, I suspect I am using the wrong search terms however. The below is what I have got so farThere appears to be lots of questions about how to replace N/A values in datasets but not when or whether it should be doneThere are a few other simalar questions on CV but they have not received an answer, e.g. hereA comment in reply to a question here, only slightly similar question suggests setting NA values to the minimum of the range, but this would significantly change my results...P.S. I am really not sure what to tag this question with so if anyone could apply better tags it would be much appreciated.Thanks","Creater_id":39684,"Start_date":"2016-08-10 11:47:15","Question_id":229243,"Tags":["missing-data","data-imputation"],"Answer_count":1,"Last_activity":"2016-08-10 14:16:33","Link":"http://stats.stackexchange.com/questions/229243/when-to-replace-not-applicable-or-n-a-values-with-a-zero","Creator_reputation":53}
{"_id":{"$oid":"5837a581a05283111e4d598d"},"View_count":25,"Display_name":"Raji Srinivasan","Question_score":1,"Question_content":"I am looking to estimate the distribution of the marginal effects for a multiple regression model with two interaction terms using the delta method.What Stata provides in the nlcom command is the average marginal effect for the entire data set, but I want to obtain the marginal effect for each observation in my data set using bootstrapping, so I can make inferences at the observation level.Thanks Raji Srinivasan","Creater_id":125403,"Start_date":"2016-08-10 13:49:23","Question_id":229264,"Tags":["estimation","stata","marginal","effects"],"Answer_count":0,"Last_activity":"2016-08-10 13:49:23","Link":"http://stats.stackexchange.com/questions/229264/stata-delta-method-to-compute-observation-level-marginal-effects-with-bootstrapp","Creator_reputation":8}
{"_id":{"$oid":"5837a581a05283111e4d598f"},"View_count":104,"Display_name":"J. Ferreira","Question_score":2,"Question_content":"How does one calculate the effect size for Games-Howell and Tukey's HSD? I work with SPSS, so does anyone know of an equation that would allow me to use the information provided by SPSS to manually calculate the effect size for these tests?","Creater_id":121826,"Start_date":"2016-08-09 21:52:50","Question_id":229112,"Tags":["anova","effect-size","power","post-hoc"],"Answer_count":0,"Last_activity":"2016-08-10 13:23:44","Link":"http://stats.stackexchange.com/questions/229112/effect-size-post-hoc-anova","Creator_reputation":75}
{"_id":{"$oid":"5837a581a05283111e4d5991"},"View_count":42,"Display_name":"Chris","Question_score":2,"Question_content":"Illustrative ProblemI have a number of geographically dispersed stores where I would like to understand what factors predict revenue. Each store has certain properties (sq ft, age, employees, etc.). I also have data for the area surrounding the store by zip code (population, industry spend, competition, etc.).I am struggling with the right way to incorporate the geographic data into my model. One potential would be to draw radii and create aggregation variables based on distance from the store. For example:Number of households 25km from a storeNumber of competitors 50km from a storeand then include these in a regression model. My question is:Is this the best technique? Are there other models better suited to do this analysis?Is there a way to statistically determine the distance I should use for each variable?","Creater_id":80631,"Start_date":"2016-08-10 12:02:04","Question_id":229247,"Tags":["regression","econometrics","spatial","geography"],"Answer_count":1,"Last_activity":"2016-08-10 13:18:19","Link":"http://stats.stackexchange.com/questions/229247/what-is-the-right-way-to-incorporate-geographic-data-into-a-prediction","Creator_reputation":113}
{"_id":{"$oid":"5837a581a05283111e4d5993"},"View_count":50,"Display_name":"Great38","Question_score":3,"Question_content":"I have n subjects. I perform an experiment on all the subjects and measure the outcome in two different ways. I want to evaluate the difference between these two types of measurements. I would use a matched-pairs t-test but I will be adding in a blocking variable.It seems like ANOVA wouldn't take into account that they are paired. outcome = measureType + blockingVar + measureType*blockingVarShould I use MANOVA where the two types of measurements are the two dependent variables?   ","Creater_id":122545,"Start_date":"2016-08-10 12:11:36","Question_id":229249,"Tags":["anova","repeated-measures","manova","paired-comparisons"],"Answer_count":0,"Last_activity":"2016-08-10 13:11:36","Link":"http://stats.stackexchange.com/questions/229249/what-is-the-anova-equivalent-of-a-matched-pairs-t-test-manova","Creator_reputation":111}
{"_id":{"$oid":"5837a581a05283111e4d5995"},"View_count":16,"Display_name":"Jonathan Gray","Question_score":1,"Question_content":"I have multiple dependant marketing variables measured on dichotomous scales and one independant variable with three levels (students, university personnel, both). I would like to define the type of study design in order to chose the correct statistical test for group differences. I want to see if the different marketing activities are different according to the main type of market (students, university personnel, both). 1) Between subject designs? 2) Within subject designs?3) Mixed designs?What actually worries  me is the category \"Both\". If not I would hae already classified it as \"Between subject design\"Can you help me with some ideas and suggestions? Thank you in advance!","Creater_id":83627,"Start_date":"2016-08-10 12:47:55","Question_id":229256,"Tags":["experiment-design"],"Answer_count":1,"Last_activity":"2016-08-10 13:08:38","Link":"http://stats.stackexchange.com/questions/229256/type-of-study-design","Creator_reputation":11}
{"_id":{"$oid":"5837a581a05283111e4d5997"},"View_count":32,"Display_name":"Jay","Question_score":3,"Question_content":"I am creating a linear regression where:Dependent Variable - Revenue by zipcodeIndependent Variables - Number of Competitors, Population by zip, and minimum distance from one of our two locations. What I want to do is see if we added a new location, what would be the revenue of that location? Would it be mathematically correct to go back to each of observations in the regression and put the values and coefficients together assuming the new location existed? If I do this, should I only do it for zip codes where the minimum distance is less to the new location, than to one of our other locations?","Creater_id":121821,"Start_date":"2016-08-10 13:04:58","Question_id":229258,"Tags":["regression","predictive-models"],"Answer_count":0,"Last_activity":"2016-08-10 13:07:07","Link":"http://stats.stackexchange.com/questions/229258/how-to-make-a-prediction-from-a-multiple-linear-regression","Creator_reputation":16}
{"_id":{"$oid":"5837a581a05283111e4d5999"},"View_count":22,"Display_name":"Vladmir Putin","Question_score":0,"Question_content":"If I have data on ten stock market indices, each with 1025 observations. Would it be alright to run a VECM(5) that estimates around 520 coefficients/constants? The LR statistic suggests a VECM(5) while the others recommend VECM(2). However, the VECM(5) specification seems to be giving the most stable representations and satisfies the tests for autocorrelation and normality of residuals etc. Please advise. ","Creater_id":105444,"Start_date":"2016-08-10 11:57:38","Question_id":229246,"Tags":["time-series","lags","vecm"],"Answer_count":0,"Last_activity":"2016-08-10 13:01:07","Link":"http://stats.stackexchange.com/questions/229246/vecm-lag-length","Creator_reputation":30}
{"_id":{"$oid":"5837a581a05283111e4d599b"},"View_count":11,"Display_name":"Jef Van Alsenoy","Question_score":1,"Question_content":"In my dataset I believe two factors (say 'elevation' and 'number of neighbours') affect a parameter ('A') under investigation. I took two subsets of the data (10 points with high A and 10 points with minimal A) and I would like to check whether the combination elevation:neighbours is significantly different for the low A subset compared to the high A subset.Elevation and neighbours are parameters on a very different scale. My plan was to combine them into one parameter by standardizing both (x-mean_x/sd) and then add them up. I'd do this for subset highA and subset lowA.Then I planned to do a kolmogorov-smirnov test to check whether both samples come from different distributions. I though they would, but they obviously don't because I standardized my data. What's another approach to this problem?","Creater_id":123928,"Start_date":"2016-08-10 12:51:04","Question_id":229257,"Tags":["standardization","kolmogorov-smirnov"],"Answer_count":0,"Last_activity":"2016-08-10 12:51:04","Link":"http://stats.stackexchange.com/questions/229257/alternative-approach-to-comparing-standardized-samples","Creator_reputation":26}
{"_id":{"$oid":"5837a581a05283111e4d599d"},"View_count":73,"Display_name":"jonaprieto","Question_score":1,"Question_content":"My database has missing data values at random. I want to impute these missing values. The data values are all categorical. I would like to work with hot-deck techniques and reduced models. What are the most well-known or representative algorithms of such techniques?A brief explanation in a few words of these techniques would be appreciated. ","Creater_id":79138,"Start_date":"2016-08-07 23:02:35","Question_id":228735,"Tags":["missing-data","data-imputation"],"Answer_count":1,"Last_activity":"2016-08-10 12:45:28","Link":"http://stats.stackexchange.com/questions/228735/hot-deck-methods-and-reduce-models","Creator_reputation":58}
{"_id":{"$oid":"5837a581a05283111e4d599f"},"View_count":15,"Display_name":"Mshakour","Question_score":1,"Question_content":"HiI feel a little bit confused about transforming the parameters of normal distribution to log normal. I have a random variable that is N(0.06,0.000144). Because this random variable cannot take negative values, I want to use an equivalent lognormal distribution.I calculated the mu (location parameter) as -2.83 and sigma squared (scale parameter) as 0.39. The model that I am using for simulation takes the mean and standard deviation of the lognormal distribution. when I calculate the mean and standard deviation of lognormal using the mu and sigma I get the original mean and standard deviation of the normal distribution. Is that correct?Ps. I used the following formulas to calculate mu, sigma, mean and standard deviation of the lognormal:","Creater_id":127051,"Start_date":"2016-08-08 14:54:28","Question_id":228876,"Tags":["normal-distribution","lognormal"],"Answer_count":0,"Last_activity":"2016-08-08 14:54:28","Link":"http://stats.stackexchange.com/questions/228876/converting-the-mean-and-std-of-normal-distribution-to-the-mean-and-std-of-lognor","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d59a1"},"View_count":113,"Display_name":"Jill Russek","Question_score":3,"Question_content":"What's the overall point of training error in the goal of regression (i.e, making predictions)?You might say something like, \"well, you see, training error can help you determine which model of complexity is the best to use. \"And to that, some would say, \"No you can't. Low training error could just mean that your model is conforming to whatever data you're training the model with, A.K.A overfitting\"What's the point of calculating training error if it's not a good predictive measure of performance?Especially when we go through and say, to hell with training error, just use validation error..When will we ever use training error?Low training error can be indicative of overfitting.. is that the only use of it?","Creater_id":127039,"Start_date":"2016-08-08 12:59:30","Question_id":228856,"Tags":["regression","machine-learning","train"],"Answer_count":1,"Last_activity":"2016-08-08 14:22:07","Link":"http://stats.stackexchange.com/questions/228856/training-error-whats-the-point","Creator_reputation":137}
{"_id":{"$oid":"5837a581a05283111e4d59ae"},"View_count":13,"Display_name":"German Demidov","Question_score":0,"Question_content":"My dataset comprises from a \"count data\", but the number of counts are typically huge (more than 200) so I can use normal approximation of sqrt(data).Then I fit a lot of mixtures of normals by EM algorithm. Standard deviations are considered as being equal for different components.Unfortunately, some (less than 0.01%) of the \"regions\" in dataset are produced from small counts so the distribution is looking more like Neg Binomial and Gaussian mixture makes errors (fits too many clusters and the BIC is really small so it looks like a good fit).I would like to filter these points out, but not because of low counts. I want to say: \"If Normal Mixtures' centroids are far away from each other in terms of within cluster variance, then the Mixture is correct\".How can I do this? Can I just use the rule \"If the centroids are \u003e 6 standard deviations from each other, then it is a good separation\"?I have read in wiki:\"If sufficiently separated, namely by twice the (common) standard deviation, so , these form a bimodal distribution\"but it uses the common  and  will be evidently big for mixture of 3-4 Normals.(I have tried to work with Poisson/Neg Binomial mixtures, and the Normal Mixture works better and faster because of complex preliminary normalization).","Creater_id":100673,"Start_date":"2016-08-08 14:21:59","Question_id":228871,"Tags":["gaussian-mixture"],"Answer_count":0,"Last_activity":"2016-08-08 14:21:59","Link":"http://stats.stackexchange.com/questions/228871/mixture-of-normals-measure-of-quality-of-separation","Creator_reputation":344}
{"_id":{"$oid":"5837a581a05283111e4d59b0"},"View_count":16,"Display_name":"jnam27","Question_score":1,"Question_content":"I'm trying to assess goodness of fit for survival.I am using output of a parametric model for survival. I have the the survival at each year from time 0 to time 70 months. This model was created using individual patient data but then modified manually by changing hazard risks for a component risk of death. I do have knowledge of how the manual adjustment was made and I can reproduce it. So, to re-iterate, there is no statistical model that represents the output; I just have the survival at each time from 0 to 70 months.I also have the individual patient data.My objective is to assess how well the manually modified parametric model fits the observed data. How do I do this?","Creater_id":25174,"Start_date":"2016-07-13 08:35:20","Question_id":223581,"Tags":["survival","goodness-of-fit"],"Answer_count":0,"Last_activity":"2016-08-08 14:14:17","Link":"http://stats.stackexchange.com/questions/223581/how-do-i-assess-goodness-of-fit-when-i-dont-have-a-model-only-results-of-it","Creator_reputation":130}
{"_id":{"$oid":"5837a581a05283111e4d59b2"},"View_count":31,"Display_name":"mackbox","Question_score":2,"Question_content":"I have fitted several Bayesian network models to complete training data with different local probabilistic distribution but with a fixed network structure. I want to choose only one of them. How can I go about this? I've read that I can choose the model which gives the lowest prediction error or other loss metric, as well as some goodness of fit criteria like some information criterion. But how can I tell which of these approaches are better? Thanks!      ","Creater_id":73733,"Start_date":"2016-08-08 12:21:38","Question_id":228846,"Tags":["bayesian","graphical-model","bayesian-network"],"Answer_count":1,"Last_activity":"2016-08-08 14:09:07","Link":"http://stats.stackexchange.com/questions/228846/how-to-choose-a-bayesian-network","Creator_reputation":128}
{"_id":{"$oid":"5837a581a05283111e4d59bf"},"View_count":23,"Display_name":"Firebug","Question_score":1,"Question_content":"It's said the -penalty term is based on the -norm. Indeed, the term often is written as .Notice, though, that the norm is squared, differing from the -penaltym which is simply the -norm. It obviously helps in differentiation of the function, but does it change the interpretation of the penalty term?Would a regularization term like  lead to different results or are them equivalent?How does it generalizes to  regularization with ? Take or not take the -root of the penalty term?","Creater_id":60613,"Start_date":"2016-08-08 13:44:17","Question_id":228863,"Tags":["regularization"],"Answer_count":0,"Last_activity":"2016-08-08 13:44:17","Link":"http://stats.stackexchange.com/questions/228863/squaring-the-ell-2-norm-in-ell-2-regularization","Creator_reputation":2542}
{"_id":{"$oid":"5837a581a05283111e4d59c1"},"View_count":54,"Display_name":"siby","Question_score":0,"Question_content":"Does using PSO have advantages/disadvantages over back-propagation when training neural networks? Please give your opinion if you have used PSO or other heuristic methods.","Creater_id":127022,"Start_date":"2016-08-08 13:26:17","Question_id":228862,"Tags":["neural-networks","backpropagation","heuristic"],"Answer_count":0,"Last_activity":"2016-08-08 13:26:17","Link":"http://stats.stackexchange.com/questions/228862/using-particle-swarm-optimization-pso-h-in-neural-network-training","Creator_reputation":72}
{"_id":{"$oid":"5837a581a05283111e4d59c3"},"View_count":20,"Display_name":"Richard Rublev","Question_score":0,"Question_content":"I am reading tutorial written by Johan Suykens:Least Squares Support Vector MachinesOn page 19,he mentions link with kernel Fisher Discriminant AnalysisProject data from the original input space to one dimensional variable.I do not understand this,can someone elaborate more on this topic.","Creater_id":107372,"Start_date":"2016-04-11 07:27:52","Question_id":206661,"Tags":["svm","discriminant-analysis"],"Answer_count":1,"Last_activity":"2016-08-08 13:20:37","Link":"http://stats.stackexchange.com/questions/206661/link-between-the-fda-and-ls-svm","Creator_reputation":106}
{"_id":{"$oid":"5837a581a05283111e4d59d0"},"View_count":19,"Display_name":"Alexander David","Question_score":0,"Question_content":"I am having trouble figuring out the best approach for a classification problem:My data:For each physician in my data, I have a feature set of every different medical procedure where the feature value is the number of times the individual physician performed that procedure.  The features have been transformed using a tf-idf weighting to account for procedure volume and commonality.  In addition, there is a single feature which marks the physicians specialty (ex: OB/GYN, cardiology, etc).Classification problem:There are sub-specialties among the specialties.  For example, you could be a Reproductive Endocrinology and Infertility (REI) specialist under your OB/GYN credentials.  There are certain procedures that are 100% predictive of the REI specialty (that is, only REI specialists will bill for them).  But not all REI specialists will perform for those particular procedures.  So what I have for my training is three classes:OBGYN \u0026amp; potentially REIOBGYN \u0026amp; definitely REINon-OBGYN.I would like to develop a model which uses the \"definitely REI\" to predict which of the \"potentially REI\" are most likely to be REI's.  I'm struggling with how to appropriately train the classifier since it's very tough to label true negatives - OBGYN's who are not REI's.Thoughts so far:Train it using the labeled definite REI's as my postives, and my not-OBGYN as my negatives.  Then I would predict on my \"OBGYN/potentially REI\" set.  This seems like it would be a bad idea because the model will likely select for features that are unique to OBGYN's, not necessarily REI's.I'm really only familiar with supervised learning, so if this is a well-established semi-supervised/unsupervised problem, I apologize.  Any insight would be a huge help!","Creater_id":92406,"Start_date":"2016-08-08 12:44:37","Question_id":228853,"Tags":["machine-learning","logistic","classification"],"Answer_count":1,"Last_activity":"2016-08-08 13:18:04","Link":"http://stats.stackexchange.com/questions/228853/classification-with-partially-labelled-data-potential-positives","Creator_reputation":8}
{"_id":{"$oid":"5837a581a05283111e4d59dd"},"View_count":36,"Display_name":"maximalc","Question_score":0,"Question_content":"To give a simple hypothetical example, suppose one was interested in using linear regression to predict how horses would perform in a race.  Let's suppose the race includes 10 horses and that I have a variety of potential explanatory effects for all of them (e.g. weight, age, record, jockey, and so on) as well as a data set with a year's worth of past results and those same explanatory effects.  Let's also suppose that I am only interested in predicting a top three finish; outcomes outside of the top three are of no importance to me.My first instinct is that ordinary least squares would be inappropriate for this situation because OLS is used to predict the conditional mean, whereas I am interested in predicting a conditional quantile (the 70th percentile, essentially).  Am I even correct in thinking that quantile regression is the proper technique for this purpose?Assuming my choice of quantile regression is appropriate, my primary question is this: which quantile ought I to aim for if attempting to identify a horse that finishes in at least the top three positions?I might attempt to predict at the 70th percentile, operating under the logic that this is the threshold I am interested in the horse crossing, however that would seem to leave little margin for error, as I'd be predicting right on the boundary of success and failure.  Would it be more prudent to predict the 75th, or perhaps even the 90th percentile, operating under the logic that even if the horse predicted to achieve at that level underperforms, there is margin for error because I'm setting the bar higher than necessary?My first inclination is to go the former route, because I feel like there would have to be some trade-off with predicting a higher quantile than necessary, but I can't identify what that trade-off is.  Surely there is a sound reason for not simply attempting to predict the 1st place finisher when a 3rd place finish is equally satisfying to me, no?Is there a more proper alternative entirely, like composite quantile regression, perhaps?  I'm interested to hear how one would approach this situation, as I normally read about quantile regression in the context of comparing the influence of effects across quantiles rather than using quantile regression for prediction per se.Thanks in advance.","Creater_id":127030,"Start_date":"2016-08-08 12:28:03","Question_id":228849,"Tags":["regression","prediction","quantiles","quantile-regression"],"Answer_count":1,"Last_activity":"2016-08-08 12:45:18","Link":"http://stats.stackexchange.com/questions/228849/choosing-the-proper-quantile-for-prediction","Creator_reputation":1}
{"_id":{"$oid":"5837a581a05283111e4d59e9"},"View_count":51,"Display_name":"Speldosa","Question_score":3,"Question_content":"I have the following example data, stored in the variable TheData:Study    d    Variance    Category1        0    0.1         A1        5    0.1         B1        10   0.1         C2        20   0.1         A2        25   0.1         BThat is, I'm missing data for Category C for Study 2.I then fit two meta-analytic models on this data with Category as the moderator in both cases. Analysis1 is a two-level model model, with Study as the random factor, Analysis1 \u0026lt;- rma.mv(d, Variance, random = ~ 1 | Study, mods = ~ factor(Category) - 1, data=TheData)which gives me the following output:Multivariate Meta-Analysis Model (k = 5; method: REML)Variance Components:               estim     sqrt  nlvls  fixed  factorsigma^2    199.9504  14.1404      2     no   StudyTest for Residual Heterogeneity: QE(df = 2) = 4000.0000, p-val \u0026lt; .0001Test of Moderators (coefficient(s) 1,2,3): QM(df = 3) = 626.6563, p-val \u0026lt; .0001Model Results:                   estimate       se    zval    pval    ci.lb    ci.ub   factor(Category)A   10.0000  10.0013  0.9999  0.3174  -9.6021  29.6021   factor(Category)B   15.0000  10.0013  1.4998  0.1337  -4.6021  34.6021   factor(Category)C   19.9975  10.0050  1.9987  0.0456   0.3880  39.6070  *---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Analysis2 is a single-level model (and therefore without any random factors),Analysis2 \u0026lt;- rma.mv(d, Variance, mods = ~ factor(Category) - 1, data=TheData)which gives me the following output:Multivariate Meta-Analysis Model (k = 5; method: REML)Variance Components: noneTest for Residual Heterogeneity: QE(df = 2) = 4000.0000, p-val \u0026lt; .0001Test of Moderators (coefficient(s) 1,2,3): QM(df = 3) = 7500.0000, p-val \u0026lt; .0001Model Results:                   estimate      se     zval    pval    ci.lb    ci.ub     factor(Category)A   10.0000  0.2236  44.7214  \u0026lt;.0001   9.5617  10.4383  ***factor(Category)B   15.0000  0.2236  67.0820  \u0026lt;.0001  14.5617  15.4383  ***factor(Category)C   10.0000  0.3162  31.6228  \u0026lt;.0001   9.3802  10.6198  ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 As can be seen, when using the single-level model, Analysis2, each category estimate is simply the mean of the values for that category. However, when using the three-level model, Analysis1, the estimates for Category A and B are the same as in the single-level model, while Category C seems to be estimated based on an extrapolation of what its value would/should be in Study 2. That is, what seems to be going on (put in an extremely non-mathematical language, since I haven't fully grasped the inner workings of how these analyses are performed) is that the model goes: \"Oh, there's no data for Category C in study 2. Well, Category C was larger than both A and B in Study 1 so I guess it should be larger than Category A and B in Study 2 as well.\"This is probably fine for a lot of cases, but are there situations when it's simply not appropritate to let the model extrapolate in this way? For example, I have a dataset where the different categories represent different categories of tests that the participants in psychological studies participated in. That is, the categories are things like verbal, visuo-spatial, and tactile, where each category merely designates the type of the test and not a specific test (for example, the category tactile could be applicable on hundereds of different tests). Further, all categories are never represented within the same study, so the data is just filled with missing data.Here, it seems quite strange to extrapolate when trying to estimate the actual values of each category, especially since I have so many different categories as well (is it realistic to think that the extrapolation would be any good when I, for example, only have 2 out of 9 possible categories represented for a given study?). It simply seems messy to apply this method on this type of data. Am I on to something here?","Creater_id":3812,"Start_date":"2016-08-02 05:06:13","Question_id":226859,"Tags":["multilevel-analysis","meta-analysis","intuition"],"Answer_count":1,"Last_activity":"2016-08-08 12:40:24","Link":"http://stats.stackexchange.com/questions/226859/are-there-times-when-a-multi-level-meta-analytic-model-is-discouraged","Creator_reputation":206}
{"_id":{"$oid":"5837a581a05283111e4d59f6"},"View_count":75,"Display_name":"themli","Question_score":2,"Question_content":"Suppose two variables are perfectly uncorrelated (in a multiple regression with three parameters say), rather than perfect collinearity. What would this mean exactly? ","Creater_id":122071,"Start_date":"2016-08-08 11:47:16","Question_id":228843,"Tags":["regression","multiple-regression","econometrics"],"Answer_count":1,"Last_activity":"2016-08-08 12:36:19","Link":"http://stats.stackexchange.com/questions/228843/what-would-be-the-opposite-of-multicollinearity","Creator_reputation":16}
{"_id":{"$oid":"5837a581a05283111e4d5a03"},"View_count":73,"Display_name":"radek","Question_score":0,"Question_content":"I'm trying to get the CI of a proportion in Stata. Starting with some reproducible example:sysuse auto, cleartab foreign                     // for proportion* some dummy categorical variablesgen expen = price \u0026gt; 6165        // stratala de expen 0 \"Cheap\" 1 \"Expensive\"la val expen expentab expengen mpg_cat = mpg \u0026gt; 21          // categories for proportionla de mpg_cat 0 \"Low mpg \" 1 \"High mpg\"la val mpg_cat mpg_cattab mpg_cattab foreign mpg_cat if expen == 0, colI was trying to hijack logistic regression \u0026amp; margins to give me CI for proportion, however I get the negative value of LCI:. logit foreign i.mpg_cat if expen == 0Logistic regression                             Number of obs     =         52                                                LR chi2(1)        =       6.00                                                Prob \u0026gt; chi2       =     0.0143Log likelihood = -26.243758                     Pseudo R2         =     0.1025------------------------------------------------------------------------------     foreign |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------     mpg_cat |   High mpg  |   1.673976   .7359801     2.27   0.023      .231482    3.116471       _cons |  -2.079442   .6123724    -3.40   0.001    -3.279669   -.8792136------------------------------------------------------------------------------. eststo mpg_cat_0: margins mpg_cat, postAdjusted predictions                            Number of obs     =         52Model VCE    : OIMExpression   : Pr(foreign), predict()------------------------------------------------------------------------------             |            Delta-method             |     Margin   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------     mpg_cat |   Low mpg   |   .1111111   .0604812     1.84   0.066    -.0074299    .2296521   High mpg  |         .4   .0979796     4.08   0.000     .2079635    .5920365------------------------------------------------------------------------------Should I resort to using ci command instead?. eststo mpg_cat_0: bysort mpg_cat: ci proportions foreign if expen == 0, exact--------------------------------------------------------------------------------------\u0026gt; mpg_cat = Low mpg                                                         -- Binomial Exact --    Variable |        Obs  Proportion    Std. Err.       [95% Conf. Interval]-------------+---------------------------------------------------------------     foreign |         27    .1111111    .0604812        .0235275    .2915869--------------------------------------------------------------------------------------\u0026gt; mpg_cat = High mpg                                                         -- Binomial Exact --    Variable |        Obs  Proportion    Std. Err.       [95% Conf. Interval]-------------+---------------------------------------------------------------     foreign |         25          .4    .0979796        .2112548    .6133465","Creater_id":22,"Start_date":"2016-07-13 02:27:20","Question_id":223507,"Tags":["confidence-interval","stata","proportion"],"Answer_count":2,"Last_activity":"2016-08-08 12:33:40","Link":"http://stats.stackexchange.com/questions/223507/in-stata-how-to-avoid-negative-values-of-lower-confidence-interval-of-proportio","Creator_reputation":563}
{"_id":{"$oid":"5837a581a05283111e4d5a11"},"View_count":11,"Display_name":"snoram","Question_score":0,"Question_content":"Looking for inspirations for my amateur data analysis project, I came across the statement below skimming the book The Art of Data Science by R. D. Peng and E. Matsui.  [...] sometimes evaluating uncertainty is not  necessary because some types of analyses are not intended to make  inferences about a larger overall population. If, for example, you  wanted to understand the relationship between age and dollars spent  per month on your company’s products, you may have all of the data on  the entire, or “overall” population you are interested in -- which is  your company’s customers. In this case you do not have to rely on a  sample, because your company collects data about the age and purchases  of ALL of their customers. In this case, you would not need to  consider the uncertainty that your result reflects the truth for the  overall population because your analysis result is the truth for your  overall population.(emphasis mine).Currently I am working with data that covers the whole population of my unit of analysis. However, I realized my thinking about my problem is very different since what I am aiming at is not what has happened in my population, but how likely the pattern I am looking at are the result of an underlying mechanism (data generating process) or simply randomness. Therefore, even though I uncover some seemingly clear patterns, they could still arise from random noise. The goal is not \"the truth\" about the population, but the truth about how that \"truth\" arose -- what is happening in the population. Next step then is trying to get at the directions of causality.I came across a similar question asked here. But the top two answers seem to represent the two opposed views described here.My core questions: Is my approach not fundamentally opposed to Peng and Matsui's which is at best misleading in saying that \"evaluating uncertainty is not necessary\"?  Do you have any advice or recommended literature to continue thinkingabout the problem?","Creater_id":38379,"Start_date":"2016-08-08 12:21:53","Question_id":228847,"Tags":["inference","methodology"],"Answer_count":0,"Last_activity":"2016-08-08 12:21:53","Link":"http://stats.stackexchange.com/questions/228847/uncertainty-of-inference-when-analysing-data-for-the-overall-population","Creator_reputation":309}
{"_id":{"$oid":"5837a581a05283111e4d5a13"},"View_count":71,"Display_name":"putut purwandono","Question_score":1,"Question_content":"I am doing fuzzy RDD recently and I am facing some challenging moment dealing with Stata. I am using the command -rdplot- and -rdrobust-. I have some questions regarding the fuzzy RDD and the commands;When the cut-offs is not known, is it still possible to seek for a discontinuity using fuzzy RDD (or the sharp one)?the -rdrobust- command has an option fuzzy(treatment) to implement the fuzzy RDD, it has results on the coefficient. Let say my outcome variable is LN number of passengers, running variable is a distance between two cities. When I run the fuzzy RDD using a specific cut-off, I obtained significant result with the value of coefficient is 7.833. What does it mean? I read a paper by Imbens and Lemieux (2007) as \"the ratio of the jump in the regression of the outcome on the covariate to the jump in the regression of the treatment indicator on the covariate\". So, how do I interpret the 7.833?Thank you.","Creater_id":124643,"Start_date":"2016-07-26 18:38:30","Question_id":225818,"Tags":["regression","econometrics","discontinuity"],"Answer_count":1,"Last_activity":"2016-08-08 12:17:47","Link":"http://stats.stackexchange.com/questions/225818/fuzzy-regression-discontinuity-design-fuzzy-rdd","Creator_reputation":21}
{"_id":{"$oid":"5837a581a05283111e4d5a1f"},"View_count":17,"Display_name":"Guilherme Duarte","Question_score":0,"Question_content":"Suppose we have individuals ( 1, 2, ...,i, ..., n) such that each of them belongs to a specific group ( 1, 2, ...,j, ..., g ) in k tests. I'm thinking about two IRT models. The first one is a common 2-dimensional model in which I'm considering only individuals ( is the parameter for individuals):y_{ik}  = - \\alpha_k + \\beta_{2k} \\theta_{1i} + \\beta_{2k} \\theta_{2i}In the second case, I'm constraining the second dimension, using the information about groups ( is the parameter for groups. So:y_{ik}  = - \\alpha_k + \\beta_{1k} \\theta_{i} + \\beta_{2k} \\mu_{j}It's worth saying that groups and individuals correlates perfectly. In other words, there's no way of changing groups.Suppose every parameter here is identified.What's the crucial difference between both models? Could it be the case that  (first model) and  (second model) estimate the same thing? Why? Could someone provide references?","Creater_id":102853,"Start_date":"2016-08-08 11:53:02","Question_id":228844,"Tags":["irt"],"Answer_count":0,"Last_activity":"2016-08-08 11:53:02","Link":"http://stats.stackexchange.com/questions/228844/irt-models-with-constrained-dimensions","Creator_reputation":157}
{"_id":{"$oid":"5837a581a05283111e4d5a21"},"View_count":208,"Display_name":"tintinthong","Question_score":3,"Question_content":"I am a little bit confused about what an orthogonal design is and how it relates to the model matrix. It appears that there are many perspectives into the definition of orthogonal inside this post but none exactly helps me understand my problem. I am seeking an explanation with an experimental design context.In Bailey 2008 pg 179, It introduces two factors G and F as orthogonal iff  the subspaces  and  are both orthogonal to each other (or  and   is orthogonal)A more intuitive theorem in the same book says (in terms of factors and levels) that F and G on the same set are orthogonal to each other iff every F-class meets every G-classall these interesections have size proportional to the product of the sizes of the relevant F-class and G-classHowever, the problem with these definitions is that it does help with 3 factors being orthogonal to each other or or when continuous covariates are included in your model. I was guessing that perhaps the model matrix when encoded can reveal something about orthogonality. However, my guess is that this is not true because It is obvious that when dummy coding the columns of the model matrix is not orthogonal to each other. So my question is, What is a general definition of an orthogonal design? Is there a more general definition of orthogonal design? Including continuous covariatesWhat are the advantages of orthogonal designs? Does the model matrix reveal anything about orthogonality?","Creater_id":121671,"Start_date":"2016-08-08 08:00:10","Question_id":228797,"Tags":["experiment-design"],"Answer_count":1,"Last_activity":"2016-08-08 11:37:08","Link":"http://stats.stackexchange.com/questions/228797/what-is-an-orthogonal-design","Creator_reputation":117}
{"_id":{"$oid":"5837a581a05283111e4d5a2d"},"View_count":59,"Display_name":"Shantanu","Question_score":2,"Question_content":"If a data set is stationary, does it mean it has no trend?Can we use ARIMA or AR models if there is no trend in the data? If there is AR term, it means that our current value is dependent on previous data, and hence it means there will be some trend as future values are dependent on previous ones. So in that scenario, we should have trend at least in our data if we want to use ARIMA or AR models. Please clarify.","Creater_id":9079,"Start_date":"2016-08-08 07:18:20","Question_id":228790,"Tags":["time-series","arima","stationarity","trend","non-stationary"],"Answer_count":2,"Last_activity":"2016-08-08 11:33:45","Link":"http://stats.stackexchange.com/questions/228790/arima-requires-stationarity-but-it-generates-trends-paradox","Creator_reputation":47}
{"_id":{"$oid":"5837a581a05283111e4d5a3a"},"View_count":130,"Display_name":"CCheckpoint","Question_score":2,"Question_content":"I would like to create a linear distributed lag model in order to do some forecast and also being able to interpret the results.Unfortunately I'm a bit confused with the process I should follow.Concept of time series is quite new for me so I'm looking for something simple.I have a variable Y that I want to express by the lags of several other variables X1,...X4. It seems that the R-package dynlm is well adapted for this kind of model.At the end, I would like to have this kind of relation :So I would like to ascertain which lags of my exogeneous variables are significant for modeling Y. I first thought using cross-correlation (ccf() in R) but after browsing on CrossValidated, it seems that this is not that simple.Indeed, all of my variables except one(X3) are not stationary. I could difference all of them but how can I then interpret the results ?Furthermore, should I also prewhiten my data? (I know there is a function prewhithen() included in TSA package).Here are my time series :    ############################################## CROSS VALIDATED ##################################################################    library(dynlm)    library(tseries)    Y\u0026lt;-c(2.39,2.29,2.54,2.53,2.57,2.59,2.58,2.64,2.79,2.78,2.81,2.79,2.38,3.09,2.94,2.91,3.15,2.93,2.83,2.92,3.18,3.08,3.10,3.13,0.91,3.28,3.72,3.89,3.97,6.00,5.84,5.66,6.35,6.26,6.14,6.04,4.28,4.55,7.78,7.12,6.43,5.93,5.32,5.26,5.77,5.65,5.52,5.05,4.56,5.21,3.66,4.01,4.11,4.19,3.87,4.06,4.14,4.12,4.15,4.37,4.58,4.32,4.11,3.83,3.66,3.58,3.34,3.41,3.61,3.55,3.51,3.25,3.09,3.14,2.80,2.92,3.09,3.07,2.89,2.93,2.97,2.92,2.83,3.01,2.75,2.60,1.17,1.52,1.80,1.69,1.76,2.30,2.13)    X1\u0026lt;-c(3.8,4.0,4.3,4.4,4.7,4.4,5.0,5.2,5.2,5.2,5.4,5.5,5.8,6.3,6.3,6.7,6.9,6.5,5.8,5.5,5.0,5.0,4.9,4.8,5.0,5.0,4.9,5.0,4.8,4.7,4.7,4.7,4.6,4.8,3.6,3.6,3.5,3.3,3.2,3.3,3.4,3.2,3.1,3.0,3.1,3.1,3.0,3.0,3.0,3.2,3.1,3.2,3.1,2.9,2.7,2.8,3.0,2.9,3.0,3.0,3.0,2.9,3.0,2.9,2.8,2.6,2.5,2.5,2.6,2.5,2.6,2.6,2.5,2.5,2.6,2.6,2.7,2.5,2.3,2.4,2.4,2.3,2.3,2.3,2.3,2.3,2.2,2.2,2.2,2.2,2.0,2.1,2.2)    X2\u0026lt;-c(NA,6.6,6.9,7.4,6.2,7.3,7.1,7.3,8.1,8.1,8.7,8.3,8.7,9.7,10.1,10.4,9.8,9.4,9.1,9.3,9.8,9.8,9.6,9.0,8.8,8.7,8.1,8.0,8.0,7.7,6.7,6.9,7.9,7.8,7.2,6.8,6.8,7.1,6.7,6.9,6.5,6.5,5.8,6.2,6.1,6.3,7.0,6.1,6.3,6.8,6.1,6.5,6.3,6.0,5.5,6.1,5.6,5.7,5.7,5.7,5.8,5.8,5.8,5.4,5.2,5.0,4.7,4.9,4.9,4.9,4.7,4.5,4.7,4.9,5.0,5.1,5.0,4.5,4.3,4.5,4.3,4.4,4.4,4.1,4.0,4.1,3.9,4.0,3.9,4.2,3.8,4.1,4.1)    X3\u0026lt;-c(NA, NA, NA, 9.7, 10.3, 9.8, 10.8, 12.0, 10.7, 12.0, 10.2, 10.7, 10.0, 10.4, 10.3, 10.9, 11.4, 12.5, 11.7, 10.9, 10.4, 9.6, 8.9, 8.2, 8.3, 8.8, 9.3, 14.1, 10.7, 10.3, 9.4, 8.8, 8.8, 10.1, 10.4, 10.0, 11.0, 11.2, 10.4, 10.3, 11.0, 11.3, 10.9, 10.6, 10.2, 12.3, 11.9, 11.1, 10.8, 10.8, 12.1, 11.6, 11.3, 11.8, 11.4, 9.8, 10.2, 12.1, 10.9, 11.4, 12.2, 11.8, 12.0, 11.3, 11.6, 10.4, 10.9, 10.4, 10.2, 11.4, 11.4, 10.6, 11.2, 11.2, 12.1, 12.2, 11.5, 10.7, 10.4, 9.8, 10.6, 11.7, 10.6, 11.0, 10.7, 11.0, 11.2, 10.2, 11.1, 12.1, 10.4, 9.9, 9.5)    X4\u0026lt;-c(2.4,2.2,3.0,2.5,2.7,2.7,2.5,3.1,4.0,2.7,3.1,2.5,2.4,3.8,2.7,2.8,4.1,1.8,2.2,3.6,5.3,2.1,3.3,3.5,0.9,5.6,7.8,5.7,4.9,30.9,3.8,3.1,16.9,4.8,4.0,4.2,4.3,4.8,14.2,5.2,3.7,3.4,1.7,4.9,9.8,4.6,4.2,0.0,4.6,5.9,0.6,5.1,4.5,4.6,1.9,5.4,4.8,4.0,4.4,6.8,4.6,4.1,3.7,3.0,3.0,3.2,1.9,3.9,5.3,3.0,3.2,0.2,3.1,3.2,2.1,3.3,3.8,2.9,1.8,3.2,3.3,2.5,1.9,5.0,2.7,2.5,-1.7,2.6,2.9,1.2,2.2,5.9,0.8)    ## Time series Creation    Yts\u0026lt;-ts(Y, start=c(1998,1), end=c(2005,9), frequency = 12)    X1ts\u0026lt;-ts(X1,start = c(1998,1),end = c(2005,9), frequency = 12)    X2ts\u0026lt;-ts(X2,start = c(1998,1),end = c(2005,9), frequency = 12)    X3ts\u0026lt;-ts(X3,start = c(1998,1),end = c(2005,9), frequency = 12)    X4ts\u0026lt;-ts(X4,start = c(1998,1),end = c(2005,9), frequency = 12)And this is a plot of my time series :Tell me if something is unclear, and sorry for my english.Any help would be much appreciated!edit : I reduced a bit my message to make it more concise :) ","Creater_id":122518,"Start_date":"2016-07-08 02:55:56","Question_id":222727,"Tags":["r","time-series","forecasting","stationarity","cross-correlation"],"Answer_count":1,"Last_activity":"2016-08-08 11:26:12","Link":"http://stats.stackexchange.com/questions/222727/identifying-lagged-effects-distributed-lag-model","Creator_reputation":11}
{"_id":{"$oid":"5837a581a05283111e4d5a47"},"View_count":35,"Display_name":"Evan","Question_score":0,"Question_content":"I have the above equation. The letter's , , , , , and f correspond to z-scores. I would like to solve this equation for the lowest sum of . EDIT: Let's also assume that the range of variable is between -3 and 3.I would then like to calculate that the odds that this event happens. For example ,one solution to the above equation is when all values = 0.786. A score that high or higher happens 22.48% of the time. If all these variables were independent then this event would happen  of the time.Yet I have regression  between all of the variables that I feel like I can use so I don't have to be overly conservative assuming complete independence. Yet I don't know how to incorporate those into the probability calculation.","Creater_id":127023,"Start_date":"2016-08-08 10:30:34","Question_id":228835,"Tags":["probability","statistical-significance","minimum"],"Answer_count":0,"Last_activity":"2016-08-08 11:23:12","Link":"http://stats.stackexchange.com/questions/228835/how-to-minimize-the-six-z-scores-in-this-linear-equation-and-then-calculate-a-pr","Creator_reputation":103}
{"_id":{"$oid":"5837a581a05283111e4d5a49"},"View_count":182,"Display_name":"John","Question_score":8,"Question_content":"If couples are seated randomly at a round table, what is the chance that no-one is seated opposite their partner?If there are four people the answer is 2/3.If there are six it is 8/15, I think.After that, my step by step method, filling in all the possibilities and ending up with a sum of various expected values, becomes pretty laborious. Intriguingly, an intuitive approach gets the right answer for 6 people in the form of (4/5) x (2/3), but I'm struggling to generalise this. Is there a neat method, leading to a formula for the case of 2n people (n couples)? ","Creater_id":60031,"Start_date":"2016-08-05 08:52:55","Question_id":228467,"Tags":["probability"],"Answer_count":2,"Last_activity":"2016-08-08 11:14:38","Link":"http://stats.stackexchange.com/questions/228467/probability-of-people-not-facing-their-partner-at-a-round-table","Creator_reputation":53}
{"_id":{"$oid":"5837a581a05283111e4d5a57"},"View_count":35,"Display_name":"Alexvonrass","Question_score":1,"Question_content":"Please help me word exactly what is wrong with using total spends (split by different media) as variables in regression analysis, with sales/store traffic or something similar as the dependant variableI work in advertising and everyone in the industry is building a lot of models like this (mostly to rationalize increasing ad spends to clients), which seems very wrong from a theoretical point of viewMy own thoughts:1) Multicollinearity makes it impossible to interpret coefficients for each media, so it would be wrong to say \"Spends in X media increase sales by -coefficient- units\"2) Endogenity between advertising and sales (for example similar seasonality due to advertisers increasing activity during high sales season)Please feel free to add to this or correct me if i'm wrong","Creater_id":126927,"Start_date":"2016-08-07 13:21:21","Question_id":228681,"Tags":["regression","causality"],"Answer_count":2,"Last_activity":"2016-08-08 10:48:51","Link":"http://stats.stackexchange.com/questions/228681/advertising-spends-as-a-variable-in-regression-is-it-wrong","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d5a65"},"View_count":112,"Display_name":"Footy","Question_score":-1,"Question_content":"favoriteIn labour economics, the return to education denotes the response of the wage of an individual to an increase in individual’s education. It is estimated with a linear regression model:where  is the individual i’s wage (in dollars),  is the individual i’s amount of years in education (in years), and  is the noise. Let us assume the coefficient  is estimated to be 13000 and the coefficient  is estimated to be 1500.What is ?","Creater_id":122959,"Start_date":"2016-08-06 08:03:03","Question_id":228567,"Tags":["regression"],"Answer_count":1,"Last_activity":"2016-08-08 10:36:10","Link":"http://stats.stackexchange.com/questions/228567/intepreting-linear-regression","Creator_reputation":9}
{"_id":{"$oid":"5837a581a05283111e4d5a72"},"View_count":23,"Display_name":"Martin Teyk","Question_score":-1,"Question_content":"I have done an online survey for my dissertation and now I am a little bit confused: I tested the following model:Variables A -----\u003e B (Variable A negatively correlates with Variable B).Moderating Variable: M on the relationship of A -----\u003e B.My results show:That the relationship between A -----\u003e B is not statistically significant (.233).The moderating variable M is statistically significant (.025).My Problem:How can I interpret these results? I am confused on how to argue about that the moderating variable has an impact on a non-significant relationship between A -----\u003e B? I mean, if the 'underlying' relationship between A ------\u003e B is not statistically significant, how can there be a moderating impact on it? The only explanation that comes to my mind is that the moderating variable M only would occur, if further research shows that the relationship between A ------\u003e B is significant (e.g. because of a larger sample size - I had only 100 participants)? Am I on the right path here?Thanks a lot!","Creater_id":126963,"Start_date":"2016-08-08 01:15:20","Question_id":228744,"Tags":["spss","effects"],"Answer_count":1,"Last_activity":"2016-08-08 10:34:07","Link":"http://stats.stackexchange.com/questions/228744/spss-moderating-variable-confusion","Creator_reputation":1}
{"_id":{"$oid":"5837a581a05283111e4d5a7f"},"View_count":40,"Display_name":"user45867","Question_score":1,"Question_content":"Our sales team is handing me a forecast with 'ranges' so to speak.To keep it simple, say that the forecast for Product A for 2017 is 100k, but the sales team says this can be from 80k-120k likely. They don't really give an exact confidence interval here of course - just judgement.For Product B, the forecast is 60k, but they say in likelihood it could range from 30k-90k. Again, no confidence intervals here.Now if you were to combine these rough estimates into an expected range, of course you'd say we 'expect' based on this judgement that the combined forecast for A and B would be 160k, the middle of the estimates. However, what would the new likely interval be? I mean if you're assuming a normal distribution for both these 'ranges' they've provided, the likelihood of 110k and 210k would be very unlikely. But how exactly would you come up with a new pessimistic/ optimistic scenario? I know the fact that these are unclear ranges may be the problem. Should we assume they are 80% or 90% confidence intervals with a normal distribution and refactor from there? Still, I'm unsure (say there are 10 products) how to come up with a new range. Perhaps we're going about this wrong. Perhaps the forecasts should be aggregated and then a range created?I'm not sure how to aggregate calculations with a highly judgmental forecast.","Creater_id":45867,"Start_date":"2016-08-08 09:00:31","Question_id":228812,"Tags":["forecasting","prediction-interval","aggregation"],"Answer_count":0,"Last_activity":"2016-08-08 10:26:47","Link":"http://stats.stackexchange.com/questions/228812/how-to-combine-prediction-intervals-forecast-ranges","Creator_reputation":46}
{"_id":{"$oid":"5837a581a05283111e4d5a81"},"View_count":22,"Display_name":"siby","Question_score":0,"Question_content":"I have developed a new kind of artificial neuron and a 2 layer network with this neuron. It was developed with the aim of learning trajectories. Is there any standardized problem that that I can teach this network which would allow me to see how good or bad my network is? Any suggestion would be very useful","Creater_id":127022,"Start_date":"2016-08-08 10:26:23","Question_id":228834,"Tags":["neural-networks"],"Answer_count":0,"Last_activity":"2016-08-08 10:26:23","Link":"http://stats.stackexchange.com/questions/228834/standard-test-problem-to-compare-performance-of-new-type-of-neural-network","Creator_reputation":72}
{"_id":{"$oid":"5837a581a05283111e4d5a83"},"View_count":5766,"Display_name":"Gyan Veda","Question_score":5,"Question_content":"I am using Python's scikit-learn to train and test a logistic regression.scikit-learn returns the regression's coefficients of the independent variables, but it does not provide the coefficients' standard errors. I need these standard errors to compute a Wald statistic for each coefficient and, in turn, compare these coefficients to each other.I have found one description of how to compute standard errors for the coefficients of a logistic regression (here), but it is somewhat difficult to follow.If you happen to know of a simple, succint explanation of how to compute these standard errors and/or can provide me with one, I'd really appreciate it! I don't mean specific code (though please feel free to post any code that might be helpful), but rather an algorithmic explanation of the steps involved.","Creater_id":34872,"Start_date":"2014-03-10 09:10:43","Question_id":89484,"Tags":["logistic","python","standard-error","regression-coefficients","scikit-learn"],"Answer_count":3,"Last_activity":"2016-08-08 10:25:45","Link":"http://stats.stackexchange.com/questions/89484/how-to-compute-the-standard-errors-of-a-logistic-regressions-coefficients","Creator_reputation":261}
{"_id":{"$oid":"5837a581a05283111e4d5a92"},"View_count":41,"Display_name":"Pinocchio","Question_score":1,"Question_content":"I wanted to use Early Stopping regularization to get the best generalization of my model trained via an iterative algorithm (e.g. some variant of SGD on a NN). I was thinking of implementing this idea as following:Run SGD for N iterationsRecord train, validation (cv) and test error for each epoch (even possible save the current model each epoch or even just the one with best cv)Then report the test error (and return model) of the epoch with the lowest cross-validation.Is this a sound way (maybe even shortcut) to implementing Early Stopping Regularization?I've heard of other methods like track K cross-validation errors, etc but under the condition of sufficient memory space and patience for large N, I thought that maybe this could be a sound method to implement the idea instead.The question is. If I train a NN for 1000 iterations and want to do early stopping. Can I just look at all the validation errors from all steps, note which one is the smallest validation error and then report the models test error relating to that step. Obviously training is only done with the train. The train data is for training, validation for choosing when to early stop and the test for reporting a test error that is not underestimated.I don't understand why the order based on time/iteration even matters honestly, it seem arbitrary.","Creater_id":37632,"Start_date":"2016-08-07 14:38:04","Question_id":228693,"Tags":["machine-learning","neural-networks","regularization","gradient-descent","conv-neural-network"],"Answer_count":1,"Last_activity":"2016-08-08 10:16:26","Link":"http://stats.stackexchange.com/questions/228693/is-reporting-the-test-error-of-the-model-with-the-smallest-validation-error-a-so","Creator_reputation":795}
{"_id":{"$oid":"5837a581a05283111e4d5a9e"},"View_count":43,"Display_name":"Giorgio Spedicato","Question_score":1,"Question_content":"I'm using mda package + caret infrastucture to perform a flexible discriminant analysis for a classification problems. I have 26 features of mixed type. I found little if any guidance on the computational time. How does it vary as a function of both predictors and dataset size?","Creater_id":6547,"Start_date":"2016-07-23 08:53:22","Question_id":225275,"Tags":["r","caret","fda"],"Answer_count":1,"Last_activity":"2016-08-08 10:07:29","Link":"http://stats.stackexchange.com/questions/225275/flexible-discriminant-analysis-computational-completixy","Creator_reputation":1124}
{"_id":{"$oid":"5837a581a05283111e4d5aab"},"View_count":35,"Display_name":"Sanjay Kumar","Question_score":0,"Question_content":"I am learning ordination by using r \"vegan\" package and trying to correlate my species abundance with their respective biomass using  metaMDS. The data is similar to dune and dune env. Distance matrix is set to bray. I have read so many research paper where author correlate nmds first and second axis with any external factor. I am trying to do same but don't know how to do this. How to extract nmds axis score and how to correlate. I also read about envfit command but get no success. Please give me any suggestion. ","Creater_id":29022,"Start_date":"2016-08-08 10:00:18","Question_id":228827,"Tags":["r"],"Answer_count":0,"Last_activity":"2016-08-08 10:00:18","Link":"http://stats.stackexchange.com/questions/228827/how-to-correlate-nmds-axis-with-environmental-variables","Creator_reputation":21}
{"_id":{"$oid":"5837a581a05283111e4d5aad"},"View_count":44,"Display_name":"Cinta","Question_score":1,"Question_content":"I have pre and post measures of the same participants, divided over a Treatment condition and a control condition (no treatment). I have calculated the diff-in-diff by hand and want to know if this is significant.Therefore, I want to perform a regression with a Treatment dummy (1= treatment, 0=no treatment), a Time dummy (1=post test, 0= prestest) and and the interaction between both. My dependent variable is a math test score. I know how to create the Treatment dummy. My question is about the Time dummy: Every participant is measured pre and post, so I have 2 columns of math score data in my spss file. I would like to know how to create the Time dummy, as every participant is measured at both times (pre and post). (I know how to create a dummy variable in spss, but I dont know what the Time dummy should look like).","Creater_id":126715,"Start_date":"2016-08-06 03:55:05","Question_id":228549,"Tags":["regression"],"Answer_count":1,"Last_activity":"2016-08-08 09:51:30","Link":"http://stats.stackexchange.com/questions/228549/how-to-create-a-pre-post-dummy","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d5aba"},"View_count":2054,"Display_name":"Ellen Marshall","Question_score":1,"Question_content":"I have two dependent variables, Abundance and Richness of moths, and 12 independent climate variables. These are Temperature, Rainfall and Sunlight, for each of the 4 seasons. How do I go about analysing this? From doing individual simple linear regression I have found significance for summer rainfall and winter temperature as factors influencing my dependent variables, but I know that this isn't very statistically viable! Is principle component analysis a suitable way of analysing this data? Are there any other multivariate techniques I could use?Thanks.","Creater_id":40889,"Start_date":"2014-03-19 07:20:06","Question_id":90568,"Tags":["regression","multivariate-analysis"],"Answer_count":1,"Last_activity":"2016-08-08 09:47:55","Link":"http://stats.stackexchange.com/questions/90568/how-to-analyse-data-with-multiple-dependent-and-independent-variables","Creator_reputation":28}
{"_id":{"$oid":"5837a581a05283111e4d5ac7"},"View_count":42,"Display_name":"anonymous","Question_score":0,"Question_content":"Is it a technique where Geometry is used to solve probabilistic problems?Is it a kind of probability which grows Geometrically when we conduct experiments?Is it a kind of distribution?I am confused. Coz, my teacher is actually lecturing by mixing up all those three aforementioned items.Also, I am not finding any book relevant to his lectures.","Creater_id":109372,"Start_date":"2016-08-08 09:04:37","Question_id":228815,"Tags":["probability","geometric-distribution","geometry","information-geometry"],"Answer_count":1,"Last_activity":"2016-08-08 09:27:43","Link":"http://stats.stackexchange.com/questions/228815/what-is-geometrical-probability","Creator_reputation":196}
{"_id":{"$oid":"5837a581a05283111e4d5ad4"},"View_count":76,"Display_name":"Sam","Question_score":0,"Question_content":"I have 100 customers, 40 are Females and 60 are Males. My marketing team has created 2 separate campaigns with different offers for both groups. We create an A/B test for each group to study campaign lift. Below are the splits (A = No offer | B = offer depending on the group)Counts :     Group  |  A  |  B  | Total  -----------------------------   Females |  10 |  30 |  40   Males   |  20 |  40 |  60  -----------------------------   Total   |  30 |  70 | 100Here are the results,  Results : 5 |  3 |  3.6| $5.3|  47%What's funny is the lift of Total-A vs. Total-B is more than individual groups, and I realise that this has got something to do with the proportion of A:B across groups. (Females = 1:3, Males = 1:2)My question is what the best way to solve for this discrepancy ?","Creater_id":125448,"Start_date":"2016-08-02 21:23:08","Question_id":226994,"Tags":["sampling","proportion","marketing"],"Answer_count":1,"Last_activity":"2016-08-08 09:19:43","Link":"http://stats.stackexchange.com/questions/226994/how-to-deal-with-unequal-proportions-in-an-a-b-test","Creator_reputation":1}
{"_id":{"$oid":"5837a581a05283111e4d5ae1"},"View_count":54,"Display_name":"jojo","Question_score":3,"Question_content":"I am wondering if CNN are a right tool for classification of human vital data. My data base consists of vector measurements and has a dimension of \\mathsf{3000 \\times 1 \\times  1 \\times  numberOfSamples}My concern is that data units are so low dimensional that resourcing to deep learning techniques is pointless. On the other hand our team tried before many other machine learning alorithms but without success. Maybe somebody wants to share their opinion and possibly a suggestion regarding network architecture and hyper parameters.","Creater_id":126992,"Start_date":"2016-08-08 06:00:52","Question_id":228780,"Tags":["dataset","conv-neural-network","convolution"],"Answer_count":1,"Last_activity":"2016-08-08 09:17:33","Link":"http://stats.stackexchange.com/questions/228780/convolutional-neural-networks-trained-with-vital-data-eg-ekg","Creator_reputation":16}
{"_id":{"$oid":"5837a581a05283111e4d5aee"},"View_count":39,"Display_name":"Colin Talbert","Question_score":1,"Question_content":"I'm trying to calculate a KDE bandwidth using the least-squares cross validation (LSCV) method.  I'm using the statsmodels KDEMultivariate function in Python. But I'm running into an issue where the result is very dependent on the data supplied and sometimes returns a bandwidth that does not make sense.  By moving one of the input points slightly it returns a reasonable answer.  Is this an error in statsmodels or to be expected with this technique?  If the error is inherent in the technique can it be handled with jittering, normalization or some other data cleanup? If the error is in statsmodels is there alternate statistical library that can calculate a bandwidth using the LSCV method?import statsmodels.api as sm#this is the original datadf_breaks = pd.DataFrame([[  538139.47293084,  3506243.03653242],                         [  538150.09004195,  3506237.73434416],                         [  538080.50079698,  3506151.83736346],                         [  538124.70281065,  3506254.52157404],                         [  538136.83381139,  3506245.77673011],                         [  538155.19176266,  3506217.94499408],                         [  538157.14459963,  3506216.01139807],                         [  538120.47723684,  3506250.05577059],                         [  538094.69172001,  3506211.3862365 ],                         [  538140.71494773,  3506238.02450588],                         [  538150.71773573,  3506242.01951111]])kde = sm.nonparametric.KDEMultivariate(df_breaks, \"cc\", \"cv_ls\")print \"kde.bw\", kde.bwkde.bw [  1.87127711e+01   1.00000000e-10]import statsmodels.api as sm#this is the original data with the y value of the last moved slightlydf_works = pd.DataFrame([[  538139.47293084,  3506243.03653242],                         [  538150.09004195,  3506237.73434416],                         [  538080.50079698,  3506151.83736346],                         [  538124.70281065,  3506254.52157404],                         [  538136.83381139,  3506245.77673011],                         [  538155.19176266,  3506217.94499408],                         [  538157.14459963,  3506216.01139807],                         [  538120.47723684,  3506250.05577059],                         [  538094.69172001,  3506211.3862365 ],                         [  538140.71494773,  3506238.02450588],                         [  538150.71773573,  3506242.5]])kde = sm.nonparametric.KDEMultivariate(df_works, \"cc\", \"cv_ls\")print \"kde.bw\", kde.bwkde.bw [ 9.61735219  5.29485534]Thanks for any insight,Colin","Creater_id":127009,"Start_date":"2016-08-08 08:59:13","Question_id":228811,"Tags":["python","kernel-smoothing","statsmodels"],"Answer_count":0,"Last_activity":"2016-08-08 08:59:13","Link":"http://stats.stackexchange.com/questions/228811/problem-with-least-squares-cross-validation-in-statsmodels","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d5af0"},"View_count":13,"Display_name":"RDK","Question_score":0,"Question_content":"I have a dataset of photographs of forms (say 1000 images). Since the forms belong to about 50 different layouts (i.e. templates), I expect the corresponding images to be clustered. I want to visualize these clusters prior to performing any classification or advanced processing. My approach is to compute a 64-bit hash for each image and reduce to two dimensions before plotting. Conceptually, hash(\"img001.jpg\") = \"0100101...1\" (64 bits). When applied on all 1000 images, this yields a 1000 x 64 matrix. However, I have doubts about PCA being the right tool for the dimensionality reduction step. PCA seems appropriate for quantitative-valued matrices with Euclidean distance metric. For hashes, the proper distance metric is different (Hamming distance, for instance). As a result, I'm not sure if the traditional PCA-based visualization would make sense.What is a more appropriate dimensionality-reduction approach for bit strings with Hamming-like similarity measure? More generally, is there a better approach to visualize clusters of images than my current hash -\u0026gt; dimensionality reduction -\u0026gt; visualization approach?I've researched (1) Correspondence Analysis and (2) Nonlinear PCA but am yet to find an example applying it to hash-like objects.  ","Creater_id":92277,"Start_date":"2016-08-08 08:57:25","Question_id":228810,"Tags":["pca","dimensionality-reduction"],"Answer_count":0,"Last_activity":"2016-08-08 08:57:25","Link":"http://stats.stackexchange.com/questions/228810/what-is-an-appropriate-dimensionality-reduction-approach-for-visualization-of-im","Creator_reputation":101}
{"_id":{"$oid":"5837a581a05283111e4d5af2"},"View_count":190,"Display_name":"user104051","Question_score":0,"Question_content":"I am not able to understand the difference between the joint density function and density function for a random variable , where  are uniform rvs in .I think joint density in this case is  (reference)Likewise the density function  is defined as convolution of  and  (reference: page 8)Could someone please explain the difference between the two?Thanks, RG","Creater_id":104051,"Start_date":"2016-02-13 14:17:49","Question_id":195433,"Tags":["pdf"],"Answer_count":2,"Last_activity":"2016-08-08 08:56:44","Link":"http://stats.stackexchange.com/questions/195433/difference-between-joint-density-and-density-function-of-sum-of-two-independent","Creator_reputation":9}
{"_id":{"$oid":"5837a581a05283111e4d5b00"},"View_count":7,"Display_name":"Winterflags","Question_score":0,"Question_content":"Let's assume we have these results from a Multiple Regression analysis done in Excel.I want to understand how the Standard Error of the Coefficient (the column under cell C16) is calculated, in order to use it for manual calculation.My study material lays out the following formulas:I'm trying to figure out if the Excel output for the coefficients correspond to  or  above – or something else.","Creater_id":102450,"Start_date":"2016-08-08 08:49:07","Question_id":228809,"Tags":["regression","multiple-regression","excel"],"Answer_count":0,"Last_activity":"2016-08-08 08:54:43","Link":"http://stats.stackexchange.com/questions/228809/excel-how-is-the-standard-error-of-the-coefficient-calculated-in-regression-ana","Creator_reputation":157}
{"_id":{"$oid":"5837a581a05283111e4d5b02"},"View_count":138,"Display_name":"Ereck","Question_score":4,"Question_content":"I would like to apply lars algorithm to some datadset.First, I fitted the model to the training set and then examined it on test set.My questions:1- After I used cross validation \"cv.lars\" I dont know how to choose the minimum cross validation error in order to choose best model. while it is clear when I used glmnet by writing in R program cv\\lambda$) based on cross-validation.This is the cross validation plot 2- I also plotted lars and lasso, but i did not see any differences. Could you clarify the differences between them please?3- By using glmnet function, I can plot lambda values on the x-axis. Does this work with lars function?4-How to calculate the mean squared error on the test set?Thanks in advance. ","Creater_id":123446,"Start_date":"2016-08-06 08:06:00","Question_id":228568,"Tags":["cross-validation","lasso","lars"],"Answer_count":1,"Last_activity":"2016-08-08 08:52:30","Link":"http://stats.stackexchange.com/questions/228568/lars-vs-lasso-and-cross-validation","Creator_reputation":62}
{"_id":{"$oid":"5837a581a05283111e4d5b0f"},"View_count":26,"Display_name":"user2253546","Question_score":0,"Question_content":"Suppose I am measuring 3 variables over time in 5 minute intervals. These variables are measuring the same thing, but using different metrics. My goal is to determine which one of these metrics is the best. When “unusual” behavior occurs one (or many) of the signals catches it, but the others don’t. Does there exist a statistical way to quantify how well one signal does compared to others based on the other’s signals inability to catch certain “unusual” behavior?I could just do a count of the number of “unusual” events that are caught by each signal and choose the one with the greatest output, but what if my sample of time frame does not contain data that is representative of what might happen in the future? As you can see, I am at a very rudimentary level of this problem and some suggestions/ideas would be greatly appreciated!","Creater_id":95537,"Start_date":"2016-08-04 22:00:10","Question_id":228382,"Tags":["time-series","anomaly-detection"],"Answer_count":1,"Last_activity":"2016-08-08 08:38:27","Link":"http://stats.stackexchange.com/questions/228382/comparing-metrics","Creator_reputation":37}
{"_id":{"$oid":"5837a581a05283111e4d5b1c"},"View_count":51,"Display_name":"lindiswtf","Question_score":0,"Question_content":"I have made a model which is supposed to classify the trend of a stock index as an \"up day\" (=1) or a \"no change\"/\"down day\"(=0), where I have coded an \"up day\" as when the percent change for the index today is \u003e 0. The model has been trained and validated on data where I know if the index has been a 1 or a 0.However, I want to apply my model to days where I don't know the direction of today's market. How would I go about solving this? Any advice is much appreciated :) Also, first question on this forum (have been a reader for some time, finally took the step right? :))My model looks like the following:direction ~ Bo+B1x1+...+B19x19, where direction = 1 if index \u003e 0, 0 otherwiseB1 to B5 are lagged variables of the index I want to predict and the rest is variables with closing time before or after the index of interest.Since I don't know the direction of the index today, I don't have anything to predict(?)","Creater_id":126936,"Start_date":"2016-08-07 15:31:13","Question_id":228699,"Tags":["regression","machine-learning","logistic"],"Answer_count":1,"Last_activity":"2016-08-08 08:24:21","Link":"http://stats.stackexchange.com/questions/228699/classification-using-logistic-regression-on-stock-data","Creator_reputation":4}
{"_id":{"$oid":"5837a581a05283111e4d5b29"},"View_count":231,"Display_name":"Neil Aronson","Question_score":0,"Question_content":"I've been learning about Naive Bayes classifiers using the nltk package in Python. I'm working on a gender classification model. I have some labeled data for names with male/female probabilities, and to create the model I used a 80:20 split between training and testing sets. I understand the importance of keeping these sets separate when you are optimizing your model, but once you've determined the features you want to include, doesn't it make sense to shift all of your existing labeled data into the training set when you're actually implementing the model on new data? My intuition is that this way when I apply the model to new, unseen names in the context of a real-world application, my model will have been trained on a larger data set. Is this correct, or are you supposed to keep your split and use only part of the data for training even after you've settled on the features you want to include? If you do maintain the split, do you have to always use the same exact data in the training set or can you shuffle which data is in training and testing sets each time you run the model? (My intuition here is that the training set should stay fixed, but not sure)","Creater_id":122578,"Start_date":"2016-08-08 08:11:32","Question_id":228801,"Tags":["machine-learning","naive-bayes"],"Answer_count":1,"Last_activity":"2016-08-08 08:21:07","Link":"http://stats.stackexchange.com/questions/228801/test-training-data-set-split-for-naive-bayes-classifier-after-model-finalized","Creator_reputation":25}
{"_id":{"$oid":"5837a581a05283111e4d5b36"},"View_count":166,"Display_name":"vaxent","Question_score":3,"Question_content":"If I am doing a hypothesis test with a sample of 100 from a population of 1,000 it would be more precise than doing a test with a sample of 100 from a population of 100,000.  How can I show this? Is it the thing that is called \"power of the test\"? Where can I read some easy-to-understand guides about it and how to calculate it for my hypothesis tests?Thank you all in advance!","Creater_id":76577,"Start_date":"2015-05-07 15:40:44","Question_id":151336,"Tags":["hypothesis-testing","power-analysis"],"Answer_count":1,"Last_activity":"2016-08-08 08:14:20","Link":"http://stats.stackexchange.com/questions/151336/a-question-on-accuracy-of-hypothesis-testing","Creator_reputation":37}
{"_id":{"$oid":"5837a581a05283111e4d5b43"},"View_count":28,"Display_name":"Eric","Question_score":1,"Question_content":"I'd like to use association rules with R. I've got a file in a transactional format (a Customer who buys 5 items generates 5 rows).I've got 3 fields in my file: CustomerId, Products bought and Date of the transaction.To use apriori() algo, do I need to restructure my file? If I import it as a data frame, it doesn't work in apriori().Which format does it have to be used in apriori() algo?Thanks,Eric","Creater_id":126734,"Start_date":"2016-08-05 04:22:01","Question_id":228424,"Tags":["r","association-rules"],"Answer_count":0,"Last_activity":"2016-08-08 08:04:50","Link":"http://stats.stackexchange.com/questions/228424/association-rules-format-data-source","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d5b45"},"View_count":111,"Display_name":"Zailei Chen","Question_score":9,"Question_content":"I am trying to figure out the distribution of(n-1) \\sum_{i=1}^n Z_i^2 - \\left( \\sum_{i=1}^n Z_i \\right)^2 \\qquad (*)where , i.i.d.  I know that, taking each of the terms separately,\\sum_{i=1}^n Z_i^2 \\sim \\chi^2(n)and \\frac{1}{n}\\left( \\sum_{i=1}^n Z_i \\right)^2 \\sim \\chi^2(1).But I am unsure about the distribution of (*)","Creater_id":126823,"Start_date":"2016-08-06 06:07:21","Question_id":228558,"Tags":["probability","distributions","normal-distribution"],"Answer_count":1,"Last_activity":"2016-08-08 08:04:43","Link":"http://stats.stackexchange.com/questions/228558/distribution-of-quadratic-form-of-normals","Creator_reputation":46}
{"_id":{"$oid":"5837a581a05283111e4d5b52"},"View_count":59,"Display_name":"J.Gourlay","Question_score":1,"Question_content":"There are a lot of statistic tests to investigate if our OLS assumptions are right or wrong. My question : How to know if a test is better than another ?This study about normal distribution gave us an answer to choose, the power of tests.Doing my research, I've read something on tests for heteroscedasticity. There could be différences between results of test which are trying to answer the same question so I need to give me rules on how to choose one of them.Is the criterion of power the only way to compare between tests ? Assuming that the Shapiro-Wilk got the higher power, is it the best to test normality in every case ?EditSome readings later, I got to say that @Peter Flom was right on his opinon about \"There is no single best test of the assumption of normality of residuals in a regression\" because it depends on the type of data / the size etc... But it would be pretty intresting for me (as somebody who's working on econometrics) to make a guide for the following tests for OLS assumptions :Normality of residuals, Heteroscedasticity, Multicolinearity ...I'll work on it to get an other method than just using robust statistics","Creater_id":122222,"Start_date":"2016-07-29 02:27:59","Question_id":226245,"Tags":["hypothesis-testing","multiple-regression","least-squares"],"Answer_count":1,"Last_activity":"2016-08-08 08:01:49","Link":"http://stats.stackexchange.com/questions/226245/how-to-compare-statistical-tests","Creator_reputation":6}
{"_id":{"$oid":"5837a581a05283111e4d5b5e"},"View_count":54,"Display_name":"Alex","Question_score":0,"Question_content":"I have a data matrix M, and it's correlation matrix .  If the correlation matrix () has higher magnitude elements off the diagonal, does this imply anything about the rank of the data matrix ?","Creater_id":96019,"Start_date":"2016-08-08 07:08:08","Question_id":228788,"Tags":["correlation"],"Answer_count":1,"Last_activity":"2016-08-08 07:49:30","Link":"http://stats.stackexchange.com/questions/228788/off-diagonal-elements-of-correlation-and-data-rank","Creator_reputation":33}
{"_id":{"$oid":"5837a581a05283111e4d5b6b"},"View_count":5162,"Display_name":"tSchema","Question_score":7,"Question_content":"Random forests are well known to perform fairly well on a variety of tasks and have been referred to as the leatherman of learning methods. Are there any types of problems or specific conditions in which one should avoid using a random forest?","Creater_id":17130,"Start_date":"2014-08-16 13:37:30","Question_id":112148,"Tags":["machine-learning","classification","random-forest"],"Answer_count":3,"Last_activity":"2016-08-08 07:27:48","Link":"http://stats.stackexchange.com/questions/112148/when-to-avoid-random-forest","Creator_reputation":90}
{"_id":{"$oid":"5837a582a05283111e4d5b7a"},"View_count":305,"Display_name":"JPmiaou","Question_score":10,"Question_content":"On baby-naming forums, prospective parents repeat some version of their Fear of Jennifer all the time: \"I don't want my child to be one of 5 in his class with his name.\" Thing is, no name comes even close to that sort of popularity any more, and even at the height of the Jennifer craze, you didn't get five of them in a class. I would like some sort of answer for these parents of just how unlikely such a coincidence of name repetition would be. Using the Social Security Administration's extensive baby-name data (https://www.ssa.gov/oact/babynames/limits.html), can someone tell me how to figure out the chances of an elementary school class in the U.S. having five children with the same name? (For simplicity, by \"same name\" I mean same spelling, and by \"school class\" I mean all the kids were born in the same year.) I'm not specifying a class size, but it should definitely be greater than 4. :-)","Creater_id":125542,"Start_date":"2016-08-03 12:22:44","Question_id":227139,"Tags":["probability","combinatorics"],"Answer_count":2,"Last_activity":"2016-08-08 07:17:13","Link":"http://stats.stackexchange.com/questions/227139/probability-of-five-children-in-the-same-class-having-the-same-given-name","Creator_reputation":153}
{"_id":{"$oid":"5837a582a05283111e4d5b88"},"View_count":21,"Display_name":"user90772","Question_score":0,"Question_content":"Assume 3 datasets (experiments) for the same population of a discrete random variable, dataset 1 has observed values {1, 2, 3, NA} values, dataset 2 {1, 2, 3, 4, 5, NA} and dataset 3 {1, 2, 3, 4, 5, 6, 7, NA}. These are very large datasets (in the order of thousands) but with fewer samples in higher values (e.g. 6,7 here). I would like to calculate the p.m.f. and the c.d.f. by aggregating the datasets. NA values are due to some unknown uncensoring mechanism (some of the observations are right-censored, some just lost because of technical issues independently of the observation itself).I tried to get the empirical probabilities for each value by just aggregating all possible datasets, i.e. for value 1 all datasets, for value 5 the last two and for value 6 only the third dataset. My problem is that this does not give a valid p.m.f. (the sum is \u003e 1). Is there any way to normalize or another way to combine these datasets?","Creater_id":91239,"Start_date":"2016-08-08 05:41:35","Question_id":228778,"Tags":["probability","distributions","cdf","ecdf","probability-calculus"],"Answer_count":1,"Last_activity":"2016-08-08 06:43:10","Link":"http://stats.stackexchange.com/questions/228778/probability-mass-function-from-multiple-datasets-with-different-ranges","Creator_reputation":66}
{"_id":{"$oid":"5837a582a05283111e4d5b95"},"View_count":22,"Display_name":"Hasan Tekin","Question_score":0,"Question_content":"My sample is balanced panel data and includes 6,071 firm-year observations for the period 2002-2014.I test the book leverage (total debt/total asset) by using lagged factors as follows: 9 firm-specific factors, 2 industry-specific factors, 3 macro-specific factors (inflation, interest rate, and GDP growth), and 3 dummies. At the same time I use 12 year dummies for 13 year analysis.Can I use only time dummies instead of macro-specific dummies or opposite? I cannot find any empirical evidence about this usage.","Creater_id":126979,"Start_date":"2016-08-08 03:53:18","Question_id":228768,"Tags":["binary-data","finance"],"Answer_count":1,"Last_activity":"2016-08-08 06:40:27","Link":"http://stats.stackexchange.com/questions/228768/year-dummies-versus-macro-specific-factors","Creator_reputation":1}
{"_id":{"$oid":"5837a582a05283111e4d5ba2"},"View_count":75,"Display_name":"StatsStudent","Question_score":4,"Question_content":"Here is the motivation for my question. I have a sensor that reports data to me. The occurrence of the reports from the sensor follows a Poisson process (so, obviously, the inter-event times are exponential). I assume a constant event rate .The device, however, can fail. Let  be the failure time. After failure, the event occurrences are not reported. So what I observe are event times  that have occurred on some interval . I do not have prior information. So this is just a standard Poisson \"set-up\" except that I don't know the length of the interval over which the events can be observed. I want to estimate both the rate  and the interval length .I have tried writing down the equations for the maximum likelihood estimates for  and , but I am finding that they have no solution. (Maybe I have made a mistake.)It seems that this should be a simple enough standard problem. I have not been able to find an answer (in part because searches that involve the term \"interval\" return large numbers of pages/answers about confidence intervals). Any help or pointers to references would be greatly appreciated.","Creater_id":126906,"Start_date":"2016-08-07 09:04:26","Question_id":228661,"Tags":["poisson","poisson-process"],"Answer_count":1,"Last_activity":"2016-08-08 06:13:36","Link":"http://stats.stackexchange.com/questions/228661/poisson-distribution-estimating-rate-parameter-and-the-interval-length","Creator_reputation":23}
{"_id":{"$oid":"5837a582a05283111e4d5baf"},"View_count":3291,"Display_name":"Tal Galili","Question_score":57,"Question_content":"My question in the title is self explanatory, but I would like to give it some context.The ASA released a statement earlier this week “on p-values: context, process, and purpose”, outlining various common misconceptions of the p-value, and urging caution in not using it without context and thought (which could be said just about any statistical method, really).In response to the ASA, professor Matloff wrote a blog post titled: After 150 Years, the ASA Says No to p-values. Then professor Benjamini (and I) wrote a response post titled It’s not the p-values’ fault – reflections on the recent ASA statement. In response to it professor Matloff asked in a followup post:  What I would like to see [... is] — a good, convincing example  in which p-values are useful. That really has to be the bottom line.To quote his two major arguments against the usefulness of the -value:  With large samples, significance tests pounce on tiny, unimportant departures from the null hypothesis.  Almost no null hypotheses are true in the real world, so performing a significance test on them is absurd and bizarre.I am very interested in what other crossvalidated community members think of this question/arguments, and of what may constitute a good response to it.","Creater_id":253,"Start_date":"2016-03-11 04:44:59","Question_id":201146,"Tags":["hypothesis-testing","bayesian","p-value","inference","frequentist"],"Answer_count":8,"Last_activity":"2016-08-08 06:12:03","Link":"http://stats.stackexchange.com/questions/201146/what-is-a-good-convincing-example-in-which-p-values-are-useful","Creator_reputation":7696}
{"_id":{"$oid":"5837a582a05283111e4d5bc3"},"View_count":83,"Display_name":"BobbyJohnsonOG","Question_score":1,"Question_content":"I want to find the best model process for a machine learning pipeline. In other words, normalize  feature select  test model performance. For example, let's say I want to try Ridge, Lasso, and Elastic Net regression and I am doing normalization, feature selection, and a cross-validated hyperparameter search for all models. I want to pick the best out of the three.Does it theoretically make sense to run cross-validation where I run the entire pipeline on each of the left out folds? In SKLearn, something like this:models = [Ridge(), Lasso(), ElasticNet()]for model in models:    pipe = Pipeline([('scaling', scaler),                      ('feature_selection', selectorCV),                      ('param_searcm', gridsearchCV)])    scores.append(cross_val_score(pipe, X, y))# get the best model pipeline from cross_val_score, fit on all my data, #  and whoop there is my best model","Creater_id":125252,"Start_date":"2016-08-08 05:08:25","Question_id":228774,"Tags":["feature-selection","python","model-selection","model","scikit-learn"],"Answer_count":1,"Last_activity":"2016-08-08 05:33:30","Link":"http://stats.stackexchange.com/questions/228774/cross-validation-of-a-machine-learning-pipeline","Creator_reputation":34}
{"_id":{"$oid":"5837a582a05283111e4d5bcf"},"View_count":22,"Display_name":"Aure","Question_score":1,"Question_content":"I am doing a research where the dependent variable is nominal (composed of 5 groups). Four of these groups were experimentally manipulated and the last one is a control. I carried out a Dunnett test in order to see if my control group were different from the others. However, the result is not significant. My question is: how do I interpret such result? Does this mean that my control group is not a reliable for comparison? Should I continue to compare my result with this group or should it be discarded and I only work with the four other groups instead (that are different from one another)?Thank you for your answer!Aure ","Creater_id":126986,"Start_date":"2016-08-08 05:08:55","Question_id":228775,"Tags":["anova","post-hoc","control"],"Answer_count":0,"Last_activity":"2016-08-08 05:08:55","Link":"http://stats.stackexchange.com/questions/228775/how-to-interpret-non-significant-dunnett-test","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5bd1"},"View_count":33,"Display_name":"user3676846","Question_score":0,"Question_content":"I am searching for a topic for my masters thesis and came upon risk based pricing. Is this a good topic to explore using machine learning (ML), in the sense that is there scope for a masters student to make some contribution to the topic? Would data required for analysis be easily available? What should I start reading to get a fair idea of the topic? I have no clue where to get started and what ML approach to take. ","Creater_id":123038,"Start_date":"2016-08-08 04:46:15","Question_id":228772,"Tags":["machine-learning","svm","finance","risk"],"Answer_count":0,"Last_activity":"2016-08-08 04:46:15","Link":"http://stats.stackexchange.com/questions/228772/what-is-a-good-way-to-start-for-a-project-on-risk-based-pricing-using-machine-le","Creator_reputation":21}
{"_id":{"$oid":"5837a582a05283111e4d5bd3"},"View_count":2201,"Display_name":"user34790","Question_score":1,"Question_content":"I am trying to use sequentialfs to do some feature selection in matlab. I have huge dimensional data of 22215 features. When I tried to use sequentialfs with svm as classifier so that it selects the best subset of features, it just keeps on running, probably its because of the huge number of dimensions. However, weka does the same thing very quickly. Weka has wrapper filters and it does it so quickly. What sort of heuristic does it use? Even though I tried with 5000 features in weka since it was not taking 22215 features, it gave me results quickly with wrapper filters. What should I do with sequentialfs in matlabThis is the command I am using in matlabc = cvpartition(yS1,'k',12);opts = statset('display','iter');[fs, history] = sequentialfs(@SVM_class_fun, X, yS1,'cv', c, 'options', opts);SVM_class_funfunction err = SVM_class_fun(xTrain, yTrain, xTest, yTest)  model = svmtrain(xTrain, yTrain);  err = sum(svmclassify(model, xTest) ~= yTest);endHere the dimension of X is 100x22215 where I have 100 examples each of dimension 22215.","Creater_id":12329,"Start_date":"2013-03-22 15:23:48","Question_id":53042,"Tags":["matlab","feature-selection"],"Answer_count":2,"Last_activity":"2016-08-08 04:20:29","Link":"http://stats.stackexchange.com/questions/53042/issues-with-feature-selection-in-matlab","Creator_reputation":1463}
{"_id":{"$oid":"5837a582a05283111e4d5be1"},"View_count":48,"Display_name":"Annamarie","Question_score":1,"Question_content":"Do I have to eliminate variables that are highly correlated before doing an exploratory factor analysis, like it has been discussed for PCA already here?To specify, some items of my data are highly correlated r = 0.8, some items stem from a similar/partially same test [Example: Persons had to remember 20 words, they had to repeat them directly after (one item) and many minutes after (second item).] Even though this should capture different cognitive dimensions (working memory and short term memory), they are of course highly correlated. Can I use both such highly correlated items as an exploratory factor analysis? (and yes, they do load highly on the same factor). Is there a cutoff for a correlation between items that is ok?","Creater_id":73581,"Start_date":"2016-08-08 03:44:29","Question_id":228766,"Tags":["correlation","factor-analysis"],"Answer_count":1,"Last_activity":"2016-08-08 04:18:11","Link":"http://stats.stackexchange.com/questions/228766/highly-correlated-variables-in-exploratory-factor-analysis","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5bee"},"View_count":42,"Display_name":"Jhaltiga68","Question_score":0,"Question_content":"I am looking for the correct formula to convert IRT discrimination parameters obtained from a logit-link model to a correlation metric. Because of the logit scaling factor (1.702), I am a bit unsure as to the correct formula.In the case of a probit model it is simply , where  is the discrimination parameter. But how precisely does this formula change for the logit-link discrimination conversion?","Creater_id":101183,"Start_date":"2016-08-07 00:34:44","Question_id":228629,"Tags":["irt"],"Answer_count":1,"Last_activity":"2016-08-08 04:03:03","Link":"http://stats.stackexchange.com/questions/228629/conversion-of-irt-logit-discrimination-parameter-to-factor-loading-metric","Creator_reputation":68}
{"_id":{"$oid":"5837a582a05283111e4d5bfb"},"View_count":8160,"Display_name":"luciano","Question_score":7,"Question_content":"I have just read a paper in which the authors carried out a multiple regression with two predictors. The overall r-squared value was 0.65. They provided a table which split the r-squared between the two predictors. The table looked like this:            rsquared beta    df pvaluewhole model     0.65   NA  2, 9  0.008predictor 1     0.38 1.01 1, 10  0.002predictor 2     0.27 0.65 1, 10  0.030In this model, ran in R using the mtcars dataset, the overall r-squared value is 0.76.summary(lm(mpg ~ drat + wt, mtcars))Call:lm(formula = mpg ~ drat + wt, data = mtcars)Residuals:    Min      1Q  Median      3Q     Max -5.4159 -2.0452  0.0136  1.7704  6.7466 Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)   30.290      7.318   4.139 0.000274 ***drat           1.442      1.459   0.989 0.330854    wt            -4.783      0.797  -6.001 1.59e-06 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 3.047 on 29 degrees of freedomMultiple R-squared:  0.7609,    Adjusted R-squared:  0.7444 F-statistic: 46.14 on 2 and 29 DF,  p-value: 9.761e-10How can I split the r-squared value between the two predictor variables?","Creater_id":12492,"Start_date":"2013-06-04 10:06:55","Question_id":60872,"Tags":["r","multiple-regression","r-squared","variance-decomposition"],"Answer_count":3,"Last_activity":"2016-08-08 03:32:32","Link":"http://stats.stackexchange.com/questions/60872/how-to-split-r-squared-between-predictor-variables-in-multiple-regression","Creator_reputation":3091}
{"_id":{"$oid":"5837a582a05283111e4d5c0a"},"View_count":52,"Display_name":"S. Sebastian","Question_score":2,"Question_content":"I am currently trying to test a moderated mediation model with nested data and repeated measures. My independent, moderator, mediator, and dependent variables are on the individual level. I used repeated measures and collected the data for all variables from individual respondents on 6 different points in time. These individual respondents were nested in groups. I had too few respondents to examine time series per individual and therefore decided to make a cross-sectional data file. This file is structured as following (considering two hypothetical respondents residing in different groups):Respondent# - Time - Group 1 -            1 -     11 -            2 -     11 -            3 -     11 -            4 -     11 -            5 -     11 -            6 -     12 -            1 -     22 -            2 -     22 -            3 -     22 -            4 -     22 -            5 -     2 I am wondering if it is statistically possible to conduct an individual-level moderated mediation that takes nesting in groups/time into account, using this data. I could not find any resources for this type of analysis (I did find a syntax for testing a multilevel moderated mediation with a level 2 moderator. However, my model includes a level 1 moderator). Also, if you have experience with this sort of analysis, I would greatly appreciate if you could share the syntax (e.g., for Mplus) you used. Your help is very welcome!","Creater_id":126960,"Start_date":"2016-08-08 01:33:42","Question_id":228749,"Tags":["repeated-measures","multilevel-analysis","mediation","moderation","mplus"],"Answer_count":0,"Last_activity":"2016-08-08 03:04:07","Link":"http://stats.stackexchange.com/questions/228749/multilevel-moderation-with-nested-data-and-repeated-measures","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5c0c"},"View_count":29,"Display_name":"greenglass","Question_score":1,"Question_content":"I have read through:http://robjhyndman.com/hyndsight/smape/ and https://www.otexts.org/fpp/2/5and a lot ofHyndman, R. J. and Koehler, A. B. (2006) ‘Another look at measures of forecast accuracy’, International journal of forecasting, 22(4), pp. 679–688.Having read  those, I understand the reasons MASE exists. I also believe I understand how it works in the examples given, but the examples tend to be for similar problems in which the training data and test data are consecutive. In my case I have a year of training data, and I want to test a model based on the data using a different year which does not directly follow.As I am not attempting to forecast one step ahead, it does not seem appropriate to me to use  as the naive method.Am I correct in thinking it is inappropriate?If so, what would be appropriate?One thought was that the denominator be the value recorded at that point in the training year: would that be more appropriate, or appropriate at all?","Creater_id":126969,"Start_date":"2016-08-08 02:26:57","Question_id":228757,"Tags":["forecasting","error","mase"],"Answer_count":0,"Last_activity":"2016-08-08 02:52:46","Link":"http://stats.stackexchange.com/questions/228757/is-mase-mean-absolute-scaled-error-and-the-usually-described-denominator-appro","Creator_reputation":106}
{"_id":{"$oid":"5837a582a05283111e4d5c0e"},"View_count":32,"Display_name":"erensezener","Question_score":0,"Question_content":"I want to solve a simple 1D linear regression problem:such that  and .How can I solve this problem? Could this be framed as a linear programming problem?","Creater_id":64720,"Start_date":"2016-08-08 01:57:45","Question_id":228750,"Tags":["regression","linear-model","constrained-regression"],"Answer_count":1,"Last_activity":"2016-08-08 02:34:47","Link":"http://stats.stackexchange.com/questions/228750/1d-linear-regression-with-inequality-constraint","Creator_reputation":26}
{"_id":{"$oid":"5837a582a05283111e4d5c1b"},"View_count":33,"Display_name":"J. Ferreira","Question_score":0,"Question_content":"I have three groups with different sample sizes, with normally distributed variables, however the levene's test is significant. Can I still perform an ANOVA test? I have analysed the variance also through the formulae that indicates that we should divide the variance of the group with highest variance for the variance of the group with the lowest variance and if the results is inferior to 2 would can still use a ANOVA. All the results are above 2, so I definitely do not have variance homogeneity.Is it ok if I use a non-parametric test instead?","Creater_id":121826,"Start_date":"2016-08-08 01:16:59","Question_id":228745,"Tags":["statistical-significance","anova","nonparametric","parametric","levenes-test"],"Answer_count":0,"Last_activity":"2016-08-08 02:33:33","Link":"http://stats.stackexchange.com/questions/228745/variance-significant-levenes-test","Creator_reputation":75}
{"_id":{"$oid":"5837a582a05283111e4d5c1d"},"View_count":59,"Display_name":"Lin Ma","Question_score":1,"Question_content":"Not sure if I read the density diagram correctly from R, I think it means overall, most a and b happens in the dark red area, and for specific a values, for example, if a is 0.1, most b values are in the small red circle area I drawn, and when a value is 0.2, most b values are in the bigger red ellipse area I drawn?Post the density diagram and sample code,b\u0026lt;-log10(rgamma(1000,6,3))a\u0026lt;-log10((rweibull(1000,8,2)))density\u0026lt;-kde2d(a,b,n=100)filled.contour(density,color.palette=colorRampPalette(c('white','blue','yellow','red','darkred')))","Creater_id":18254,"Start_date":"2016-08-05 17:52:00","Question_id":228528,"Tags":["r","probability","density-estimation"],"Answer_count":1,"Last_activity":"2016-08-08 02:12:56","Link":"http://stats.stackexchange.com/questions/228528/read-density-diagram-generated-in-r","Creator_reputation":108}
{"_id":{"$oid":"5837a582a05283111e4d5c28"},"View_count":39,"Display_name":"Joe King","Question_score":2,"Question_content":"Suppose we have repeated observations on pupils, where pupils are nested within schools. In lme4 I believe the correct way to specify the random effects part of the formula is(1|School/Pupil)which expands to(1|School) + (1|School:Pupil)My questions are:What exactly is the : symbol doing in the formula ? Does it make this an interaction term, as per the usual way that R works or is it something else ?What would be the interpretation of a formula which only contains (1|School:Pupil) ?What would be the interpretation of a model specified as (1|School) + (1|Pupil) + (1|School:Pupil)","Creater_id":11405,"Start_date":"2016-08-06 09:35:39","Question_id":228574,"Tags":["r","mixed-model","lme4"],"Answer_count":1,"Last_activity":"2016-08-08 02:01:32","Link":"http://stats.stackexchange.com/questions/228574/specification-of-nested-random-effects-in-lme4","Creator_reputation":811}
{"_id":{"$oid":"5837a582a05283111e4d5c35"},"View_count":14,"Display_name":"Santi Pe\u0026#241;ate-Vera","Question_score":0,"Question_content":"I have a  situation where I want to be able to forecast a point y of M dimensions, given a point x of N dimensions, having trained my system with many X and Y previous points.for example my prior knowledge (X, Y) is a time series like this:solar_irradiation | wind_speed | total_demand | house_1_demand | house_2_demand--------------------------------------------------------------------------------0.3               |  10        |  25.4        |  16.5          |  9.60.4               |  11        |  27.2        |  13.7          |  6.80.5               |  09        |  28.7        |  11.6          |  5.80.6               |  13        |  23.6        |  10.5          |  3.70.7               |  11        |  21.1        |  10.4          |  8.60.2               |  10        |  22.2        |  11.3          |  9.3* this table is just an exampleI consider the labels solar_irradiation, wind_speed, total_demand as X, and the labels house_1_demand, house_2_demand as Y.With this data, I want to provide a point x=[solar_irradiation, wind_speed and total_demand] and get values for y=[house_1_demand, house_2_demand]Right now I am using a multidimensional interpolation which provides values that make sense, however I would like to use other techniques. I have seen some tutorials of SVR and KRR and those methods seem to accept any number of labels for X, by Y must be a single dimension.Are SVR techniques suitable for this problem?  If not, which Machine Learning technique would be more suitable?The end game is to use many time series data to forecast many values.PS: Please bear with me since I am a self learner here.","Creater_id":124021,"Start_date":"2016-08-08 01:23:55","Question_id":228747,"Tags":["machine-learning","time-series","forecasting"],"Answer_count":0,"Last_activity":"2016-08-08 01:33:23","Link":"http://stats.stackexchange.com/questions/228747/machine-learning-to-fit-xn-dimensions-and-ym-dimensions","Creator_reputation":108}
{"_id":{"$oid":"5837a582a05283111e4d5c37"},"View_count":131,"Display_name":"madsthaks","Question_score":-1,"Question_content":"There's not much more to ask than what I've written in the title. Some of the values I want to predict are outside of the range used to build the regression model.","Creater_id":125449,"Start_date":"2016-08-07 21:18:54","Question_id":228729,"Tags":["r","regression","model","extrapolation"],"Answer_count":2,"Last_activity":"2016-08-08 01:19:36","Link":"http://stats.stackexchange.com/questions/228729/what-is-the-best-way-to-extrapolate-when-working-with-a-linear-regression-model","Creator_reputation":45}
{"_id":{"$oid":"5837a582a05283111e4d5c45"},"View_count":27,"Display_name":"Amitai","Question_score":3,"Question_content":"I face a random variable whose distribution I don't know.Someone draws a sample of k observations from a population and tells me their average. He repeats the process m times.I assume m is in order of magnitude of hundreds.If 1 \u0026lt; k \u0026lt; 20, What can I tell about the population variance?What about other lower moments?If k=1, I can trivially draw the emplirical distribution. What is the closest analoug for 1 \u0026lt; k \u0026lt; 20? ","Creater_id":126909,"Start_date":"2016-08-07 08:53:38","Question_id":228660,"Tags":["estimation","derived-distributions"],"Answer_count":0,"Last_activity":"2016-08-08 00:33:45","Link":"http://stats.stackexchange.com/questions/228660/assessing-a-distribution-from-multiple-estimates-of-its-mean","Creator_reputation":116}
{"_id":{"$oid":"5837a582a05283111e4d5c47"},"View_count":45,"Display_name":"ShanZhengYang","Question_score":0,"Question_content":"Let's say you have a density plot of data in 2D (or even 1D). Surely there are algorithms which infer the number of clusters which exist in the data without users having to explicitly set this number (e.g. like with traditional k-means).How does one use unsupervised learning to solve this? ","Creater_id":80118,"Start_date":"2016-08-07 18:19:46","Question_id":228718,"Tags":["machine-learning","clustering","unsupervised-learning"],"Answer_count":1,"Last_activity":"2016-08-07 22:03:34","Link":"http://stats.stackexchange.com/questions/228718/how-do-you-infer-the-number-of-clusters-in-data-using-unsupervised-learning","Creator_reputation":207}
{"_id":{"$oid":"5837a582a05283111e4d5c54"},"View_count":18,"Display_name":"ceoec","Question_score":0,"Question_content":"In one-dimensional IRT, the difficulty parameter b can be visualized in the item trace line and could be explained by word easily (the threshold trait value which the subject more likely to endorsed the items than the other one). While I understand b = -d/a, I found it difficult to describe what d really mean... (I understand why d is preferred -- its format resembles the logic of regression, and I can understand it is an \"intercept\".... just found it hard to explain what it really is....) Also, is it possible to represent b in terms of d in bifactor/mirt model? Thanks!","Creater_id":67800,"Start_date":"2016-08-07 21:48:09","Question_id":228730,"Tags":["irt"],"Answer_count":0,"Last_activity":"2016-08-07 21:56:13","Link":"http://stats.stackexchange.com/questions/228730/how-to-describe-what-the-difficulty-parameter-d-in-mirt-represents","Creator_reputation":287}
{"_id":{"$oid":"5837a582a05283111e4d5c56"},"View_count":128,"Display_name":"Xxxo","Question_score":3,"Question_content":"In conveying information between layers/nodes/neurons in a deep neural network one can choose between multiplication, addition, and concatenation. So, lets say that we have an input which passes the data to two, different, layers ( and ) and these layers have as output a vector of size  for   and  for . Then, we have another layer, , to which we want to pass the information of the  and . What would be the difference of using addition or concatenation? I know that multiplication is used to weight the information to be conveyed. But what about addition and concatenation? What is the conceptual/model-wise result in the information conveyance?  ","Creater_id":106300,"Start_date":"2016-08-02 01:03:36","Question_id":226816,"Tags":["deep-learning","information-theory"],"Answer_count":1,"Last_activity":"2016-08-07 21:42:20","Link":"http://stats.stackexchange.com/questions/226816/multiplication-addition-and-concatenation-in-deep-neural-networks","Creator_reputation":80}
{"_id":{"$oid":"5837a582a05283111e4d5c63"},"View_count":6038,"Display_name":"user3139","Question_score":13,"Question_content":"I would like to get the coefficients for the LASSO problem ||Y-X\\beta||+\\lambda ||\\beta||_1.The problem is that glmnet and lars functions give different answers. For the glmnet function I ask for the coefficients of  instead of just , but I still get different answers.Is this expected? What is the relationship between the lars  and glmnet ? I understand that glmnet is faster for LASSO problems but I would like to know which method is more powerful?deps_stats I am afraid that the size of my dataset is so large that LARS can not handle it, whereas on the other hand glmnet can handle my large dataset.mpiktas I want to find the solution of (Y-Xb)^2+L\\sum|b_j|but when I ask from the two algorithms(lars \u0026amp; glmnet) for their calculated coefficients for that particular L, I get different answers...and I wondering is that correct/ expected? or I am just using a wrong lambda for the two functions.","Creater_id":null,"Start_date":"2011-02-10 09:23:48","Question_id":7057,"Tags":["r","machine-learning","regression","lasso","regularization"],"Answer_count":3,"Last_activity":"2016-08-07 19:43:39","Link":"http://stats.stackexchange.com/questions/7057/glmnet-or-lars-for-computing-lasso-solutions","Creator_reputation":null}
{"_id":{"$oid":"5837a582a05283111e4d5c72"},"View_count":37,"Display_name":"user3007275","Question_score":0,"Question_content":"I'm hoping to do some text mining/analysis on some documents originally available in a PDF format but have been converted into excel files as flat files. There is no objective in mind other than trying to find whether these different files (representing different reports) have anything interesting in common that could be valuable.I was hoping to gather the techniques that would be useful and perhaps steps that I should take to gain something valuable out of this excercise - fairly new to this area and limited with time so I can only do so much reading/research before I run out of time actually implementing something concrete.The flat file structure is consistent across all reports i.e. it contains the same dimensions/column headers. The flat file essentially captures the meta data (name, type, headers) as well as content for each reported segment within a given report. i.e. each report captures some form of business data (e.g. balance sheet, financial statement, captial ratios) etc. but there are nuances across reports since each report captures the data somewhat differently and has different interpreation for the reported fields. This interpretation is also captured in the flatfile under a \"comment\" column and this column, to me, is most critical since it contains the most amount of text and possibly the most relevant too.Help is appreciated.","Creater_id":35402,"Start_date":"2016-08-07 18:03:14","Question_id":228717,"Tags":["text-mining","natural-language"],"Answer_count":0,"Last_activity":"2016-08-07 18:40:42","Link":"http://stats.stackexchange.com/questions/228717/text-mining-policy-documents","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5c74"},"View_count":47,"Display_name":"Saad","Question_score":-1,"Question_content":"We would like to determine whether the true mean systolic blood pressure  of healthyadults differs from 120. We obtain a sample of healthy adults and conduct an appropriatehypothesis test, which results in a P-value of 0.021. Which of the following statementsis true?I. A 96% confidence interval for mu  would contain the value 120.II. A 98% confidence interval for mu  would contain the value 120.III A 99% confidence interval for mu  would not contain the value 120.I am doing review for finals and got stuck solving this question. Idk where to start from. I don't want anyone to solve this for me. i just need a hint. ","Creater_id":122969,"Start_date":"2016-08-07 17:08:15","Question_id":228709,"Tags":["self-study","statistical-significance","confidence-interval","inference"],"Answer_count":1,"Last_activity":"2016-08-07 18:10:20","Link":"http://stats.stackexchange.com/questions/228709/confidence-interval-question-final-exam-review","Creator_reputation":16}
{"_id":{"$oid":"5837a582a05283111e4d5c80"},"View_count":54,"Display_name":"lep","Question_score":2,"Question_content":"I have to model 5 variables (in the same model to test for the most important factor) which include measures of distance (m) and percentages. At the beginning, I transformed the variable distance  to log(x), as it seems to be the general recommendation for distance/area variables... Then I scaled all the 5 variables to z (as I understand it is correct when comparing multiple models or variables with different scales as in this case). However, I just want to confirm if it is ok to double transform in this way, or if I should just rely on z values from the raw data and do not transform distance measures previously. [just in case: my response variables is abundance of species, and the predictors are cover (categorical), percentage of forest, distance to roads, distance to towns, and ndvi-a vegetation index related to the reflectance of plants-; I´m using GLM for multivariate abundance data in mvabund package). Many thanks in advance.","Creater_id":125108,"Start_date":"2016-08-05 21:00:05","Question_id":228533,"Tags":["regression","generalized-linear-model","data-transformation","back-transformation"],"Answer_count":1,"Last_activity":"2016-08-07 17:51:04","Link":"http://stats.stackexchange.com/questions/228533/is-valid-to-log-transform-a-variable-and-then-re-transform-to-z-score","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5c8d"},"View_count":26,"Display_name":"Sofia","Question_score":1,"Question_content":"Can I run a mixed model ANOVA with a within-subject factor of time (baseline vs. after the training) and a between-subject factor of condition (2 experimental groups and 1 control)? In that case, are contrast tests pointless to look at? I will also split the file to compare two groups (each with 3 conditions). Would you suggest a different analysis? In addition to this, is the normality of the outcome variables important for mixed ANOVAs? If I transform the variables that are not normal would that be okay? Thank you!","Creater_id":126949,"Start_date":"2016-08-07 17:42:53","Question_id":228715,"Tags":["anova","mixed-model","spss","normality","contrasts"],"Answer_count":0,"Last_activity":"2016-08-07 17:42:53","Link":"http://stats.stackexchange.com/questions/228715/mixed-model-anova-contrasts-and-normality","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5c8f"},"View_count":7195,"Display_name":"Nor Hisham Haron","Question_score":5,"Question_content":"I'm new to the R language. I would like to know how to simulate from a multiple linear regression model that fulfills all four assumptions of the regression.","Creater_id":13614,"Start_date":"2013-05-14 19:56:18","Question_id":59062,"Tags":["r","multiple-regression","simulation"],"Answer_count":3,"Last_activity":"2016-08-07 17:40:11","Link":"http://stats.stackexchange.com/questions/59062/multiple-linear-regression-simulation","Creator_reputation":87}
{"_id":{"$oid":"5837a582a05283111e4d5c9e"},"View_count":30,"Display_name":"Po Stulat","Question_score":0,"Question_content":"I am currently modelizing the price of real-estate in France. I chose three type of models : an arima, an mce, a var ... i of course fitted them in the best way possible thourgh the usual prodecure (box-jenkins and such) ... NOWI am (killing myself trying) to identify and index that would allow me to select the best model. Currently reading the Model Selection and Multimodel Inference: A Practical Information- Theoretic Approach, Second Edition by Burnham \u0026amp; Anderson but i get the feeling that this metric (AIC) is used to select \"a model\" ... well this is the issue actually. I am not trying to select the best model amongst (ie) different regressions (same instrument) ... but the best model between different instruments (arima, var, mce) I of course can identify it graphically but i need to have a metric ? Currently researching the AIC, BIC, HQ, RMSE, MAE, MAPE and studying the general Kullback et Leibler distance but i have an issue whenever the litteratre states \" selecting amongst differents models \" ... because i have to select among different \"instruments\"NB : i KNOW i'm not making any sense right now. ","Creater_id":74736,"Start_date":"2016-08-06 05:46:39","Question_id":228556,"Tags":["time-series","predictive-models","model-selection","model"],"Answer_count":1,"Last_activity":"2016-08-07 17:35:16","Link":"http://stats.stackexchange.com/questions/228556/time-series-predictive-model-selection-index-will-aic-do-the-trick","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5cab"},"View_count":96,"Display_name":"Richard Hardy","Question_score":5,"Question_content":"Consider a regression model y = X \\beta + \\varepsilon. I will use ridge regression to estimate . Ridge regression contains a tuning parameter (the penalty intensity) . If I were given a grid of candidate  values, I would use cross validation to select the optimal . However, the grid is not given, so I need to design it first. For that I need to choose, among other things, a maximum value .Question: How do I sensibly choose  in ridge regression?There needs to be a balance betweena  that is \"too large\", leading to wasteful computations when evaluating the performance of (possibly many) models that are penalized too harshly;a  that is \"too small\" leading to a forgone opportunity to penalize more intensely and get better performance.(Note that the answer is simple in the case of LASSO; there you take  such that all coefficients are set exactly to zero for any .)","Creater_id":53690,"Start_date":"2016-08-03 06:28:32","Question_id":227071,"Tags":["regression","cross-validation","regularization","ridge-regression"],"Answer_count":1,"Last_activity":"2016-08-07 17:14:54","Link":"http://stats.stackexchange.com/questions/227071/maximum-penalty-for-ridge-regression","Creator_reputation":13002}
{"_id":{"$oid":"5837a582a05283111e4d5cb8"},"View_count":21,"Display_name":"Richard Foo","Question_score":0,"Question_content":"Start off with a population of 200 balls. There are 2 types of tests that can be performed: call it A and B. The results of each test is either true or false. If we randomly select 100 balls and perform only test A on it. Then perform only test B on the remainder. If we get for the first hundred, A is true = 98, A is false = 2. And for the remainder, B is true = 89, B is false = 11. What sort of quantitative statements can I make about the underlying population of 200 balls?At at even simpler level, if we get A_(true) = 100, B_(true) = 100, chances are 100% that the underlying population is homogeneous (i.e. of a single type). But how does one compute the confidence of this statement? Do we compare it to a population of A_(true) = 50 and B_(true) = 50? But one is then comparing it to a random population. Is a Bayesian approach corresponding to different possibilities, i.e. underlying population of 1 species, 2 species, 2 species, etc., the right way to proceed?It would be enough if people can share the thinking behind how they would go about working out the above scenario. ","Creater_id":44663,"Start_date":"2016-08-07 07:20:21","Question_id":228652,"Tags":["mathematical-statistics"],"Answer_count":1,"Last_activity":"2016-08-07 17:11:57","Link":"http://stats.stackexchange.com/questions/228652/quantifying-statistical-confidence-from-combining-separate-tests","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5cc5"},"View_count":27,"Display_name":"gradstudent","Question_score":2,"Question_content":"If one does say \"Kernel Ridge Regression\" or \"Kernel PCA\" using the Gaussian kernel then do we know how the choice of the width of the Gaussian kernel affects the quality of the answer? Like does in some provable sense the answer so obtained get increasingly \"useless\" as one keeps increasing the Gaussian width?Very specifically : Do we know if the RKHS at two different Gaussian widths are isomorphic or not as function spaces?  ","Creater_id":124097,"Start_date":"2016-08-06 15:17:29","Question_id":228609,"Tags":["pca","kernel-trick","ridge-regression"],"Answer_count":0,"Last_activity":"2016-08-07 15:29:37","Link":"http://stats.stackexchange.com/questions/228609/about-the-effect-of-the-width-of-the-gaussian-kernel","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5cc7"},"View_count":99,"Display_name":"kenorb","Question_score":5,"Question_content":"Given list of numbers which looks pseudo-random (like lotto numbers, stock prices, pseudo-random), is it is possible to train the network to attempt to predict the next numbers?Which network would be more suitable for this task? Feedforward, recurrent or any other neural network?Especially the one which will work without memorizing the entire training set, but the one which can find some patterns or statistical association.","Creater_id":12989,"Start_date":"2016-08-06 12:49:50","Question_id":228598,"Tags":["neural-networks","random-generation","recursive-model"],"Answer_count":1,"Last_activity":"2016-08-07 14:37:18","Link":"http://stats.stackexchange.com/questions/228598/can-neural-network-can-be-used-to-predict-pseudo-random-numbers","Creator_reputation":293}
{"_id":{"$oid":"5837a582a05283111e4d5cd4"},"View_count":29,"Display_name":"Sebastiaan","Question_score":2,"Question_content":"Suppose that I have a function  and suppose that the pdf of  is . To avoid cumbersome numerical integration I approximate the expected value of  as ,with the right hand side evaluated in  random draws from . Is it somehow possible to apply a similar trick when I have  as the lower bound of the integration? (the domain of  remains )","Creater_id":94683,"Start_date":"2016-08-07 14:08:40","Question_id":228687,"Tags":["expected-value","monte-carlo","numerical-integration"],"Answer_count":1,"Last_activity":"2016-08-07 14:22:38","Link":"http://stats.stackexchange.com/questions/228687/approximation-expectation-integral","Creator_reputation":13}
{"_id":{"$oid":"5837a582a05283111e4d5ce1"},"View_count":43,"Display_name":"kenorb","Question_score":1,"Question_content":"For example there is the MNIST database which is used to test artificial neural network (ANN), however it's not so challenging, because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent and the same with other OCR tests.Therefore OCR datasets are 'no longer perceived as an exemplar of \"artificial intelligence\"'wiki.Are there any similar equivalent image-recognition tests, especially these which have the most challenging tasks with dataset which are commonly used as benchmark tests to challenge the AI which are fairly reliable and they're possible to pass (e.g. by humans), but most networks are struggling to achieve the lower error rate?","Creater_id":12989,"Start_date":"2016-08-06 12:02:16","Question_id":228592,"Tags":["dataset","image-processing","conv-neural-network","performance","computer-vision"],"Answer_count":0,"Last_activity":"2016-08-07 14:10:06","Link":"http://stats.stackexchange.com/questions/228592/what-are-the-current-most-challenging-mnist-like-tasks-aiming-to-achieve-the-low","Creator_reputation":293}
{"_id":{"$oid":"5837a582a05283111e4d5ce3"},"View_count":19,"Display_name":"quirik","Question_score":0,"Question_content":"With respect to the example of effect of years of education on wages, positive correlation between unobservables means that unobservables are positively correlated to working and positively related to wages.What about when rho is negative? What are 'real-life' examples of negative correlation between unobservables that affect selection and outcome equation in Heckmam selection model?","Creater_id":43204,"Start_date":"2016-08-07 13:38:53","Question_id":228684,"Tags":["model-selection","heckman"],"Answer_count":0,"Last_activity":"2016-08-07 13:38:53","Link":"http://stats.stackexchange.com/questions/228684/correlation-of-unobservables-in-heckman-selection-model","Creator_reputation":113}
{"_id":{"$oid":"5837a582a05283111e4d5ce5"},"View_count":24,"Display_name":"Peter Smith","Question_score":2,"Question_content":"I have read many articles about PLS, but I could not understand the mathematical description yet. I know that it is quite similar to principal component regression (PCR), except that it takes into account the direction of the response variable.Could you please provide for me a simple mathematical explanation e.g the formula and coefficient estimates form?","Creater_id":124456,"Start_date":"2016-08-07 12:27:25","Question_id":228678,"Tags":["pca","dimensionality-reduction","pls","shrinkage"],"Answer_count":0,"Last_activity":"2016-08-07 12:38:41","Link":"http://stats.stackexchange.com/questions/228678/a-mathematical-description-of-partial-least-squares-pls","Creator_reputation":41}
{"_id":{"$oid":"5837a582a05283111e4d5ce7"},"View_count":139,"Display_name":"Karnivaurus","Question_score":0,"Question_content":"How does a deep neural network, trained for regression with back propagation, deal with cases when different training pairs have the same input value, but different output values, i.e. the relationship between input and output is not smooth?In other words, suppose I train a deep neural network for regression from  to , using back propagation. Two of my data pairs have the same  ( and ), but very different 's ( and ). After training the network, what would be the output  for an input of ? Would it be , or the average of  and ?","Creater_id":72307,"Start_date":"2015-11-16 18:38:23","Question_id":182112,"Tags":["regression","machine-learning","neural-networks"],"Answer_count":1,"Last_activity":"2016-08-07 12:25:13","Link":"http://stats.stackexchange.com/questions/182112/deep-neural-network-regression-with-non-smooth-data","Creator_reputation":738}
{"_id":{"$oid":"5837a582a05283111e4d5cf4"},"View_count":56,"Display_name":"user126691","Question_score":2,"Question_content":"The data set is employee progression data from date of joining through the years getting promoted from one grade to the next at different points depending on tenure in that grade, employee performance (and implicitly company growth rate in adding junior employees).Stories the data could tell are  - the minimum tenure empoyees have to serve in each grade before moving   to next grade - proportion who make it in the first cycle after minimum tenure - proportion who take a lot more time to get promoted - any such delays causing higher attrition among such population   (hoping historical employee data will be provided)As a newbie, I considered simple graphs (such as bar graphs) which can bring out these stories in a series of graphs.  They do to an extent but I came across few other non-standard visualizations and am wondering if there is a more appropriate viz for this data and the narrative.Mosaic plots do show the proportions well within a grade but it isunclear how that can be translated to show a group doing well atfirst progression but falling behind in the next cycleParallel sets seem promising in visualizing the data but not sure ifthe stories will jump-outI like Sankey diagrams best.  I hear that it is apt for flowing data (such as liquids in a factory) but I am wondering all the stories will nicely jump out - such as a group of employees who join and move together initially but later diverge progressing at different paces or proportion who get promoted in first cycle, proportion in second cycle and proportion still at same level. I did a hand drawing of Sankey (attached) and I like it.  I think something like this could answer all questions from audience (considering this a few thousand data line items, the audience will look for insights that they already don't know from anecdotal evidence)Between these three, for this data, if one visualization can beat the other two, which would that be.From the data visualization catalog site, I couldn't find any other that even comes close (except of course if a series of graphs / animation can be used).I do know that the set will have data on 500-1000 employees, 15 years of progression (dates) and 9 levels.","Creater_id":126691,"Start_date":"2016-08-04 21:50:17","Question_id":228381,"Tags":["data-visualization"],"Answer_count":1,"Last_activity":"2016-08-07 11:48:08","Link":"http://stats.stackexchange.com/questions/228381/right-data-visualization-for-employee-progression-data-sankey-mosaic-or-parall","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5d01"},"View_count":10,"Display_name":"MInner","Question_score":0,"Question_content":"I ones visited a lecture where professor mentioned that one method of approximate inference for MRFs (we have a grid of binary nodes and [binary] potential functions for pairs of those nodes; we need to find the minimum of corresponding energy function) is to factor a grid into E- and Ш-shaped trees (that form a grid when intersected) and search for best ~shared solution for both problems (which is easier, as soon as these are trees now and something like Message Passing is applicable).What is the general term for \"approximate factoring of your problem into two simpler problems and searching for a shared solution\"?","Creater_id":41167,"Start_date":"2016-08-07 11:44:49","Question_id":228675,"Tags":["markov-random-field"],"Answer_count":0,"Last_activity":"2016-08-07 11:44:49","Link":"http://stats.stackexchange.com/questions/228675/factoring-mrf-into-e-and-%d0%a8-trees-what-is-the-general-term-for-this","Creator_reputation":101}
{"_id":{"$oid":"5837a582a05283111e4d5d03"},"View_count":28,"Display_name":"Andrew Haynes","Question_score":2,"Question_content":"If we compute two seperate variograms for the same type of measurement, but the areas which they cover overlap, is there a logical way (or does it even make sense) to combine the two variograms? Obviously, in this situation I am looking for a solution other than computing a single variogram over the combined datasets.","Creater_id":126912,"Start_date":"2016-08-07 11:26:19","Question_id":228673,"Tags":["r","spatial","geostatistics","variogram"],"Answer_count":0,"Last_activity":"2016-08-07 11:40:35","Link":"http://stats.stackexchange.com/questions/228673/is-there-an-approach-to-combining-variograms","Creator_reputation":21}
{"_id":{"$oid":"5837a582a05283111e4d5d05"},"View_count":79,"Display_name":"MiniQuark","Question_score":1,"Question_content":"When initializing connection weights in a feedforward neural network, it is important to initialize them randomly to avoid any symmetries that the learning algorithm would not be able to break.The recommendation I have seen in various places (eg. in TensorFlow's MNIST tutorial) is to use the truncated normal distribution using a standard deviation of , where  is the number of inputs to the given neuron layer.I believe that the standard deviation formula ensures that backpropagated gradients don't dissolve or amplify too quickly. But I don't know why we are using a truncated normal distribution as opposed to a regular normal distribution. Is it to avoid rare outlier weights?","Creater_id":109561,"Start_date":"2016-08-07 10:41:39","Question_id":228670,"Tags":["neural-networks","backpropagation","weights","truncated-normal"],"Answer_count":0,"Last_activity":"2016-08-07 11:17:02","Link":"http://stats.stackexchange.com/questions/228670/what-is-the-benefit-of-the-truncated-normal-distribution-in-initializing-weights","Creator_reputation":429}
{"_id":{"$oid":"5837a582a05283111e4d5d07"},"View_count":21,"Display_name":"Reza_Research","Question_score":0,"Question_content":"I have got three different sources which of each generate a stream of  discrete numbers in the range of [1,200] each hour. The length of streams are not limited.Each of these resources produce data from a very unique Gaussian distribution. But I need a single distribution to represent and combine the outputs of three sources.Could anyone possibly do me a favor and tell me if I can use a Gaussian Mixture Model (GMM) to display a single distribution for all three resources?What does this new distribution (derived from GMM) show?Thank you very much :)    ","Creater_id":126608,"Start_date":"2016-08-07 09:22:14","Question_id":228663,"Tags":["normal-distribution","gaussian-mixture"],"Answer_count":0,"Last_activity":"2016-08-07 09:22:14","Link":"http://stats.stackexchange.com/questions/228663/gaussian-mixture-model","Creator_reputation":48}
{"_id":{"$oid":"5837a582a05283111e4d5d09"},"View_count":59,"Display_name":"Filippo","Question_score":0,"Question_content":"The literature on Survival Analysis is mainly from the Medical science where tipically the researcher want to evaluate the effect of a treatment to that of another one. So far, all the example I read and studied thus contain one or more categorical variable (with at least 2 levels) and possibly some continuous variable as a covariate. Anyway the main interest is on a categorical variable (e.g. treatment).Is it possible and correct to run a non parametric Cox model (or alternatively a parametric one) using only one or more continuous variables? In particular without categorizing the continuous var into 2 or more groups? Something like a logistic regression. To give you a more practical example, I'm trying to model the survival of say bush in a field depending on the number of cows in the same field. I'm pretty sure it can be done but the lack of examples leave me in the doubt.If possible how can one use the predict function for example to predict the survival when the predictor has a specified value? like survival of my plant when 10 cows are in the field...any help is welcome!","Creater_id":41953,"Start_date":"2016-08-05 12:12:10","Question_id":228493,"Tags":["r","regression","survival","prediction","cox-model"],"Answer_count":1,"Last_activity":"2016-08-07 09:16:30","Link":"http://stats.stackexchange.com/questions/228493/can-a-cox-proportional-hazards-model-be-built-only-with-continuous-predictors","Creator_reputation":18}
{"_id":{"$oid":"5837a582a05283111e4d5d16"},"View_count":8,"Display_name":"ShanZhengYang","Question_score":1,"Question_content":"Let's say I want to do regression on dependent variable y with several features A, B, C, D, E. My data is as follows:import pandas as pddf = pd.read_csv('filename1.csv')dfy      A     B      C      D      E2.3    1     22     52     1      235.3    2     15     8.5    1.5    214.7    5     99     8.3    1.3    211.2    6     1      3.5    1.2    22.5....So, we would run our favorite regression model on this to predict y. Now, let's say I want to create a regression model to predict the difference of y between pairs of y. pair1 = 2.3 - 5.3 = 3pair2 = 2.3 - 4.7 = 2.4pair3 = 2.3 - 1.2 = 1.1...So I am predicting the absolute difference between each unique pair of data points. Question: what does one do with the features? Do you take the mean between two datapoints, or perhaps take the difference as well? ","Creater_id":80118,"Start_date":"2016-08-07 08:24:57","Question_id":228659,"Tags":["regression","feature-construction","covariate","differences"],"Answer_count":0,"Last_activity":"2016-08-07 08:24:57","Link":"http://stats.stackexchange.com/questions/228659/when-doing-regression-modeling-on-the-absolute-difference-between-pairs-how-to","Creator_reputation":207}
{"_id":{"$oid":"5837a582a05283111e4d5d18"},"View_count":80,"Display_name":"Eimear","Question_score":2,"Question_content":"I am conducting a meta-analysis in which some of the studies compare two or more interventions to the same control. The Cochrane Handbook recommends:  Combine groups to create a single pair-wise comparison (recommended).Select one pair of interventions and exclude the others.Split the ‘shared’ group into two or more groups with smaller sample size, and include two or more (reasonably independent) comparisons.Include two or more correlated comparisons and account for the correlation.The third option is the only feasible one. How do I calculate the mean and SD of the split control group? ","Creater_id":125544,"Start_date":"2016-08-03 12:28:18","Question_id":227142,"Tags":["multiple-comparisons","meta-analysis","network-meta-analysis"],"Answer_count":3,"Last_activity":"2016-08-07 08:23:27","Link":"http://stats.stackexchange.com/questions/227142/meta-analysis-splitting-control-group-for-multiple-comparisons","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5d27"},"View_count":80,"Display_name":"Reza_Research","Question_score":2,"Question_content":"Imagine that one has learned the parameters for a HMM model (\u0026#955;=(A,B,\u0026#960;)). How can I evaluate or even measure the accuracy of my model?In classification models, it is straightforward and one can use F1, AUC and Accuracy. But here in HMM models. I am not quite sure what should I do?Thanks everyoneI love Cross Validated site and I am very happy for being here :)  ","Creater_id":126608,"Start_date":"2016-08-04 08:31:35","Question_id":228277,"Tags":["hidden-markov-model","measurement","model-evaluation"],"Answer_count":1,"Last_activity":"2016-08-07 07:33:12","Link":"http://stats.stackexchange.com/questions/228277/how-to-evaluate-a-hmm-model","Creator_reputation":48}
{"_id":{"$oid":"5837a582a05283111e4d5d33"},"View_count":87,"Display_name":"aburkov","Question_score":1,"Question_content":"Conditional Random Fields (CRFs) is a typical solution for a sequence labelling/segmentation problem. For example, a sequence is a string and CRFs are used to label each word as being a part of a company name, a location, an event, etc.What is currently the state-of-the-art equivalent in the deep learning community to CRFs for sequence labelling/segmentation?CRFs have several implementations, including C++ and Java. Does an implementation exist on the deep learning side?","Creater_id":106222,"Start_date":"2016-08-03 17:31:08","Question_id":227185,"Tags":["deep-learning","natural-language","sequence-analysis","conditional-random-field"],"Answer_count":1,"Last_activity":"2016-08-07 07:25:56","Link":"http://stats.stackexchange.com/questions/227185/crf-equivalent-in-deep-learning","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5d40"},"View_count":41,"Display_name":"Albert","Question_score":0,"Question_content":"The Bayes' rule states thatP(w|x) \\propto P(x|w) \\cdot P(w).For speech recognition, we want to find the word sequence  which maximizes  for a given audio sequence . Via the Bayes' rule we separate it into the acoustic model for  and the language model for . Those will usually be totally independent models. Often they are weighted likeP(w|x) \\propto P(x|w)^\\alpha \\cdot P(w)^\\beta .In practice,  is modelled likeP(x|w) \\propto \\sum_{s:w} \\prod_t \\frac{P(s_t | s_{t-1})}{P(s_t)} \\cdot P(s_t|x) \\quad\\quad(1)where  is a hidden variable which corresponds to a HMM state sequence which matches the word sequence . You can use  and set  arbitrarily fixed or even leave them away.This is somewhat counter intuitive for me why  is used like that. One simplification I have seen (in the context of CTC) is to modelP'(w|x) = \\sum_{s:w} \\prod_t P(s_t|x) . \\quad\\quad(2)In that case, people often just useP(w|x) \\propto P'(w|x)^\\alpha \\cdot P(w)^\\beta .So, this is mathematically not correct anymore. People think of this as a log-linear combination of those two models.  is still important to be used because it is usually more powerful.When you compare (1) with (2), the only difference is how  and  are being used there. They are usually also weighted in some way.  is just used as \"time-distortion penalty scores\" and don't define a real probability distribution. The model for  also will model the bias / prior  already in some form.I'm not even sure how to define my questions. Maybe someone can just provide some statistical background. But here some questions:Can you say in any way which of these is more \"correct\", or will lead to better probability estimations? It seems that you can just interchange  with  and interpret it as Bayes' rule in the one case and as a log-linear combination in the other case.Considering the equation in (1), we can think of the inner term also as a log-linear combination likeP(s_t|s_{t-1})^a \\cdot P(s_t)^b \\cdot P(s_t|x)^cwhere all three probabilities are independent probability models.In the equation, we have . However, I'm wondering, if the whole equation (1) is supposed to model , maybe  makes more sense? Or maybe, as  will also model the prior in some way, maybe just ?","Creater_id":2244,"Start_date":"2016-08-07 05:51:32","Question_id":228648,"Tags":["time-series","bayesian","conditional-probability"],"Answer_count":0,"Last_activity":"2016-08-07 06:58:08","Link":"http://stats.stackexchange.com/questions/228648/bayes-rule-when-combining-two-independent-stochastic-models","Creator_reputation":283}
{"_id":{"$oid":"5837a582a05283111e4d5d42"},"View_count":28,"Display_name":"kenorb","Question_score":3,"Question_content":"The SAS FAQ suggest that for unordered two categories I should one dummy variables, for example:  The common practice of using target values of .1 and .9 instead of 0 and 1 prevents the outputs of the network from being directly interpretable as posterior probabilities, although it is easy to rescale the outputs to produce probabilities (Hampshire and Pearlmutter, 1991, figure 3).Following above logic, how do I know which target values I should use for three categories and why?","Creater_id":12989,"Start_date":"2016-08-06 12:05:42","Question_id":228593,"Tags":["classification","bayesian","categorical-data","posterior"],"Answer_count":0,"Last_activity":"2016-08-07 06:15:30","Link":"http://stats.stackexchange.com/questions/228593/how-should-three-unordered-categories-be-encoded-in-a-bayesian-network-framework","Creator_reputation":293}
{"_id":{"$oid":"5837a582a05283111e4d5d44"},"View_count":40,"Display_name":"hmluqman","Question_score":0,"Question_content":"I am defining a Normal Distribution in Octave by using this command \"y = normpdf(x,0,2)\". The standard deviation is 2 and x is independent uniform variable values. When I calculate standard deviation from std(y) then I do not get answer 2. Why is that so ?","Creater_id":110637,"Start_date":"2016-08-07 05:04:17","Question_id":228644,"Tags":["normal-distribution","pdf","stochastic-processes"],"Answer_count":1,"Last_activity":"2016-08-07 06:06:03","Link":"http://stats.stackexchange.com/questions/228644/contradiction-in-variance-and-standard-deviation-values-in-octave","Creator_reputation":3}
{"_id":{"$oid":"5837a582a05283111e4d5d51"},"View_count":167,"Display_name":"Tim H","Question_score":1,"Question_content":"I want to test if two indepent experimental groups are sign. different in terms of one outcome variable, but face the following questions:Basic setup2 groups with 200 observations eachOutcome variable: continuousranging between 0 and 600very peaked: more than 60% of values are equal to 300Group 1: mean = 221.9807, median = 300Group 2: mean = 239.6396, median = 300Result of statistical testsAlthough the outcome varible violates the normality assumption, I orginally used a two-sample independent t-test based on my large group sizes and the central-limit theorem:Result of T-test: no signifcant difference (p=0.166).However, to be on the safe side I also used a nonparmetric test, i.e. the Mann-Whitney U test:Result: of Mann-Whitney U test significant difference (p=0.023).QuestionsMy interpretation: Mean outcome not sig. different, but distribution of outcome varible differs between groups. Is that correct?Could it be a problem that the vast majority of my values are equal to 300? Does the MW test ignore values that are equal to the median?","Creater_id":81866,"Start_date":"2015-07-09 04:06:57","Question_id":160633,"Tags":["t-test","median","mann-whitney-u-test","sample-mean"],"Answer_count":2,"Last_activity":"2016-08-07 05:27:48","Link":"http://stats.stackexchange.com/questions/160633/interpreting-contradicting-results-t-test-vs-mann-whitney-test-2-independent-s","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5d5f"},"View_count":42,"Display_name":"Chechy Levas","Question_score":1,"Question_content":"If I have a standard deviation of log returns, what do I need to know to convert it to a standard deviation of simple returns?I have got the following R functions which seem to give consistent results for 2 different symmetric distributions.getSDapproxNorm = function(sig, mu){  rs = rnorm(n = 100000, mean = mu, sd = sig)  sd(exp(rs) - 1) }getSDapproxT = function(sig, mu){  rs = rt(n = 100000, df = 10)*sig/(sqrt(10/8)) + mu  sd(exp(rs) - 1) }doplot = function(mu){  sds = seq(0.01, 0.2, by = 0.01)  vals = NULL  for (s in sds){    a = getSDapproxNorm(s, mu)    b = getSDapproxT(s,mu)    vals = rbind(vals, c(a,b))  }  plot(x = sds, y = vals[,1], main = \"\", xlab = \"input\", ylab = \"output\", type = \"n\")  grid()  lines(x = sds, y = vals[,1], col = \"black\")  lines(x = sds, y = vals[,2], col = \"red\")  legend(x = \"topleft\",legend = c(\"norm\",\"r\"), col = c(\"black\",\"red\"), lty = 1)}doplot(0)doplot(0.05)doplot(0.1)doplot(0.2)doplot(0.4)Is it safe to use one of these functions to approximate the conversion, if all I know is the mean and the standard deviation?EDIT: description of R functionsThe R functions simulate a bunch of random log returns from a specific distribution such that the resulting series has the required mean and standard deviation. It then converts each log return to a simple return and measures the standard deviation of the simple returns. getSDapproxNorm models log returns as normally distributed which would make the corresponding simple returns log-normally distributed. getSDapproxT models the log returns as T distributed with degrees of freedom equal to 10.","Creater_id":92342,"Start_date":"2016-08-06 22:59:55","Question_id":228621,"Tags":["standard-deviation","lognormal","approximation"],"Answer_count":0,"Last_activity":"2016-08-07 04:31:11","Link":"http://stats.stackexchange.com/questions/228621/is-there-a-way-to-approximately-convert-a-standard-deviation-of-log-returns-to-a","Creator_reputation":85}
{"_id":{"$oid":"5837a582a05283111e4d5d61"},"View_count":27,"Display_name":"matsuo_basho","Question_score":0,"Question_content":"I would like to understand the mechanism by which a particular variable is selected as the basis for a decision stump in Adaboost.  Does this happen randomly?  If so, a given variable may well appear more than once as the basis variable for the decision stump.  Also, with Adaboost, we are assigning higher weights to the misclassified instances from using one weak learner, but then using those higher weights to train on a totally different decision stump.  This doesn't make sense to me, since the new weak learner is likely to misclassify different instances.  Finally, what happens if we have 20 variables, but 40 iterations in Adaboost.  Since a decision tree is deterministic, will we have 2 identical trees for each variable in the Adaboost model?Thanks in advance ","Creater_id":100399,"Start_date":"2016-08-07 03:47:54","Question_id":228638,"Tags":["adaboost"],"Answer_count":0,"Last_activity":"2016-08-07 03:47:54","Link":"http://stats.stackexchange.com/questions/228638/understand-adaboost-feature-selection","Creator_reputation":58}
{"_id":{"$oid":"5837a582a05283111e4d5d63"},"View_count":848,"Display_name":"user40380","Question_score":1,"Question_content":"Is it possible to use advance optimisation(L-BFGS, Conjugate gradient) for an collaborative filtering systems vs just using gradient decent? I ask this because of the need to calculate both X and Theta simutaniously","Creater_id":101054,"Start_date":"2016-01-27 18:53:59","Question_id":192833,"Tags":["machine-learning","recommender-system","collaborative"],"Answer_count":2,"Last_activity":"2016-08-07 03:44:01","Link":"http://stats.stackexchange.com/questions/192833/using-advance-optimisation-techniques-for-collaborative-filtering-systems-is-it","Creator_reputation":27}
{"_id":{"$oid":"5837a582a05283111e4d5d70"},"View_count":38,"Display_name":"Noosentin","Question_score":1,"Question_content":"I want to run a classification tree using rpart but the variable that I want to predict has a lot of class imbalance:Behaviour:a     b     c     d     e35    100   32    405   34301I have downsampled the data for Random Forest (RF) but when I split into 2/3 training and 1/3 test data for the classification tree and then downsample to the minority class, I lose alot of data. I've tried using SMOTE but it seems that only works when there are binary classes. It oversamples \"c\" and downsamples everything else so that \"a\" and \"b\" are around zero in order to bring \"e\" down to a suitable size. Cost sensitive learning doesn't look to be an option as I can't implement it properly in RF and I want to compare the two. I'd really appreciate any suggestions.","Creater_id":110480,"Start_date":"2016-08-01 14:39:13","Question_id":228636,"Tags":["r","machine-learning"],"Answer_count":1,"Last_activity":"2016-08-07 03:37:52","Link":"http://stats.stackexchange.com/questions/228636/multiclass-imbalance-classification-tree-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5d7b"},"View_count":16,"Display_name":"Ilario De Toma","Question_score":0,"Question_content":"I wanted to ask you suggestion for the best statistical approach to analyze the following data.I have two genotypes (A and B).Mice for each genotype undergo two experimental sessions:1) exposure of the mice to an object and a mouse from the A genotype2) exposure of the mice to an object and mouse from the B genotypeFor each session you calculate the social preference ratio: (time exploring the mouse - time exploring the object) /total time of exploration, which goes from 1 (the mouse explore only the other mouse) to -1 (the mouse explores only the object).So in the end you end up with four measures:- social preference ratio of mice A when exposed to object/mouse B- social preference ratio of mice A when exposed to object/mouse A- social preference ratio of mice B when exposed to object/mouse B- social preference ratio of mice B when exposed to object/mouse AWith what statistical test would you compare these data results?Our main question is if there are differences in social preferences in A mice versus B mice.Would it be preferable to analyse each pair-wise comparison separately, or could we fill a model with several contrasts such as: -social preference of A vs B when exposed to object/A- social preference of A vs B when exposed to object/B- social preference of A vs B when exposed to the respective same genotype- social preference of A vs B when exposed to the other genotypeI thank you in advance for any suggestion.Ilario","Creater_id":115271,"Start_date":"2016-05-11 03:40:33","Question_id":211990,"Tags":["model","statistical"],"Answer_count":1,"Last_activity":"2016-08-07 03:36:37","Link":"http://stats.stackexchange.com/questions/211990/analyzing-social-preference-data-with-2-genotype-combinations","Creator_reputation":1}
{"_id":{"$oid":"5837a582a05283111e4d5d88"},"View_count":98,"Display_name":"Paula","Question_score":3,"Question_content":"I'm trying to understand this paper but I can't figure out what the difference between SIR and SMC is. I thought that SIR is an example of SMC but the authors seem to distinguish between them. They state:  In this section, we show how it is possible to use any local  move—including MCMC moves— in the SIS framework while circumventing  the calculation of distribution (9),where (9) corresponds to the importance distribution. However, I don't see why this would be a problem in SIS and how SMC is different.I would be very grateful for help!","Creater_id":126827,"Start_date":"2016-08-06 08:13:56","Question_id":228569,"Tags":["mcmc","monte-carlo","importance-sampling"],"Answer_count":1,"Last_activity":"2016-08-07 03:27:08","Link":"http://stats.stackexchange.com/questions/228569/difference-between-sequential-importance-resampling-and-sequential-monte-carlo","Creator_reputation":38}
{"_id":{"$oid":"5837a582a05283111e4d5d95"},"View_count":56,"Display_name":"S\u0026#248;r\u0026#235;n","Question_score":4,"Question_content":"I'm facing some doubts in understanding how are degrees of freedom considered in distributions.In particular let's refer to  Student variable, that ist=\\frac{x-\\bar{x}}{\\hat{s}}=\\frac{x-\\bar{x}}{\\sqrt{\\frac{\\sum(x_i-\\bar{x})^2}{N-1}}}\\tag{1}Where  is a gaussian variable,  is the mean value,  is the standard deviation taken from data.Student probability density function is f(t)=C (1+\\frac{t^2}{\\nu})^{-\\frac{\\nu+1}{2}}\\tag{2}And on my textbook I find  \"because in  appears the mean value , calculated from data, which implies the loss of a degree of freedom\".My doubt is: shouldn't it be ? In  I have both  and  so there are two parameters determined from data.On the other hand  in the second form I wrote in ,  does not appear, so maybe only  should be considered as a constraint on data. But this does not make a lot of sense.So in these cases where both the mean value and the standard deviation are determined from data, are the degree of freedom lost 2 or only 1?This is kind of a more general doubt: when more than one parameter is determined from data, but in some ways these parameters are related (as it is for  and ) how many degrees of freedom are lost if all these parameter are considered?Say for istance I determine  parameters  from the same set of data. All the parameters  can be expressed as functions of data and . Now I consider all the parameters togheter: how many d.f. did I lose?  or just ?","Creater_id":114858,"Start_date":"2016-07-30 12:25:01","Question_id":226483,"Tags":["distributions","degrees-of-freedom","t-distribution","constraint"],"Answer_count":0,"Last_activity":"2016-08-07 02:42:19","Link":"http://stats.stackexchange.com/questions/226483/degrees-of-freedom-does-the-determination-of-mean-value-and-standard-deviation","Creator_reputation":162}
{"_id":{"$oid":"5837a582a05283111e4d5d97"},"View_count":54,"Display_name":"michalOut","Question_score":4,"Question_content":"Assume  is an  Gassian matrix, i.e., its entries are i.i.d. standard normal random variables, with . Take  for some fixed real scalars. I am interested in finding the p.d.f. of the  \"unitary\" matrix  from the QR decomposition of  (and possibly , etc.). It is known that if  and , the identity matrix, then  is distributed with respect to the Haar meassure on the Lie group of orthonormal matrices of order .  Can you provide any insight on the general case for  and/or general ?I also tried to look for the simplest case, i.e., . Then the QR decomposition coincide with a simple normalization. I have found this result for common varience, i.e., the case . Can this be easily generalized for the general case with different ?I attempted in the simplest case to scale the matrix  (which is for  just an  dimensional random vector). Indeed, then the above mentioned result is applicable and one gets DM=DUR, where  is the QR decomposition of  and the p.d.f. of entries of  is known from the above. Nonetheless, I haven't found any easy way to connect the p.d.f. of  with the one of . Thanks in advance.","Creater_id":125515,"Start_date":"2016-08-04 03:21:51","Question_id":228224,"Tags":["normal-distribution","linear-algebra","matrix-decomposition","chi-distribution"],"Answer_count":0,"Last_activity":"2016-08-07 02:40:30","Link":"http://stats.stackexchange.com/questions/228224/qr-decomposition-of-normally-distributed-matrices","Creator_reputation":21}
{"_id":{"$oid":"5837a582a05283111e4d5d99"},"View_count":786,"Display_name":"Victor","Question_score":3,"Question_content":"In this article, the author links linear discriminant analysis (LDA) to principal component analysis (PCA). With my limited knowledge, I am not able to follow how LDA can be somewhat similar to PCA.I have always thought that LDA was a form of classification algorithm, similar to logistic regression. I will appreciate some help in understanding how LDA is similar to PCA, i.e how is it a dimensionality reduction technique.","Creater_id":61158,"Start_date":"2015-08-30 20:06:57","Question_id":169436,"Tags":["classification","pca","dimensionality-reduction","discriminant-analysis","canonical-correlation"],"Answer_count":1,"Last_activity":"2016-08-07 02:32:22","Link":"http://stats.stackexchange.com/questions/169436/how-lda-a-classification-technique-also-serves-as-dimensionality-reduction-tec","Creator_reputation":1250}
{"_id":{"$oid":"5837a582a05283111e4d5da6"},"View_count":33,"Display_name":"user3771535","Question_score":1,"Question_content":"In the context of Social Network Analysis, I have two different social networks (in fact both are subgraphs of the same graph, but all the edges are distinct) and I would like to find a rigourous tool to compare their assortativity coefficients (associated to the same set of categories). In other words, these I obtain two assortativity coefficients which are equal to 26.97% and 30.24%, and I would like to assess the significance of the difference.Is there any kind of statistical hypothesis test I could perform in order to do it?","Creater_id":126881,"Start_date":"2016-08-07 01:44:28","Question_id":228632,"Tags":["hypothesis-testing","statistical-significance","social-network"],"Answer_count":0,"Last_activity":"2016-08-07 01:44:28","Link":"http://stats.stackexchange.com/questions/228632/how-to-compare-the-assortativity-of-two-distinct-networks","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5da8"},"View_count":56,"Display_name":"M.S.","Question_score":0,"Question_content":"I'm aware a silhouette score ranges from -1 to 1. But what can be considered a significant increase? 0.1 to 0.2 (because 100%) or 0.5 to 0.6?Obviously higher is better, but is there some measure of significance when it comes to silhouette scores?","Creater_id":122592,"Start_date":"2016-07-13 08:01:56","Question_id":223574,"Tags":["statistical-significance","clustering","validation"],"Answer_count":1,"Last_activity":"2016-08-07 01:04:03","Link":"http://stats.stackexchange.com/questions/223574/silhouette-score-significance-what-is-a-significant-increase-in-silhouette-scor","Creator_reputation":20}
{"_id":{"$oid":"5837a582a05283111e4d5db5"},"View_count":19,"Display_name":"user1060598","Question_score":0,"Question_content":"I have a problem with continuous feature and outcome data. The features are weak predictors. I'd like to be able to cluster my features into  classes. This is not semi-supervised learning so much as poorly-supervised learning---I want to cluster the feature data, with some influence from the known outcomes in the training data, but not take it as gospel.Is there a nice way to cluster while incorporating the outcome training data in a single step?I can think of regressing the data and then clustering the predicted outcomes (each a scalar); however, the regression is not very good so I don't want to rely on it heavily. I can also cluster the data unsupervised (-means) and then rank them by the mean outcome of each cluster in the training data. The latter works surprisingly well, but doesn't taken into account the training data that I have available.","Creater_id":83835,"Start_date":"2016-07-25 12:29:51","Question_id":225583,"Tags":["classification","clustering","k-means","supervised-learning","semi-supervised"],"Answer_count":1,"Last_activity":"2016-08-07 00:53:29","Link":"http://stats.stackexchange.com/questions/225583/training-classification-clustering-with-regression-data","Creator_reputation":3}
{"_id":{"$oid":"5837a582a05283111e4d5dc2"},"View_count":4,"Display_name":"Sebi","Question_score":0,"Question_content":"I was going through the paper describing the approach taken by GoogleNet in pattern recognition and the results at ILSVRC. Have there been any efforts in recognizing patterns in auditive input? ","Creater_id":121884,"Start_date":"2016-08-07 00:25:50","Question_id":228628,"Tags":["pattern-recognition"],"Answer_count":0,"Last_activity":"2016-08-07 00:25:50","Link":"http://stats.stackexchange.com/questions/228628/pattern-recognition-for-auditive-input","Creator_reputation":101}
{"_id":{"$oid":"5837a582a05283111e4d5dc4"},"View_count":353,"Display_name":"Anonymous","Question_score":6,"Question_content":"If any random variable has zero variance, then is it right to say that:   A random variable with zero variance is not a random variable","Creater_id":103252,"Start_date":"2016-08-06 23:43:09","Question_id":228624,"Tags":["variance","standard-deviation","random-variable"],"Answer_count":1,"Last_activity":"2016-08-07 00:18:47","Link":"http://stats.stackexchange.com/questions/228624/random-variable-with-zero-variance","Creator_reputation":46}
{"_id":{"$oid":"5837a582a05283111e4d5dd1"},"View_count":19,"Display_name":"MeidaiT","Question_score":1,"Question_content":"My model is an OLS with a single independent variable on cross-sectional data (n=3500). The relation is linear and there's a very good fit and there are no outliers, but the residuals' variance increases as x increases, clearly visible as a fan-shape in a residual-versus-predicted plot, so there's an issue with heteroskedasticity (which the Breusch-Pagan test confirms). The residuals' variance seems to be normally distributed and indeed symmetric above and below any value of x. I then calculated robust standard errors.This model has two objectives:To predict y for out-of-sample values of x. That's why transforming the variables to reduce heteroskedasticity won't help much: I'll have to de-transform the results back to their original units and the variance in the errors will return.To convey the message that in this model specification there is heteroskedasticity with a specific pattern that as x increases, the variance in the errors increases too, making predictions of high value x's less reliable but within a certain calculable range (i.e. confidence intervals).My questions are:can I use robust standard errors to calculate confidence intervals for out-of-sample x's?I'd like to plot the in-sample and out-of-sample variables together with the regression line and 95% confidence intervals obtained through robust standard errors. Is there any special issue to be aware of, or can they be plotted just like regular CIs?Just out of curiosity I also tried bootstrapped standard errors, which came out remarkably similar to the robust standard errors. Is there any reason to prefer one over the other?Is there any other way to deal with heteroskedasticity in a case like this without introducing another independent variable?I hope these questions make sense, if not please bear with me as I'm quite new at this and I'd like to learn why. Thank you!","Creater_id":126871,"Start_date":"2016-08-07 00:05:41","Question_id":228625,"Tags":["predictive-models","heteroscedasticity","robust-standard-error"],"Answer_count":0,"Last_activity":"2016-08-07 00:05:41","Link":"http://stats.stackexchange.com/questions/228625/heteroskedasticity-and-out-of-sample-predictions","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5dd3"},"View_count":17,"Display_name":"user126875","Question_score":1,"Question_content":"What is the distribution that would result when a person's height is plotted over time. eg., A person's height since he was 1 yr old to when he'd be 70 years old.","Creater_id":126875,"Start_date":"2016-08-06 23:31:37","Question_id":228623,"Tags":["beta-distribution"],"Answer_count":0,"Last_activity":"2016-08-06 23:31:37","Link":"http://stats.stackexchange.com/questions/228623/distribution-that-results-when-an-individuals-height-is-plotted-over-time","Creator_reputation":6}
{"_id":{"$oid":"5837a582a05283111e4d5dd5"},"View_count":21,"Display_name":"rec","Question_score":1,"Question_content":"After doing a bunch of reading in some papers I think I have a decent understanding of an SVR model.However what I'm having trouble visualizing is that SVR approximates non-linear regression by performing linear regression on the data that is moved into the higher dimension. As a result this higher dimensional linear regression ends up being non-linear on the original data.Is there a tutorial, exercise, or some math that would help me understand and picture this process in my head?Thank you!","Creater_id":124589,"Start_date":"2016-08-06 22:54:31","Question_id":228619,"Tags":["regression","svm","nonlinear-regression","libsvm"],"Answer_count":0,"Last_activity":"2016-08-06 22:54:31","Link":"http://stats.stackexchange.com/questions/228619/how-does-an-svr-approximate-non-linear-regression","Creator_reputation":43}
{"_id":{"$oid":"5837a582a05283111e4d5dd7"},"View_count":22,"Display_name":"Sayan Pal","Question_score":0,"Question_content":"Recently I was reading about restricted Boltzmann machine (RBM). And from that I have came to know that to train an RBM, Contrastive Divergence (CD) can be used in which Gibbs sampling is used to sample hidden, and visible variable.During Gibbs sampling in CD, sampling is performed as follows:\\textbf{h} \\tilde{} sigmoid(\\textbf{h}|\\textbf{v})\\textbf{v} \\tilde{} sigmoid(\\textbf{v}|\\textbf{h})Now I have couple of questions regarding RBM, those are as follows.How we can sample from this  function? I have seen here (check for propup, and ample_h_given_v functions) that binomial distribution is used somehow to perform this sampling. So basically  function returns the probability of being the variable 1, and then using that, we can sample the variable from a binomial variable parameterized with this probability. Is this explanation right?After the training is done, we have inferred the associated weights, and biases. Now, for a new input vector, how we can get the corresponding values of the hidden variables?Please help me understanding the same.P.S. Also I am not proficient in python, as I usually use R. Thus, it will be great if you also add comments in the code if sharing a code snippet in python.","Creater_id":90880,"Start_date":"2016-08-05 23:56:43","Question_id":228541,"Tags":["binomial","graphical-model","gibbs","rbm"],"Answer_count":0,"Last_activity":"2016-08-06 22:44:43","Link":"http://stats.stackexchange.com/questions/228541/few-doubts-regarding-restricted-boltzmann-machine-rbm","Creator_reputation":101}
{"_id":{"$oid":"5837a582a05283111e4d5dd9"},"View_count":98,"Display_name":"guest99921","Question_score":4,"Question_content":"is there a general solution to that? I have seen simple examples for Y+X=Z but I was wondering how this would be with rescaling? ","Creater_id":126731,"Start_date":"2016-08-05 04:02:35","Question_id":228421,"Tags":["pdf","convolution"],"Answer_count":2,"Last_activity":"2016-08-06 20:20:03","Link":"http://stats.stackexchange.com/questions/228421/general-solution-sum-of-two-uniform-random-variables-aybx-z","Creator_reputation":21}
{"_id":{"$oid":"5837a582a05283111e4d5de7"},"View_count":48,"Display_name":"Hugh","Question_score":1,"Question_content":"I have a variable  which has a bivariate normal distribution with mean vector  and covariance matrix .The vector  is composed of two variables A function  is a function of  onlyI already calculated that the mean of  is I am trying to find the variance of . I thought this would be straightforward but when I try to verify it numerically the results don't match the predictions. This is my approach:Let Let  be the probability density function of the bivariate normal variable.For  then , otherwise , so the integral simplifies toAnd a minor change:I use wolfram alpha to get the result (for ) and substitute it into the equation for the variance:The formula for the variance matches with a monte-carlo numerical integration so wolfram is doing the integration correctly. Unfortunately, the predictions don't match the sample variance from a numerical simulation of the bivariate distribution. Because of this I think that the way i set up the integration was incorrect. It seems straightforward but I haven't done this with piecewise functions or multiple variables before.Is there any obvious error in the way I set up the integral for ?","Creater_id":25299,"Start_date":"2016-08-06 15:58:27","Question_id":228611,"Tags":["variance","bivariate","multivariate-normal"],"Answer_count":1,"Last_activity":"2016-08-06 18:17:47","Link":"http://stats.stackexchange.com/questions/228611/calculating-variance-of-a-piecewise-function-of-bivariate-normal-variables","Creator_reputation":961}
{"_id":{"$oid":"5837a582a05283111e4d5df4"},"View_count":582,"Display_name":"Zhubarb","Question_score":8,"Question_content":"My question is generally on Singular Value Decomposition (SVD), and particularly on  Latent Semantic Indexing (LSI).Say, I have  that contains frequencies of 5 words for 7 documents.A =  matrix(data=c(2,0,8,6,0,3,1,                   1,6,0,1,7,0,1,                   5,0,7,4,0,5,6,                   7,0,8,5,0,8,5,                   0,10,0,0,7,0,0), ncol=7, byrow=TRUE)rownames(A) \u0026lt;- c('doctor','car','nurse','hospital','wheel')I get the matrix factorization for  by using SVD: . s = svd(A)D = diag(sd^0.5 ) # diag matrix with square roots of singular values.In 1 and 2, it is stated that:  gives the  word similarity matrix, where the rows of  represent different words.  WordSim =  sDocSim= S \\cdot V^TDocSimv)Questions:Algebraically, why are  and  word/document similarity matrices? Is there an intuitive explanation?Based on the R example given, can we make any intuitive word count / similarity observations by just looking at  and  (without using cosine similarity or correlation coefficient between rows / columns)? ","Creater_id":28740,"Start_date":"2014-07-16 05:31:53","Question_id":108156,"Tags":["r","svd","natural-language","latent-semantic-indexing"],"Answer_count":1,"Last_activity":"2016-08-06 17:51:03","Link":"http://stats.stackexchange.com/questions/108156/understanding-singular-value-decomposition-in-the-context-of-lsi","Creator_reputation":3070}
{"_id":{"$oid":"5837a582a05283111e4d5e01"},"View_count":58,"Display_name":"Chris Rackauckas","Question_score":4,"Question_content":"I am trying to make QQ-Plots in Julia, but the only solution I could find is this old example using Gadfly. Given that it's being deprecated, I wanted to know if anyone knew of a solution for making the QQ-plots in Plots.jl. Maybe there's a stats recipe for QQ-plots around? Or how would you make such a series recipe?The problem can be boiled down to this: Given two arrays representing samples of distributions, how does transform this into the quantile-quantile information which is used in a qq-plot?","Creater_id":43792,"Start_date":"2016-08-06 11:13:44","Question_id":228585,"Tags":["qq-plot","julia"],"Answer_count":0,"Last_activity":"2016-08-06 17:08:11","Link":"http://stats.stackexchange.com/questions/228585/qq-plots-in-julia-via-plots-jl","Creator_reputation":308}
{"_id":{"$oid":"5837a582a05283111e4d5e03"},"View_count":41,"Display_name":"fbrundu","Question_score":1,"Question_content":"I have a dataset composed by a matrix of  observations and  predictors (features), where each observation is accompanied by a classification label.My objective is to train a Random Forest classifier on matrix and classification label array.The distribution of classification labels is strongly unbalanced, therefore I decided to proceed with downsizing all classes to the size of the minority class. Keeping this in mind, I had two ideas:random subsampling of  observations from each class (drawing without replacement)Principal Component Analysis (PCA) on matrix samples to get  meta-observations (principal components) from each class - my idea was to get the  most representatives observations which could describe each classGiven that the size of the minority class is , and I have two classes, I set the final number of observations to retain to .From the side of feature selection, I decided to use the mRMR algorithm [1-2] which seems to be suitable to get the minimum number of features which are most relevant to predict a classification variable. It makes use of mutual information, minimizing the mutual information between each pair of features and maximizing the mutual information between each feature and the classification variable.Regarding the number of features to retain, I read from [3] that a robust rule of thumb is to retain no more than  features, where  is the number of observations. Therefore, given that I want to have  observations, I shall not select more than  features.Keeping this in mind, I did two experiments:In the first one I used mRMR to get  features, then random subsampling of  observations, then Random Forest classificationIn the second one I first reduced the number of observations to  with PCA (I could not do it after mRMR -features selection because it would lead to only  principal components), then I used mRMR to get  features, finally Random Forest classificationAt this point, my expectations were to see an increase of performance int the second experiment w.r.t. the first experiments. Instead, in the first experiment I get a macro-averaged ROC/AUC score of %, while in the second I reach only %. Why the second experiment does not work as I expected?Peng, Hanchuan, Fuhui Long, and Chris Ding. \"Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy.\" IEEE Transactions on pattern analysis and machine intelligence 27.8 (2005): 1226-1238.Ding, Chris, and Hanchuan Peng. \"Minimum redundancy feature selection from microarray gene expression data.\" Journal of bioinformatics and computational biology 3.02 (2005): 185-205.Johnstone, Iain M., and D. Michael Titterington. \"Statistical challenges of high-dimensional data.\" Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences 367.1906 (2009): 4237-4253.","Creater_id":29911,"Start_date":"2016-08-05 04:55:06","Question_id":228428,"Tags":["classification","pca","feature-selection","random-forest","mutual-information"],"Answer_count":1,"Last_activity":"2016-08-06 16:22:19","Link":"http://stats.stackexchange.com/questions/228428/pca-for-observations-subsampling-before-mrmr-feature-selection-affects-downstrea","Creator_reputation":284}
{"_id":{"$oid":"5837a582a05283111e4d5e10"},"View_count":205,"Display_name":"Scott","Question_score":3,"Question_content":"I'm using R to try and compare the results of variable chemical compositions, following on from an article I've read.  In it, the authors used CDA to do something very similar to what I want to do, but I've been told by another researcher (without much of an explanation) that LDA would be better suited.  I could go into the specifics of why supervised learning is the avenue chosen, etc. but I won't post that unless someone asks.After doing some background reading (which hasn't really cleared up the difference between the two), I figured I'd try to explore this myself and compare the results.  The primary difference between my data and that in this article is that instead of just using the compositions, I've created 3 new variables (S-, F- and V-) for the CDA that are functions of the original compositional data (see code below).However, when I run the two analyses I get EXACTLY the same results - identical plots.  This doesn't seem possible, but I can't find an error in my coding.  My two questions are:Is it possible for LDA and CDA to return the exact same result?What are the practical differences between LDA and CDA?Data:library(MASS)library(candisc)library(ggplot2)al2o3\u0026lt;-runif(20,5,10)sio2\u0026lt;-runif(20,10,30)feo\u0026lt;-runif(20,40,60)country\u0026lt;-c(rep(\"England\",6), rep(\"Scotland\",6), rep(\"Wales\",4), rep(\"France\",4))df\u0026lt;-data.frame(country,al2o3,sio2,feo)LDA:lda \u0026lt;- lda(country ~ feo+sio2+al2o3, data=df)plda \u0026lt;- predict(object = lda, newdata = df)dataset = data.frame(country = df[,\"country\"], lda = pldaalso3/dffeo)/(dfsio2))vvalue\u0026lt;-(dffeo)mod \u0026lt;- lm(cbind(feo,sio2,al2o3) ~ country, data=df)can2 \u0026lt;- candiscList(mod)mod2 \u0026lt;- lm(cbind(fvalue,svalue,vvalue) ~ country, data=df)can3 \u0026lt;- candiscList(mod2)ggplot(can2scores, aes(x=Can1,y=Can2)) + geom_point(aes(color=country))","Creater_id":120789,"Start_date":"2016-08-02 04:36:16","Question_id":226854,"Tags":["r","multivariate-analysis","terminology","discriminant-analysis","canonical-correlation"],"Answer_count":1,"Last_activity":"2016-08-06 16:20:40","Link":"http://stats.stackexchange.com/questions/226854/differences-between-linear-and-canonical-discriminant-analyses-lda-and-cda","Creator_reputation":20}
{"_id":{"$oid":"5837a582a05283111e4d5e1d"},"View_count":71,"Display_name":"andrewH","Question_score":3,"Question_content":"Under what conditions does the law of large numbers hold (or fail, if that is easier to describe) for independent identically distributed random variables drawn from a distribution with a finite mean and infinite or non-existent variance? Is it any different if the distribution is stationary but not independent?","Creater_id":12923,"Start_date":"2016-06-26 19:33:35","Question_id":220773,"Tags":["independence","stationarity","iid","fat-tails","law-of-large-numbers"],"Answer_count":1,"Last_activity":"2016-08-06 16:14:30","Link":"http://stats.stackexchange.com/questions/220773/when-does-the-law-of-large-numbers-hold-for-rvs-from-a-distribution-with-infinit","Creator_reputation":611}
{"_id":{"$oid":"5837a582a05283111e4d5e2a"},"View_count":412,"Display_name":"N F N","Question_score":5,"Question_content":"First off, I'd like to say that i'm not very experienced with statistics and not a native English speaker so feel free to tell me if my question is completly obvious or unclear.I'm currently working on a unbalanced dataset ( fraud detection) and I have a problem with a few variable categories that are highly correlated to the response. For example, there's a variable that represents payment type. Most people trying to fraud have chosen checks and  a third of people who have chosen checks are frauds so  my model tends to classify as a fraud any observation with check as the payment type.The correlation between the two variables is ~50% and the precision i get after applying bagged Cart is around 0.4 and other algorithms also work rather poorly.How can I deal with this without removing the variable from my dataset? ","Creater_id":79536,"Start_date":"2016-08-05 08:11:13","Question_id":228458,"Tags":["correlation","imbalanced"],"Answer_count":3,"Last_activity":"2016-08-06 15:38:47","Link":"http://stats.stackexchange.com/questions/228458/how-to-handle-predictors-that-are-highly-correlated-to-the-response","Creator_reputation":48}
{"_id":{"$oid":"5837a582a05283111e4d5e39"},"View_count":62,"Display_name":"kenorb","Question_score":2,"Question_content":"What are the main differences between two types of feedforward networks such as multilayer perceptrons (MLP) and radial basis function (RBF)?What are the fundamental differences between these two types?","Creater_id":12989,"Start_date":"2016-08-06 12:10:13","Question_id":228595,"Tags":["model-comparison","perceptron","radial-basis","differences","rbf-network"],"Answer_count":1,"Last_activity":"2016-08-06 12:15:39","Link":"http://stats.stackexchange.com/questions/228595/what-is-the-difference-between-mlp-and-rbf","Creator_reputation":293}
{"_id":{"$oid":"5837a582a05283111e4d5e46"},"View_count":22,"Display_name":"kenorb","Question_score":1,"Question_content":"Leave-one-out cross-validation model technique is very similar to jackknifing resampling, because both omitting each training case and perform retraining of the network on the left-out subset.On wiki page we can read that jackknifing computes a statistic from the kept samples only, while LOOCV computes a statistic on the left-out sample(s). However it is still not clear for me.Can anybody elaborate on it, ideally with same practical examples?","Creater_id":12989,"Start_date":"2016-08-06 12:07:34","Question_id":228594,"Tags":["cross-validation","resampling","method-comparison"],"Answer_count":0,"Last_activity":"2016-08-06 12:07:34","Link":"http://stats.stackexchange.com/questions/228594/what-is-the-difference-between-jackknifing-and-loocv","Creator_reputation":293}
{"_id":{"$oid":"5837a582a05283111e4d5e48"},"View_count":30,"Display_name":"user3275222","Question_score":1,"Question_content":"I wish to predict the values of a stock, but not as simply as it is.I have two variables: the date\\time, measures in resolution of minutes, and the value of the stock. Predicting a stock value is very hard, otherwise everyone would do it. So I was asked, to predict the value in the next 10 minutes, based on the value in the previous 20 minutes. The data contains around a month, so I have plenty of data points. Normally, if I had to simply predict, I would try ARIMA. However, if X1, X2, X3, .... is the value at minute t, then I wish to predict X21,...X30, based on X1,...X20, but not just for the first 30 minutes, but for all data. Any idea of how to do it ? I thought maybe to take to correlations, and not ARIMA. Can you give me a creative idea ? Thank you.","Creater_id":81477,"Start_date":"2016-08-06 11:54:08","Question_id":228591,"Tags":["time-series","forecasting"],"Answer_count":0,"Last_activity":"2016-08-06 11:54:08","Link":"http://stats.stackexchange.com/questions/228591/predicting-the-value-of-a-stock","Creator_reputation":159}
{"_id":{"$oid":"5837a582a05283111e4d5e4a"},"View_count":43,"Display_name":"Morgan","Question_score":2,"Question_content":"I've constructed a Negative Binomial Regression wherein I predict the number of words an individual will find from a set of seven letters based on the average frequency of words that the set of letters can produce, the total number of words that set of letters can produce, and the participant's score on a vocabulary test. In R:WholeModel \u0026lt;- glm.nb(NumWordsFound ~ AvgFrequency * TotalWords + VocabScore,  data = p)My question is in regards to the inclusion of an offset term.  Right now my model does not contain an offset term as the amount of time each participant had to produce words was exactly the same.  While I would like to see the effect of the total number of words the set of letters could produce on actual productivity, should I instead include that number as an offset value?","Creater_id":82337,"Start_date":"2016-08-06 09:32:06","Question_id":228573,"Tags":["r","negative-binomial","count-data","offset"],"Answer_count":1,"Last_activity":"2016-08-06 11:53:07","Link":"http://stats.stackexchange.com/questions/228573/offset-for-negative-binomial-regression","Creator_reputation":16}
{"_id":{"$oid":"5837a582a05283111e4d5e57"},"View_count":11,"Display_name":"kenorb","Question_score":1,"Question_content":"According to this study, how does using DNN-based binary mask helps with localization precision and recognition of the object types in general? Does it give a better results for high-resolution object detection?","Creater_id":12989,"Start_date":"2016-08-06 11:48:26","Question_id":228589,"Tags":["deep-learning","image-processing"],"Answer_count":0,"Last_activity":"2016-08-06 11:48:26","Link":"http://stats.stackexchange.com/questions/228589/how-using-dnn-generated-masks-improve-the-object-detection","Creator_reputation":293}
{"_id":{"$oid":"5837a582a05283111e4d5e59"},"View_count":36,"Display_name":"user109464","Question_score":0,"Question_content":"I have a monthly time series of wheat's importation, I want to predict its future values. I don't know if it exists seasonal component there is the plot, What do you think ?","Creater_id":109464,"Start_date":"2016-08-06 08:24:22","Question_id":228570,"Tags":["time-series","seasonality"],"Answer_count":0,"Last_activity":"2016-08-06 11:42:00","Link":"http://stats.stackexchange.com/questions/228570/possible-seasonality","Creator_reputation":11}
{"_id":{"$oid":"5837a582a05283111e4d5e5b"},"View_count":11,"Display_name":"kenorb","Question_score":1,"Question_content":"I like the idea of CoDi model which uses a von Neumann neighborhood method which has four types of cells.How this models compares to the deep network in terms of efficiency?Are there any scenarios where CoDi can outperform the deep network with hidden layers?","Creater_id":12989,"Start_date":"2016-08-06 11:29:53","Question_id":228588,"Tags":["deep-learning","performance","efficiency"],"Answer_count":0,"Last_activity":"2016-08-06 11:29:53","Link":"http://stats.stackexchange.com/questions/228588/how-does-codi-compares-to-the-deep-network","Creator_reputation":293}
{"_id":{"$oid":"5837a582a05283111e4d5e5d"},"View_count":21,"Display_name":"kenorb","Question_score":1,"Question_content":"What are the benefits of a deep neural network architecture over a shallow architecture with many computational elements.What are pros and cons of these two architectures and its limitations?","Creater_id":12989,"Start_date":"2016-08-06 11:22:07","Question_id":228586,"Tags":["neural-networks","deep-learning","method-comparison"],"Answer_count":0,"Last_activity":"2016-08-06 11:22:07","Link":"http://stats.stackexchange.com/questions/228586/what-are-the-benefits-of-a-deep-over-a-shallow-architectures","Creator_reputation":293}
{"_id":{"$oid":"5837a582a05283111e4d5e5f"},"View_count":98,"Display_name":"mackbox","Question_score":3,"Question_content":"I guess it probably is, but just want a confirmation. Thanks! ","Creater_id":73733,"Start_date":"2016-08-05 12:54:13","Question_id":228503,"Tags":["aic","loss-functions"],"Answer_count":1,"Last_activity":"2016-08-06 11:02:25","Link":"http://stats.stackexchange.com/questions/228503/is-akaike-information-criterion-a-kind-of-loss-function","Creator_reputation":128}
{"_id":{"$oid":"5837a582a05283111e4d5e6c"},"View_count":52,"Display_name":"michek","Question_score":0,"Question_content":"I am unable to find much literature about independent component analysis applied to time series, except for http://andrewback.com/webpapers/ica_finance/ica_finance_ijns.htmFrom what I understand the time series data must be multivariate i.e it must be a nxp matrix with at least p\u003e1 dimension. x_i(t) = \\sum_{j=1}^na_{ij}s_j(t)where  are the observed signals and which we assume are the result of a mixing matrix . My question is: is it possible to decompose a 1-dimensional time series response into a (predefined) sum of  mixed signals, if  is a 1 dimensional vector? Or is it necessary to have more than one vector of observations?","Creater_id":102689,"Start_date":"2016-08-06 10:51:20","Question_id":228582,"Tags":["time-series","ica"],"Answer_count":0,"Last_activity":"2016-08-06 10:51:20","Link":"http://stats.stackexchange.com/questions/228582/ica-for-time-series-data","Creator_reputation":121}
{"_id":{"$oid":"5837a582a05283111e4d5e6e"},"View_count":37,"Display_name":"josech aming\u0026#39;a","Question_score":0,"Question_content":"I carried out a log transformation on some data about patient admission in the hospital, aiming to generate an additive model. I went ahead to forecast the same data using: MYFORECAST = forecast.Arima(auto.arima(admission, d=3, D=NA, stationary=FALSE,                             seasonal=FALSE, ic=\"aic\", trace=TRUE, allowdrift=FALSE,                             allowmean=TRUE)in R. I got the forecasts but I was left wondering:  Should report the forecasts or take exponent of the values? ","Creater_id":126831,"Start_date":"2016-08-06 09:46:41","Question_id":228576,"Tags":["r","time-series"],"Answer_count":1,"Last_activity":"2016-08-06 10:40:32","Link":"http://stats.stackexchange.com/questions/228576/time-series-analysis-in-r","Creator_reputation":1}
{"_id":{"$oid":"5837a582a05283111e4d5e7b"},"View_count":295,"Display_name":"AnarKi","Question_score":8,"Question_content":"I am rather new to the field of Gaussian processes and how they are being applied in machine learning. I keep reading and hearing about the covariance functions being the main attraction of these methods. So could anyone explain in an intuitive manner what is happening in these covariance functions?Otherwise, if you could point out to a specific tutorial or document explaining them.","Creater_id":45277,"Start_date":"2016-08-06 04:54:08","Question_id":228552,"Tags":["machine-learning","probability","bayesian"],"Answer_count":2,"Last_activity":"2016-08-06 10:31:40","Link":"http://stats.stackexchange.com/questions/228552/covariance-functions-or-kernels-what-exactly-are-they","Creator_reputation":72}
{"_id":{"$oid":"5837a582a05283111e4d5e89"},"View_count":20,"Display_name":"Saurabh Sinha","Question_score":0,"Question_content":"The pearson correlation coefficients between observe and modeled value is 0.87 and its 95% confidence interval is (0.7981378, 0.9242291). The number of observe and modeled value is 59 each. How can i describe and interpret it. ","Creater_id":120606,"Start_date":"2016-08-06 10:26:46","Question_id":228580,"Tags":["confidence-interval"],"Answer_count":0,"Last_activity":"2016-08-06 10:26:46","Link":"http://stats.stackexchange.com/questions/228580/pearson-correlation-and-confidence-interval","Creator_reputation":21}
{"_id":{"$oid":"5837a582a05283111e4d5e8b"},"View_count":9055,"Display_name":"zca0","Question_score":15,"Question_content":"Is there a relationship between regression and linear discriminant analysis (LDA)? What are their similarities and differences? Does it make any difference if there are two classes or more than two classes?","Creater_id":7329,"Start_date":"2012-06-30 19:49:37","Question_id":31459,"Tags":["regression","logistic","discriminant-analysis","canonical-correlation","reduced-rank-regression"],"Answer_count":4,"Last_activity":"2016-08-06 10:16:11","Link":"http://stats.stackexchange.com/questions/31459/what-is-the-relationship-between-regression-and-linear-discriminant-analysis-ld","Creator_reputation":311}
{"_id":{"$oid":"5837a582a05283111e4d5e9b"},"View_count":2468,"Display_name":"mbq","Question_score":5,"Question_content":"It is easy to find a package calculating area under ROC, but is there a package that calculates the area under precision-recall curve?","Creater_id":88,"Start_date":"2011-05-08 00:32:35","Question_id":10501,"Tags":["r","precision-recall"],"Answer_count":4,"Last_activity":"2016-08-06 09:54:21","Link":"http://stats.stackexchange.com/questions/10501/calculating-aupr-in-r","Creator_reputation":17903}
{"_id":{"$oid":"5837a582a05283111e4d5eab"},"View_count":110,"Display_name":"JRBNB","Question_score":4,"Question_content":"I am reading Berkes et al. (2003) about the GARCH model. Could someone help me figure out the proof of one lemma in the paper?   If  is a sequence of identically distributed random variables satisfying \\begin{equation}\\mathrm{E}\\log^{+}|\\xi_0|\u0026lt;\\infty, \\tag{1}\\end{equation} then  converges with probability one for any .Note,  if , and  otherwise.  By the Borel-Cantelli lemma it is enough to prove that, for any ,\\begin{equation}\\sum_{k=1}^{\\infty}P\\{|\\xi_{k}|\u0026gt;\\zeta^{k}\\}\u0026lt;\\infty. \\tag{2}\\end{equation}The distribution of  does not depend on , so \\begin{align}\\sum_{k=1}^{\\infty} P\\{|\\xi_{k}|\u0026gt;\\zeta^{k}\\}\u0026amp;=P\\{\\log^{+}|\\xi_k|\u0026gt;k\\log\\zeta\\}\\nonumber  \\\\\u0026amp;=\\sum_{k=1}^{\\infty} P\\{\\log^{+}|\\xi_0|\u0026gt;k\\log\\zeta\\}\\tag{3}  \\\\[10pt]\u0026amp;\\leq\\mathrm{E} \\log^{+} |\\xi_0|/\\log \\zeta, \\tag{4}\\end{align}and thus  implies .  It seems to me that  does not imply . It is natural to apply the Markov inequality to  and we have  . Since the harmonic sequence, , does not converge, we cannot get  by using the Markov inequality.Did I miss something here?   References:I.  Berkes,  L.  Horváth  and  P.  Kokoszka. GARCH processes: structure and estimation. Bernoulli 9 (2003), no. 2, 201--227. doi:10.3150/bj/1068128975. ","Creater_id":126677,"Start_date":"2016-08-04 15:47:31","Question_id":228348,"Tags":["probability","mathematical-statistics","proof","probability-inequalities"],"Answer_count":2,"Last_activity":"2016-08-06 09:53:48","Link":"http://stats.stackexchange.com/questions/228348/proof-with-probability-inequalities-and-infinite-sequences","Creator_reputation":61}
{"_id":{"$oid":"5837a582a05283111e4d5eb9"},"View_count":4484,"Display_name":"category","Question_score":9,"Question_content":"Apparently,  the Fisher analysis aims at simultaneously maximising the  between-class separation, while minimising the within-class  dispersion. A useful measure of the discrimination power of a variable  is hence given by the diagonal quantity: .http://root.cern.ch/root/htmldoc/TMVA__MethodFisher.htmlI understand that the size (n x n) of the Between (B) and Within-Class (W) matrices are given by the number of input variables, n. Given this, how can  be a \"useful measure of the discrimination power\" of a single variable? At least two variables are required to construct the matrices B and W, so the respective traces would represent more than one variable.Update: Am I right in thinking that  is not a trace over a trace, where the sum is implied, but the matrix element  divided by ? Currently that is the only way I can reconcile the expression with the concept. ","Creater_id":17508,"Start_date":"2013-01-29 08:09:14","Question_id":48786,"Tags":["algorithms","discriminant-analysis","fisher"],"Answer_count":2,"Last_activity":"2016-08-06 09:53:06","Link":"http://stats.stackexchange.com/questions/48786/algebra-of-lda-fisher-discrimination-power-of-a-variable-and-linear-discriminan","Creator_reputation":146}
{"_id":{"$oid":"5837a582a05283111e4d5ec7"},"View_count":22,"Display_name":"user2916044","Question_score":0,"Question_content":"Can someone please explain me what is the difference between Elliptic Fourier analysis and Elliptic Fourier descriptors in the case of outline analysis?","Creater_id":32130,"Start_date":"2016-08-06 09:51:54","Question_id":228578,"Tags":["analysis"],"Answer_count":0,"Last_activity":"2016-08-06 09:51:54","Link":"http://stats.stackexchange.com/questions/228578/elliptic-fourier-analysis-and-elliptic-fourier-descriptors","Creator_reputation":18}
{"_id":{"$oid":"5837a582a05283111e4d5ec9"},"View_count":46,"Display_name":"rg255","Question_score":0,"Question_content":"I have performed mixed effect Cox hazard regressions, and reconstructed the slopes to get group specific slopes (e.g. sex-specific responses to the explanatory variable). I aim to test whether the slopes differ from one another (e.g. do males and females respond differently to the explanatory variable?). To do this I will use Z-tests (here and here) whereZ=  \\frac{\\beta_1-\\beta_2}{\\sqrt{{SE_{\\beta_1}}^{2}+{SE_{\\beta_2}}^2}}However, I have performed my models in R using the coxme package which gives the following output, from which I reconstruct the sex- and group-specific slopes with the included function....Fixed coefficients                        coef exp(coef)   se(coef)      z    pSexM             0.091305017 1.0956031 0.09085235   1.00 0.31GroupG2         -0.036313825 0.9643376 0.08889039  -0.41 0.68NE              -0.192009224 0.8252993 0.01317388 -14.57 0.00SexM:GroupG2     0.009757875 1.0098056 0.12750426   0.08 0.94SexM:NE         -0.212264676 0.8087506 0.02008058 -10.57 0.00GroupG2:NE      -0.006933708 0.9930903 0.01814987  -0.38 0.70SexM:GroupG2:NE  0.044999019 1.0460268 0.02756553   1.63 0.10...coxSlopeFunc = function(model, nfixed = 1){    if(nfixed ==1){        # Slope for Females + G1        FG1 = modelcoefficients[3] + modelcoefficients[3] + modelcoefficients[3] + modelcoefficients[6] + model$","Creater_id":16542,"Start_date":"2016-08-04 06:44:46","Question_id":228257,"Tags":["r","mixed-model","cox-model","z-test"],"Answer_count":1,"Last_activity":"2016-08-06 09:47:55","Link":"http://stats.stackexchange.com/questions/228257/standard-error-of-reconstructed-slopes-coxph-coxme","Creator_reputation":344}
{"_id":{"$oid":"5837a582a05283111e4d5ed6"},"View_count":1023,"Display_name":"Felipe Pontes","Question_score":3,"Question_content":"I want to run a Canonical Correlation (in R) but I don't have the original (raw) data. I have only the correlation matrix of all the variables.I have seen some questions here about this, but my question continue unsolved. A user gave a parcial solution (http://www.stat.wmich.edu/wang/561/egs/Rcancor.html), but I need the canonical loadings, the percentual of variance in set Y that was explained from set X, and the variables significance.Does anyone here could help me?P.S.: I am a new R user. I have experience only on Eviews, GRETL and SPSS (also a little bit in Stata).","Creater_id":35142,"Start_date":"2013-11-21 14:11:46","Question_id":77287,"Tags":["r","algorithms","canonical-correlation"],"Answer_count":1,"Last_activity":"2016-08-06 08:24:49","Link":"http://stats.stackexchange.com/questions/77287/canonical-correlation-analysis-without-raw-data-algebra-of-cca","Creator_reputation":16}
{"_id":{"$oid":"5837a582a05283111e4d5ee3"},"View_count":824,"Display_name":"Franck Dernoncourt","Question_score":5,"Question_content":"In word2vec's CBOW and skip-gram models, how does choosing word vectors from  (input word matrix) vs. choosing word vectors from  (output word matrix) impact the quality of the resulting word vectors?CBOW:Skip-gram:","Creater_id":12359,"Start_date":"2015-10-19 08:03:37","Question_id":177667,"Tags":["neural-networks","natural-language","word2vec","word-embeddings"],"Answer_count":1,"Last_activity":"2016-08-06 08:16:17","Link":"http://stats.stackexchange.com/questions/177667/input-vector-representation-vs-output-vector-representation-in-word2vec","Creator_reputation":8000}
{"_id":{"$oid":"5837a582a05283111e4d5ef0"},"View_count":26,"Display_name":"mackbox","Question_score":1,"Question_content":"For a Bayesian, if he/she can make predictions using the entire posterior, why bother to calculate a Bayes estimate like the posterior mean or MAP?Thanks!","Creater_id":73733,"Start_date":"2016-08-06 06:34:05","Question_id":228560,"Tags":["bayesian","estimation","point-estimation"],"Answer_count":1,"Last_activity":"2016-08-06 07:13:57","Link":"http://stats.stackexchange.com/questions/228560/bayesian-estimator-and-prediction","Creator_reputation":128}
{"_id":{"$oid":"5837a582a05283111e4d5efd"},"View_count":13342,"Display_name":"CodeGuy","Question_score":2,"Question_content":"Consider the following scenario. I ran an experiment with 200 trials, each with a different stimulus. Each subject did exactly 200 trials. The subject responds with a single number anywhere between 5 and 50. The correct answer also ranges between 5 - 50. For each subject that did the experiment, I compute a single value, , for that person. This value  uses computations that use the expected answer and the observed answer for each trial. That is all the experiment does. It allows me to find  for a subject.I was asked to find Cronbach's alpha for this particular experiment. How do I do it? I see the formula on Wikipedia, however the denominator is  and in my case, , so how would I find Cronbach's Alpha? Should I find a single Cronbach's alpha value for the experiment as a whole, or is each subject getting an alpha value? I couldn't find many good online resources to learn about Cronbach's alpha value, so if anyone has any good suggestions, I would love to see links to places I can learn this stuff. Thanks!I will be using R, and I have a CSV file where column 1 is the subject, column 2 is the expected answer, and column 3 is the subjects answer. There are 200 rows.","Creater_id":8627,"Start_date":"2012-01-22 14:48:23","Question_id":21515,"Tags":["reliability","cronbachs-alpha"],"Answer_count":2,"Last_activity":"2016-08-06 07:13:25","Link":"http://stats.stackexchange.com/questions/21515/how-to-compute-cronbachs-alpha-with-only-one-measurement-per-subject","Creator_reputation":219}
{"_id":{"$oid":"5837a583a05283111e4d5f0b"},"View_count":28,"Display_name":"user308485","Question_score":0,"Question_content":"I'm using Bishop's Pattern Recognition and Machine Learning. In section 1.5.5, loss functions for regression, namely the squared loss, is discussed.  The book makes the following remark:The resulting expression shown above is substituted into the loss function, integrated over , and then it is seen that the cross-term (the second term) vanishes. The result obtained is:What I don't understand is the algebra involved to get the final result. Why does the cross-term vanish? For the last term, how are you bringing  outside the integral over ? Perhaps I am missing something here, could someone care to explain?","Creater_id":106852,"Start_date":"2016-08-06 06:40:43","Question_id":228561,"Tags":["regression"],"Answer_count":0,"Last_activity":"2016-08-06 06:40:43","Link":"http://stats.stackexchange.com/questions/228561/loss-functions-for-regression-proof","Creator_reputation":121}
{"_id":{"$oid":"5837a583a05283111e4d5f0d"},"View_count":54,"Display_name":"Ali Turab Lotia","Question_score":4,"Question_content":"Supposing I have fit some models using predictors (and the response variable) from the same data set.What changes to the model will make it unreasonable for me to compare the models on the basis of AIC?1) Supposing, if I log transform the dependent variable, is it fair to compare it to a model where there was no transformation?2) If I was to remove predictors from the model could I compare it with models with all the predictors added to it?3) If I fit two glms with different families for the two, can I still compare them on the basis of AIC? What about with different link functions?Thank you for your input.","Creater_id":124010,"Start_date":"2016-08-05 05:41:32","Question_id":228436,"Tags":["aic"],"Answer_count":1,"Last_activity":"2016-08-06 05:15:10","Link":"http://stats.stackexchange.com/questions/228436/what-breaks-the-comparibility-of-models-with-respect-to-the-aic","Creator_reputation":69}
{"_id":{"$oid":"5837a583a05283111e4d5f1a"},"View_count":174,"Display_name":"Taufi","Question_score":1,"Question_content":"At the moment I am estimating GARCH models for financial return data in Stata. For one series, I checked the squared residuals for autocorrelation: there is. Also the ARCH-LM test affirms the finding, but with a very high lag order (see picture 1). Now, when trying to fit a GARCH model to the data, I can hardly fit a statistically significant ARCH coefficient whereas the first GARCH term is already highly significant (see picture 2). Is it possible to have GARCH effects without ARCH effects present?Also, I found an ARCH effect at lag  instead of . But how do I make that clear in the notation? GARCH(1,1) suggests lags  for both the autoregressive as well as moving average component. And GARCH(3,1) would suggest an ARCH model with three lags into the past instead of one at lag . ","Creater_id":107036,"Start_date":"2016-03-06 11:09:04","Question_id":200249,"Tags":["stata","garch","arch"],"Answer_count":1,"Last_activity":"2016-08-06 05:03:31","Link":"http://stats.stackexchange.com/questions/200249/are-garch-effects-without-arch-effects-possible","Creator_reputation":93}
{"_id":{"$oid":"5837a583a05283111e4d5f27"},"View_count":316,"Display_name":"Richard Hardy","Question_score":2,"Question_content":"Suppose I have a VAR model with different regressors in different equations (this could be due to restricting some coefficients of a full VAR() model to zero or having some different exogenous regressors in different equations). Suppose also that the model is correctly specified. Such a model can be estimated by GLS or by equation-by-equation OLS. Both GLS and OLS estimators will be unbiased. GLS estimator will be more efficient than equation-by-equation OLS estimator. (Suppose for simplicity that the sample size is large enough so that we can rely on asymptotics where GLS is more efficient than OLS.)Question 1: When I do forecasting, will the model estimated by GLS yield smaller mean squared error (MSE) than the one estimated by OLS?I guess the answer is \"YES\". GLS will yield more precise estimates of model coefficients, which in turn will yield more accurate forecasts. But I am a bit confused: OLS will by definition give the smallest MSE in sample, for each equation in the system. Thus it will look better than GLS in terms of MSE in sample. Question 2: Does that mean that the in-sample MSE from OLS will be overly optimistic (with reference to forecasting), especially when compared with the in-sample MSE from GLS?Here is a remotely related question comparing OLS and GLS estimation of a VAR model.","Creater_id":53690,"Start_date":"2015-02-10 06:57:48","Question_id":137085,"Tags":["estimation","forecasting","var","mse","generalized-least-squares"],"Answer_count":2,"Last_activity":"2016-08-06 04:43:53","Link":"http://stats.stackexchange.com/questions/137085/forecasting-with-a-var-estimated-by-gls-versus-ols","Creator_reputation":13002}
{"_id":{"$oid":"5837a583a05283111e4d5f35"},"View_count":3797,"Display_name":"Gilles","Question_score":6,"Question_content":"I'm simply trying to recalculate with dnorm() the log-likelihood provided by the logLik function from a lm model (in R).It works (almost perfectly) for high number of data (eg n=1000)  : \u0026gt; n \u0026lt;- 1000\u0026gt; x \u0026lt;- 1:n\u0026gt; set.seed(1)\u0026gt; y \u0026lt;- 10 + 2*x + rnorm(n, 0, 2)\u0026gt; mod \u0026lt;- glm(y ~ x, family = gaussian)\u0026gt; logLik(mod)'log Lik.' -2145.562 (df=3)\u0026gt; sigma \u0026lt;- sqrt(summary(mod)dispersion)\u0026gt; sum(log(dnorm(x = y, mean = predict(mod), sd = sigma)))[1] -9.192832\u0026gt; sum(log(dnorm(x = resid(mod), mean = 0, sd = sigma)))[1] -9.192832Because of small dataset effect I thought it could be due to the differences in residual variance estimates between lm and glm but using lm provides the same result as glm : \u0026gt; modlm \u0026lt;- lm(y ~ x)\u0026gt; logLik(modlm)'log Lik.' -8.915768 (df=3)\u0026gt; \u0026gt; sigma \u0026lt;- summary(modlm)$sigma\u0026gt; sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma)))[1] -9.192832\u0026gt; sum(log(dnorm(x = resid(modlm), mean = 0, sd = sigma)))[1] -9.192832Where am I wrong ? ","Creater_id":31679,"Start_date":"2013-10-18 15:27:08","Question_id":73196,"Tags":["r","generalized-linear-model","likelihood","lm"],"Answer_count":1,"Last_activity":"2016-08-06 04:32:23","Link":"http://stats.stackexchange.com/questions/73196/recalculate-log-likelihood-from-a-simple-r-lm-model","Creator_reputation":53}
{"_id":{"$oid":"5837a583a05283111e4d5f42"},"View_count":67,"Display_name":"Qwerty","Question_score":7,"Question_content":"I have the following question at hand.  Suppose  represents a set of bi-variate observations on  such that  Under what conditions will the Least Square Regression line of  on  be identical to the Least Absolute Deviation line?I know that say we want to find  and  such that ; the LSQ method will give \\hat\\beta={\\sum\\limits_{i=1}^{10} (x_i-\\bar x)y_i\\over \\sum\\limits_{i=1}^{10}(x_i-\\bar x)x_i} and hence . Can someone help me proceed?","Creater_id":95845,"Start_date":"2016-08-05 08:54:38","Question_id":228468,"Tags":["regression","self-study","least-squares","absolute-deviation"],"Answer_count":1,"Last_activity":"2016-08-06 04:18:31","Link":"http://stats.stackexchange.com/questions/228468/when-does-least-square-regression-lsq-line-equal-to-least-absolute-deviation","Creator_reputation":454}
{"_id":{"$oid":"5837a583a05283111e4d5f4f"},"View_count":228,"Display_name":"Richard Hardy","Question_score":1,"Question_content":"I am wondering about the exact definition of ARIMA model in function arima in R when exogenous regressors are included.I understand that arima(y, order=c(p,0,q), xreg=x) is equivalent to estimating the following equation (where  and  stand for the means of  and , respectively):(1) Or is it(2) (only the last term differs between (1) and (2))?Or perhaps I got both of them wrong?Edit: I now realize that including both { and } and  in (2) was superfluous.","Creater_id":53690,"Start_date":"2014-10-28 08:43:48","Question_id":121749,"Tags":["r","arima"],"Answer_count":1,"Last_activity":"2016-08-06 03:45:05","Link":"http://stats.stackexchange.com/questions/121749/definition-of-arima-with-exogenous-regressors-in-r","Creator_reputation":13002}
{"_id":{"$oid":"5837a583a05283111e4d5f5c"},"View_count":91,"Display_name":"Richard Hardy","Question_score":1,"Question_content":"It seems that the auto.arima function in the \"forecast\" package in R only considers full ARIMA models. By \"full\" I mean that if an AR lag  is included, AR lag  will also be included for ,  (and the same with MA in place of AR). This was claimed here by the author of the auto.arima function himself. I am interested in non-full (restricted) ARIMA models, e.g. an AR(2) model where the first AR lag is restricted to zero: .Question 1: Is there a good theoretical reason for not considering the non-full ARIMA models?Question 2: Is there a good practical reason for not considering the non-full ARIMA models? (Besides the argument of high computational burden if all sub-models within given maximum AR and MA orders are to be estimated.)","Creater_id":53690,"Start_date":"2014-10-24 05:49:46","Question_id":121298,"Tags":["r","forecasting","model-selection","arima"],"Answer_count":0,"Last_activity":"2016-08-06 03:34:58","Link":"http://stats.stackexchange.com/questions/121298/why-only-full-arima-models-in-auto-arima","Creator_reputation":13002}
{"_id":{"$oid":"5837a583a05283111e4d5f5e"},"View_count":750,"Display_name":"Richard Hardy","Question_score":4,"Question_content":"I have a sample (of size 250) from a population. I do not know the distribution of the population. The main question: I want a point estimate of the 1st-percentile of the population, and then I want a 95% confidence interval around my point estimate.My point estimate will be the sample 1st-percentile. I denote it . After that, I try to build the confidence interval around the point estimate. I wonder if it makes sense to use bootstrap here. I am very inexperienced with bootstrap, so pardon if I fail to use the appropriate terminology etc.Here is how I tried to do it. I draw 1000 random samples with replacement from my original sample. I obtain the 1st-percentile from each of them. Thus I have 1000 points - \"the 1st-percentiles\". I look at the empirical distribution of these 1000 points. I denote the mean of it . I denote a \"bias\" as follows: . I take the 2.5th-percentile and 97.5th percentile of the 1000 points to obtain the lower and the higher end of what I call a 95% confidence interval around the 1st-percentile of the original sample. I denote these points  and .The last remaining step is to adapt this confidence interval to be around the 1st-percentile of the population rather than around the 1st-percentile of the original sample. Thus I take  as the lower end and  as the upper end of the 95% confidence interval around the point estimate of the population's 1st-percentile. This last interval is what I was seeking for. A crucial point, in my opinion, is whether it makes sense to use bootstrap for 1st-percentile which is rather close to the tail of the unknown underlying distribution of the population. I suspect it might be problematic; think about using bootstrap for building a confidence interval around a minimum (or a maximum).But perhaps this approach is flawed? Please let me know.EDIT: Having thought about the problem a little more, I see that my solution implies the following: the empirical 1st percentile of the original sample may be a biased estimator of the 1st percentile of the population. And if so, the point estimate should be bias-adjusted: . Otherwise the bias-adjusted confidence interval would not be compatible with the bias-unadjusted point estimate. I need to adjust either both the point estimate and the confidence interval or none of them. If, on the other hand, I did not allow for the estimate to be biased, I would not have to do the bias adjustment. That is, I would take  as the point estimate and  as the lower end and  as the upper end of the 95% confidence interval. I am not sure whether this interval makes sense...So does it make any sense to assume that the sample 1st percentile is a biased estimate of the population 1st percentile? And if not, is my alternative solution correct?","Creater_id":53690,"Start_date":"2014-10-12 02:01:37","Question_id":119748,"Tags":["confidence-interval","bootstrap","quantiles","extreme-value"],"Answer_count":2,"Last_activity":"2016-08-06 03:31:13","Link":"http://stats.stackexchange.com/questions/119748/using-bootstrap-to-obtain-sampling-distribution-of-1st-percentile","Creator_reputation":13002}
{"_id":{"$oid":"5837a583a05283111e4d5f6c"},"View_count":678,"Display_name":"Richard Hardy","Question_score":0,"Question_content":"I wonder if a GARCH model with only \"autoregressive\" terms and no lagged innovations makes sense. I have never seen examples of GARCH(,0) in the literature. Should the model be discarded altogether? E.g. GARCH(1,0):  \\sigma^2_t = \\omega + \\delta \\sigma^2_{t-1}.   From the above expression one can derive (by repeated substitution) that \\sigma^2_t \\rightarrow \\frac{ \\omega }{ 1-\\delta } for all , if an infinite past of the process is assumed. In other words, GARCH(1,0) implies homoskedasticity and thus the \"autoregressive\" term, and indeed the whole model, becomes redundant. Edit:My argumentation in the paragraph above was imprecise and likely misleading. The point I was trying to make (and John's answer below helped me realize and formulate it better) is that whatever the initial conditional variance is, after a long enough time the conditional variance will stabilize around the level . However, it will at the same time obey the law of motion . The two can only be reconciled with  and . The latter implies constant conditional variance. Hence, GARCH(1,0) only makes sense when  and , which means the whole GARCH model is redundant as the conditional variance is constant.(End of edit)Of course, when estimating models in practice, we do not have infinite past; but for long enough time series this approximation should be reasonably representative.Is this right? Should we never use GARCH(,0)?","Creater_id":53690,"Start_date":"2014-08-26 07:31:14","Question_id":113294,"Tags":["garch","volatility-forecasting"],"Answer_count":3,"Last_activity":"2016-08-06 03:09:34","Link":"http://stats.stackexchange.com/questions/113294/does-garchp-0-make-sense-at-all","Creator_reputation":13002}
{"_id":{"$oid":"5837a583a05283111e4d5f7b"},"View_count":66,"Display_name":"Oleg","Question_score":0,"Question_content":"I've had calculus 1 and 2 also numerical analysis but I can't seem to organize the topics from the books needed for a decent machine learning knowledge.Could you please give some materials for such a beginner ?So far I considered 2 books :The Elements of Statistical learningStatistics by Freedman, Pisani and PurvesWhich one I should read first ?(Given that I already passed the courses above)Elements of Statistical learning is harder and I wonder if it's reasonable to start with this book.Of course my main goal is to implement/build such kind of learning algorithms not only know how to use them. Therefore I assume that a book with a theory focus not linked to any programming language would suit me best.","Creater_id":108326,"Start_date":"2016-08-06 02:19:30","Question_id":228544,"Tags":["machine-learning"],"Answer_count":0,"Last_activity":"2016-08-06 02:19:30","Link":"http://stats.stackexchange.com/questions/228544/prerequisites-for-machine-learning","Creator_reputation":43}
{"_id":{"$oid":"5837a583a05283111e4d5f7d"},"View_count":183,"Display_name":"crazydriver","Question_score":3,"Question_content":"I know this probably has been discussed somewhere else, but I have not been able to find an explicit answer. I am trying to use the formula  to calculate out-of-sample  of a linear regression model, where  is the sum of squared residuals and  is the total sum of squares. For the training set, it is clear that SST = \\Sigma (y - \\bar{y}_{train})^2 What about the testing set? Should I keep using  for out of sample , or use  instead? I found that if I use , the resulting  can be negative sometimes. This is consistent with the description of sklearn's r2_score() function, where they used  (which is also used by their linear_model's score() function for testing samples). They state that \"a constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\" However, in other places people have used  like here and here (the second answer by dmi3kno). So I was wondering which makes more sense? Any comment will be greatly appreciated!","Creater_id":126800,"Start_date":"2016-08-05 23:32:09","Question_id":228540,"Tags":["regression","r-squared","out-of-sample"],"Answer_count":0,"Last_activity":"2016-08-05 23:32:09","Link":"http://stats.stackexchange.com/questions/228540/how-to-calculate-out-of-sample-r-squared","Creator_reputation":16}
{"_id":{"$oid":"5837a583a05283111e4d5f7f"},"View_count":88,"Display_name":"Ramesh-X","Question_score":3,"Question_content":"I'm new to HMM and still learning. I'm currently using HMM to tag part-of-speech. To implement the viterbi algorithm I need transition probabilities () and emission probabilities ().I'm generating values for these probabilities using supervised learning method where I give a sentence and its tagging. I calculate emission probabilities as:b_i(o) = \\frac{\\Count(i \\to o)}{\\Count(i)}where  is the number of times tag  occurs in the training set and  is the number of times where the observed word  maps to the tag .But when using this trained  for tagging, there might be observed variables in the given sentence that never appeared when finding the value for . In such a case how do you estimate a value for  for that instance?","Creater_id":115826,"Start_date":"2016-05-16 20:28:44","Question_id":212961,"Tags":["probability","hidden-markov-model","viterbi-algorithm"],"Answer_count":1,"Last_activity":"2016-08-05 23:20:47","Link":"http://stats.stackexchange.com/questions/212961/calculating-emission-probability-values","Creator_reputation":121}
{"_id":{"$oid":"5837a583a05283111e4d5f8c"},"View_count":23,"Display_name":"J. Ferreira","Question_score":1,"Question_content":"When comparing 3 different groups on a certain variable and one of the groups does not have a normal distribution (as verified by the Shapiro test) whilst the other to do, would you think it is best to compare both groups with normal distributions using a parametric test and then conduct the other comparisons (between the groups with normal and not normal distribution) using the nonparametric tests or do you think it would be best to just use nonparametric testing for all comparisons?To make this clear:group A has a normal distributiongroup B has a normal distributiongroup c does not have a normal distributionDo you think it would be best to use parametric testing when comparing the performance of group A and B and use nonparametric testing when comparing the performance of group A and C /B and C? Or should we just use non parametric testing for groups A, B and C all together?","Creater_id":121826,"Start_date":"2016-08-03 16:18:28","Question_id":227178,"Tags":["nonparametric","multiple-comparisons","manova","parametric"],"Answer_count":0,"Last_activity":"2016-08-05 22:18:08","Link":"http://stats.stackexchange.com/questions/227178/group-comparisons-normality-of-the-distribution","Creator_reputation":75}
{"_id":{"$oid":"5837a583a05283111e4d5f8e"},"View_count":1075,"Display_name":"Daniel","Question_score":4,"Question_content":"I'm using R and the lme4 package to compute mixed effects models with binary outcome (glmer). I have included continuous coefficients (e.g. how many hours per week does a person care for an elderly relative) and - as it is a comparison of 6 countries - country as random intercept (see my question here for details on the model).Now I'd like to interpret the fixed effects with consideration of the random effects (country intercept).I do this already for fixed effects without considering random effects, by multiplying each \"x\" value from the coefficients with their related estimates (xbeta, first line in sample code below) and then have a formula to convert intercept of fixed effects + each xbeta from odds ratio to probabilities:mydf.valsvalue * (fixef(fit)[coef.pos])mydf.valsxbeta))))(this is roughly the same approach as described like in this question)Now my question is: If I'd like to see how a coefficients / an odds ratio varies between countries (random intercept), would it be correct to retrieve random effects (with ranef) to get the estimates (intercepts) for each country level and then repeat the above formula for each country level?for example:ranef(fit)prob \u0026lt;- (1/(1+exp(-(rand.ef[i, ] + mydf.valsReaction.dicho \u0026lt;- ifelse(sleepstudyReaction.dicho, na.rm=T),0,1)# fit modelfit \u0026lt;- glmer(Reaction.dicho ~ Days + (1 | Subject),             sleepstudy,             family = binomial(\"logit\"))# plot random effects and probability curve of fixed effectssjp.glmer(fit, showContPredPlots = T)The above code produces following two plots:The first plot shows the random effects as retrieved by ranef, plus conf.int, which are retrieved by arm::se.ranef * 1.96.The \"probability curve\" in the 2nd plot is calculated by multiplying fixed-effects-intercept (-3.4601, see summary(fit)) with each value from Days (range from 0 to 9) multiplied with Days's estimate.Example:x1 = (1 / (1 + exp(-(-3.4601 + 0 * 0.7426))))x2 = (1 / (1 + exp(-(-3.4601 + 1 * 0.7426))))# and so on, from 0 to 9       ^ hereNow, this \"probability curve\" is only based on fixed effects. My question is, if it would make sense to plot this curve for each \"intercept estimate\" from the random effects, i.e. to use the above formula and replace -3.4601 with each random effects value (which are, in this case, random intercepts, i.e. the intercepts of each group level).Would this be an appropriate way to interpret the \"differences\" or variance of ´Days` in each group?EDIT 2: Another exampleLet me give a comprehensive example of what I want to do:library(lme4)library(reshape2)library(ggplot2)# create binary responsesleepstudyReaction \u0026lt;= median(sleepstudyDays))# melt variablemydf.vals \u0026lt;- data.frame(melt(vals.unique))# add \"counter\" from 1 to length of unique vals# in this particular case, \"Days\" is also a \"normal\" sequence,# so there's not much benefit here - however, if you have e.g.# \"workhours per week\", you may have values from 20 to 80, with certain# values not in the data (if no one works 25 hours). In that case, # the following \"counter\"-sequence makes sensemydf.vals \u0026lt;- cbind(seq(from = 1, to = nrow(mydf.vals), by = 1), mydf.vals)# set colnames. x = x-axis value, \"value\" is the \"real\" data value,# which was observedcolnames(mydf.vals) \u0026lt;- c(\"x\", \"value\")# calculate x-beta by multiplying original values of \"Days\" # with estimate of \"Days\"mydf.valsvalue * fixef(fit)[2]# the data frame for plottingfinal.df \u0026lt;- data.frame()final.grp \u0026lt;- c()# example only for the first 6 grouping levels,# plot would else be too overloaded...for (i in 1 : 6) {  # y-value (probability of odds ratio), by adding x-betas from  # \"Days\" to each random intercept estimate (estimate for each  # group level)  mydf.valsxbeta))))  final.df \u0026lt;- rbind(final.df, cbind(days = mydf.valsprob))  # need to add grp vector later to data frame,  # else \"x\" and \"prob\" would be coerced to factors  final.grp \u0026lt;- c(final.grp,                  rep(row.names(rand.ef)[i], times = length(mydf.valssubject \u0026lt;- final.grp# plot probability curve here...ggplot(final.df, aes(x = days, y = prob, colour = subject)) +  geom_point() +  geom_line()Above we see the prob. curves for each Subject along each Days value, calculated by \"summed\" odds ratio of each Subject`s intercept + each Days' estimates. So, I \"linked\" random effects and fixed effects.Is this ok, or is it nonsense to do that? My aim is to say: Taking Subject variance into account, we see a delay in reaction time for Subject 310, while for subject 331 it is much more likely to have high reactions.","Creater_id":54740,"Start_date":"2014-11-12 04:23:49","Question_id":123703,"Tags":["probability","mixed-model","lme4","odds-ratio"],"Answer_count":1,"Last_activity":"2016-08-05 22:03:59","Link":"http://stats.stackexchange.com/questions/123703/probabilities-of-odds-ratios-in-random-intercept-models","Creator_reputation":372}
{"_id":{"$oid":"5837a583a05283111e4d5f9b"},"View_count":46,"Display_name":"BGTP33","Question_score":4,"Question_content":"For standard normal Z, the change in probability density associated with equally sized changes in z is obviously greater for values of z that are further away from the mean/mode. For example, if z=0, then . However, if z=2, then . Thus the density change associated with increasing z by 0.01 is 5 times as large in the latter case. I have two questions related to this. Does this property (or a similar property) have a name? (That is, the property where for rv X with PDF f and mean value :  when .) Does this general idea - that is the extent to which the PDF's derivative changes over the distribution's support - have a name?Are probability distributions with this property (or a similar property) systematically classified?  Can they be identified in some straightforward way? Obviously, all normal distributions have it, but do all unimodal continuous finite distributions?I am asking because I am working with a finding that is dependent on the above property and I'd like to figure out the best way to succinctly talk about it, and also how to accurately think and report about its generality (or lack thereof). ","Creater_id":42627,"Start_date":"2016-08-04 22:29:18","Question_id":228384,"Tags":["distributions","mathematical-statistics","pdf","elasticity"],"Answer_count":1,"Last_activity":"2016-08-05 20:33:50","Link":"http://stats.stackexchange.com/questions/228384/name-for-probability-density-elasticity-property","Creator_reputation":50}
{"_id":{"$oid":"5837a583a05283111e4d5fa8"},"View_count":19,"Display_name":"M.S.","Question_score":1,"Question_content":"I have a cancer patient dataset  which consists of numerous features and two different sets of labels  \u0026amp; . I want to show that  is a better clustering of patients than ; the groups that  creates are more homogenous.I also have another dataset  which has no overlap in terms of features with  but has a strong biological correlation with it. This means that I should be able to use  as an external validator for my sets of labels  \u0026amp; .Is it valid to fit  to a supervised classifier and try to predict  or  and see which experiment does better in terms of classificatory accuracy? If not, what could be an alternative?","Creater_id":122592,"Start_date":"2016-08-05 18:06:19","Question_id":228529,"Tags":["machine-learning","clustering","k-means","biostatistics","validation"],"Answer_count":0,"Last_activity":"2016-08-05 18:06:19","Link":"http://stats.stackexchange.com/questions/228529/cluster-validation-using-supervised-learning","Creator_reputation":20}
{"_id":{"$oid":"5837a583a05283111e4d5faa"},"View_count":1331,"Display_name":"Remi D","Question_score":3,"Question_content":"I have been reading a lot about Dynamic Time Warping (DTW) lately. I am very surprised that there is no literature at all on the application of DTW to irregular time series, or at least I could not find it.Could anybody give me a reference to something related to that issue, or maybe even an implementation of it?","Creater_id":43464,"Start_date":"2014-07-25 05:28:41","Question_id":109343,"Tags":["time-series","correlation","distance"],"Answer_count":4,"Last_activity":"2016-08-05 16:20:18","Link":"http://stats.stackexchange.com/questions/109343/dynamic-time-warping-for-irregular-time-series","Creator_reputation":35}
{"_id":{"$oid":"5837a583a05283111e4d5fba"},"View_count":263,"Display_name":"madsthaks","Question_score":2,"Question_content":"Let me preface this by saying I'm new to statistics. I'm working with regression models, attempting to understand transformations a bit more. I'm modeling (Y~X) and I get an  of 0.4. I see that the residuals of this plot are left skewed so I take (Y^2~X) assuming that would correct the issue but now my  is 0.3. Just out of curiosity, I did (Log(Y)~X and got an  of 0.5.I'm really not sure what is going on and not sure what transformation I should use going forward. ","Creater_id":125449,"Start_date":"2016-08-03 18:02:56","Question_id":228184,"Tags":["r","regression","data-transformation","linear"],"Answer_count":2,"Last_activity":"2016-08-05 16:06:00","Link":"http://stats.stackexchange.com/questions/228184/understanding-the-transformation-on-response-variable","Creator_reputation":45}
{"_id":{"$oid":"5837a583a05283111e4d5fc7"},"View_count":24,"Display_name":"Kodiakflds","Question_score":0,"Question_content":"I have been going back and forth with a coworker on what the most appropriate test might be for our data. We are looking at two different survey techniques/tools for observing fish. One idea was to use the odds ratio. As I've seen it in medical applications, it is often used to look at Presence/Absence of a disease, given the Presence/Absence of a drug. In the example below, I have simulated data for Presence/Absence of a given species for each survey technique (2 techniques) at 100 sites. My thought was that I could look at whether the Presence/Absence of Species A in the first survey technique had any relationship to seeing the species in the second survey technique (or vis versa).Basically I'm asking if the Presence/Absence of two variables can be simply compared with odds ratios. library(devtools)# install_github('mjwestgate/sppairs')library(sppairs) #for or.asymmetric() and or.glmlibrary(epitools) #for oddsratio()set.seed(5)tool.1=rbinom(100,1,.5)set.seed(4)tool.2=rbinom(100,1,.5)Survey.Data=data.frame(tool.1,tool.2)t=table(Survey.Datatool.2)chisq.test(t) #Indicates that there is a dependence between sets of variablesoddsratio(t)  #from epitools package#or this function from sppairs packageor.symmetric(Survey.Data)The odds ratio here is 2.7- So I believe that the odds of seeing this particular species with survey tool #1 is 2.7 times greater when I've observed the species with the second survey tool as well. Any thoughts on whether this seems a valid use of odds ratios? ","Creater_id":120950,"Start_date":"2016-08-05 15:44:07","Question_id":228520,"Tags":["probability","odds-ratio","ratio","odds"],"Answer_count":0,"Last_activity":"2016-08-05 15:44:07","Link":"http://stats.stackexchange.com/questions/228520/odds-ratio-is-it-the-right-analysis","Creator_reputation":13}
{"_id":{"$oid":"5837a583a05283111e4d5fc9"},"View_count":18,"Display_name":"ihadanny","Question_score":1,"Question_content":"I'm trying to follow Andrew NG cs course on supervised learning. He defines the exponential family as: p(y;\\eta) = b(y)exp(\\eta T(y) -a(\\eta))and then continues to say that \"our goal is to predict the expected value of  given \", and that \"the canonical response function is \"Why is that? why do we try to predict the mean of some  we don't care anything about instead of trying to predict ?In a Canadian stat course, I found another definition of the exponential family: f_{Y}(y;\\theta)=exp(yb(\\theta) + c(\\theta) + d(y))  and there they clearly say: \"In a GLM, the relationship between a function of  and the parameters is linear.\"So who's right here? what are we trying to model and what's the intuition behind it? is  an important concept to understand - and if so what's the intuition behind it as well?Thanks...","Creater_id":37793,"Start_date":"2016-08-05 15:01:12","Question_id":228516,"Tags":["generalized-linear-model","intuition"],"Answer_count":0,"Last_activity":"2016-08-05 15:01:12","Link":"http://stats.stackexchange.com/questions/228516/in-glm-do-we-try-to-model-ety-or-ey","Creator_reputation":280}
{"_id":{"$oid":"5837a583a05283111e4d5fcb"},"View_count":137,"Display_name":"Devil","Question_score":6,"Question_content":"Given \\left(\\begin{array}{c} X_1 \\\\ X_2 \\end{array}\\right) \\sim \\mathcal{N} \\left(\\left(\\begin{array}{c} 0 \\\\ 0 \\end{array}\\right), \\left(\\begin{array}{cc} 1 \u0026amp; \\rho \\\\ \\rho \u0026amp; 1 \\end{array}\\right)\\right),I want to show that \\mathbb{E}\\left[ \\text{sign}(X_1) \\text{sign}(X_2)\\right] = \\frac{2}{\\pi}\\sin^{-1}(\\rho) This seems to be the essence of the proof of Geomans-Williamson MAX-CUT SDP relaxation. Is there an easy way to see it?","Creater_id":51945,"Start_date":"2016-08-02 12:25:09","Question_id":226940,"Tags":["probability","correlation","normal-distribution"],"Answer_count":2,"Last_activity":"2016-08-05 14:40:35","Link":"http://stats.stackexchange.com/questions/226940/correlation-of-signs-of-a-jointly-gaussian-rv","Creator_reputation":250}
{"_id":{"$oid":"5837a583a05283111e4d5fd9"},"View_count":20,"Display_name":"Fico","Question_score":0,"Question_content":"Does anyone know how to test the assumption of linearity for a multinomial regression? This means the dependant variabel has more than 2 categories. There are several independant continues and categorical variables.I know the 'Box-Tidwell (1962) procedure to test for linearity' can be performed for a binomial logistic regression. But I can't figure out how to do this for a multinomial regression.","Creater_id":85555,"Start_date":"2016-08-05 14:32:17","Question_id":228512,"Tags":["multiple-regression","spss","multinomial"],"Answer_count":0,"Last_activity":"2016-08-05 14:32:17","Link":"http://stats.stackexchange.com/questions/228512/multinomial-logistic-regression-assumption-of-linearity","Creator_reputation":106}
{"_id":{"$oid":"5837a583a05283111e4d5fdb"},"View_count":5,"Display_name":"Theodor","Question_score":0,"Question_content":"I am primarily interested in selecting a model given a large number of covariates, which are all measured on a Likert-type scale with 5 levels, where level 0 means \"no  symptom\", up to level 4 which means \"total symptom\". I want to try out different model selection procedures, but even the univariate models take a very long time to estimate (the model is a GLMM with a number of random effects). As such, which one would be the better idea:Re-label the variables into new binary variables, with 0 for \"no symptom\" and 1 for \"any kind of symptoms\". Treat the variables as continuousI know that both are somewhat wrong, but I am just curious which one you think is more wrong. ","Creater_id":55082,"Start_date":"2016-08-05 14:02:28","Question_id":228509,"Tags":["categorical-data","glmm","optimal-scaling"],"Answer_count":0,"Last_activity":"2016-08-05 14:02:28","Link":"http://stats.stackexchange.com/questions/228509/simplifying-ordinal-factor-variable","Creator_reputation":1041}
{"_id":{"$oid":"5837a583a05283111e4d5fdd"},"View_count":59,"Display_name":"Makers_F","Question_score":1,"Question_content":"I have a corpus of publications in CS divided by year.What I'd like to discover from it isThe subject (only one) of each article ( for example testing, software engineering, networking, architecture, etc)Does an article not fall in any subject (so is it a new subject, at least compared to the subjects of the previous year)?Right now I'm experimenting with LDA, butThe topic it finds are not easily mapped with the ones I'd like to have (for example some topics are general, like about writing papers, or performing experiments. Some other are really hard to understand what they are about. In general they will not be as clearly defined as the ones is stated before)I need to provide the number of topics. I'd like to discover if there was a change in topic from one year to the other, but as long as I provide the number of topics LDA will always find that number topicsEach document belongs to multiple topics, while I'd like to give just one label (even if that means losing information and doing some kind of approximation when there is a paper which might fall in 2 or more labels)Of course these are drawbacks which are known and expected from the algorithm, but I'm not sure what alternatives there are to make up for them.I can not label the collection (what I'm trying to do is exactly labeling the collection in an automatic way), but I can provide papers (which are not in the collection) about a known subject.What are the algorithms best suited for this kind of task?I'm considering LDA (which I'm testing now, in addition to a clustering-LDA algorithm), LSA and LSI.How can the latter 2 score compared to LDA?I'd need algorithms which are already available in some kind of library/framework (like gensim).To try and label my documents I'm thinking about training LDA over my corpus, then transforming documents on a specific subject with the trained LDA, and looking for the documents in my corpus which better match that distribution (for example I transform 100 papers from about testing and look for papers in my corpus with a topic distribution similar to those).Could this method work (or will it produce rubbish)? Does exist a (sound) algorithm that does something similar to that?","Creater_id":96853,"Start_date":"2015-12-01 22:25:56","Question_id":184592,"Tags":["classification","clustering","topic-models"],"Answer_count":1,"Last_activity":"2016-08-05 14:00:03","Link":"http://stats.stackexchange.com/questions/184592/what-method-for-grouping-documents-by-topic","Creator_reputation":111}
{"_id":{"$oid":"5837a583a05283111e4d5fe9"},"View_count":59,"Display_name":"Repmat","Question_score":2,"Question_content":"I am working with matching as descibed in this paper (*.pdf). The dataset I use is quite large, I therefore had to extract a (sub)-sample from it, in order to actually get anywhere. I am using the MatchIt package in R (written in conjuction with the above article). I use nearest neighbor matching, matching on the propensity score estimated from a logit model.Now I have been wondering; Since the estimation of the logit model is quite fast (2min for 8,000,000 obs), and the mathcing search is very slow, would it be possible to parallelize the matching algorithm? Using multiple CPU's, to speed up the process?I realize that this is not possible in the package, as it stands now, but could it work in theory? Psudo-code, or quick run-down would be greatly appreciated.","Creater_id":71364,"Start_date":"2015-06-03 13:41:19","Question_id":155394,"Tags":["matching"],"Answer_count":0,"Last_activity":"2016-08-05 13:39:20","Link":"http://stats.stackexchange.com/questions/155394/is-it-possible-to-parallelize-a-matching-method","Creator_reputation":1909}
{"_id":{"$oid":"5837a583a05283111e4d5feb"},"View_count":35,"Display_name":"lemon","Question_score":4,"Question_content":"The hypergeometric distribution is given by P(X=k) = \\frac{\\binom Kk \\binom {N-K}{n-k}}{\\binom Nn} Is there a nice expression for the less-than probability, ?Or am I just going to have to numerically evaluate the sum ?","Creater_id":44526,"Start_date":"2016-08-05 12:30:15","Question_id":228497,"Tags":["probability","distributions","hypergeometric"],"Answer_count":1,"Last_activity":"2016-08-05 12:50:13","Link":"http://stats.stackexchange.com/questions/228497/hypergeometric-distribution-less-than-pxk","Creator_reputation":165}
{"_id":{"$oid":"5837a583a05283111e4d5ff8"},"View_count":44,"Display_name":"statsGuy","Question_score":1,"Question_content":"So here is my problem....At the start of a week, a coal mine has a high-capacity storage bin that is half full. During the week, 20 loads of coal are added to the storage bin. Each load of coal has a volume that is normally distributed with mean 1.50 cubic yards and standard deviation0.25 cubic yards.During the same week, coal is removed from the storage bin and loaded into 4 railroad cars. The amount of coal loaded into each railroad car is normally distributed with mean 7.25 cubic yards and standard deviation 0.50 cubic yards.The amounts added to the storage bin or removed from the storage bin are mutually independent.Calculate the probability that the storage bin contains more coal at the end of the week than it had at the beginning of the week.Add=A~Remove=R~I am applying the following linear transformation:My approach:My problem: Solution shows Their solution:With each load of coal having mean 1.5 and standard deviation 0.25, twenty loads have a meanof 20(1.5) = 30 and a variance of 20(0.0625) = 1.25. The total amount removed is normal withmean 4(7.25) = 29 and standard deviation 4(0.25) = 1. The difference is normal with mean 30 –29 = 1 and standard deviation sqrt(1.25 + 1) = 1.5. If D is that difference, then My QuestionIs the solution wrong? I thought that This is from #235 in SOA handbook for probability problems.","Creater_id":114713,"Start_date":"2016-08-05 11:47:07","Question_id":228489,"Tags":["variance","expected-value"],"Answer_count":1,"Last_activity":"2016-08-05 12:46:29","Link":"http://stats.stackexchange.com/questions/228489/linear-transformations-of-mean-and-variance","Creator_reputation":30}
{"_id":{"$oid":"5837a583a05283111e4d6005"},"View_count":1307,"Display_name":"Werner","Question_score":5,"Question_content":"I'd like to forecast (or predict) a time series with weights.The following works using the regular linear modelling techniques of lm by applying a (sigmoidal) weight distribution to the input data, essentially weighing the latter data points more heavily than the former:library(\"stats\")lm.weight.function \u0026lt;- function(x) {10 / (1 + exp(-x))} # Sigmoidallm.weights \u0026lt;- lapply(seq(-13, 14, length.out = 27), lm.weight.function)lm.input \u0026lt;- as.data.frame(c(23957, 46771, 60767, 73284, 60296, 73122, 78304, 87154, 80459, 76885, 56479, 18809, 13453, 13951, 25140, 12035, 11920, 20683, 30357, 35019, 37732, 46150, 47856, 41931, 20985, 32526, 27283))lm.input \u0026lt;- cbind(1:27, lm.input)colnames(lm.input) \u0026lt;- c('x', 'y')lm.model \u0026lt;- lm(formula = y ~ log(x), data = lm.input, weights = unlist(lm.weights))predict.input \u0026lt;- as.data.frame(28:55)colnames(predict.input) \u0026lt;- 'x'predict.model \u0026lt;- predict(lm.model, predict.input)plot(1:(27+28), c(lm.input$y, predict.model), type = 'l', xlab = 'x', ylab = 'y')Now I wish to do the same using the forecast package. However, I'm having difficulty specifying the weights:library(\"forecast\")ts.weight.function \u0026lt;- function(x) {10 / (1 + exp(-x))} # Sigmoidalts.weights \u0026lt;- as.data.frame(lapply(seq(-13, 14, length.out = 27), ts.weight.function))colnames(ts.weights) \u0026lt;- 'trend'ts.input \u0026lt;- ts(c(23957, 46771, 60767, 73284, 60296, 73122, 78304, 87154, 80459, 76885, 56479, 18809, 13453, 13951, 25140, 12035, 11920, 20683, 30357, 35019, 37732, 46150, 47856, 41931, 20985, 32526, 27283), frequency = 1)ts.model \u0026lt;- tslm(formula = ts.input ~ log(trend), weights = unlist(ts.weights))The above prints an error:Error in eval(expr, envir, enclos) :   ..1 used in an incorrect context, no ... to look inHow can I use tslm to forecast a time series with weights?","Creater_id":13438,"Start_date":"2014-07-02 16:53:02","Question_id":105605,"Tags":["r","time-series","forecasting"],"Answer_count":1,"Last_activity":"2016-08-05 12:39:11","Link":"http://stats.stackexchange.com/questions/105605/forecasting-a-time-series-with-weights","Creator_reputation":150}
{"_id":{"$oid":"5837a583a05283111e4d6012"},"View_count":31,"Display_name":"user3294195","Question_score":4,"Question_content":"Given design matrix  and response vector , I want to find the variance-covariance matrix of the coefficients  from an -regularized logistic regression with regularization parameter . If I understand correctly, there is no closed-form solution for this as the penalized log-likelihood function  is not differentiable. Approximations have been proposed in Tibshirani (1996), and Li and Fan (2001), but these only apply to the non-zero coefficients in . Bootstrap and Bayesian lasso (2010) are other methods to compute standard errors, but I think they would be too computationally intensive for my purposes. There is a question here with an answer that only applies to -regularized logistic regression. Are there any other fast and accurate alternatives for variance-covariance matrix computation of the coefficients in frequentist lasso logistic regression?Thanks.","Creater_id":43444,"Start_date":"2016-08-05 12:14:29","Question_id":228495,"Tags":["logistic","references","lasso","regularization"],"Answer_count":0,"Last_activity":"2016-08-05 12:28:26","Link":"http://stats.stackexchange.com/questions/228495/variance-covariance-matrix-for-l-1-regularized-binomial-logistic-regression","Creator_reputation":173}
{"_id":{"$oid":"5837a583a05283111e4d6014"},"View_count":56,"Display_name":"Waqas","Question_score":3,"Question_content":"On the plot black is the data and red are the fitted values obtained from fitted i.e. one step forecast, I am using 365 days for training and then 3000+ days for testing, I choose value of k using cross-validation on 365 data points. Following is the model I used:Arima(data, order=c(2,0,2),xreg=forecast::fourier(min_temp_aus,57))How can I improve the fit on both extremes?PS: Square loss is of 17524 considering I am predicting 3000+ data points. The way I am looking at it is, if I am off by 1 with every prediction still it makes a loss of 3000. I thought it is good, but maybe I am wrong.","Creater_id":119117,"Start_date":"2016-08-04 18:21:20","Question_id":228364,"Tags":["r","time-series","arima","fourier-transform"],"Answer_count":1,"Last_activity":"2016-08-05 12:21:16","Link":"http://stats.stackexchange.com/questions/228364/arima-fourier-for-periodic-data-improvement-suggestions","Creator_reputation":37}
{"_id":{"$oid":"5837a583a05283111e4d6021"},"View_count":47,"Display_name":"Ray ben ","Question_score":0,"Question_content":"One of the most using algorithm for clustering text documents that represented by VSM is k-means this notes according to Agrawal(Text mining book 2016 2nd edition); I would like to know if its possible to used Cosine Measure with K-means to measure similarity and clustering Docs that represented in high dimensional vectors (BOW),so, Is possible to used Cosine similarity and what is the formula of this metric when applying under high dimensional representation to give an effort measures? ","Creater_id":105797,"Start_date":"2016-08-05 03:34:44","Question_id":228419,"Tags":["clustering","k-means","high-dimensional","cosine-similarity"],"Answer_count":1,"Last_activity":"2016-08-05 12:12:55","Link":"http://stats.stackexchange.com/questions/228419/can-we-apply-k-means-algorithm-with-cosine-similarity-measure","Creator_reputation":9}
{"_id":{"$oid":"5837a583a05283111e4d602e"},"View_count":231,"Display_name":"rano","Question_score":2,"Question_content":"I am trying to apply glasso on a very simple as well as sparse dataset made by 60+ features and 30k+ observations. Here you can find it in a csv format, if you are interested in reproducing the issue.I am using the sklearn implementation with very few lines of code, by trying different values for the regularization coefficient :for alpha in [0.00000001, 0.0000001, 0.000001, 0.00001, 0.0001]:    glasso_model = GraphLasso(alpha=alpha, mode='lars', max_iter=2000)    glasso_model.fit(scaled_train)What I am experiencing is that the model cannot fit a covariance estimate since it stops after raising an exception complaining about the non PSD nature of the problem:/usr/local/lib/python3.4/dist-packages/sklearn/covariance/graph_lasso_.py in graph_lasso(emp_cov, alpha, cov_init, mode, tol, max_iter, verbose, return_costs, eps, return_n_iter)    245         e.args = (e.args[0]    246                   + '. The system is too ill-conditioned for this solver',)--\u0026gt; 247         raise e    248     249     if return_costs:/usr/local/lib/python3.4/dist-packages/sklearn/covariance/graph_lasso_.py in graph_lasso(emp_cov, alpha, cov_init, mode, tol, max_iter, verbose, return_costs, eps, return_n_iter)    236                 break    237             if not np.isfinite(cost) and i \u0026gt; 0:--\u0026gt; 238                 raise FloatingPointError('Non SPD result: the system is '    239                                          'too ill-conditioned for this solver')    240         else:FloatingPointError: Non SPD result: the system is too ill-conditioned for this solver. The system is too ill-conditioned for this solverIf I try to do an mle of the covariance with another function by sklearn (which is btw the same function that the graph_lasso procedure uses), this matrix is indeed PSD. So, I suspect that the problem lies somewhere in the computation of the code.Now I am normalizing or standardazing the data (zero mean, 1.0 var) the data before applying the method but the problem still persist.Any idea about it? Am I missing some keypoint in applying the glasso. Is it possible to do something meaningful with another toolkit?","Creater_id":14101,"Start_date":"2015-09-17 05:42:24","Question_id":172911,"Tags":["regression","lasso","graphical-model","scikit-learn"],"Answer_count":1,"Last_activity":"2016-08-05 11:38:35","Link":"http://stats.stackexchange.com/questions/172911/graphical-lasso-numerical-problem-not-spd-matrix-result","Creator_reputation":134}
{"_id":{"$oid":"5837a583a05283111e4d603b"},"View_count":26,"Display_name":"Matt Brenneman","Question_score":1,"Question_content":"Minor question, but I am wondering if there is a convention in how we say the order of variables in a chi square test for association between 2 categorical variables, say X and Y. In regression, for example, the order of the variables in the phrase \"regress Y on X\" indicates which is the response and which is the explanatory variable. Similarly, if we had a case where we thought of one categorical var (say X) as potentially causing an effect in the other categorical var (i.e. Y), would we choose to say \"we are testing for an association between X and Y\" rather than \"we are testing for an association between Y and X\"?I know technically the order doesn't matter (both as far as the test is concerned and also in the conjunction used in the phrase itself) and of course if you simply call it the \"chi-square test for independence\" the issue is moot, but I was just wondering.Thanks,Matt","Creater_id":36862,"Start_date":"2016-08-05 10:45:42","Question_id":228483,"Tags":["hypothesis-testing","categorical-data","inference","education"],"Answer_count":0,"Last_activity":"2016-08-05 11:10:59","Link":"http://stats.stackexchange.com/questions/228483/order-of-variables-in-chi-square-test","Creator_reputation":217}
{"_id":{"$oid":"5837a583a05283111e4d603d"},"View_count":6230,"Display_name":"Milktrader","Question_score":19,"Question_content":"Inter-market analysis is a method of modeling market behavior by means of finding relationships between different markets. Often times, a correlation is computed between two markets, say S\u0026amp;P 500 and 30-Year US treasuries. These computations are more often than not based on price data, which is obvious to everyone that it does not fit the definition of stationary time series. Possible solutions aside (using returns instead), is the computation of correlation whose data is non-stationary even a valid statistical calculation?Would you say that such a correlation calculation is somewhat unreliable, or just plain nonsense?","Creater_id":3306,"Start_date":"2011-02-18 06:07:06","Question_id":7376,"Tags":["correlation","stationarity"],"Answer_count":2,"Last_activity":"2016-08-05 11:06:20","Link":"http://stats.stackexchange.com/questions/7376/does-correlation-assume-stationarity-of-data","Creator_reputation":352}
{"_id":{"$oid":"5837a583a05283111e4d604b"},"View_count":38,"Display_name":"Gil Or","Question_score":4,"Question_content":"I am interested in the following problem - Suppose I have  independent random variables, with CDFs  accordingly (i.e  has CDF ). In addition, let's suppose that it is easy to sample from each .What is the simplest way to generate, say 100, independent random numbers from the distribution ?I'm thinking about Accept-Reject algorithm.  Any suggestions?","Creater_id":125465,"Start_date":"2016-08-03 00:25:42","Question_id":227008,"Tags":["random-variable","random-generation","cdf"],"Answer_count":1,"Last_activity":"2016-08-05 10:57:55","Link":"http://stats.stackexchange.com/questions/227008/generating-random-numbers-from-a-multiplication-of-cdfs","Creator_reputation":21}
{"_id":{"$oid":"5837a583a05283111e4d6058"},"View_count":21,"Display_name":"Ritwik","Question_score":1,"Question_content":"I have a population of 1000+ values of which I choose 300+ as my sample. I repeat this 3000 times.I calculate for each experiment the estimated x-bar, SE(x-bar) as s/√n and using a normal distribution and a confidence value of (say) 90%, I calculate the interval.Now I check whether the actual population mean lies within this interval in each experiment and it turns out that in ~90.7% cases of the 3000+, it does, as was expected.Similar results are obtained for proportion (~89.5% times when 90% was chosen)But when I do the same with Standard Deviation/Variance using a chi-square distribuion, with degrees of freedom (say) 299, and confidence level of 80%, using (n-1)s^2/chi-square, the actual value lies in the range for almost 98% of the experiments!Calculations are done in excel so I'm stumped as to why the discrepancy is coming up. Is it because the population needs to be normally distributed for chi-square intervals to work?","Creater_id":126762,"Start_date":"2016-08-05 10:50:35","Question_id":228484,"Tags":["confidence-interval","chi-squared","standard-deviation"],"Answer_count":0,"Last_activity":"2016-08-05 10:50:35","Link":"http://stats.stackexchange.com/questions/228484/why-does-sd-not-satisfy-confidence-interval-value-over-multiple-experiments-but","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d605a"},"View_count":67,"Display_name":"MSIS","Question_score":1,"Question_content":"Say I have a multilinear regression  ( having more than one variable; not necessarily having 3 independent variables)We then want the residuals :{}to be normally-distributed (with mean =0, as pointed in the answer by Enzo).  Question: If the  are normally-distributed, doesit follow that each of the residuals {}, {}, {} (i.e., we regress Y against each variable separately ) is also normally -distributed?Also, does this depend on whether :{} is jointly normal?Thanks in Advance.","Creater_id":122311,"Start_date":"2016-08-04 14:47:51","Question_id":228338,"Tags":["regression","normal-distribution","multiple-regression","multivariate-normal"],"Answer_count":3,"Last_activity":"2016-08-05 10:33:20","Link":"http://stats.stackexchange.com/questions/228338/normality-of-residuals","Creator_reputation":138}
{"_id":{"$oid":"5837a583a05283111e4d6069"},"View_count":135,"Display_name":"Richard","Question_score":4,"Question_content":"I am taking part in a classification challenge (classes are 0 and 1) where the inputs are encrypted (because these are expensive financial data). As the encryption is order-preserving I can only use the fact that e.g.x_1 \u0026gt; x_2but not d = x_1-x_2Besides trees, which machine learning algorithms give sound models under these circumstances?EDIT: I assume that neural nets, SVM or logistic regression are not appropriate in this setting as they use linear transformations  which I can not apply as I don't have the \"numerical structure\" for this.EDIT 2:I am given data of the following form:(0.2,0.1,0.5,0);(0.1,0.2,0.3,1); (0.02,0.7,0.33,1)and thousands of rows of them (and in my application more columns). In this example the first 3 entries are inputs and the 4th one is the target. All clumns consist of 1001 unique values in the range [0,1]. So I really think that only comparisons are possible.I am sorry if my question was not formulated precisely enough ... I hope now the problem is clearer!","Creater_id":12147,"Start_date":"2016-08-01 08:13:36","Question_id":226699,"Tags":["machine-learning","classification","predictive-models","censoring","ranks"],"Answer_count":2,"Last_activity":"2016-08-05 10:06:06","Link":"http://stats.stackexchange.com/questions/226699/predictive-classification-when-only-ranks-are-observable","Creator_reputation":970}
{"_id":{"$oid":"5837a583a05283111e4d6077"},"View_count":30,"Display_name":"Jay Patel","Question_score":1,"Question_content":"I have N documents.I have found out cosine similarity between all documents. T = (N*(N-1)/2)Because docSim between a document with itself is 1.Now, I have one query and I want to find out all documents which are similar to it. I don't want to rank it but cluster it based on threshold value.Here is how I calculated threshold value.Average: Mean of all(T) cosineSimilaritiesAverage(T) + alpha*standardDeviation(T)How do you find the parameter alpha?I found this formula herehttps://www.researchgate.net/post/Determination_of_threshold_for_cosine_similarity_scoreIs there any other way to find out threshold?Some suggestions on formula might improve my cluster.Thank you","Creater_id":126756,"Start_date":"2016-08-05 09:36:50","Question_id":228478,"Tags":["machine-learning","standard-deviation","mean","threshold"],"Answer_count":0,"Last_activity":"2016-08-05 09:36:50","Link":"http://stats.stackexchange.com/questions/228478/threshold-value-for-cluster-after-document-similarity","Creator_reputation":106}
{"_id":{"$oid":"5837a583a05283111e4d6079"},"View_count":67,"Display_name":"jfive","Question_score":0,"Question_content":"I have a dataset. Some of the points (roughly 15%) have been labelled to a particular cluster, but not all of them.I run the clustering algorithm on the data and evaluate it by:Matching each class to the cluster with the highest number of class instancesEvaluating the Precision and Recall of the cluster to get a harmonic mean F-scoreThe algorithm has parameters, such as the minimum similarity for a pair of points to be clustered (via cosine similarity)Would it be considered overfitting to run the algorithm on a range of values for each parameter and report the best results?If so, what approach can I take to avoid this?Should I hold out some data as a test set?","Creater_id":107747,"Start_date":"2016-08-04 09:00:49","Question_id":228282,"Tags":["clustering","optimization","overfitting","parametric","parameter-optimization"],"Answer_count":2,"Last_activity":"2016-08-05 09:29:14","Link":"http://stats.stackexchange.com/questions/228282/can-parameter-tuning-cause-overfitting-in-a-clustering-problem","Creator_reputation":1}
{"_id":{"$oid":"5837a583a05283111e4d6086"},"View_count":11,"Display_name":"tintinthong","Question_score":0,"Question_content":"A very few authors use Hasse Diagrams to illustrate linear models and anova. Most of the examples such as this pdf by Bailey use them to illustrate model with factors. Can we use Hasse Diagrams to illustrate regression models or ANCOVA models?I have found that the great thing about Hasse Diagrams is that it gives you a way to check whether you are fitting the correct model in R by looking at the degrees of freedom. Hence, I find using Hasse Diagram as an attractive idea to use. Let's consider an example with a factor with three levels,A and a continuous covariate,x which are used to model a response, Y under the model Y~A+x. I know that because A has three levels, it has 3 parameters to estimate so it has a dimension value of 3. And since x is a covariate it has a dimension of 1. From here, how do I draw the Hasse diagram including the models   and  as used in the notation in the link I gave. My guess is that   has 0  df as stated above (By taking dim()-dim()). Hence, the model is invalid. But, what If I can form a model as a combination of covariates. ie Y~A+(x1+x2). I know that A has dimension 3 as before and x1+x2 has dimension 2 so it has 1 df(By taking dim()-dim()). But, how do I Draw the Hasse diagram including the models  and ?","Creater_id":121671,"Start_date":"2016-08-05 08:57:07","Question_id":228469,"Tags":["regression","anova","ancova"],"Answer_count":0,"Last_activity":"2016-08-05 09:24:28","Link":"http://stats.stackexchange.com/questions/228469/how-to-draw-hasse-on-ancova-model","Creator_reputation":117}
{"_id":{"$oid":"5837a583a05283111e4d6088"},"View_count":144,"Display_name":"SpeedBirdNine","Question_score":3,"Question_content":"In the book Elements of Statistical Learning in Chapter 7 (page 228), the training error is defined as:\\overline{err} = \\frac{1}{N}\\sum_{i=1}^{N}{L(y_i,\\hat{f}(x_i))}Whereas in-sample error is defined asErr_{in} = \\frac{1}{N}\\sum_{i=1}^{N}{E_{Y^0}[L(Y_{i}^{0},\\hat{f}(x_i))|\\tau]}  The  notation indicates that we observe N new response values at  each of the training points .Which seems to be exactly the same as training error because training error is also calculated i.e by computing the response of the training set using the fitted estimate . I have checked this and this explanation of this concept, but could not understand the difference between training error and in-sample error, and why optimism is not always 0:op\\equiv Err_{in}-\\overline{err}So how are the errors  and  different, and what is the intuitive understanding of optimism in this context? Additionally, what does the author mean by \"usually biased downward\" in the statement:  This is typically positive since err is usually biased downward as an estimate of prediction error.while describing Optimism (Elements of Statistical Learning, page 229)","Creater_id":56105,"Start_date":"2016-08-05 00:39:45","Question_id":228394,"Tags":["variance","error","bias","intuition","in-sample"],"Answer_count":1,"Last_activity":"2016-08-05 08:58:59","Link":"http://stats.stackexchange.com/questions/228394/what-is-the-difference-between-in-sample-error-and-training-error-and-intuition","Creator_reputation":304}
{"_id":{"$oid":"5837a583a05283111e4d6094"},"View_count":65,"Display_name":"Felix Labelle","Question_score":2,"Question_content":"I`m currently an Intern and one of my tasks is to asses whether certain variables were optimal for a control system. In total there are four variables which can vary, and to avoid a complicated analysis I decided to, from the default settings (as a control), only vary one of these variables at a time and see how it impacted the stabilization time of the system. Each of these sample consists of 2500 tests and the time it takes to converge for each test. Just to clarify, my values are discrete as time is the number of cycles it takes to converge.This approach seemed simple enough, however after preliminary analysis it became apparent that the distributions were not normal and that one of the parameters played heavily with the skew and shape of the PDF. Non-parametric tests seemed most-appropriate and I felt the median would be most representative of the time it took to settle, due to the heavy skew. The Mann-Whitney test seemed perfect, but the assumptions it required to be valid to compare the medians were too restrictive (My data is discrete and the distributions do not appear to just be shifted). The next test I thought of using was a permutation test to compare the medians of our two distributions, however I have no Idea how to conclude from the distribution of the medians. I though about just noting the percentage of the time that one settled faster than the other, but have no idea how to prove if that is statistically significant. If additional information is required, please do not hesitate to ask. Thank you ahead of time!Edit:Ok, minutes after having posted something clicked and now everything seems clear. My question now is rather does it make sense or is there a better approach?HistogramsThe first histogram is that of my control settings (epsilon = 0.1) and the other is when epsilon = 0.4. As epsilon grows the data's skew tends to become (from what I can see) more and more to the right. Due to my lack of reputation points I can only post two images, but the trends persists for all the values of epsilon I have tested thus far.","Creater_id":125507,"Start_date":"2016-08-03 07:25:45","Question_id":227079,"Tags":["nonparametric","median","permutation"],"Answer_count":1,"Last_activity":"2016-08-05 08:49:14","Link":"http://stats.stackexchange.com/questions/227079/comparison-of-medians","Creator_reputation":11}
{"_id":{"$oid":"5837a583a05283111e4d60a1"},"View_count":58,"Display_name":"ching","Question_score":1,"Question_content":"I'm currently working with the MICE algorithm to impute missing data.After I did the imputation I wanted to do some kind of quality check of the imputed data set.There are some suggestions herehttps://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/where I can plot the original data and the imputed data to compare them.For example compare their densities.Now I'm wondering:Since MICE assumes MAR, meaning that the original data and the missing data have the same distribution, isn't it obvious that the density is almost the same? So why even performing that \"test\"?What I was thinking instead is:Remove some data points from the original data. Then perform MICE and them compare the imputed datas with the original one.That makes sense to me, but why isn't standard procedure?Sure if I do that, my imputation model will be not as good as if I don't remove the data, since less data -\u003e less information.However,I just can't find any information about that in the internet.I would really appreciate it, if some of you can give me some thought or advices.Best wishesChing","Creater_id":84119,"Start_date":"2016-08-05 08:42:17","Question_id":228463,"Tags":["missing-data","multiple-imputation","quality-control","mice"],"Answer_count":0,"Last_activity":"2016-08-05 08:42:17","Link":"http://stats.stackexchange.com/questions/228463/perform-quality-check-for-imputed-data-with-mice-in-r","Creator_reputation":57}
{"_id":{"$oid":"5837a583a05283111e4d60a3"},"View_count":173,"Display_name":"user89356","Question_score":1,"Question_content":"I have 3 variables ( GDP, exportation, industrial production). i found that variables are not stationary in level but stationary in their first difference. I made the Johansen test, I must choose between the five models. That's why i made the test which summarise all tests.  The appropriate model is chosen according to the information criteria. The problem that I can't make an interpretation for the result that i have.","Creater_id":89356,"Start_date":"2015-09-13 05:42:38","Question_id":172296,"Tags":["hypothesis-testing","interpretation","cointegration"],"Answer_count":1,"Last_activity":"2016-08-05 08:31:58","Link":"http://stats.stackexchange.com/questions/172296/interpret-results-from-johansen-cointegration-test","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d60b0"},"View_count":36,"Display_name":"CoconutBandit","Question_score":2,"Question_content":"I have four independent exponential random variables  with rates  respectively. For given values of  and , I'm looking for the probability of either of the following events occurring:Combined, this gives the event:\\{A\u0026lt;t_A\\} \\cap \\{A\u0026lt;N_1\\} \\cap \\Big( \\{A \u0026lt; B\\} \\cup \\left( \\{B\u0026lt;A\\} \\cap \\{t_B\u0026lt;B\\} \\cap \\{t_B\u0026lt;N_2\\}\\right)\\Big)My strategy is to marginalize over  and then , but I'm getting caught up on the bounds of integration. I can reduce it to the following integral: \\int_0^{t_A} \\Pr[a \u0026lt; N_1] \\cdot \\Big(\\Pr[a \u0026lt; B] + \\Pr[t_B\u0026lt;N_2]\\cdot\\int_{t_B}^a \\Pr[B=b] db\\Big) \\cdot \\Pr[A=a] da But this only seems to hold when  when I tried to simulate it. Should the upper bound on the integral be something like ? ","Creater_id":113714,"Start_date":"2016-08-05 05:27:54","Question_id":228433,"Tags":["probability","conditional-probability","independence"],"Answer_count":1,"Last_activity":"2016-08-05 08:24:05","Link":"http://stats.stackexchange.com/questions/228433/probability-of-a-combination-of-independent-events","Creator_reputation":68}
{"_id":{"$oid":"5837a583a05283111e4d60bd"},"View_count":40,"Display_name":"user2539738","Question_score":0,"Question_content":"When I run mnrfit in matlab, I get different results (different intercept/coefficients) than if I use linear_model.LogisticRegression() in pythonIn python, there are lots of input variables I can dial in, but I don't know what to change to get it to match my matlab output.  What can I do to reconcile the difference in outputs?","Creater_id":126648,"Start_date":"2016-08-05 08:19:59","Question_id":228460,"Tags":["regression","logistic","matlab","python","multinomial"],"Answer_count":0,"Last_activity":"2016-08-05 08:19:59","Link":"http://stats.stackexchange.com/questions/228460/multinomial-logistic-regression-matlab-vs-python","Creator_reputation":111}
{"_id":{"$oid":"5837a583a05283111e4d60c0"},"View_count":5984,"Display_name":"chl","Question_score":73,"Question_content":"I like G van Belle's book on Statistical Rules of Thumb, and to a lesser extent Common Errors in Statistics (and How to Avoid Them) from Phillip I Good and James W. Hardin. They address common pitfalls when interpreting results from experimental and observational studies and provide practical recommendations for statistical inference, or exploratory data analysis. But I feel that \"modern\" guidelines are somewhat lacking, especially with the ever growing use of computational and robust statistics in various fields, or the introduction of techniques from the machine learning community in, e.g. clinical biostatistics or genetic epidemiology.Apart from computational tricks or common pitfalls in data visualization which could be addressed elsewhere, I would like to ask: What are the top rules of thumb you would recommend for efficient data analysis? (one rule per answer, please).I am thinking of guidelines that you might provide to a colleague, a researcher without strong background in statistical modeling, or a student in intermediate to advanced course. This might pertain to various stages of data analysis, e.g. sampling strategies, feature selection or model building, model comparison, post-estimation, etc.","Creater_id":930,"Start_date":"2010-09-16 03:21:36","Question_id":2715,"Tags":["modeling","eda","rule-of-thumb"],"Answer_count":18,"Last_activity":"2016-08-05 08:19:02","Link":"http://stats.stackexchange.com/questions/2715/rules-of-thumb-for-modern-statistics","Creator_reputation":37824}
{"_id":{"$oid":"5837a583a05283111e4d60de"},"View_count":71,"Display_name":"kusur","Question_score":2,"Question_content":"Let us say that the predictor X perfectly predicts the label Y such that Y = 1 if X \u003e= c and Y = 0 if X \u0026lt; c. Then why can't linear regression give zero classification error?This question was asked in Machine Learning course on Coursera and according to them, zero classification rate isn't possible. I can't see why is this the case. Of course, I am assuming that this condition is true forever and not just for the training set (no such mention in the question)Edit:- Let me break the problem down so that you know what I am thinking -According to the question, there is a variable X and a variable Y such that if X \u003e= C (some fixed value) then Y = 1 and if X \u0026lt; C then Y = 0.Now, according to the definition of classification error -classification error = (no. of misclassified objects)/(total no. of objects)By this definition, it means that Y was supposed to be 1 when it was classified as 0 (or the other way around).This means that there exists a value C such that X \u003e= C but the hypothesis h(x) classified it as 0. (Am I right till here?)But according to the question, this can never be the case.Now, here is the part where I am getting confused - is the question only talking about training data? Or is the question actually stating the property of the entire population?If the question is talking only about training data, then the answer is understandable. However, if the property of the predictor is true throughout the population, then I am unable to understand it.Please let me know if you need more details.","Creater_id":26117,"Start_date":"2016-08-04 15:14:39","Question_id":228344,"Tags":["regression","self-study","classification"],"Answer_count":1,"Last_activity":"2016-08-05 08:16:49","Link":"http://stats.stackexchange.com/questions/228344/why-a-linear-regression-cannot-obtain-a-zero-classification-error-on-a-predictor","Creator_reputation":11}
{"_id":{"$oid":"5837a583a05283111e4d60eb"},"View_count":54,"Display_name":"user1701545","Question_score":1,"Question_content":"I have binomial data (meaning k successes out of n trials) for a set of conditions. I would like to fit a glm in order to quantify the effect of each condition on the success. Since the data are overdisperesed I thought of using a negative binomial glm (glm.nb from the R MASS package does that).Code snippet (though not really overdisperesed):set.seed(1)df \u0026lt;- data.frame(k = as.integer(runif(200,1,20)),                 n = as.integer(runif(200,100,200)),                 cond = rep(LETTERS[1:20],10),                 stringsAsFactors = F)dfcond)library(MASS)fit \u0026lt;- glm.nb(k ~ cond + offset(n), data = df)Obviously cond A will be set as baseline and all effects will be relative to it. However, this makes interpretation very difficult for me. Therefore my question is how do I fit a glm.nb model where the effects are relative to the mean across all conditions rather than the dummy variable set as baseline?","Creater_id":42425,"Start_date":"2016-08-04 18:05:32","Question_id":228362,"Tags":["r","regression","categorical-data","generalized-linear-model","negative-binomial"],"Answer_count":1,"Last_activity":"2016-08-05 07:58:43","Link":"http://stats.stackexchange.com/questions/228362/setting-average-as-baseline-rather-than-a-dummy-variable-in-a-glm","Creator_reputation":198}
{"_id":{"$oid":"5837a583a05283111e4d60f7"},"View_count":227,"Display_name":"highBandWidth","Question_score":2,"Question_content":"I want to get the first few eigenvectors of real symmetric matrices with missing values. Since it has missing values, I won't be able to use the common linear programming techniques, but stochastic gradient will work. Funk's SVD used in recommendation engines solves a more general problem of low-rank SVD approximation using gradient descent. So some flavor of Funk's SVD should be able to solve this. I will be using this in R so any R or C++ library that plays well with Rcpp will do. Are there any standard implementations or libraries that I can use? It doesn't make sense to roll my own for such a standard algorithm.","Creater_id":2728,"Start_date":"2015-11-24 21:37:04","Question_id":183477,"Tags":["svd","gradient-descent","recommender-system","eigenvalues","c++"],"Answer_count":1,"Last_activity":"2016-08-05 07:55:40","Link":"http://stats.stackexchange.com/questions/183477/standard-library-for-funk-svd-or-other-gradient-descent-svd-eigenvalue","Creator_reputation":1104}
{"_id":{"$oid":"5837a583a05283111e4d6104"},"View_count":20,"Display_name":"StatsScared","Question_score":1,"Question_content":"For example, if one is comparing the dispersion of prices of 5 different goods between two countries (assuming they have the same currency), is there a reason one should use the standard deviation, over all 5 goods, of the log of the ratio of the price of a good in country X over  the price of the same good in country Y?I would think that taking logs of this price ratio, between the two countries, may create inaccuracies because the logs are only an approximation to the percent difference in price for the same good in both locations.","Creater_id":41267,"Start_date":"2016-08-04 18:10:24","Question_id":228363,"Tags":["summary-statistics","logarithm"],"Answer_count":0,"Last_activity":"2016-08-05 07:28:46","Link":"http://stats.stackexchange.com/questions/228363/is-there-a-reason-to-prefer-the-standard-deviation-of-log-differences-over-just","Creator_reputation":165}
{"_id":{"$oid":"5837a583a05283111e4d6106"},"View_count":1244,"Display_name":"Karnivaurus","Question_score":28,"Question_content":"When doing regression, for example, two hyper parameters to choose are often the capacity of the function (eg. the largest exponent of a polynomial), and the amount of regularisation. What I'm confused about, is why not just choose a low capacity function, and then ignore any regularisation? In that way, it will not overfit. If I have a high capacity function together with regularisation, isn't that just the same as having a low capacity function and no regularisation?","Creater_id":72307,"Start_date":"2016-07-31 07:36:14","Question_id":226553,"Tags":["regression","machine-learning","optimization","regularization","polynomial"],"Answer_count":4,"Last_activity":"2016-08-05 07:10:09","Link":"http://stats.stackexchange.com/questions/226553/why-use-regularisation-in-polynomial-regression-instead-of-lowering-the-degree","Creator_reputation":738}
{"_id":{"$oid":"5837a583a05283111e4d6116"},"View_count":49,"Display_name":"user5201880","Question_score":1,"Question_content":"I ran a 2 x 2 repeated measures ANOVA using SPSS. Both main effects and interaction were significant. Then I ran t-tests to locate significant comparisons, but all the comparisons are significant. I thought that in these cases, we only get significant main effects (graph shows two lines going down in parallel). Can anyone explain?","Creater_id":126739,"Start_date":"2016-08-05 06:03:16","Question_id":228440,"Tags":["hypothesis-testing","statistical-significance","anova","t-test","spss"],"Answer_count":0,"Last_activity":"2016-08-05 06:25:57","Link":"http://stats.stackexchange.com/questions/228440/2x2-repeated-measures-anova-significant-interaction-all-the-pairwise-compariso","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d6118"},"View_count":84,"Display_name":"psql","Question_score":5,"Question_content":"I've noticed the Levenberg-Marquardt algorithm is only used with least squares problem and I didn't find any library in R or Python  which allow to minimise the absolute values of the residual (and not the square of the residual)Is there any reason of that ?Does it make sense to apply sqrt(abs(residual)) on the residual before it's been squared if we want to use this algorithm with Least Absolute Deviation instead of Least Squares Deviation ?","Creater_id":74665,"Start_date":"2015-10-17 10:01:25","Question_id":177395,"Tags":["regression","nonlinear-regression"],"Answer_count":1,"Last_activity":"2016-08-05 06:22:49","Link":"http://stats.stackexchange.com/questions/177395/why-is-levenberg-marquardt-only-used-with-least-squares-problem","Creator_reputation":74}
{"_id":{"$oid":"5837a583a05283111e4d6125"},"View_count":147,"Display_name":"Minkyu Choi","Question_score":3,"Question_content":"I am studying MCMC with \"Pattern Recognition and Machine Learning\" Book by Christopher Bishop. In the chapter of MCMC, this book introduces Markov Chain also a little bit. However, while reading the book, I wonder why Markov chain is needed. Because to conduct the Metropolis-Hasting algorithm, for every step I make sample from proposal distribution and then decide whether it is accepted or not. For me, this Metropolis-Hasting algorithm is more like rejection sampling. Adopting proposal distribution that we can directly draw sample from and deciding the sample should be accepted or not are similar. Where is the room for using Markov chain? Do I need to calculate 'transition matrix' for Metropolis-Hasting algorithm? At this my confusing  state, I feel that I can conduct Metropolis-Hasting sampling without need of transition matrix of Markov Chain. I am very confusing now. This book just says \"under some circumstances a Markov chain converges to the desired distribution\". But it does not say how can I design the Markov chain to converge distribution I desire. And many of the materials in the Internet also seems to skip this part. I thought designing transition matrix of MC is important part to conduct MCMC. But now, I guess it is not necessary. Thanks in advance. ","Creater_id":109812,"Start_date":"2016-08-05 05:43:18","Question_id":228437,"Tags":["probability","sampling","mcmc","monte-carlo","markov-process"],"Answer_count":3,"Last_activity":"2016-08-05 06:08:18","Link":"http://stats.stackexchange.com/questions/228437/does-markov-chain-monte-carlo-method-really-need-markov-chain","Creator_reputation":28}
{"_id":{"$oid":"5837a583a05283111e4d6134"},"View_count":503,"Display_name":"Sandy","Question_score":3,"Question_content":"I am using the code below to fit a gamma GAMM introducing a variance structure that informs the model that variance of the response variable is much larger in one of the levels of the factor coast than in the other. I am using the gamma distribution to ensure strictly positive fitted values, but I am getting the following error message: model \u0026lt;- gamm(abundance ~ s(exposure)+s(depth),            random = list(coast =~ 1),            family=Gamma(link=\"log\"),           weights = varIdent(form =~ 1|coast),           data=census, method=\"REML\")Error in gamm(abundance ~ s(exposure) + s(depth), random = list(coast = ~1),  :   weights must be like glm weights for generalized caseI am assuming the syntax to specify the variance structure needs to change, but I don't know how. Does anyone out there know how to improve this code to avoid the error message?Very grateful!","Creater_id":35926,"Start_date":"2014-10-10 07:49:15","Question_id":118592,"Tags":["r","gamma-distribution","mixed-model","gam","gamm4"],"Answer_count":1,"Last_activity":"2016-08-05 05:55:35","Link":"http://stats.stackexchange.com/questions/118592/adding-a-variance-structure-when-fitting-a-gamm-with-gamma-distribution","Creator_reputation":36}
{"_id":{"$oid":"5837a583a05283111e4d6141"},"View_count":40,"Display_name":"Cromack","Question_score":1,"Question_content":"I'm learning about combining -values and I have a couple of (somewhat elementary, perhaps) questions. Suppose I have  independent tests, each of these to test its own null hypothesis , . Also suppose that, for each of these independent tests, I must calculate a test statistic  (under the null hypothesis). My questions are:(1) What's the main difference between calculating  (and then obtaining its -value) and using Fisher's method?(2) Would the global null hypothesis be \"We can't reject any individual null hypothesis\" and the global alternative hypothesis \"We can reject at least one individual null hypothesis\"?","Creater_id":113000,"Start_date":"2016-08-05 02:32:46","Question_id":228407,"Tags":["hypothesis-testing","distributions","chi-squared","combining-p-values"],"Answer_count":1,"Last_activity":"2016-08-05 05:44:31","Link":"http://stats.stackexchange.com/questions/228407/difference-between-fishers-method-and-the-sum-of-chi-square-test-statistics","Creator_reputation":30}
{"_id":{"$oid":"5837a583a05283111e4d614e"},"View_count":571,"Display_name":"Peter Calhoun","Question_score":2,"Question_content":"There are two popular R packages to build random forests introduced by Breiman (2001): randomForest and randomForestSRC.  I am noticing small, yet significant discrepancies in terms of accuracy between the two packages, even when I try to use the same input parameters.  I understand we would expect a slightly different random forest, but in example below, randomForestSRC package consistently outperforms the randomForest package.  I'm guessing there are other examples where randomForest is superior.  Can someone please explain why these packages provide different predictions?  Is there a way to generate a random forest for both packages using the same methodology?In the example, there's no missing data, all values are distinct, mtry=1, and trees are grown until nodesplit=5.  I believe the same bootstrap approach and split rule is used too.  Increasing ntree or number of observations in the simulated dataset does not change the relative difference between the two packages.library(randomForest)library(randomForestSRC)set.seed(130948) #Other seeds give similar comparative resultsx1\u0026lt;-runif(1000)y\u0026lt;-rnorm(1000,mean=x1,sd=.3)data\u0026lt;-data.frame(x1=x1,y=y)#Compare MSE using OOB samples based on output(modRF\u0026lt;-randomForest(y~x1,data=data,ntree=500,nodesize=5))(modRFSRC\u0026lt;-rfsrc(y~x1,data=data,ntree=500,nodesize=5))#Compare MSE using a test samplex1new\u0026lt;-runif(10000)ynew\u0026lt;-rnorm(10000,mean=x1new,sd=.3)newdata\u0026lt;-data.frame(x1=x1new,y=ynew)mean((predict(modRF,newdata=newdata)-newdatapredicted-newdata$y)^2) #MSE using randomForestSRC","Creater_id":90678,"Start_date":"2016-01-15 17:20:11","Question_id":190911,"Tags":["r","random-forest"],"Answer_count":2,"Last_activity":"2016-08-05 05:28:15","Link":"http://stats.stackexchange.com/questions/190911/randomforest-vs-randomforestsrc-discrepancies","Creator_reputation":403}
{"_id":{"$oid":"5837a583a05283111e4d615c"},"View_count":52,"Display_name":"Marek","Question_score":1,"Question_content":"Let  be identically distributed but not necessarily independent random variables with .In part a) we are required to show that for :This bit is not a problem. However I am stuck showing the next part:b) Hence, show that , as .My working so far:\\begin{align*}n^{-1}E \\left(\\max_{1 \\leq i \\leq n} |X_i|^{\\alpha}\\right) \u0026amp; \\leq n^{-1}E \\left( \\lambda^{\\alpha} + \\sum_{i=1}^{n} |X_i|^{\\alpha} I(|X_i|\u0026gt;\\lambda) \\right) \\\\\u0026amp; = \\frac{\\lambda^{\\alpha}}{n} + \\frac{E \\sum_{i=1}^{n} |X_i|^{\\alpha}I(|X_i|\u0026gt;\\lambda)}{n} \\\\\u0026amp;= \\frac{\\lambda^{\\alpha}}{n} + \\frac{ \\sum_{i=1}^{n} E(|X_i|^{\\alpha}I(|X_i|\u0026gt;\\lambda))}{n} \\\\\u0026amp;= \\frac{\\lambda^{\\alpha}}{n} + \\frac{ \\sum_{i=1}^{n} E(|X_1|^{\\alpha}I(|X_1|\u0026gt;\\lambda))}{n}  \\quad \\text{(identically distributed)}\\\\\u0026amp;= \\frac{\\lambda^{\\alpha}}{n} + \\frac{ \\sum_{i=1}^{n} \\beta}{n} \\quad \\text{for some finite } \\\\\u0026amp;= \\frac{\\lambda^{\\alpha}}{n} + \\beta\\end{align*}Which clearly doesn't go to 0. Can anyone help me spot any errors or provide some insight?PS this question is for self study - thanks!","Creater_id":94725,"Start_date":"2015-11-10 20:07:02","Question_id":181189,"Tags":["random-variable","expected-value","probability-inequalities"],"Answer_count":1,"Last_activity":"2016-08-05 05:21:15","Link":"http://stats.stackexchange.com/questions/181189/expectation-of-maximum-of-a-sequence-of-identically-distributed-but-not-independ","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d6169"},"View_count":40,"Display_name":"Qwerty","Question_score":1,"Question_content":"Please help me with this problem.Suppose that  is a random variable for which . Prove that \\Bbb{P}(|X-\\mu|\\ge t)\\le \\frac{E[(X-\\mu)^4]}{t^4}The only thing I have been able to do is that  and also I have at hand the Cheychef’s inequality but I have got stuck here as  ","Creater_id":95845,"Start_date":"2015-12-31 05:35:56","Question_id":188827,"Tags":["probability","self-study","expected-value","probability-inequalities"],"Answer_count":1,"Last_activity":"2016-08-05 05:13:41","Link":"http://stats.stackexchange.com/questions/188827/related-to-chebychefs-inequality","Creator_reputation":454}
{"_id":{"$oid":"5837a583a05283111e4d6176"},"View_count":38,"Display_name":"Jonathan","Question_score":1,"Question_content":"I believe that I have been having trouble figuring out the proper first initial step to realizing the following problem. Any suggestions upon how to begin would be very much so appreciated. Here's the problem: Assume that  are an i.i.d. random variables having  for , and . Show that  in  as .","Creater_id":125453,"Start_date":"2016-08-02 22:36:12","Question_id":226998,"Tags":["probability","probability-inequalities"],"Answer_count":1,"Last_activity":"2016-08-05 05:08:37","Link":"http://stats.stackexchange.com/questions/226998/applying-minkowskis-inequality-to-show-convergence-of-overline-x-n-in-the-l","Creator_reputation":8}
{"_id":{"$oid":"5837a583a05283111e4d6183"},"View_count":29,"Display_name":"gregorp","Question_score":1,"Question_content":"I am using a generalized linear model in R with categorical independent variables. The model is calibrated and validated, but the results are not of good practical use, because the differences in the response variable vary too much across segments. In other words the predictions are too far apart. Is there a way to constrain the GLM coefficients in a way that they will represent only half (or some other quantity) of their real contributions to the variance? That way the differences should be smaller.","Creater_id":77407,"Start_date":"2016-07-27 02:21:09","Question_id":225857,"Tags":["generalized-linear-model","constraint"],"Answer_count":1,"Last_activity":"2016-08-05 05:03:52","Link":"http://stats.stackexchange.com/questions/225857/constraint-glm-coefficients","Creator_reputation":27}
{"_id":{"$oid":"5837a583a05283111e4d618f"},"View_count":21,"Display_name":"Emily Pinto","Question_score":2,"Question_content":"I am doing an EEG study with a small sample size (N=13) and I have many variables that I need to compare (for example, onset of readiness potential, maximal point of RP, time or awareness, etc), so i need to perform many paired-sample comparisons. I am not sure though, whether I should be using a dependent t-test, or a non- parametric test such as Wilcoxon's signed rank?I was advised that my sample size is too small to reliably test for the normality of the distribution of the differences between my variables, but then also read elsewhere that normality tests should theoretically work on extremely small sample sizes, so I'm not sure what to believe!Can I test for normality on small sample sizes? or should I just use Wilcoxons for all the comparisons?thanks very much,Emily","Creater_id":124723,"Start_date":"2016-07-27 08:24:32","Question_id":225921,"Tags":["t-test","nonparametric","assumptions","wilcoxon"],"Answer_count":1,"Last_activity":"2016-08-05 05:02:34","Link":"http://stats.stackexchange.com/questions/225921/dependent-t-test-or-wilcoxons-signed-rank-for-small-sample-size","Creator_reputation":11}
{"_id":{"$oid":"5837a583a05283111e4d619c"},"View_count":1718,"Display_name":"sruzic","Question_score":3,"Question_content":"(I am following this paper, from page 47 on http://www.bundesbank.de/Redaktion/EN/Downloads/Tasks/Banking_supervision/working_paper_no_14_studies_on_the_validation_of_internal_rating_systems.pdf?__blob=publicationFile)I have some model from which I can construct ROC and calculate its . 95% confidence interval will be . How do I interpret it? I assume that if lower bound of interval is higher than 0.5 then I can conclude that my model is better than random one. What confuses me is that  is in the middle of interval so it will always be inside CI. I think that maybe if my model was applied to some different observation, I would be 95% sure that its  fit into CI. Am I right?Thanks.","Creater_id":83391,"Start_date":"2015-08-06 12:51:07","Question_id":165033,"Tags":["confidence-interval","interpretation","roc","auc"],"Answer_count":2,"Last_activity":"2016-08-05 04:32:51","Link":"http://stats.stackexchange.com/questions/165033/how-to-interpret-95-confidence-interval-for-area-under-curve-of-roc","Creator_reputation":23}
{"_id":{"$oid":"5837a583a05283111e4d61aa"},"View_count":30,"Display_name":"user3826556","Question_score":0,"Question_content":"The OOB score in SK learn Random Forest Regressor gives the R2 score.Now, from what I know, R2 score is only valid for linear data.How do I validate the score of my model if my Random forest model is for non linear data?","Creater_id":52660,"Start_date":"2016-08-04 11:01:14","Question_id":228303,"Tags":["machine-learning","cross-validation","random-forest","scikit-learn"],"Answer_count":2,"Last_activity":"2016-08-05 04:24:34","Link":"http://stats.stackexchange.com/questions/228303/validating-the-performance-of-a-random-forest-regressor-for-non-linear-data","Creator_reputation":2}
{"_id":{"$oid":"5837a583a05283111e4d61b8"},"View_count":44,"Display_name":"Kadi ","Question_score":0,"Question_content":"I'm working on a linear regression formula for a forecasting model. My model requires non-negative predictions. The model works well when predictors have big values; however, I get negative predictions pretty often when predictors have small values. I was hoping to find a solution whereby the model wouldn't be linear but somewhat exponential to converge to 0, but not getting there unless the variables are both 0.This is an example of my constant and coefficients for two variables:CONST   -202,4356389COV        0,741149304USERS    369,5808457","Creater_id":126723,"Start_date":"2016-08-05 02:42:09","Question_id":228409,"Tags":["regression","multiple-regression","forecasting","regression-coefficients"],"Answer_count":1,"Last_activity":"2016-08-05 04:07:44","Link":"http://stats.stackexchange.com/questions/228409/getting-rid-of-negative-predictions-in-linear-regression","Creator_reputation":1}
{"_id":{"$oid":"5837a583a05283111e4d61c4"},"View_count":77,"Display_name":"Imlerith","Question_score":0,"Question_content":"I want to plot a learning curve to see how the error rate of my model varies as the number of training data increases.To get the training error, it's simple, I just train and evaluate my model on an increasing portion of the dataset.However, to get the cross validation error, I don't know the correct way of combining the k-fold cross validation technique while gradually incrementing the size of the training dataset.What is the correct approach to use for plotting cross validation learning curve and using k-fold cross validation?I know it would be easier if my test set was fixed like when using the holdout method instead of k-fold. In that case I would have 30% of my dataset assigned for testing. I would gradually increment the training set from the remaining 70% and test my model on the 30% holdout set.But this method is criticised in many textbooks I read. So, I'd rather use the k-fold cross validation method instead.","Creater_id":124405,"Start_date":"2016-08-02 18:37:24","Question_id":226982,"Tags":["variance","cross-validation","bias"],"Answer_count":1,"Last_activity":"2016-08-05 03:36:35","Link":"http://stats.stackexchange.com/questions/226982/plot-learning-curves-with-k-fold-cross-validation","Creator_reputation":10}
{"_id":{"$oid":"5837a583a05283111e4d61d0"},"View_count":45,"Display_name":"Padraic","Question_score":1,"Question_content":"I am aiming to solve a 'raw' L0/L1-Minimisation problem, i.e, trying to find \\min_{\\alpha=0,1} \\|x\\|_{\\alpha} : Ax=b where  are complex-valued matrices.In particular, I've been trying to find a software package (preferably in Python, Julia or C though I'm also open to Matlab) that would help me solve this. Most code for these  minimisation problems seems to come from the machine learning community, and in particular seems to focus on either the L1-approximation problem (), or the L1-regularized least-squares problem. I'm new to this kind of optimisation problem, so I'd like to ask the community:Is there a way to translate my problem to fit one of these software implementations?Do people know a software package to solve this 'bare' problem?Would an algorithm to solve my optimisation problem be relatively simple to implement myself? ","Creater_id":126730,"Start_date":"2016-08-05 03:32:32","Question_id":228418,"Tags":["optimization","minimum"],"Answer_count":0,"Last_activity":"2016-08-05 03:32:32","Link":"http://stats.stackexchange.com/questions/228418/l0-and-l1-minimization-of-matrix-equation","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d61d2"},"View_count":31,"Display_name":"Shahnawaz","Question_score":1,"Question_content":"I have a daily data for doing forecasting.My series getting frequency/period 7 from findfrequency() function from forecast package.But when i used day of the week variable like Monday to saturday variable as a regressor in a ARIMA model its gives error.Urban_UH_RB001001_train_arima\u0026lt;-Arima((Urban_UH_RB001001_train_ts),order=c(4,0,3),xreg=Urban_UH_RB001001_train[,c(\"monday_flag\",\"Tuesday_flag\",\"Wednesday_flag\",\"salary_day_flag\",\"long_week\",\"Nov_flag\",\"week1_flag\"),with=FALSE],seasonal=list(order=c(0,1,1),period=7))Error in optim(init[mask], armaCSS, method = optim.method, hessian = FALSE,  :   non-finite value supplied by optimAs per my daily data those variable are most important to capture the day effect in my model.So can anyone suggest me why this happen with Arima model?","Creater_id":91873,"Start_date":"2016-08-05 02:07:56","Question_id":228403,"Tags":["time-series","forecasting","arima"],"Answer_count":0,"Last_activity":"2016-08-05 03:12:04","Link":"http://stats.stackexchange.com/questions/228403/why-arima-model-not-accepting-the-day-of-the-week-variables-for-seasonality-peri","Creator_reputation":23}
{"_id":{"$oid":"5837a583a05283111e4d61d4"},"View_count":49,"Display_name":"Sergio Espejo","Question_score":0,"Question_content":"I asked this question in stack Overflow, but no one gave me an answer.I managed to optimize a line in order to get a line of best fit using curve_fit, but I can't seem to get the R squared value the way I can for linear regression, this is my code:import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inlinefrom scipy.optimize import *from scipy.integrate import *from scipy.interpolate import * df=pd.read_csv('F:/Data32.csv')df2=df['Temperature']df3=df['CO2-Rh']def f(x,a,b,c) :   return a*np.exp(b*x)+cparams, extras = curve_fit(f, df2, df3)print('a=%g,b=%g, c=%g' %(params[0],df2[1],df3[2]))plt.plot(df2,df3,'o')plt.plot(df2,f(df2,params[0],params[1],params[2]))plt.legend(['data','fit'],loc='best')plt.show()","Creater_id":126705,"Start_date":"2016-08-05 00:26:44","Question_id":228391,"Tags":["regression","python","nonlinear-regression"],"Answer_count":1,"Last_activity":"2016-08-05 03:09:13","Link":"http://stats.stackexchange.com/questions/228391/is-there-a-way-to-get-an-r-squared-value-for-nonlinear-regression-line","Creator_reputation":3}
{"_id":{"$oid":"5837a583a05283111e4d61e1"},"View_count":22,"Display_name":"smdufb","Question_score":1,"Question_content":"I have a limited set of monthly sales numbers ranging from Mar-13 to Jul-16 (i.e. current) with a large seasonal variance. To predict short term future sales I want to get a factor for this seasonal variance. Giving a years weakest month a value of 1 etc.. This is easy enough.However the problem I have is these monthly sales also contain a near 100% yearly growth rate in sales. So as I add a new months sales, with substantially more sales compared to the same month last year, and recalculate the factors they are distorted. The more current months will be too 'strong' and the following months (that are obviously most important to the prediction) will be too 'weak'.How can I account for this? Let me know if actual numbers are of any use and I will add them.","Creater_id":125503,"Start_date":"2016-08-05 02:57:46","Question_id":228412,"Tags":["time-series","mathematical-statistics"],"Answer_count":0,"Last_activity":"2016-08-05 02:57:46","Link":"http://stats.stackexchange.com/questions/228412/getting-seasonal-variation-factor-from-monthly-sales-while-accounting-for-growth","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d61e3"},"View_count":12,"Display_name":"Jack","Question_score":0,"Question_content":"Is there any way to map nodes of a weighted adjacency matrix onto some real space in a meaningful way. So that the euclidean distance on the real space is related to the adjacency weight of the matrix?","Creater_id":24763,"Start_date":"2016-08-05 02:56:03","Question_id":228411,"Tags":["data-visualization","similarities","graph-theory","networks","social-network"],"Answer_count":0,"Last_activity":"2016-08-05 02:56:03","Link":"http://stats.stackexchange.com/questions/228411/map-weighted-adjacency-matrix-onto-real-space","Creator_reputation":51}
{"_id":{"$oid":"5837a583a05283111e4d61e5"},"View_count":49,"Display_name":"XR SC","Question_score":0,"Question_content":"I found this very awesome playground for visualizing the output of a neural network: http://playground.tensorflow.orgI try to understand how the neural network works with basic dataset:for circle dataset, it is possible to build a nnet with 2 features ( and ) and 0 hidden layer.for exclusive or dataset, we can use the  feature and 0 hidden layerfor gaussian dataset, we can also use  and  features and 0 hidden layerIt is possible to obtain the \"good\" solutions with this above. Now when it comes to spiral dataset, I don't know how to find a simple solution.How could you explain, with a particular dataset, the importance of hidden layers? For example, for dataset (except spiral dataset), what is your suggestion to mitigate the predictions and why?","Creater_id":96531,"Start_date":"2016-08-05 02:48:34","Question_id":228410,"Tags":["neural-networks","tensorflow"],"Answer_count":0,"Last_activity":"2016-08-05 02:48:34","Link":"http://stats.stackexchange.com/questions/228410/how-to-understand-neural-network-with-tensorflow","Creator_reputation":31}
{"_id":{"$oid":"5837a583a05283111e4d61e7"},"View_count":728,"Display_name":"Sina","Question_score":8,"Question_content":"I have a binary classification problem and I experiment different classifiers on it:I want to compare the classifiers. which one is a better measure AUC or accuracy? And why?Raondom Forest: AUC: 0.828  Accuracy: 79.6667 %           SVM: AUC: 0.542  Accuracy: 85.6667 %","Creater_id":25533,"Start_date":"2013-05-11 17:49:26","Question_id":58756,"Tags":["machine-learning","classification","auc"],"Answer_count":2,"Last_activity":"2016-08-05 02:48:12","Link":"http://stats.stackexchange.com/questions/58756/compare-classifiers-based-on-auroc-or-accuracy","Creator_reputation":165}
{"_id":{"$oid":"5837a583a05283111e4d61f5"},"View_count":212,"Display_name":"Sagar Patel","Question_score":1,"Question_content":"I have data frame with 44,353 entries with 17 variables (4 categorical + 13 continuous). Out of all variables only 1 categorical variable (with 52 factors) has NAs  No of factors in the categorical variables are 1601, 6, 52 and 15When I use missforest package it throws error that it cannot handle categorical predictors with more that 53 categories.Please suggest an imputation method in R for best accuracy. Also since the variable to be imputed is categorical I would prefer to avoid methods that use regression techniques to impute values.","Creater_id":125113,"Start_date":"2016-07-30 23:23:12","Question_id":226514,"Tags":["r","data-imputation","data-preprocessing","data-cleaning"],"Answer_count":3,"Last_activity":"2016-08-05 02:44:36","Link":"http://stats.stackexchange.com/questions/226514/data-imputation-in-r-with-nas-in-only-one-variable-categorical","Creator_reputation":9}
{"_id":{"$oid":"5837a583a05283111e4d6204"},"View_count":37,"Display_name":"CodeVib","Question_score":-2,"Question_content":"I know cost function helps in error calculation but what's the significance of taking the sum of the errors instead of just considering the error array which we obtained as an outputX is a 100x4 array and y is a 100x1 array and theta a 4x1 array are the weights and I am trying to predict the value of y.Cost function calculates the sigmoid value of the dot product of X and theta and then calculates the difference between our predicted value with original y array and then sums the array. costFunction(X, y, theta):     hThetaX = sigmoid(dotProduct(X, theta)) #hThetaX is predicted output    return np.sum(np.abs(y - hThetaX))","Creater_id":126582,"Start_date":"2016-08-04 01:44:54","Question_id":228213,"Tags":["machine-learning","python"],"Answer_count":1,"Last_activity":"2016-08-05 02:41:23","Link":"http://stats.stackexchange.com/questions/228213/why-are-we-taking-the-sum-of-the-cost-error-function-output-array-whats-its-si","Creator_reputation":3}
{"_id":{"$oid":"5837a583a05283111e4d6210"},"View_count":4377,"Display_name":"ahra","Question_score":28,"Question_content":"This is probably an amateur question, but I am interested in how did the scientists come up with the shape of the normal distribution probability density function? Basically what bugs me is that for someone it would perhaps be more intuitive that the probability function of normally distributed data has a shape of an isosceles triangle rather than a bell curve, and how would you prove to such a person that the probability density function of all normally distributed data has a bell shape? By experiment? Or by some mathematical derivation? After all what do we actually consider normally distributed data? Data that follows the probability pattern of a normal distribution, or something else? Basically my question is why does the normal distribution probability density function has a bell shape and not any other? And how did scientists figure out on which real life scenarios can the normal distribution be applied, by experiment or by studying the nature of various data itself?So I've found this link to be really helpful in explaining the derivation of the functional form of the normal distribution curve, and thus answering the question \"Why does the normal distribution look like it does and not anything else?\". Truly mindblowing reasoning, at least for me.","Creater_id":125486,"Start_date":"2016-08-03 03:39:44","Question_id":227034,"Tags":["normal-distribution","history"],"Answer_count":4,"Last_activity":"2016-08-05 02:40:50","Link":"http://stats.stackexchange.com/questions/227034/how-did-scientists-figure-out-the-shape-of-the-normal-distribution-probability-d","Creator_reputation":249}
{"_id":{"$oid":"5837a583a05283111e4d6216"},"View_count":132,"Display_name":"SKM","Question_score":1,"Question_content":"In the supervised learning problem, the goal  is, given a training set, to learn a functionso that is a“good” predictor for the corresponding value of. If  takes discrete values, then it is a classification problem. is the training set of  examples where each example  has  number of elements in binary. Then the problem formulation becomes :y^{(i)} = \\theta^T x^{(i)} + e^{(i)} \\tag{1}where  and  is the prediction error.Please correct me if I am wrong in saying that this is a linear regression formulation. Estimating the unknown parameter  for all the training examples can be done using Least Squares. I think  will be a vector of size .I have training data that consists of  time series where each  is of length . The problem is to learn a transformation coefficient  for all  examples such that the output  . The target output should be close to the input for all the examples in such a way that the error between  is minimum. The data  comes from a nonlinear dynamical map TEnt map  , so each example  is obtained from  iterations of  different maps. The question is based on the document http://www.ece.rice.edu/~erzsebet/ANNcourse/handouts502/course-cf-3.pdf which talks about Nonlinear Hebb learning. But it is unclear to me how I can apply to my model in Eq(2)I cannot represent the problem mathematically.I tried something like this but not sure if it makes sense and so need help in formulation. Is the following problem formulation correct?\\mathbf{y}_i= Wg(\\mathbf{x}_i) + \\mathbf{e}_i \\tag{2} and assuming error to be  Gaussian distributed. I cannot understand how to derive Hebb's learning law for weight update . Can somebody please provide the solution? Thank you.","Creater_id":21160,"Start_date":"2016-07-25 13:44:44","Question_id":225595,"Tags":["machine-learning","neural-networks","unsupervised-learning"],"Answer_count":1,"Last_activity":"2016-08-05 02:37:52","Link":"http://stats.stackexchange.com/questions/225595/help-in-problem-formuation-hebbs-learning","Creator_reputation":49}
{"_id":{"$oid":"5837a583a05283111e4d6218"},"View_count":76,"Display_name":"enricoferrero","Question_score":2,"Question_content":"When learning from only positive and unlabelled data (PU learning), how are performance measures affected, when compared to a standard supervised setting?For simplicity, let's assume that the entire unlabelled set is treated as negative.For example, intuitively, I think that the number of true positives will be underestimated due to the fact that some of the unlabelled observations are positive in reality.However, I can't really wrap my head around other measures. What happens to these?True PositivesTrue NegativesFalse PositivesFalse NegativesAccuracyAUCPrecisionRecall/SensitivitySpecificityThank you.","Creater_id":32032,"Start_date":"2016-07-15 08:16:59","Question_id":223956,"Tags":["machine-learning","classification","semi-supervised"],"Answer_count":1,"Last_activity":"2016-08-05 01:52:34","Link":"http://stats.stackexchange.com/questions/223956/how-are-performance-measures-affected-in-pu-learning","Creator_reputation":202}
{"_id":{"$oid":"5837a583a05283111e4d621a"},"View_count":33681,"Display_name":"David Hollman","Question_score":57,"Question_content":"Has any study been done on what are the best set of colors to use for showing multiple series on the same plot?  I've just been using the defaults in matplotlib, and they look a little childish since they're all bright, primary colors.","Creater_id":1120,"Start_date":"2014-10-06 07:33:45","Question_id":118033,"Tags":["data-visualization"],"Answer_count":9,"Last_activity":"2016-08-05 01:50:33","Link":"http://stats.stackexchange.com/questions/118033/best-series-of-colors-to-use-for-differentiating-series-in-publication-quality","Creator_reputation":403}
{"_id":{"$oid":"5837a583a05283111e4d621c"},"View_count":20,"Display_name":"user120192","Question_score":0,"Question_content":"I am considering the following linear model:model1 = glm(outcome ~ tool * surface + type, data=df)where tool and surface are factor variables.I understand that I can obtain the predicted values using predict(model1, type=\"response\"). Since the link function is identity, I get the same result using predict(model1, type=\"link\").My question is regarding the fitted values of the terms.How are the fitted value of the term tool, which are obtained using predict(model1, type=\"terms\"), calculated?How do I interpret these fitted values of the terms?Thanks for the answer.","Creater_id":120192,"Start_date":"2016-08-05 01:46:14","Question_id":228399,"Tags":["r","generalized-linear-model"],"Answer_count":0,"Last_activity":"2016-08-05 01:46:14","Link":"http://stats.stackexchange.com/questions/228399/calculation-and-interpretation-of-fitted-values-of-terms-in-glm","Creator_reputation":13}
{"_id":{"$oid":"5837a583a05283111e4d621e"},"View_count":66,"Display_name":"ylnor","Question_score":3,"Question_content":"I am working on Sentiment-Analysis/Opinion-Mining of Tweets, focused on Finance related tweets.One of the biggest issues I am facing is the unability of my algorithm to detect equivalent entities (Definition in B.Liu 2012: Page 18-19) when Financial slang is used. For example, for those familiar with it I would like the following entities to be detected as equivalent after lemmatization :Government-Bonds = Govies = Sovereign-DebtCash = MonetaryStocks = Equities      FX = Forex = Currency-exchange = Foreign-ExchangeBund = German-Bonds = Bundesbank 10y T-Notes = US10 = Treasury-Notes = US-Govies = American-Sovereign-DebtEtc...Here are the two sides of my question:I was thinking about using some supervised learning(Naive-Bayesian-Classification) for such task, but can't find anyclassified set of data for training. Do you know if such datasetexists?Do you have any alternative idea regarding how to perform such task (without dataset maybe..)?Thanks.","Creater_id":116879,"Start_date":"2016-08-04 09:40:00","Question_id":228291,"Tags":["machine-learning","classification","data-mining","natural-language","sentiment-analysis"],"Answer_count":0,"Last_activity":"2016-08-05 01:11:37","Link":"http://stats.stackexchange.com/questions/228291/financial-slang-and-nlp-for-sentiment-analysis","Creator_reputation":39}
{"_id":{"$oid":"5837a583a05283111e4d6220"},"View_count":83,"Display_name":"Richard Hardy","Question_score":3,"Question_content":"Consider a multiple regression model  y = X\\beta + \\varepsilon. with  regressors in . If the model is correctly specified, the OLS estimator  will be the minimum-variance (or best) linear unbiased estimator (MVLUE or BLUE) of .When it comes to minimizing the mean squared error (MSE), ridge regression and lasso may provide estimators  and , respectively, with smaller MSEs than that of OLS for some intervals of penalty intensity  and .As far as I understand, this concerns the entire parameter vector  rather than each individual parameter  for .Question: Can something concrete and useful be said about the individual parameter estimates  versus  and  in terms of MSE?","Creater_id":53690,"Start_date":"2016-08-01 07:51:18","Question_id":226691,"Tags":["regression","lasso","ridge-regression","estimators","mse"],"Answer_count":0,"Last_activity":"2016-08-05 00:50:08","Link":"http://stats.stackexchange.com/questions/226691/mse-of-an-individual-coefficient-from-ridge-or-lasso-vs-ols","Creator_reputation":13002}
{"_id":{"$oid":"5837a583a05283111e4d6222"},"View_count":20,"Display_name":"Reza_Research","Question_score":0,"Question_content":"While defining Markov Models, my pamphlet says \"A statistical tool used for modeling generative sequences characterized by a set of observable sequences.\"I just can't make head or tail out of the term \"generative sequences\"Though I know about the HMM itself and its applicationsI appreciate any hints or cluesthanks","Creater_id":126608,"Start_date":"2016-08-04 23:51:23","Question_id":228388,"Tags":["hidden-markov-model"],"Answer_count":0,"Last_activity":"2016-08-04 23:51:23","Link":"http://stats.stackexchange.com/questions/228388/generative-sequence","Creator_reputation":48}
{"_id":{"$oid":"5837a583a05283111e4d6224"},"View_count":23,"Display_name":"Harry UNL","Question_score":0,"Question_content":"I have one dataset of two variables (x,y). When the data is plotted in a 2D diagram, I see some data points create a good cluster, while the other data points are scattered randomly.Here is an example: (These data points were collected through an experiment. The weight is equal among all points. After plotting the data points, I found that some points can create a cluster. Then, I changed the color of these points to green color to show those better.)Based on the plot, I have two clusters: (1) Green points (2) Red Points. Each data point in both clusters has two variables: X, Y. My question is that:How can I conduct a statistical test to (statistically) show that the Green cluster has lower entropy than the red cluster?H0: No differencesHa: Green cluster is statistically better (Less Entropy) than the red cluster.I actually want to show that among all data points (Red + Green), the green data points create a good cluster. ","Creater_id":126672,"Start_date":"2016-08-04 14:49:40","Question_id":228339,"Tags":["hypothesis-testing","statistical-significance","clustering"],"Answer_count":2,"Last_activity":"2016-08-04 23:21:20","Link":"http://stats.stackexchange.com/questions/228339/statistical-test-to-show-the-importance-of-one-cluster","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d6226"},"View_count":4397,"Display_name":"user31712","Question_score":1,"Question_content":"I have no statistics background, but an trying to complete my first quantitative research survey writeup for my researcher in education class and hope someone can direct me.I created a poll of 62 teachers and am trying to analyze the following 4 questions Do they use social media personally (yes/no)Do they use SM in the classroom (yes/no)If they plan to use in future in classroom (yes/no)What type of secondary school do you work in (public/private)I am running into a challenge with how to complete a \"test of statistical significance for this data\"The professor suggested using either a t-test or ANOVA and provided links, but the data I have do not seem to fit into the formulas.Is anyone able to provide some guidance about how to test this type of survey? Is there a different way to analyze the data?","Creater_id":31712,"Start_date":"2013-10-20 07:40:07","Question_id":73293,"Tags":["self-study","statistical-significance"],"Answer_count":1,"Last_activity":"2016-08-04 21:21:35","Link":"http://stats.stackexchange.com/questions/73293/statistical-significance-in-yes-no-poll-question","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d6228"},"View_count":35,"Display_name":"jay","Question_score":0,"Question_content":"I have a connection to an Aginity redshift database called db, and a table within called fact_activities. The table contains the variables time and sv1. I would like to use dplyr to pull a subset of the data from this table, while also replacing the values of sv1 with a character string. The resulting table will feed into a subsequent join, and I would like to do the entire operation remotely (i.e. avoid collecting the data into R).When I run this queryfactActivities \u0026lt;- tbl(db, 'fact_activities') actsQuery \u0026lt;- factActivities %\u0026gt;%  filter(time\u0026gt;=\"2016-08-03\") %\u0026gt;%  mutate(randomID=random()) %\u0026gt;%  collapse() %\u0026gt;%  filter(randomID\u0026lt;0.01) %\u0026gt;% # bringing in a random subset of rows  transmute(time=time, sv1=\"someString\", sv2=\"someString\") %\u0026gt;%  collapse()actsQuery  I get the following result:Source: postgres 8.0.2 [annalectnzsa@annalect-nz-dw-instance01.cisru0k4d0te.ap-southeast-2.redshift.amazonaws.com:5439/omd2758datamodel]From: \u0026lt;derived table\u0026gt; [?? x 3]                  time         sv1        sv2                (time)       (dbl)      (chr)1  2016-08-03 00:03:39          NA someString2  2016-08-03 00:29:59          NA someString3  2016-08-03 00:32:39          NA someString4  2016-08-03 00:33:48          NA someStringi.e. I can define sv2 as a character string, but sv1 pulls in the original values.Moreover, if I translate this code into SQL I get the following warnings:translate_sql(factActivities %\u0026gt;%                filter(time\u0026gt;=\"2016-08-03\") %\u0026gt;%                mutate(randomID=random()) %\u0026gt;%                collapse() %\u0026gt;%                filter(randomID\u0026lt;0.01) %\u0026gt;% # bringing in a random subset of rows                transmute(time=time, sv1=\"someString\", sv2=\"someString\") %\u0026gt;%                collapse())Warning messages:1: Named arguments ignored for SQL MUTATE 2: Named arguments ignored for SQL TRANSMUTE My questions: Is it possible to redefine sv1 as a character string without collecting the query into R (i.e. without running collect())?What do these warnings mean?Thanks, Jay","Creater_id":15949,"Start_date":"2016-08-04 21:11:44","Question_id":228379,"Tags":["r","sql","database","dplyr"],"Answer_count":0,"Last_activity":"2016-08-04 21:11:44","Link":"http://stats.stackexchange.com/questions/228379/replacing-variable-values-in-remote-database-with-dplyr-query","Creator_reputation":163}
{"_id":{"$oid":"5837a583a05283111e4d622a"},"View_count":28,"Display_name":"user43790","Question_score":0,"Question_content":"I need to test for the difference in mean of two unbalanced samples that are dependent. (dependent in the sense that i collect two measurements on each person) My samples also suffer from non-normality. Is there a non-parametric test to do that? Thanks a lot.","Creater_id":43790,"Start_date":"2016-08-04 21:03:46","Question_id":228378,"Tags":["nonparametric"],"Answer_count":0,"Last_activity":"2016-08-04 21:10:15","Link":"http://stats.stackexchange.com/questions/228378/comparing-means-of-two-unbalanced-samples-that-are-dependent","Creator_reputation":83}
{"_id":{"$oid":"5837a583a05283111e4d622c"},"View_count":20,"Display_name":"Frank","Question_score":1,"Question_content":"I am reading the OverFeat paper \"[OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks]\"I am confused about these two models: fast model and accurate model. The two models got different archetectures. In section 3.3 in the paper, it talked about resolution augmentation on the unpooled layer 5 so I guess it talked about the fast model as the fast model has conv layers until layer 5. But when it talked about total subsampling ratio (2x3x2x3) it is obviously the accurate model (we can deduced from the pool layers). In appendix Table 5, from the calculation of input sizes until layer 5 I guess it talked about the accurate model which however has conv layer until layer 6, then how come the dense pooling operation or so-called resolution augmentation is still applied on layer 5 just as that fast model which has 5 layer conv layers?I did not get any response from the OverFeat google discussion group, so I come here to try my luck. I attached the related tables for your information.Thanks guys.","Creater_id":122872,"Start_date":"2016-08-04 20:36:42","Question_id":228376,"Tags":["conv-neural-network","pooling"],"Answer_count":0,"Last_activity":"2016-08-04 20:36:42","Link":"http://stats.stackexchange.com/questions/228376/overfeat-paper-questions-confused-fast-and-accurate-models","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d622e"},"View_count":66,"Display_name":"aha","Question_score":1,"Question_content":"In machine learning (for regression problems), I often see mean-squared-error (MSE) or mean-absolute-error (MAE) being used as the error function to minimize (plus the regularization term). I am wondering if there are situations where using correlation coefficient would be more appropriate? if such situation exists, then:Under what situations is correlation coefficient a better metric compared to MSE/MAE ?In these situations, is MSE/MAE still a good proxy cost function to use?Is maximizing correlation coefficient directly possible? Is this a stable objective function to use? I couldn't find cases where correlation coefficient is used directly as the objective function in optimization. I would appreciate if people can point me to information in this area.","Creater_id":44971,"Start_date":"2016-08-04 20:22:30","Question_id":228373,"Tags":["regression","machine-learning","svm","optimization","deep-learning"],"Answer_count":0,"Last_activity":"2016-08-04 20:22:30","Link":"http://stats.stackexchange.com/questions/228373/use-pearsons-correlation-coefficient-as-optimization-objective-in-machine-learn","Creator_reputation":106}
{"_id":{"$oid":"5837a583a05283111e4d6230"},"View_count":18,"Display_name":"Alex","Question_score":1,"Question_content":"I am familiar with modelling count data with Poisson regression, for example for count responses  which are random variables with distribution  with means  such that  is a linear combination of covariates .Is it still suitable to use Poisson regression, if the responses  now Poisson rate parameters instead of counts?","Creater_id":22199,"Start_date":"2016-08-04 19:09:45","Question_id":228370,"Tags":["generalized-linear-model","poisson-regression"],"Answer_count":0,"Last_activity":"2016-08-04 19:09:45","Link":"http://stats.stackexchange.com/questions/228370/is-a-poisson-model-appropriate-when-the-response-are-poisson-rate-parameters","Creator_reputation":727}
{"_id":{"$oid":"5837a583a05283111e4d6232"},"View_count":9,"Display_name":"Michael Ray","Question_score":1,"Question_content":"As we know, in the field of fingerprint or palmprint recognition, both the sift features and the minutiae features can be used to construct the descriptor. However, what's the relationship  between them? ","Creater_id":120405,"Start_date":"2016-08-04 18:25:29","Question_id":228366,"Tags":["computer-vision"],"Answer_count":0,"Last_activity":"2016-08-04 18:43:51","Link":"http://stats.stackexchange.com/questions/228366/whats-the-relationship-between-the-sift-features-and-the-minutiae-features","Creator_reputation":11}
{"_id":{"$oid":"5837a583a05283111e4d6234"},"View_count":42,"Display_name":"madsthaks","Question_score":1,"Question_content":"As the titles states, I would like to compare two coefficients in my multiple regression model but I'm not quite sure how.Coefficients:                 Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)       68.9483    29.7439   2.318 0.024493 *  Shots.PG          -0.5074     1.4696  -0.345 0.731334    Shots.OT.PG        7.4992     3.1410   2.388 0.020707 *  Dribbles.PG        0.6081     0.8121   0.749 0.457401    Fouled.PG         -0.9856     0.8783  -1.122 0.267031    Offsides.PG        1.0520     3.0728   0.342 0.733477    Tackles.PG         0.2705     0.6721   0.402 0.689016    Fouls.PG          -0.4230     0.7893  -0.536 0.594329    Ints.PG            0.3414     0.5962   0.573 0.569451    Shots.Allowed.PG  -3.3604     0.8063  -4.167 0.000119 ***Above are the results I've obtained. At first glance I thought it was interesting Shots OT has double the impact of Shots Allowed but I see that their standard errors are significantly different so that worries me.How would I go about comparing these two values?Using linear.hypothesis() I get:Linear hypothesis testHypothesis:Shots.OT.PG  + 2 Shots.Allowed.PG = 0Model 1: restricted modelModel 2: Points ~ Shots.PG + Shots.OT.PG + Dribbles.PG + Fouled.PG + Offsides.PG +     Tackles.PG + Fouls.PG + Ints.PG + Shots.Allowed.PG  Res.Df    RSS Df Sum of Sq      F Pr(\u0026gt;F)1     52 4488.5                           2     51 4484.2  1    4.2107 0.0479 0.8277How do I interpret this? Does this mean they are not different due to its large P Value. I am trying to find out whether or not the Shots OT has a larger effect on the Points total than the Shots Allowed PG","Creater_id":125449,"Start_date":"2016-08-04 16:00:26","Question_id":228351,"Tags":["r","regression","regression-coefficients"],"Answer_count":2,"Last_activity":"2016-08-04 17:03:05","Link":"http://stats.stackexchange.com/questions/228351/how-to-compare-coefficients-within-the-same-multiple-regression-model","Creator_reputation":45}
{"_id":{"$oid":"5837a583a05283111e4d6236"},"View_count":24,"Display_name":"carley","Question_score":0,"Question_content":"A hypothetical study I am creating focuses on whether deer assess each other's fighting prowess based on their antler size.For a sample size of 20, whereby I have all 20 deer antler measurements (this is more than one factor and will be length, width and number of points), I would like to assess different levels of interaction with other deer. To clarify, deer A who has 3 antler points, 12cm length and 3 width has 3 ignore him, 4 get within 5 metres, 6 engage in parallel walking, and the remainder engaging in full battle. I then want to compare whether this deer is approached for fights more than the remaining 19 deer, based on the different features of their antlers and the differing levels of interactions.Does anyone know what statistics test would be best for SPSS? This paper is primarily based on animal behaviour and statistics is not a focus, but of course accuracy and insight would be great. I'm a basic statistician so simple terms would be appreciated, and please do remember I will not need to run the test, merely state what I would do.EDIT - Through research and my statistics class notes, my current thoughts are that multinomial logistic regression might work? Or, a Spearmans Correlation?Many thanks.","Creater_id":90813,"Start_date":"2016-08-04 11:40:11","Question_id":228312,"Tags":["self-study","spss","multiple-comparisons"],"Answer_count":0,"Last_activity":"2016-08-04 16:57:57","Link":"http://stats.stackexchange.com/questions/228312/which-stats-test-to-use-for-the-following-data","Creator_reputation":13}
{"_id":{"$oid":"5837a583a05283111e4d6238"},"View_count":22,"Display_name":"Justin Burrows","Question_score":1,"Question_content":"I have no idea where I came across this code, but I am modeling biological activity of plant tannins in response to an environmental variable (yes,no format) and two continuous variables.  The format I was using is:mod=lm(response~treat/continuousvar1+treat/continuousvar2-1)My output looks like this, and I think that it is giving me two multiple regressions, one in each environmental treatment (with yes and no rows giving intercepts) and both continuous predictors slopes within that regression. But, if that's the case, what is up with reporting the overall model p value and R^2?  If I do this with a single continuous variable, I get back the same slopes and intercepts as when I split the data into yes and no treatments and fit a regression line. The big questions being: is it doing what I think it is, and what can I make of the overall model p?Call:lm(formula = stemnit ~ treat/pbo + treat/nitrogenprcnt - 1)Residuals:      Min        1Q    Median        3Q       Max -0.199408 -0.024005 -0.003304  0.039079  0.149653 Coefficients:                       Estimate Std. Error t value Pr(\u0026gt;|t|)  treatNo                 0.84716    0.37564   2.255   0.0478 *treatYes                0.61179    0.50882   1.202   0.2569  treatNo:pbo             0.06525    0.53332   0.122   0.9050  treatYes:pbo            0.39890    0.42716   0.934   0.3724  treatNo:nitrogenprcnt   0.01604    0.33802   0.047   0.9631  treatYes:nitrogenprcnt  0.22960    0.46016   0.499   0.6286  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.1123 on 10 degrees of freedomMultiple R-squared:  0.9909,    Adjusted R-squared:  0.9854 F-statistic:   181 on 6 and 10 DF,  p-value: 1.309e-09","Creater_id":126670,"Start_date":"2016-08-04 14:58:16","Question_id":228341,"Tags":["multiple-regression","interaction","linear-model"],"Answer_count":1,"Last_activity":"2016-08-04 16:40:23","Link":"http://stats.stackexchange.com/questions/228341/multiple-regression-with-categorical-predictor-at-two-levels-in-lm","Creator_reputation":8}
{"_id":{"$oid":"5837a583a05283111e4d623a"},"View_count":36,"Display_name":"user6571411","Question_score":1,"Question_content":"I have daily time-series data over the same 10 year period for the number of visits of specific individuals to a certain website, i.e. number of visits per user per day. The visitors are from a specific geographical area and are effected by the same seasonal patterns (day of the week, holiday-effects, day of the month, yearly seasonality). An intervention occurs on January 1st 2011. It would be expected to have a gradual increasing effect until January 1st 2012 and after that a constant effect. It would also be expected that only some users would be affected. I'm interested in classifying users based on wether or not they are affected, and by quantify the magnitude of effect. This is in some respects longitudinal/oanel data and in others more like a multiple time-series problem. Either way I'm not sure were to begin formulating a solution. Any pointers or help is appreciated.  The data is such that we can know the following for every visitor:1) Each visitor has at least one visit for at least 100 days.2) The total number of visits for each user over the period is at least 500.3) Each user has at least one visit prior to January 1st of 2009 4) Each user has at least one visit following January 1st 2013The following graph gives a visual idea of the problem (here –20 users are shown, the dataset contains several thousand). ","Creater_id":123935,"Start_date":"2016-08-04 16:21:16","Question_id":228354,"Tags":["regression","time-series","panel-data","intervention-analysis"],"Answer_count":0,"Last_activity":"2016-08-04 16:21:16","Link":"http://stats.stackexchange.com/questions/228354/intervention-analysis-with-multiple-time-series-of-daily-counts","Creator_reputation":28}
{"_id":{"$oid":"5837a583a05283111e4d623c"},"View_count":23,"Display_name":"Richard Rast","Question_score":2,"Question_content":"Take the following situation. We believe that  is drawn from a normal distribution with unknown mean and variance; we wish to estimate the population mean.Previously, several samples were drawn from the population of , say  of them. They were drawn appropriately and independently, and we have their sample means and sample variances.If we had their sample sizes as well, we could essentially do a weighted average of all the sample means and get a good estimate of the population mean (right?).However, in my context (essentially an online learning algorithm) we do not have the sample sizes. So how would we go about estimating the population mean?I thought about maximum-likelihood estimators, but it seems really hard - it seems like we need to estimate the population variance at the same time, and the result is a lot of calculus that (a) I can't solve analytically, and (b) doesn't look like it has a unique solution, due to an apparently lack of concavity.Is this a classical problem with a classical solution?","Creater_id":89762,"Start_date":"2016-08-04 16:00:12","Question_id":228350,"Tags":["normal-distribution","maximum-likelihood"],"Answer_count":1,"Last_activity":"2016-08-04 16:14:05","Link":"http://stats.stackexchange.com/questions/228350/how-to-combine-different-sample-means","Creator_reputation":155}
{"_id":{"$oid":"5837a583a05283111e4d623e"},"View_count":64,"Display_name":"MsBaffled","Question_score":1,"Question_content":"I have a question about the elastic net penalty as implemented in glmnet in R compared to the original paper by Zou and Hastie (2005). In glmnet the penalty is listed as (1-\\alpha)/2||\\beta||_2^2+α||\\beta||_1.but in the paper it is(1-\\alpha)||\\beta||_1 + \\alpha||\\beta||_2^2. Does anyone know where the factor  some from? (Never mind the fact that the 's were swapped between the two parameterisations.) In both cases the penalties are multiplied by , but what are the mathematical/technical arguments for not using a simple convex combination of the lasso and ridge penalties?","Creater_id":126671,"Start_date":"2016-08-04 14:58:59","Question_id":228342,"Tags":["ridge-regression","glmnet","elastic-net"],"Answer_count":1,"Last_activity":"2016-08-04 15:27:57","Link":"http://stats.stackexchange.com/questions/228342/elastic-net-penalty","Creator_reputation":8}
{"_id":{"$oid":"5837a583a05283111e4d6240"},"View_count":43,"Display_name":"Chris","Question_score":0,"Question_content":"The R binom library has confidence intervals function for the Bayes method that uses the Beta distribution. According to the binom documentation:The default prior is Jeffreys prior which is a Beta(0.5, 0.5) distribution. Thus the posterior mean is (x + 0.5)/(n + 1).p|x ~ Beta(x + prior.shape1, n - x + prior.shape2)The prior.shape1 and prior.shape2 can be passed in like so:binom.bayes(x, n,            conf.level = 0.95,            type = c(\"highest\", \"central\"),            prior.shape1 = 0.5,            prior.shape2 = 0.5,            tol = .Machine$double.eps^0.5,            maxit = 1000, ...)Remembering that the default Bayes formula is (x + 0.5)/(n + 1) what would the shape parameters be to modify the 0.5 and 1 as i and j in the next formula.(x + i)/(n + j)such that i and j effectively makes the a different centralizing ratio rather than just merely 0.5.The justification for this is that we may believe the actual mean is closer to say .3 rather than 0.5.What are the rules to picking different shape parameters for differing prior knowledge i.e. 0.0 to 1.0 or 0% to 100%?For example, for the Sun to rise tomorrow, we would have a priori knowledge of believe of 100% but for 10 billion years from now, it may be 80%.UpdateHere is a graph of the various Beta histograms:","Creater_id":70282,"Start_date":"2016-08-03 09:26:36","Question_id":227107,"Tags":["bayesian","confidence-interval"],"Answer_count":1,"Last_activity":"2016-08-04 15:12:49","Link":"http://stats.stackexchange.com/questions/227107/bayesian-confidence-interval-jeffreys-prior-other-than-the-0-5-centroid","Creator_reputation":410}
{"_id":{"$oid":"5837a583a05283111e4d6242"},"View_count":15281,"Display_name":"fmark","Question_score":13,"Question_content":"How can I calculate the confidence interval of a mean in a non-normally distributed sample?I understand bootstrap methods are commonly used here, but I am open to other options. While I am looking for a non-parametric option, if someone can convince me that a parametric solution is valid that would be fine. The sample size is \u003e 400.  If anyone could give a sample in R it would be much appreciated.","Creater_id":179,"Start_date":"2011-10-04 20:14:34","Question_id":16516,"Tags":["confidence-interval","nonparametric","bootstrap","descriptive-statistics","skewness"],"Answer_count":3,"Last_activity":"2016-08-04 14:53:51","Link":"http://stats.stackexchange.com/questions/16516/how-can-i-calculate-the-confidence-interval-of-a-mean-in-a-non-normally-distribu","Creator_reputation":2040}
{"_id":{"$oid":"5837a583a05283111e4d6244"},"View_count":50,"Display_name":"weskpga","Question_score":3,"Question_content":"I am trying to run PCR on a list of a few time series, and no matter which dates I run this on, I am getting a difference between my predicted and actual of zero for the last entry in every time series.  To explain that in more detail, I'm running the following Python code:import pandas as pdimport numpy as npfrom sklearn.decomposition import PCAfrom sklearn.linear_model import LinearRegressiondef rich_cheap_pcr_between_dates(df, dependent_col, n_components=3, start=\"\", end=\"\"):    # Preprocess data    df = slice_df_between_dates(df, start, end)    df.sort_index(ascending=True, inplace=True)    y = df[dependent_col]    X = df.drop(dependent_col, 1)    # Get log returns of X Dataframe    x_scaled = X.pct_change(periods=1).dropna()    x_scaled = x_scaled + 1    x_scaled = x_scaled.apply(np.log)    # Get log returns of y Dataframe    y_scaled = y.pct_change(periods=1).dropna()    y_scaled = y_scaled + 1    y_scaled = y_scaled.apply(np.log)    # PCA    pca = PCA(n_components=n_components)    X_reduced = pca.fit_transform(x_scaled)    # Linear Regression on principal components    regr = LinearRegression()    regr.fit(X_reduced, y_scaled)    # Prediction and richness/cheapness to actual    pcr_predicted = regr.predict(X_reduced)    rich_cheap = y_scaled - pcr_predicted    plt.plot(y_scaled.index, y_scaled.cumsum())    plt.plot(y_scaled.index, pcr_predicted.cumsum())symbols_list = ['ORCL', 'TSLA', 'IBM','YELP', 'MSFT']d = {}for ticker in symbols_list:    d[ticker] = DataReader(ticker, \"yahoo\", '2014-12-01')pan = pd.Panel(d)df = pan.minor_xs('Adj Close')rich_cheap_pcr_between_dates(df, \"IBM\", start=\"01/01/16\", end=\"07/29/16\")Where pcr_predicted is the predicted daily log returns of the data series based on the PCR, and rich_cheap is the difference between that and the actual log returns of the series.  The strange part that I can't figure out is, no matter which data series I run this on, I continue to get the same exact final values for pcr_predicted.cumsum() and y_scaled.cumsum().  Or, in other words, when I ran this for the above code, I got a graph that looks like this:Here's the same code run but with IBM replaced by ORCL:Notice that the two values fluctuate around each other, but then converge to the same value at the very end.  Now, no matter what I choose as my starting and ending dates or time series, I see the same phenomenon, with the last values converging to the same thing.  This isn't what I would expect, unless I am doing something in my code that's forcing that to happen.  Can anyone help me figure out what's causing this behavior?  ","Creater_id":126660,"Start_date":"2016-08-04 12:34:03","Question_id":228322,"Tags":["regression","time-series","pca","python","scikit-learn"],"Answer_count":0,"Last_activity":"2016-08-04 14:37:52","Link":"http://stats.stackexchange.com/questions/228322/principal-component-regression-on-log-returns-exactly-predicting-last-value-of-t","Creator_reputation":16}
{"_id":{"$oid":"5837a583a05283111e4d6246"},"View_count":22,"Display_name":"Stats newbee","Question_score":1,"Question_content":"I am more or less familiar with the stats. However, recently I ran in the following problem.Problem: I would like to compare two distributions that are zero inflated (to some extend). The two distr. y and z I would like to compare with look as follows (I am using R code to illustrate the issue):y \u0026lt;- c(521.0,13319.0,67860.0,143780.0,158374.0,81496.0,15612.0,538.0)z \u0026lt;- c(0.0,404.0,39788.0,217048.0,191345.0,32335.0,580.0,0.0)As you can see, the counts are quite huge and in the variable z we have two zeros. The bins can be described as follows:x \u0026lt;- c(0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0)which reflect a likert-scale from 0-7. Hence, if you would plot the count in a histogram, the x-axis would be of ordinal nature.Using a chi-square test:t \u0026lt;- cbind(y,z)chisq.test(t,correct = TRUE)gives me the following output:Pearson's Chi-squared testdata:  tX-squared = 73707, df = 7, p-value \u0026lt; 2.2e-16suggesting that the two distr. are sign different to each other. Now, I learned that a chi-squared test is only useful, if there are no zeros in any of the bins, nor the number of observations should be at least 5 per bin. Hence, a chi-squared test shouldn't be the right procedure to go for. In some books, a fisher's exact test is suggested in this case, though this fails as the counts are too large.If you plot the two distributions:p \u0026lt;- ggplot(data=df, aes(x))p \u0026lt;- p + geom_line(aes(y = prop.table(y), colour = \"Combined\"),size=2) + geom_line(aes(y = prop.table(z), colour = \"Seperated\"),size=2) p \u0026lt;- p + labs(aesthetic='custom text') + theme(legend.title=element_blank())p \u0026lt;- p + scale_x_discrete(name =\"Rating score\",  limits=c(0,1,2,3,4,5,6,7))p \u0026lt;- p + ylab(\"Probability (%)\")p \u0026lt;- p + theme(axis.text = element_text(size = 15)) + theme(axis.title = element_text(size = 25))p \u0026lt;- p + theme(legend.text=element_text(size=15))pyou can see that they are quite similar in shape. Now comes the issue, what test is the right way to go here? Is the chi-squared test still appropriate to use? In some other books and also here (http://www.basic.northwestern.edu/statguidefiles/conting_anal_alts.html#Zeroes) it is suggested to use a logistic regression model in this case, as the X is ordinal.However, I have no experience with this kind of analysis, and would very much appreciate it, if there would be someone to show me the right R code (if possible, based on my example) and help me to understand and interpret the outcomes.Thanks a lot in advance for your help!","Creater_id":126669,"Start_date":"2016-08-04 14:32:59","Question_id":228336,"Tags":["distributions","paired-comparisons","zero-inflated"],"Answer_count":0,"Last_activity":"2016-08-04 14:32:59","Link":"http://stats.stackexchange.com/questions/228336/comparing-two-distributions-with-zeros-x-ordinal-y-count","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d6248"},"View_count":28,"Display_name":"alloppp","Question_score":1,"Question_content":"I am trying to calculate the overlap area (normalized by the total area) of two plots. The plots are from X Y datasets.I saw a lot of questions about getting the overlap area but from normal distributions(Percentage of overlapping regions of two normal distributions)data1https://drive.google.com/open?id=0BwQ2EnAgefKRZV9lQjZKbDJIY0kdata2https://drive.google.com/open?id=0BwQ2EnAgefKRdEhBSGlrcG9VM3M","Creater_id":126667,"Start_date":"2016-08-04 14:27:00","Question_id":228334,"Tags":["r","python","ggplot2","matplotlib","overlapping-data"],"Answer_count":0,"Last_activity":"2016-08-04 14:27:00","Link":"http://stats.stackexchange.com/questions/228334/how-to-compute-the-normalized-overlap-area-between-two-curves","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d624a"},"View_count":12,"Display_name":"Rodrigo Zepeda","Question_score":1,"Question_content":"The idea of the delta-method is that if you have a \"nice\" function  and a consistent estimator  of , then:\\begin{equation}\\sqrt{n} \\big( f(B) - f(\\beta) \\big) \\dot{\\sim} N\\big(0, \\nabla{f(\\beta)^T} \\Sigma \\nabla{f(\\beta)} \\big),\\end{equation}where  denotes \"asymptotically distributed\". This presentation of the delta method assumes you have  observations of both variables in . In real life things are not so pretty... You might have  observations of , and  observations of  of which only  observations of  have non-missing data both for  and If you wanted to use the delta method you could keep the subset of those s that have all measurements for both variables ( variables only). But that seems such a terrible waste!  Is there a way in which you can include all the observations of your sample in the delta method's ?","Creater_id":81181,"Start_date":"2016-08-04 14:12:59","Question_id":228332,"Tags":["self-study","missing-data","delta-method"],"Answer_count":0,"Last_activity":"2016-08-04 14:12:59","Link":"http://stats.stackexchange.com/questions/228332/using-the-delta-method-when-data-is-missing","Creator_reputation":108}
{"_id":{"$oid":"5837a583a05283111e4d624c"},"View_count":32,"Display_name":"JamesV","Question_score":1,"Question_content":"I was wondering if anyone had any insight or information on how one might go about determining point coordinates in an n-dimensional space from point pair distances.  For example, say I start with a table of pairwise Euclidean distances between points for 10 points .  This would have a distance for each distinct pair of points, a through j, so 45 separate distances.  I want to be able to produce an n-dimensional vector for each point so that the Euclidean distance between each of the points matches the point pair distances from my original table.Any insight on the subject would be much appreciated.  ","Creater_id":125547,"Start_date":"2016-08-03 12:55:21","Question_id":227146,"Tags":["distance-functions","multidimensional-scaling","euclidean","vector-fields"],"Answer_count":1,"Last_activity":"2016-08-04 13:50:40","Link":"http://stats.stackexchange.com/questions/227146/constructing-n-dimensional-vectors-out-of-point-distances","Creator_reputation":8}
{"_id":{"$oid":"5837a583a05283111e4d624e"},"View_count":378,"Display_name":"user1176252","Question_score":1,"Question_content":"I'm trying to understand user characterization from twitter data. How can I infer a user's interests from their status updates? LDA (Latent Dirichlet Allocation) seems to be a suitable approach to topic modeling, based on my reading. I've collected data, removed stop words and punctuation. However, I don't have any training data or example topics. Do I need to specific a topic list for lda? Also, which is the best java library for me?","Creater_id":41646,"Start_date":"2014-03-10 05:57:24","Question_id":89450,"Tags":["machine-learning","topic-models"],"Answer_count":4,"Last_activity":"2016-08-04 13:46:46","Link":"http://stats.stackexchange.com/questions/89450/using-lda-in-non-realtime-twitter-data","Creator_reputation":6}
{"_id":{"$oid":"5837a583a05283111e4d6250"},"View_count":4315,"Display_name":"HIGGINS","Question_score":32,"Question_content":"I want to learn Neural Networks.  I am a Computational Linguist. I know statistical machine learning approaches and can code in Python. I am looking to start with its concepts, and know one or two popular models which may be useful from a Computational Linguistics perspective.I browsed the web for reference and found a few books and materials.Ripley, Brian D. (1996) Pattern Recognition and Neural Networks, CambridgeBishop, C.M. (1995) Neural Networks for Pattern Recognition, Oxford: Oxford University Press.some links, like this thesis, these course notes (University of Toronto Psychology Department), these course notes (University of Wisconsin Computer Science) and this slideshow (Facebook Research).Coursera courses are generally nice, if anyone knows anything relevant from them. I prefer materials with lucid language and ample examples. ","Creater_id":2329,"Start_date":"2016-08-02 09:35:34","Question_id":226911,"Tags":["machine-learning","neural-networks","references","deep-learning","natural-language"],"Answer_count":3,"Last_activity":"2016-08-04 13:33:35","Link":"http://stats.stackexchange.com/questions/226911/neural-network-references-textbooks-online-courses-for-beginners","Creator_reputation":114}
{"_id":{"$oid":"5837a584a05283111e4d6252"},"View_count":32,"Display_name":"Taylor","Question_score":1,"Question_content":"I have to maximize  with respect to  at every iteration of my EM algorithm. It boils down to solving these two equations for  and  (all the s are sufficient statistics):0 = -\\frac{1}{2}(1 + e^{\\eta} - e^{-\\eta} - e^{-2\\eta} ) + 2e^{-\\gamma}\\left[s_3(1+e^{-\\eta}) + (s_0 - s_1)(1-e^{-\\eta})\\right],0 = -\\frac{T}{2} + e^{-\\gamma}\\left\\{(e^{\\eta}+e^{-\\eta} + 2)^{-1})\\left[ 2(s_0-s_1-s_3) \\right] + \\frac{s_1+s_2}{2} + s_3  \\right\\}.I try to relegate the algebra to the computer, but the following code just hangs for a super long time.from sympy.solvers import solvefrom sympy import Symbol, exp# constantsT = Symbol('T')s0 = Symbol('s0')s1 = Symbol('s1')s2 = Symbol('s2')s3 = Symbol('s3')s4 = Symbol('s4')#  variables eta = Symbol('eta')gamma = Symbol('gamma')solve([-T/2+exp(-gamma)*(2*(s0-s1-s3)/(exp(eta)+exp(-eta)+2)+(s1+s2)/2 +s3),       -(1/2)*(1 + exp(eta)-exp(-eta)-exp(-2*eta)) + 2*exp(-gamma)*(s3*(1+exp(-eta))+(s0-s1)*(1-exp(-eta)))],        [eta,gamma])Any advice on getting things to run or solve the system by hand? Solving by hand would do the job, but I would prefer an answer that deals with getting the code to run. I figure I'll be in a similar spot when I try to fit a different model in the future.","Creater_id":8336,"Start_date":"2016-08-03 20:12:21","Question_id":228192,"Tags":["python","expectation-maximization","simultaneous-equation","sympy"],"Answer_count":1,"Last_activity":"2016-08-04 12:52:42","Link":"http://stats.stackexchange.com/questions/228192/help-with-some-symbolic-computation-em-algorithm","Creator_reputation":1336}
{"_id":{"$oid":"5837a584a05283111e4d6254"},"View_count":56,"Display_name":"Hirak Sarkar","Question_score":0,"Question_content":"I have a model for a biological experiment which has a typical bayesian structure, that is . Now let's assume for control the parameters are  and . Here  is the observed data in one condition, and  is the observed data in another condition.Now I have two hypotheses,  and . Now according to the bayesian hypotheses testing the bayes factor will be calculated as follows, \\begin{align}BF \u0026amp; = \\frac{\\int \\int P(X_1,X_2|H_1)P(\\lambda_1,\\lambda_2|H_1)d\\lambda_1d\\lambda_2}{\\int P(X_1,X_2|H_0)P(\\lambda_1,\\lambda_2|H_0)d\\lambda_1d\\lambda_2} \\\\\\end{align}Now I am unable to move further from it, because I don't know how to reflect  and  are differenty in the integration. I can assume  then I can do anormal hypothesis testing. But my distribution does not alow me to have a closed form with such a parameter .  ","Creater_id":123199,"Start_date":"2016-07-26 18:27:41","Question_id":225816,"Tags":["hypothesis-testing","bayesian","model-comparison"],"Answer_count":1,"Last_activity":"2016-08-04 12:38:14","Link":"http://stats.stackexchange.com/questions/225816/how-to-do-bayesian-model-comparison-for-control-and-treatment","Creator_reputation":15}
{"_id":{"$oid":"5837a584a05283111e4d6256"},"View_count":27,"Display_name":"Kevin S. Van Horn","Question_score":4,"Question_content":"Suppose you have i.i.d. variables  in  modeled asP(x_i = k) = \\theta_kand and you want to infer the probability vector . A Bayesian approach puts a prior over , and the Dirichlet distribution is often used for this purpose when  is not too large.I am interested in the case where  may be very large -- for example,  may correspond to the -th word in a very large dictionary. Furthermore, we expect to see some sort of power-law behavior. If we order the elements of  by size, that is we have some permutation  on  with  for all , then we expect the sum\\sum_{k\u0026gt;n} \\theta_{\\pi(k)}to be roughly proportional to  for some . Put another way, I'm looking for a prior withE\\left[\\sum_{k\u0026gt;n} \\theta_{\\pi(k)}\\right] \\propto n^{-a}for  andE\\left[\\theta_k\\right] = 1/Kfor .Does anyone know of a prior that has this property? Any academic papers that look at this kind of thing?","Creater_id":61062,"Start_date":"2016-08-04 12:20:09","Question_id":228321,"Tags":["bayesian","categorical-data","prior"],"Answer_count":0,"Last_activity":"2016-08-04 12:20:09","Link":"http://stats.stackexchange.com/questions/228321/bayesian-prior-over-long-probability-vectors","Creator_reputation":86}
{"_id":{"$oid":"5837a584a05283111e4d6258"},"View_count":30,"Display_name":"Kyllopardiun","Question_score":1,"Question_content":"I have a team with agents that I have to audit their services (which has a standardized rating system that I follow).There are 11 guys that I need to audit and each of them work a different amount of cases per week. I.e.Agent   Cases workedGuy A       93Guy B       53Guy C       81Guy D       111...         ...Guy K       73I was reading about statistical sampling, but didn't figure how to make an effective way to split my work and still have a significant confidence in my audits. Should I use something like Sample Size Calculator for each of them? Or is there another systematic approach that works better for this scenario? I want to have their average scores per week but I am not able to investigate all cases.","Creater_id":126651,"Start_date":"2016-08-04 11:40:55","Question_id":228313,"Tags":["sampling"],"Answer_count":0,"Last_activity":"2016-08-04 12:08:40","Link":"http://stats.stackexchange.com/questions/228313/statistical-sampling-for-auditing","Creator_reputation":106}
{"_id":{"$oid":"5837a584a05283111e4d625a"},"View_count":33,"Display_name":"krock","Question_score":1,"Question_content":"BackgroundI am trying to perform a test for difference in means between two groups in a dataset with around 25k records, where 97% of the Y values are 0, and the non-zero Y values are heavily skewed, like so:Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 2.00   25.00   50.00   85.05  100.00 3000.00My dataset looks like this (I am checking for a difference in average amount between segments 1 and 2):\u0026gt; head(march)  Campaign Amount Donated  Segment 1    March      0       0 Segment1 2    March     50       1 Segment1 3    March    100       1 Segment2 where Donated is a dummy variable indicating whether Amount is greater than 0.Based on my reading (papers and other Cross Validated links below), it seems that my best option is to use a zero inflated or hurdle model, and the best fit I got was with a hurdle model.The questionBoth zero inflated and hurdle models give me two distinct p-values, one for whether both segments are equally likely to donate, and one for whether the average donation between the two groups is different, given that Amount is greater than 0.Technically neither of these tells me whether the overall group means are statistically different. Can I back into a single test statistic from here? Or do I need to use a different approach?Current modelThe best fit I have so far is:\u0026gt; marchL \u0026lt;- glm(formula = Donated ~ Segment, family = binomial(link = \"logit\"), data=march)\u0026gt; marchD = lm(log(Amount) ~ Segment, data = subset(march, Donated == 1))\u0026gt; summary(marchL)Coefficients:                Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)     -3.57033    0.05404 -66.062   \u0026lt;2e-16 ***Segment2        -0.19525    0.08023  -2.434   0.0149 *  ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026gt; summary(marchD)Coefficients:                Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)      3.79865    0.04682  81.141  \u0026lt; 2e-16 ***Segment2         0.27405    0.06959   3.938 9.12e-05 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1So segment 2 is less likely to get a donation, but that donation will be ~27% higher, which doesn't actually answer the question.What I consulted alreadyWhat is the difference between zero-inflated and hurdle distributions (models)?http://stats.stackexchange.com/a/111626Regression Models for Count Data in R (A Zeileis)Modelling skewed data with many zeros: A simple approach combining ordinary and logistic regression (D FLETCHER 2005)Comparing species abundance models (JM Potts 2006)These all deal with modeling the data, but not with testing for a difference in meansthanks in advance for any help","Creater_id":126645,"Start_date":"2016-08-04 11:47:14","Question_id":228317,"Tags":["hypothesis-testing","skewness","zero-inflation"],"Answer_count":0,"Last_activity":"2016-08-04 11:47:14","Link":"http://stats.stackexchange.com/questions/228317/testing-for-difference-in-group-means-with-skewed-data-with-many-zeros","Creator_reputation":6}
{"_id":{"$oid":"5837a584a05283111e4d625c"},"View_count":21,"Display_name":"Bibi541","Question_score":1,"Question_content":"I want to predict a binary response variable y using logistic regression. x1 to x4 are the log  of continuous variables and x5 to x7 are binary variables. Call:glm(formula = y ~ x1 + x2 + x3 + x4 + x5 +     x6 + x7, family = binomial(), data = df)Deviance Residuals:     Min       1Q   Median       3Q      Max  -2.6604  -0.5712   0.4691   0.6242   2.4095  Coefficients:              Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)   -2.84633    0.31609  -9.005  \u0026lt; 2e-16 ***x1             0.14196    0.04828   2.940  0.00328 ** x2             4.05937    0.22702  17.881  \u0026lt; 2e-16 ***x3            -0.83492    0.08330 -10.023  \u0026lt; 2e-16 ***x4             0.05679    0.02109   2.693  0.00709 ** x5             0.08741    0.18955   0.461  0.64467    x6            -2.21632    0.53202  -4.166  3.1e-05 ***x7             0.25282    0.15716   1.609  0.10769    ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 1749.5  on 1329  degrees of freedomResidual deviance: 1110.5  on 1322  degrees of freedomAIC: 1126.5Number of Fisher Scoring iterations: 5The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:anova \u0026lt;- anova(model, test = \"Chisq\")   # Anova1 - pchisq(sum(anovanull.deviance - modeldf.null - modely, model$fitted.values, g = 8)     # Homer Lemeshow testpR2(model)                                            # Pseudo-R^2tell me that there is a lack of evidence to support my model.More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables. So I calculated the absolute error abs(y - y_hat), and obtained the following:77% of my absolute errors were in [0;0.25], which I think is very good!On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.My question is thus the following:The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? ","Creater_id":125417,"Start_date":"2016-08-04 11:47:05","Question_id":228316,"Tags":["r","regression","logistic","goodness-of-fit"],"Answer_count":0,"Last_activity":"2016-08-04 11:47:05","Link":"http://stats.stackexchange.com/questions/228316/absolute-error-as-a-tool-to-evaluate-model","Creator_reputation":116}
{"_id":{"$oid":"5837a584a05283111e4d625e"},"View_count":46,"Display_name":"MBaz","Question_score":5,"Question_content":"I take  samples from a fully specified, discrete, finite uniform random variable  with mean  and variance . I want to find the probability that the absolute error of the empirical mean  of the  samples is larger than a supplied . I can use Chebyshev's inequality to bound this probability: P(|\\mu - \\bar{\\mu}|\u0026gt;\\varepsilon)\\leq\\frac{\\sigma_X^2}{N\\varepsilon^2}. However, Monte Carlo simulation shows this bound to be very loose. Is there a tighter bound for this specific distribution?","Creater_id":126639,"Start_date":"2016-08-04 10:28:45","Question_id":228297,"Tags":["simulation","monte-carlo","uniform","probability-inequalities","bounds"],"Answer_count":1,"Last_activity":"2016-08-04 11:38:55","Link":"http://stats.stackexchange.com/questions/228297/improving-chebyshev-type-bound-for-discrete-uniform-distribution","Creator_reputation":128}
{"_id":{"$oid":"5837a584a05283111e4d6260"},"View_count":4137,"Display_name":"Metrics","Question_score":2,"Question_content":"I was testing the cross sectional independence test using Pesaran CD test in Stata 10 for the panel data using the N=50 and T=18. After running the fixed effect with time dummies (effects), I got a negative value for the test with the p value \u003e1 (which is not possible). I checked the paper on this: http://www.stata-journal.com/article.html?article=st0113, but they didn't talk about the negative value at all. I would really appreciate if you can help me to interpret the results.     xtreg Y  X1 X2 i.year,feFixed-effects (within) regression               Number of obs      =       900Group variable: state                           Number of groups   =        50R-sq:  within  = 0.4336                         Obs per group: min =        18       between = 0.0417                                        avg =        18       overall = 0.3772                                        max =        18                                                F(19,825)          =     33.24corr(u_i, Xb)  = -0.0215                        Prob \u0026gt; F           =    0.0000------------------------------------------------------------------------------            Y |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------           X1 |  -.0008466   .0005767    -1.47   0.142    -.0019787    .0002854           X2 |    .185078   .0304043     6.09   0.000     .1253991     .244757             |        year |       1993  |  -.0101846   .0029659    -3.43   0.001    -.0160061   -.0043631       1994  |  -.0173621   .0028749    -6.04   0.000     -.023005   -.0117192       1995  |  -.0046886   .0029062    -1.61   0.107     -.010393    .0010159       1996  |  -.0175546   .0028879    -6.08   0.000     -.023223   -.0118862       1997  |   -.009572    .002962    -3.23   0.001     -.015386    -.003758       1998  |   -.012109   .0029123    -4.16   0.000    -.0178254   -.0063926       1999  |  -.0097348   .0029043    -3.35   0.001    -.0154356   -.0040341       2000  |  -.0137918   .0028784    -4.79   0.000    -.0194416   -.0081419       2001  |    .004144   .0029323     1.41   0.158    -.0016116    .0098996       2002  |   .0188509   .0028925     6.52   0.000     .0131733    .0245285       2003  |   .0058601   .0028772     2.04   0.042     .0002127    .0115076       2004  |  -.0005801   .0028731    -0.20   0.840    -.0062195    .0050594       2005  |  -.0085907   .0029534    -2.91   0.004    -.0143877   -.0027937       2006  |   -.018118   .0028702    -6.31   0.000    -.0237516   -.0124843       2007  |  -.0164648   .0028771    -5.72   0.000    -.0221121   -.0108174       2008  |  -.0352191   .0028805   -12.23   0.000    -.0408731   -.0295652       2009  |   .0094679   .0032904     2.88   0.004     .0030094    .0159265             |       _cons |   .0467515   .0024088    19.41   0.000     .0420234    .0514797-------------+----------------------------------------------------------------     sigma_u |  .00686051     sigma_e |  .01433916         rho |   .1862708   (fraction of variance due to u_i)------------------------------------------------------------------------------F test that all u_i=0:     F(49, 825) =     4.02             Prob \u0026gt; F = 0.0000xtcsd, pesaran abs Pesaran's test of cross sectional independence =    -2.673, Pr = 1.9925  Average absolute value of the off-diagonal elements =     0.215","Creater_id":14860,"Start_date":"2012-11-09 10:21:28","Question_id":43243,"Tags":["stata","panel-data"],"Answer_count":1,"Last_activity":"2016-08-04 11:38:04","Link":"http://stats.stackexchange.com/questions/43243/pesaran-cross-sectional-dependence-test-in-panel-data","Creator_reputation":1706}
{"_id":{"$oid":"5837a584a05283111e4d6262"},"View_count":15,"Display_name":"Layla","Question_score":0,"Question_content":"I would like to know what is the relationship between multivariate analysis and other topics such as: linear regression, neural networks or support vector machines.According to Wikipedia:  Multivariate statistics is a subdivision of statistics encompassing  the simultaneous observation and analysis of more than one outcome  variable. The application of multivariate statistics is multivariate  analysis.And for what I know a neural network also allows having multiple outcomes. Also in the same wikipedia page in types of analysis they consider other techiques such as clustering, so what do they have in common multivariate analysis with these data mining techniques? Are one subset of the other or how is it?I do not know too much about multivariate analysis, but I have learned neural networks, svm and clustering from the Computer Science point of view and not from the statistical one. Any help would be great.","Creater_id":69395,"Start_date":"2016-08-04 10:00:33","Question_id":228294,"Tags":["multivariate-analysis","data-mining"],"Answer_count":1,"Last_activity":"2016-08-04 11:32:07","Link":"http://stats.stackexchange.com/questions/228294/what-is-the-relationship-between-these-two-topics","Creator_reputation":105}
{"_id":{"$oid":"5837a584a05283111e4d6264"},"View_count":171,"Display_name":"Frobby Frob","Question_score":1,"Question_content":"Ok, so I've been trying to run this test on the the iris dataset to see if it flags the clusters within the data as samples that aren't from the same population. from sklearn import datasetsiris = datasets.load_iris()X=iris.databut when I run the Anderson Darling k-sample test, I get a negative test statistic with a warning message:            stats.anderson_ksamp(X)      (-7.5303855723035387, array([ 0.65422412,  1.29943382,  1.69811439,  2.05150559,  2.47260634]), 1.5192999959017166e-05, array([-0.29565939, -0.84674275, -0.70510477]))      Warning (from warnings module):  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/stats/morestats.py\", line 1353    warnings.warn(\"approximate p-value will be computed by extrapolation\")UserWarning: approximate p-value will be computed by extrapolationI added another return value, pf, that is used to calculate the p-value:p = math.exp(np.polyval(pf, A2))where A2 is the test statistic. Now, I know having  a negative test statistic in this particular case messes around with the p-value (giving p-values \u003e 1). I also tried running the test on a well-defined cluster within this dataset, samples 0-49 and still got a very negative test statistic.             stats.anderson_ksamp(X[0:49])      (-7.2161038796439101, array([ 0.6374498 ,  1.31073023,  1.7353192 ,  2.11769803,  2.58073305]), 0.0029686585640793673, array([-0.22692954, -0.92818677, -0.70082687]))      I was wondering if I am performing this test incorrectly, or if I should be using a different test, to check if many samples fall within the same distribution. Thanks","Creater_id":78252,"Start_date":"2015-06-11 10:32:37","Question_id":156537,"Tags":["p-value","python","scipy","anderson-darling"],"Answer_count":1,"Last_activity":"2016-08-04 11:30:14","Link":"http://stats.stackexchange.com/questions/156537/scipy-stats-anderson-ksamp-negative-return-values-for-test-statistic","Creator_reputation":13}
{"_id":{"$oid":"5837a584a05283111e4d6266"},"View_count":65,"Display_name":"MadRunner","Question_score":1,"Question_content":"I'd like to use RNN for binary classification. One example of such task is sentiment analysis, where embedding of words from a sample are fed into network one by one. But all implementations I have found do define maximal length of a sample and crop or pad samples to this length. Why is this necessary? I thought the very purpose of RNN was to deal with variable length samples, because if we know sample length beforehand, simple convolutional network can deal with it. There will be more weights, but they will be easier to train.","Creater_id":26095,"Start_date":"2016-02-14 02:23:41","Question_id":195502,"Tags":["neural-networks","rnn"],"Answer_count":1,"Last_activity":"2016-08-04 11:20:43","Link":"http://stats.stackexchange.com/questions/195502/rnn-classification-without-sequence-maximum-length","Creator_reputation":13}
{"_id":{"$oid":"5837a584a05283111e4d6268"},"View_count":42,"Display_name":"Prerit","Question_score":1,"Question_content":"I am reading the book statistical inference by Casella and Berger. I am having trouble in understanding the following theorem:Let X have cdf (x), let Y = g(X), and let  and  be the sample spaces, a. If g is an increasing function on , (y) = ((y)) for y  .b. If g is a decreasing function on  and X is a continuous random variable, (y) = 1 - ((y)) for y  .Thanks.","Creater_id":126628,"Start_date":"2016-08-04 08:54:33","Question_id":228281,"Tags":["self-study","distributions","random-variable"],"Answer_count":1,"Last_activity":"2016-08-04 11:18:45","Link":"http://stats.stackexchange.com/questions/228281/distributions-of-functions-and-random-variables","Creator_reputation":4}
{"_id":{"$oid":"5837a584a05283111e4d626a"},"View_count":27,"Display_name":"jkh107","Question_score":0,"Question_content":"My data look like this:UnigueID  Region   Sex   etc.4567      4        M3452      2        F2316     12        F2347      4        F3987      7        M9567      7        Mand so on for 15,000 some obs.I have multiple regions, different numbers of obs in each region, and I want to know first and foremost are the gender proportions different? And then ideally which are different and by how much (confidence limits, etc). I thought to do an ANOVA test using proportion of females in each group as a gender mean but I wasn't getting an F-value presumably because there is no spread using that idea it is just a single number for each group, so there was no error.I'm not sure what I'm screwing up conceptually.","Creater_id":125250,"Start_date":"2016-08-01 08:59:06","Question_id":226707,"Tags":["anova","proportion"],"Answer_count":1,"Last_activity":"2016-08-04 11:12:19","Link":"http://stats.stackexchange.com/questions/226707/determining-if-there-are-significant-differences-in-gender-ratio-across-multiple","Creator_reputation":1}
{"_id":{"$oid":"5837a584a05283111e4d626c"},"View_count":37,"Display_name":"B. Rowan","Question_score":1,"Question_content":"I am trying to figure out what variables most significantly affect the download rate of my app. Right now I have a data frame with a column for dates, downloads per day, payouts per day, and a few more other variables that correspond to the date column. This may be impossible but I'm looking for some way to be able to predict how many downloads I will have based on the values of the factors I have. ","Creater_id":125561,"Start_date":"2016-08-04 09:45:23","Question_id":228292,"Tags":["regression","time-series","forecasting"],"Answer_count":3,"Last_activity":"2016-08-04 11:06:07","Link":"http://stats.stackexchange.com/questions/228292/time-series-regression-question","Creator_reputation":16}
{"_id":{"$oid":"5837a584a05283111e4d626e"},"View_count":30,"Display_name":"esh88","Question_score":0,"Question_content":"I have a problem related to Product defects. Lets assume, there is a computer chip that over time will become defective due to various factors (defect categories) named 1,2,3,....22. I have data collected over time on millions of chips and 22 defect categories. A chip could become defective due to more than 1 factor (i.e. chip#1234 was defective due to defect categories 1, 5, and 22). A chip could also become defective due to one predominant factor (i.e. chip #3454 was defective due to defect category #5). I have done Principal Component Analysis (PCA) and Factor Analysis (FA) to to understand the relationships between or among the 22 defect category variables.Is there something in probability theory that let me identify defect categories that are most dominant, potential defect categories that are likely to occur sooner than some other defect categories, and is a chip likely to become defective with certain categories way before some other wear categories and therefore we can stop tracking these defects that are likely to happen towards the end of the life cycle?Any help would be much appreciated.","Creater_id":87091,"Start_date":"2016-08-04 10:20:35","Question_id":228295,"Tags":["probability","pca","factor-analysis"],"Answer_count":0,"Last_activity":"2016-08-04 10:36:15","Link":"http://stats.stackexchange.com/questions/228295/probability-theory-with-factor-analysis","Creator_reputation":15}
{"_id":{"$oid":"5837a584a05283111e4d6270"},"View_count":31,"Display_name":"Aaron Macy","Question_score":1,"Question_content":"I'm an ecology grad student struggling to confirm proper analysis for the following experimental design:I identified 3 spatial blocks, each containing the same 3 plant types (tree, grass, shrub). There was only a single representative of each plant type within each block (9 representatives total). I measured %nitrogen of leaf tissue 10 times (date) with an irregular frequency on the same representatives.I am interested in the plant type effect \u0026amp; the interaction effect of plant type \u0026amp; date. I would like to know if there was an effect of block and/or date. I include subject to account for its respective variation.I believe the following model would be appropriate:%nitrogen ~ (1|Block) + (1|Block:Subject) + (1|Date) + Vegetation + (1|Vegetation:Date)...such that %nitrogen = response; Vegetation = fixed factor; \u0026amp; Block, Subject, \u0026amp; Date are random factors (as well as all interactions containing a random factor)Is my nesting appropriate? Are my factor designations for \"random\"/\"fixed\" appropriate?Thank you for any advice you can provide!!","Creater_id":125523,"Start_date":"2016-08-03 09:42:38","Question_id":227116,"Tags":["time-series","mixed-model","repeated-measures","experiment-design","multilevel-analysis"],"Answer_count":1,"Last_activity":"2016-08-04 10:29:14","Link":"http://stats.stackexchange.com/questions/227116/repeated-spatial-temporally-nested-design-confirmation","Creator_reputation":8}
{"_id":{"$oid":"5837a584a05283111e4d6272"},"View_count":40,"Display_name":"lmcshane","Question_score":3,"Question_content":"I'm trying to determine if there is a group difference in answering a multiple choice question. For example, I want to answer the question of, do \"Change Instigators\" (group A) answer \"communication\" as a strength more often than the group mean (average of group A, B, and C). You can select up to 2 answers, \"communication\" being one of 5. Here's how my data is formulated:  Q: What's your strength?Strength              Change Instigators     GroupB     GroupCCommunication         300                    100        122Productivity          200                    212        500Conflict Resolution   150                    157        130Vision                216                    256        233Personnel             350                    300        222What type of analysis do I perform when I'm comparing percentages between groups within a question? Any suggestions for other types of analysis?","Creater_id":107127,"Start_date":"2016-08-02 13:57:34","Question_id":226956,"Tags":["chi-squared","multiple-comparisons","survey","group-differences"],"Answer_count":0,"Last_activity":"2016-08-04 10:27:24","Link":"http://stats.stackexchange.com/questions/226956/determine-chi-square-group-differences-in-multiple-choice-questionnaire","Creator_reputation":136}
{"_id":{"$oid":"5837a584a05283111e4d6274"},"View_count":21,"Display_name":"Nico Real","Question_score":3,"Question_content":"I am starting to develop a hybrid ARIMA-ANN model for forecasting. Most of the journals I read mention mostly a linear component for ARIMA and a nonlinear for ANN. How can you know which components in the data are linear and nonlinear?What are the definitions of the linear and nonlinear components of a time series?","Creater_id":126571,"Start_date":"2016-08-03 21:40:31","Question_id":228194,"Tags":["time-series","neural-networks","arima","linear","nonlinear"],"Answer_count":0,"Last_activity":"2016-08-04 09:56:19","Link":"http://stats.stackexchange.com/questions/228194/linear-and-nonlinear-components-of-a-time-series","Creator_reputation":16}
{"_id":{"$oid":"5837a584a05283111e4d6276"},"View_count":49,"Display_name":"user1157751","Question_score":1,"Question_content":"I'm making a model with 5 features (Paths, PathSegs, Polygons, Scalar Instances, and Standard Vias), and the output is memory consumption.Essentially using these 5 features, I can predict how much memory consumption I will use.Here is a diagram of the features vs memory usage.Some features seems to be Heteroscedastic, especially Scalar Instances and Standard Vias. If I trained a model (linear regression), and then plotted the output with the scalar instances, I would get something like this.The problem is with the big jump when the Scalar instances are low. I imagine that when the features are small, then the memory consumption would also be small as well, then gradually increase.I've tried a few different approaches with different models:And all of them have the same issues. So I'm thinking if I threw away Standard Vias, and Pathsegs, which has a big jump at low numbers, then I would get a more stable model. However, is there are systematic way of doing this?","Creater_id":125263,"Start_date":"2016-08-03 10:51:50","Question_id":227129,"Tags":["regression","outliers","heteroscedasticity"],"Answer_count":1,"Last_activity":"2016-08-04 09:01:23","Link":"http://stats.stackexchange.com/questions/227129/linear-regression-how-to-remove-features-behaves-heteroscedastic","Creator_reputation":108}
{"_id":{"$oid":"5837a584a05283111e4d6278"},"View_count":42,"Display_name":"E James","Question_score":3,"Question_content":"I have a language learning study with a few different dependent variables. For the majority of the tasks, responses to each item are either right (1) or wrong (0), and thus the data has been analysed with a mixed effects logistic regression.However, I have one task that has allowed for partial scoring - so participants can score 0, 0.5, or 1. Is there any way to analyse this in a consistent way to the other tasks? I wondered about multinomial regressions but (a) these seem complicated (no lme4 option?), and (b) I'm not sure whether it's appropriate given that the categories are related? There are also very few 0.5 scores.Any pointers would be much appreciated, many thanks in advance. ","Creater_id":125251,"Start_date":"2016-08-01 08:26:17","Question_id":226703,"Tags":["r","mixed-model","binomial","multinomial"],"Answer_count":1,"Last_activity":"2016-08-04 08:32:12","Link":"http://stats.stackexchange.com/questions/226703/not-quite-binomial-mixed-effects","Creator_reputation":18}
{"_id":{"$oid":"5837a584a05283111e4d627a"},"View_count":20812,"Display_name":"user1731927","Question_score":14,"Question_content":"I am trying to learn how Neural Network works on image recognition. I have seen some examples and become even more confused. In the example of letter recognition of a 20x20 image, the values of each pixel become the input layer. So 400 neurons. Then a hidden layer of neurons and 26 output neurons. Then train the network, and then it works, not perfect. What confused me about the Neural Network is, how it learns about what's in an image. You don't need to do thresholding,or segmentation, or measurement, somehow the network learns to compare images and recognize. It is like magic to me now. Where to start to learn neural network?","Creater_id":14831,"Start_date":"2012-10-09 09:50:31","Question_id":39037,"Tags":["machine-learning","neural-networks","image-processing"],"Answer_count":4,"Last_activity":"2016-08-04 08:12:16","Link":"http://stats.stackexchange.com/questions/39037/how-does-neural-network-recognise-images","Creator_reputation":79}
{"_id":{"$oid":"5837a584a05283111e4d627c"},"View_count":25,"Display_name":"H Hobson","Question_score":1,"Question_content":"First time posting - will try to be as clear as possible! I'm running a ridge regression, and am having problems calculating the right bias factor (k) to use. I have 4 predictors, one of which is an interaction between 2 predictors. I'd like to use the HKB (Hoerl, Kennard and Baldwin, 1975) method for estimating the the bias factor (I realise there's a lot of discussion about which method is best, but I thought I'd start with this one, as it appeared relatively straightforward). The formula I'm using is:(...so the number of predictors x the residual mean square, over the sum of the coefficients squared).When I try to use this formula to calculate k for my ridge regression, I get a huge number (~25). Looking at the ridge trace plots, I was expecting something more in the region of 0.5... (I've posted the ridge trace below - you can see three of the predictors level out early, but one levels out much later. I had assumed that the right k would be somewhere between these points.)I've been using \"Regression Analysis by example \" (by Chatterjee and Hadi), which has a walked through example of using this method. A few things I noticed about the example in the book was that their coefficients are generally larger than mine, and their residual mean square is smaller. My coefficents (Bs) from when k=0 (i.e. a normal OLS regression) are : -.058, -.008, 0.365, and .030. p=4Residual mean square = 0.89...so for me: k= 4 x 0.89 / (-0.058)2 + (-.008)2 +(0.365)2 +(.030)2 = 25.85So my questions are:Is a k of 25 normal, or is it clearly an error? My understanding is that you might sometimes get a k in excess of 1, but 25 seems very high!Does a k this high suggest that a ridge regression is inappropriate? Maybe to do with my small coefficients...?I standardized my variables before running the ridge regression (I'm using the SPSS Ridge Macro), so all the variables have a mean of 0 and SD of 1. My understanding was that variables should be standardized before going into a ridge regression. Could this be the source of my problem?Any ideas greatly appreciated!UPDATE:Having double checked my workings using other examples, I'm pretty confident it has nothing to do with standardising the variables. This formula seems to work fine for getting k values for datasets other than mine!!  I can therefore only assume my dataset violates the assumptions of a ridge regression in some way. Could skewed variables give a result like this? Even my standardised variables are not normally distributed. ","Creater_id":125479,"Start_date":"2016-08-03 03:53:43","Question_id":227038,"Tags":["regression","ridge-regression"],"Answer_count":0,"Last_activity":"2016-08-04 08:03:29","Link":"http://stats.stackexchange.com/questions/227038/hkb-algorithm-gives-huge-k-in-ridge-regression","Creator_reputation":6}
{"_id":{"$oid":"5837a584a05283111e4d627e"},"View_count":26,"Display_name":"harshal.c","Question_score":1,"Question_content":"I have a panel data of the format| age.months | app_id | rank_category | promoted | avg_monthly_rating |...I am using xtmixed where level 2 is app_id. My primary regressor of interest :i.promoted#i.rank_categorychanges at rank_category level. I am also using autoregressive model residuals(ar 1, by(rank_category) t(age.months)) .I am confused if I should be clustering the standard errors at level of app_id or that of rank_category.Another conceptual doubt that I have is, if I cluster errors at app_id level, it would take care of the serial autocorrelation between different observations for each app_id. However, that was the reasoning behind using the autoregressive model. So, is it necessary to use both approaches together, or is it alright to cluster the errors at rank_category level? To come out clean, clustering at rank_category level gives me favorable results.xtmixed avg_monthly_rating age.months i.promoted##i.rank_category, || app_id : age.months, covariance(unstructured) residuals(ar 1, by(rank_category) t(age.months)) variance mle vce(cluster rank_category)Any suggestions would be greatly appreciated! Thanks!Edit : It is a panel data with around 11 observations for each of 22,000 apps. The column 'promoted' is a boolean variable which goes from 0 to 1 when a particular app is promoted, staying 1 for all the later observations. What I am interested in is the discontinuous jump/fall that happens in monthly_avg_rating when an app is promoted. I am interacting it with rank_category to check if there is any difference in the magnitude of the jump for different rank categories. There is a big difference when I plot these.Edit : Cross-listed on statalist.org","Creater_id":102519,"Start_date":"2016-08-03 15:48:10","Question_id":227175,"Tags":["mixed-model","stata","multilevel-analysis","robust-standard-error"],"Answer_count":0,"Last_activity":"2016-08-04 07:30:16","Link":"http://stats.stackexchange.com/questions/227175/robust-clustering-of-standard-errors-mixed-autoregressive-models","Creator_reputation":6}
{"_id":{"$oid":"5837a584a05283111e4d6280"},"View_count":54,"Display_name":"muni","Question_score":2,"Question_content":"I have a dataset, that has a \"products\" column. It has some 1000 distinct values.Can I directly convert it to integers (say 1-1000) and use this column as a single variable or should I convert it to dummy coding 1/0 for all products as columns? Does it make a difference?","Creater_id":124691,"Start_date":"2016-08-04 07:02:39","Question_id":228260,"Tags":["machine-learning","categorical-data","python","xgboost"],"Answer_count":1,"Last_activity":"2016-08-04 07:26:17","Link":"http://stats.stackexchange.com/questions/228260/does-it-make-a-difference-to-run-xgboost-on-hot-encoded-variables-or-single-fact","Creator_reputation":81}
{"_id":{"$oid":"5837a584a05283111e4d6282"},"View_count":181,"Display_name":"Sofia","Question_score":1,"Question_content":"I am working on analyzing panel data of countries with several independent variables. I am aware from previous literature on panel data that an OLS model could be performed. However, because the estimation results for count data will suffer bias in OLS where dependent values are allowed to take both negative and positive values, I chose the Poisson model. I have dependent variables that are of count data, over dispersed, and having excess of zeroes.I performed a count data hurdle regression with Poisson and to prevent multicollinearity, variance inflation factors are checked to arrive at my optimal model. Every single output that I got (be it univariate or multivariate analysis) had narrow confidence intervals, low coefficents and extremely low p values (\u0026lt; 2.2e-16). Here is my final model -    Call:hurdle(formula = Y ~ A+B+C+     offset(log(Pop.Density)) | 1, data = Data, dist = \"poisson\")Pearson residuals:     Min       1Q   Median       3Q      Max  -0.7931  -0.7807  -0.7401   0.2798 301.3965 Count model coefficients (truncated poisson with log link):                          Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)              -0.694339   0.004247 -163.47   \u0026lt;2e-16 ***A                        -0.396599   0.005468  -72.53   \u0026lt;2e-16 ***B                        -0.328605   0.004792  -68.58   \u0026lt;2e-16 ***C                        -0.240072   0.004170  -57.58   \u0026lt;2e-16 ***Zero hurdle model coefficients (binomial with logit link):            Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)  -0.4635     0.0393  -11.79   \u0026lt;2e-16 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Number of iterations in BFGS optimization: 18 Log-likelihood: -1.381e+05 on 5 Df\u0026gt; vif(Mv7)                   A                         B                       C                3.564376                 2.419123                 3.663317My primary question is, what could be causing these p values to be so low in my output? My dataset is not extremely huge, but I would say fairly large (3083 obs of 15 variables). A, B,C have values that range from negative to positive.Could there be a time effect that effect my results that I did not consider? Thanks!\u0026gt; Call:hurdle(formula = Y ~ A+B+C+ offset(log(Pop.Density)) | 1, data = Data, dist = \"negbin\")Pearson residuals:    Min      1Q  Median      3Q     Max -0.6743 -0.3575 -0.3489 -0.2177 37.8530 Count model coefficients (truncated negbin with log link):                         Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)               0.67061    0.06041  11.100  \u0026lt; 2e-16 ***A                         0.16976    0.10964   1.548   0.1215    B                        -0.43295    0.09676  -4.474 7.66e-06 ***C                        -0.13336    0.07475  -1.784   0.0744 .  Log(theta)               -1.06590    0.06579 -16.201  \u0026lt; 2e-16 ***Zero hurdle model coefficients (binomial with logit link):            Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)  -0.4635     0.0393  -11.79   \u0026lt;2e-16 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Theta: count = 0.3444Number of iterations in BFGS optimization: 25 Log-likelihood: -8014 on 6 Df\u0026gt; vif(Mv7)             A                    B                         C       5.387540                 3.981045                 2.848343 When I use population total as an offset, I get an error in R - Error: no valid set of coefficients has been found: please supply starting valuesWhen I use log(Population Total) as an offset this is what I get -\u0026gt; Call:hurdle(formula = Y ~ A+B+C+    offset(log(Pop.Total)), data = Data, dist = \"negbin\")Pearson residuals:      Min        1Q    Median        3Q       Max  -0.64145  -0.40354  -0.23862  -0.07396 250.78934 Count model coefficients (truncated negbin with log link):                          Estimate Std. Error  z value Pr(\u0026gt;|z|)    (Intercept)              -11.98678    0.05473 -219.016  \u0026lt; 2e-16 ***A                        -0.33166    0.09544   -3.475 0.000511 ***B                        -0.22369    0.09157   -2.443 0.014567 *  C                         0.55690    0.07117    7.825 5.07e-15 ***Log(theta)               -0.84623    0.06062  -13.960  \u0026lt; 2e-16 ***Zero hurdle model coefficients (binomial with logit link):                          Estimate Std. Error  z value Pr(\u0026gt;|z|)    (Intercept)              -16.44459    0.05300 -310.294  \u0026lt; 2e-16 ***A                         -0.38690    0.09239   -4.188 2.82e-05 ***B                          0.12640    0.08411    1.503   0.1329    C                         -0.13685    0.07855   -1.742   0.0815 .  ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Theta: count = 0.429Number of iterations in BFGS optimization: 43 Log-likelihood: -7389 on 9 Df\u0026gt; AIC(Mv8)[1] 14795.51\u0026gt; vif(Mv8)          A                        B                        C       3.815796                 3.371319                 3.406296 This is my outcome without using an offset. Call:hurdle(formula = Y~A+B+C,     data = Data, dist = \"negbin\")Pearson residuals:    Min      1Q  Median      3Q     Max -0.8101 -0.4350 -0.3287 -0.1220 21.8952 Count model coefficients (truncated negbin with log link):                         Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)               4.27904    0.04229 101.191  \u0026lt; 2e-16 *** A                       -0.01475    0.07926  -0.186   0.8524     B                       -0.11324    0.05358  -2.113   0.0346 *   C                       -0.49852    0.06001  -8.308  \u0026lt; 2e-16 ***Log(theta)               -0.28185    0.04783  -5.893  3.8e-09 ***Zero hurdle model coefficients (binomial with logit link):                         Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)              -0.55545    0.04305 -12.904   \u0026lt;2e-16 ***A                         0.09582    0.07769   1.233    0.217    B                         0.06450    0.06931   0.931    0.352    C                        -0.97937    0.06872 -14.251   \u0026lt;2e-16 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Theta: count = 0.7544Number of iterations in BFGS optimization: 18 Log-likelihood: -7558 on 9 Df\u0026gt; AIC(Mv7)[1] 15133.16\u0026gt; vif(Mv7)                     A                        B                        C                3.683669                 2.178420                 2.897415 Offset with population total scaled to 1. ( I divided each country's population total with largest population) -Call:hurdle(formula = Y~A+B+C+     offset(Pop.Total.Test), data = POP, dist = \"negbin\", link = \"logit\")Pearson residuals:     Min       1Q   Median       3Q      Max -0.88522 -0.45546 -0.34142 -0.06619 18.90660 Count model coefficients (truncated negbin with log link):                         Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)               4.15278    0.03762 110.386   \u0026lt;2e-16 ***A                        -0.11584    0.06897  -1.680    0.093 .  B                         0.02039    0.05119   0.398    0.690    C                        -0.45935    0.05182  -8.864   \u0026lt;2e-16 ***Log(theta)               -0.07232    0.04557  -1.587    0.113    Zero hurdle model coefficients (binomial with logit link):                         Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)              -0.58007    0.04310 -13.458   \u0026lt;2e-16 ***A                         0.08308    0.07774   1.069    0.285    B                         0.06679    0.06962   0.959    0.337    C                        -0.95615    0.06867 -13.923   \u0026lt;2e-16 ***---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Theta: count = 0.9302Number of iterations in BFGS optimization: 13 Log-likelihood: -7433 on 9 Df\u0026gt; AIC(Mv7)[1] 14884.03For my second dependent variable (occurrence) - My optimal model was A+B+log(pop.density). To calculate occurrence rate, I offset it with log(pop.total). The code follows below.Call:hurdle(formula = Occurence ~ A +B+ log(Pop.Density) +     offset(log(Pop.Total)), data = Data, dist = \"negbin\", link = \"logit\")Pearson residuals:    Min      1Q  Median      3Q     Max -1.1211 -0.5798 -0.2758  0.0748 20.7457 Count model coefficients (truncated negbin with log link):                       Estimate Std. Error  z value Pr(\u0026gt;|z|)    (Intercept)           -16.08087    0.15542 -103.467  \u0026lt; 2e-16 ***A                     -0.35991    0.07428   -4.846 1.26e-06 ***B                     -0.13195    0.06224   -2.120  0.03399 *  log(Pop.Density)       -0.21688    0.03435   -6.314 2.73e-10 ***Log(theta)              0.38368    0.12342    3.109  0.00188 ** Zero hurdle model coefficients (binomial with logit link):                       Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)           -16.02224    0.16485 -97.194  \u0026lt; 2e-16 ***A                     -0.28540    0.07408  -3.852 0.000117 ***B                     -0.09963    0.07744  -1.287 0.198255    log(Pop.Density)       -0.08668    0.03829  -2.264 0.023590 *  ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Theta: count = 1.4677Number of iterations in BFGS optimization: 12 Log-likelihood: -3090 on 9 Df\u0026gt; AIC(Mvfrequency)[1] 6198.895\u0026gt; vif(Mvfrequency)                 A                     B      log(Pop.Density)              2.776789              3.469304             11.629956 The output when I remove population density as a factor. Call:hurdle(formula = Occurence ~ A+B + offset(log(Pop.Total)),     data = Data, dist = \"negbin\", link = \"logit\")Pearson residuals:     Min       1Q   Median       3Q      Max -1.04306 -0.57164 -0.26873  0.05331 18.46993 Count model coefficients (truncated negbin with log link):                       Estimate Std. Error  z value Pr(\u0026gt;|z|)    (Intercept)           -17.02648    0.06424 -265.048  \u0026lt; 2e-16 ***A                     -0.38514    0.07957   -4.840  1.3e-06 ***B                     -0.13069    0.06732   -1.941   0.0522 .  Log(theta)              0.14126    0.12476    1.132   0.2575    Zero hurdle model coefficients (binomial with logit link):                       Estimate Std. Error  z value Pr(\u0026gt;|z|)    (Intercept)           -16.38060    0.05235 -312.893  \u0026lt; 2e-16 ***A                     -0.31557    0.07265   -4.344  1.4e-05 ***B                     -0.07864    0.07668   -1.026    0.305    ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Theta: count = 1.1517Number of iterations in BFGS optimization: 10 Log-likelihood: -3115 on 7 Df\u0026gt; AIC(Mvfrequency)[1] 6243.296\u0026gt; vif(Mvfrequency)               A                     B              2.694534              3.328758 ","Creater_id":125048,"Start_date":"2016-07-29 18:21:40","Question_id":226392,"Tags":["regression","time-series","p-value","panel-data"],"Answer_count":1,"Last_activity":"2016-08-04 07:16:22","Link":"http://stats.stackexchange.com/questions/226392/suspiciously-low-p-values-and-narrow-cis","Creator_reputation":6}
{"_id":{"$oid":"5837a584a05283111e4d6284"},"View_count":2121,"Display_name":"L.C","Question_score":1,"Question_content":"I'm using the NLPCA to reduce the dimensionality of nine variables (4 nominal / 3 ordinal /2 numeric) to obtain the object-scores to be used as dependent variable in a regression model.I'm using the package homals (http://www.jstatsoft.org/v31/i04/paper).The output is:X \u0026lt;- homals(sintomi[,2:10], ndim = 1, active = TRUE, level = c(\"numerical\", rep(\"ordinal\",3),\"numerical\",rep(\"ordinal\",4)))XCall: homals(data = sintomi[, 2:10], ndim = 1, level = c(\"numerical\",     rep(\"ordinal\", 3), \"numerical\", rep(\"ordinal\", 4)), active = TRUE)Loss: 0.0002077596 Eigenvalues:    D1 0.0208 Variable Loadings:           D1V1  0.2096132V2  0.2420691V3  0.2171551V4  0.1370880V5  0.1508221V6  0.2537438V7  0.1959783V8  0.1769447V12 0.2154044predict(X)Classification rate:  Variable Cl. Rate %Cl. Rate1       V1   0.5149     51.49 2       V2   0.7292     72.92 3       V3   0.7589     75.89 4       V4   0.7768     77.68 5       V5   0.6336     63.36 6       V6   0.9048     90.48 7       V7   0.8869     88.69 8       V8   0.8036     80.36 9      V12   0.8661     86.61I'm having the following questions:Is it best to consider Ndim = rank = 1 or Ndim = rank = max(rank)  to reduce the dimensionality of data in one component?Is there a command to automatically calculate the proportion ofvariance explained by the first component? Otherwise, how can Icalculate it by hand? Is it necessary to standardize numeric variables before perfoming \"homals\"?If anyone has any thoughts for this, responses would be greatly appreciated.Thanks.","Creater_id":25871,"Start_date":"2013-05-20 06:11:46","Question_id":59510,"Tags":["pca","multivariate-analysis","nonlinear"],"Answer_count":0,"Last_activity":"2016-08-04 07:11:06","Link":"http://stats.stackexchange.com/questions/59510/nonlinear-pca-r-package-homals","Creator_reputation":6}
{"_id":{"$oid":"5837a584a05283111e4d6286"},"View_count":24,"Display_name":"mackbox","Question_score":3,"Question_content":"Suppose I want to calculate the relative entropy:D(q||p)= \\sum q(x)\\log \\frac{q(x)}{p(x)}If, for some , I have , does the corresponding factor in the sum becomes ? ","Creater_id":73733,"Start_date":"2016-08-04 06:00:57","Question_id":228250,"Tags":["mathematical-statistics","distance","kullback-leibler"],"Answer_count":1,"Last_activity":"2016-08-04 07:09:52","Link":"http://stats.stackexchange.com/questions/228250/what-is-the-factor-equal-to-if-the-true-and-empirical-distribution-both-are-0-fo","Creator_reputation":128}
{"_id":{"$oid":"5837a584a05283111e4d6288"},"View_count":34,"Display_name":"Warner","Question_score":0,"Question_content":"I found that in the forecast package in R that I can easily incorporate an exogenous variable Y in my ARIMA model meant to forecast X. While I have a general understanding of the kind of process I need to go through in order to determine the right ARIMA model on X, I'm not sure what I need to understand about Y in order to incorporate it as an exogenous variable.I was wondering if anybody could help me understand the implications of having this variable in my model and whether there are any tests/checks I can perform to determine whether it's a good idea to have it in my model (other than my intuition and subject matter understanding). Any reading material on this would be helpful. Thanks!","Creater_id":76482,"Start_date":"2016-08-04 06:58:49","Question_id":228258,"Tags":["time-series","arima"],"Answer_count":0,"Last_activity":"2016-08-04 06:58:49","Link":"http://stats.stackexchange.com/questions/228258/exogenous-variables-in-time-series-model","Creator_reputation":106}
{"_id":{"$oid":"5837a584a05283111e4d628a"},"View_count":14,"Display_name":"Reza_Research","Question_score":0,"Question_content":"We have always heard about the applications of GMM. But what does it mean to have a Multinomial Mixture Model (MMM)?1- What are the differences with GMM?2- When should we fit a MMM on data?Thank you ","Creater_id":126608,"Start_date":"2016-08-04 06:35:00","Question_id":228256,"Tags":["gaussian-mixture","mixture"],"Answer_count":0,"Last_activity":"2016-08-04 06:35:00","Link":"http://stats.stackexchange.com/questions/228256/applications-of-multinomial-mixture-model-mmm","Creator_reputation":48}
{"_id":{"$oid":"5837a584a05283111e4d628c"},"View_count":160,"Display_name":"kilojoules","Question_score":2,"Question_content":"In this paper: http://salserver.org.aalto.fi/vanhat_sivut/Opinnot/Mat-2.4108/pdf-files/emat08.pdf [1] equation 6 shows that if  is positive LHS does not reduce random sampling error. Is there a rule of thumb for determining if LHS sampling will actually have less error than random sampling? I do not want to try this out across all points of interest, it would be much more useful to have a general method for qualitatively determining LHS efficiency.[1] Anna Matala (2008), \"Sample Size Requierement for Monte Carlo - simulations using Latin Hypercube Sampling\", 60968U (20.5.2008) Mat-2.4108 Independent Research Projects in Applied Mathematics, Helsinki University of Technology       ","Creater_id":83526,"Start_date":"2016-07-25 17:12:22","Question_id":225617,"Tags":["probability","sampling","latin-hypercube"],"Answer_count":1,"Last_activity":"2016-08-04 06:33:14","Link":"http://stats.stackexchange.com/questions/225617/when-is-latin-hypercube-sampling-lhs-a-good-idea","Creator_reputation":83}
{"_id":{"$oid":"5837a584a05283111e4d628e"},"View_count":30,"Display_name":"rg255","Question_score":0,"Question_content":"In R, when using mixed effect regression models, a summary of the effects can be produced. Using mixed effect Cox models I get the following (see here for dummy data \u0026amp; script), where sex (M or F) and group (G1 or G2) are both two level factors, and NE is a continuous variable.coxdum \u0026lt;- coxme(SurvObj ~ Sex*Group*NE + (1|Year), data = dumDF)coxdum\u0026gt; coxdumCox mixed-effects model fit by maximum likelihood  Data: dumDF  events, n = 1000, 2000  Iterations= 1 7                     NULL Integrated   FittedLog-likelihood -6597.047   -6104.89 -6104.89                   Chisq df p    AIC    BICIntegrated loglik 984.32  8 0 968.32 929.05 Penalized loglik 984.32  7 0 970.32 935.96Model:  SurvObj ~ Sex * Group * NE + (1 | Year) Fixed coefficients                        coef exp(coef)   se(coef)      z      pSexM             1.971525293 7.1816222 0.10196386  19.34 0.0000GroupG2         -0.035924373 0.9647132 0.08891258  -0.40 0.6900NE              -0.176770390 0.8379722 0.01356340 -13.03 0.0000SexM:GroupG2     0.004847530 1.0048593 0.12708703   0.04 0.9700SexM:NE         -0.053164686 0.9482238 0.01909755  -2.78 0.0054GroupG2:NE      -0.001227879 0.9987729 0.01866811  -0.07 0.9500SexM:GroupG2:NE  0.043730917 1.0447012 0.02699633   1.62 0.1100I am interested in comparing the response to NE between various combinations of sex and group, for example, comparing the response to NE in males and females of group G2 (the null hypothesis being; males from group G2 and females from group G2 respond to NE equally).To test this should I: a) compare the coef for SexM:GroupG2:NE to GroupG2:NEb) reconstruct slopes by combining the coef for NE, SexM:NE, GroupG2:NE and SexM:GroupG2:NE to get the male specific slope, and NE and GroupG2:NE to get the female specific slope, and then compare thesec) something else entirely (e.g. just look at the coef for SexM:GroupG2:NE)I think b is correct (and then for example, coef for NE is equivalent to the regression coefficient for the response to NE specific to females from group G1), and would be done by this function: coxSlopeFunc = function(model, nfixed = 1){    if(nfixed ==1){    # Slope for Females + Group 1    FG1 = modelcoefficients[3] + modelcoefficients[3] + modelcoefficients[3] + modelcoefficients[6] + model$coefficients[7]    matrix(c(FG1,MG1,MG2,FG2), ncol = 1)    }}coxSlopeFunc(coxdum)\u0026gt; coxSlopeFunc(coxdum)             [,1][1,] -0.176770390[2,] -0.229935076[3,] -0.187432037[4,] -0.177998268This makes sense because I have not generated any sex or group specific effects apart from slightly stronger response in G1 males, thus all combinations of sex and group should respond similarly, with a slightly steeper slope in G1 males.","Creater_id":16542,"Start_date":"2016-08-04 03:43:53","Question_id":228229,"Tags":["regression","mixed-model","cox-model"],"Answer_count":0,"Last_activity":"2016-08-04 06:11:05","Link":"http://stats.stackexchange.com/questions/228229/reconstructing-slopes-in-mixed-effect-models","Creator_reputation":344}
{"_id":{"$oid":"5837a584a05283111e4d6290"},"View_count":106,"Display_name":"Cesar","Question_score":5,"Question_content":"How can I sample a value from a symmetric geometric distribution, as defined in this link.There, the density of the symmetric geometric proposal distribution is given by\\begin{equation*}f(\\theta;p_g)\\propto \\frac{p_g (1 - p_g)^{|\\theta|}}{2(1- p_g)},\\end{equation*}where the symmetric centers at . However, it seems that this distribution is not widely known under this name. It seems that this distribution is somehow related to the Laplace distribution, for which I know how to sample values, but I couldn't establish this relation myself.As such, my question is actually twofold: Is this distribution known under a more common name, and Is there any other distribution I can relate it to, in order to generate values from this distribution more easily?","Creater_id":1538,"Start_date":"2016-08-03 14:28:17","Question_id":227164,"Tags":["distributions","sas","random-generation","geometric-distribution"],"Answer_count":3,"Last_activity":"2016-08-04 06:09:20","Link":"http://stats.stackexchange.com/questions/227164/how-to-sample-value-from-symmetric-geometric-distribution","Creator_reputation":435}
{"_id":{"$oid":"5837a584a05283111e4d6292"},"View_count":6,"Display_name":"Mark Verheyden","Question_score":1,"Question_content":"Just wanted to check whether ANOVA can be used to study effects (e.g.: effect of age on intensity of social media use) based on the data from a one-off survey. Frequency of social media use is expressed as an ordinal variable with 5 categories: (1) not at all, (2) rarely, (3) sometimes, (4) often, (5) a great deal","Creater_id":111121,"Start_date":"2016-08-04 06:01:28","Question_id":228251,"Tags":["anova","survey"],"Answer_count":0,"Last_activity":"2016-08-04 06:01:28","Link":"http://stats.stackexchange.com/questions/228251/can-anova-be-used-to-study-effects-when-doing-a-one-off-survey","Creator_reputation":11}
{"_id":{"$oid":"5837a584a05283111e4d6294"},"View_count":9681,"Display_name":"dassouki","Question_score":35,"Question_content":"I was wondering if there is a statistical model \"cheat sheet(s)\" that lists any or more information:when to use the modelwhen not to use the modelrequired and optional inputsexpected outputshas the model been tested in different fields (policy, bio, engineering, manufacturing, etc)?is it accepted in practice or research?expected variation / accuracy / precisioncaveatsscalabilitydeprecated model, avoid or don't useetc ..I've seen hierarchies before on various websites, and some simplistic model cheat sheets in various textbooks; however, it'll be nice if there is a larger one that encompasses various types of models based on different types of analysis and theories.","Creater_id":59,"Start_date":"2010-08-04 09:39:49","Question_id":1252,"Tags":["references","modeling"],"Answer_count":4,"Last_activity":"2016-08-04 05:59:35","Link":"http://stats.stackexchange.com/questions/1252/statistical-models-cheat-sheet","Creator_reputation":519}
{"_id":{"$oid":"5837a584a05283111e4d6296"},"View_count":30,"Display_name":"Reza_Research","Question_score":2,"Question_content":"I am a little bit familiar with Hidden Markov Models. I have always seen cases with only one layer of hidden states and one layer of observations. Now I wonder to see if there is a possibility to add several layers of observations? and How can I handle the learning of parameters?While I was searching the google, I confronted with some cases with several layers of Hidden states (Gahramani's Factorial HMM model); However I am seeking for a one with multi layers of observations. To myself, it seems possible to learn a GMM for A, B and C observations; but I don't know exactly how to put it to work.I would appreciate if someone could possibly help me with a logical approach?Any hints, suggestions or good references would also help.Thanks everyone :) ","Creater_id":126608,"Start_date":"2016-08-04 05:52:38","Question_id":228249,"Tags":["machine-learning","model","hidden-markov-model","latent-variable"],"Answer_count":0,"Last_activity":"2016-08-04 05:52:38","Link":"http://stats.stackexchange.com/questions/228249/hidden-markov-model-with-several-observations","Creator_reputation":48}
{"_id":{"$oid":"5837a584a05283111e4d6298"},"View_count":55,"Display_name":"Dwaipayan Gupta","Question_score":1,"Question_content":"A speculator invests in the currency markets. Let  denote the amount of money in her account at the end of day . Her daily gains, , are independent random variables, each having distribution as shown below: P(J_n = j) = 0.18, \\; \\; if \\; \\; j = - £200. P(J_n = j) = 0.30, \\; \\; if \\; \\; j = - £100. P(J_n = j) = 0.12, \\; \\; if \\; \\; j = £0. P(J_n = j) = 0.40, \\; \\; if \\; \\; j = + £100.The speculator's aim is to increase her capital to £4000, when she will stop trading. She will also stop if the account falls to zero. Let  denote the probability that the speculator's capital ultimately reaches £4000, given that her account currently holds .Now, how do I calculate the values of ,  and  ?","Creater_id":88754,"Start_date":"2016-08-02 01:37:02","Question_id":226822,"Tags":["probability","distributions","random-variable"],"Answer_count":1,"Last_activity":"2016-08-04 05:51:31","Link":"http://stats.stackexchange.com/questions/226822/problem-involving-a-probability-distribution-and-random-variables","Creator_reputation":154}
{"_id":{"$oid":"5837a584a05283111e4d629a"},"View_count":23,"Display_name":"Rakesh","Question_score":1,"Question_content":"How to judge the accuracy of time series decomposition provided by R?Because at different level of data aggregation and seasonality different repetitive patterns appear in seasonality index. How to know that what is the best suited seasonality and granularity of data on which the decomposition is reliable?I am trying to find signal to noise ration where signal is considered as trend + seasonality and noise is reminder component. Is there any theoretical backing through which we can be confident of that at least we can rely on this decomposition for underlying patterns. ","Creater_id":126606,"Start_date":"2016-08-04 05:34:54","Question_id":228247,"Tags":["time-series","decomposition"],"Answer_count":0,"Last_activity":"2016-08-04 05:34:54","Link":"http://stats.stackexchange.com/questions/228247/accuracy-of-time-series-decomposition","Creator_reputation":6}
{"_id":{"$oid":"5837a584a05283111e4d629c"},"View_count":13,"Display_name":"SeisMike","Question_score":1,"Question_content":"I am very new to time series analysis and I couldn't find a satisfying answer on other posts about statistical significance.I have 3 times series for 3 distinct stations. Each of them corresponds to an average of 3 distinct measurements and display a striking, coherent change.I am asked to present the statistical significance of this change (not to run a significance test) with at least 95% confidence limits on my plots.My  biggest problem is that I am not 100% sure I understand what is expected from me ,and I only one shot to do this. Does that mean I should put the confidence interval based on the measurements used in the average?","Creater_id":126604,"Start_date":"2016-08-04 04:55:10","Question_id":228244,"Tags":["time-series","statistical-significance","confidence-interval"],"Answer_count":0,"Last_activity":"2016-08-04 04:55:10","Link":"http://stats.stackexchange.com/questions/228244/how-to-present-the-statistical-significance-of-changes-for-time-series","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d6320"},"View_count":34,"Display_name":"Gloomy","Question_score":2,"Question_content":"In my textbook they use the lifetimes of lightbulbs (or other mechanical failures) as an example for an application of the exponential distribution.Can/is this actually done in real life? The reason of my doubt is that the exponential distribution has the memoryless property, meaning that  P(X \\geq t + h \\ | \\ X \\geq t) = P(X \\geq h)  But for lightbulbs (or mechanical failures), because of physical considerations like wear and tear, consecutive expansion/contraction of the wire when turning the light bulb on/off, and so on, the probability does change if you start observing after a time , meaning P(X \\geq t + h \\ | X \\geq t) \u0026lt; P(X \\geq h) So that the exponential distribution isn't a valid model? Or is the difference so low that it can be ignored? ","Creater_id":126602,"Start_date":"2016-08-04 04:41:40","Question_id":228241,"Tags":["distributions","exponential","application"],"Answer_count":0,"Last_activity":"2016-08-04 04:41:40","Link":"http://stats.stackexchange.com/questions/228241/can-the-exponential-distribution-actually-be-used-to-model-light-bulbs-lifetime","Creator_reputation":11}
{"_id":{"$oid":"5837a585a05283111e4d6322"},"View_count":11,"Display_name":"tsitsi","Question_score":1,"Question_content":"I  want to test for heteroscedasticity in multivariate time series modelling for economic variables (at most 3). Which tests are the best and suitable for this modelling?","Creater_id":126592,"Start_date":"2016-08-04 03:15:35","Question_id":228223,"Tags":["multivariate-analysis","impulse-response"],"Answer_count":0,"Last_activity":"2016-08-04 04:36:28","Link":"http://stats.stackexchange.com/questions/228223/test-for-heteroscedasticity-in-multivariate-time-series-application-with-economi","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d6324"},"View_count":80,"Display_name":"mining","Question_score":0,"Question_content":"Both linear principal component analysis (PCA) and kernel principal component analysis (KPCA) are unsupervised dimension reduction methods. I have a dataset with  training samples and  test samples. The dimension of their features is . To fit the i.i.d assumption of PCA and KPCA, I randomly select a number of anchor samples from both training set and test set, to conduct PCA and KPCA. Suppose it is 5000 and use Gaussian kernel as the kernel function of KPCA. The  is selected from the range .Let their reduced dimensions  be selected from the range . Then based on the reduced features, I conduct a RBF-SVM classification. The hyper-parameters of RBF is selected by 5-fold cross-validation.From the classification results, it seems to be no significant different between PCA and KPCA. But in common sense, KPCA should be better than PCA. Are there any possible explanations?","Creater_id":30834,"Start_date":"2016-01-27 00:03:13","Question_id":192673,"Tags":["mathematical-statistics","pca","dimensionality-reduction","kernel-trick"],"Answer_count":1,"Last_activity":"2016-08-04 04:35:13","Link":"http://stats.stackexchange.com/questions/192673/how-to-compare-pca-with-kpca-for-dimension-reduction","Creator_reputation":205}
{"_id":{"$oid":"5837a585a05283111e4d6331"},"View_count":57,"Display_name":"Johanna","Question_score":2,"Question_content":"I'm having a strange problem running a meta-regression using the function rma.mv() in the 'metafor' package in R.Since some of my data are from multiple-endpoint studies, I have calculated the variance-covariance matrix so that correlations between outcomes are taken into account. I'm also using random effects at study and treatment level. As far as I'm aware, I have now covered all issues with regard to dependent effect sizes.The model looks like this:cov_mod \u0026lt;- rma.mv(Hedges_g, cov, mods = ~ days, random = ~ treatment | study, data = rev)When running the code, it gives this error message:Error in rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  :   Error during optimization.In addition: Warning message:In rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  :  V appears to be not positive definite.I have discovered that the problem lies with one particular study (9 effect sizes in total, coming from 3 treatment groups that were each tested at 3 moments in time). When I remove this study from the data set, the code runs without problem.Thus, apparently this particular study causes the matrix to be 'not positive definite'. I have read that this likely means that \"at least one of [the] variables can be expressed as a linear combination of the others\" (source).However, here comes the strange thing: I have replaced all values in the variance-covariance matrix relating to this particular study with random numbers between 0-1 (maintaining the symmetry), and the error message remains unchanged. I am puzzled, because the matrix can no longer be linearly predictable if it contains random numbers.What could be the issue?","Creater_id":94863,"Start_date":"2016-08-04 04:18:11","Question_id":228238,"Tags":["r","meta-analysis","covariance-matrix","meta-regression"],"Answer_count":0,"Last_activity":"2016-08-04 04:31:01","Link":"http://stats.stackexchange.com/questions/228238/not-positive-definite-variance-covariance-matrix-in-meta-regression-using-metaf","Creator_reputation":78}
{"_id":{"$oid":"5837a585a05283111e4d6333"},"View_count":26,"Display_name":"user90772","Question_score":0,"Question_content":"I have a number of right-censored datasets on the same population with different maximum, fixed uncensored values seen, e.g. one dataset with maximum observed value at 10, another at 20 and another at 80 (I think this is called type I censoring). There is high censoring (\u003e60%) and so the Kaplan-Meier estimate is biased. One thing that is weird is that the survival function is underestimated (I know the ground truth in some data and KM is always below the actual survival function), although according to the literature I have read that KM typically overestimates this under such heavy censoring.Are there any other estimators that can be used in such a setting? I have tried to fit parametric distributions but they are still quite biased. Is there anything else I could do, e.g. some way to estimate the bias and try to remove this? I know there has been a similar question here and here but I did not find anything on how to practically deal with this, unless there is nothing to do about it.","Creater_id":91239,"Start_date":"2016-08-04 03:25:27","Question_id":228225,"Tags":["survival","kaplan-meier"],"Answer_count":0,"Last_activity":"2016-08-04 04:27:07","Link":"http://stats.stackexchange.com/questions/228225/kaplan-meier-overestimate-of-survival-probability","Creator_reputation":66}
{"_id":{"$oid":"5837a585a05283111e4d6335"},"View_count":13,"Display_name":"Cantfindname","Question_score":1,"Question_content":"I have a task, where I want to perform anomaly detection on a 1-dimensional input. The values that the inputs take are by default bounded in the range [0,1], but the distribution is skewed very much towards 0, in the point that almost 95% of the values in my training set are 0. In other words, only few of the features in each input vector are non-zero (not the same features in every input), but still tend to stay close to 0.For performing the anomaly detection, I plan to use an autoencoder (with sigmoid units) that reconstructs the input and use the reconstruction error as a measure of anomaly.However, using the raw data to train the model, I figured that the reconstruction is noise with values close to 0, so that the error stays low at all times and no real training is performed.My question is what preprocessing can I perform on this kind of data to help the autoencoder learn? Normalization? Some other way to aggregate the data to higher values?","Creater_id":92927,"Start_date":"2016-08-04 04:11:30","Question_id":228235,"Tags":["neural-networks","autoencoders"],"Answer_count":0,"Last_activity":"2016-08-04 04:11:30","Link":"http://stats.stackexchange.com/questions/228235/autoencoder-with-sparce-input","Creator_reputation":126}
{"_id":{"$oid":"5837a585a05283111e4d6337"},"View_count":27,"Display_name":"m.a","Question_score":1,"Question_content":"I have a problem in understanding the concept of bai-perron test on structural break pointsparticularly my question is on SupF_t (m)=F_T (λ_1….,λ_m)would you explain what the supF is and how the value of this statistic is resulted? I want to know step by step how do the \"m\" is chosen by supF? also SupF(l+1|l) is hard to get...","Creater_id":125094,"Start_date":"2016-08-04 04:11:09","Question_id":228234,"Tags":["econometrics","structural-change"],"Answer_count":0,"Last_activity":"2016-08-04 04:11:09","Link":"http://stats.stackexchange.com/questions/228234/testing-of-bai-perron-structural-changes","Creator_reputation":11}
{"_id":{"$oid":"5837a585a05283111e4d6339"},"View_count":19,"Display_name":"Ping","Question_score":0,"Question_content":"Say, there are . Also, there is the estimated conditional distribution of  given , . If I draw a  from  for each , is it reasonable to say that  is a sample from ? Is it also reasonable to say that  is a sample from the estimated joint distribution ?","Creater_id":93431,"Start_date":"2016-08-03 21:28:15","Question_id":228193,"Tags":["sampling","conditional-probability"],"Answer_count":1,"Last_activity":"2016-08-04 04:05:44","Link":"http://stats.stackexchange.com/questions/228193/sample-from-a-estimated-conditional-distribution","Creator_reputation":62}
{"_id":{"$oid":"5837a585a05283111e4d6346"},"View_count":6806,"Display_name":"Chris","Question_score":6,"Question_content":"Can someone explain me in a intuitive way what the periodicity of a Markov chain is?It is defined as follows:For all states  in =gcdThank you for your effort! ","Creater_id":10749,"Start_date":"2013-01-29 19:17:16","Question_id":48838,"Tags":["stochastic-processes","markov-process"],"Answer_count":2,"Last_activity":"2016-08-04 04:01:20","Link":"http://stats.stackexchange.com/questions/48838/intuitive-explanation-for-periodicity-in-markov-chains","Creator_reputation":367}
{"_id":{"$oid":"5837a585a05283111e4d6354"},"View_count":36,"Display_name":"Marloes","Question_score":0,"Question_content":"This is a textbook question of which I don't understand why B should be the correct answer:  52% of the population in country X are men.John draws a sample of 50  persons from this population and counts the amount of men in the  sample. Steve takes a sample of 200 persons from this population and  also counts the amount of men. Steve draws a sketch to show John what  difference between both sampling distributions should be expected.  Which sketch could be used for this?To my knowledge both sampling distributions should be distributed around the same center, namely the 52% proportion. Only will Steve have a better chance of finding a sample proportion closer to the population proportion. This would lead me to answer A, however, according to the book it should be answer B. It could have something to do with the fact that there are more men in the population, so less accurate samples have a higher probability of picking more men than less men, and thus ending up with higher proportions. But then still the difference in sketch B seems very large.","Creater_id":18334,"Start_date":"2016-08-04 02:50:26","Question_id":228222,"Tags":["self-study","sampling","proportion"],"Answer_count":0,"Last_activity":"2016-08-04 03:49:11","Link":"http://stats.stackexchange.com/questions/228222/sampling-distributions-of-the-same-proporion-but-different-n","Creator_reputation":337}
{"_id":{"$oid":"5837a585a05283111e4d6356"},"View_count":89,"Display_name":"Boern","Question_score":1,"Question_content":"I'm currently working on a scientific paper and I'm struggle to answer the following question: What is the right term for the output (ranging from 0 to 1) of a logistic regression?Neither of these works (since being already taken): Certainty, probability, confidence.Ideas: rank, probability of occurrence, probability of success..?","Creater_id":75315,"Start_date":"2016-08-03 00:25:47","Question_id":227009,"Tags":["regression","logistic"],"Answer_count":3,"Last_activity":"2016-08-04 03:40:36","Link":"http://stats.stackexchange.com/questions/227009/logistic-regression-how-to-call-the-output","Creator_reputation":108}
{"_id":{"$oid":"5837a585a05283111e4d6365"},"View_count":156,"Display_name":"user3639557","Question_score":1,"Question_content":"I am reading this ICML2016 paper, and am puzzled with the first inequality (converted to equality) on section 2.2. Assume the model is  where  are the hidden variables. Also assume  is an unbiased estimator of the likelihood term, . From this we can conclude that E_{P(x)}[\\hat{I}]-P(x)=0\\Rightarrow E_{P(x)}[\\hat{I}] = P(x) now assume we want to establish a lower bound on  (similar to EM approach) and plug in the estimator , instead of  in the lower bound formulation. For this imagine the posterior distribution over the latent variables , to be estimated using   (i.e.,  is a variational posterior). So if we want to write \\begin{align}\u0026amp;\\log P(x) =\\log \\sum P(x,h)\\\\\u0026amp;\\Rightarrow \\log P(x) = \\log \\sum P(x,h)\\frac{Q(h|x)}{Q(h|x)}\\\\\u0026amp;\\Rightarrow \\log E_{Q(h|x)}[P(x,h)] \\ge E_{Q(h|x)}[\\log P(x,h)]\\\\ \u0026amp;\\Rightarrow \\log \\hat{I} \\ge E_{Q(h|x)}[\\log P(x,h)]\\end{align}Here are two the puzzling parts:I don't understand how they could driveE_{Q(h|x)}[\\log \\hat{I}]\\leq \\log E_{Q(h|x)}[\\hat{I}] = \\log P(x) given all mentioned in the above.They also say since  is an unbiased, it can be writtenE_{Q(h|x)}[ \\hat{I}] = P(x)  which is not clear why, given the unbiased estimator definition.","Creater_id":56676,"Start_date":"2016-08-03 23:38:41","Question_id":228205,"Tags":["bayesian","expectation-maximization","unbiased-estimator","variational-bayes","log-likelihood"],"Answer_count":1,"Last_activity":"2016-08-04 03:35:49","Link":"http://stats.stackexchange.com/questions/228205/expectation-of-an-unbiased-estimator-under-variational-inference-setting","Creator_reputation":298}
{"_id":{"$oid":"5837a585a05283111e4d6371"},"View_count":200,"Display_name":"RUser4512","Question_score":10,"Question_content":"I have been working with large data sets lately and found a lot of papers of streaming methods. To name a few:Follow-the-Regularized-Leader and Mirror Descent:Equivalence Theorems and L1 Regularization(http://jmlr.org/proceedings/papers/v15/mcmahan11b/mcmahan11b.pdf)Streamed Learning: One-Pass SVMs (http://www.umiacs.umd.edu/~hal/docs/daume09onepass.pdf)Pegasos: Primal Estimated sub-GrAdient SOlver for SVM http://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdfor here : Can SVM do stream learning one example at a time?Streaming Random Forests (http://research.cs.queensu.ca/home/cords2/ideas07.pdf)However, I have been unable to find any documentation regarding how they compare to each other. Every article I read seem to run experiments on different data set.I know about sofia-ml, vowpal wabbit, but they seem to implement very few methods, compared to the huge amount of existing methods! Are the less common algorithms not performant enough? Is there any paper trying to review as many methods as possible?","Creater_id":73794,"Start_date":"2015-09-21 04:36:31","Question_id":173470,"Tags":["machine-learning","references","large-data","online"],"Answer_count":0,"Last_activity":"2016-08-04 03:16:53","Link":"http://stats.stackexchange.com/questions/173470/state-of-art-streaming-learning","Creator_reputation":3354}
{"_id":{"$oid":"5837a585a05283111e4d6373"},"View_count":49,"Display_name":"RUser4512","Question_score":3,"Question_content":"Considering the number of features constant, Barnes-Hut t-SNE has a complexity of  random projections and PCAs have a complexity of  making them \"affordable\" for very large data sets. On the other hand, methods relying on Multidimensional scaling have a  complexity.Are there other dimension reduction techniques (apart from trivial ones, like looking at the first  columns, of course) whose complexity is lower than  ?","Creater_id":73794,"Start_date":"2016-08-03 15:20:15","Question_id":227173,"Tags":["pca","dimensionality-reduction","large-data","multidimensional-scaling","tsne"],"Answer_count":0,"Last_activity":"2016-08-04 03:16:20","Link":"http://stats.stackexchange.com/questions/227173/scalable-dimension-reduction","Creator_reputation":3354}
{"_id":{"$oid":"5837a585a05283111e4d6375"},"View_count":93,"Display_name":"Nikolay Nenov","Question_score":2,"Question_content":"I am using a randomForest package in R to discriminate between 4 categories (i.e. I want to predict to what class an observation belongs). My data consists of 80k+ observations and is heavily unbalanced with around 70% of all observations being in a single category.I want to decrease the class error for the other 3 categories (optionally, without completely trashing the error rate of the 1st class) and so I am experimenting with assigning different weights to the classes.I was wondering if there's a good method to determine in advance what would be an optimal class weights or does it all depend on what class error am I willing to tolerate?Many thanks!","Creater_id":24462,"Start_date":"2014-02-07 06:00:33","Question_id":85750,"Tags":["r","machine-learning","random-forest"],"Answer_count":0,"Last_activity":"2016-08-04 03:06:11","Link":"http://stats.stackexchange.com/questions/85750/identifying-what-weights-to-give-to-each-class-in-a-random-forest","Creator_reputation":135}
{"_id":{"$oid":"5837a585a05283111e4d6377"},"View_count":75,"Display_name":"Beta","Question_score":2,"Question_content":"I'm new to propensity score matching (PSM). So, my questions can be bit trivial.1) Suppose I've 3 treatment levels and want to check the effectiveness of the treatment levels. Treatment levels are taking drug on time, not taking drug on time and not taking drug at regular interval. For this I need to do multinomial logistic regression. But in PSM we do case-control study. So, how we are going to define which will be the case and which will be the control? Will it be the case that we will use one group as control for each time and other 2 groups as case?2) Can anyone please tell me which package to use for multilevel group in R. I checked this link. But this link is old. I also checked this package which seems to do multilevel. But is there any other option for packages?","Creater_id":4278,"Start_date":"2016-08-03 01:15:14","Question_id":227013,"Tags":["r","propensity-scores"],"Answer_count":2,"Last_activity":"2016-08-04 02:49:55","Link":"http://stats.stackexchange.com/questions/227013/propensity-score-matching-for-more-than-2-groups","Creator_reputation":1563}
{"_id":{"$oid":"5837a585a05283111e4d6385"},"View_count":25,"Display_name":"J. Morgan","Question_score":1,"Question_content":"I have fitted a copula to my two data series (orange yield and orange prices). Now that I have the relationship between two variables from the copula, I need to simulate a 100 000 values from the fitted copula, but I don't know how to do that.Any help will be greatly appreciated.","Creater_id":126576,"Start_date":"2016-08-04 02:45:20","Question_id":228219,"Tags":["monte-carlo","copula"],"Answer_count":0,"Last_activity":"2016-08-04 02:45:20","Link":"http://stats.stackexchange.com/questions/228219/how-to-do-a-monte-carlo-simulation-from-a-fitted-copula","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d6387"},"View_count":43,"Display_name":"user86533","Question_score":0,"Question_content":"I´d like to create a machine-learning classification model with caret based on MR-imaging data that I extract from a tumor. Lets assume that the tumor is composed of three different subregions (e.g. edema, necrosis, enhancement) and I extract several parameters (continuous variables) from these subregions. However, not every tumor shows necrosis so necrosis-related parameters cannot be calculated for some patients. How should I deal with this parameters so that I can include all patients (and all parameters) into my model? Excluding these patients would be an option, however I´d like to avoid it since I´d loose 10% of my sample size. Setting the values to NA and impute them is probably not the right solution (since the values are in not missing (in a classical sense), but simply cannot be calculated)Setting the values to \"0\" is probably also not recommended? So is there any good method how I should deal with this situation so that I can include all parameters from all these patients into my machine-learning model? ","Creater_id":86533,"Start_date":"2016-08-04 01:49:59","Question_id":228215,"Tags":["machine-learning","missing-data"],"Answer_count":1,"Last_activity":"2016-08-04 02:37:47","Link":"http://stats.stackexchange.com/questions/228215/handling-missing-data-in-machine-learning","Creator_reputation":4}
{"_id":{"$oid":"5837a585a05283111e4d6394"},"View_count":37,"Display_name":"user126588","Question_score":1,"Question_content":"I have a database of wild fish stocks which looks something like the attached picture and I would like to use a pivot table to explore the data. But I am having trouble exploring the data on fishing nation resulting from the fact that multiple nations are fishing each stock. Let's say I want a pivot breaking down the number of stocks each nation exploits by species: In the top version of the data the resulting pivot table shows me results for each possible combination of countries rather than the desired result of a breakdown by country. In the lower table, where separate countries are represented by multiple columns, the breakdown is fragmented in meaningful ways and so is equally unhelpful. I have a similar problem if I represent the multiple countries in separate rows, thus duplicating data points and this duplication also leads to problems with other summaries, such as counting the total number of stocks under a given condition.What I am ideally looking for is an approach that allows something like the pivot table at the bottom of the picture, which I made manually, to be generated automatically. Essentially I picture this being achieved by somehow including multiple pieces of data in a single cell. I know excel can sort of do this with the dropdown menu tool but I have not had any success in getting it to view the data simultaneously when using the pivot table; it just uses which ever data point is selected at the time. I’m not entirely sure excel can perform the trick I am suggesting and am open to using another program (I can pick-up excel like things quite quickly but only have programming experience with R). I am also open to an entirely new approach, but please keep in mind that my ultimate goal is to make a user-friendly interface for analysing fisheries data. After exploring several stack sites I felt that Cross-Validated was the best outlet for this problem. If you feel differently please feel free to suggest an alternative. ","Creater_id":null,"Start_date":"2016-08-04 02:37:11","Question_id":228217,"Tags":["excel","pivot-table"],"Answer_count":0,"Last_activity":"2016-08-04 02:37:11","Link":"http://stats.stackexchange.com/questions/228217/how-to-combine-multiple-variables-in-excel","Creator_reputation":null}
{"_id":{"$oid":"5837a585a05283111e4d6396"},"View_count":61,"Display_name":"NoChance","Question_score":0,"Question_content":"I have the following data sample (I have much more data):Language No. Of Articles    No. Of followersRussian 100                 1000English 80                  900Korean  30                  525I have the following assumptions to select from:1-Language affects No. of followers,2-Language and No. Of Articles affects No. Of followers.3-No. of Articles affects No. of Followers. For this I can use the Correlation test.What statistical test or measure do you suggest I use to identify which option is correct.In short I want to know which factor(s) affect the number of followers?Note: This is not a homework. I know little about Statistics! Please help. Thx.Edit - This is the graph of the tow variables (log/log):","Creater_id":124872,"Start_date":"2016-08-03 19:42:44","Question_id":228191,"Tags":["hypothesis-testing","basic-concepts"],"Answer_count":1,"Last_activity":"2016-08-04 02:23:14","Link":"http://stats.stackexchange.com/questions/228191/statistical-test-to-identify-cause-and-effect-which-is-the-best-test","Creator_reputation":103}
{"_id":{"$oid":"5837a585a05283111e4d63a3"},"View_count":14,"Display_name":"Rossella","Question_score":0,"Question_content":"I am working on a large project where multiple different technologies have been run on the same set of samples.To analyze microarray data I used limma. My doubt is the following: can limma be applied to other technologies as well like metabolomics (~600 metabolites) or CyTOF (comparison of cell population frequencies across group, ~ 10 cell populations) or was it specifically developed for gene expression? Not sure if I should switch to traditional linear modeling for this (lm) or I can recycle the code I used for limma.Any help will be greatly appreciated.","Creater_id":3614,"Start_date":"2016-08-04 01:30:45","Question_id":228211,"Tags":["hypothesis-testing"],"Answer_count":0,"Last_activity":"2016-08-04 01:30:45","Link":"http://stats.stackexchange.com/questions/228211/can-limma-be-used-with-other-omics-techologies-than-gene-expression","Creator_reputation":81}
{"_id":{"$oid":"5837a585a05283111e4d63a5"},"View_count":76,"Display_name":"Euphe","Question_score":1,"Question_content":"I have used an ARIMA(1,1,0) model on a stationary time series.The time series shows amount of fires (number between 0 and 12) per day over a few years in regions of Moscow.Both fitted and predicted values from ARIMA, however, lie between -4 and +4. Predicted values also coverage to zero almost instantly.I only need to predict for a week ahead. The model predicts negative amounts of fires.How do I interpret the results of ARIMA prediction? If these are not absolute values, how do I transition to absolute values?You can view the IPython notebook with more details here:https://www.dropbox.com/s/592ug74i4h1i7tb/ARIMA%2B2005-2008-eng%20%281%29.html?dl=0The results.fit().summary() output is as follows:                             ARIMA Model Results                              ==============================================================================Dep. Variable:                  D.САО   No. Observations:                 1247Model:                 ARIMA(1, 1, 0)   Log Likelihood               -2759.563Method:                       css-mle   S.D. of innovations              2.212Date:                Thu, 04 Aug 2016   AIC                           5525.126Time:                        10:01:24   BIC                           5540.512Sample:                    01-02-2005   HQIC                          5530.911                         - 06-01-2008                                         ===============================================================================                  coef    std err          z      P\u0026gt;|z|      [0.025      0.975]-------------------------------------------------------------------------------const           0.0040      0.041      0.096      0.923      -0.077       0.085ar.L1.D.САО    -0.5114      0.024    -21.020      0.000      -0.559      -0.464                                    Roots                                    =============================================================================                 Real           Imaginary           Modulus         Frequency-----------------------------------------------------------------------------AR.1           -1.9554           +0.0000j            1.9554            0.5000-----------------------------------------------------------------------------","Creater_id":114581,"Start_date":"2016-08-03 05:20:54","Question_id":227051,"Tags":["time-series","forecasting","predictive-models","arima"],"Answer_count":0,"Last_activity":"2016-08-04 00:02:29","Link":"http://stats.stackexchange.com/questions/227051/interpreting-arima-prediction-results","Creator_reputation":140}
{"_id":{"$oid":"5837a585a05283111e4d63a7"},"View_count":17,"Display_name":"ShanZhengYang","Question_score":0,"Question_content":"Due to a fundamental characteristic of the data, points are clustered together on a 1-D grid-like structure with equal spacing. Plotting these points in a histogram shows a pdf with several individual peaks. What clustering techniques could I use to infer the variance (the width) of each of these peaks, as well as the number of peaks? Is this done with density estimation? Maybe Dirichlet processes? ","Creater_id":80118,"Start_date":"2016-08-03 23:44:12","Question_id":228206,"Tags":["variance","nonparametric","pdf","inference","density-estimation"],"Answer_count":0,"Last_activity":"2016-08-03 23:44:12","Link":"http://stats.stackexchange.com/questions/228206/density-estimation-for-points-regularly-spaced-on-a-grid-infer-spacing-between","Creator_reputation":207}
{"_id":{"$oid":"5837a585a05283111e4d63a9"},"View_count":1253,"Display_name":"Peter Flom","Question_score":11,"Question_content":"Suppose that two groups, comprising  and  each rank a set of 25 items from most to least important. What are the best ways to compare these rankings?Clearly, it is possible to do 25 Mann-Whitney U tests, but this would result in 25 test results to interpret, which may be too much (and, in strict use, brings up questions of multiple comparisons). It is also not completely clear to me that the ranks satisfy all the assumptions of this test.I would also be interested in pointers to literature on rating vs. ranking.Some context: These 25 items all relate to education and the two groups are different types of educators. Both groups are small.EDIT in response to @ttnphns:I did not mean to compare the total rank of items in group 1 to group 2 - that would be a constant, as @ttnphns points out. But the rankings in group 1 and group 2 will differ; that is, group 1 may rank item 1 higher than group 2 does. I could compare them, item by item, getting mean or median rank of each item and doing 25 tests, but i wondered if there was some better way to do this.","Creater_id":686,"Start_date":"2013-03-04 08:56:00","Question_id":51295,"Tags":["ranks"],"Answer_count":3,"Last_activity":"2016-08-03 23:34:05","Link":"http://stats.stackexchange.com/questions/51295/comparison-of-ranked-lists","Creator_reputation":57762}
{"_id":{"$oid":"5837a585a05283111e4d63b8"},"View_count":21,"Display_name":"user3275222","Question_score":1,"Question_content":"I have time series data, of some numerical measure. The time interval is in seconds (although I have gaps in the data - missing values). I have data from 2 consecutive months, while for each month I have all the days, and 24 hours a day. For each hour, I have 60 minutes, and for each minute I have values for the seconds. This sums up to a series with over 1 million observations. The question I need to answer is: Given a history of 20 seconds, is it possible to predict the rate of the numerical variable in the next 10 seconds? My variables are: Time variable (date+time), Month, Day, Hour and Minute (all extracted from the time variable), and the numerical variable itself.I am not sure how to approach this problem. Can you give me some advice please? So far I even failed in visually looking at it, the statistical software can't plot it due to the size of it.Thank you in advance. ","Creater_id":81477,"Start_date":"2016-08-03 23:21:59","Question_id":228203,"Tags":["time-series","forecasting","large-data"],"Answer_count":0,"Last_activity":"2016-08-03 23:21:59","Link":"http://stats.stackexchange.com/questions/228203/large-time-series","Creator_reputation":159}
{"_id":{"$oid":"5837a585a05283111e4d63ba"},"View_count":42,"Display_name":"Gabriel","Question_score":3,"Question_content":"I'm aware of the Bland–Altman plot where the differences between two measures of a given parameter (obtained with two different methods) are plotted against their averages.What I need to plot, are the differences between two measures of parameter , against the difference between two measures of parameter .Both parameters are properties associated to the same physical phenomena (star clusters to be more precise, for example age and distance).Is there a name for such a plot?To make it more clear, this is the plot I'm producing:where . Each of those is the value of parameter , obtained with a different method, i.e.: \"Method 1\" and \"Method 2\"  (same for ).The curves are iso-density lines, for a fitted 2D Gaussian Kernel on the data.","Creater_id":10416,"Start_date":"2016-04-01 13:45:55","Question_id":205016,"Tags":["data-visualization","bland-altman-plot"],"Answer_count":0,"Last_activity":"2016-08-03 23:17:08","Link":"http://stats.stackexchange.com/questions/205016/is-there-a-name-for-a-differences-versus-differences-plot","Creator_reputation":690}
{"_id":{"$oid":"5837a585a05283111e4d63bc"},"View_count":49,"Display_name":"tom","Question_score":3,"Question_content":"Estimating a simple Confirmatory Factor Analysis (CFA) model with lavaan gives inconsistent results when I use cfa and lavaan(method = \"cfa\").library(lavaan)10 observations, four indicators foo \u0026lt;-  structure(list(ind1 = c(0.2, -1, -0.2, 0.6, 0.6, -0.2, -0.2,                             -1, 0.6, 0.2),                    ind2 = c(0.6, -0.2, -0.2, 0.6, 0.6, -0.2, -1,                            -1, 0.6, 0.6),                   ind3 = c(0.6, -0.2, -0.2, 0.6, 0.2, 0.2, -0.6,                             0.6, 0.6, 0.6),                    ind4 = c(1, -0.5, -1, 0.5, 0.5, -0.5, 0, 0, -0.5,                             0.5)),               codepage = 65001L, .Names = c(\"ind1\", \"ind2\", \"ind3\", \"ind4\"),              row.names = 1:10,              class = \"data.frame\")This model cfa(model = 'latent =~ ind1 + ind2 + ind3 + ind4', data = foo)Works fine. But this, which I thought was equivalentlavaan(model = 'latent =~ ind1 + ind2 + ind3 + ind4',   data = foo,    model.type = \"cfa\")causes an error:Error in lav_model_estimate(lavmodel = lavmodel, lavsamplestats = lavsamplestats,  :   lavaan ERROR: initial model-implied matrix (Sigma) is not positive definite;  check your model and/or starting parameters.What am I missing?","Creater_id":5941,"Start_date":"2016-08-03 13:25:14","Question_id":227150,"Tags":["r","sem","lavaan"],"Answer_count":1,"Last_activity":"2016-08-03 22:45:36","Link":"http://stats.stackexchange.com/questions/227150/lavaan-works-with-cfa-provides-error-with-lavaanmethod-cfa","Creator_reputation":133}
{"_id":{"$oid":"5837a585a05283111e4d63c9"},"View_count":13,"Display_name":"ShanZhengYang","Question_score":1,"Question_content":"When I plot my data into bins, there is a frequency of data points per bin, which I can plot with a histogram. Based on this probability density function, I would like to find the maximum likelihood estimation of various parameters associated with this pdf. How can I infer the number of \"peaks\" of this pdf? Is this possible using a Gaussian Mixture Model (let's say using Expectation Maximization)? I believe in this case, we are inferring the location and variance of each \"peak\" using Gaussian clusters. Secondly, how does one infer the number of clusters which model the data best? This would be the least number of clusters? ","Creater_id":80118,"Start_date":"2016-08-03 22:09:10","Question_id":228198,"Tags":["pdf","inference","expectation-maximization","gaussian-mixture","parametric"],"Answer_count":0,"Last_activity":"2016-08-03 22:29:59","Link":"http://stats.stackexchange.com/questions/228198/is-it-possible-to-infer-the-number-of-peaks-in-a-pdf-using-a-gaussian-mixture","Creator_reputation":207}
{"_id":{"$oid":"5837a585a05283111e4d63cb"},"View_count":73,"Display_name":"Antoni Parellada","Question_score":2,"Question_content":"Both a Random variable an a Statistic (understood as \"single measure of some attribute of a sample\") map to the real line the results of an outcome of a random experiment (in the case of a random variable), or a sample of such random experiments (in the case of a statistic). In both instances, and despite the name (“random”), the functions are deterministic: Once the outcome of the actual random experiment is actualized, the mapping to the real line that defines a random variable as a function, , is clearly specified, either verbally (e.g. \"number of heads in four coin tosses\") or algebraically (e.g. ). This seems to be equivalent to a statistic as a function applied to a sample (e.g. ), at least in its deterministic nature. Also, the interchangeability of both concepts seems reinforced by the this line in the Wikipedia entry: A statistic is an observable random variable.A straightforward answer would be to point out the summary use of sample statistics, such as the mean or the variance, as well as the use of test statistics for inference.So it may be at this point confusing why I am asking this question at all. It stems from a post yesterday on confidence intervals as random variables, and a response I ultimately decided to withdraw after this doubt became nagging (I will reinstate it just in case someone is interested in understanding the possible confusion better).Question: Are these two concepts interrelated enough to use them interchangeable  in explaining concepts such as confidence intervals? Is it OK to consider their main difference their reference to a single outcome (random variable) versus a sample (statistic)?","Creater_id":67822,"Start_date":"2016-08-02 17:29:13","Question_id":226977,"Tags":["random-variable","terminology","summary-statistics"],"Answer_count":1,"Last_activity":"2016-08-03 21:34:58","Link":"http://stats.stackexchange.com/questions/226977/how-to-define-a-sample-and-a-test-statistic-in-kolmogorovs-theory","Creator_reputation":7692}
{"_id":{"$oid":"5837a585a05283111e4d63d8"},"View_count":447,"Display_name":"Dbr","Question_score":9,"Question_content":"I've encountered this sentence while reading an article on sciencemag.org.  In the end, responses from just 7600 researchers in 12 countries were included because the remaining data were not considered statistically significant. Is this a proper way to do research?  To leave out results because they were not considered statistically significant?","Creater_id":5837,"Start_date":"2011-09-30 01:06:16","Question_id":16268,"Tags":["statistical-significance","sampling","outliers","theory"],"Answer_count":4,"Last_activity":"2016-08-03 20:01:17","Link":"http://stats.stackexchange.com/questions/16268/can-one-leave-out-data-from-research-because-it-is-not-significant","Creator_reputation":969}
{"_id":{"$oid":"5837a585a05283111e4d63e8"},"View_count":24,"Display_name":"VCG","Question_score":4,"Question_content":"I've got 2 independent draws from these two distributions : and . I want to find . I know that for two (0,1) independent Uniforms:However now when the support changes I'm struggling to figure it out. I thought maybe splitting it over the 2 supports because when z leaves X's support the CDf is 1. I tried to do a conditional thing like: but then I'd have to find P(z\u0026lt;1). Context: Mechanism design problem with 2 bidders in a first price sealed bid auction so I wanted to calculate expected revenue. ","Creater_id":124896,"Start_date":"2016-08-03 18:55:10","Question_id":228188,"Tags":["uniform"],"Answer_count":1,"Last_activity":"2016-08-03 19:39:59","Link":"http://stats.stackexchange.com/questions/228188/distribution-of-max-of-2-uniforms-with-different-support","Creator_reputation":518}
{"_id":{"$oid":"5837a585a05283111e4d63f5"},"View_count":42,"Display_name":"James Bond","Question_score":0,"Question_content":"I am building a predictive model for a company to predict counter party default risk.In a linear format, it looks like y = bXwhere X is around 50 independent variables, such as income, age, sex, years-of-education etcwhere y is either the profit we can make from the interest payment, or the loss of the principal when the counter party really goes default.Since counter parties will normally be able to pay off their interest, the y is heavily skewed to positive. However, when one of the counter parties really went to default, we lose the principal, and that's much bigger than the interest. So, the y vector contains a large number of +1 or +2, and a small fraction of large negative numbers like -10, -20, -50I tried OLS and it seems to have some predicting power, but I have big concerns about y's distribution. I read from somewhere saying that linear model doesn't require y to be normal. If that's the case, why we need generalized linear model (GLM)?can anyone clarify this a bit?I will appreciate any comments herethanks","Creater_id":36521,"Start_date":"2016-08-03 11:45:28","Question_id":227133,"Tags":["regression","generalized-linear-model"],"Answer_count":1,"Last_activity":"2016-08-03 17:05:41","Link":"http://stats.stackexchange.com/questions/227133/dependent-variable-is-not-normal-distribution-whats-the-best-linear-model-i-sh","Creator_reputation":106}
{"_id":{"$oid":"5837a585a05283111e4d63f7"},"View_count":2790,"Display_name":"Nick Riches","Question_score":6,"Question_content":"We’ve run a mixed effects logistic regression using the following syntax;# fit modelfm0 \u0026lt;- glmer(GoalEncoding ~ 1 + Group + (1|Subject) + (1|Item), exp0,             family = binomial(link=\"logit\"))# model outputsummary(fm0)Subject and Item are the random effects. We’re getting an odd result which is the coefficient and standard deviation for the subject term are both zero;Generalized linear mixed model fit by maximum likelihood (LaplaceApproximation) [glmerMod]Family: binomial  ( logit )Formula: GoalEncoding ~ 1 + Group + (1 | Subject) + (1 | Item)Data: exp0AIC      BIC      logLik deviance df.resid 449.8    465.3   -220.9    441.8      356 Scaled residuals: Min     1Q Median     3Q    Max -2.115 -0.785 -0.376  0.805  2.663 Random effects:Groups  Name        Variance Std.Dev.Subject (Intercept) 0.000    0.000   Item    (Intercept) 0.801    0.895   Number of obs: 360, groups:  Subject, 30; Item, 12Fixed effects:                Estimate Std. Error z value Pr(\u0026gt;|z|)     (Intercept)     -0.0275     0.2843    -0.1     0.92     GroupGeMo.EnMo   1.2060     0.2411     5.0  5.7e-07 *** --- Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Correlation of Fixed Effects:             (Intr) GroupGM.EnM -0.002This should not be happening because obviously there is variation across subjects. When we run the same analysis in stataxtmelogit goal group_num || _all:R.subject || _all:R.itemNote: factor variables specified; option laplace assumedRefining starting values: Iteration 0:   log likelihood = -260.60631  Iteration 1:   log likelihood = -252.13724  Iteration 2:   log likelihood = -249.87663  Performing gradient-based optimization: Iteration 0:   log likelihood = -249.87663  Iteration 1:   log likelihood = -246.38421  Iteration 2:   log likelihood =  -245.2231  Iteration 3:   log likelihood = -240.28537  Iteration 4:   log likelihood = -238.67047  Iteration 5:   log likelihood = -238.65943  Iteration 6:   log likelihood = -238.65942  Mixed-effects logistic regression               Number of obs      =       450Group variable: _all                            Number of groups   =         1                                                Obs per group: min =       450                                                               avg =     450.0                                                               max =       450Integration points =   1                        Wald chi2(1)       =     22.62Log likelihood = -238.65942                     Prob \u0026gt; chi2        =    0.0000------------------------------------------------------------------------------        goal |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------   group_num |   1.186594    .249484     4.76   0.000     .6976147    1.675574       _cons |  -3.419815   .8008212    -4.27   0.000    -4.989396   -1.850234------------------------------------------------------------------------------------------------------------------------------------------------------------  Random-effects Parameters  |   Estimate   Std. Err.     [95% Conf. Interval]-----------------------------+------------------------------------------------_all: Identity               |               sd(R.subject) |   7.18e-07   .3783434             0           .-----------------------------+------------------------------------------------_all: Identity               |                 sd(R.trial) |   2.462568   .6226966      1.500201    4.042286------------------------------------------------------------------------------LR test vs. logistic regression:     chi2(2) =   126.75   Prob \u0026gt; chi2 = 0.0000Note: LR test is conservative and provided only for reference.Note: log-likelihood calculations are based on the Laplacian approximation.the results are as expected with a non-zero coefficient / s.e. for the Subject term.Originally we thought this might be something to do with the coding of the Subject term, but changing this from a string to an integer did not make any difference.Obviously the analysis is not working properly, but we are unable to pin down the source of the difficulties. (NB someone else on this forum has been experiencing a similar issue, but this thread remains unanswered link to question)Any comments greatly appreciated!","Creater_id":24109,"Start_date":"2014-09-11 03:07:40","Question_id":115090,"Tags":["mixed-model","lme4"],"Answer_count":2,"Last_activity":"2016-08-03 16:56:15","Link":"http://stats.stackexchange.com/questions/115090/using-glmer-why-is-my-random-effect-zero","Creator_reputation":164}
{"_id":{"$oid":"5837a585a05283111e4d63f9"},"View_count":35,"Display_name":"user2543622","Question_score":0,"Question_content":"I have data related to a grocery store. I have 4 price points - 2.5, 3, 4, 5, and for each price point I have number of units that were sold on various days. Lets says that price 2.5 was active for 100 daysprice 3 was active for 120 daysprice 4 was active for 200 daysprice 5 was active for 300 daysI would like to understand relationship between price and qty sold by day. Lets assume that I can fit only a linear model for qty vs price.I can do 2 things:I can take average of sales by day for each price point. For examplefor 2.5 I will have average sale over 100 days. And then I can fit a regression line against price points. In this case I will have only 4 data point (1 for each price point)I can keep day level information and fit a regression line againstprice points (I will have 720 data points).Which method is better and will give a better regression equation. Why?","Creater_id":29065,"Start_date":"2016-08-03 16:40:12","Question_id":227182,"Tags":["regression","linear-model","least-squares","linear"],"Answer_count":0,"Last_activity":"2016-08-03 16:40:12","Link":"http://stats.stackexchange.com/questions/227182/linear-regression-multiple-points-vs-average","Creator_reputation":161}
{"_id":{"$oid":"5837a585a05283111e4d63fb"},"View_count":249,"Display_name":"morcillo","Question_score":1,"Question_content":"I am a beginner in the world of neural networks and I have a basic question that I'd like to get the answer to. Is there a way to estimate the maximum value of a weight or a way to keep it in an established range?ORIGIN OF THE PROBLEMFor now I've only been messing around with neural networks with simple problems (logic gates: xor, and, or, with 2 and more inputs). And what I want to do is to transfer a neural network from a PC to an FPGA. So far, I've been able to convert the inputs for most problems to value between 0 and 1, which is simple, so I'd like to know if there is a way to estimate the maximum value a weight can have in a specific neural network. THE PROBLEMThe problem with the size comes from the FPGA's multipliers, which are all 18bits (in my case) for a single multiplier and are not very limited in quantity, but are also not abundant. I know I can increase the resolution by using more than 1 multiplier per multiplication, but it would cut my resources in half and if I dare use floating point, I'd be cutting my resources by 1/5, which is bad for problems with many neurons. For example: My FPGA has 220 multipliers, which would allow for a maximum of 44 neurons per neural network (IF I don't want to use multipliers for the sigmoid function, which is possible in some cases) and I want to execute a neural network that has 64 neurons (real network that I actually would like to execute), so floating point is out of the question in this case. Imagine now that the maximum weight value is about 60,000, then 16 bits are necessary to represent it, and 2 bits for the decimal points (fixed point arithmetic), which may be bad, depending on the system's needs (I think). Imagine a system where  a value like 18.12 wold be translated to 18.00 or 18.25. There is a chance that a problem may have a solution in the PC but not in the FPGA.So that's my problem. I hope I've been able to give a clear description of the problem so that somebody can help me. ","Creater_id":92422,"Start_date":"2015-10-17 14:54:26","Question_id":177428,"Tags":["neural-networks"],"Answer_count":2,"Last_activity":"2016-08-03 15:30:49","Link":"http://stats.stackexchange.com/questions/177428/maximum-weight-value-for-a-neural-network","Creator_reputation":15}
{"_id":{"$oid":"5837a585a05283111e4d63fd"},"View_count":115,"Display_name":"RockTheStar","Question_score":5,"Question_content":"When do we use matrix norm? matrix norm is one of the property of a matrix, but I am not sure when I will use it. Do we use it for calculating a upper bound of a matrix?https://en.wikipedia.org/wiki/Matrix_norm","Creater_id":41749,"Start_date":"2015-09-29 10:06:11","Question_id":174706,"Tags":["matrix","summary-statistics","matrix-decomposition"],"Answer_count":1,"Last_activity":"2016-08-03 15:29:01","Link":"http://stats.stackexchange.com/questions/174706/when-do-we-use-matrix-norm","Creator_reputation":1643}
{"_id":{"$oid":"5837a585a05283111e4d63ff"},"View_count":38,"Display_name":"Chiara","Question_score":1,"Question_content":"I have calaculated a SOM (with the kohonen package in R, 18x18 heaxagonal grid, 500 iterations, 92 variables, 1189 cases) and am currently trying to access it's usability. Calling a functions for topological error (nodedist) 1.354823. This as well as average quantization error of 10.34 indicate huge dissimilarities in my data. Increasing the grid lead to higher topological errors and using a 10x10 grid almost doubled the quantisation error. I have calculated one in WEKA with the same 18x18 setup and it outputs clusters and the means, SE of the variables. Additionally it includes a variable called value, which I believe to state the value of the variable in the cluster, maybe similar to the weight (?).I am very new to this field as my data basically forced me into it (shiloettecoeffiient for h.cluster did not 'find' any structure). Therefore I am now a bit unsure on what to do with the result, as most literature only deals with \"this case worked\". Can I still report it with these error measurements?Also, it gives me constellations of the variables within it's \"clusters\", can I still use them or should I rather treat that like a super-insignificant p value thing?I am very grateful for help and have not included data examples, as my main question is how to deal with the above stated output properly.Thanks!!PS: Although predictive value would have been a desirable outcome it was mainly computed in order to observe if different groups map together at the same spot in such a map. Which they do not (already know how to report that) but my main concern is, could I still use the clusters WEKA generated?","Creater_id":125526,"Start_date":"2016-08-03 15:05:07","Question_id":227171,"Tags":["self-organizing-maps"],"Answer_count":0,"Last_activity":"2016-08-03 15:25:48","Link":"http://stats.stackexchange.com/questions/227171/som-which-topological-error-and-average-distance-are-acceptable","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d6401"},"View_count":24,"Display_name":"Jfly","Question_score":0,"Question_content":"We want to visualize the relationship between y (continuous), x (continuous), and treatment (two groups A and B). One way of doing this is plot y~x, with treatment group shown as two series / colors. This should demonstrate the interaction between x and treatment on y.Now, Let's say we have a regression model: y = intercept + a * Treatment + b * Covariates + c * x + d * x*TreatmentMy question is, how to visualize the relationship of y, x, and treatment, with the covariates considered? ","Creater_id":400,"Start_date":"2016-08-03 14:36:32","Question_id":227167,"Tags":["data-visualization","interaction"],"Answer_count":0,"Last_activity":"2016-08-03 15:24:57","Link":"http://stats.stackexchange.com/questions/227167/how-to-visualize-interaction-with-other-variables-controlled","Creator_reputation":233}
{"_id":{"$oid":"5837a585a05283111e4d6403"},"View_count":25,"Display_name":"MeiCheng Shih","Question_score":1,"Question_content":"Normally, the first layer models in stacking and bledning method are trained by all features in a data set.  However, what will be the performance of a big model based on first layer models trained by different features in a same data set? For example, if a big model contains 4 first layer models was built to predict the labels of a data set which contains 12 features.  Now, instead of using all 12 features to train each of the first layer models, the data set is divided into 4 subsets which each of them contains partial features.  For each of the subsets, it is used to train one of the sub models. What will be the accuracy of this model comparing to the model using all 12 featues to train each of its first layer models?Thanks! ","Creater_id":125558,"Start_date":"2016-08-03 15:18:33","Question_id":227172,"Tags":["machine-learning","stacking"],"Answer_count":0,"Last_activity":"2016-08-03 15:18:33","Link":"http://stats.stackexchange.com/questions/227172/stacking-models-which-trained-by-different-features-in-a-data-set-for-a-classifi","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d6405"},"View_count":1533,"Display_name":"dassouki","Question_score":9,"Question_content":"Provided a sample size \"N\" that I plan on using to forecast data. What are some of the ways to subdivide the data so that I use some of it to establish a model, and the remainder data to validate the model?I know there is no black and white answer to this, but it would be interesting to know  some \"rules of thumb\" or usually used ratios. I know back at university, one of our professors used to say model on 60% and validate on 40%.","Creater_id":59,"Start_date":"2010-07-26 11:24:35","Question_id":638,"Tags":["machine-learning","modeling","sample","validation"],"Answer_count":4,"Last_activity":"2016-08-03 15:15:51","Link":"http://stats.stackexchange.com/questions/638/calculating-ratio-of-sample-data-used-for-model-fitting-training-and-validation","Creator_reputation":519}
{"_id":{"$oid":"5837a585a05283111e4d6407"},"View_count":168,"Display_name":"jstaker7","Question_score":0,"Question_content":"I am training a deep neural network with several convolutional layers and a fully connected layer at the bottom and I am generating histograms of the weight distributions to try and understand how the network is training. When looking at the graphs, I found something puzzling: most of the weights are near zero and only a small portion of the weights are getting very large. Why is this happening? Is this good and expected, or is this undesirable? Although I have only posted two example layers, this is happening throughout my network.Additional information:Data is very sparse and nearly binary (mostly 1's, very few 0's)Input is normalized to be in the range 0-1Not using L1/L2 yet since weights are mostly smallActivations are all leaky Relu (a=0.3)I am performing batch normalization after each preactivation","Creater_id":93857,"Start_date":"2016-02-26 10:59:14","Question_id":198762,"Tags":["neural-networks"],"Answer_count":1,"Last_activity":"2016-08-03 15:04:03","Link":"http://stats.stackexchange.com/questions/198762/understanding-weight-distribution-in-neural-network","Creator_reputation":134}
{"_id":{"$oid":"5837a585a05283111e4d6409"},"View_count":7778,"Display_name":"Martin Velez","Question_score":10,"Question_content":"I have read about Student's t-test but it appears to work when we can assume that the original distributions are normally distributed.  In my case, they are definitely not.  Also, if I have 13 distributions, do i need to do 13^2 tests?","Creater_id":20589,"Start_date":"2014-02-26 01:12:54","Question_id":87921,"Tags":["r","hypothesis-testing","distributions"],"Answer_count":3,"Last_activity":"2016-08-03 14:42:21","Link":"http://stats.stackexchange.com/questions/87921/how-do-i-test-if-two-non-normal-distributions-differ","Creator_reputation":195}
{"_id":{"$oid":"5837a585a05283111e4d640b"},"View_count":36,"Display_name":"M.M","Question_score":1,"Question_content":"Given a set  and each element  has an assigned probability .Then, a selection process is applied to the set  such that, each element   is removed from the set  based on its probability .We do this process for  times, and for each time we want to find the number of deleted items from the set .Here starts my question:The number of deleted items, , from the list at any time  can be obtained by:Is the above formula correct?Do we need to consider the probability that an item  was not deleted at  when we counting the number of deleted items at , such that:thanks!","Creater_id":11917,"Start_date":"2016-08-03 13:25:33","Question_id":227151,"Tags":["probability"],"Answer_count":1,"Last_activity":"2016-08-03 14:39:49","Link":"http://stats.stackexchange.com/questions/227151/probabilistic-deletion-of-elements-from-a-set","Creator_reputation":198}
{"_id":{"$oid":"5837a585a05283111e4d640d"},"View_count":14,"Display_name":"andrewH","Question_score":0,"Question_content":"I have an experimental outcome in three outcome variables, y1, y2, y3. My objective function is:a * ln(y1  - y1*) +  b * ln(y2  - y2*) + ln(y3  - y3*) for a, b, c, y1, y2, y3 \u003e0. The starred variables represent the minimum acceptable value for each outcome variable.The outcome is the effect of five treatments, each of which is continuously variable, plus a random factor. I can set the value of each control variable, but trials are time consuming and costly, and need to be performed sequentially. Trials that pass one of lower acceptable values are potentially very, very costly. I have a very rough idea of the location of the optimum. The response curves are generally unimodal for each variable, but not necessarily symmetric. The form of interaction between them is unknown, but I have reason to believe that there are interactions, and that they are probably not terribly complicated. I have reason to believe that simultaneous halving or doubling of all the treatments at once would hit one or more of the minimum acceptable values, though this need not be true of any individual treatment -- it might be optimal to zero out one treatment while increasing others to compensate.Given these constraints, I would like to design a sensible experimental protocol. What sort of designs should I consider? Is there a good text explaining alternatives for problems of this sort? My statistical background is in econometrics, so my training in multivariate controlled experimental protocols is pretty limited. I am not sure if i should treat this as a problem in search or in estimation of a response surface Also, if anyone is aware of a literature on determining optimal dosing in multi-drug regimes – for efficacy, not toxicity, especially for the small-sample or n-of-one case –  a pointer toward it would be particularly appreciated.","Creater_id":12923,"Start_date":"2016-08-03 14:32:17","Question_id":227166,"Tags":["multivariate-analysis","interaction","optimization","experiment-design","treatment-effect"],"Answer_count":0,"Last_activity":"2016-08-03 14:32:17","Link":"http://stats.stackexchange.com/questions/227166/what-is-the-optimal-small-sample-experimental-design-for-multiple-treatments-and","Creator_reputation":631}
{"_id":{"$oid":"5837a585a05283111e4d640f"},"View_count":15,"Display_name":"Ido_f","Question_score":1,"Question_content":"I have a very short vector of intensities (let's say 5 elements per vector). The intensities are a discrete sampling of a curve around its maxima (the maxima is not discrete though). What I'd like to do then is to use those entires to approximate the curve and find the actual maxima. Yet, I cannot use an iterative algorithm, so I'll have to find something with a close form solution. Any suggestions?I experimented a bit with the comments fromGetting a second-order polynomial trend line from a set of databut it didn't seem to provide correct results.Thanks","Creater_id":42280,"Start_date":"2016-08-03 14:19:24","Question_id":227162,"Tags":["curve-fitting","approximation"],"Answer_count":0,"Last_activity":"2016-08-03 14:19:24","Link":"http://stats.stackexchange.com/questions/227162/a-close-form-solution-to-a-low-dimensional-curve-fitting","Creator_reputation":131}
{"_id":{"$oid":"5837a585a05283111e4d6411"},"View_count":43,"Display_name":"Daniel Pinto","Question_score":1,"Question_content":"I have a VAR model with two sets of variables, X and Y. Granger causality tests say that X granger causes Y but not the other way around.However, the impulse response functions suggest that the variables in Y systematically affect the variables in X in a statistically significant and positive fashion more often and with higher magnitude than the other way around. I find this extremely unintuitive.While it is true that Granger causality is pretty much a wald test on coefficients and impulse responses are based on innovations to the structural term, it is also true that the impulse response functions are themselves a function of the estimated coefficients...The only explanation I can find for this is that the standard errors on which the wald (granger causality) tests are based on tend to be very large for coefficients with large magnitude and smaller for coefficients with \"relatively\" small magnitude, so that we have significance (and hence granger causality) for variables associated with relatively low coefficients and not for variables with relatively high coefficients (because the standard errors are larger more than proportionally).","Creater_id":99392,"Start_date":"2016-08-03 14:15:23","Question_id":227161,"Tags":["granger-causality","impulse-response"],"Answer_count":0,"Last_activity":"2016-08-03 14:15:23","Link":"http://stats.stackexchange.com/questions/227161/granger-causality-and-impulse-response-functions-yield-conflicting-results","Creator_reputation":44}
{"_id":{"$oid":"5837a585a05283111e4d6413"},"View_count":34,"Display_name":"user2954573","Question_score":1,"Question_content":"I'm designing a machine learning system in which i'm training 4 different classifiers and voting among it's predictions to decide on an output. Presently i'm just making a simple vote (i.e. selecting the most recurring predicted class) but can't I do something better using the outputted probabilities of the models as a sort of weight to decide on the best class?","Creater_id":116088,"Start_date":"2016-08-03 13:38:38","Question_id":227155,"Tags":["machine-learning","classification","voting-system"],"Answer_count":1,"Last_activity":"2016-08-03 14:08:55","Link":"http://stats.stackexchange.com/questions/227155/scheme-for-weighted-voting-on-ensemble-machine-learning-system","Creator_reputation":21}
{"_id":{"$oid":"5837a585a05283111e4d6415"},"View_count":25,"Display_name":"Anf\u0026#228;nger","Question_score":0,"Question_content":"I have data that looks like| Product | Number Sold | Number Returned | Percent Returned ||---------+-------------+-----------------+------------------|| A       |          30 |              29 |            96.7% || B       |          15 |               3 |             6.7% || C       |           1 |               1 |             100% |Is there a better way to understand the rate of product return other than the percent returned? Having 29 out of 30 products returned seems way worse than having 1 out of 1 returned.","Creater_id":98316,"Start_date":"2016-08-03 12:23:16","Question_id":227140,"Tags":["descriptive-statistics","summary-statistics","basic-concepts"],"Answer_count":1,"Last_activity":"2016-08-03 14:04:11","Link":"http://stats.stackexchange.com/questions/227140/weighted-percentages","Creator_reputation":8}
{"_id":{"$oid":"5837a585a05283111e4d6417"},"View_count":1708,"Display_name":"Jota","Question_score":11,"Question_content":"I am using the glmer function from the lme4 package in R, and I'm using the bobyqa optimizer (i.e. the default in my case).  I am getting a warning, and I'm curious what it means.  Warning message:In optwrap(optimizer, devfun, start, rho$lower, control = control,  :  convergence code 3 from bobyqa: bobyqa -- a trust region step failed to reduce qI searched \"a trust region step failed to reduce q.\"  Found some information in the minqa package, which said \"Consult Powell for explanation.\"  I did (you can too, if you want! see the references and links to them below), but I fail to understand.  In fact, I failed to find anything about reducing q. M. J. D. Powell (2007) \"Developments of NEWUOA for unconstrained minimization withoutderivatives\", Cambridge University, Department of Applied Mathematics and Theoretical Physics,Numerical Analysis Group, Report NA2007/05, http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2007_05.pdf.M. J. D. Powell (2009), \"The BOBYQA algorithm for bound constrained optimization withoutderivatives\", Report No. DAMTP 2009/NA06, Centre for Mathematical Sciences, University ofCambridge, UK. http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2009_06.pdf.P.s. I know I can change the optimizer, and I will to see if I can get output without warnings or errors.  I will also check the gradient and Hessian if I can, as per a comment/answer by Ben Bolker.  I'm using glmer within dredge from MuMIn and I'm not sure if Ben's answer will work without some additional tinkering, but I'll work on it once my computer finishes what it's doing, anyway, I digress.UpdateAs per Dr. Bolker's comment below, I began looking through the FORTRAN code (Here is the code for anyone interested in looking but not downloading it).  \"430\" appears in the bobyqb.f portion of the code.  Simply search for \"430\" or \"reduce Q\" to find the relevant code.  This is my first encounter with FORTRAN code, but I think the code says that if the following conditions are met, produce the warning: NTRITS \u003e 0, VQUAD \u003e= 0, IPRINT \u003e 0.  \"The integer NTRITS is set to the number \"trust region\" iterations that have occurred since the last \"alternative\" iteration.\" VQUAD appears several times, and I'm not yet clear on it's significance as its value seems to be dependent on a variety of other variables, the values of which sometimes depend on other variables.From bobyqa.f: \"The value of IPRINT should be set to 0, 1, 2 or 3, which controls the amount of printing. Specifically, there is no output if IPRINT=0 and there is output only at the return if IPRINT=1.\".So, it seems the task is to figure out the significance of VQUAD being \u003e= 0 and, perhaps, understanding how / when IPRINT became \u003e 0.  I'll have to go back to the paper to have a look, but the math, or at least its symbolic expression, is a bit of a barrier for me.  Unless, someone knows about the algorithm or has the desire to learn about it, I think I'll have to iteratively increase my understanding of the warning by going back and forth between the papers, the code, and the internet until I understand what it means.","Creater_id":4521,"Start_date":"2014-03-13 15:18:34","Question_id":89945,"Tags":["r","optimization","lmer","convergence","glmm"],"Answer_count":1,"Last_activity":"2016-08-03 13:55:21","Link":"http://stats.stackexchange.com/questions/89945/meaning-of-a-convergence-warning-in-glmer","Creator_reputation":341}
{"_id":{"$oid":"5837a585a05283111e4d6419"},"View_count":37,"Display_name":"mpr","Question_score":2,"Question_content":"I am using PCA on a square matrix of pairwise distances between 6000 elements, where the columns can be viewed as variables and rows as observations.Here are some of my questions and concerns:1) What are the general assumptions in using PCA on large data, and what tests should I run beforehand? (for instance, is it necessary to check Bartlett's and KMO, and is it sufficient to just look at skewness values to confirm normality?)2) Is it reasonable to keep all components with eigenvalues greater than 1? (my analysis yielded about 200 such components that explain 90% of the variance, without any definite breaking point in the scree plot)3) Once the factors are extracted, what is a reasonable cutoff point for assigning a variable to a component? (I want to discard variables with loadings below 0.5, but this has meant keeping only about a third of them, leaving a few empty components. I don't mind this, since it means lower dimensionality, but I'm wondering whether it would be a terrible crime to discard so many variables)4) After discarding variables that load poorly or cross-load, should the PCA be repeated again with only the retained variables?I would be grateful for any guidance you can offer","Creater_id":125295,"Start_date":"2016-08-03 13:52:17","Question_id":227157,"Tags":["pca","data-mining","matlab","large-data","robust"],"Answer_count":0,"Last_activity":"2016-08-03 13:52:17","Link":"http://stats.stackexchange.com/questions/227157/several-questions-about-using-pca-on-large-data","Creator_reputation":11}
{"_id":{"$oid":"5837a585a05283111e4d641b"},"View_count":15,"Display_name":"Thiago","Question_score":1,"Question_content":"I want to find a mathematically-based way to decide wether or not to pause a keyword belonging to an online advertisement campaign.A keyword might make a user click my ad which might lead to a sale. If, during the last 90 days, a keyword didn't generate any sale, I have to consider pausing it. In that case, I use the average performance of the campaign to make the decision:Then, the question is: has the keyword performed far from the campaign (on average) because it's a bad choice (and should be paused) or because it's not been given enough time (not enough clicks, in consequence)?The method I thought of is as follows:Do a Chi-Squared test with some significance level (e.g. 5%) and find the needed number of clicks.   ()On average, we expect the keyword to generate  sales each day.So: Which yields:Now, is it correct to say that if we had a number of clicks as big as or bigger than , the keyword is actually a bad choice and should be paused?","Creater_id":125538,"Start_date":"2016-08-03 13:50:52","Question_id":227156,"Tags":["hypothesis-testing","statistical-significance","chi-squared"],"Answer_count":0,"Last_activity":"2016-08-03 13:50:52","Link":"http://stats.stackexchange.com/questions/227156/online-advertisement-performance","Creator_reputation":106}
{"_id":{"$oid":"5837a585a05283111e4d641d"},"View_count":24,"Display_name":"Jin","Question_score":1,"Question_content":"I am currently working with some survey data that has ordinal categories for drinking/substance abuse problem (0=Never binge drink, 1=once a month, 2=once a week...4=nearly every day). Right now, my primary focus is trying to test association of having at least one \"violent\" crime charge (coded as violent=1) vs. having none (coded as violent=0) on these substance abuse distributions. I am not trying to build a through model, just trying to get a feel about the population I'm working with, and find possible leads on variables to look out for. First method I thought of was Fisher's Exact Test, as most of my other survey questions were binary and it resulted in a nice, clean 2x2 contingency table (e.g. History of mental illness (Y/N) X Violent Crime). But then I realized it's sub-optimal to use Fisher's Exact Test when the variable was ordinal rather than simply categorical. Deriving the mean score for the likert scale and running a t-test was another idea, but a lot of people seem to be saying I shouldn't do that, as calculating the mean of something like a likert scale can be misleading. I've tried using Mann-Whitney, but I haven't worked with non parametric methods a lot so I'm shaky on that, and although it seems like the correct test to use, I am no expert and would probably want guidance on that. Lastly, I am kind of averse to using cumulative ordered logit models since I am nowhere near checking all the assumptions thoroughly and building a model that includes all the relevant variables. I am afraid if I just run something like -ologit violent alcohol-, the resulting p-value could be screwed up as the model is imperfect.So what test would you recommend that I use to test for association in a 2x5 contingency table where 2 is categorical/dummy for the violence status and 5 are ordinal categories? Could you correct me if I am making some erroneous assumptions about the tests I am using/thinking of using?","Creater_id":125550,"Start_date":"2016-08-03 13:29:56","Question_id":227153,"Tags":["ordinal","contingency-tables","mann-whitney-u-test","fishers-exact"],"Answer_count":0,"Last_activity":"2016-08-03 13:29:56","Link":"http://stats.stackexchange.com/questions/227153/tests-of-association-when-using-likert-style-ordinal-data","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d641f"},"View_count":29,"Display_name":"Robert Brown","Question_score":0,"Question_content":"I am trying to find out whether the performance of the geometric mean of a distribution as a measure of its central tendency would be impaired by the distribution being multimodal. For example, imagine I want to estimate the most likely central value for the probability distribution of stock market returns, which is unknown. I draw a random sample of equities in order to use sample moments to estimate the population moments. I can do this using sample metrics such as arithmetic / geometric mean, median, mode etc. If I use a geometric mean to estimate the most likely central value for the population pdf, and I have bimodal / multimodal data, will the geometric mean of that sample be more biased as an estimate of the central tendency of the population than it would be were the data unimodal? For instance, I know that the arithmetic mean of both unimodal and bimodal samples here would produce the same value. What if the whole population followed a bimodal distribution? Would a geometric mean be inappropriate to use to estimate population mean from a sample mean in this case?Sorry if this is unclear. I am still new to statistics. Thanks in advance, Robert. ","Creater_id":88732,"Start_date":"2016-08-03 12:18:06","Question_id":227138,"Tags":["distributions","descriptive-statistics","bias","bimodal"],"Answer_count":0,"Last_activity":"2016-08-03 13:29:20","Link":"http://stats.stackexchange.com/questions/227138/geometric-mean-appropriateness-with-bimodally-distributed-data","Creator_reputation":21}
{"_id":{"$oid":"5837a585a05283111e4d6421"},"View_count":531,"Display_name":"StevieP","Question_score":4,"Question_content":"I was wondering if it was possible to train an SVM (say a linear one, to make things easy) using back propagation?Currently, I'm at a road block, because I can only think about writing the classifier's output asf(\\mathbf{x};\\theta,b) = \\text{sgn}(\\theta\\cdot\\mathbf{x} - (b+1)) = \\text{sgn}(g(\\mathbf{x};\\theta,b))Hence, when we try and calculate the \"backwards pass\" (propagated error) we get\\begin{align}\\frac{\\partial E}{\\partial \\mathbf{x}} \u0026amp;= \\frac{\\partial E}{\\partial f(\\mathbf{x};\\theta,b)} \\frac{\\partial f(\\mathbf{x};\\theta,b)}{\\mathbf{x}} \\\\\u0026amp;= \\frac{\\partial E}{\\partial f(\\mathbf{x};\\theta,b)} \\frac{\\partial \\text{sgn}(g(\\mathbf{x};\\theta,b))}{\\partial g(\\mathbf{x};\\theta,b)} \\frac{\\partial g(\\mathbf{x};\\theta,b)}{\\partial \\mathbf{x}} \\\\\u0026amp;= \\delta \\, \\frac{d \\text{sgn}(z)}{dz} \\, \\theta \\\\\u0026amp;= \\delta \\cdot 0 \\cdot \\theta \\\\\u0026amp;= \\mathbf{0}\\end{align}since the derivative of  is\\frac{d\\text{sgn}(x)}{dx} =\\begin{cases}0 \u0026amp;\\text{if }\\\\2\\delta(x) \u0026amp;\\text{if }\\end{cases}Similarly, we find that , which means we cannot pass back any information, or perform gradient updates!What gives?","Creater_id":31120,"Start_date":"2015-06-25 15:33:35","Question_id":158712,"Tags":["machine-learning","svm","neural-networks","gradient-descent"],"Answer_count":2,"Last_activity":"2016-08-03 13:27:37","Link":"http://stats.stackexchange.com/questions/158712/train-an-svm-via-back-propagation","Creator_reputation":258}
{"_id":{"$oid":"5837a585a05283111e4d6423"},"View_count":18,"Display_name":"octern","Question_score":0,"Question_content":"Summary:A multilevel linear model for a repeated measures experiment can represent condition as a subject level variable (coded \"treatment\" for treatment participants, even at pre-treatment observations) or an observation level variable (coded \"treatment\" only for measurements after the individual has received the treatment) that interacts with timepoint. This choice leads to small differences in parameter estimates. Is this due to the effect of generating a parameter estimate for the random and usually small pre-treatment differences between conditions? Which approach is preferable?Explication:Imagine an experiment in which all participants are measured at t1, half receive a treatment, and all are remeasured at t2. There are: 1) an underlying baseline for each participant that doesn't change, 2) an overall positive change over time, and 3) a positive effect of receiving the treatment.#1 is a random subject-level intercept and #2 is a fixed effect of time. #3 could be described in two ways, depending on how you code the data:3a. Each observation has a condition variable based on whether the treatment had been received at that time (0 for everyone at t1, and 0 for controls or 1 for treatment at t2). This creates a fixed effect of condition.3b. Each participant has a subject-level condition variable (0 for control participants, 1 for treatment participants). This creates no fixed effect of condition, but a fixed effect for the interaction between condition and time.3a is just 3b with the interaction coding already done. They're logically identical. But when I test them using a multilevel linear model, the results aren't exactly the same. See R code below for a demonstration.Question 1: I think they're different because the main effect of condition in model 3b isn't really zero. It's based on random pre-treatment differences between conditions, and this affects the other parameter estimates. Is this the only reason the models differ?Question 2: Which is better?Model 3a, because it's not bothering to estimate an effect that we know is just noise?Model 3b, because estimating a main effect of condition controls for imperfections in randomization and gives a slightly more accurate estimate of the effect of interest?Demonstration:library(lmerTest)# random intercept for each participantparticipants \u0026lt;- rnorm(200) * .4df \u0026lt;- data.frame(\"participant\"=rep(1:200, each=2))dfparticipant]#dummy variable for timedftime.val \u0026lt;- dfcond.subjectlevel \u0026lt;- rep(c(0,1), each=200)#dummy variable for condition (time-varying, always 0 at pre-treatment)dfcond.subjectlevel * dfcond.val \u0026lt;- dfdv \u0026lt;- dfcond.val + dfcoefficientssummary(lmb)$coefficients","Creater_id":9816,"Start_date":"2016-08-03 07:49:38","Question_id":227086,"Tags":["mixed-model","repeated-measures","multilevel-analysis","lme4"],"Answer_count":0,"Last_activity":"2016-08-03 13:21:30","Link":"http://stats.stackexchange.com/questions/227086/multilevel-regression-for-pre-post-experiment-estimate-meaningless-main-effec","Creator_reputation":310}
{"_id":{"$oid":"5837a585a05283111e4d6425"},"View_count":425,"Display_name":"hw.fu","Question_score":1,"Question_content":"I want to find 24 topics in 800,000 documents by using LDA model, but how many iterations should I give? It is extremely slow when the parameter is large, like 3000.Are there any strategies to ensure the stability? Seems giving the iteration a large value is the only way I can think of.","Creater_id":29614,"Start_date":"2013-09-10 19:31:06","Question_id":69727,"Tags":["topic-models"],"Answer_count":2,"Last_activity":"2016-08-03 13:16:37","Link":"http://stats.stackexchange.com/questions/69727/iteration-parameter-in-latent-dirichlet-allocation-model","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d6427"},"View_count":1124,"Display_name":"Colin K","Question_score":18,"Question_content":"Let's say I'm computing some model parameters my minimizing the sum squared residuals, and I'm assuming my errors are Gaussian. My model produces analytical derivatives, so the optimizer does not need to use finite differences. Once the fit is complete, I want to calculate standard errors of the fitted parameters.Generally, in this situation, the Hessian of the error function is taken to be related to the covariance matrix by: \\sigma^2 H^{-1} = C where  is the variance of the residuals.When no analytical derivatives of the error are available, it is typically impractical to compute the Hessian, so  is taken as a good approximation.However, in my case, I've got an analytical J, so it's relatively cheap for me to compute H by finite differencing J.So, my question is this: Would it be more accurate to approximate H using my exact J and applying the above approximation, or to approximate H by finite differencing J?","Creater_id":2426,"Start_date":"2013-09-26 08:46:36","Question_id":71154,"Tags":["standard-error","fitting"],"Answer_count":1,"Last_activity":"2016-08-03 13:04:07","Link":"http://stats.stackexchange.com/questions/71154/when-an-analytical-jacobian-is-available-is-it-better-to-approximate-the-hessia","Creator_reputation":262}
{"_id":{"$oid":"5837a585a05283111e4d6429"},"View_count":14,"Display_name":"ShanZhengYang","Question_score":1,"Question_content":"Let's say you begin with a binary matrix (only 0 and 1) and you must impute 60% missing values. How would you do this? It is possible to use nearest neighbors? ","Creater_id":80118,"Start_date":"2016-08-03 12:30:53","Question_id":227144,"Tags":["matrix","data-imputation","sparse"],"Answer_count":0,"Last_activity":"2016-08-03 12:30:53","Link":"http://stats.stackexchange.com/questions/227144/how-to-impute-a-highly-sparse-binary-matrix","Creator_reputation":207}
{"_id":{"$oid":"5837a585a05283111e4d642b"},"View_count":32,"Display_name":"xralphyx","Question_score":0,"Question_content":"If I understand the phia documentation correctly, when using testInteractions for factors, the value it returns is a contrast of adjusted means, but if the \"slope\" argument is included, it is then returning a contrast between two slopes, which I interpret to mean a parameter coefficient.  Is this correct?So if I run:testInteractions(model, pairwise=\"Factor\")and get:P-value adjustment method: holm                          Value Df Chisq Pr(\u0026gt;Chisq)ConditionA-ConditionB 0.059987  1 1.453     0.2281The correct way to report this would be: Condition A was not significantly higher than condition B, m = .06, X^2(1) = 1.45, p = .23.But if I run:testInteractions(model, slope=\"Covariate\", pairwise=\"Factor\")and get:Adjusted slope for Covariate Chisq Test: P-value adjustment method: holm                            Value Df  Chisq Pr(\u0026gt;Chisq)ConditionA-ConditionB -0.0094811  1 1.3427     0.2466Then the correct way to report this would be: The slope for Covariate was not significantly different for condition A and condition B, b = -.009, X^2(1) = 1.34, p = .25.Is this correct? The fact that they both are simply labelled \"Value\" makes me second guess this interpretation as it seems to me to be implying that they represent the same type of statistic...I am using the lmer function in lme4 to fit this model, if that makes any difference.","Creater_id":120375,"Start_date":"2016-08-02 09:45:29","Question_id":226912,"Tags":["r","lmer","reporting"],"Answer_count":0,"Last_activity":"2016-08-03 11:52:25","Link":"http://stats.stackexchange.com/questions/226912/reporting-results-from-r-package-phia","Creator_reputation":13}
{"_id":{"$oid":"5837a585a05283111e4d642d"},"View_count":286,"Display_name":"Rinrin","Question_score":0,"Question_content":"\\begin{align}P_k\u0026amp;= \\text{probability of a randomly chosen family having exactly k children} \\\\\u0026amp;= αp_k,\\qquad k=1,2,..\\end{align}Suppose that all sex distribution of  children are equally likely. Find the probability that a family has exactly  boys, . Find the conditional probability that a family has at least two boys, given that it has at least one boy.So  and If I were to get the probability that a family has exactly  boys then that isP(R=r)=\\sum P(K=k)P(R=r|K=k)I substituted values to getP(R=r)=\\sum αp_k(_kC_r(0.5)^k)Now the probability being asked is P(R≥2 | R≥1)= P(R≥2 ∩ R≥1) / P(R≥1) and  is just  soP(R≥2 | R≥1)= P(R≥2) / P(R≥1)????","Creater_id":89219,"Start_date":"2015-09-11 01:40:56","Question_id":172001,"Tags":["probability","self-study"],"Answer_count":1,"Last_activity":"2016-08-03 11:37:18","Link":"http://stats.stackexchange.com/questions/172001/probability-of-a-family-having-at-least-two-boys-given-that-it-has-at-least-one","Creator_reputation":19}
{"_id":{"$oid":"5837a585a05283111e4d642f"},"View_count":21,"Display_name":"Steve Scher","Question_score":1,"Question_content":"The general question is: If Model 1 has a certain structure, and in Model 2 you free some of the parameters, and fix others, are the models nested?Specifically in the case I'm dealing with:One model is a bifactor model (see Reise, 2012), but with two 'general' factors, and the specific-level factors correlated. So: My indicators each load on two factors - 1 of 6 specific factors, and one of 2 'general' factors. The correlations within the specific factors are free.The second model removes the correlations within the specific factors (i.e., fixes them to zero) and adds a truly general factor (so that all the indicators load on it; so the loadings are freed compared to the previous model).Are these models nested?Thanks.Figures of the models:MODEL ONE:MODEL TWO:","Creater_id":123700,"Start_date":"2016-08-03 09:53:18","Question_id":227118,"Tags":["nested","confirmatory-factor"],"Answer_count":0,"Last_activity":"2016-08-03 11:27:01","Link":"http://stats.stackexchange.com/questions/227118/are-these-models-nested-cfa-sem","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d6431"},"View_count":24,"Display_name":"pfournier","Question_score":-1,"Question_content":"The problem at hand was to predict two values,  and , from another one, . We decided that a mixed model would be apropriate considering the clustered nature of data. After visual inspection, we decided to model a different intercept for each  as well as a random slope for each cluster/ combination. The model can be stated asY_{ij}^k = \\beta_0^k + (\\beta_1^k + u_j^k) X_{ij} + \\epsilon_{ij}^kwhere  is the cluster,  is the subject within cluster  and  is the measured value.  is the intercept for variable .  and  are is the fixed and random effect of .  is the residual variance. Both  and  have a multivariate normal distribution. Just so I'm clear, the covariance matrix for  is\\mathbf{G} = \\mathbf{Id}_m \\otimes \\pmatrix{\\eta_1^2 \\quad \\eta_{1 2} \\\\ \\eta_{1 2} \\quad \\eta_2^2}.The covariance matrix for  is\\mathbf{R} = \\mathbf{Id}_n \\otimes \\pmatrix{\\sigma_1^2 \\quad \\sigma_{1 2} \\\\ \\sigma_{1 2} \\quad \\sigma_2^2}.We fitted the model using SAS proc mixed and everything went fine. Now that it's done, we're looking for a book or a paper to back up our method. After some research, we came up with a few article, but none of them was close enough to what we've done. Those who were the closest were specifically about repeated measures designs. Since we're not taking the same measure twice on each subject but rather two different measures, they did not seem apropriate. Others did pretty much what we've done, but none of them modelised an intercept ( in our model). Finaly, I even read a paper that reformulated a similar model in term of structural equations, but I would like to avoid those. Every idea is welcomed!","Creater_id":122404,"Start_date":"2016-07-06 13:17:35","Question_id":222494,"Tags":["clustering","mixed-model","references","multivariate-regression","intercept"],"Answer_count":0,"Last_activity":"2016-08-03 11:19:59","Link":"http://stats.stackexchange.com/questions/222494/academic-reference-request-on-multivariate-mixed-model","Creator_reputation":1}
{"_id":{"$oid":"5837a585a05283111e4d6433"},"View_count":23,"Display_name":"dirkpelt","Question_score":1,"Question_content":"I have a dataset of people who completed a computer adaptive test (CAT), and calculated a person fit statistic for each examinee. Since the distribution of the statistic is unknown, I want to use a bootstrap procedure to approximate the sampling distribution of the 95th quantile, based on a number of resamples, in order to get a cut off to flag \"high\" (i.e. \"bad\") person fit scores.So, I simulated =1000 examinees with  from distribution , ran the CAT, calculated the fit statistic (vector  of length ), performed the bootstrapping procedure and took the median as the cut off score. However, as I simulate a different set of 1000 examinees and repeat this procedure, I (naturally) arrive at a somewhat different cut off score. I want to arrive at a \"best estimate\" of the cut off score, which in my eyes implies replications and then taking the mean. My question is: when to replicate? And when do I take the mean?Do I repeat the above procedure  times and then take the mean of the outcomes as my cut off (seems odd, taking the mean of the medians)? Or do I simulate a new set of examinees  times, \"save\" vector  each time (creating an  matrix), take the mean over the columns (creating a new vector , i.e. \"mean fit statistic over  replications\", of length ) and then perform the bootstrapping procedure?Or are there other/better alternatives?","Creater_id":125527,"Start_date":"2016-08-03 11:07:20","Question_id":227131,"Tags":["bootstrap","simulation","irt"],"Answer_count":0,"Last_activity":"2016-08-03 11:07:20","Link":"http://stats.stackexchange.com/questions/227131/where-when-to-implement-replications-in-bootstrapping","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d6435"},"View_count":28,"Display_name":"jjjjjj","Question_score":0,"Question_content":"I'm trying to understand cross validation in the context of two different models:Historic / backward-looking modelMarket-implied / forward-looking modelFor example, suppose that I want to understand the future distribution of an interest rate in six months' time, particularly its  percentile.  If I look at the historical, observed six-month change in this interest rate over many six-month time horizons corresponding to the number of days for which I have data (e.g., beginning 2/1/01 and continuing through 2/1/16, I may have  such observations), I can easily compute the  percentile of the observed, historic changes and use this as a \"model\" for my go-forward analysis of the rate in the next six months.If instead I compute the probability of rates reaching a theoretical  percentile that is implied by market prices, I can use this rate instead for my go-forward analysis.My question is: how may I cross validate both of these \"models\" (if that is an appropriate use of the term here), and what is the theoretical difference in doing so for a model based on historic data and a model based on future expectations?","Creater_id":113279,"Start_date":"2016-08-02 21:33:54","Question_id":226995,"Tags":["forecasting","cross-validation","intuition"],"Answer_count":0,"Last_activity":"2016-08-03 10:37:11","Link":"http://stats.stackexchange.com/questions/226995/compare-statistical-vs-nonstatistical-forecast-using-cross-validation","Creator_reputation":30}
{"_id":{"$oid":"5837a585a05283111e4d6437"},"View_count":24,"Display_name":"KevinKim","Question_score":0,"Question_content":"Say we want to know whether treatment A is different than treatment B.The most ideal situation will be: I randomly sample people from my target population (mathematically, I generate a set of i.i.d. random variables), then I randomly assign subjects in my sample to treatment A and B such that I ensures that the probability of each subjects being assigned to each treatment is 0.5, then I collect the data, run a test.The underlying statistical process (I believe) is the following: conceptually, you can think that your target population has been made an identical copy, and one of them ALL take treatment A, and the other ALL take treatment B. The people in your sample who take treatment A or B can be regarded as the i.i.d. sample from the conceptual population A or B. Then we know that the i.i.d. sample mean follows a distribution. So this sets the statistical foundation of test in this case. In this case, the experiment has both internal validity (we can attribute the difference in observation as treatment) and external validity (since our samples are representative of their population).In practice, in many cases, you post an ads on the website and ask subjects to come to your experiment. In this case, your sample cannot represent the population. So external validity is gone. You can still performs randomization to the sample. However, I am questioning whether it still makes sense to do test? Because from the above argument, we know that only the sample mean which comes from independent draw from the same population (which means identically distributed) will have distribution. Here your sample is no way independent identically draw from the distribution (say students distribution, not even mention the human distribution). So in this case I described, is it still valid to use test to check statistical significance? and is this experiment still at least has internal validity?","Creater_id":66461,"Start_date":"2016-08-03 09:43:29","Question_id":227117,"Tags":["experiment-design","randomization"],"Answer_count":1,"Last_activity":"2016-08-03 10:32:05","Link":"http://stats.stackexchange.com/questions/227117/statistical-foundation-of-t-test-in-experiment-with-random-assignment","Creator_reputation":1635}
{"_id":{"$oid":"5837a585a05283111e4d6439"},"View_count":77,"Display_name":"Bert Carremans","Question_score":1,"Question_content":"I have a dataset that consists of only categorical variables and a target variable. I want to predict the (binary) target variable with the categorical variables.I am trying to do this in Python and sklearn.The categorical variables have many different values. I was thinking to create dummy variables for each value in all the categorical variables. However, in the end this results in a sparse data set with thousands of variables.How would you go about to reduce the number of dummy variables? Would you use Chi2 to select useful features?Or would you not make dummy variables at all? Thanks!","Creater_id":123458,"Start_date":"2016-08-03 10:20:43","Question_id":227125,"Tags":["machine-learning","categorical-data","scikit-learn","dimensionality-reduction"],"Answer_count":0,"Last_activity":"2016-08-03 10:20:43","Link":"http://stats.stackexchange.com/questions/227125/preprocess-categorical-variables-with-many-values","Creator_reputation":106}
{"_id":{"$oid":"5837a585a05283111e4d643b"},"View_count":194,"Display_name":"Paul Igor Costea","Question_score":1,"Question_content":"I'm trying to project a point into an existing PCOA (Principal Coordinates Analysis) space (in R). I am under the impression this must be possible, but I can't figure out how to go about it.Here's how far I've gotten (a toy example):x \u0026lt;- c(1:10)y \u0026lt;- c(10:1)z \u0026lt;- c(rnorm(10,mean=0,sd=2),rnorm(10,mean=10,sd=2))m \u0026lt;- cbind(x,y,z)d \u0026lt;- dist(m)r \u0026lt;- pcoa(d)biplot(r,m)The biplot generates the representation I want. Now, given a new point P=(x,y,z) I would like to project it into the above space. The reason I need this and can't simply add this point to the original matrix is that this new point is going to be an outlier and would change the projection of the original space. What I want is to know where this point ends up relative to the untainted representation.Also note that I don't actually use a Euclidean distance in reality, so doing a PCA is not an option.","Creater_id":29676,"Start_date":"2013-08-29 06:55:46","Question_id":68678,"Tags":["machine-learning","distance-functions","pcoa"],"Answer_count":2,"Last_activity":"2016-08-03 10:14:50","Link":"http://stats.stackexchange.com/questions/68678/out-of-sample-embedding-into-principal-coordinate-space","Creator_reputation":33}
{"_id":{"$oid":"5837a585a05283111e4d643d"},"View_count":25,"Display_name":"SeamusX","Question_score":1,"Question_content":"I'm trying to create a predictive ARMA model with the following code in Stata:arima depvar indepvars, ar(1) ma(1/3) nocpredict m1s, yline depvar period || line m1s periodBut for any combinations of regressors, AR processes, and MA processes I've tried, the results look somewhat like this (the model is somewhat overfitted to illustrate):Where the fitted value often lags one period behind, especially in later periods for important fluctuations. The same happens if I predict the last 12 periods training using period \u0026lt;= 120 as well:Is this an issue with incorrect specification, or is it a mistake in the codes?","Creater_id":125519,"Start_date":"2016-08-03 09:24:00","Question_id":227106,"Tags":["time-series","forecasting","stata","arma"],"Answer_count":0,"Last_activity":"2016-08-03 10:06:25","Link":"http://stats.stackexchange.com/questions/227106/arma-fitted-value-lagging-one-period-behind-in-stata","Creator_reputation":6}
{"_id":{"$oid":"5837a585a05283111e4d643f"},"View_count":9,"Display_name":"Mike B","Question_score":0,"Question_content":"I have some chemical kinetics data for which I am estimating parameters (similar to classic Michaelis-Menten) with a generalized nonlinear least squares model. I have three genotypes that I am comparing and three replicates for each (corresponding to separate preparations of enzyme). For each replicate, data was collected from all three genotypes.I am including both genotype and replicate as categorical variables in the model, along with the interaction between them, so as to allow for each genotype-by-replicate sample to have it's \"own\" set of parameter estimates. The effect of genotype on the kinetic parameters is the primary research question; the replicate is essentially a \"nuisance\" variable.When I fit the data to such a model, I get a significant genotype*replicate interaction term, but the interactions are not all crossing. In particular, the same genotype (let's call it \"B\") has the highest estimates for one of the kinetic parameters (Vmax) across all three replicates, which corresponds with certain a priori assumptions. I would like to test whether this difference is statistically significant.Specifically, I would like to test that across replicates, genotype \"B\" has a higher Vmax than both genotypes \"A\" and \"C.\" But I don't know what the appropriate test would be.As I understand it, there is some disagreement about the meaningfulness of testing these sorts of \"simple effects\" in the face of a significant interaction term, so I am also willing to be convinced that this is entirely the wrong angle to take.I've generated some dummy data that is similar in form to my own in R:library(nlme)vmaxes = matrix(c(90, 100, 95, 94, 99, 93, 92, 101, 89), nrow = 3)kms = matrix(c(0.5, 0.55, 0.52, 0.49, 0.45, 0.53, 0.44, 0.51, 0.51), nrow = 3)rates.df = data.frame(  replication = rep(1:3, each = 90),  genotype = rep(1:3, each = 30, times = 3),  conc = rep(1:10/10, 27))rxnrate = function(genotype, replication, conc) {  rnorm(1, 1, 0.01)*vmaxes[genotype, replication]*conc/    (kms[genotype, replication] + conc)}set.seed(1234)rates.dfgenotype[x], dataconc[x]))genotypes = c(\"A\", \"B\", \"C\")rates.dfgenotype]rates.dfreplication)mm.gnls = gnls(  rate ~ vmax*conc/(km+conc),  data = rates.df,  start = list(    vmax = rep(100, 9),    km = rep(0.5, 9)  ),  params = list(    vmax + km ~ genotype*replication  ),  weights = varPower())And a plot of the dummy data:In this instance, the test would be that overall, the green genotype (\"B\") has a higher asymptote than the other two genotypes.","Creater_id":78746,"Start_date":"2016-08-03 10:03:27","Question_id":227121,"Tags":["interaction","least-squares","nonlinear-regression"],"Answer_count":0,"Last_activity":"2016-08-03 10:03:27","Link":"http://stats.stackexchange.com/questions/227121/testing-repeated-differences-between-cell-means-in-a-generalized-nonlinear-least","Creator_reputation":28}
{"_id":{"$oid":"5837a585a05283111e4d6441"},"View_count":28,"Display_name":"Fabiola Fern\u0026#225;ndez","Question_score":0,"Question_content":"Apologies if this is a bit basic question but examples and other questions I have seen so far do not address my problem. Here it is:I'm trying to understand multivariate analysis for survival data and I've got the following problem. I've got a small sample size study with a main variable for which I want to do a survival analysis. I did some fisher's/chi-square tests to see whether there were associations between my main variable and other variables present on the data. There were none. Now, I did a survival analysis (Kaplan-Meier and log-rank test) using my main variable and I got significance. I fitted also a Cox proportional hazards model and I got significance. Someone suggested to adjust my model for one of the other variables and that resulted on a significant effect. However, I've got quite small sample size in two of the subgroups (N=4 for two of the subgroups) and given that there was no association between these two variables using the fisher's exact test, my opinion would be to not to take this second variable to the model since there was no association and I've got very small sample size.Can I do that? Should I still include that second variable? If so, how can I interpret the results all together (with the no-association found)? Can anybody suggest any book/paper/blog/link where they address this issue?Thank you so muchEdits following the comments: Number of total events: 28As I mentioned: Initially I just wanted to look one variable and I was suggested to adjust for another variable, so in total, my model would have two variables.All patients are dead at the end of the analysis. No censored data. No recurrent events. ","Creater_id":54913,"Start_date":"2016-08-02 08:35:58","Question_id":226902,"Tags":["multivariate-analysis","cox-model"],"Answer_count":1,"Last_activity":"2016-08-03 09:59:52","Link":"http://stats.stackexchange.com/questions/226902/taken-variables-that-did-not-show-association-with-a-main-variable-to-a-multivar","Creator_reputation":15}
{"_id":{"$oid":"5837a586a05283111e4d64a7"},"View_count":16,"Display_name":"Federico Giorgi","Question_score":0,"Question_content":"I am using MatrixEQTL to perform an eQTL analysis. I am dubious about the errorCovariance parameter of the main function Matrix_eQTL_main. The manual describes it simply as \"The error covariance matrix.\"What should be used as error covariance matrix? The covariance matrix of the expression matrix? I guess not, as it is clearly defined that we are talking about the covariance of the error.But how to estimate the error?In one of the MatrixEQTL demos, the author generates the standard deviation of the noise as such (n is the number of samples):noise.std = 0.1 + rnorm(n)^2;Then, he inputs the errorCovariance as such:errorCovariance = diag(noise.std^2)Other examples use an identical approach to input the error, which is then simply gaussian noise squared. But how should I infer noise from my dataset? Assume that I have an expression matrix and a genotype matrix, with several covariates associated to each sample.Thanks for your help!","Creater_id":9640,"Start_date":"2016-08-03 09:32:23","Question_id":227113,"Tags":["r","covariance","error"],"Answer_count":0,"Last_activity":"2016-08-03 09:32:23","Link":"http://stats.stackexchange.com/questions/227113/errorcovariance-in-matrixeqtl","Creator_reputation":52}
{"_id":{"$oid":"5837a586a05283111e4d64a9"},"View_count":9024,"Display_name":"Infinity","Question_score":37,"Question_content":"I am not comfortable with Fisher information, what it measures and how is it helpful. Also it's relationship with the Cramer-Rao bound is not apparent to me.Can someone please give an intuitive explanation of these concepts?","Creater_id":4101,"Start_date":"2011-05-09 13:43:10","Question_id":10578,"Tags":["estimation"],"Answer_count":3,"Last_activity":"2016-08-03 09:27:32","Link":"http://stats.stackexchange.com/questions/10578/intuitive-explanation-of-fisher-information-and-cramer-rao-bound","Creator_reputation":328}
{"_id":{"$oid":"5837a586a05283111e4d64b7"},"View_count":33,"Display_name":"Sangwoong Yoon","Question_score":0,"Question_content":"Given the true density function  and the estimated density , I want to evaluate how close these two density are by Integrated Squared Error (ISE) criterion, .In some  and , ISE can be derived analytically. However there are cases where it can not. I wonder if there is any Monte Carlo-style method for evaluate ISE in that case.In other words, I want to evaluate the ISE integral with random points and function evaluations on the points.I am especially concerned with the case where the support of  is infinite, for example Gaussian distribution. (Although for Gaussian  and , ISE can be evaluated exactly.)","Creater_id":93401,"Start_date":"2016-08-03 08:20:32","Question_id":227093,"Tags":["monte-carlo","kernel-smoothing","numerical-integration"],"Answer_count":1,"Last_activity":"2016-08-03 09:05:51","Link":"http://stats.stackexchange.com/questions/227093/monte-carlo-evaluation-of-integrated-square-error-for-kernel-density-estimation","Creator_reputation":87}
{"_id":{"$oid":"5837a586a05283111e4d64c4"},"View_count":34,"Display_name":"rg255","Question_score":0,"Question_content":"I am using the coxme package in R to perform mixed effect cox regression analysis, and would greatly appreciate some advice on how to interpret the regression slopes. The data is a response variable (lifespan), converted to a survival object (age at death, and information on censoring). There are two sexes (M: male, and F: female) and two groups (G1 and G2), and sampling was performed in two separate years, and there is a continuous explanatory variable (NE). I use sex, group and NE as fixed effects, and year as a random effect, because the aim is to quantify the differences in response to NE (), for the within-sex and with-group comparisons: So here is some dummy data and the model (in R):n = 2000set.seed(1239)dumDF = data.frame(    \"Sex\" = sample(c(\"M\",\"F\"), size = n, replace = T),     \"Group\" = sample(c(\"G1\",\"G2\"), replace = T),    \"Year\" = sample(c(\"1994\",\"1995\"), replace = T),     \"NE\" = rnorm(n,0,5),     \"Status\" = sample(c(1,2,2,2), replace = T),     \"Life\" = rnorm(n, 50, 5))dumDFSex==\"M\", dumDFNE, dumDFNE)dumDFcoefficients[3]    # Slope for Males + Group 1    MG1 = modelcoefficients[5]    # Slope for Females + Group 2    FG2 = modelcoefficients[6]    # Slope for Males + Group 2    MG2 = modelcoefficients[5] + modelcoefficients[7]    matrix(c(FG1,MG1,MG2,FG2), ncol = 1)    }}coxSlopeFunc(coxdum)\u0026gt; coxSlopeFunc(coxdum)             [,1][1,] -0.176770390[2,] -0.229935076[3,] -0.187432037[4,] -0.177998268As you can see, the slopes are all negative ([1,] through [4,]). It is understanding the meaning of these slopes that I am stuck on. Would it be correct to say that a negative slope means there is a decreasing hazard (and therefore reduced mortality) with increasing NE? In other words, if the slope is negative, the explanatory variable reduces the rate of mortality? Furthermore, if the value of NE increases by a unit of 1, is the hazard () expected to change by ?Update: As I read this I interpret the response to be the log of the individual hazard function, , as given on page 3. Thus, a negative slope would indicate that the hazard (mortality) reduces with increasing NE.","Creater_id":16542,"Start_date":"2016-08-03 06:24:19","Question_id":227068,"Tags":["regression","cox-model","proportional-hazards"],"Answer_count":0,"Last_activity":"2016-08-03 09:04:53","Link":"http://stats.stackexchange.com/questions/227068/cox-regression-slope-understanding-the-sign-of-the-slope","Creator_reputation":344}
{"_id":{"$oid":"5837a586a05283111e4d64c6"},"View_count":40,"Display_name":"Remi.b","Question_score":0,"Question_content":"I computed an anova (in R) and got the following output\u0026gt; anova(lm(dA*dC))Analysis of Variance TableResponse: dA                       1   1924  1923.6  1.1541 0.28965  dC                       1    965   965.3  0.5791 0.45148  dA:dB:dA:dA:dC               1    476   475.9  0.2855 0.59632  Residuals                 37  61673  1666.8                  ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026gt; summary(lm(dLUIindex:dType*dMilieu))r^2Abondance_max ~ dr.squared even though I report the p.value corresponding to the full test?Should I report 1 as a degree of freedom (as I did above) or 37?","Creater_id":24097,"Start_date":"2016-08-03 08:54:04","Question_id":227101,"Tags":["regression","multiple-regression","r-squared","degrees-of-freedom","reporting"],"Answer_count":0,"Last_activity":"2016-08-03 09:03:53","Link":"http://stats.stackexchange.com/questions/227101/how-to-report-results-from-multivariate-linear-regression","Creator_reputation":1141}
{"_id":{"$oid":"5837a586a05283111e4d64c8"},"View_count":14,"Display_name":"user53020","Question_score":0,"Question_content":"The data that I have is response y, predictor x1 and x2 measured over 30 years. The correlation between x1 and x2 is around -0.5 I construct two modelsmdl1\u0026lt;-lm(x1 ~ year)x1.res\u0026lt;-mdl1residualsIf I check the correlation between x1.res and  x2.res, it increases to -0.8 which is serious.I wanted to ask if correlation between x1 and x2 should be considered or x1.res and x2.res should be considered before including them as predictors.Thanks","Creater_id":53020,"Start_date":"2016-08-03 09:00:53","Question_id":227102,"Tags":["r","regression","correlation"],"Answer_count":0,"Last_activity":"2016-08-03 09:00:53","Link":"http://stats.stackexchange.com/questions/227102/r-correlation-between-residuals-of-two-predictors","Creator_reputation":63}
{"_id":{"$oid":"5837a586a05283111e4d64ca"},"View_count":18,"Display_name":"cat.vet","Question_score":1,"Question_content":"I would be very grateful for the opinion of someone, with more experience of mixed-effects models than myself, on whether the following sounds reasonable:I would like to look at whether several variables, such as blood test results, significantly change among diabetic cats as they enter diabetic remission. I have the variable results from a selection of diabetic cats who successfully entered remission, and also from a group of diabetic cats who never achieved remission. To analyse my data, I was planning to split the results from the remission cats into several groups according to their \"stage\" of remission. These groups are:1) more than 45 days before stopping remission2) less than 30 days before stopping insulin3) less than 28 days after stopping insulin4) more than 28 days after stopping insulin (in long-standing remission)There is also a group 5 made up of the cats who never entered remission.I was then planning on using a mixed effects model to study the effect of Group (my fixed effect) on my various outcome variables. I was going to add each cat's identity as a random effect. All the cats in group 5 are unique, but cats who entered remission might contribute samples to any of groups 1 to 4. Also, some cats might have several samples that are eligible for one, or several, of groups 1 to 4 e.g. a cat might have 2 samples that could technically be included in group 1. Unfortunately, not all remission cats have a sample in every one of groups 1 to 4.I would be very grateful if anyone could advise me on whether the above seems suitable, and whether it would be possible to include multiple samples from an individual cat in any of groups 1 to 4? I would also be grateful to know which covariance structure might be most suitable.Thank you very much in advance!","Creater_id":114523,"Start_date":"2016-08-03 08:22:24","Question_id":227094,"Tags":["mixed-model"],"Answer_count":0,"Last_activity":"2016-08-03 08:22:24","Link":"http://stats.stackexchange.com/questions/227094/advice-on-whether-a-mixed-effects-model-is-suitable","Creator_reputation":6}
{"_id":{"$oid":"5837a586a05283111e4d64cc"},"View_count":15,"Display_name":"user53020","Question_score":0,"Question_content":"I constructed two models:mdl\u0026lt;-lm(y ~ x)mdl.res\u0026lt;-mdlresidualsSince mdl1 is a polynomial model, I wanted to know whether the residuals of mdl1 have the same units as residuals of mdl. Are they on the same scale?Thanks","Creater_id":53020,"Start_date":"2016-08-03 08:06:50","Question_id":227090,"Tags":["r","regression","polynomial"],"Answer_count":0,"Last_activity":"2016-08-03 08:06:50","Link":"http://stats.stackexchange.com/questions/227090/units-of-residuals-of-a-polynomial-regression","Creator_reputation":63}
{"_id":{"$oid":"5837a586a05283111e4d64ce"},"View_count":14,"Display_name":"T.S","Question_score":1,"Question_content":"My DV is parametric, some of my other variables are too, others are not, such as 'how much do you value tolerance'. I know pearsons is for parametric and spearmans is for non-parametric, but what if the data is mixed? Also, is there an issue with spearmans if many of the data units have the same number. E.g. Many participants choose the number 6 as their value of tolerance? Which statistical test is best for this kind of correlation? (Post hoc/ exploratory/ multiple comparisons) ","Creater_id":125512,"Start_date":"2016-08-03 07:58:46","Question_id":227087,"Tags":["correlation","statistical-significance"],"Answer_count":0,"Last_activity":"2016-08-03 07:58:46","Link":"http://stats.stackexchange.com/questions/227087/correlation-statistics","Creator_reputation":6}
{"_id":{"$oid":"5837a586a05283111e4d64d0"},"View_count":35,"Display_name":"what","Question_score":1,"Question_content":"Suppose I have data from a questionnaire. The questionnaire contained eight items, of which the first four comprise one scale, the second four another scale. My data now looks like this:dat:            | scale.1                     | scale.2participant | item.1 item.2 item.3 item.4 | item.5 item.6 item.7 item.81           | 3      7      1      2      | 2      3      6      42           | 2      2      5      2      | 8      9      7      6...How do I calculate Cronbach’s alpha for this questionnaire as a whole and each individual scale?When I calculate alpha for scale.1, do I calculate it as if the items comprising scale.2 did not exist ? Like this:library(psy)cronbach(data.frame(datitem.2, datitem.4))Or is there some (better? correct?) way where I input the whole dataframe into a function, tell it which columns belong to which scale, and I get internal consistencies that considers the scales in the context of the whole questionnaire? scoreItems from the psych package seems to do something like this, but I'm too stupid to make sense of the manual.","Creater_id":14650,"Start_date":"2016-08-02 03:12:16","Question_id":226839,"Tags":["r","cronbachs-alpha"],"Answer_count":2,"Last_activity":"2016-08-03 07:44:01","Link":"http://stats.stackexchange.com/questions/226839/cronbachs-alpha-for-a-questionnaire-consisting-of-several-scales","Creator_reputation":96}
{"_id":{"$oid":"5837a586a05283111e4d64dd"},"View_count":28,"Display_name":"Hirak Sarkar","Question_score":2,"Question_content":"I have two sets of count data, one with the application of drug and another without applying the drug. Let's assume the data at condition  are  and . Similarly for condition , the data is  and . Moreover we assume that the individual count data comes from a poisson distribution. So , , , . Now I frame my hypothesis as follows, \\begin{align}H_0 : \\frac{\\lambda_i^1}{\\lambda_j^1} \u0026amp;= \\frac{\\lambda_i^2}{\\lambda_j^2} \u0026amp;H_1 : \\frac{\\lambda_i^1}{\\lambda_j^1} \u0026amp; \\neq \\frac{\\lambda_i^2}{\\lambda_j^2}\\end{align}So the intuition is if the ratio of rates does not change across conditions then we accept the null hypothesis, otherwise we reject it. I am searching for a mathematical analysis on this framework, that is, what should be the estimates for  in this situation. Please let me know if there is any paper that explained it.","Creater_id":123199,"Start_date":"2016-08-03 07:23:08","Question_id":227078,"Tags":["hypothesis-testing","model-selection","poisson"],"Answer_count":0,"Last_activity":"2016-08-03 07:33:45","Link":"http://stats.stackexchange.com/questions/227078/hypothesis-testing-on-ratio-of-poisson-means-rates-with-four-parameters","Creator_reputation":15}
{"_id":{"$oid":"5837a586a05283111e4d64df"},"View_count":65,"Display_name":"draco_alpine","Question_score":2,"Question_content":"I have a machine learning problem that I am tackling with binary logistic regression. My needles occur at a rate of about 4%. Following on from [1] and with ~50 variables I conclude that I need about 300 needles.Unfortunately I don't have enough items of hay to build a training set with the correct ratio of needles to hay (ie 96 to 4).Is there a sensible way around this?At the moment I am considering building an incorrectly balanced sample (90/10) and then adjusting the algorithm [2] parameters for 'weights'. (this doesn't work, see comment)All help appreciated.[1] How large a training set is needed?[2] http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html","Creater_id":123287,"Start_date":"2016-08-03 06:18:10","Question_id":227066,"Tags":["machine-learning","logistic","scikit-learn"],"Answer_count":1,"Last_activity":"2016-08-03 07:29:18","Link":"http://stats.stackexchange.com/questions/227066/working-around-rare-data-logistic-regresion","Creator_reputation":120}
{"_id":{"$oid":"5837a586a05283111e4d64eb"},"View_count":12,"Display_name":"Eyatn Moor","Question_score":1,"Question_content":"I have a simple 2x2 block of data which I analyzed with a two-way repeated mesures ANOVA. Now I need to do a post-hoc procedureAfter reading some articles, Tukey's HSD was suggested as the less conservative option and t-test with Bonferroni's correction as a safer (more conservative) option. However, it seems to me that by giving up the pairing of data, you lose too much statistical power (like using a t-test when you should be using a paired t-test). Is this true? Is there any way to get around it? ","Creater_id":125509,"Start_date":"2016-08-03 07:29:15","Question_id":227081,"Tags":["anova","repeated-measures","post-hoc","two-way"],"Answer_count":0,"Last_activity":"2016-08-03 07:29:15","Link":"http://stats.stackexchange.com/questions/227081/post-hoc-procedures-for-two-way-repeated-measures-anova","Creator_reputation":6}
{"_id":{"$oid":"5837a586a05283111e4d64ed"},"View_count":181,"Display_name":"John Smith","Question_score":2,"Question_content":"I'm currently learning ho to use Cronbach Alpha in R. I have a scale with 7 items and 63 respondents. The aim is just to get a practical understanding of what Cronbach Alpha is. There are some nulls in the datasetThe method itself has a number of assumptions. I wish to test these assumptions in R. So my questions are related to a couple of these assumptionsAssumption of unidimensionality. I have gotten the scales from other research papers so i assume i use confirmatory analysis to test for unidimensionality?Error Terms are uncorrelated. How do i test this in R? Im currently using the code alpha(myscales) to generate my alpha statistics but i want to test if the scales violate thisHow to test for Tau Equivalence. This I think can be assumed based on the results of the same scale from other researchersI realize there are better tests for example the omega test (its next on my list to learn)Thank you for your time.","Creater_id":28774,"Start_date":"2016-04-30 05:10:07","Question_id":210150,"Tags":["r","reliability","cronbachs-alpha"],"Answer_count":1,"Last_activity":"2016-08-03 07:19:34","Link":"http://stats.stackexchange.com/questions/210150/cronbach-alpha-assumptions","Creator_reputation":90}
{"_id":{"$oid":"5837a586a05283111e4d64fa"},"View_count":38,"Display_name":"Ghilas BELHADJ","Question_score":0,"Question_content":"Assume the following input sequences.___JJJJIII___I____OOO0O______HHHHHH_____MMNNN___AAAAAI_NNNNNNNNNNNA_AAAA____MMMM______A______RR_I______KK_which corresponds to the following labels:JOHNANNAMARKHow can I build a model that can learn to 'translate' those sequences into the right labels.I've though that a sequence to sequence recurrent network will be able to do this, but the generated sequence I got is nearly random.I've also tried a Connectionist Temporal Classification which seems to do exactly what I want.But the implementation I've tried returns me an empty stringsAny working exemple for the previously cited methods ?Any suggestion of an other method to solve this problem ? Edit:seq2seq RNN is rendering something coherent when working (training and testing) with constant length sequences only. (I already tried to pad the other sequences to get the same length ... same thing)","Creater_id":72240,"Start_date":"2016-08-02 06:56:00","Question_id":226878,"Tags":["time-series","deep-learning","lstm","sequential-pattern-mining"],"Answer_count":0,"Last_activity":"2016-08-03 06:58:33","Link":"http://stats.stackexchange.com/questions/226878/how-to-deal-with-this-sequence-alignement-problem","Creator_reputation":121}
{"_id":{"$oid":"5837a586a05283111e4d64fc"},"View_count":44,"Display_name":"user3639557","Question_score":4,"Question_content":"I am reading this and am puzzled by equation 8. I don't understand the last bit: why can we move the gradient out of the expectation? E_Q[\\nabla_\\phi\\log Q_\\phi(h|x)] = E_Q[\\frac{\\nabla_\\phi Q_\\phi(h|x)}{Q_\\phi(h|x)}]=\\nabla_\\phi E_Q[1]=0.","Creater_id":56676,"Start_date":"2016-08-03 06:15:42","Question_id":227065,"Tags":["optimization","expected-value","gradient"],"Answer_count":1,"Last_activity":"2016-08-03 06:39:29","Link":"http://stats.stackexchange.com/questions/227065/expectation-of-gradients","Creator_reputation":310}
{"_id":{"$oid":"5837a586a05283111e4d6508"},"View_count":79,"Display_name":"Jermu","Question_score":0,"Question_content":"I've got some data that looks like it is Gamma distributed. I've constructed the prior distribution from mean=232 and standard deviation = 150, which yield the Gamma distribution parameters:a_prior (shape) =2.392 andb_prior (scale) =96.98Now, I'm looking at the Wikipedia page for conjugate priors, and the update rules for the Gamma hyperparameters are: \\alpha_0 + n\\alpha,\\beta_0+\\sum_{i=1}^n x_iThe mean of my sample data is  and the standard deviation of the sample data is . These yield  and  to be  and , respectively. The sample data consists of  data points.I don't understand the update rules from Wikipedia. As I can tell, the posterior a' = prior_a + n * a, which in this case is 2.39 + 15 * 2.944, which would give as the posterior shape parameter 46.49, which seems quite large.Not to mention the posterior b' parameter, which looks like it's the prior b + sum(data points). My data points add up to 4451, so the posterior b' is:posterior b' = 96.98 + 4451 = 4548, which, again seems quite large for the scale parameter.Can someone tell me how the updating for a conjugate Gamma should go? It seems to me that the larger your sample size n is, the more the shape parameter will blow up. Similarly, the more data points you have in the sum, the scale parameter will grow extremely large.Clearly I am misunderstanding something or then the data is simply not Gamma distributed.I looked at the Gamma conjugate for Poisson, which contains calculating the equivalent sample size, but I don't see how it relates to updating Gamma prior to Gamma posterior.Any help on understanding my misunderstanding is appreciated.","Creater_id":125487,"Start_date":"2016-08-03 03:43:33","Question_id":227035,"Tags":["bayesian","prior"],"Answer_count":1,"Last_activity":"2016-08-03 06:30:10","Link":"http://stats.stackexchange.com/questions/227035/updating-gamma-conjugate-prior","Creator_reputation":1}
{"_id":{"$oid":"5837a586a05283111e4d6515"},"View_count":27,"Display_name":"YLi","Question_score":1,"Question_content":"Hi I am new to Bayesian Theorem and currently learning to do model selecting by bayes factor using the BayesFactor package in R. After sampling from the posterior distribution using posterior(), I have some problems interpreting the summary() results:I have \"Dose\", \"score1\" and their interaction term as variables. I don't see the estimation for the intercept, and what do \"sig2\" and \"g\" stand for? Thank you in advance!","Creater_id":123451,"Start_date":"2016-07-15 12:14:36","Question_id":224006,"Tags":["r","regression","bayes-factors"],"Answer_count":0,"Last_activity":"2016-08-03 06:16:43","Link":"http://stats.stackexchange.com/questions/224006/sampling-by-using-posterior-from-bayesfactor-package","Creator_reputation":6}
{"_id":{"$oid":"5837a586a05283111e4d6517"},"View_count":33,"Display_name":"Georgios Nikolaidis","Question_score":0,"Question_content":"I am trying to obtain a Cholesky decomposition matrix in STATA, but the covariance matrix is not positive definite, so i get back that error. I am told that in that case i cannot perform decomposition and i have to assume independence between the estimated parameters. Could i try any transformations in order to succeed in getting the Cholesky?Thanks","Creater_id":122014,"Start_date":"2016-08-03 06:13:09","Question_id":227063,"Tags":["covariance-matrix","matrix-decomposition","cholesky"],"Answer_count":0,"Last_activity":"2016-08-03 06:13:09","Link":"http://stats.stackexchange.com/questions/227063/cholesky-decomposition-but-non-positive-definite-covariance-matrix","Creator_reputation":11}
{"_id":{"$oid":"5837a586a05283111e4d6519"},"View_count":48,"Display_name":"rrrj","Question_score":2,"Question_content":"I am trying to to a weighted implementation of Random Forests where i want to give greater weights to better performing trees in a forest.I have currently split my data set into 60:20:20 (Train Set: Performance Measure Set: Test Set). I am operating on data sets where the minority class has an incident rate of less than 1%.I am training my random forest on the train set, then measuring their performance on the Performance measure Set, assigning weights for each trees based on their performance and executing weighted prediction on the test set.However results does not give me an improvement over the majority voting implementationWhat is theoretically wrong with my implementation? Should a weighted approach theoretically give better results? Wouldn't weighing of trees give better results for skewed data sets where trees trained on the minority class information should ideally be given more weights?How can i get a measure of tree performance from the training set itself? I have looked at the following field on the output from Rf - 'err.rate'. How can i get performance measures for individual trees from the training set?Can i rerun my model on the training set and look at votes from each trees to gauge each tree's performance.Any help is highly appreciated. ","Creater_id":124537,"Start_date":"2016-08-03 06:09:54","Question_id":227062,"Tags":["machine-learning","random-forest","performance","weighted-data"],"Answer_count":0,"Last_activity":"2016-08-03 06:09:54","Link":"http://stats.stackexchange.com/questions/227062/performance-weighing-random-forests","Creator_reputation":33}
{"_id":{"$oid":"5837a586a05283111e4d651b"},"View_count":41,"Display_name":"hplieninger","Question_score":0,"Question_content":"I conduct a simulation study in which I generate data from a specific model, fit that model to the generated data, and repeat that  times. The model that I work with (in my case it's an IRT model, but that doesn't matter much), has multiple parameters of the same type. For example, it has  person parameters, one for each person (e.g., the persons' ability).When summarizing the results, I feel like I have two options.I may look at each of the  runs separately, calculate the statistic I'm interested in (e.g., mean bias), and then pool the results across runs (e.g., make a box plot of  mean bias values).The focus of this option is on a generated data set, it's the perspective of a researcher with a single sample at hand, variability is smaller here.I may look at all  estimates simultaneously, calculate the statistic I'm interested in (e.g., bias), and then describe the result (e.g., make a box plot of  bias values).  The focus here is on the parameter itself, variability is larger here; but why not doing a single sample with  persons in the first place. Moreover, it may well be that this means correlating (or scatter plotting) pairs of 1,000,000 or even more values.I feel lost in deciding which route to take. Papers I looked at seem to go with option (2). (Please comment if you have an illustrative example of the first option).The outcomes that I'm actually interested in are (a) correlation of true and observed value, (b) bias, and (c) RMSE. The purpose of my work is to study the recovery of the model parameters when fitting the correct model (or when fitting a misspecified model).So the question is how should I summarize the results of my simulation study.AppendixHere is a MWE in R, which is not critical for the question, but which may clarify things or may be used to play around with.install.packages(\"eRm\")install.packages(\"reshape2\")reps \u0026lt;- 100N \u0026lt;- 100J \u0026lt;- 10res \u0026lt;- rep(list(as.data.frame(matrix(NA, nrow = N, ncol = 2,                                     dimnames = list(NULL, c(\"true\", \"obs\"))))), reps)for (iii in seq_len(reps)) {    ppar \u0026lt;- rnorm(N)                # true parameters    X \u0026lt;- eRm::sim.rasch(ppar, J)    # generate data    m1 \u0026lt;- eRm::RM(X)                # fit model    res[[iii]][, \"true\"] \u0026lt;- ppar    res[[iii]][, \"obs\"] \u0026lt;- eRm::person.parameter(m1)`Person Parameter`}res2 \u0026lt;- reshape2::melt(res, id.vars = c(\"true\", \"obs\"))### Option 1 #### Biasmean(sapply(by(res2, res2obs - xL1, function(x) xtrue), mean))# Cormean(by(res2, res2obs, xL1, function(x) cor(xtrue)))### Option 2 #### Biasmean(res2true)          # mean bias identical to (1) of courseboxplot(res2true)# Corcor(res2true)plot(res2true)","Creater_id":27276,"Start_date":"2016-08-03 06:08:01","Question_id":227060,"Tags":["r","simulation","psychometrics","confirmatory-factor","irt"],"Answer_count":0,"Last_activity":"2016-08-03 06:08:01","Link":"http://stats.stackexchange.com/questions/227060/analysis-report-of-a-simulation-study-on-the-level-of-parameters-or-on-the-level","Creator_reputation":535}
{"_id":{"$oid":"5837a586a05283111e4d651d"},"View_count":28,"Display_name":"Rasmus","Question_score":0,"Question_content":"I have a question, which has puzzled me for some time now. I hope this community can provide an answer. I have a method which can measure conversion of pyruvate into lactate in the body, the results are expressed in ratios, e.g. 0,2; 0,4 and so on. I have conducted several animal experiments using this method and am now interested in perfecting my animal method. Specifically, I want to know if it is better to use fasted or fed animals. I have results from a sample of fasted animals, n=15 and results from a sample of fed animals, n=5. What I want to know is, does fasting increase the variability of the results? Are the animals more \"in line\" with each other when they've been fed, i.e. do the results lie closer together? I cant seem to find a good way of testing for this - I've thought of using coefficient of variation or a Bartlett's or Levene's test, but am concerned as to the difference in sample size (n=15 vs. n=5). Specifically, I don't want my power to detect a difference in the variance to be diminished because of sample size-difference. Can anyone come up with a good way of testing for something like this? I'm somewhat of a novice in statistics, so please forgive me if something is unclear - or just plain wrong.Thank you in advance! ","Creater_id":125489,"Start_date":"2016-08-03 03:46:44","Question_id":227036,"Tags":["variance","sample-size","coefficient-of-variation"],"Answer_count":1,"Last_activity":"2016-08-03 05:53:54","Link":"http://stats.stackexchange.com/questions/227036/how-can-one-test-for-difference-in-variation-between-two-groups-of-different-sam","Creator_reputation":1}
{"_id":{"$oid":"5837a586a05283111e4d6528"},"View_count":123,"Display_name":"TPArrow","Question_score":5,"Question_content":"I am studying LASSO (least absolute shrinkage and selection operator) at the meantime. I see that the optimal value for regularization parameter can be chosen by cross validation. I see also in ridge regression and many methods that apply regularization, we can use CV in order to find the optimal regularization parameter(saying penalty). Now my question is about initial values for upper and lower bound of parameter and how to determine the the length of the sequence. To be specific, assume we have a LASSO problem LogLikelihood = (y-x\\beta)'(y-x\\beta) + \\lambda \\sum|\\beta|_1and we want to find the optimal value for penalty, . Then how can we choose a lower and upper bound for ? and how many splits in between these two values ?","Creater_id":46139,"Start_date":"2015-09-30 12:11:11","Question_id":174897,"Tags":["lasso","regularization","shrinkage"],"Answer_count":1,"Last_activity":"2016-08-03 05:48:02","Link":"http://stats.stackexchange.com/questions/174897/choosing-the-range-and-grid-density-for-regularization-parameter-in-lasso","Creator_reputation":1106}
{"_id":{"$oid":"5837a586a05283111e4d6535"},"View_count":25,"Display_name":"Jack","Question_score":0,"Question_content":"I have a system which users will visit at different time points. Some users will visit and then won't return again. I wanted to predict the probability that they won't return again based on the time since their last visit and a few other covariates (e.g. total number of visits).My thoughts was to use some kind of logistic regression. The problem is that I do not 'observe' the time at which users have left the system. Is it okay to censor all the values above the maximum time at which I do see users recurring? Does anyone have a better way of going about this problem?","Creater_id":24763,"Start_date":"2016-08-03 05:11:28","Question_id":227050,"Tags":["regression","machine-learning","logistic","survival","censoring"],"Answer_count":0,"Last_activity":"2016-08-03 05:44:04","Link":"http://stats.stackexchange.com/questions/227050/logistic-regression-with-censored-dependent-variable","Creator_reputation":51}
{"_id":{"$oid":"5837a586a05283111e4d6537"},"View_count":66,"Display_name":"lacerbi","Question_score":3,"Question_content":"I am reading Geyer's lecture notes on MCMC. A condensed version of these notes constitutes Chapter 1 of the Handbook of Markov Chain Monte Carlo (ed. Brooks et al., 2011).Geyer notes that the composition of valid transition operators is still valid:  It is clear that if an update mechanism  preserves  a specified distribution, and so does another update mechanism , then so  does  followed by , which we will denote .Later on he notes that  is in general not reversible, but one could easily build a reversible composite update via a palindromic update, such as  or .Reversibility is a sufficient condition to prove that a given update leaves the target distribution invariant, and that is why we want elementary updates to be reversible. However, once we prove that, we do not need the composite update to be reversible. (For example, the typical fixed-scan Gibbs sampler is not reversible.)Is there any theoretical or practical gain in keeping composite updates reversible?My question is general, not only for Gibbs; although I suspect that the effect of ordering of operators might have been studied the most in the case of Gibbs sampling.In practice, I am currently building MCMC updates by composing a number of elementary operators of various kind (see also this question).I could combine the operators in a palindromic way -- would that matter?PS: I am aware that probabilistic mixtures of operators are valid and reversible, but here I am interested in composition.","Creater_id":80479,"Start_date":"2016-07-31 06:21:42","Question_id":226550,"Tags":["machine-learning","mcmc","gibbs"],"Answer_count":1,"Last_activity":"2016-08-03 05:41:32","Link":"http://stats.stackexchange.com/questions/226550/reversibility-in-mcmc","Creator_reputation":1747}
{"_id":{"$oid":"5837a586a05283111e4d6544"},"View_count":51,"Display_name":"dan","Question_score":1,"Question_content":"General question about predictions in PLS, using the chemometrics package in R here for an example. The standard example for pls1_nipals isdata(PAC)res \u0026lt;- pls1_nipals(PACy,a=5)This returns the coefficient vector resX,PACX[1:200,],PACX[201:209,]%*%resy[201:209])leads to    [,1]   [,2]470103.8 486.81457944.5 488.18491483.9 495.01487573.0 495.45499962.8 497.66487223.3 500.00656960.3 501.32661962.2 503.89635861.9 503.91The reason for the extreme difference is, that X and Y were centered. The centering of Y can be eliminated bei adding mean(PACX[201:209,]%*%resX[1:200,]), but it didn't work.cbind((PACX[1:200,]),9),ncol=467,nrow=9,byrow=TRUE))              %*%resy[1:200]),      PAC$y[201:209])Any ideas? Thanks.","Creater_id":116228,"Start_date":"2016-08-01 02:53:49","Question_id":226651,"Tags":["r","self-study","prediction","pls","chemometrics"],"Answer_count":1,"Last_activity":"2016-08-03 05:31:09","Link":"http://stats.stackexchange.com/questions/226651/pls-prediction-if-x-is-centered-using-f-e-r-package-chemometrics","Creator_reputation":78}
{"_id":{"$oid":"5837a586a05283111e4d6551"},"View_count":56,"Display_name":"Helene ","Question_score":0,"Question_content":"So right now I have an okay understanding of 10 folds cross validation from reading and studying. I have a quick question about the training-set vs holdout set.Let's say I'm doing 10 folds with 1 hold out on a linear regression model. I break my dataset into 10 pieces, I understand that 9 is used to train and 1 is used to test. Each of the 10 folds will be selected as the test set at some point.My question is, how does 10 folds method determine the best parameter in the end? In case I wasn't clear, here's an example: so I understand that each fold is compared against the other 8 for the best fit parameters by running it with the validation set. Let's say the first time param A is selected, the second time around another fold is selected, let's call it param B. How does the method make a decision if param A or B is better?","Creater_id":124110,"Start_date":"2016-08-02 18:38:16","Question_id":226983,"Tags":["cross-validation"],"Answer_count":2,"Last_activity":"2016-08-03 04:56:36","Link":"http://stats.stackexchange.com/questions/226983/10-folds-cross-validation-training-sets","Creator_reputation":136}
{"_id":{"$oid":"5837a586a05283111e4d655f"},"View_count":90,"Display_name":"Richard Cheung","Question_score":1,"Question_content":"Suppose I have two variables, which have over 500 data points per variable (though they are not continuous on timeline). what other kind of methods or test in can use in R to test the correlation? Except Pearson, Kendall and SpearmanI know how to do Pearson and Spearman and Kandall test (which is quite basic to my personal opinion). And if I want to study further about the type of correlation, such as the causality of one to another variable or predictive power of one variable to another. What test is available in R? P.S. I am a rookie in R. So if u can venture a example in code or package name would be perfect! ","Creater_id":null,"Start_date":"2015-08-19 05:59:51","Question_id":167872,"Tags":["r","hypothesis-testing","correlation"],"Answer_count":1,"Last_activity":"2016-08-03 04:41:45","Link":"http://stats.stackexchange.com/questions/167872/types-or-methods-of-testing-correlation","Creator_reputation":null}
{"_id":{"$oid":"5837a586a05283111e4d656c"},"View_count":35,"Display_name":"Matthew","Question_score":0,"Question_content":"I have a empirical distribution with fat tails (i.e. ). I want to reduce the dimensionality of this object: for any integer , I want a parametric family of distributions with fat tails. The GB2 family is perfect if  but I'd like to find a family for any .  I was thinking of using something like a generalized GB2. Are there more natural generalizations?","Creater_id":55035,"Start_date":"2016-07-30 06:48:25","Question_id":226449,"Tags":["distributions","power-law"],"Answer_count":1,"Last_activity":"2016-08-03 03:57:03","Link":"http://stats.stackexchange.com/questions/226449/parametrize-fat-tailed-distributions","Creator_reputation":121}
{"_id":{"$oid":"5837a586a05283111e4d6579"},"View_count":31,"Display_name":"plotti","Question_score":1,"Question_content":"I implemented a \"normalized\" confidence rule (see here: ) according to a course from coursera https://www.coursera.org/learn/recommender-systems) in pandas (see here: https://d2.maxfile.ro/knwouzvjhg.html) but I am unable to find any additional information on the formula that the instructor provided on the net. Can anyone help me out? Is this something similar to association rule mining?How can I evaluate the performance of such a recommender?","Creater_id":12902,"Start_date":"2016-03-16 03:35:50","Question_id":201968,"Tags":["pandas"],"Answer_count":1,"Last_activity":"2016-08-03 03:53:32","Link":"http://stats.stackexchange.com/questions/201968/nonpersonalized-recommender","Creator_reputation":63}
{"_id":{"$oid":"5837a586a05283111e4d657c"},"View_count":99,"Display_name":"Sjon","Question_score":1,"Question_content":"For my thesis I am conducting a factor analysis of a Belgian personality questionnaire, using the lavaan package for R. I have applied a split-sample procedure, and use sample 1 for exploratory factor analysis (EFA), and sample 2 for confirmatory factor analysis (CFA). Both samples have N \u003e 300. All models below have a two-factor solution.However, when I test the model that I identified in EFA (sample 1) using CFA (sample 2), I get a poor fitting model (low CFI \u0026amp; TLI, high RMSEA). On the other hand, several modification indices are suggested that seem to make sense (i.e., let several error terms of similar items correlate), after which I do get a good fitting adjusted model. This is indicated by a CFI and TLI of .95, and an RMSEA of .05.Now, my main problem is that when I compare the fit of my Belgian model with the fit of a previously identified factor structure of the English version of the questionnaire (tested on the same data), that the AIC of this English model is lower (7500ish vs 8500ish) than my adjusted model. This makes sense as my new model is more complex and AIC controls for this. However, the overall fit of the English model is not very high (CFI/TLI .89ish, RMSEA .07ish). Thus, although the English model has a lower AIC and thus is the preferred model, it fits rather poorly on the current data.I want to use the factors (subscales) I identified in a future study in which I relate these to behavioral measures, but I don't have time for another factor analysis to test any model of the questionnaire again. Should I continue using my own Belgian model/subscales, or is it better to use the English model/subscales instead? Also, is it commonly accepted to use subscales from a questionnaire from a different language if this better fits the data than native-language questionnaires?I hope this somewhat makes sense - thanks for any replies!Sjon","Creater_id":125480,"Start_date":"2016-08-03 02:56:27","Question_id":227027,"Tags":["factor-analysis","aic","sem","confirmatory-factor"],"Answer_count":1,"Last_activity":"2016-08-03 03:46:11","Link":"http://stats.stackexchange.com/questions/227027/best-fitting-model-aic-or-cfi-tli-rmsea","Creator_reputation":8}
{"_id":{"$oid":"5837a586a05283111e4d6589"},"View_count":308,"Display_name":"user2109988","Question_score":0,"Question_content":"I am trying to implement Linear Discriminant Analysis for face recognition. I have 3 classes and each classes have 10 image each. The dimension of matrix in class A, B and C is 10*500. So each row will represent an image.If I find the mean matrix of each class I am getting dimension of 1*500. That is I will be adding the row and divide by 10.Global mean matrix of all classes I am getting 1*500 dimension.Within Scatter Matrix Sw= The dimension of Matrix is 10*10 Matrix.Between Scatter Matrix Sb= The dimension of Matrix is 1*1. Next Step is I have to find Inverse(Sw)*Sb. But the matrix dimension is totally different. I know I am doing some mistake but I don't know where?Could you please help me to solve this problem?Can you please tell me how the dimension of the matrix should be?","Creater_id":42803,"Start_date":"2014-04-01 12:47:52","Question_id":92170,"Tags":["classification","pattern-recognition","discriminant-analysis"],"Answer_count":1,"Last_activity":"2016-08-03 03:28:54","Link":"http://stats.stackexchange.com/questions/92170/confused-about-scatter-matrix-dimensions-in-linear-discriminant-analysis","Creator_reputation":1}
{"_id":{"$oid":"5837a586a05283111e4d6596"},"View_count":17,"Display_name":"A K R","Question_score":0,"Question_content":"I ran FAVAR code on Eviews on a data which is not seasonally adjusted. However, it has been converted into stationary series and has also been normalized in accordance with what the model required but I am getting somewhat peculiar impulse responses which are periodic. Please tell me where am I going wrong. What could have produced such impulse reponses?","Creater_id":110352,"Start_date":"2016-08-03 02:59:21","Question_id":227028,"Tags":["var","impulse-response"],"Answer_count":0,"Last_activity":"2016-08-03 02:59:21","Link":"http://stats.stackexchange.com/questions/227028/peculiar-impulse-responses-with-favar","Creator_reputation":21}
{"_id":{"$oid":"5837a586a05283111e4d6598"},"View_count":23,"Display_name":"user_anon","Question_score":1,"Question_content":"I really think I don't understand what a sample really is. All the definitions I came up say that a (random) sample   , where  are random variables. So far, so good. But when a sample is given into a problem, it is given as follows:   a sequence of numbers. How come? I read that a sample should not be confounded by the realization of it, but how can we do that if every single problem I ran into is talking about samples as a sequence of numbers, and not as a sequence of random variables?","Creater_id":125475,"Start_date":"2016-08-03 02:19:59","Question_id":227023,"Tags":["mathematical-statistics","random-variable","sample"],"Answer_count":0,"Last_activity":"2016-08-03 02:47:36","Link":"http://stats.stackexchange.com/questions/227023/sample-set-of-random-variables-set-of-numbers","Creator_reputation":102}
{"_id":{"$oid":"5837a586a05283111e4d659a"},"View_count":30,"Display_name":"Cromack","Question_score":2,"Question_content":"If I have  test statistics , such that , is it true (as it happens with random variables) that ?","Creater_id":113000,"Start_date":"2016-08-03 00:13:15","Question_id":227007,"Tags":["hypothesis-testing","distributions","chi-squared"],"Answer_count":1,"Last_activity":"2016-08-03 02:03:07","Link":"http://stats.stackexchange.com/questions/227007/sum-of-chi-square-test-statistics","Creator_reputation":30}
{"_id":{"$oid":"5837a586a05283111e4d65a7"},"View_count":556,"Display_name":"Socratease","Question_score":0,"Question_content":"Having trouble finding straightforward information this topic.Basically, I'm trying to use the lme4 package to analyze my data, and the model looks something like (A ~ BCD) + (random effects term 1) + (random effects term 2).'A' is a yes/no response, which, based on what I've read, indicates that I should use glmer. However, my experiment uses repeated measures - each subject undergoes many trials. It's a psychophysical experiment, so there are many subjects who essentially make yes/no judgements about many, many images. I've read that when there are many trials within a subject, you should use lmer.What's the best way to go here? Sorry if the info given is too sparse; if anyone thinks they can help me out with this, I'll provide as much info as necessary.TL;DR: When exactly should one use lmer vs glmer, especially in the context of psychophysical experiments where one subject will undergo many trials with binomial outcomes?More info/part 2 of question: I initially analyzed my data using ANOVAs in SPSS. The SPSS indicated a highly significant interaction, one that is logical and predicted. When running the same data to modeled in glmer, that interaction in highly insignificant. When running through lmer, it is significant again.If anyone can help shed some light on whether this makes sense or why it would be so, I'd appreciate it very much.","Creater_id":125404,"Start_date":"2016-08-02 13:00:45","Question_id":226946,"Tags":["r","generalized-linear-model","random-effects-model","lmer","glmer"],"Answer_count":1,"Last_activity":"2016-08-03 01:50:07","Link":"http://stats.stackexchange.com/questions/226946/r-lmer-vs-glmer","Creator_reputation":6}
{"_id":{"$oid":"5837a587a05283111e4d65b4"},"View_count":19,"Display_name":"DreX","Question_score":0,"Question_content":"Problem DefinitionThe classification problem in hand is to detect an event that should occur (target=1) within the next forward window of fw days using the current environment state which contains 26 features.The dataset is unbalanced, with 508 event occurrences (target=1) vs. 3024 non-occurrences (target=0).The dataset was normalized (min-max-scaling) and the SVM was trained on 70% of the dataset. Classifying the unseen data returned class 0 for all, however about 12% should be 1. I first suspected that this is due to the dataset being biased towards non-occurrences, so I balanced the dataset by up-sampling occurrences, however the same result was obtained. A grid search on C and gamma for the RBF kernel was performed, each time the classification is 0.QuestionIs it possible that the result obtained is because the input features are not sufficient to detect the event occurrence? Or is there something else the could be experimented with prior to changing the dataset features?","Creater_id":125460,"Start_date":"2016-08-03 01:02:49","Question_id":227011,"Tags":["classification","svm","unbalanced-classes","libsvm","one-class"],"Answer_count":1,"Last_activity":"2016-08-03 01:32:18","Link":"http://stats.stackexchange.com/questions/227011/svm-model-never-fires","Creator_reputation":3}
{"_id":{"$oid":"5837a587a05283111e4d65c1"},"View_count":40,"Display_name":"Georgios Nikolaidis","Question_score":2,"Question_content":"I am trying to understand how to derive the quantile function from the cdf.The Singh-Maddala cdf is Should i just solve for x ?Thank you","Creater_id":122014,"Start_date":"2016-08-02 15:53:49","Question_id":226970,"Tags":["cdf","inverse-cdf"],"Answer_count":0,"Last_activity":"2016-08-03 01:13:29","Link":"http://stats.stackexchange.com/questions/226970/quantile-function-of-singh-maddala","Creator_reputation":11}
{"_id":{"$oid":"5837a587a05283111e4d65c3"},"View_count":85,"Display_name":"Johanna","Question_score":3,"Question_content":"I want to run a meta-analysis over several studies reporting one-sample data (e.g. comparing participants' scores against a baseline score of zero). I calculated Cohen's d by dividing the difference of the sample mean and the baseline score by the sample standard deviation (as reported in the first answer here). How do I get the sampling variance of this effect size? (needed in the meta-analysis for calculating inverse variance weights)","Creater_id":94863,"Start_date":"2016-08-02 02:54:13","Question_id":226836,"Tags":["r","variance","meta-analysis"],"Answer_count":1,"Last_activity":"2016-08-03 00:57:04","Link":"http://stats.stackexchange.com/questions/226836/sampling-variance-for-meta-analysis-one-sample-data","Creator_reputation":78}
{"_id":{"$oid":"5837a587a05283111e4d65cf"},"View_count":36,"Display_name":"Tiếu Thủy","Question_score":0,"Question_content":"I have n series of values as follow. Let's say they are scores of students in a class (just example)s1: x1, x2 ... xks2: x1, x2 ... xk..sn: x1, x2 ... xkI want to test the difference between these series, i.e. to answer the question that: are performances of different student similar or not. What kind of test I can use?Thanks a lot,UpdateThe purpose is, I want to see if students are quite different, they should be divided in different class rather than stay in a same class.","Creater_id":91530,"Start_date":"2016-08-01 23:06:11","Question_id":226804,"Tags":["statistical-significance","group-differences"],"Answer_count":1,"Last_activity":"2016-08-03 00:54:53","Link":"http://stats.stackexchange.com/questions/226804/testing-difference-between-group-of-values","Creator_reputation":37}
{"_id":{"$oid":"5837a587a05283111e4d65dc"},"View_count":140,"Display_name":"ahajib","Question_score":0,"Question_content":"I have been reading several papers, articles and blog posts about RNNs (LSTM specifically) and how we can use them to do time series prediction. In almost all examples and codes I have found, the problem is defined as finding the next x values of a time series based on previous data. What I am trying to solve is the following:Assuming We have t values of a time series, what would be its value at time t+1?So using different LSTM packages (deeplearning4j, keras, ...) that are out there, here is what I am doing right now:Create a LSTM network and fit it to t samples. My network has one input and one output. So as for input I will have the following patterns and I call them train data:  t_1,t_2    t_2,t_3    t_3,t_4The next step is to use for example t_4 as input and expect t_5 as output then use t_5 as input and expect t_6 as output and so on.When done with prediction, I use t_5,t_6 to update my model.My question: Is this the correct way of doing it? If yes, then I have no idea what does batch_size mean and why it is useful. Note: An alternative that comes to my mind is something similar to examples which generate a sequence of characters, one character at a time. In that case, batch_size would be a series of numbers and I am expecting the next series with the same size and the one value that I'm looking for would be the last number in that series. I am not sure which of the above mentioned approaches are correct and would really appreciate any help in this regard.Thanks","Creater_id":117831,"Start_date":"2016-07-12 12:37:24","Question_id":223420,"Tags":["time-series","prediction","deep-learning","lstm","rnn"],"Answer_count":1,"Last_activity":"2016-08-03 00:44:05","Link":"http://stats.stackexchange.com/questions/223420/using-rnn-lstm-for-predicting-one-feature-value-of-a-time-series","Creator_reputation":132}
{"_id":{"$oid":"5837a587a05283111e4d65e9"},"View_count":78,"Display_name":"Ole","Question_score":1,"Question_content":"I have a question regarding a binary logistic regression.Its about the plausibility of the influence of one independent variable.I am analyzing the influence of the size of a farm on the use of the plough.Here I am showing the average values of this describing variable (there are all in all six variables). In the analysis I take the logarithm of this variable. group 1 (n=66): non-user of plough (average value of farm size 267 hectares; log of farm size=2,2; histogram below) group 2 (n=181):      user of plough (average value of farm size = 333 hectares; log farm size = 2,2; histogram below)Edit: Regression results + collinearity diagnostics: The results say, the larger the farm size, the more likely it is to belong to group 1. Although the farm size in this group is obviously much lower.I can find no errors in the data, but I am surprised about this result.Would You say that these results are an indication for misspecification or problems within the data?I think taking the logarithm leads to an re-adjusting of the farm size variable. However,I dont like to make this process undone, because I am analyzing further three dependent variables with the same set of independent variables. ","Creater_id":53707,"Start_date":"2016-08-01 08:09:52","Question_id":226698,"Tags":["regression","logistic"],"Answer_count":2,"Last_activity":"2016-08-03 00:15:35","Link":"http://stats.stackexchange.com/questions/226698/logistic-regression-confused-about-one-independent-variable","Creator_reputation":41}
{"_id":{"$oid":"5837a587a05283111e4d65f6"},"View_count":25,"Display_name":"Simon","Question_score":0,"Question_content":"Lets say I measure peoples reaction time across 2 manipulations (condition 1 and condition 2) within subjects (i.e. the same participant completes both conditions). My prediction is the RT measure in condition 2 is significantly more variable than it is in condition 1A bit more detail about my setup...each person completes 5 trials per condition. Within each trial, they are pressing a button in response to a stimulus. They do this 20 times each trial, and I take the average of the 20, which is their mean reaction time for that trial. Over the course of the experiment, this means I would get 5 mean reaction times per condition (1 for each trial). I then average over these 5 to get a mean RT for that person, for that condition. Repeat for the second condition.Ultimately, I end up with 2 numbers per participant (1 for each condition) which is the mean of (mean) RTs across all the sub trials. I want to see whether the variability of mean RT scores is greater in one condition vs. the other across all of my participants.Here is an example of my data layout:What would be the correct way to test this?Could it simply be an F test, with the Fvalue tested being var(1)/var(2)? Could Levene's test be used for this?Given that condition is a within subjects factor, should I be averaging the sub trials the way that I am? Because if I calculate variance of condition 1, thats the variance across all participants, rather than for the individual subject. So I feel like I'm actually losing the benefit of using a within subjects/repeated measures design by doing this. The reason for doing it this way is due to concerns about violating independence if multiple rows correspond to the same subject","Creater_id":68268,"Start_date":"2016-08-02 16:15:34","Question_id":226973,"Tags":["anova","variance","experiment-design","independence"],"Answer_count":1,"Last_activity":"2016-08-02 23:30:37","Link":"http://stats.stackexchange.com/questions/226973/comparing-variability-between-conditions","Creator_reputation":185}
{"_id":{"$oid":"5837a587a05283111e4d6603"},"View_count":58,"Display_name":"James","Question_score":2,"Question_content":"Say I have a time-series of a parameter . Each value of  has an uncertainty of  due to how it was measured. What is the uncertainty on the calculated rms or standard deviation?As an example, say I was measuring a voltage signal . The voltage measurement device has a specified uncertainty () of 0.5%. If I want to say the AC voltage signal had an RMS of 240 volts , how would I know the uncertainty on the rms?","Creater_id":80547,"Start_date":"2016-08-02 19:58:07","Question_id":226991,"Tags":["uncertainty"],"Answer_count":0,"Last_activity":"2016-08-02 23:25:16","Link":"http://stats.stackexchange.com/questions/226991/uncertainty-on-standard-deviation-and-rms","Creator_reputation":111}
{"_id":{"$oid":"5837a587a05283111e4d6605"},"View_count":52,"Display_name":"Pinocchio","Question_score":2,"Question_content":"I was trying to train an auto-encoder. Specifically, I was trying to train an MNIST auto-encoder. Auto-encoders usually minimize the reconstruction error. When I train my auto-encoder should I normalize the input data and the output data or just the input data and then leave the target data normalized? So train with normalized MNIST but reconstruct the original MNIST? Or both normalized?Formally:Let  denote the normalizing transform. If I have a training data set  (what I call input) and the labels (or target) , usually in the auto-encoder setting .  Intuitively we want  where  denotes the auto-encoder making a reconstruction. We could train the auto-encoder with  as training data but leave  unnormalized (so its the same as the original \"virgin\" data). In this setting we want . When training there are 4 options. We could have:Input: , Target: Input: , Target: Input: , Target: Input: , Target: which of the four options is correct? My hunch is that 1 and 3 are the only sound ones. 4 sounds like the worst idea. Though 2 does sound tempting but I am not 100%. Which do people usually do? Specially if I want to, say, compare my auto-encoder with state of the art?  ","Creater_id":37632,"Start_date":"2016-08-02 22:46:33","Question_id":226999,"Tags":["machine-learning","neural-networks","pca","conv-neural-network","autoencoders"],"Answer_count":0,"Last_activity":"2016-08-02 22:46:33","Link":"http://stats.stackexchange.com/questions/226999/should-one-normalize-both-the-input-and-the-target-distribution-when-training-an","Creator_reputation":795}
{"_id":{"$oid":"5837a587a05283111e4d6607"},"View_count":846,"Display_name":"Arvo P.","Question_score":1,"Question_content":"In R, I have two variables, x and y, and a basic VAR model with just one lag, i.e. (as I understand it) the model basically is:x(t) = a*x(t-1) + b*y(t-1) + c + error1y(t) = d*x(t-1) + e*y(t-1) + f + error2with a,b,c,d,e,f some constants.How do you interpret the irf output (impulse response coefficients)? What magnitude is the impulse and what are the irf plot y-axis units?Specifically I was puzzled by the observation that when I scaled my two time series by multiplying both by 100, the irf plot y-axis values and impulse response coefficients were also multiplied by 100. I would have thought that they remain the same (e.g. \"one unit shock in x(t) leads to  units response in y(t+1)\", and that the coefficient between the two does not depend on the units we use for both of the variables).Can you help with the irf result interpretation and why the scaling leads to such a result?","Creater_id":108442,"Start_date":"2016-03-22 15:02:42","Question_id":203122,"Tags":["var","impulse-response"],"Answer_count":1,"Last_activity":"2016-08-02 21:36:20","Link":"http://stats.stackexchange.com/questions/203122/interpreting-var-impulse-response","Creator_reputation":11}
{"_id":{"$oid":"5837a587a05283111e4d6613"},"View_count":64,"Display_name":"Alex","Question_score":5,"Question_content":"I quote (emphasis mine) from the wikipedia definition:  The proposition in probability theory known as the law of total expectation, ..., states that if X is an integrable random variable (i.e., a random variable satisfying E( | X | ) \u0026lt; ∞) and Y is any random variable, not necessarily integrable, on the same probability space, then   \\operatorname{E}(X) = \\operatorname{E} ( \\operatorname{E} ( X \\mid Y))I don't understand what they mean by the same probability space, and do not know why this is an important part of the definition. Take the example further down on the page:  Suppose that two factories supply light bulbs to the market. Factory  X's bulbs work for an average of 5000 hours, whereas factory Y's bulbs  work for an average of 4000 hours. It is known that factory X supplies  60% of the total bulbs available. What is the expected length of time  that a purchased bulb will work for?The random variables here seem to be:The amount of time a light bulb lasts for.Which factory a light bulb comes from.How can these two have the same probability space?","Creater_id":22199,"Start_date":"2016-08-02 20:57:13","Question_id":226993,"Tags":["probability","expected-value","conditional-expectation"],"Answer_count":0,"Last_activity":"2016-08-02 21:14:59","Link":"http://stats.stackexchange.com/questions/226993/law-of-total-expecation-tower-rule-why-must-both-random-variables-come-from-the","Creator_reputation":727}
{"_id":{"$oid":"5837a587a05283111e4d6615"},"View_count":12,"Display_name":"Nor Hisham Haron","Question_score":0,"Question_content":"I have a little difficulty to do a simulation.I need to simulate a data for a certain distribution (say a normal distribution with mean=0, and variance = 1). Of course, we will not get the exactly value for the mean and variance. Usually when we do a simulation, at a certain point the simulated will achieve a stagnant (stationary) value mean and variance. My question is how to get the stagnant (stationary) value in RThank you very much. Hope someone can help me.","Creater_id":13614,"Start_date":"2016-08-02 19:44:24","Question_id":226990,"Tags":["simulation"],"Answer_count":0,"Last_activity":"2016-08-02 19:44:24","Link":"http://stats.stackexchange.com/questions/226990/how-to-get-a-stagnant-stationary-value-in-simulation","Creator_reputation":85}
{"_id":{"$oid":"5837a587a05283111e4d6617"},"View_count":36,"Display_name":"Lukasz","Question_score":1,"Question_content":"I'm attempting to work through the backpropagation through time terms using this source: http://www.deeplearningbook.org/contents/rnn.htmlThe final formulas are given on pages 385 and 386, but I wanted to work through the algebra to get a better understanding for them. I've computed the first two error terms but my solutions do not match 100% what's presented. The network is characterized as follows:I'm basing my work on the followingwhere  and  are the biases for there respective neurons.  is the weight matrix from the previous activation ,  is the weight matrix for the input vector , and  is the weight matrix for our current activations vector . The Loss function is the negative log likelihood and the softmax function is used for output activations to obtain a vector  of probabilities over the output. I have the following example to help with the understanding of the network. Let the inputs ,  the outputs  and the actual values , furthermore we let the weights  and , and finally the biases , and . Written out fully we would have:\\begin{equation}\\begin{pmatrix}a_1^{(t)}\\\\a_2^{(t)}\\\\a_3^{(t)}\\\\a_4^{(t)}\\\\a_5^{(t)}\\end{pmatrix} = \\begin{pmatrix}b_1^{(t)}\\\\b_2^{(t)}\\\\b_3^{(t)}\\\\b_4^{(t)}\\\\b_5^{(t)}\\end{pmatrix} + \\begin{pmatrix}w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \u0026amp; w_{14} \u0026amp; w_{15}\\\\w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \u0026amp; w_{24} \u0026amp; w_{25}\\\\w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \u0026amp; w_{34} \u0026amp; w_{35}\\\\w_{41} \u0026amp; w_{42} \u0026amp; w_{43} \u0026amp; w_{44} \u0026amp; w_{45}\\\\w_{51} \u0026amp; w_{52} \u0026amp; w_{53} \u0026amp; w_{54} \u0026amp; w_{55}\\end{pmatrix} \\begin{pmatrix}h_1^{(t-1)}\\\\h_2^{(t-1)}\\\\h_3^{(t-1)}\\\\h_4^{(t-1)}\\\\h_5^{(t-1)}\\end{pmatrix} + \\begin{pmatrix}u_{11} \u0026amp; u_{12} \u0026amp; u_{13} \u0026amp; u_{14}\\\\u_{21} \u0026amp; u_{22} \u0026amp; u_{23} \u0026amp; u_{24}\\\\u_{31} \u0026amp; u_{32} \u0026amp; u_{33} \u0026amp; u_{34}\\\\u_{41} \u0026amp; u_{42} \u0026amp; u_{43} \u0026amp; u_{44}\\\\u_{51} \u0026amp; u_{52} \u0026amp; u_{53} \u0026amp; u_{54}\\end{pmatrix} \\begin{pmatrix}x_1^{(t)}\\\\x_2^{(t)}\\\\x_3^{(t)}\\\\x_4^{(t)}\\end{pmatrix}\\end{equation}Which we can write a little more compactly as \\begin{align}a_1^{(t)} \u0026amp;= b_1^{(t)} + \\sum\\limits_{i=1}^5 w_{1i}h_i^{(t-1)} + \\sum\\limits_{j=1}^4 u_{1j}x_j^{(t)}\\\\a_2^{(t)} \u0026amp;= b_2^{(t)} + \\sum\\limits_{i=1}^5 w_{2i}h_i^{(t-1)} + \\sum\\limits_{j=1}^4 u_{2j}x_j^{(t)}\\nonumber\\\\a_3^{(t)} \u0026amp;= b_3^{(t)} + \\sum\\limits_{i=1}^5 w_{3i}h_i^{(t-1)} + \\sum\\limits_{j=1}^4 u_{3j}x_j^{(t)}\\nonumber\\\\a_4^{(t)} \u0026amp;= b_4^{(t)} + \\sum\\limits_{i=1}^5 w_{4i}h_i^{(t-1)} + \\sum\\limits_{j=1}^4 u_{4j}x_j^{(t)}\\nonumber\\\\a_5^{(t)} \u0026amp;= b_5^{(t)} + \\sum\\limits_{i=1}^5 w_{5i}h_i^{(t-1)} + \\sum\\limits_{j=1}^4 u_{5j}x_j^{(t)}\\nonumber\\end{align}We could do the same for the output so that we arrive at \\begin{equation} \\label{o_vector}\\begin{pmatrix}o_1^{(t)}\\\\o_2^{(t)}\\\\o_3^{(t)}\\end{pmatrix} = \\begin{pmatrix}c_1^{(t)}\\\\c_2^{(t)}\\\\c_3^{(t)}\\end{pmatrix} + \\begin{pmatrix}v_{11} \u0026amp; v_{12} \u0026amp; v_{13} \u0026amp; v_{14} \u0026amp; v_{15}\\\\v_{21} \u0026amp; v_{22} \u0026amp; v_{23} \u0026amp; v_{24} \u0026amp; v_{25}\\\\v_{31} \u0026amp; v_{32} \u0026amp; v_{33} \u0026amp; v_{34} \u0026amp; v_{35}\\end{pmatrix} \\begin{pmatrix}\\tanh\\big(a_1^{(t)}\\big)\\\\\\tanh\\big(a_2^{(t)}\\big)\\\\\\tanh\\big(a_3^{(t)}\\big)\\\\\\tanh\\big(a_4^{(t)}\\big)\\end{pmatrix}\\end{equation}Which can also be rewritten as:\\begin{align}o_1^{(t)} \u0026amp;= c_1^{(t)} + \\sum\\limits_{i=1}^5 v_{1i}h_i^{(t)}\\\\o_2^{(t)} \u0026amp;= c_2^{(t)} + \\sum\\limits_{i=1}^5 v_{2i}h_i^{(t)}\\nonumber\\\\o_3^{(t)} \u0026amp;= c_3^{(t)} + \\sum\\limits_{i=1}^5 v_{3i}h_i^{(t)}\\nonumber\\end{align}and the softmax outputs:\\begin{equation}\\begin{pmatrix}\\hat{y}_1^{(t)}\\\\\\hat{y}_2^{(t)}\\\\\\hat{y}_3^{(t)}\\end{pmatrix} = \\frac{1}{\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)}\\begin{pmatrix}\\exp\\big(o_1^{(t)}\\big)\\\\\\exp\\big(o_2^{(t)}\\big)\\\\\\exp\\big(o_3^{(t)}\\big)\\end{pmatrix}\\end{equation}In the derivation of the backpropagation through time we assume that the outputs  are used as the argument to the softmax function to obtain the vector  of the probabilities over the output. It is also assumed that the loss is the negative log-likelihood of the true target  given the input so far. We start the recursion with the nodes immediately preceding the final loss so that:\\begin{equation}\\frac{\\partial L}{\\partial L^{(t)}} = 1\\end{equation}For the example the loss is expressed as:\\begin{align}L \u0026amp;= -\\sum\\limits_{i=1}^3 y_i^{(t)}\\ln\\big(\\hat{y}_i^{(t)}\\big)\\\\\u0026amp;= -\\bigg( y_1^{(t)}\\ln\\big(\\hat{y}_1^{(t)}\\big) + y_2^{(t)}\\ln\\big(\\hat{y}_2^{(t)}\\big) + y_3^{(t)}\\ln\\big(\\hat{y}_3^{(t)}\\big)\\bigg)\\\\\u0026amp;= -\\Bigg[y_1^{(t)}\\ln\\begin{pmatrix} \\frac{\\exp\\big(o_1^{(t)}\\big)}{\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)}\\end{pmatrix} + y_2^{(t)}\\ln\\begin{pmatrix} \\frac{\\exp\\big(o_2^{(t)}\\big)}{\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)}\\end{pmatrix} + y_3^{(t)}\\ln\\begin{pmatrix} \\frac{\\exp\\big(o_3^{(t)}\\big)}{\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)}\\end{pmatrix} \\Bigg]\\\\\u0026amp;= -\\Bigg[y_1^{(t)}\\bigg(o_1^{(t)} - \\ln\\Big(\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)\\Big)\\bigg) +  y_2^{(t)}\\bigg(o_2^{(t)} - \\ln\\Big(\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)\\Big)\\bigg) \\nonumber\\\\\u0026amp; \\ \\qquad + y_3^{(t)}\\bigg(o_3^{(t)} - \\ln\\Big(\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)\\Big)\\bigg)\\Bigg]\\end{align}When computing the gradient  on the outputs at time step  for all , I get:\\begin{align}\\frac{\\partial L}{\\partial o_1^{(t)}} = \\frac{\\partial L}{\\partial L^{(t)}}\\frac{\\partial L^{(t)}}{\\partial o_1^{(t)}} \u0026amp;= - \\Bigg[ y_1^{(t)} \\begin{pmatrix}1 - \\frac{\\exp(o_1^{(t)})}{\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)}\\end{pmatrix} + y_2^{(t)} \\begin{pmatrix} - \\frac{\\exp(o_1^{(t)})}{\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)}\\end{pmatrix} \\\\\u0026amp;  \\ \\qquad + y_3^{(t)} \\begin{pmatrix} - \\frac{\\exp(o_1^{(t)})}{\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(t)}\\big)}\\end{pmatrix} \\Bigg]\\nonumber\\end{align}which can be simplified to \\begin{align}\\frac{\\partial L}{\\partial o_1^{(t)}} = \\frac{\\partial L}{\\partial L^{(t)}}\\frac{\\partial L^{(t)}}{\\partial o_1^{(t)}} \u0026amp;= -\\Bigg[y_1^{(t)} \\Big(1 - \\hat{y}_1^{(t)}\\Big) + y_2^{(t)}\\Big(-\\hat{y}_1^{(t)}\\Big)+ y_3^{(t)}\\Big(-\\hat{y}_1^{(t)}\\Big) \\Bigg]\\end{align}distributing the minus sign and writing this in matrix form we have:\\begin{equation}\\begin{pmatrix}\\Big(\\hat{y}_1^{(t)}-1\\Big) \u0026amp; \\hat{y}_1^{(t)} \u0026amp; \\hat{y}_1^{(t)}\\\\\\hat{y}_2^{(t)} \u0026amp; \\Big(\\hat{y}_2^{(t)}-1\\Big) \u0026amp; \\hat{y}_2^{(t)}\\\\\\hat{y}_3^{(t)} \u0026amp; \\hat{y}_3^{(t)} \u0026amp;  \\Big(\\hat{y}_3^{(t)}-1\\Big)\\end{pmatrix} \\begin{pmatrix}y_1^{(t)} \\\\y_2^{(t)}\\\\y_3^{(t)}\\end{pmatrix}  \\end{equation}Yet this is calculated to be: \\begin{equation}\\big(\\nabla_{\\mathbf{o}^{(t)}} L\\big)_i  = \\frac{\\partial L}{\\partial o_i^{(t)}} = \\frac{\\partial L}{\\partial L^{(t)}}\\frac{L^{(t)}}{\\partial o_i^{(t)}} = \\hat{y}_i^{(t)} - \\mathbf{1}_{i, y^{(t)}}\\end{equation}Where  if the condition is true, else is zero. (I'm relatively certain that my calculations are correct and it's just a notation issue between what I have and what's presented in the book)Moving on, we take a look at  at the final time step  and begin by computing \\begin{align}\\hat{y}_1^{(\\tau)} \u0026amp;= \\frac{\\exp(o_1^{(\\tau)})}{\\sum\\limits_{i=1}^3 \\exp\\big(o_i^{(\\tau)}\\big)}\\\\\u0026amp;= \\frac{\\exp\\bigg(c_1^{(\\tau)} + \\sum\\limits_{i=1}^5 v_{1i}h_i^{(\\tau)}\\bigg)}{\\sum\\limits_{k=1}^3 \\exp\\bigg(c_k^{(\\tau)} + \\sum\\limits_{l=1}^5 v_{kl}h_l^{(\\tau)}\\bigg)}\\end{align}For simplicity we let\\begin{equation}S=\\sum\\limits_{k=1}^3 \\exp\\bigg(c_k^{(\\tau)} + \\sum\\limits_{l=1}^5 v_{kl}h_l^{(\\tau)}\\bigg)\\end{equation}then the partial derivative with respect to  is:\\begin{align}\\frac{\\partial \\hat{y}_1^{(\\tau)}}{\\partial h_1} =\\Bigg[ v_{11}\\exp\\bigg(c_1^{(\\tau)} + \\sum\\limits_{i=1}^5 v_{1i}h_i^{(\\tau)}\\bigg) \\times S - \\Bigg(\u0026amp;v_{11}\\exp\\bigg(c_1^{(\\tau)} + \\sum\\limits_{i=1}^5 v_{1i}h_i^{(\\tau)}\\bigg) \\\\\u0026amp; \\ + v_{21}\\exp\\bigg(c_2^{(\\tau)} + \\sum\\limits_{i=1}^5 v_{1i}h_i^{(\\tau)}\\bigg)\\nonumber\\\\\u0026amp; + v_{31}\\exp\\bigg(c_3^{(\\tau)} + \\sum\\limits_{i=1}^5 v_{1i}h_i^{(\\tau)}\\bigg)\\Bigg)\\nonumber\\\\\u0026amp; \\ \\times \\exp\\bigg(c_1^{(\\tau)} + \\sum\\limits_{i=1}^5 v_{1i}h_i^{(\\tau)}\\bigg)\\Bigg]\\nonumber\\\\\\Bigg/ S^2\\nonumber\\end{align}written a little more compactly we would have\\begin{align}\\frac{\\partial \\hat{y}_1^{(\\tau)}}{\\partial h_1} \u0026amp;= \\frac{v_{11}\\exp(o_1^{(\\tau)})\\times S - \\bigg( v_{11}\\exp(o_1^{(\\tau)}) + v_{21}\\exp(o_2^{(\\tau)}) + v_{31}\\exp(o_3^{(\\tau)})\\bigg) \\times \\exp(o_1^{(\\tau)})}{S^2}\\\\\u0026amp;= v_{11} \\hat{y}_1^{(\\tau)} - v_{11} \\hat{y}_1^{(\\tau)} \\hat{y}_1^{(\\tau)} - v_{21} \\hat{y}_2^{(\\tau)} \\hat{y}_1^{(\\tau)} - v_{31} \\hat{y}_3^{(\\tau)} \\hat{y}_1^{(\\tau)}\\\\\u0026amp;=  -\\hat{y}_1^{(\\tau)}\\bigg[v_{11}\\Big( \\hat{y}_1^{(\\tau)}-1\\Big) + v_{21} \\hat{y}_2^{(\\tau)} + v_{31} \\hat{y}_3^{(\\tau)}\\bigg]\\end{align}We could do the same for  and  to yield:\\begin{align}\\frac{\\partial \\hat{y}_1^{(\\tau)}}{\\partial h_2} \u0026amp;= -\\hat{y}_1^{(\\tau)}\\bigg[v_{12}\\Big( \\hat{y}_1^{(\\tau)}-1\\Big) + v_{22} \\hat{y}_2^{(\\tau)} + v_{32} \\hat{y}_3^{(\\tau)}\\bigg]\\\\\\frac{\\partial \\hat{y}_1^{(\\tau)}}{\\partial h_3} \u0026amp;= -\\hat{y}_1^{(\\tau)}\\bigg[v_{13}\\Big( \\hat{y}_1^{(\\tau)}-1\\Big) + v_{23} \\hat{y}_2^{(\\tau)} + v_{33} \\hat{y}_3^{(\\tau)}\\bigg]\\\\\\frac{\\partial \\hat{y}_1^{(\\tau)}}{\\partial h_4} \u0026amp;= -\\hat{y}_1^{(\\tau)}\\bigg[v_{14}\\Big( \\hat{y}_1^{(\\tau)}-1\\Big) + v_{24} \\hat{y}_2^{(\\tau)} + v_{34} \\hat{y}_3^{(\\tau)}\\bigg]\\\\\\frac{\\partial \\hat{y}_1^{(\\tau)}}{\\partial h_5} \u0026amp;= -\\hat{y}_1^{(\\tau)}\\bigg[v_{15}\\Big( \\hat{y}_1^{(\\tau)}-1\\Big) + v_{25} \\hat{y}_2^{(\\tau)} + v_{35} \\hat{y}_3^{(\\tau)}\\bigg]\\end{align}for  this can be written as:\\begin{equation}-\\hat{y}_1^{(\\tau)}\\begin{pmatrix}v_{11} \u0026amp; v_{21} \u0026amp; v_{31}\\\\v_{12} \u0026amp; v_{22} \u0026amp; v_{32}\\\\v_{13} \u0026amp; v_{23} \u0026amp; v_{33}\\\\v_{14} \u0026amp; v_{24} \u0026amp; v_{34}\\\\v_{15} \u0026amp; v_{25} \u0026amp; v_{35}\\end{pmatrix} \\begin{pmatrix}\\hat{y}_1^{(\\tau)}-1\\\\\\hat{y}_2^{(\\tau)}\\\\\\hat{y}_3^{(\\tau)}\\end{pmatrix}\\end{equation}If we were to compute  and  we would have something resembling . We would have:I'm unsure whether I have made an error or if there is a problem in the source material. My computations show that the matrices would be multiplied by  which is nowhere to be found in the source material, and I'm unsure whether the mistake is no my part or I'm missing something.Tex file for the abovePDF file for the above","Creater_id":116337,"Start_date":"2016-08-02 14:48:59","Question_id":226962,"Tags":["machine-learning","neural-networks","backpropagation"],"Answer_count":0,"Last_activity":"2016-08-02 19:39:49","Link":"http://stats.stackexchange.com/questions/226962/backpropagation-through-time-error-computation","Creator_reputation":111}
{"_id":{"$oid":"5837a587a05283111e4d6619"},"View_count":27,"Display_name":"m33lky","Question_score":0,"Question_content":"I'm having trouble understanding intuitively how the bootstrapping technique helps improve quality of estimators. How and why can I get a better linear regression model using bootstrap? Is it because the mean for each parameter in linear regression will be more accurate through bootstrap than by optimizing cross-validation error alone?","Creater_id":39997,"Start_date":"2016-08-02 19:06:30","Question_id":226986,"Tags":["bootstrap","estimators"],"Answer_count":0,"Last_activity":"2016-08-02 19:06:30","Link":"http://stats.stackexchange.com/questions/226986/how-does-bootstrap-improve-estimators","Creator_reputation":108}
{"_id":{"$oid":"5837a587a05283111e4d661b"},"View_count":66,"Display_name":"Waqas","Question_score":0,"Question_content":"I am trying to fit different SARIMA models in R. I first tried to fit on the first 12 data points, I got an error; I thought seasonal differencing requires more data, so I changed it to first 24 data points, still I got an error. Now I am fitting it on the entire data, and still I am getting an error. Following is the code:install.packages(\"MASS\")install.packages(\"forecast\")# Loading:require(MASS)require(forecast)# Data:nottem_ts \u0026lt;- ts(nottem, frequency=12, start=1920)# Problemp \u0026lt;- 0:2;  d \u0026lt;- 0:1;  q \u0026lt;- 0:2;  P \u0026lt;- 0:2;  D \u0026lt;- 0:1;  Q \u0026lt;- 0:2comb    \u0026lt;- as.matrix(expand.grid(p,d,q,P,D,Q))aic_vec \u0026lt;- numeric(nrow(comb))for(k in 1:nrow(comb)){  aic_vec[k] \u0026lt;- AIC(Arima(nottem_ts, order=c(comb[k,1], comb[k,2], comb[k,3]),                           seasonal=list(order=c(comb[k,4], comb[k,5], comb[k,6]),                           period=12), method=\"ML\"))}aic_mat \u0026lt;- as.matrix(aic_vec)   result  \u0026lt;- cbind(comb,aic_mat)sorted  \u0026lt;- result[order(aic_vec),]Is there any solution to this problem and why I am having following error?Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  :non-finite finite-difference value [2]There are no errors if I remove seasonal part, thus I know code is correct and that's why I have not asked the question in a different place, such as Stack Overflow.Remark Removing period=12 and using nottem_ts with frequency 1  makes the error disappear, so seasonality will be captured daily rather than monthly, does doing this make any sense?Comment As far the analysis problem is concerned I can get rid of this by using fourier in Arima and do what I want to do, but I think it is interesting to know the reason of this error.","Creater_id":119117,"Start_date":"2016-08-02 09:45:39","Question_id":226913,"Tags":["r","arima","error-message"],"Answer_count":0,"Last_activity":"2016-08-02 17:56:31","Link":"http://stats.stackexchange.com/questions/226913/non-finite-finite-difference-value-error-in-sarima-models","Creator_reputation":37}
{"_id":{"$oid":"5837a587a05283111e4d661d"},"View_count":62,"Display_name":"tadatitam","Question_score":2,"Question_content":"In hypothesis testing, we have the null hypothesis () and the alternate hypothesis (). The null hypothesis typically states that units drawn from the two groups have identical outcomes, whereas the alternate states that they differ. The Permutation test rejects the null  if the computed p-value is less than the significance criterion (say ). The permutation test itself does not require assumptions of i.i.d, rather a weaker assumption of exchangeability under the null hypothesis is enough for its application.Power is P[rejection | ]. Is it possible to compute power without making i.i.d assumptions? I understand that distributional assumptions (like normality) will make it easier to compute the power, but I'm trying to understand the minimum set of assumptions needed. For example, if one assumes independence and the data is binary, a particular form of  makes the outcomes follow the Binomial distribution. For example, , where  is the probability of a unit in the  group getting outcome . Thus, with just the independence assumption, power can be computed for the specific form of .I can see the assumption of ‘identically distributed’ may be necessary. If the alternative hypothesis is indeed false, we cannot draw any conclusions about the distribution of measurements in the sample unless the measurements are identically distributed. One example of non-identically distributed outcomes is that outcomes are drawn such that all units get  - leading to a certain failure in rejecting the null. Unless  is so extreme that it makes it impossible for one group to get the outcome , this drawing is possible. With identically distributed measurements, this particular drawing is still possible, but it has a low probability (if the alternate is true), thereby leading to a rejection of the null. But, is independence necessary? Edit: Monte Carlo simulations are a way to estimate power, but that requires model assumptions on how real world data is generated. I reworded the question to indicate that we are interested in an exact power computation.","Creater_id":111355,"Start_date":"2016-08-01 12:11:51","Question_id":226736,"Tags":["hypothesis-testing","independence","power-analysis","iid"],"Answer_count":1,"Last_activity":"2016-08-02 16:02:52","Link":"http://stats.stackexchange.com/questions/226736/is-exact-power-analysis-of-the-permutation-or-randomization-test-possible-with","Creator_reputation":18}
{"_id":{"$oid":"5837a587a05283111e4d662a"},"View_count":145,"Display_name":"wannabe_nerd","Question_score":1,"Question_content":"I have a training set (2GB) that contains GIS trajectory data for multiple taxi rides. I want to cluster the final destinations based on their spatial density and have therefore been trying to use the DBSCAN algorithm with the distance metric as the Haversine formula. As a baseline I was able to use K-means with minibatches/online by reading chunks from my pandas dataframe but I've had no success with DBSCAN (lots of comparisons).I'm using scikit/python and have tried reading the csv into a Pandas dataframe and with GraphLab's SFrame. Any suggestions on how to do this with these tools? If not, what is the best way to apply clustering on such large datasets? I haven't tried directly working on a database.","Creater_id":109186,"Start_date":"2016-06-12 04:32:28","Question_id":218530,"Tags":["machine-learning","clustering","data-mining","gis"],"Answer_count":1,"Last_activity":"2016-08-02 15:47:34","Link":"http://stats.stackexchange.com/questions/218530/applying-dbscan-to-a-huge-gis-dataset-with-a-haversine-distance-metric","Creator_reputation":11}
{"_id":{"$oid":"5837a587a05283111e4d6637"},"View_count":38,"Display_name":"jaunty","Question_score":1,"Question_content":"Forgive me if this is a bad question, but I'm a newbie who get's confused with terminology frequently. I am seeking to plan an experiment based on a few factors:-continuous response variable (heart rate)-treatment (a drug - 3 levels)-sleep (survey)-mental state (survey)Two Questions: What type of model should I be focusing on? Since I'm not controlling the factors from the surveys does that make this a multivariate design? If I choose to categorize the survey data into levels, is that what makes the model a simple multiple regression?Should I be adding some other factors such as gender or sex? I noticed that is common practice. Does that have to do with block design?","Creater_id":121836,"Start_date":"2016-08-02 13:25:26","Question_id":226951,"Tags":["regression"],"Answer_count":1,"Last_activity":"2016-08-02 15:43:45","Link":"http://stats.stackexchange.com/questions/226951/multiple-regression-model-vs-multivariate-model","Creator_reputation":8}
{"_id":{"$oid":"5837a587a05283111e4d6644"},"View_count":23,"Display_name":"user125428","Question_score":1,"Question_content":"I'm testing the following hypothesis using OLS: Renewable energy has a positive impact on GDP. I converted all the variables into log.In the 1st run I used GDP as the dependent variable and in the 2nd I used GDPPC. Why are the results so different? I could understand some change but this is a pretty big discrepancy. I\"ve looked at a bunch of papers that do the same thing (albiet with different methods, only one using OLS) and the difference in their R-sq values are usually really small. I've checked to make sure the data and settings are consistent. Any help would be appreciated","Creater_id":125428,"Start_date":"2016-08-02 15:34:42","Question_id":226967,"Tags":["regression","least-squares","macroeconomics"],"Answer_count":0,"Last_activity":"2016-08-02 15:34:42","Link":"http://stats.stackexchange.com/questions/226967/why-does-using-gdppc-as-a-dependent-variable-dramatically-change-results-from-th","Creator_reputation":6}
{"_id":{"$oid":"5837a587a05283111e4d6646"},"View_count":32,"Display_name":"Georg Sievelson","Question_score":1,"Question_content":"In the Guide to Uncertainty in Measurement (GUM), two classes of methods to evaluate uncertainties are distinguished : a type A evaluation is a statistical method, applicable when a set of independently repeated observations is available, and all others means are classified as type B methods.Consider a voltmeter connected to some electrical circuit assumed to give a constant DC voltage, that we wish to measure. Due to (e.g. thermal) noise, the signal is actually not constant, but fluctuates around a average value. On the one hand, using a set of measurements (repeated in time) and a type A evaluation, the average value of the series of observation and the experimental variance of this average value can be evaluated : they provide a measurement result and a type A standard uncertainty. With a large enough number of data points, the standard uncertainty can be made as small as one wishes (and in this case, it is not difficult to get a very large number of experimental observations).On the other hand, a type B evaluation of a single result the voltmeter requires to take into account the nominal precision given in its datasheet. (The question is obviously not specific to voltmeters!)As long as the type A uncertainty for the set of observations  is large with respect to the single-result type B uncertainty , I have to problem to trust the correctness of the type A uncertainty . However, I am confused to what happens when they become of the same order of magnitude. I was not able to find a discussion of this point in the GUM or its supplements (perhaps I missed them). I see different/related possibilities, but which are not satisfactory :switch to type B evaluation: but in this case, how to take into account the additional uncertainty due to the noise ? consider  as a composite quantity and use the composite error propagation: but in this case, how to experimentally determine the covariance  ?combine the standard uncertainties from type A and type B in quadrature, as , but I don't see why this would be justified (in the GUM framework) ?Is there a standard way to treat this situation ?Thanks","Creater_id":125426,"Start_date":"2016-08-02 15:21:56","Question_id":226964,"Tags":["references","measurement-error","uncertainty","error-propagation"],"Answer_count":0,"Last_activity":"2016-08-02 15:21:56","Link":"http://stats.stackexchange.com/questions/226964/type-b-uncertainties-and-statistical-analysis","Creator_reputation":106}
{"_id":{"$oid":"5837a587a05283111e4d6648"},"View_count":59,"Display_name":"Raji Srinivasan","Question_score":0,"Question_content":"I am new to using xtivreg2 - and have a question. Sorry if this has been addressed before. Here goes. My question is on 2SLS regression with panel data.I am interested in estimating an interaction effects model with one endogenous regressor (X1) and two endogenous regressors X2 and X3Y = X1 + X1* X2 + X1* X3 + controls1X1 is endogenousX2 and X3 are exogenousControls1- set of controls to explain YAnd X1= Z+ controls2Controls2 – set of controls to explain X1Right now I am doing it 2SLS by hand, by predicting X1hat and sticking it in the equation for Y. I am concerned that this may not be the right way to do it. Based on some material I have seen on Stata forums, I understand that I have to instrument X1*X2 by Z*X2.. and in essence estimate this model as a multiple endogenous regressor model. Thanks for your help in advance, Rajashri Srinivasan","Creater_id":125403,"Start_date":"2016-08-02 12:37:50","Question_id":226943,"Tags":["econometrics","panel-data","stata","instrumental-variables"],"Answer_count":1,"Last_activity":"2016-08-02 14:53:06","Link":"http://stats.stackexchange.com/questions/226943/stata-interaction-effects-between-one-endogenous-regressor-and-two-exogenous-re","Creator_reputation":8}
{"_id":{"$oid":"5837a587a05283111e4d6655"},"View_count":23,"Display_name":"Pinocchio","Question_score":0,"Question_content":"I was reading section 3.2 1 [enter link description here]1 of the batch normalization paper and it said that they applied BN layer to the layer  rather than to  directly. The justification they gave for this was the following:  since u is likely the output of another nonlinearity, the shape of its distribution is likely to change during training, and constraining its first and second moments would not eliminate the covariate shift.what confuses me is their justification. In particular there are several things that confuse me:What do they mean by the phrase \"constraining its first and second moments\". Does constraining the first and second moments they mean making them 1 and 0 via batch normalization?why does constraining the first and second moments not eliminate the covariate shift when the layer is the output of a non-linearity? What's special about non-linearities that one cannot restrain the moments nor control their covariate shift?why can't we batch normalize the output of a non-linearity and expect to eliminate the covariate shift problem? Does it depend on the non-linearity? Say if I use a sigmoid is it harder to eliminate the covariate shift? What if I use a ReLu? Or even maybe a RBF or a Gaussian?1: Ioffe S. and Szegedy C. (2015),\"Batch Normalization: Accelerating Deep Network Training by ReducingInternal Covariate Shift\",Proceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015.Journal of Machine Learning Research: W\u0026amp;CP volume 37","Creater_id":37632,"Start_date":"2016-08-02 13:29:01","Question_id":226952,"Tags":["machine-learning","neural-networks","optimization","conv-neural-network","batch-normalization"],"Answer_count":0,"Last_activity":"2016-08-02 14:16:40","Link":"http://stats.stackexchange.com/questions/226952/how-does-one-decide-to-what-set-of-activations-to-apply-batch-normalization","Creator_reputation":795}
{"_id":{"$oid":"5837a587a05283111e4d6657"},"View_count":89,"Display_name":"Joe Half Face","Question_score":2,"Question_content":"Reference: http://deeplearning.net/tutorial/lenet.html  To form a richer representation of the data, each hidden layer is  composed of multiple feature maps, . The weights W  of a hidden layer can be represented in a 4D tensor containing  elements for every combination of destination feature map, source  feature map, source vertical position, and source horizontal position.What I can't understand: let's say we slide 2 3-D filters across image, producing two feature maps.  How we can be sure, that parameters of our 2 filters won't move close to same value through backpropagation? So that we won't obtain from 3 channels (RGB) two channels, which are highly correlated?And the more filters I want to introduce, the less obvious for me that some of them (at least) or many of them (in worst case) won't be basically similar vectors in weights space ()","Creater_id":125410,"Start_date":"2016-08-02 13:55:39","Question_id":226953,"Tags":["neural-networks","deep-learning","conv-neural-network"],"Answer_count":0,"Last_activity":"2016-08-02 14:06:39","Link":"http://stats.stackexchange.com/questions/226953/convolutional-neural-network-how-to-ensure-filters-of-same-layer-learn-differe","Creator_reputation":111}
{"_id":{"$oid":"5837a587a05283111e4d6659"},"View_count":15,"Display_name":"Dwaipayan Gupta","Question_score":0,"Question_content":"A species of anaerobic bacterium switches back and forth between the Active state and the Dormant state depending on a variety of external factors, which may be regarded as being random. If active at time , the probability it becomes dormant during the interval  is \u000e; conversely,if dormant at time , the probability it becomes active during the interval  is .Let  denote the conditional probability that  given .Now, my questions are :(a) how do I establish relationship between  and  ?(b) how do I show that  satisfies\\frac{\\partial}{\\partial t} \\bigg \\{ p_{AA}(s,t) \\; \\;exp \\bigg( \\int_0^t [\\delta (u) + \\alpha (u) ] du \\bigg) \\bigg \\} = \\alpha(t) \\; exp \\bigg( \\int_0^t [\\delta (u) + \\alpha (u) ] du \\bigg) \\; \\; ?","Creater_id":88754,"Start_date":"2016-08-02 13:16:46","Question_id":226949,"Tags":["conditional-probability","stochastic-processes","partial","integration"],"Answer_count":0,"Last_activity":"2016-08-02 13:16:46","Link":"http://stats.stackexchange.com/questions/226949/problem-regarding-kolmogorov-forward-equation","Creator_reputation":154}
{"_id":{"$oid":"5837a587a05283111e4d665b"},"View_count":56,"Display_name":"Nucular","Question_score":1,"Question_content":"I have a set of items (say, A, B and C) and all possible pairwise rankings, in the form of probabilities. For example, P(item A \u0026gt; item B) = 0.6.How do you convert into a global probabilistic ranking, such that you have a probability for each combination?For example:P(A \u0026gt; B \u0026gt; C) = ...P(A \u0026gt; C \u0026gt; B) = ...P(B \u0026gt; A \u0026gt; C) = ...etc. Is this problem solved or attacked successfully?","Creater_id":8760,"Start_date":"2016-08-02 12:37:30","Question_id":226942,"Tags":["machine-learning","probability","ranking"],"Answer_count":0,"Last_activity":"2016-08-02 13:04:20","Link":"http://stats.stackexchange.com/questions/226942/how-to-convert-probabilistic-pairwise-rankings-to-probabilistic-global-ranking","Creator_reputation":81}
{"_id":{"$oid":"5837a587a05283111e4d665d"},"View_count":280,"Display_name":"JamesS","Question_score":10,"Question_content":"I realise this topic has come up a number of times before e.g. here, but I'm still unsure how best to interpret my regression output. I have a very simple dataset, consisting of a column of x values and a column of y values, split into two groups according to location (loc). The points look like thisA colleague has hypothesised that we should fit separate simple linear regressions to each group, which I have done using y ~ x * C(loc). The output is shown below.                            OLS Regression Results                            ==============================================================================Dep. Variable:                      y   R-squared:                       0.873Model:                            OLS   Adj. R-squared:                  0.866Method:                 Least Squares   F-statistic:                     139.2Date:                Mon, 13 Jun 2016   Prob (F-statistic):           3.05e-27Time:                        14:18:50   Log-Likelihood:                -27.981No. Observations:                  65   AIC:                             63.96Df Residuals:                      61   BIC:                             72.66Df Model:                           3                                         Covariance Type:            nonrobust                                         =================================================================================                    coef    std err          t      P\u0026gt;|t|      [95.0% Conf. Int.]---------------------------------------------------------------------------------Intercept         3.8000      1.784      2.129      0.037         0.232     7.368C(loc)[T.N]      -0.4921      1.948     -0.253      0.801        -4.388     3.404x                -0.6466      0.230     -2.807      0.007        -1.107    -0.186x:C(loc)[T.N]     0.2719      0.257      1.057      0.295        -0.242     0.786==============================================================================Omnibus:                       22.788   Durbin-Watson:                   2.552Prob(Omnibus):                  0.000   Jarque-Bera (JB):              121.307Skew:                           0.629   Prob(JB):                     4.56e-27Kurtosis:                       9.573   Cond. No.                         467.==============================================================================Looking at the p-values for the coefficients, the dummy variable for location and the interaction term are not significantly different from zero, in which case my regression model essentially reduces to just the red line on the plot above. To me, this suggests that fitting separate lines to the two groups might be a mistake, and a better model might be a single regression line for the whole dataset, as shown below.                            OLS Regression Results                            ==============================================================================Dep. Variable:                      y   R-squared:                       0.593Model:                            OLS   Adj. R-squared:                  0.587Method:                 Least Squares   F-statistic:                     91.93Date:                Mon, 13 Jun 2016   Prob (F-statistic):           6.29e-14Time:                        14:24:50   Log-Likelihood:                -65.687No. Observations:                  65   AIC:                             135.4Df Residuals:                      63   BIC:                             139.7Df Model:                           1                                         Covariance Type:            nonrobust                                         ==============================================================================                 coef    std err          t      P\u0026gt;|t|      [95.0% Conf. Int.]------------------------------------------------------------------------------Intercept      8.9278      0.935      9.550      0.000         7.060    10.796x             -1.2446      0.130     -9.588      0.000        -1.504    -0.985==============================================================================Omnibus:                        0.112   Durbin-Watson:                   1.151Prob(Omnibus):                  0.945   Jarque-Bera (JB):                0.006Skew:                           0.018   Prob(JB):                        0.997Kurtosis:                       2.972   Cond. No.                         81.9==============================================================================This looks OK to me visually, and the p-values for all the coefficients are now significant. However, the AIC for the second model is much higher than for the first.I realise that model selection is about more than just p-values or just the AIC, but I'm not sure what to make of this. Can anyone offer any practical advice regarding interpreting this output and choosing an appropriate model, please? To my eye, the single regression line looks OK (though I realise none of them are especially good), but it seems as though there is at least some justification for fitting separate models(?).Thanks!Edited in response to comments@Cagdas OzgencThe two-line model was fitted using Python's statsmodels and the following codereg = sm.ols(formula='y ~ x * C(loc)', data=df).fit()As I understand it, this is essentially just shorthand for a model like thisy = \\beta_0 + \\beta_1 x + \\beta_2 l + \\beta_3 x lwhere  is a binary \"dummy\" variable representing location. In practice this is essentially just two linear models, isn't it? When ,  and the model reduces toy = \\beta_0 + \\beta_1 xwhich is the red line on the plot above. When ,  and the model becomesy = (\\beta_0 + \\beta_2) + (\\beta_1 +\\beta_3) xwhich is the blue line on the plot above. The AIC for this model is reported automatically in the statsmodels summary. For the one line model I simply usedreg = ols(formula='y ~ x', data=df).fit()I think this is OK?@user2864849I don't think the single line model is obviously better, but I do worry about how poorly constrained the regression line for  is. The two locations (D and N) are very far apart in space, and I wouldn't be at all surprised if gathering additional data from somewhere in the middle produced points plotting roughly between the red and blue clusters I already have. I don't have any data yet to back this up, but I don't think the single line model looks too terrible and I like to keep things as simple as possible :-)Edit 2Just for completeness, here are the residual plots as suggested by @whuber. The two-line model does indeed look much better from this point of view.Two-line modelOne-line modelThanks all!","Creater_id":5467,"Start_date":"2016-06-13 05:43:50","Question_id":218667,"Tags":["regression","p-value","least-squares","aic"],"Answer_count":3,"Last_activity":"2016-08-02 12:47:09","Link":"http://stats.stackexchange.com/questions/218667/simple-linear-regression-p-values-and-the-aic","Creator_reputation":224}
{"_id":{"$oid":"5837a587a05283111e4d666c"},"View_count":48,"Display_name":"user125321","Question_score":0,"Question_content":"I have 2 years day-wise data of stock price.What frequency I should take in this case for next 1 year day wise prediction?I have one day minute-wise data.What frequency I should take for next day forecast minute wise?Actually I want to know what is frequency w.r.t. time series and prediction?I have given tou a scenario : I have 2 years day wise data of stock price.what frequency I should take in this case for next 1 year day wise prediction.ie i want to convert my raw data of stock price for last two years(daily basis)and I want to convert this into a time series for ARIMA model.I want to know what value for frequency argument I should give inside ts() of R s/w.And how it is decided .Is it clear now?","Creater_id":125321,"Start_date":"2016-08-01 23:43:16","Question_id":226810,"Tags":["time-series","forecasting","frequency"],"Answer_count":1,"Last_activity":"2016-08-02 12:31:02","Link":"http://stats.stackexchange.com/questions/226810/what-is-frequency-in-time-series-in-general-and-in-my-examples","Creator_reputation":1}
{"_id":{"$oid":"5837a587a05283111e4d6679"},"View_count":48,"Display_name":"MattCrow","Question_score":2,"Question_content":"When you are performing your analyses, how do you keep your results organized?Do you keep a parallel word document and copy/paste the result as you build your 'story'? Was curious to see how other researchers and statisticians organize their workflows. ","Creater_id":125265,"Start_date":"2016-08-02 11:45:22","Question_id":226937,"Tags":["r","biostatistics","communication"],"Answer_count":1,"Last_activity":"2016-08-02 12:27:26","Link":"http://stats.stackexchange.com/questions/226937/how-do-you-organize-your-output-for-a-project","Creator_reputation":26}
{"_id":{"$oid":"5837a587a05283111e4d6686"},"View_count":36,"Display_name":"tom","Question_score":1,"Question_content":"I'm an engineering student and I'm facing a problem related to regression. My data set consists of m measurements  with normal distributed errors in  as well as in .The angle measurements are in .The functional connection between  and  is known as for . The equation is linear in  and . My question now is which type of regression method should I use to obtain good estimates for  and . I never made a regression without an intercept term before. Are there any other preprocessing step necessary?Best regardsTom","Creater_id":125136,"Start_date":"2016-07-31 05:56:33","Question_id":226546,"Tags":["multiple-regression"],"Answer_count":1,"Last_activity":"2016-08-02 12:12:39","Link":"http://stats.stackexchange.com/questions/226546/which-type-of-regression-should-i-use","Creator_reputation":6}
{"_id":{"$oid":"5837a587a05283111e4d6693"},"View_count":4154,"Display_name":"schvost","Question_score":1,"Question_content":"Is \"linear-by-linear association\" in SPSS another name for the chi-squared test for trend? If not, what is it?","Creater_id":75151,"Start_date":"2015-12-03 15:04:00","Question_id":184918,"Tags":["chi-squared","terminology","trend"],"Answer_count":2,"Last_activity":"2016-08-02 12:10:49","Link":"http://stats.stackexchange.com/questions/184918/what-does-linear-by-linear-association-in-spss-mean","Creator_reputation":36}
{"_id":{"$oid":"5837a587a05283111e4d66a1"},"View_count":22,"Display_name":"Hamideh Iraj","Question_score":1,"Question_content":"I have a dataset of categorical variables. Consider the following predictors:Age (18-23,23-28, 28-35, 35+) Education(high-school,two-year degree, bachelor,master,phd) Experience(0-3,3-5,5-7,7+) I want to predict salary (0-1.5,1.5-3,3-4.5,4.5+)  Before using chi-squre test or log-linear model or logistic regression, I created a contingency table to make sure my cells have at least 5 (or 10) values. Here a problem comes in: there are empty cells that cannot be filled logically. For example, phds cannot fall into 18-23 or 23-28 ranges. As another example, 18-23 year olds are very unlikely to have 4.5+ years of experience. The Common practice is combining categories so that each cell in the contingency table has more than 5 (or 10) values. If I do that, I lose the details in my data.  What should I do?  ","Creater_id":41879,"Start_date":"2016-08-02 10:05:47","Question_id":226918,"Tags":["categorical-data","contingency-tables"],"Answer_count":2,"Last_activity":"2016-08-02 12:04:42","Link":"http://stats.stackexchange.com/questions/226918/problem-in-categorical-data-impossible-cells-in-contingency-table","Creator_reputation":134}
{"_id":{"$oid":"5837a587a05283111e4d66af"},"View_count":145,"Display_name":"moku","Question_score":7,"Question_content":"I've been studying statistics for a little while and I keep coming back to Bayes' theorem trying to relearn it and have that \"ah ha\" moment with it. I keep coming back to it because I understand just how important Bayes is in statistics and I want to have a deep understanding of it aside from just applying the formula. So, are there any resources, series of problem, lectures, diagrams etc. that really popped the lid off Bayes for you and if so what were they? Thanks again for the input ","Creater_id":53048,"Start_date":"2015-01-05 13:54:42","Question_id":131322,"Tags":["probability","conditional-probability","bayes"],"Answer_count":1,"Last_activity":"2016-08-02 11:50:04","Link":"http://stats.stackexchange.com/questions/131322/resources-for-the-ah-ha-moment-when-learning-bayes-theorem","Creator_reputation":163}
{"_id":{"$oid":"5837a587a05283111e4d66bc"},"View_count":135,"Display_name":"Joe","Question_score":4,"Question_content":"In the Wikipedia article on the Rubin causal model I stumbled upon the following quote:  We require that \"the [potential outcome] observation on one unit should be unaffected by the particular assignment of treatments to the other units\" (Cox 1958, §2.4). This is called the Stable Unit Treatment Value Assumption (SUTVA), which goes beyond the concept of independence.(https://en.wikipedia.org/wiki/Rubin_causal_model)Please, can anyone tell me in what sense the SUTVA 'goes beyond the concept of independence'? I don't get the difference between the two.","Creater_id":98634,"Start_date":"2016-07-03 08:24:48","Question_id":221939,"Tags":["econometrics","independence","causality"],"Answer_count":1,"Last_activity":"2016-08-02 11:46:05","Link":"http://stats.stackexchange.com/questions/221939/sutva-vs-independence","Creator_reputation":26}
{"_id":{"$oid":"5837a587a05283111e4d66c9"},"View_count":74,"Display_name":"States.the.Obvious","Question_score":1,"Question_content":"I have bi-weekly data for an event for which I am trying to build a forecasting model. When I plot the ACF and PACF, I get the following plots:From what I understand, the plots show that the data are seasonal and seasonality has almost a fixed period of length 13 (as there are 13 bars in each block in the ACF plot). The data also seem to have a downward trend because of the auto correlation diminishes from left to right in the plot. My questions are:Am I interpreting the plots correctly?What types of models should I try with such data?I have already tried auto.arima() and HoltWinters() from the forecast package without much success. Any guidance is appreciated! Thanks!","Creater_id":90264,"Start_date":"2016-08-02 11:35:19","Question_id":226934,"Tags":["r","time-series","forecasting","model-selection","autocorrelation"],"Answer_count":0,"Last_activity":"2016-08-02 11:43:00","Link":"http://stats.stackexchange.com/questions/226934/how-to-use-these-acf-and-pacf-plots-for-forecasting","Creator_reputation":120}
{"_id":{"$oid":"5837a587a05283111e4d66cb"},"View_count":31,"Display_name":"rec","Question_score":2,"Question_content":"Right now I am using some machine learning techniques (in particular SVMs) to fit to some time series data. The time series version of k-fold cross validation is walk-forward analysis with a large enough window. This solves the problem of model verification, but what about model parameter selection?Suppose we have a nonstationary process that we want to model. For parameter selection we can use a grid search. However, if we train on the data at time  and then test on  and repeat this process over and over again fitting on a bunch of different subsets of the data in sequence, does this really tell us what the best parameters are? It's possible the data at  is old and no longer fully reflects the behavior of future data. So if a model that did extremely well too far in the past exists, it could accidentally beat out a model that has worse past performance, but better overall recent performance.My solution to this is that we choose a set of parameters, and then cross-validate the parameters (either with walk-forward analysis or another technique) and then use the score coming from that to determine the best parameters. We repeat this process for all of the parameters we are grid searching on and then choose the best. Is this the best way to handle this problem, or is there a better way? Any links to papers/websites discussing this would be awesome. Rob J. Hyndman had an excellent blog post \"Why every statistician should know about cross-validation\" that made me think about this.","Creater_id":124589,"Start_date":"2016-08-02 10:16:02","Question_id":226920,"Tags":["time-series","svm","cross-validation","model-selection"],"Answer_count":0,"Last_activity":"2016-08-02 11:39:28","Link":"http://stats.stackexchange.com/questions/226920/selecting-the-best-performing-model-for-time-series-analysis","Creator_reputation":43}
{"_id":{"$oid":"5837a587a05283111e4d66cd"},"View_count":107,"Display_name":"sbrown","Question_score":6,"Question_content":"Full disclosure: I am not a statistician, nor do I claim to be one.  I am a lowly IT administrator.  Please play gentle with me. :)I am responsible for collecting and forecasting disk storage use for our enterprise.  We collect our storage use monthly and use a simple rolling twelve month linear regression for forecasts (in other words, only the previous twelve months of data are considered when making a projection).  We use this information for allocation and capital expense planning, e.g. \"Based on this model, we will need to purchase x amount if storage in y months to meet our needs.\"  This all works well enough to suit our needs.Periodically, we have large one-time movements in our numbers that throws the forecasting off.  For example, someone finds 500GB of old backups that aren't needed anymore and deletes them.  Good for them for reclaiming the space!  However our forecasts are now skewed way off by this large drop in one month.  We have always just accepted that a drop like this takes 9-10 months to make its way out of the models, but that can be a really long time if we are entering capital expense planning season.I'm wondering if there is a way to handle these one-time variances such that the forecasted values aren't impacted as much (e.g. the slope of the line doesn't change as dramatically), but they are taken into account (e.g. a one-time change in the y-value associated with a particular point in time).  Our first attempts at tackling this have yielded some ugly results (e.g. exponential growth curves).  We do all of our processing in SQL Server if that matters.","Creater_id":103818,"Start_date":"2016-02-11 13:27:55","Question_id":195124,"Tags":["regression","forecasting","adjustment"],"Answer_count":1,"Last_activity":"2016-08-02 11:32:21","Link":"http://stats.stackexchange.com/questions/195124/adjustments-to-linear-regression-forecast","Creator_reputation":31}
{"_id":{"$oid":"5837a587a05283111e4d66da"},"View_count":29,"Display_name":"TheCompleatIdiot","Question_score":1,"Question_content":"I would like to run a Monte Carlo simulation to identify the probability of an event E occurring. While these are Bernoulli trials, each run will incorporate random selection of several independent, uniformly distributed values. I expect the probability of E to be very low (p \u0026lt; .001, possibly on the order of p ≈ .00001).Is there a simple way to estimate the number of runs I will need to say that, for instance, p \u0026lt; .001 with 99% confidence? I have seen methods for estimation of the number of runs, but they all caveat their poor fit for particularly small or large values of p.As an aside, is the number of runs I need in any way contingent on the number of variables involved?","Creater_id":125396,"Start_date":"2016-08-02 11:30:22","Question_id":226932,"Tags":["monte-carlo","bernoulli-distribution","rare-events"],"Answer_count":0,"Last_activity":"2016-08-02 11:30:22","Link":"http://stats.stackexchange.com/questions/226932/estimating-number-of-monte-carlo-runs-for-low-probability-event","Creator_reputation":6}
{"_id":{"$oid":"5837a587a05283111e4d66dc"},"View_count":79,"Display_name":"Christian","Question_score":2,"Question_content":"Is there a general and completely automated solution to modelling and forecasting time series data? I think this question is extremely important. If it is not possible, please provide an explanation why it isn't. I'm interested to know whether anyone has completely automated the process of model building and forecasting in a way that works and allows a machine or programme do so without intervention.   I'm not so interested in whether a human can improve on what a machine can do. I'm more interested in finding out whether a computer / programme can do this kind of work for us in a way that is \"reasonable\" - I don't have any metric in mind for what \"reasonable\" means.","Creater_id":30192,"Start_date":"2016-08-01 17:50:59","Question_id":226780,"Tags":["machine-learning","time-series","forecasting"],"Answer_count":0,"Last_activity":"2016-08-02 11:19:22","Link":"http://stats.stackexchange.com/questions/226780/is-there-a-general-and-completely-automated-i-e-programmed-solution-to-modell","Creator_reputation":143}
{"_id":{"$oid":"5837a587a05283111e4d66de"},"View_count":343,"Display_name":"alxlvt","Question_score":0,"Question_content":"I'm using lme4 in R, and I have a model set up that uses a three-level hierarchy for a negative binomial regression.There is previously a question (How compute the Intra-Class Correlation for a Negative Binomial Mixed Model in lme4) that addresses how to calculate ICC for a 2-level negative binomial model, but it's unclear to me how to include the third level.","Creater_id":30760,"Start_date":"2015-09-24 15:53:47","Question_id":174071,"Tags":["r","multilevel-analysis","negative-binomial","lme4","intraclass-correlation"],"Answer_count":1,"Last_activity":"2016-08-02 11:06:46","Link":"http://stats.stackexchange.com/questions/174071/how-to-compute-intraclass-correlation-icc-for-three-level-negative-binomial-hi","Creator_reputation":10}
{"_id":{"$oid":"5837a587a05283111e4d66eb"},"View_count":1205,"Display_name":"David","Question_score":2,"Question_content":"Trying to run a panel logistic model.  In the parameters a default NULL is specified for the \"start\" parameter.  My model is:res\u0026lt;-pglm(DFLT_DEBT_01.3~NGDPRPC+NGDP_RPCH+BCA_NGDPD+LC_PER_USD+OFF_RSVA+M2 ,index=c(\"Country\",\"Date\"),start=NULL,data=dat,family=\"binomial\", model=\"within\")However, even if I explicitly set start=NULL I receive the message:Error in prepare Fixed(start = start, activePar = activePar, fixed = fixed) :   argument \"start\" is missing, with no defaultI'm not sure how I can specify starting values as I have not seen anywhere the precise order of specification of the parameters.","Creater_id":73582,"Start_date":"2015-04-14 22:01:46","Question_id":146434,"Tags":["r","logistic","econometrics","panel-data"],"Answer_count":1,"Last_activity":"2016-08-02 11:00:40","Link":"http://stats.stackexchange.com/questions/146434/why-pglm-fails-for-within-model","Creator_reputation":11}
{"_id":{"$oid":"5837a587a05283111e4d66f8"},"View_count":114,"Display_name":"Thomas Browne","Question_score":9,"Question_content":"I would like to understand how I can get the percentage of variance of a data set, not in the coordinate space provided by PCA, but against a slightly different set of (rotated) vectors. set.seed(1234)xx \u0026lt;- rnorm(1000)yy \u0026lt;- xx * 0.5 + rnorm(1000, sd = 0.6)vecs \u0026lt;- cbind(xx, yy)plot(vecs, xlim = c(-4, 4), ylim = c(-4, 4))vv \u0026lt;- eigen(cov(vecs))valuesa1 \u0026lt;- vv[, 1]a2 \u0026lt;- vv[, 2]theta = pi/10rotmat \u0026lt;- matrix(c(cos(theta), sin(theta), -sin(theta), cos(theta)), 2, 2)a1r \u0026lt;- a1 %*% rotmata2r \u0026lt;- a2 %*% rotmatarrows(0, 0, a1[1], a1[2], lwd = 2, col = \"red\")arrows(0, 0, a2[1], a2[2], lwd = 2, col = \"red\")arrows(0, 0, a1r[1], a1r[2], lwd = 2, col = \"green3\")arrows(0, 0, a2r[1], a2r[2], lwd = 2, col = \"green3\")legend(\"topleft\", legend = c(\"eigenvectors\", \"rotated\"), fill = c(\"red\", \"green3\"))So basically I know that the variance of the dataset along each of the red axes, given by PCA, is represented by the eigenvalues. But how could I get the equivalent variances, totalling the same amount, but projected the two different axes in green, which are are a rotation by pi/10 of the principal component axes. IE given two orthogonal unit vectors from the origin, how can I get the variance of a dataset along each of these arbitrary (but orthogonal) axes, such that all the variance is accounted for (ie \"eigenvalues\" sum to the same as that of PCA). ","Creater_id":4705,"Start_date":"2016-08-02 08:46:18","Question_id":226905,"Tags":["r","variance","pca","linear-algebra"],"Answer_count":1,"Last_activity":"2016-08-02 10:54:55","Link":"http://stats.stackexchange.com/questions/226905/how-to-get-eigenvalues-percentages-of-explained-variance-of-vectors-that-are","Creator_reputation":303}
{"_id":{"$oid":"5837a587a05283111e4d6705"},"View_count":15,"Display_name":"Joe","Question_score":1,"Question_content":"Suppose I want to model some returns by \\begin{aligned}r_t \u0026amp;= \\mu_t + a_t \\\\a_t \u0026amp;=\\sigma_t \\epsilon_t \\\\\\sigma_t^2 \u0026amp;= \\alpha_0 + \\alpha_1 a_{t-1}^2 + \\dots + \\alpha_m a_{t-m}^2\\end{aligned}where  denotes a stationary, low-order ARMA process and the error terms  follows an ARCH process. The literature says that the standardized residuals of the ARCH model have to be white noise for the model to be well specified.Can someone please explain me how the residuals of the ARCH model are precisely defined within the above setup?; and why I have to check the standardized residuals instead of the normal ones?","Creater_id":98634,"Start_date":"2016-08-02 07:32:42","Question_id":226883,"Tags":["time-series","residuals","arch"],"Answer_count":1,"Last_activity":"2016-08-02 10:46:46","Link":"http://stats.stackexchange.com/questions/226883/why-check-the-standarized-residuals-of-an-arch-process","Creator_reputation":26}
{"_id":{"$oid":"5837a587a05283111e4d6712"},"View_count":23,"Display_name":"Bajcz","Question_score":1,"Question_content":"Suppose I have two independent variables, X1 and X2. X1 is binary--coded as 1 or 0. X2 has three ordered but most likely not equidistant levels--let's call them \"low,\" \"medium,\" and \"high.\" I want to know if there is a significant interaction between X1 and X2 in a regression I am running.Classically, one would treat X2 as an ordinal variable. However, the interaction between X1 and X2 is not fully crossed--there are no observations for which X1 is equal to 0 and X2 is equal to \"medium.\" This makes the coefficients matrix rank-deficient--one factor level combination is \"missing.\" Moreover, when you include an ordinal variable as an independent variable in a model in R, it models these variables using linear and quadratic terms (which is logical), which introduces a level of complexity into my model I'm not comfortable with, if I can avoid it.I see two alternative approaches:Removal all observations for which X2 is equal to \"medium\" so that I no longer have to treat X2 as anything other than a binary factor. In that case, the interaction between X1 and X2 would become fully crossed. This throws away data though, so I'd rather not go this route.Violate the \"statistics rules\" I had been taught and treat X2 as continuous/numeric instead, coded as something like 0, 1, and 2 for \"low,\" \"medium,\" and \"high,\" respectively. The model would run this way, but the coefficients would be a little messy to interpret, and so this would only be a qualitative assessment of significance (which would probably be good enough, actually, as long as it was unbiased). I would opt for option 2, but I was wondering if this is the \"right\" way to proceed and, if not, what alternatives am I maybe not considering?P.S. I have seen here that some fields treat ordinal variables as continuous routinely. However, as discussed in this thread, one must think carefully about what one is really after when considering this type of question. For me, I am guessing that there is either going to be a very significant interaction, or a very insignificant one. As such, a \"messy\" but \"unbiased\" approach would be sufficient, I think?Edit: For a test model, I ran both approaches listed above and the beta coefficients and P values for the interaction term end up being very, very similar between the two models. My conclusion would be the same in each case (the interaction is significant at alpha = 0.05). Does this potentially strengthen my justification for choosing option 2?","Creater_id":61175,"Start_date":"2016-08-02 10:22:36","Question_id":226922,"Tags":["regression","interaction","ordinal","binary-data","predictor"],"Answer_count":0,"Last_activity":"2016-08-02 10:30:05","Link":"http://stats.stackexchange.com/questions/226922/how-must-one-handle-ordinal-independent-variables-when-modeling-interactions","Creator_reputation":130}
{"_id":{"$oid":"5837a587a05283111e4d6714"},"View_count":77,"Display_name":"Benjamin","Question_score":5,"Question_content":"There is a cluster criterion defined as:\\mathcal{C} = \\operatorname{tr}(S_W^{-1}S_B) = \\sum_{i=1}^d \\lambda_i,where  is the trace,  is the pooled within-group scatter matrix, and  is the between-group scatter matrix;  is the number of features (or dimensions of the scatter matrices). I have two sources for it, one here (eq 103) and the other here (p.22).The R package clusterCrit computes this as the matrix inverse of  (I compared their output with a \"manual\" calculation), but the slides, based on the book by Duda, call this \"the ratio of between to within-cluster scatter in the direction of eigenvectors\".My questions:Is it the matrix inverse or a division? I find a similar term in linear discriminant analysis which is clearly a division. EDIT: Clearly this is the matrix inverse (no \"matrix division\").In the direction of which eigenvectors? What does that direction represent? What do the eigenvectors of  represent? If this is the matrix inverse, then I understand that  can be interpreted as a precision matrix. What is the intuitive interpretation of the trace of ?","Creater_id":8240,"Start_date":"2016-02-29 11:26:20","Question_id":199182,"Tags":["clustering","discriminant-analysis","eigenvalues","matrix-inverse"],"Answer_count":1,"Last_activity":"2016-08-02 10:09:52","Link":"http://stats.stackexchange.com/questions/199182/interpretation-of-the-cluster-criterion-operatornametrs-w-1s-b","Creator_reputation":157}
{"_id":{"$oid":"5837a587a05283111e4d6721"},"View_count":14,"Display_name":"Sorin","Question_score":1,"Question_content":"I have the following problem - I have ran MIKE11 software to compute the c(x,t) = concentration of pollutant in point x on a river (1D case) considering a pollution scenario where quantity q of pollutant at concentration c0 is thrown on the river at location x0. So, by varying x0, q, c0, x, t and running MIKE, I collected a number of 4 millions values of the function   c = f(x,t,x0,q,c0) so a function with 5 variables. So I need now to interpolate this function and I was considering to do spline multivariate interpolation. Are there other alternatives of doing this and what advantages/disadvantages could have.For example I have read about Kriging,  inverse distance weighting, etc. - but are these methods good candidates for this problem?Thank youRegards","Creater_id":125386,"Start_date":"2016-08-02 10:02:22","Question_id":226916,"Tags":["multivariate-analysis","interpolation"],"Answer_count":0,"Last_activity":"2016-08-02 10:09:00","Link":"http://stats.stackexchange.com/questions/226916/interpolation-methods-for-prediction-of-water-pollution","Creator_reputation":6}
{"_id":{"$oid":"5837a587a05283111e4d6723"},"View_count":73,"Display_name":"user2390246","Question_score":1,"Question_content":"I have found some references to \"Descriptive Discriminant Analysis\" or DDA. In particular, this paper recommends it as a follow-up to a significant MANOVA result. Briefly, it is described as \"a statistical procedure which creates a set of perfectly uncorrelated  linear  equations  that  together  model  the differences  among  groups  in  the  MANOVA\". However, I have found surprisingly little mention of DDA elsewhere, which made me wonder if it has a more common name, or if it has been superceded by other methods, or if it is in fact the same as Linear Discriminant Analysis (LDA). I have tried following up references within the above source, but other articles do not seem to make a distinction, and simply refer to discriminant analysis, or LDA. Here it is mentioned briefly in contrast to \"predictive\" discriminant analysis. I can see that there is a difference in motivation between simply wanting to describe differences among groups and wanting to predict group membership, but it is not clear to me why these different aims cannot both be met by an LDA.DDA is implemented separately to Linear Discriminant Analysis (LDA) in the R package DiscriMiner. There the main reference is in French, which made me wonder if this is simply an issue of mis-translating the name of another test. Does \"Descriptive Discriminant Analysis\" exist in its own right, and if so, how does it differ from Linear DA?","Creater_id":109013,"Start_date":"2016-08-02 09:54:25","Question_id":226915,"Tags":["terminology","discriminant-analysis"],"Answer_count":0,"Last_activity":"2016-08-02 09:58:26","Link":"http://stats.stackexchange.com/questions/226915/what-is-descriptive-discriminant-analysis","Creator_reputation":205}
{"_id":{"$oid":"5837a587a05283111e4d6725"},"View_count":25,"Display_name":"Christoph Hanck","Question_score":3,"Question_content":"What is the expected length of a memory game when neither player (you may as well play against yourself) remembers any of the cards being uncovered in previous rounds? Initially and until the first match (because players do not remember), when playing with  pairs, there is a probability of  of a match, as there are  remaining cards which could be a match for the first card you draw. The game is over when all  pairs are matched, and in principle the game could last forever. Hence, a negative binomial distribution would seem like a good starting point, as we aim to model the probability of  successes in a sequence of Bernoulli experiments.However, once a match has been found, the success probability increases to , as the two matches are removed from the game. Hence, the success probability in the trials is not constant. Once there are only two cards left, you are bound to have a match.I found a paper here that discusses the case in which the players play optimally, i.e., forget nothing.","Creater_id":67799,"Start_date":"2016-08-02 08:08:04","Question_id":226890,"Tags":["probability","expected-value","negative-binomial","games"],"Answer_count":1,"Last_activity":"2016-08-02 09:52:40","Link":"http://stats.stackexchange.com/questions/226890/expected-length-of-a-memory-game-of-two-clueless-players","Creator_reputation":9989}
{"_id":{"$oid":"5837a587a05283111e4d6732"},"View_count":29,"Display_name":"Krombopulos Michael","Question_score":2,"Question_content":"I've created a Monte Carlo simulation that randomly divides my data into \"test\" and \"training\"-Samples and then trains a neural network. The ratio of 0 and 1 (19.62%) Category is stabilized on sampling.My results show a highly fluctuating model accuracy (min=0.6452, M=0.7792, max=0.8925).What could be possible reasons for this effect and how should I choose a test/train/model for my discussion?","Creater_id":121161,"Start_date":"2016-06-24 08:06:42","Question_id":220485,"Tags":["neural-networks","cross-validation","monte-carlo"],"Answer_count":1,"Last_activity":"2016-08-02 09:20:48","Link":"http://stats.stackexchange.com/questions/220485/sample-dependency-in-neural-net-training-cross-validation","Creator_reputation":18}
{"_id":{"$oid":"5837a587a05283111e4d673f"},"View_count":56,"Display_name":"Clarinetist","Question_score":0,"Question_content":"From Principles and Theory for Data Mining and Machine Learning, Clarke et al.:  One strategy for choosing , if enough data are available, is to plot the predicted MSE as a function of the size of the training sample. Once the curve levels off, there is no need to increase the size of the portion of the data used for fitting. Thus, the complement gives the size of the holdout portion, and dividing  by this gives an estimate of the optimal .I don't understand this.MSE is dependent on the training sample and test sample: training sample is used to build the model, test sample is used for the points for the MSE comparison. So if I want to choose a training sample of, say, 5 from some large data set, obviously, the MSE I compute will be dependent on what points I choose and not solely the training sample size. So that makes me wonder, how in the world would I compute that curve?","Creater_id":46427,"Start_date":"2016-08-02 08:16:11","Question_id":226894,"Tags":["machine-learning","cross-validation","data-mining"],"Answer_count":1,"Last_activity":"2016-08-02 08:59:04","Link":"http://stats.stackexchange.com/questions/226894/plotting-predicted-mse-as-a-function-of-the-training-sample-size-for-choosing-k","Creator_reputation":343}
{"_id":{"$oid":"5837a587a05283111e4d674c"},"View_count":84,"Display_name":"user2109988","Question_score":0,"Question_content":"I am trying to implement Linear Discriminant Analysis. I have 10 classes and each class has 3 observations at various instances: class 1 = {{a1,a2,a3}          {b1,b2,b3}          {c1,c2,c3}}a,b,c are 3 observations found at various instances a1,a2,a3. Class 1 is a 3*3 Matrix!Now I have to find the mean observation of each class. For example, I have to find the mean of class 1: A = (a1+a2+a3)/3B = (b1+b2+b3)/3C = (c1+c2+c3)/3mean of class1 = (A+B+C)/3I am confused at this point, kindly help me to solve this?Clarification update: I am trying to implement image recognition using LDA, my class1 matrix is of size 10*32256 of 10 sample images. Like this I have 5 classes. I was confused how to take mean for this matrix: whether to add all the instance of row 1 and divide by 32256 or add the column and divide by 10.","Creater_id":42803,"Start_date":"2014-03-30 19:59:16","Question_id":91955,"Tags":["classification","pattern-recognition","discriminant-analysis"],"Answer_count":1,"Last_activity":"2016-08-02 08:49:45","Link":"http://stats.stackexchange.com/questions/91955/confused-about-within-class-scatter-matrix-in-linear-discriminant-analysis","Creator_reputation":1}
{"_id":{"$oid":"5837a587a05283111e4d6759"},"View_count":18,"Display_name":"Sean Lin","Question_score":1,"Question_content":"I encounter an issue when trying to calculate sample size for a two-sample t test, which may due to very large mean and very small variance of the data. Suppose I have some control group data simulated as below:set.seed(1)x = rnorm(10, 1000, 1)And from prior knowledge, treatment group has a 10% larger mean than control group. Significant level is 5%, power is 80%. Per my understanding, effect size can be calculated as D = mean(x)*10%/sd(x)# 1281.258Clearly, this effect size is too large, and if I use package 'pwr' to sample size npwr.t.test(d = D, power = 0.8, sig.level = 0.05, type = 'two.sample')Function returns error due to this large effect size. So, in such a situation, is there a way to calculate sample size, maybe by transforming data or something else; or do I even need to calculate sample size since 10% of mean is quite large to variance, and people can easily tell which group the data come from?","Creater_id":95435,"Start_date":"2016-08-02 08:30:49","Question_id":226899,"Tags":["sample-size","power-analysis"],"Answer_count":0,"Last_activity":"2016-08-02 08:30:49","Link":"http://stats.stackexchange.com/questions/226899/calculate-sample-size-when-data-has-large-mean-and-small-variance","Creator_reputation":6}
{"_id":{"$oid":"5837a587a05283111e4d675b"},"View_count":80,"Display_name":"Minaj","Question_score":0,"Question_content":"Does linear discriminant analysis always project the points to a line? Most of the graphical illustrations of LDA that I see online  use an example of 2 dimensional points which are projected onto a straight line y=mx+c. If the points were each a 10-dimensional vector, does LDA still project them to a line?Or would it project them to a hyperplane with 9 dimensions or less.ANother question about projections: If I have a vector Y=[a,b,c,d]. The projection of this vector onto a given line is the product of the direction vector V of the line and the vector Y. This is equivalent to a dot product given by transpose(V).Y, and gives just one number (a scalar).This seems to be the way how LDA works. So, if I may ask, does LDA map a full n-dimensional vector onto a scalar (a singe number)?Apologies in advance for my newbie question.","Creater_id":86231,"Start_date":"2015-08-18 08:00:04","Question_id":167690,"Tags":["projection"],"Answer_count":2,"Last_activity":"2016-08-02 08:19:54","Link":"http://stats.stackexchange.com/questions/167690/linear-discriminant-analysis-newbie-question","Creator_reputation":1}
{"_id":{"$oid":"5837a587a05283111e4d6763"},"View_count":31,"Display_name":"Nuno Gil Fonseca","Question_score":1,"Question_content":"For a research project that I am working on, I need to predict the probability of the occurrence of a certain event.To be more precise, I need to predict the probability of a student submitting any solution to a certain assignment (Y) or not (N).The only thing that I have to help me is data from previous years in this form: Student 1 – Y Y Y Y N Y Y YStudent 2 – N N N Y Y Y Y N … Each line represents one student and the columns represent the ordered list of assignments (Y- the student has submitted at leat one solution for that assignment), (N – the student hasn’t submitted any solution for that assignment)Given a certain sequence of events, I need to calculate the probability of the next event. For instance:If in the first three assignments the student had one of the following behavior, what will happen for assignment number 4?Y N Y ?N N N ?Y Y Y ?…I’ve already calculated the probabilities based on the overall occurrence of a Y or N for a given assignment. I’ve also calculated the probability of the occurrence of a Y after an N, and a Y after a Y, and so on… I’ve also developed a method to determine the probability using a Markov Chain... However, I am not sure which method is the most \"scientifically fitted\" for this kind of problem…Any suggestions will be very welcome.Tks.Nuno.","Creater_id":125285,"Start_date":"2016-08-01 14:28:03","Question_id":226761,"Tags":["probability","stochastic-processes","markov-process"],"Answer_count":0,"Last_activity":"2016-08-02 08:17:47","Link":"http://stats.stackexchange.com/questions/226761/predicting-the-probability-of-the-occurence-of-an-event","Creator_reputation":6}
{"_id":{"$oid":"5837a587a05283111e4d6765"},"View_count":332,"Display_name":"Anton","Question_score":8,"Question_content":"Can someone help me find a way to estimate the variance of the Kaplan-Meier estimate with dependent observations? Specifically, I have failure time data from patients with several different observations for each patient (and different patients may have different number of observations). The observations for different patients are assumed to be independent but observations from the same patient are expected to be dependent.I was suggested two publications, \"Kaplan-Meier Analysis of Dental Implant Survival: A Strategy for Estimating Survival with Clustered Observations\" and \"The Kaplan-Meier Estimate for Dependent Failure Time Observations\", the second of which is cited by the first.However I was unable to make sense of these. The first has errors and the second seems far more rigorous but the equation for the variance does not make sense (double integration on a 1-form).","Creater_id":5031,"Start_date":"2011-12-21 08:16:48","Question_id":20114,"Tags":["variance","survival","non-independent"],"Answer_count":0,"Last_activity":"2016-08-02 08:14:53","Link":"http://stats.stackexchange.com/questions/20114/variance-of-the-kaplan-meier-estimate-for-dependent-observations","Creator_reputation":163}
{"_id":{"$oid":"5837a587a05283111e4d6767"},"View_count":31,"Display_name":"Biiniza Perez Ni\u0026#241;o","Question_score":0,"Question_content":"I am interested in knowing how different climate variables affect the abundance in bees in two different habitats (meadows and buildings). I had 10 transects in meadows areas and 10 in buildings areas. I walked each transect 15 times. Response variable: AbundanceFixed factors: Air temperature, SunshineRandom factors: Transect nested in habitat, times (1 to 15)For example, using R, I fitted this model:library (lme4)model \u0026lt;- glmer(Abundance ~ Airtemp * Sunshine + (1|Habitat/transect) + (1|Times),               family = poisson, data = bees)Is this a good way to address my research question ?","Creater_id":125366,"Start_date":"2016-08-02 06:47:02","Question_id":226875,"Tags":["r","lme4","glmm"],"Answer_count":1,"Last_activity":"2016-08-02 08:12:17","Link":"http://stats.stackexchange.com/questions/226875/how-to-model-repeated-measures","Creator_reputation":3}
{"_id":{"$oid":"5837a587a05283111e4d6773"},"View_count":4344,"Display_name":"Matt Kemp","Question_score":5,"Question_content":"I'm calculating the covariance of a distribution in parallel and I need to combine the distributed results into on singular Gaussian. How do I combine the two?Linearly interpolating between the two almost works, if they are similarly distributed and sized.Wikipedia provides a forumla at the bottom for combination but it doesn't seem right; two identically distributed distributions should have the same covariance, but the formula at the bottom of the page doubles the covariance.Is there a way to combine two matrices?","Creater_id":21687,"Start_date":"2013-03-07 11:24:57","Question_id":51622,"Tags":["covariance","moments"],"Answer_count":1,"Last_activity":"2016-08-02 08:08:32","Link":"http://stats.stackexchange.com/questions/51622/combining-two-covariance-matrices","Creator_reputation":128}
{"_id":{"$oid":"5837a587a05283111e4d6780"},"View_count":15,"Display_name":"Sandro","Question_score":0,"Question_content":"We are trying to throw up warnings for when the current number of sales () in a product group is \"significantly\" lower than the number of sales in a reference timeframe (). We believe that this process satisfies the condition for assuming a Poisson distribution (which we estimate by a normal distribution). Mind you, by number of sales we really mean number of invoices on which products of a product group show up, not number of purchased items.The idea is that when the likelihood for the current number of sales (or worse) in the current timeframe is lower than x%, we throw a warning and somebody should take a look at this, i.e.  (the number of sales hasn't changed) is rejected when  (I hope I am getting the formulas right here)Now all this seems to work fine for reasonably small  (can't put a number on that though). If  gets too big though, what seems reasonable fluctuation is suddenly flagged as a seemingly false negative. Say  and , the likelihood of that is returned as way less then , even though it seems to be just a small fluctuation.The standard deviation for the normal distribution is assumed to be  and I am pretty sure, that the culprit is here. Questions:Am I right in thinking that the standard deviation is the problem here? Is there any way to adjust it along with  to negate this effect?Thanks!","Creater_id":62179,"Start_date":"2016-08-02 07:53:29","Question_id":226886,"Tags":["normal-distribution","standard-deviation","poisson"],"Answer_count":0,"Last_activity":"2016-08-02 07:53:29","Link":"http://stats.stackexchange.com/questions/226886/finding-negative-trends-in-product-groups-by-number-of-sales","Creator_reputation":23}
{"_id":{"$oid":"5837a587a05283111e4d6782"},"View_count":26,"Display_name":"Renthal","Question_score":1,"Question_content":"After reading: What is the correct formula to between-class scatter matrix in LDA? I've been bugged with the balanced version of .The original whithin-class scatter matrix is:\\begin{equation}W=\\sum_i^k\\sum_j^{N_i}(x_{ij}−\\overline{x}_i)(x_{ij}−\\overline{x}_i)^T \\end{equation}And the proposed balanced formula found in the answer there is:\\begin{equation}W'= \\overline{n} \\sum_i^k \\frac{1}{N_i} \\sum_j^{N_i}(x_{ij}−\\overline{x}_i)(x_{ij}−\\overline{x}_i)^T \\end{equation}where  is the mean number of points per class. However, here, despite the formula proposed in the slide is the original (unbalanced) version, in the numerical example the following is used:\\begin{equation}W''= \\sum_i^k \\frac{1}{N_i} \\sum_j^{N_i}(x_{ij}−\\overline{x}_i)(x_{ij}−\\overline{x}_i)^T \\end{equation}So to me is now not clear: is the term  in front of the formula necessary? ","Creater_id":125326,"Start_date":"2016-08-02 01:04:09","Question_id":226817,"Tags":["covariance-matrix","discriminant-analysis"],"Answer_count":0,"Last_activity":"2016-08-02 07:49:11","Link":"http://stats.stackexchange.com/questions/226817/correct-formula-for-balanced-within-class-scatter-matrix-in-lda","Creator_reputation":51}
{"_id":{"$oid":"5837a587a05283111e4d6784"},"View_count":62,"Display_name":"JaneBirkin","Question_score":0,"Question_content":"I am trying to test for differences between to treatments within the same individuals, where the data I am dealing with is qualitative (yes=1 or no=0).Am I correct to use a McNemar's test for it, and does my code make sense or am I forgetting something to make sure that I am testing for a paired design?How do I make sure the individual is kept in consideration? That's what my table looks like table \u0026lt;- data.frame (treatment, songmatching)            songmatching treatment  0      1consistent 22    23  control    28     7mcnemar.test(table(treatment, songmatching), correct = TRUE)McNemar's Chi-squared test with continuity correctiondata:  table(treatment, songmatching)McNemar's chi-squared = 4.7805, df = 1, p-value = 0.02878Is this correct?Thanks for your help! Edit: I'm adding this here 'cause I don't know how to insert a table in the commentsEach individual has two rows (one for each treatment), but I don't think the table really shows the individual, as so far it looks like this:   treatment   songmatching1  control         02  consistent      13  consistent      14  control         05  control         06  consistent      0...and so on for a total of 70 cases for 35 individuals. Do I need to insert the individual variable in my table? And if yes, how do I proceed with a McNemar's Chi-squared test?","Creater_id":125362,"Start_date":"2016-08-02 06:15:52","Question_id":226869,"Tags":["r","binomial","paired-data","mcnemar-test"],"Answer_count":0,"Last_activity":"2016-08-02 07:46:27","Link":"http://stats.stackexchange.com/questions/226869/mcnemar-test-for-paired-binomial-data","Creator_reputation":4}
{"_id":{"$oid":"5837a587a05283111e4d6786"},"View_count":12,"Display_name":"LeonDK","Question_score":0,"Question_content":"Working with biostatistics, I have a system of 10 matrices (experiments), each with the same 300 rows (samples) and the same 20 columns (variables), but with different real values in each matrix. The rows of each matrix are labelled either A or B consistently across all matrices. Furthermore I have heavy heteroscedasticity, such that lower values have much higher uncertainty and there is also multicollinearity between the rows of each matrix. Lastly I have missing values and zero-inflation.I am interested in investigating whether groups A and B can be significantly distinguished usingAll matrices at onceIndividual matrices and which matrices are betterHow do I do this given the described system?","Creater_id":71268,"Start_date":"2016-08-02 06:30:31","Question_id":226872,"Tags":["regression","classification","multivariate-analysis","heteroscedasticity","multicollinearity"],"Answer_count":0,"Last_activity":"2016-08-02 07:45:40","Link":"http://stats.stackexchange.com/questions/226872/how-to-approach-two-group-multidimensional-dataset-with-heteroscedasticity-and-i","Creator_reputation":11}
{"_id":{"$oid":"5837a587a05283111e4d6788"},"View_count":87,"Display_name":"Llopis","Question_score":1,"Question_content":"Suppose I have all the necessary technical skills to analyze a large data set and I perform analysis to I obtain results following the answer on a related question.Now I am overwhelmed by the task of interpreting all the results. Which skills and steps should I have/follow to get the picture of a large scale analysis?As a concrete example I have analyzed 46991 genes using the procedures on WGCNA, a correlation network tool, which resulted on 39 modules of genes plus the non-assigned genes in another module using as power 12 and minimum module size of 30 and maximum module size of 4000 and a cutoff of 0.25 which later is modifyied by merging those which are close a 0.25 of a distance tree. I correlated (pearson correlation) the modules with 25 clinical variables and then selecting just 27 modules as relevant in terms of Pearson correlation coefficient and p-value (r \u003e= 0.3 p.value \u0026lt;= 0.05), where some are correlated with more than one clinical variable. For each selected module I performed an enrichment (looking which with fisher exact test which ontologies are enriched on those modules) with genes ontologies and pathways from reactome and KEGG to know the biological information behind these genes.How can I use these information to answer my question under study \"What happens in the the cells in the conditions studied?\"? Do I need more information/test?","Creater_id":105234,"Start_date":"2016-07-26 03:51:09","Question_id":225677,"Tags":["regression","multiple-regression","multivariate-analysis","large-data"],"Answer_count":0,"Last_activity":"2016-08-02 07:22:16","Link":"http://stats.stackexchange.com/questions/225677/what-skills-are-required-to-interpret-the-result-of-large-scale-statistical-anal","Creator_reputation":155}
{"_id":{"$oid":"5837a587a05283111e4d678a"},"View_count":171,"Display_name":"Joe Ricci","Question_score":6,"Question_content":"I'm having trouble understanding this question from my statistics class:  I handed out a flyer about a rock show to 20 people who, at that time, did not plan on attending the show. My hypothesis is that 5 or more people are now going to the rock show. I decide to follow up with 8 of them (chosen randomly) to see if they went to the show. I find that 2 of those 8 people went to the show.  What is the p-value of the outcome of my experiment?What type of statistical test is necessary to find the p-value?","Creater_id":124215,"Start_date":"2016-07-22 15:08:49","Question_id":225203,"Tags":["hypothesis-testing","self-study","statistical-significance","p-value"],"Answer_count":2,"Last_activity":"2016-08-02 07:09:30","Link":"http://stats.stackexchange.com/questions/225203/computing-p-value-in-a-problem-about-people-going-to-a-show-with-certain-probabi","Creator_reputation":31}
{"_id":{"$oid":"5837a587a05283111e4d6798"},"View_count":25,"Display_name":"Omar Shehab","Question_score":1,"Question_content":"I have the following analytic function.My goal is to create  pairs from this function where  and use nonlinear regression to fit a curve to the data. This is for mere educational purpose.Using TensorFlow I have created a neural network with one hidden layer and used it to fit a curve to the dataset generated from the analytic function. My results are as follows.In the legend ,  = number of training steps,  = number of nodes in the hidden layer, and  = RMSE.My question:Why a particular  works the best among the all?","Creater_id":27686,"Start_date":"2016-08-02 06:54:12","Question_id":226877,"Tags":["regression","logistic","nonlinear-regression","logit","curve-fitting"],"Answer_count":0,"Last_activity":"2016-08-02 06:54:12","Link":"http://stats.stackexchange.com/questions/226877/nonlinear-regression-for-curve-fitting-of-a-dataset-generated-by-an-analytic-fun","Creator_reputation":140}
{"_id":{"$oid":"5837a587a05283111e4d679a"},"View_count":17,"Display_name":"Manu Andrei","Question_score":1,"Question_content":"So far, I found an article about this: Question answering SystemAs it can be seen on page 105 (section 3.1) and the graph in section 3.2, the questions can be classified depending on which words come after each why/what/where.Is there any model that you know about that is complete for describing questions? (i.e. how long is for length, how old is for age, etc)I tried searching for English Language Model Ontology but so far I didn't get too far. My problem is, probably, that I don't know the name of the term I'm trying to find.Thank you in advance!","Creater_id":123392,"Start_date":"2016-08-02 06:51:32","Question_id":226876,"Tags":["natural-language"],"Answer_count":0,"Last_activity":"2016-08-02 06:51:32","Link":"http://stats.stackexchange.com/questions/226876/identifying-question-type-natural-language","Creator_reputation":6}
{"_id":{"$oid":"5837a587a05283111e4d679c"},"View_count":92,"Display_name":"hirschme","Question_score":1,"Question_content":"I have a multiple linear model for time series data for which the regression residuals are autocorrelated and display seasonal behavior. This seasonal behavior is induced deterministically by a cyclic variable written into the model. In order to calculate corrected standard errors for the regression coefficients, I intend to use generalized least squares with correction for the autocorrelation in the residuals.But for the seasonality, I am not sure for which time series I should perform the differencing: If for raw original data (observations), or for the regression residuals. For the data, I could superimpose each cycle and take the mean, thus removing season effects, but this would alter the data, and also would generalize bad for other kinds of data. For the regression residuals, I dont know if I have to difference the residuals time series substracting for each period, or to handle it as some kind of seasonal ARMA process. Any hints?LAST EDIT: The problem behind this question might have been resolved already, and might have been product of a misconception. Regression was being done over simulations of the model. These simulations did not contain any stochastic error factor, so the simulations were purely of a deterministic nature. Regression errors were just showing an unperfect fit to the data and the regression residuals were obviously following a deterministic pattern. This pattern was not a result of some neglected explanatory variable in the model, and thus there was no reason to model it (by means of an ARMA model or any kind). When adding white noise to the data -which should be a crucial step on any simulation of real data- regression residuals were mostly dominated by stochasticity, loosing the autocorrelated behavior. ","Creater_id":96731,"Start_date":"2016-07-28 08:03:42","Question_id":226120,"Tags":["regression","time-series","seasonality","arma","generalized-least-squares"],"Answer_count":1,"Last_activity":"2016-08-02 06:36:34","Link":"http://stats.stackexchange.com/questions/226120/seasonal-and-autocorrelated-regression-residuals-difference-raw-data-or-residua","Creator_reputation":64}
{"_id":{"$oid":"5837a587a05283111e4d67a9"},"View_count":17,"Display_name":"cdeterman","Question_score":1,"Question_content":"I am currently trying to implement Nesterov Accelerated Gradient (NAG) in a neural network following the description shown here.My understanding is that it is identical to 'Classical' momentum (CM), also shown in the link above, except that the gradients are calculated on weights += mu * v.  That seeming simple enough, I implemented this and to my surprise it seems that NAG doesn't always outperform CM (typically only a small improvement if any).  In fact, I am often seeing CM outperform NAG!  This seems wrong to me.Is this expected behavior?  I have looked at multiple other questions here on NAG and I'm pretty sure my code is sound.Here is the relevant code (I know my calculate_gradients function works correctly, among other code excluded).  The part I don't know for sure is everything outside the gradient calculations in the while loop.  Have I perhaps missed something?# use_nesterov = TRUE# vt.old = 0# mu = 0.9while (epoch \u0026lt; stepmax){    # update function    vt = (mu * vt.old) - (learningrate * gradients)    weights += vt    vt.old = vt    # other non-relevant code    ...    # calculate gradients    gradients = calculate_gradients(                    if (use_nesterov) {weights + mu*vt.old} else {weights},                     ...)}","Creater_id":37428,"Start_date":"2016-08-02 06:01:53","Question_id":226866,"Tags":["neural-networks","optimization","gradient-descent"],"Answer_count":0,"Last_activity":"2016-08-02 06:22:48","Link":"http://stats.stackexchange.com/questions/226866/is-nag-always-better-than-classical-momentum","Creator_reputation":1588}
{"_id":{"$oid":"5837a587a05283111e4d67ab"},"View_count":172,"Display_name":"user83655","Question_score":1,"Question_content":"I am newbie in machine learning. I have been studying about features extraction and some classification approaches, in the term of my study, I have a question in my mind, what the reasons we need to extract a lot of features for classification? is it possible to extract one reliable feature from a dataset for good classification results?","Creater_id":83655,"Start_date":"2015-07-29 23:30:20","Question_id":163885,"Tags":["machine-learning","feature-construction"],"Answer_count":3,"Last_activity":"2016-08-02 05:53:23","Link":"http://stats.stackexchange.com/questions/163885/why-we-need-to-extract-a-lot-of-features-from-a-dataset-for-classification","Creator_reputation":108}
{"_id":{"$oid":"5837a587a05283111e4d67ba"},"View_count":57,"Display_name":"user3639557","Question_score":0,"Question_content":"I am puzzled by how this Gibbs sampler on section 6 of Escobar \u0026amp; West (1995) works. To put it in simple words, the aim is to sample . The defined terms are: \\eta\\sim \\texttt{Beta}(a,b) and \\alpha \\sim \\pi \\texttt{Gamma}(\\theta,f(\\eta))+(1-\\pi)\\texttt{Gamma}(\\theta-1,f(\\eta)) the paper says (with a bit of simplification)  It is now clear how  can be sampled at each stage of the  simulation. At each Gibbs iteration, we first sample  from the  defined Beta distribution, and use the sampled  and the fixed   to sample  from the mixture of the Gamma  distributions.the confusing bit is,  On completion of the simulation  will be  estimated by the usual Monte Carlo averaging  ,  where  are the sampled values of .Knowing that the aim in here was to sample , why do we need to estimate ? We already have a sample for , so what is the need to estimate its probability. Also not sure why can we plug in all the sampled values of  in this estimation, shouldn't one just use the sampled  based on which we sampled the corresponding ?My only explanation: Given all the sampled  (let's put them in a set ) for each sampled , we need to compute it's posterior . For this, we use all the sampled values for  from all the Gibbs iterations to compute the summation. This way each sampled  will get a Monte Carlo averaged posterior estimate. Using the accumulation of all these posterior estimates based on which we sample an  using accumulated posterior estimates of all sampled  in . Is this the correct explanation?Escobar, M. D., \u0026amp; West, M. (1995). Bayesian density estimation and inference using mixtures. Journal of the american statistical association, 90(430), 577-588.","Creater_id":56676,"Start_date":"2016-08-02 02:37:12","Question_id":226832,"Tags":["bayesian","sampling","hierarchical-bayesian","gibbs","dirichlet-process"],"Answer_count":1,"Last_activity":"2016-08-02 05:50:43","Link":"http://stats.stackexchange.com/questions/226832/how-does-this-sampler-work-for-the-concentration-parameter-of-dirichlet-process","Creator_reputation":310}
{"_id":{"$oid":"5837a587a05283111e4d67c7"},"View_count":53,"Display_name":"Waqas","Question_score":0,"Question_content":"I have the following piece of code:library(forecast)set.seed(1234)y \u0026lt;- ts(sort(rnorm(30)), start = 1978, frequency = 1) # annual datafcasts \u0026lt;- numeric(10)for (i in 1:10) { # start rolling forecast  # start from 1997, every time one more year included  win.y \u0026lt;- window(y, end = 1996 + i)   fit \u0026lt;- auto.arima(win.y)  fcasts[i] \u0026lt;- forecast(fit, h = 1)$mean}train \u0026lt;- window(y,end=1997)fit\u0026lt;- auto.arima(train)refit \u0026lt;- Arima(y, model=fit)fc \u0026lt;- window(fitted(refit), start=1998)I thought both should give same results, but why fcasts and fc give different results?","Creater_id":119117,"Start_date":"2016-08-01 07:27:30","Question_id":226685,"Tags":["r","time-series"],"Answer_count":2,"Last_activity":"2016-08-02 05:37:17","Link":"http://stats.stackexchange.com/questions/226685/difference-in-forecast-and-fitted-method-in-r","Creator_reputation":37}
{"_id":{"$oid":"5837a587a05283111e4d67d5"},"View_count":23,"Display_name":"Speldosa","Question_score":1,"Question_content":"In this thread, it is proposed that if you don't know what the variance-covariance matrix looks like for a dataset that you want to run a multi-level meta-analysis on, you can, among other things, \"make a rough/educated guess how large the correlations are\". I have a dataset with a huge amount of studies, with a nested structure where there are several samples for each study and several effect sizes for each sample. It would be virtually impossible for me to estimate reasonable intra-sample and intra-article corraltions for each single study an/or type of test, so I will basically have to settle on one intra-sample correlation and one intra-article correlation that is used across the board.Now, I'm trying to figure out how one could go about figuring out suitable correlation values that could at least somewhat be argued for. Right now, I've settled on using a correlation for intra-sample effect sizes that explains 50% of the variation and a correlation for intra-article effect sizes that explains 25% of the variation, but these values are basically only selected because they are nice round numbers, and because they feel quite conservative (the probability that I'm underestimating the correlation seems low). However, in reality, I have no idea what reasonable values would be for my field (psychology).What are some good/smart/ingenious ways going about trying to find out these correlation values? Are there other who have solved/tackled this problem before?","Creater_id":3812,"Start_date":"2016-08-02 05:20:49","Question_id":226862,"Tags":["estimation","meta-analysis","covariance-matrix"],"Answer_count":0,"Last_activity":"2016-08-02 05:20:49","Link":"http://stats.stackexchange.com/questions/226862/how-could-one-estimate-global-intra-sample-and-intra-article-correlations-for-a","Creator_reputation":206}
{"_id":{"$oid":"5837a587a05283111e4d67d7"},"View_count":61,"Display_name":"Richard Hardy","Question_score":1,"Question_content":"Consider the following objective function: \\mathbb{E}((Y-X\\beta)^2)\\rightarrow \\min_\\beta where  and  are (generally not independent) random variables and  is a constant. That is, I am looking for a constant  that would yield the smallest discrepancy in terms of mean squared error between a random variable  and a scaled random variable . (I could have put  in front of  but that does not change anything.) In search of an optimal , I take the derivative of the objective function with respect to  and equate it to zero: -2\\mathbb{E}(XY)+2\\beta\\mathbb{E}(X^2)=0. Then \\beta^*=\\frac{\\mathbb{E}(XY)}{\\mathbb{E}(X^2)} where I have put an asterisk at  to denote an \"optimal\" value.This is all fine as long as I only care about these random variables in theory. However, at this stage I would like to get an operational (feasible) counterpart of  based on a data sample. That is, I need an estimator  such that \\hat\\beta^*=\\arg\\min_\\hat\\beta \\mathbb{E}((Y-X\\hat\\beta)^2) where  is a data sample (an  matrix).Q1: Is there any good solution to this problem?I intentionally did not make more assumptions (did not specify the joint distribution of  and  nor their marginal distributions, did not restrict  to be linear), but I might accept some if the general setting is too general to work with. For example, you may take on an extra assumption that  is linear in  if that makes the problem easier.Q2: What about the case of  that includes a constant  extra to the above setting?","Creater_id":53690,"Start_date":"2016-02-04 11:25:12","Question_id":194063,"Tags":["regression","random-variable","estimators","mse"],"Answer_count":0,"Last_activity":"2016-08-02 05:10:58","Link":"http://stats.stackexchange.com/questions/194063/minimum-mse-linear-combination-of-random-variables","Creator_reputation":13091}
{"_id":{"$oid":"5837a587a05283111e4d67d9"},"View_count":177,"Display_name":"user3639557","Question_score":1,"Question_content":"Assuming that all the mixture parameters are known, how can one sample from a mixture of  distributions: \\theta \\sim \\pi \\texttt{Gamma}(\\alpha_1,\\beta_1)+(1-\\pi)\\texttt{Gamma}(\\alpha_2,\\beta_2) Is it done by sampling a whole bunch of s from either of the distributions and putting them in a pool and at the end computing their corresponding probability according to the mixture, and then sampling proportional to the accumulated computed probabilities?","Creater_id":56676,"Start_date":"2016-08-02 02:47:32","Question_id":226834,"Tags":["random-generation","gamma-distribution","mixture"],"Answer_count":1,"Last_activity":"2016-08-02 05:05:35","Link":"http://stats.stackexchange.com/questions/226834/sampling-from-a-mixture-of-two-gamma-distributions","Creator_reputation":310}
{"_id":{"$oid":"5837a587a05283111e4d67e6"},"View_count":31,"Display_name":"LearningR","Question_score":0,"Question_content":"How can I find the function 'petest' in R? A little background, I want to compare two different #regressions, y=x1+x2logy=logx1+logx2It is clear to me that I cannot use the BIC or AIC values, because the outcome variable is different. I searched in verbeek 2008 (A guide to modern Econometrics) and he suggests the PE test to compare linear and loglinear models.I searched on this website http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html and there I could find that the PE test exist in R, however, I cannot find it. I have the package lmtest, and 'petest' is supposed to be there, but when I try to use it, it says: Error: could not find function \"petest\".Also, I have looked for more explanation, like examples, or videos that show how to apply this PE model, but I did not find any. Any help on this topic is highly appreciated!","Creater_id":125350,"Start_date":"2016-08-02 04:24:17","Question_id":226849,"Tags":["r","regression"],"Answer_count":1,"Last_activity":"2016-08-02 04:38:09","Link":"http://stats.stackexchange.com/questions/226849/petest-function-in-r","Creator_reputation":1}
{"_id":{"$oid":"5837a587a05283111e4d67f3"},"View_count":73,"Display_name":"robert","Question_score":1,"Question_content":"Assume I have a chart below how a certain property is distributed (in this case number of cars over 0-100%) over a population:How is such a chart called? Percentile chart? What are the keywords to search for if I want to know more about such kind of charts?How are such charts typically drawn? Like a 'stair' in the example below?How do I write that the first 90% has 2 or less cars? E.g. \"The lowest 90% percentile has 2 or less cars?\"","Creater_id":74274,"Start_date":"2016-07-22 02:01:28","Question_id":225076,"Tags":["distributions","data-visualization","quantiles"],"Answer_count":2,"Last_activity":"2016-08-02 04:28:20","Link":"http://stats.stackexchange.com/questions/225076/how-is-this-chart-called-percentile-chart","Creator_reputation":178}
{"_id":{"$oid":"5837a587a05283111e4d6801"},"View_count":7,"Display_name":"jeffrey","Question_score":0,"Question_content":"I have two empirical studies. Both report the results for a probit model with the same dependent variable and some independent variables are the same, but not all of them. The first study reports the p-values for a z-test, whereas the second study reports the p-values for a ChiSq test. My question is: Can I directly compare these results? I.e., is it correct to say that a variable that is significant at the 5% level in study 1 has approximately the same meaning as if a variable in the second study is significant at 5%?Thanks a lot for your help!","Creater_id":60999,"Start_date":"2016-08-02 04:27:17","Question_id":226851,"Tags":["chi-squared","probit","z-test"],"Answer_count":0,"Last_activity":"2016-08-02 04:27:17","Link":"http://stats.stackexchange.com/questions/226851/compare-results-from-probit-with-p-values-for-chi-test-with-p-values-for-z-test","Creator_reputation":258}
{"_id":{"$oid":"5837a587a05283111e4d6803"},"View_count":41,"Display_name":"GreenThor","Question_score":0,"Question_content":"So I have a list of actual values and two models to predict these values. Evaluating the predictions gives me a relative (absolute) error of 50% percent for the first and 25% for the seconds.I'm pretty sure I can say that (for these actual values) the second model is 25 percent points better than the first. (If not, please correct me.)But can I also say that the second model is 50% better that the first? Because 25 percent points of 50 percent points are 50%?","Creater_id":119024,"Start_date":"2016-07-20 03:56:42","Question_id":224693,"Tags":["mathematical-statistics","estimation","definition"],"Answer_count":0,"Last_activity":"2016-08-02 03:30:46","Link":"http://stats.stackexchange.com/questions/224693/comparing-relative-error-measures","Creator_reputation":18}
{"_id":{"$oid":"5837a587a05283111e4d6805"},"View_count":89,"Display_name":"user3195078","Question_score":3,"Question_content":"I'm currently implementing a recommender system using implicit ratings (time spent on an article) but I'm wondering what would be the proper metric to assess the system. MSE doesn't seems to fit to this usecase.My algorithm is based on the paper : Collaborative Filtering for Implicit Feedback Datasets","Creater_id":125329,"Start_date":"2016-08-02 02:02:51","Question_id":226825,"Tags":["recommender-system","metric","model-evaluation","rating","als"],"Answer_count":1,"Last_activity":"2016-08-02 03:30:25","Link":"http://stats.stackexchange.com/questions/226825/what-metric-should-i-use-for-assessing-implicit-matrix-factorization-recommender","Creator_reputation":28}
{"_id":{"$oid":"5837a587a05283111e4d6812"},"View_count":63,"Display_name":"sponge_knight","Question_score":1,"Question_content":"I'm wondering: how do I select the number of previous time steps to use to predict the current one?I'm just plotting the autocorrelation plot and picking previous time steps that have statistically significant correlation with the present.Is this a proper way of doing this? I'm basically just wondering how many time steps should I include.","Creater_id":46925,"Start_date":"2016-07-30 10:27:12","Question_id":226468,"Tags":["time-series","forecasting","feature-selection","autoregressive","lags"],"Answer_count":1,"Last_activity":"2016-08-02 03:22:13","Link":"http://stats.stackexchange.com/questions/226468/how-many-lags-should-i-include-in-time-series-prediction","Creator_reputation":1522}
{"_id":{"$oid":"5837a587a05283111e4d681e"},"View_count":63,"Display_name":"tiantianchen","Question_score":2,"Question_content":"I am doing a multiple linear regression where the predictors are two categorical variables: Time (3 levels) and Treatment (3 levels). The lm() results are:Coefficients:                Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)      -1.6227     0.2627  -6.176 1.14e-06 ***TimeT2           -0.3453     0.3941  -0.876 0.388383    TimeT3           -0.9509     0.3941  -2.413 0.022628 *  TreatmentTR2      1.9081     0.4736   4.029 0.000389 ***TreatmentTR3      1.9485     0.3941   4.944 3.23e-05 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.7882 on 28 degrees of freedomMultiple R-squared:  0.5163,    Adjusted R-squared:  0.4472 F-statistic: 7.471 on 4 and 28 DF,  p-value: 0.0003159In the table the t-test p-values are calculated using the estimated t-value with t-distribution of df=n-k-1. In my data, the total number of observation n=33, number of parameters k=4. So all the t-tests above use the identical df=28.However, if I just think about the t-test for the purpose of comparing the mean of these two groups after adjusting other covariates, for instance TimeT2 vs TimeT1(ref). I would imagine that the observations from T3 are not relevant for this t-test, thus, the df would only be calculated by the number of observations from T1 and T2 and k=1.Think it in another way, if my purpose is to compare pairwise means of multiple groups and I have three groups with large difference in n (n_T1=10, n_T2=10, n_T3=200), conducting a pairwise t-test between observations from T1 and T2 only would end up with much lower df, as compared to looking at the t-test result in the lm output. In the separated t-test, we end up with df=18, while lm gives df=217. Isn't the lm t-test deflates the p-value?Thanks","Creater_id":13702,"Start_date":"2016-07-29 01:14:56","Question_id":226237,"Tags":["regression","t-test","degrees-of-freedom","lm"],"Answer_count":1,"Last_activity":"2016-08-02 03:18:43","Link":"http://stats.stackexchange.com/questions/226237/how-to-explain-the-degrees-of-freedom-of-the-t-test-in-lm-output","Creator_reputation":610}
{"_id":{"$oid":"5837a587a05283111e4d682b"},"View_count":19,"Display_name":"Lope Lh","Question_score":0,"Question_content":"I have found the following statement on the internet concerning the necessary number of samples in the central limit theorem:  In practice, some statisticians say that a sample size of 30 is large  enough when the population distribution is roughly bell-shaped. Others  recommend a sample size of at least 40. But if the original population  is distinctly not normal (e.g., is badly skewed, has multiple peaks,  and/or has outliers), researchers like the sample size to be even  larger. [1]I need some references that prove this claim (i.e. scientific papers that actually recomend this sample size of 30 or 40). ¿Do you known any of them?[1] http://stattrek.com/sampling/sampling-distribution.aspx","Creater_id":96216,"Start_date":"2016-08-02 03:12:32","Question_id":226840,"Tags":["distributions","sampling","random-variable","central-limit-theorem"],"Answer_count":0,"Last_activity":"2016-08-02 03:12:32","Link":"http://stats.stackexchange.com/questions/226840/central-limit-theorem-heuristics","Creator_reputation":134}
{"_id":{"$oid":"5837a587a05283111e4d682d"},"View_count":151,"Display_name":"Anton","Question_score":0,"Question_content":"I'm trying to create a multilevel ordinal logistic regression model in Stan and the following code would seem  to work, in the sense that Stan seems to convergence to sensible answers:stanmodel \u0026lt;- 'data {  int\u0026lt;lower=2\u0026gt; K;  // ordinal response with 4 values, 3 cutpoints  int\u0026lt;lower=0\u0026gt; N;  // number of measurements  int\u0026lt;lower=1,upper=K\u0026gt; y[N]; // response  int\u0026lt;lower=0\u0026gt; Ntests;         // number of groups  int\u0026lt;lower=1,upper=Ntests\u0026gt; tests[N];  // groups}parameters {  // population cutpoints and associated variance.  ordered[K-1]  CutpointsMean;  real\u0026lt;lower=0\u0026gt; CutpointsSigma[K-1];     ordered[K-1]  Cutpoints[Ntests];   // ordinal response cutpoints for groups}model {  CutpointsSigma ~ exponential(1);  CutpointsMean  ~ normal(2, 3);  for (i in 1:Ntests) {    Cutpoints[i][1] ~ normal(CutpointsMean[1] , CutpointsSigma[1]);    Cutpoints[i][2] ~ normal(CutpointsMean[2] , CutpointsSigma[2]);    Cutpoints[i][3] ~ normal(CutpointsMean[3] , CutpointsSigma[3]);  }  for (i in 1:N)    y[i] ~ ordered_logistic(0, Cutpoints[tests[i]]);}'I have removed the part relating to the covariates for clarity.'CutpointsMean' and 'CutpointsSigma' define the population global ordinal response while Cutpoints[i][1:3] is the ordinal response for group i.The idea is that for example the first 'cutpoint' of each group is generated from a normal distribution centered on the first 'cutpoint' of the overall population. The second 'cutpoint' of each group is generated from a normal distribution centered on the second 'cutpoint' of the overall population and so on.As Cutpoints[i] is an ordered vector of 3 elements, what happens when I write directly into Cutpoints[i][2] ?Is the write operation rejected if the constraints are not satisfied or simply the entry is written in the vector and the results is sorted?Is this the correct way of modelling a multilevel ordinal response in Stan?","Creater_id":95411,"Start_date":"2016-07-28 02:51:11","Question_id":226070,"Tags":["regression","multilevel-analysis","ordinal","stan"],"Answer_count":1,"Last_activity":"2016-08-02 02:47:46","Link":"http://stats.stackexchange.com/questions/226070/stan-multilevel-ordinal-logistic-regression","Creator_reputation":87}
{"_id":{"$oid":"5837a587a05283111e4d6839"},"View_count":68,"Display_name":"Kartheek Palepu","Question_score":2,"Question_content":"Assume, I have a classifier (It could be any of the standard classifiers like decision tree, random forest, logistic regression .. etc.) for fraud detection using the below codelibrary(randomForest)rfFit = randomForest(Y ~ ., data = myData, ntree = 400) # A very basic classifier Say, Y is a binary outcome - Fraud/Not-FraudNow, I have predicted on a unseen data set.pred = predict(rfFit, newData)Then I have obtained the feedback from the investigation team on my classification and found that I have made a mistake of classifying a fraud as Non-Fraud (i.e. One False Negative). Is there anyway that I can let my algorithm understand that it has made a mistake? i.e. Any way of adding a feedback loop to the algorithm so that it can correct the mistakes?One option I can think from top of my head is build an adaboost classifier so that the new classifier corrects the mistake of the old one. or I have heard something of Incremental Learning or Online learning. Are there any existing implementations (packages) in R?Is it the right approach? or Is there any other way to tweak the model instead of building it from the scratch?","Creater_id":85734,"Start_date":"2016-08-02 02:35:50","Question_id":226831,"Tags":["classification","supervised-learning"],"Answer_count":0,"Last_activity":"2016-08-02 02:35:50","Link":"http://stats.stackexchange.com/questions/226831/incremental-learning-for-classification-models-in-r","Creator_reputation":135}
{"_id":{"$oid":"5837a587a05283111e4d683b"},"View_count":50,"Display_name":"Steve3nto","Question_score":1,"Question_content":"I have implemented NAG following this tutorialhttp://cs231n.github.io/neural-networks-3/#adaIt works, in fact with mu = 0.95 I get a good speed-up in learning compared to standard gradient descent, but I am not sure I implemented it correctly. I have a doubt about the gradient estimate I am using. This is the matlab code:% backup previous velocityvb_prev = nn.vb{l};vW_prev = nn.vW{l};% update velocity with adapted learning ratesnn.vb{l} = nn.mu*vb_prev - lrb.*nn.db{l};nn.vW{l} = nn.mu*vW_prev - lrW.*nn.dW{l};%update weights and biases nn.b{l} = nn.b{l} - nn.mu*vb_prev + (1 + nn.mu)*nn.vb{l};nn.W{l} = nn.W{l} - nn.mu*vW_prev + (1 + nn.mu)*nn.vW{l};which is exactly what is written in the tutorial. db and dW are my gradient estimates, but I am not sure if they are correct since I estimate them at W, not at W_ahead = W + mu*v. Should I change something before forward and back-propagation to have a different gradient estimate? Or is NAG correct like this?It works, but I would like to have a correct implementation of NAG.Any help is appreciated, thank you very much!","Creater_id":107336,"Start_date":"2016-08-02 02:26:33","Question_id":226829,"Tags":["neural-networks","optimization","deep-learning","gradient-descent"],"Answer_count":0,"Last_activity":"2016-08-02 02:26:33","Link":"http://stats.stackexchange.com/questions/226829/implementation-of-nesterovs-accelerate-gradient-for-neural-networks","Creator_reputation":95}
{"_id":{"$oid":"5837a587a05283111e4d683d"},"View_count":93,"Display_name":"Jack Pierce-Brown","Question_score":2,"Question_content":"I understand the discete case i.e. the sum of  identically distributed random variables  with variance . The correlation between these random variables is given by the correlation matrix .The variance of the linear combination of random variables  is given by:\\operatorname{Var}\\left( \\sum_{i=1}^N X_i\\right) = N\\sigma^2+2\\sigma^2\\sum_{1\\le i\u0026lt;j\\le N}\\mathbf{\\rho}(X_i,X_j)Source: Wikipedia - Variance - Sum of correlated variablesI would like to consider the continuous case of a stochastic process which will be denoted as . The process is stationary with constant variance  and correlation function . Similar to above I would like to calculate the variance of the linear combination of the random variables . I think that the linear combination over some domain  can be expressed asI = \\int_0^L X(t) dtI would like to know the variance of :\\operatorname{Var}(I)=?I speculate that if the process  is completely correlated i.e.  then the variance of  is minimised maximised and is given by:\\operatorname{Var}(I)=L^2\\sigma^2If the variables are uncorrelated i.e.  then I suspect that the variance of  is maximised minimised. It may be infinite zero?I find the continuous case (i.e. infinite random variables over some domain [0,L]) difficult to understand. Could anyone provide me an expression for the variance of ?","Creater_id":112088,"Start_date":"2016-08-01 03:48:25","Question_id":226657,"Tags":["variance","stochastic-processes","linear","integral"],"Answer_count":1,"Last_activity":"2016-08-02 02:11:25","Link":"http://stats.stackexchange.com/questions/226657/variance-of-the-integral-of-a-stochastic-process","Creator_reputation":91}
{"_id":{"$oid":"5837a587a05283111e4d684a"},"View_count":136,"Display_name":"Doug7","Question_score":0,"Question_content":"Is it right to use rfImpute to impute missing feature values on the whole data set and then use other regression/classification techniques on the new data set created? Or, is an rfImpute model intended to be fitted on a subset of the data and then that fitted model is somehow used to fill in missing values in the rest of the data?To be clear, I only bring up these questions because rfImpute seems to require arguments for both X (features) and y (target variable). In addition, y cannot have any missing values. Does this mean that y gets used for the imputation of the features? Wouldn't this be harmful later when trying to fit models to the new imputed data set? Obviously the y values we are trying to predict in the future won't be known, so how could rfImpute fit into a machine learning pipeline?Link:http://math.furman.edu/~dcs/courses/math47/R/library/randomForest/html/rfImpute.htmlThank you!","Creater_id":125317,"Start_date":"2016-08-01 22:11:42","Question_id":226803,"Tags":["r","machine-learning","random-forest","data-imputation","predictor"],"Answer_count":1,"Last_activity":"2016-08-02 01:56:30","Link":"http://stats.stackexchange.com/questions/226803/what-is-the-proper-way-to-use-rfimpute-imputation-by-random-forest-in-r","Creator_reputation":1}
{"_id":{"$oid":"5837a587a05283111e4d6857"},"View_count":65,"Display_name":"llewmills","Question_score":1,"Question_content":"In Likelihood and All That Ben Bolker states \" the joint likelihood of the whole data set is the product of the likelihoods of each individual observation\". Is there a way that someone could explain to me, in simple terms, how we obtain the likelihood of an individual observation? ","Creater_id":79732,"Start_date":"2016-07-31 21:38:07","Question_id":226619,"Tags":["likelihood"],"Answer_count":3,"Last_activity":"2016-08-02 01:41:58","Link":"http://stats.stackexchange.com/questions/226619/obtaining-the-likelihood-of-an-individual-observation","Creator_reputation":120}
{"_id":{"$oid":"5837a587a05283111e4d6866"},"View_count":33,"Display_name":"Paolo Nadalutti","Question_score":1,"Question_content":"(Edited to include systematic error)I'm running some predictive experiments on a quantitative variable. The dataset is made mostly of data coming from sensors, and the outcome variable is the result of a mechanical test. So both Xs and Y are affected by some systematic error: I guess this means I will never have predictions more accurate than the total systematic error involved in process. How can I detect this error, so I can have some threshold for the maximum accuracy for the model?I don't need a perfect measure (that would be included in instruments documentation, currently not available) but at least some degree of reliability for the model.First thing that came to my mind is to find some observations with a really similar profile, and check if the real Y has some variability.Any suggestions? some literature about this?Thanks","Creater_id":124864,"Start_date":"2016-08-01 06:51:27","Question_id":226678,"Tags":["machine-learning","bias","measurement-error"],"Answer_count":0,"Last_activity":"2016-08-02 01:22:51","Link":"http://stats.stackexchange.com/questions/226678/how-to-evaluate-instrumentation-accuracy","Creator_reputation":26}
{"_id":{"$oid":"5837a587a05283111e4d6868"},"View_count":74,"Display_name":"data_hope","Question_score":2,"Question_content":"Reading up on the Neyman Pearson Lemma, I have a question about how to obtain the probability for a type I error, , when we establish a threshold  for the liklihood ratio .The probability of a type I error (rejecting the null-hypothesis, under the condition, that it is actually true) is given by the lemma by \\Pr(\\Lambda(X)\\leq \\eta\\mid H_0)=\\alphaAs I understand it, we can either choose an , and then calculate the probability of a type 1 error, or (preferrably) find an expression for  and basically find a function . Either way, I need to find a way to solve .When I tried to solve this, I found that  looks like a cumulative distribution function of . This makes me wonder: How one could obtain how  is distributed?Or to rephrase: How can I use the Neyman-Pearson lemma to perform a hypothesis test between two models, with a given type ii error-probability of .","Creater_id":43084,"Start_date":"2016-07-24 01:03:59","Question_id":225335,"Tags":["hypothesis-testing"],"Answer_count":2,"Last_activity":"2016-08-02 01:20:10","Link":"http://stats.stackexchange.com/questions/225335/how-to-work-out-the-relationship-between-error-alpha-and-threshold-eta","Creator_reputation":107}
{"_id":{"$oid":"5837a587a05283111e4d6876"},"View_count":92,"Display_name":"Iv\u0026#225;n Mauricio Burbano","Question_score":4,"Question_content":"Suppose I have a set of data  in which the uncertainty in the measurements  (which come from the propagation of systematic errors from the measurement apparatus) is different for each point. If I do a linear regression on the set of data how do I calculate the uncertainty in the slope? I would like an explicit procedure or formula. ","Creater_id":84341,"Start_date":"2015-08-06 14:00:46","Question_id":165046,"Tags":["linear-model","measurement-error","errors-in-variables"],"Answer_count":1,"Last_activity":"2016-08-02 01:20:01","Link":"http://stats.stackexchange.com/questions/165046/systematic-measurement-error-on-a-linear-regression","Creator_reputation":121}
{"_id":{"$oid":"5837a587a05283111e4d6883"},"View_count":41,"Display_name":"Sylvia","Question_score":0,"Question_content":"I have seed and seedling abundance data for several plant species.  I am trying to model the effect of my treatment on how many seeds successfully transition into seedlings (seed-to-seedling transition). My goal is to see in which treatment (made of four levels, C, P, I and R) are seeds more likely to establish and grow into seedlings, or see if there is no treatment effect at all.  I am trying to do this through the glmer function in the lme4 package using a binomial model.  I have (1|Site) as a random factor due to a nested design.  My response variable, I think, should be successes: the number of seeds that transitioned into seedlings, and failures: the number of seeds that did not transition.  However, here is my problem... I measured seeds and seedlings at different scales (.75 m2 seed traps vs 24 m2 seedling plots).  In order to compare Seed #s to Seedling #s, I should standardize by the area and get #s per m2.  But doing this gives me decimal numbers, which do not seem to work in the glmer model.  (I should also note that I have a ton of zeros in my data)Someone recommended using an offset. But I can't figure out how to set up the offset in my data since the columns for my response variable are composed of both seed and seedling data, which were both measured on different scales.  Is there anyway I can model what I am trying to do?  Or should I just try to figure out a different model?  Here is a link to my data as a CSV file:STS15 CSV file     And here is a link to the excel file, which contains both standardized and unstandardized values: STS Excel fileHere is the formula I am trying:  md1\u0026lt;-glmer(cbind(CX_S, CX_F)~Treatment + (1|Site), family=binomial, data=STS15)I get this error message:  Warning messages:    1: In eval(expr, envir, enclos) : non-integer counts in a binomial glm!    2: In checkConv(attr(opt, \"derivs\"), optpar, ctrl = controlcheckConv,  :     Hessian is numerically singular: parameters are not uniquely determinedPlease let me know if I can provide any more information. Thanks in advance for any help, I really have been struggling with this model.","Creater_id":113563,"Start_date":"2016-06-03 06:32:24","Question_id":216139,"Tags":["binomial","glmm","glmer","ecology","offset"],"Answer_count":0,"Last_activity":"2016-08-02 01:12:41","Link":"http://stats.stackexchange.com/questions/216139/using-variables-measured-at-different-scales-in-a-binomial-glmm-with-successes-a","Creator_reputation":3}
{"_id":{"$oid":"5837a587a05283111e4d6885"},"View_count":220,"Display_name":"Christian","Question_score":5,"Question_content":"I've read that if two time series,  and , are trend stationary, then regressing  on  results in a spurious regression because of an omitted time trend variable. Let  and . I want to show that  is a linear function of , a deterministic time trend and an error term. Can somebody please provide a mathematical proof of this? ","Creater_id":30192,"Start_date":"2013-10-01 19:05:52","Question_id":71650,"Tags":["regression","time-series","econometrics"],"Answer_count":1,"Last_activity":"2016-08-02 01:08:20","Link":"http://stats.stackexchange.com/questions/71650/spurious-correlation","Creator_reputation":143}
{"_id":{"$oid":"5837a587a05283111e4d6891"},"View_count":22,"Display_name":"akshay","Question_score":0,"Question_content":"In my current course on Statistical Inference, our instructor is using the following conditional probability notation to denote the Type-I error:\\begin{equation}\\alpha=Pr[H_o\\ is\\ rejected|H_o\\ is\\ true]\\end{equation}whereas I feel that the above notations makes little sense as when you're already given that  is true, you're not going to reject it in any way. On the contrary, I view Type-I error as following:\\begin{equation}\\alpha=Pr[H_o\\ is\\ rejected\\ when\\ H_o\\ is\\ true]\\end{equation}Could anyone provide some help to decide as to which notation makes sense while formulating it mathematically?","Creater_id":89298,"Start_date":"2016-08-02 00:45:55","Question_id":226815,"Tags":["hypothesis-testing","conditional-probability","inference","type-i-errors"],"Answer_count":0,"Last_activity":"2016-08-02 00:45:55","Link":"http://stats.stackexchange.com/questions/226815/probabilististic-notation-of-type-i-error-in-statistical-inference","Creator_reputation":14}
{"_id":{"$oid":"5837a587a05283111e4d6893"},"View_count":30,"Display_name":"Gautier","Question_score":0,"Question_content":"I have a question in statistics as I am not really good at it ! I am currently studying the effects of Venture Capital ownership on the stocks returns. In the same time I study the effect of Hedge Funds ownership on the same returns also. I want to make a regression on my long-term returns with these 2 ownerships explicative variables. But I do not know even which regression to use in order to proceed that. Do you know how should I do, and do you have a pratical guide to do this kind of regression please? Thank you in advance! Gautier","Creater_id":120506,"Start_date":"2016-08-01 01:19:48","Question_id":226638,"Tags":["regression","model"],"Answer_count":1,"Last_activity":"2016-08-02 00:37:38","Link":"http://stats.stackexchange.com/questions/226638/how-to-make-a-regression-with-two-explicative-variables-and-interpret-the-result","Creator_reputation":1}
{"_id":{"$oid":"5837a587a05283111e4d68a0"},"View_count":27,"Display_name":"user3277533","Question_score":0,"Question_content":"I'm looking at the abundance of different groups of humans over time. These abundance data closely follow a quadratic model of the formfor all groups of humans, where  is time and  is abundance. For my research I'm paying close attention to both the linear and quadratic terms. I found that the  and  have a strong negative correlation. I know that this is expected, since these terms are not independent. I read that you can make  and  independent by centering  around the mean before running the regression (Gelman, 2008). I can see how this works when  and  are different physical quantities (i.e. BMI vs. sugar consumption), but I don't know how to make  and  independent by centering  (i.e. time) around the mean in a way that accounts for temporal autocorrelation (e.x. the number of humans over time). How can I examine the relationship between the linear and quadratic terms (which are not independent) for temporal data.Gelman, A. (2008) Scaling regression inputs by dividing by two standard deviations.Statistics in Medicine, 27, 2865–2873.","Creater_id":45696,"Start_date":"2016-08-01 15:45:47","Question_id":226768,"Tags":["regression","polynomial","centering","quadratic-form"],"Answer_count":1,"Last_activity":"2016-08-02 00:32:31","Link":"http://stats.stackexchange.com/questions/226768/making-the-linear-and-quadratic-terms-independent-in-temporal-data","Creator_reputation":1}
{"_id":{"$oid":"5837a587a05283111e4d68ad"},"View_count":35,"Display_name":"P.Escondido","Question_score":2,"Question_content":"Let X be a random variable with support that covers 0 (i.e. r.v. can take both positive and negative values) and positive mean: . Let . Question: for which class of distributions  is a decreasing function of ? Is there a single distribution with this property? (at least for some values of )Using direct differentiation w.r.t. EX, I came up with the following question: is there a distribution for which? (alternatively:)","Creater_id":125319,"Start_date":"2016-08-01 23:20:04","Question_id":226807,"Tags":["distributions","mathematical-statistics","cdf","hazard","mathematics"],"Answer_count":0,"Last_activity":"2016-08-01 23:35:46","Link":"http://stats.stackexchange.com/questions/226807/does-distribution-with-these-properties-exist","Creator_reputation":126}
{"_id":{"$oid":"5837a587a05283111e4d68af"},"View_count":67,"Display_name":"Misty","Question_score":0,"Question_content":"  Josefina is the star athlete on her college soccer team. She especially loves to score goal(s), and she does this on a regular basis. Suppose, the random variable X = the number of goals Josefina will score tonight. The probability distribution of X is given.X       0       1       2       3P(X)    0.25    0.35    0.25    0.15    The probability that Josefina will score fewer than 1 goals tonight is _____.I'm definitely getting stuck at all of it. I need help figuring out how to get the answers I need. This is a homework question, but I need to know how to figure it out. ","Creater_id":125308,"Start_date":"2016-08-01 17:33:32","Question_id":226778,"Tags":["probability","self-study"],"Answer_count":1,"Last_activity":"2016-08-01 23:22:52","Link":"http://stats.stackexchange.com/questions/226778/homework-question-get-probability-from-distribution-table","Creator_reputation":3}
{"_id":{"$oid":"5837a587a05283111e4d68bc"},"View_count":34,"Display_name":"mackbox","Question_score":0,"Question_content":"Suppose I have a Bayesian network that can be factorized like this:P(A,B,C,D)=P(A)*P(B)*P(C|A,B)*P(D|C)Each of the variable is a binary and I've got all the tables of conditional probabilistic distribution. I then simulate from this network and got a synthetic data-set:     A  B  C  D    1  0  0  1    0  1  0  1    1  0  1  1    etc. I then estimate the parameters (the probabilities) with MLE and Bayesian estimator from the synthetic data and would like to compare these 2 learners by their KL-distances against the true distribution at various sample sizes. However, I'm not sure how to do it. The book I read about the relative entropy of Bayesian network just has notations in it and it's very hard to follow. Thanks!  ","Creater_id":73733,"Start_date":"2016-08-01 12:31:38","Question_id":226744,"Tags":["bayesian","bayesian-network","kullback-leibler"],"Answer_count":1,"Last_activity":"2016-08-01 23:19:36","Link":"http://stats.stackexchange.com/questions/226744/how-to-compute-the-kl-distance-between-2-bayesian-networks","Creator_reputation":128}
{"_id":{"$oid":"5837a587a05283111e4d68c9"},"View_count":126,"Display_name":"Jacopo Soppelsa","Question_score":3,"Question_content":"I have a vector with income values of all companies that I found (n=1821). The income should look like a lognormal distribution, but if I use the hist function in R (RStudio) the result is this: As you can see there are many values appear to be near 0, that's because lots of income values are 0 and many are quite small, and then there are a few incomes (about 10) with very high values.What should I do to more clearly show the shape with a histogram?Should I delete all 0 values from the vector?I don't know how to proceed.","Creater_id":125079,"Start_date":"2016-08-01 05:49:49","Question_id":226669,"Tags":["distributions","data-visualization","lognormal","histogram"],"Answer_count":1,"Last_activity":"2016-08-01 22:09:35","Link":"http://stats.stackexchange.com/questions/226669/my-data-are-very-skew-and-i-cant-see-any-detail-in-a-histogram-how-do-i-see-th","Creator_reputation":16}
{"_id":{"$oid":"5837a587a05283111e4d68d6"},"View_count":64,"Display_name":"Helene ","Question_score":2,"Question_content":"I've been reading a lot on confidence intervals lately and I keep seeing statements such as:  \"A 95% confidence interval is a random interval that contains the true parameter 95% of the time\" or \"A confidence interval is a random variable because x-bar (its center) is a random variable.\"Why is the confidence interval considered random? If it's truly random then why bother with confidence intervals at all? Am I missing something here?","Creater_id":124110,"Start_date":"2016-08-01 21:17:41","Question_id":226799,"Tags":["confidence-interval","p-value"],"Answer_count":2,"Last_activity":"2016-08-01 22:06:12","Link":"http://stats.stackexchange.com/questions/226799/why-is-the-confidence-interval-considered-a-random-interval","Creator_reputation":136}
{"_id":{"$oid":"5837a587a05283111e4d68e4"},"View_count":181,"Display_name":"Mad Wombat","Question_score":0,"Question_content":"I am new to the machine learning field, so I am not sure if I am asking a dumb question. I have been playing around with Theano for a while and read a lot of code examples and it looks like every time Theano graph needs to find gradients for a list of parameters it uses T.grad() function. Reading up on the grad() implementation, it seems that it is not doing backpropagation, but rather relies on chain rule and individual ops knowing how to differentiate themselves. Am I correct? How are the results different using this two methods? Does it make sense to implement backpropagation in Theano?","Creater_id":121178,"Start_date":"2016-08-01 13:27:33","Question_id":226754,"Tags":["neural-networks","backpropagation","theano"],"Answer_count":1,"Last_activity":"2016-08-01 22:03:20","Link":"http://stats.stackexchange.com/questions/226754/implementing-backpropagation-in-theano","Creator_reputation":153}
{"_id":{"$oid":"5837a587a05283111e4d68f1"},"View_count":65,"Display_name":"arshad","Question_score":1,"Question_content":"I have 10 samples which has males and females. I want to check whether the sex ratios in any of these sample are significantly different from other. Can someone please help me what test can I use in R to find this?","Creater_id":20213,"Start_date":"2016-08-01 19:39:47","Question_id":226789,"Tags":["r","count-data","ratio"],"Answer_count":2,"Last_activity":"2016-08-01 21:53:25","Link":"http://stats.stackexchange.com/questions/226789/how-to-compare-ratios-in-r","Creator_reputation":49}
{"_id":{"$oid":"5837a587a05283111e4d68fe"},"View_count":239,"Display_name":"K-1","Question_score":1,"Question_content":"Assume that I have decomposed a data set using Symlet Wavelet with six levels. How can I estimate the approximate frequency interval of each level?That would be great if you consider your answer in the Mathematica environment.If the above dataset describes turbulent velocity field, how I can decide up to which level is the bulk velocity? I need to separate the turbulent fluctuations from the mean.The sampling frequency is 1 Hz and the duration of observation is 25 hours.","Creater_id":4286,"Start_date":"2011-08-18 23:17:38","Question_id":14505,"Tags":["wavelet","mathematica"],"Answer_count":1,"Last_activity":"2016-08-01 21:25:39","Link":"http://stats.stackexchange.com/questions/14505/frequency-of-a-wavelet-filter","Creator_reputation":240}
{"_id":{"$oid":"5837a588a05283111e4d690b"},"View_count":59,"Display_name":"Christian","Question_score":1,"Question_content":"Suppose  =  has an  of 0.9. Is it possible to calculate the  between  and ? ","Creater_id":30192,"Start_date":"2016-07-31 21:13:46","Question_id":226616,"Tags":["correlation","econometrics"],"Answer_count":1,"Last_activity":"2016-08-01 20:40:46","Link":"http://stats.stackexchange.com/questions/226616/whats-the-r2-between-x-and-y-if-change-in-x-and-the-change-in-y-are-correlat","Creator_reputation":143}
{"_id":{"$oid":"5837a588a05283111e4d6917"},"View_count":38,"Display_name":"Gianfranco Giorgianni","Question_score":0,"Question_content":"I was trying to fit the parameters of a time-dependent system coupled of ODES related to a kinetic experiment with multi response data.Example:A-\u003eB+HA+H-\u003eC+HA-\u003eDdcA(t)/dt=-k1Ca(t)-k2Ca(t)*Ch(t)-k3*Ca(t)dcB(t)/dt=k1Ca(t)dcC(t)/dt=k2Ca(t)*Ch(t)dcH(t)/dt=k1Ca(t)  I am measuring the data of A, B and C and I wanted to use the maximum likelihood (ML) approach for fitting the kinetics instead of the classical least squares approach to obtain more reliable confidence intervals. All the data are single point measurement and we do not have any information about the shape of the error. The measured quantities with time are correlated to each other and not independent.Could you help me please? I have already parametrized for the solution with conventional nonlinear regression by adding the contribution of each measured variable. However, the C.I. on the parameter are still a bit large and can be improved by the ML approach which by the Bayesian theory should be much better.This procedure is something the most advanced software for modelling have built in.","Creater_id":null,"Start_date":"2016-07-23 07:06:05","Question_id":225257,"Tags":["maximum-likelihood","modeling","fitting","nonlinear","differential-equations"],"Answer_count":0,"Last_activity":"2016-08-01 20:32:42","Link":"http://stats.stackexchange.com/questions/225257/setting-up-maximum-likelihood-estimation-with-multi-response-data","Creator_reputation":null}
{"_id":{"$oid":"5837a588a05283111e4d6919"},"View_count":36,"Display_name":"Minh Mai","Question_score":0,"Question_content":"I'm new to natural language processing but I'm trying to create a comment generator with an article as a training set. I'm unsure how to properly tackle this issue. I'm thinking of conducting a sentiment analysis and then somehow generate a comment such as \"That is great!\" for a positive article.What are the best sources to learn how to get this started. Ideally, this would be in python. I've looked into SyntaxNet, but I'm looking for concepts and ideas that I should be exploring and looking at a high level to get started. Thanks!","Creater_id":85978,"Start_date":"2016-08-01 19:58:16","Question_id":226791,"Tags":["python","natural-language"],"Answer_count":0,"Last_activity":"2016-08-01 19:58:16","Link":"http://stats.stackexchange.com/questions/226791/creating-a-sentence-generator-in-python","Creator_reputation":106}
{"_id":{"$oid":"5837a588a05283111e4d691b"},"View_count":52,"Display_name":"Bill","Question_score":1,"Question_content":"I want to do clustering by using kmeans algorithm. Despite that I searched a lot, I could not find a practical solution by weighting features. Let' s assume that there is a dataset of people with the following features:number of children (integer)existence of husband (boolean)annual income in $ (integer, but with greater order than the previous integer)weight (double)money spent (integer, at most 3-digit number).In order to do the clustering, I realised that normalisation is a good practice. I implemented min-max as well as z-score, but I observed that the variance plays a crucial role.Thus, I think that it is essential to find a practical method of handling the features in a different (\"unequal\") way.","Creater_id":125304,"Start_date":"2016-08-01 16:01:43","Question_id":226769,"Tags":["clustering","normalization","algorithms"],"Answer_count":2,"Last_activity":"2016-08-01 19:46:56","Link":"http://stats.stackexchange.com/questions/226769/weighted-feature-clustering-e-g-kmeans","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d6929"},"View_count":30,"Display_name":"Chris","Question_score":1,"Question_content":"The R binom library has several confidence intervals to choose from for Binomial distributions.The Bayes method uses the Beta distribution. According to the binom documentation:The default prior is Jeffrey's prior which is a Beta(0.5, 0.5) distribution. Thus the posterior mean is (x + 0.5)/(n + 1).p|x ~ Beta(x + prior.shape1, n - x + prior.shape2)The prior.shape1 and prior.shape2 can be passed in like so:binom.bayes(x, n,            conf.level = 0.95,            type = c(\"highest\", \"central\"),            prior.shape1 = 0.5,            prior.shape2 = 0.5,            tol = .Machinelower,bmean,' ac.mean=',p.tilde,' w=',w,' weighted.avg=',check,\"\\n\")    if(x==0) {      ans0[n]=w;    }else {      ans1[n]=w;    }  }}View(data.frame(xEq0=ans0,xEq1=ans1))Output:0 / 1  \"exact\".center= 0.4875  bayes.mean= 0.25  ac.mean= 0.4  w= 39  weighted.avg= 0.4875 0 / 2  \"exact\".center= 0.4209431  bayes.mean= 0.1666667  ac.mean= 0.3333333  w= 10.64911  weighted.avg= 0.4209431 0 / 3  \"exact\".center= 0.3537991  bayes.mean= 0.125  ac.mean= 0.2857143  w= 7.259856  weighted.avg= 0.3537991 0 / 4  \"exact\".center= 0.3011823  bayes.mean= 0.1  ac.mean= 0.25  w= 6.059467  weighted.avg= 0.3011823 0 / 5  \"exact\".center= 0.2609119  bayes.mean= 0.08333333  ac.mean= 0.2222222  w= 5.456396  weighted.avg= 0.2609119 0 / 6  \"exact\".center= 0.2296291  bayes.mean= 0.07142857  ac.mean= 0.2  w= 5.095867  weighted.avg= 0.2296291 0 / 7  \"exact\".center= 0.2048082  bayes.mean= 0.0625  ac.mean= 0.1818182  w= 4.856698  weighted.avg= 0.2048082 0 / 8  \"exact\".center= 0.1847083  bayes.mean= 0.05555556  ac.mean= 0.1666667  w= 4.686665  weighted.avg= 0.1847083 0 / 9  \"exact\".center= 0.1681336  bayes.mean= 0.05  ac.mean= 0.1538462  w= 4.559672  weighted.avg= 0.1681336 0 / 10  \"exact\".center= 0.1542486  bayes.mean= 0.04545455  ac.mean= 0.1428571  w= 4.461255  weighted.avg= 0.1542486 0 / 11  \"exact\".center= 0.1424571  bayes.mean= 0.04166667  ac.mean= 0.1333333  w= 4.382768  weighted.avg= 0.1424571 0 / 12  \"exact\".center= 0.1323242  bayes.mean= 0.03846154  ac.mean= 0.125  w= 4.318726  weighted.avg= 0.1323242 0 / 13  \"exact\".center= 0.1235263  bayes.mean= 0.03571429  ac.mean= 0.1176471  w= 4.265483  weighted.avg= 0.1235263 0 / 14  \"exact\".center= 0.1158179  bayes.mean= 0.03333333  ac.mean= 0.1111111  w= 4.220525  weighted.avg= 0.1158179 Question #2 For small values of the ideal weight parameter may be as high as 39.  Would it be best to modify Agresti-Coull to adjust it's method of weight to match accordingly?References:Agresti-Coull Interval","Creater_id":70282,"Start_date":"2016-08-01 16:39:51","Question_id":226774,"Tags":["bayesian","confidence-interval"],"Answer_count":1,"Last_activity":"2016-08-01 19:40:30","Link":"http://stats.stackexchange.com/questions/226774/binomial-confidence-intervals-bayes-jeffreys-prior-vs-agresti-coull-method","Creator_reputation":410}
{"_id":{"$oid":"5837a588a05283111e4d6935"},"View_count":49,"Display_name":"RMurphy","Question_score":0,"Question_content":"I'm having difficulty replicating/deriving a result in GLM's for Binomial data.  That is, if  and we put the distribution of  into exponential family form (with a dispersion parameter), then the variance function is given by:V(\\mu) = \\frac{\\mu(1-\\mu)}{n}This can be found on page six of these lecture slides and on page 116 of Faraway's \"Extending the Linear Model with R\"I just don't see that.  Here's my approach.  Let's start by writing the response variable's pmf in exponential family form.  If we write  as our response variable,  is binomial so that  ,with , , , , and  .Now, to get the variance function, we begin by:We need this in terms of  so we plug in  and getThis is conspicuously missing a .  What am I missing?Thanks for your help!-","Creater_id":116056,"Start_date":"2016-06-19 10:45:12","Question_id":219649,"Tags":["generalized-linear-model","exponential-family"],"Answer_count":2,"Last_activity":"2016-08-01 19:36:30","Link":"http://stats.stackexchange.com/questions/219649/help-deriving-variance-function-binomial-glm","Creator_reputation":80}
{"_id":{"$oid":"5837a588a05283111e4d6943"},"View_count":30,"Display_name":"KevinKim","Question_score":1,"Question_content":"Assume we have a parametric model with parameter . We want to use this model to make predictions, e.g., credit card firm using some attributes  of the customer to predict the default rate .First, we need to estimate  in our model. There are many ways to do the estimation. Let's say we are looking at extreme estimators, which are the solution of some optimization problem, e.g., Maximum Likelihood, Minimize SSE, etc.After you estimate , you want to apply the model (with  being estimated from a specific estimation procedure you have used) to a test data set. Now we need to come up with an evaluation criterion to judge how well your model is. In the credit card example, a commonly used criterion is the misclassification rate, i.e., the average rate that your model prediction against the reality. So under this criterion, the model with smaller misclassification rate is considered as better. Now, for the same model, but with two different estimation procedure (and hence two different values of , which could be very different), then it is possible that the misclassification rates are different. My question is: given the goal (i.e., the judging criterion of the model, in this example, misclassification rate), should we always use this or similar criterion as the objective function to derive our estimator for ? However, in practice, I see in many situations, people use MLE to run the estimation (I guess, because it is unbiased and consistent), but for model evaluation, they use a different criterion, e.g., misclassification rate. I would feel more comfortable if the evaluation criterion is something like deviance if the estimation procedure is MLE, since deviance is equivalent to the log likelihood function.","Creater_id":66461,"Start_date":"2016-06-27 14:52:29","Question_id":220924,"Tags":["estimation","model-evaluation"],"Answer_count":1,"Last_activity":"2016-08-01 19:33:33","Link":"http://stats.stackexchange.com/questions/220924/should-the-underlying-logic-of-the-model-evaluation-criterion-match-with-the-est","Creator_reputation":1640}
{"_id":{"$oid":"5837a588a05283111e4d6950"},"View_count":205,"Display_name":"Sus20200","Question_score":11,"Question_content":"  If  is a probability distribution with non-zero values on ,  for what type(s) of  does there exist a constant  such that   for all ?The inequality above is actually a Kullback-Leibler Divergence between distribution  and a compressed version of it . I have found out that this inequality holds for Exponential, Gamma, and Weibull distributions and I am interested to know if that works for a larger class of probability distributions.Any idea what that inequality means?","Creater_id":96063,"Start_date":"2015-12-10 15:43:56","Question_id":186167,"Tags":["probability","stochastic-processes","kullback-leibler","probability-inequalities"],"Answer_count":1,"Last_activity":"2016-08-01 19:28:46","Link":"http://stats.stackexchange.com/questions/186167/special-probability-distribution","Creator_reputation":183}
{"_id":{"$oid":"5837a588a05283111e4d695d"},"View_count":49,"Display_name":"Mohammad Ali Nematollahi","Question_score":-1,"Question_content":"I know that Laplacian distribution function is defined as follow f(x)=\\frac{b}{2}\\exp(-b|x-\\mu|) Also, I know that the mean and variance for the ratio between two normal variables like c=\\frac{a}{b} Anyone can guide how would be mean and variance for Laplacian distribution?","Creater_id":124105,"Start_date":"2016-07-21 19:15:41","Question_id":225033,"Tags":["pdf","laplace-distribution"],"Answer_count":1,"Last_activity":"2016-08-01 18:40:31","Link":"http://stats.stackexchange.com/questions/225033/computing-the-mean-and-variance-of-the-ratio-of-two-laplace-variables","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d696a"},"View_count":52,"Display_name":"bill999","Question_score":1,"Question_content":"Is it possible to calculate P(B) if one knows P(A,B,C), P(A,B), P(B,C), P(A), and P(C)?","Creater_id":52743,"Start_date":"2016-08-01 17:06:04","Question_id":226775,"Tags":["probability"],"Answer_count":0,"Last_activity":"2016-08-01 18:27:10","Link":"http://stats.stackexchange.com/questions/226775/calculate-pb-given-some-other-probabilities","Creator_reputation":155}
{"_id":{"$oid":"5837a588a05283111e4d696c"},"View_count":64,"Display_name":"Krombopulos Michael","Question_score":1,"Question_content":"I'm writing on my first scientific paper and I'm a little bit lost how to report my results:I have trained a neural network to identify a disease risk group from genetic data. In a monte-carlo simulation the results of my training processes are a little bit unstable over different training \\ test samples:Accuracy:Min.  1st Qu.  Median    Mean   3rd Qu.   Max. 0.47  0.69     0.77      0.76   0.83      0.94 ROC-AUC:Min.  1st Qu.  Median    Mean   3rd Qu.  Max. 0.29  0.57     0.79      0.72   0.88     0.96 Overall I'm very happy with the results but how should I report those results? Should I discuss a single model \\ test \\ training sample combination more closely? If so, which? Do you have tips, what to avoid in any case? Thanks in advance!","Creater_id":121161,"Start_date":"2016-08-01 05:57:29","Question_id":226670,"Tags":["neural-networks","monte-carlo","reporting"],"Answer_count":1,"Last_activity":"2016-08-01 17:56:46","Link":"http://stats.stackexchange.com/questions/226670/how-to-report-neural-network-training-results","Creator_reputation":18}
{"_id":{"$oid":"5837a588a05283111e4d6978"},"View_count":32,"Display_name":"Dwaipayan Gupta","Question_score":0,"Question_content":"With regards to the question in the above picture and the markov chain drawn in the question, my query is whether is it possible to conclude from Ergodic theorem that this Markov chain has an invariant stationary distribution ?","Creater_id":88754,"Start_date":"2016-08-01 12:18:59","Question_id":226740,"Tags":["stochastic-processes","markov-process","ergodic"],"Answer_count":1,"Last_activity":"2016-08-01 17:39:50","Link":"http://stats.stackexchange.com/questions/226740/problem-involving-ergodic-theorem-and-markov-chain","Creator_reputation":154}
{"_id":{"$oid":"5837a588a05283111e4d6985"},"View_count":1392,"Display_name":"Hannah","Question_score":4,"Question_content":"I am comparing a random forest model to a GLS model using a univariate time series that has a deterministic linear trend. I am going to add a linear time trend covariate (among other predictors) to the GLS model to account for the changing trend. To be consistent in my comparison, I was hoping to add this predictor to the random forest regression model as well. I have been looking for literature on this subject and can't find much.Does anyone know if adding this type of predictor is inappropriate in a random forest regression for any reason? The random forest regression already includes time-lagged variables to account for autocorrelation.","Creater_id":79581,"Start_date":"2015-10-07 09:48:30","Question_id":175908,"Tags":["time-series","random-forest","trend"],"Answer_count":2,"Last_activity":"2016-08-01 17:07:04","Link":"http://stats.stackexchange.com/questions/175908/random-forest-regression-and-trended-time-series","Creator_reputation":78}
{"_id":{"$oid":"5837a588a05283111e4d6992"},"View_count":113,"Display_name":"AlphaOmega","Question_score":2,"Question_content":"Yes, I already looked here but that's too high profile for my humble mind (and it's not exactly what I'm looking for).Imagine we have a timecourse with time on the x-axis and some value on the y-axis (e.g. a signal). Now I can sample this timecourse to obtain a vector in a multidimensional vector space.My question: What does it mean if I perform a PCA on this data? What is the PCA of a single vector (such as the timeseries) and how can I interpret the resulting eigenvectors?","Creater_id":120194,"Start_date":"2016-07-29 05:27:29","Question_id":226281,"Tags":["time-series","pca"],"Answer_count":2,"Last_activity":"2016-08-01 16:38:51","Link":"http://stats.stackexchange.com/questions/226281/what-does-performing-pca-on-a-single-time-series-mean-do","Creator_reputation":163}
{"_id":{"$oid":"5837a588a05283111e4d69a0"},"View_count":6366,"Display_name":"statHacker","Question_score":9,"Question_content":"I am trying to understand the use of PCA in a recent journal article titled \"Mapping brain activity at scale with cluster computing\" Freeman et al., 2014 (free pdf available on the lab website).  They use PCA on time series data, and use the PCA weights to create a map of the brain.The data is trial-average imaging data, stored as a matrix (called  in the paper) with  voxels (or imaging locations in the brain)  time points (the length of a single stimulation to the brain).They use the SVD resulting in \\hat {\\mathbf Y} = \\mathbf{USV}^\\top ( indicating transpose of matrix ).The authors state that  The principal components (the columns of ) are vectors of length , and the scores (the columns of ) are vectors of length  (number of voxels), describing the projection of each voxel on the direction given by the corresponding component, forming projections on the volume, i.e. whole-brain maps.So the PCs are vectors of length .  How can I interpret that the \"first principal component explains the most variance\" as is commonly expressed in tutorials of PCA? We started with a matrix of many highly correlated time-series -- how does a single PC time series explain variance in the original matrix?  I understand the whole \"rotation of a Gaussian cloud of points to the most-varied axis\" thing, but am unsure how this relates to time-series.  What do the authors mean by direction when they state: \"the scores (the columns of ) are vectors of length  (number of voxels), describing the projection of each voxel on the direction given by the corresponding component\"?  How can a principal component time course have a direction?To see an example of the resulting times series from linear combinations of principle components 1 and 2 and the associated brain map, go to the following link and mouse over on the the dots in the XY plot.My second question is related to the (state-space) trajectories they create using the principal component scores.These are created by taking first 2 scores (in the case of the \"optomotor\" example I have outlined above) and project the individual trials (used to create the trial-averaged matrix described above) into the principal subspace by the equation: \\mathbf J = \\mathbf U^\\top \\mathbf Y.As you can see by the linked movies, each trace in state space represents the activity of the brain as a whole.  Can someone provide the intuition for what each \"frame\" of the state space movie means, as compared to the figure that associates the XY plot of the scores of the first 2 PCs.  What does it means at a given \"frame\" for 1 trial of the experiment to be in 1 position in the XY state-space and another trial to be in another position?  How do the XY plot positions in the movies relate to the principle component traces in the linked figure mentioned in the first part of my question?","Creater_id":61477,"Start_date":"2014-11-25 09:59:49","Question_id":125462,"Tags":["time-series","pca","state-space-models","neuroimaging","neuroscience"],"Answer_count":2,"Last_activity":"2016-08-01 16:36:17","Link":"http://stats.stackexchange.com/questions/125462/how-to-interpret-pca-on-time-series-data","Creator_reputation":25}
{"_id":{"$oid":"5837a588a05283111e4d69ae"},"View_count":1956,"Display_name":"Tim","Question_score":58,"Question_content":"We already have multiple threads tagged as p-values that reveal lots of misunderstandings about them. Ten months ago we had a thread about psychological journal that \"banned\" -values, now American Statistical Association (2016) says that with our analysis we \"should not end with the calculation of a -value\".   American Statistical Association (ASA) believes that the scientific   community could benefit from a formal statement clarifying several  widely agreed upon principles underlying the proper use and  interpretation of the -value.The committee lists other approaches as possible alternatives or supplements to -values:   In view of the prevalent misuses of and misconceptions concerning  -values, some statisticians prefer to supplement or even replace  -values with other approaches. These include methods that emphasize  estimation over testing, such as confidence, credibility, or  prediction intervals; Bayesian methods; alternative measures of  evidence, such as likelihood ratios or Bayes Factors; and other  approaches such as decision-theoretic modeling and false discovery  rates. All these measures and approaches rely on further assumptions,  but they may more directly address the size of an effect (and its  associated uncertainty) or whether the hypothesis is correct.So let's imagine post--values reality. ASA lists some methods that can be used in place of -values, but why are they better? Which of them can be real-life replacement for a researcher that used -values for all his life? I imagine that this kind of questions will appear in post--values reality, so maybe let's try to be one step ahead of them. What is the reasonable alternative that can be applied out-of-the-box? Why this approach should convince your lead researcher, editor, or readers?As this follow-up blog entry suggests, -values are unbeatable in their simplicity:  The p-value requires only a statistical model for the behavior of a  statistic under the null hypothesis to hold. Even if a model of an  alternative hypothesis is used for choosing a “good” statistic (which  would be used for constructing the p-value), this alternative model  does not have to be correct in order for the p-value to be valid and  useful (i.e.: control type I error at the desired level while offering  some power to detect a real effect). In contrast, other (wonderful and  useful) statistical methods such as Likelihood ratios, effect size  estimation, confidence intervals, or Bayesian methods all need the  assumed models to hold over a wider range of situations, not merely  under the tested null.Are they, or maybe it is not true and we can easily replace them?I know, this is broad, but the main question is simple: what is the best (and why), real-life alternative to -values that can be used as a replacement?ASA (2016). ASA Statement on Statistical Significance and -values. The American Statistician. (in press)","Creater_id":35989,"Start_date":"2016-03-08 01:32:23","Question_id":200500,"Tags":["hypothesis-testing","p-value"],"Answer_count":6,"Last_activity":"2016-08-01 16:35:34","Link":"http://stats.stackexchange.com/questions/200500/asa-discusses-limitations-of-p-values-what-are-the-alternatives","Creator_reputation":25585}
{"_id":{"$oid":"5837a588a05283111e4d69c0"},"View_count":56,"Display_name":"Helene ","Question_score":0,"Question_content":"Let's say I was given a dataset with 50 variables with the following properties:headers existing but it is in a very simple format (a, b, c, d, etc) so I can't guess what the variable is about.no explanation is given on what the dataset is about.However, I was given one hint, which is that the first column/variable is an independent variable.How would I go about running a regression and/or building a model on this dataset? My initial thought is to try and run simple correlation between pairs of variables (between the known independent variable and an unknown variable), if they are highly correlated, then I discard that variable (multicollinearity). Does this sound like a good starting point? If not, what else should I do? Any pointers would be greatly appreciated!","Creater_id":124110,"Start_date":"2016-08-01 14:13:34","Question_id":226759,"Tags":["regression","multiple-regression","predictive-models","feature-selection","random-variable"],"Answer_count":1,"Last_activity":"2016-08-01 16:18:41","Link":"http://stats.stackexchange.com/questions/226759/determining-independent-variables-in-a-dataset","Creator_reputation":136}
{"_id":{"$oid":"5837a588a05283111e4d69cd"},"View_count":61,"Display_name":"ttnphns","Question_score":3,"Question_content":"What will be your considerations when you've got to realize the intuitively plain, but meditatively manifold idea to create a data cloud which \"the same as\" that data cloud, only that it is not spherical in shape, like it, but is ellipsoid (or perhaps just spheroid)?For there may be not one ellipsoid candidates, each one \"equivalent\" to the sphere in some one but not all aspects. I'm just illustrating it now (read only if you've got popcorn, otherwise skip to the pic).For simplicity, let's consider specifically normal distribution and 2d data case. So, generate spherical data cloud, centered, with s.d.=1; we'll see s.d. as \"radius\" of the cloud, .Choose the shape of the ellipse you want, its  ratio:  is the longer radius and  is the shorter radius (they actually are equal, respectively, to the two singular values of the elliptic data divided by the square root of the sample size).OK, let , and turn our spherical cloud to that elliptic by proportional stretching-squeezing: , . This will be our ellipse1. It is \"equivalent\" to the spherical cloud in that it inherited its area, 2d volume. Indeed, the volume inside the sphere is  and the volume inside the ellipse is . (Under any , the proportional stretching-squeezing will yield ellipse equivolumetric with the parental circle. The volume of a data cloud is the product of its singular values, i.e. sq. root of the determinant of the cov. matrix.)But what about some other important statistical/geometrical properties of a data cloud, besides the volume?Multivariate sum of variances (sum of eigenvalues): . (This quantity is important in euclidean-based statistics because - due to Pythagorean theorem - it equals the mean squared euclidean distance from data points to centroid.) In ellipse1, it is , bigger than , that of the sphere. We have to reduce the ellipse1 size approximately by  (under ) in order to get ellipse2 data with the same overall variance as the sherical's: .Multivariate sum of principal st. deviations (sum of singular values): . In ellipse1, it is , bigger than , that of the sphere. We have to reduce the ellipse1 size by  (under ) in order to get ellipse3 data with the same overall st. deviation as the sherical's: .Cloud's circumference (surface area):  for circle and (good approximate formula)  for ellipse. It amounts to  in ellipse1, bigger than , that of the sphere. We have to reduce the ellipse1 size approximately by  (under ) in order to get ellipse4 data with the same overall surface as the sherical's.And so, we have 4 same-shape but different-size ellipse data clouds each of which is equivalent to the sperical data cloud in some particular aspect, not in all aspects at once.And my silly question is, as put at the beginning: in your simulation practices, when would you prefer ellipsoid1 / ellipsoid2 / ellipsoid3 / ellipsoid4 / some ellipsoidX, as \"equivalent by properties\" to a given spherical data? To hint of an example, you might be exploring the behaviour of some multivariate statistical analysis or machine learning technique or some multivariate index/statistic, its behaviour towards (a collection of) spherical vs ellipsoid data cloud(s). I.e. its reaction or sensitivity to sheer shape. That means you need datas which differ only with respect to the shape - i.e. the coefficient  ( for spherical and some selected value  for elliptical), - and \"all the rest properties being kept equal\", to claim it intuitively right. But we saw (I illustrated it) that \"all properties\" cannot be equal, we have to choose which. For examining what kind of \"statistical/learning techniques\" or \"indices/statistics\" will you choose this or that cloud's property to be equivalent b/w a sphere and an ellipsoid (normal distribution, but not necessarily 2d data)? Please share your reasoning, intuitive or rigorous as you like.(P.S. In all four ellipses above there is one property though which is the same and is identical to that of the sphere: the Mahalanobis distances. Therefore procedures basing themselves of such property of data as Mahalanobis distances or their sum will be blind to the dissimilarity between the five clouds. But Mahalanobis distance isn't very interesting issue for my question since it, by definition, just \"removes away\" any elliptical shape of a data cloud.)","Creater_id":3277,"Start_date":"2016-08-01 06:41:03","Question_id":226676,"Tags":["multivariate-analysis","simulation","multivariate-normal","ellipse"],"Answer_count":0,"Last_activity":"2016-08-01 16:09:13","Link":"http://stats.stackexchange.com/questions/226676/select-ellipsoid-data-cloud-equivalent-to-a-spherical-data-cloud","Creator_reputation":26289}
{"_id":{"$oid":"5837a588a05283111e4d69cf"},"View_count":28,"Display_name":"cheby","Question_score":1,"Question_content":"Can one use convolutions to construct the density of a sum of dependent variables, and if so, how?I understand that to construct the density of the sum of two possibly dependent random variables, it is possible to apply the convolution formula using their joint density (e.g., How to add two dependent random variables?).If one needs to construct the pdf for the sum of multiple independent variables, it is possible to take convolutions iteratively.  But I wouldn't know how to proceed for multiple dependent variables.","Creater_id":125300,"Start_date":"2016-08-01 15:22:17","Question_id":226766,"Tags":["probability","distributions","non-independent","convolution"],"Answer_count":0,"Last_activity":"2016-08-01 15:39:43","Link":"http://stats.stackexchange.com/questions/226766/how-to-find-the-density-of-a-sum-of-multiple-dependent-variables","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d69d1"},"View_count":36,"Display_name":"Czar Yobero","Question_score":1,"Question_content":"In an R Bloggers post, the author suggests that if more than 5% of the observations in a sample feature is missing, you might want to consider dropping that sample feature entirely. My question is, is there a certain threshold where it's acceptable to drop an entire feature due to missingness? If so, what's an acceptable threshold (i.e. at what percentage should you drop an entire feature completely)? I have an explanatory variable in my data set where 99% of the values for the last purchase date for a customer is missing. The goal of the analysis is essentially to identify who the top customers are and how they vary from the others. Sure, the last purchase date intuitively seems like an important feature that could explain whether or not a customer is a \"Top Customer,\" but with 99% of the data missing, what are my options?","Creater_id":105235,"Start_date":"2016-08-01 10:19:02","Question_id":226721,"Tags":["missing-data"],"Answer_count":1,"Last_activity":"2016-08-01 13:57:13","Link":"http://stats.stackexchange.com/questions/226721/when-is-it-acceptable-to-drop-an-entire-feature-from-a-data-set-due-to-missing-o","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d69de"},"View_count":96,"Display_name":"alen hajnal","Question_score":2,"Question_content":"I have data in a perception experiment where I show surfaces slanted at different angles and ask participants to judge whether the surface could be stood on. I also asked them how certain they are about their perceptual responses using a 7-point Likert scale. Here is a subset of the data:SUB COND    TRIAL   REP ANGLE   CERTAINTY 1   visual  1   1   24  61   visual  2   1   30  61   visual  3   1   48  71   visual  4   1   18  51   visual  5   1   36  61   visual  6   1   33  31   visual  7   1   27  61   visual  8   1   12  71   visual  9   1   42  71   visual  10  2   48  71   visual  11  2   33  41   visual  12  2   30  41   visual  13  2   42  71   visual  14  2   27  61   visual  15  2   24  61   visual  16  2   12  71   visual  17  2   36  71   visual  18  2   18  71   visual  19  3   30  41   visual  20  3   18  71   visual  21  3   33  61   visual  22  3   27  51   visual  23  3   36  71   visual  24  3   12  71   visual  25  3   48  71   visual  26  3   24  61   visual  27  3   42  7I analyzed the data with a cumulative link model where certainty is an ordinal variable and angle is treated as a continuous predictor variable on an interval scale. Here is the model and output:dataTRIAL-(dataidnum\u0026lt;-as.factor(dataREP) #subject number defined for each repdataCERTAINTY)fm \u0026lt;- clmm(CERTAINTY ~ ANGLE*COND +(trialB|idnum), data = data,Hess=TRUE)summary(fm)Cumulative Link Mixed Model fitted with the Laplace approximationformula: CERTAINTY ~ ANGLE * COND + (trialB | idnum)data:    data link  threshold nobs logLik   AIC     niter      max.grad cond.H  logit flexible  1178 -1834.20 3692.39 1187(3229) 4.02e-02 1.7e+05Random effects: Groups Name        Variance Std.Dev. Corr    idnum  (Intercept) 0.294610 0.54278                 trialB      0.002469 0.04969  -1.000 Number of groups:  idnum 132 Coefficients:                  Estimate Std. Error z value Pr(\u0026gt;|z|)ANGLE            -0.006263   0.007501  -0.835    0.404CONDvisual       -0.096193   0.336045  -0.286    0.775ANGLE:CONDvisual  0.002659   0.010555   0.252    0.801Threshold coefficients:    Estimate Std. Error z value1|2 -4.25987    0.32605 -13.0652|3 -3.20255    0.27404 -11.6873|4 -2.30158    0.25454  -9.0424|5 -1.49539    0.24625  -6.0735|6 -0.66281    0.24224  -2.7366|7  0.05606    0.24112   0.233My problem is that there should be an effect of ANGLE such that certainty is minimal around 30 degrees. However, the output does not confirm that. If I change the ANGLE variable into a factor (i.e. a categorical variable), I will start getting the effects of ANGLE in the output:dataANGLE) fm2 \u0026lt;- clmm(CERTAINTY ~ ANGLE1*COND +(trialB|idnum), data = data,Hess=TRUE)summary(fm2)Cumulative Link Mixed Model fitted with the Laplace approximationformula: CERTAINTY ~ ANGLE1 * COND + (trialB | idnum)data:    data link  threshold nobs logLik   AIC     niter       max.grad cond.H  logit flexible  1178 -1559.89 3171.79 3712(11231) 4.39e-03 3.5e+04Random effects: Groups Name        Variance  Std.Dev. Corr    idnum  (Intercept) 0.7032298 0.83859                 trialB      0.0009656 0.03107  -1.000 Number of groups:  idnum 132 Coefficients:                    Estimate Std. Error z value Pr(\u0026gt;|z|)    ANGLE118             -1.9991     0.5861  -3.411 0.000647 ***ANGLE124             -4.2766     0.5716  -7.482 7.32e-14 ***ANGLE127             -4.8567     0.5754  -8.440  \u0026lt; 2e-16 ***ANGLE130             -4.9210     0.5721  -8.602  \u0026lt; 2e-16 ***ANGLE133             -4.7692     0.5740  -8.309  \u0026lt; 2e-16 ***ANGLE136             -3.9440     0.5726  -6.888 5.67e-12 ***ANGLE142             -2.6715     0.5784  -4.619 3.86e-06 ***ANGLE148             -0.0415     0.7384  -0.056 0.955181    CONDvisual           -1.1651     0.6372  -1.829 0.067467 .  ANGLE118:CONDvisual   0.6994     0.7257   0.964 0.335178    ANGLE124:CONDvisual   1.5613     0.6988   2.234 0.025463 *  ANGLE127:CONDvisual   1.5620     0.6970   2.241 0.025027 *  ANGLE130:CONDvisual   1.0764     0.6945   1.550 0.121154    ANGLE133:CONDvisual   1.1225     0.6978   1.609 0.107689    ANGLE136:CONDvisual   1.2579     0.7033   1.789 0.073695 .  ANGLE142:CONDvisual   1.2967     0.7209   1.799 0.072050 .  ANGLE148:CONDvisual   0.1090     0.8946   0.122 0.902997    ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Threshold coefficients:    Estimate Std. Error z value1|2  -8.2985     0.5918 -14.0242|3  -7.1916     0.5627 -12.7823|4  -6.1824     0.5507 -11.2274|5  -5.1992     0.5437  -9.5635|6  -4.0564     0.5375  -7.5466|7  -2.9241     0.5315  -5.502Which model is the correct one, statistically speaking?  Thanks!Alen","Creater_id":124210,"Start_date":"2016-07-22 13:56:38","Question_id":225193,"Tags":["r","mixed-model","categorical-data","ordinal"],"Answer_count":2,"Last_activity":"2016-08-01 13:03:14","Link":"http://stats.stackexchange.com/questions/225193/cumulative-link-model-with-categorical-or-continuous-predictors","Creator_reputation":23}
{"_id":{"$oid":"5837a588a05283111e4d69ec"},"View_count":23,"Display_name":"ahajib","Question_score":1,"Question_content":"I am trying to use Holt-Winters method in order to do time-series prediction on some datasets. The problem that I'm dealing with is the fact that each dataset has different seasonality value (or frequency basically). As an example, in one dataset the trend occurs every 300 timesteps while for another one this might be a different number. Assuming that I do not have the option of visualizing data each time to find out about the seasonality of my data, what would be the best way for finding the optimal value of frequency to get the best performance from Holt-Winters? Note that I receive about 500 out of 4000 samples to train my model.My approach: Split train data to train and validation subsets. Use different seasonality values on train and evaluate performance on validation. For now, I think I'll use random search to find the best value such that it minimizes the prediction error on validation set. I was wondering if there are any other practical methods or approaches that any of you might be aware of?UPDATE: I have to mention that I have to implement this only in python and not R, so the findfrequency() function from forecast package is applicable here.","Creater_id":117831,"Start_date":"2016-08-01 08:01:43","Question_id":226694,"Tags":["time-series","prediction","frequency"],"Answer_count":0,"Last_activity":"2016-08-01 12:55:03","Link":"http://stats.stackexchange.com/questions/226694/optimal-seasonality-frequency-value-for-holt-winters-in-time-series-prediction","Creator_reputation":132}
{"_id":{"$oid":"5837a588a05283111e4d69ee"},"View_count":18,"Display_name":"www3","Question_score":0,"Question_content":"I was learning about kernel mean embeddings.  I was under the impression that the mean embedding was just the mean of a function with respect to a probability distribution.  Then I saw somewhere that if you have a \"one-hot\" representation of a distribution the kernel embedding becomes a vector.  Is this true?  How can you use these mean embeddings like in SVMs or Ridge Regressions if they are vectors.  Thanks,Cameron","Creater_id":93111,"Start_date":"2016-08-01 12:44:43","Question_id":226748,"Tags":["distributions","kernel-trick","kernel-smoothing"],"Answer_count":0,"Last_activity":"2016-08-01 12:54:02","Link":"http://stats.stackexchange.com/questions/226748/understanding-kernel-mean-embeddings","Creator_reputation":67}
{"_id":{"$oid":"5837a588a05283111e4d69f0"},"View_count":35,"Display_name":"m.a","Question_score":0,"Question_content":"I did a GARCH test for a stock market using daily returns but I faced a problem. Sum of the coefficients in the conditional variance equation exceeds one (so the model is explosive).What should I do? How could I interfere this result?","Creater_id":125094,"Start_date":"2016-07-30 13:31:13","Question_id":226489,"Tags":["garch","non-stationary"],"Answer_count":0,"Last_activity":"2016-08-01 12:52:38","Link":"http://stats.stackexchange.com/questions/226489/explosive-garch-test","Creator_reputation":11}
{"_id":{"$oid":"5837a588a05283111e4d69f2"},"View_count":45,"Display_name":"Lars Ahnland Nordfors","Question_score":1,"Question_content":"I am reading the Rahbek and Mosconi (1998) paper on how to introduce exogenous variables in VEC models. I understand that, in addition to the introduction of the stationary exogenous variables, I should also include them in accumulated form in the cointegration space. Critical values for this can be found in Harbo et al. (1998). I have two questions. How do I accumulate the variables? Is it correct to just add the stationary variables successively one after the other along the time series?Rahbek and Mosconi seem to argue that one has to use the restricted trend version for the asymptotics to hold. At the same time, Herbo et al. gives critical values for all models (trend specifications). If my variables are stationary without constant or trend, I should be able to use critical values stated for a restricted constant model, or? What should I do?","Creater_id":56706,"Start_date":"2016-05-27 00:20:42","Question_id":214931,"Tags":["time-series","var","cointegration","vecm"],"Answer_count":0,"Last_activity":"2016-08-01 12:43:30","Link":"http://stats.stackexchange.com/questions/214931/including-exogenous-variables-in-vecm-rahbek-and-mosconi-1998","Creator_reputation":34}
{"_id":{"$oid":"5837a588a05283111e4d69f4"},"View_count":86,"Display_name":"Jack","Question_score":3,"Question_content":"I was planning to use a graph theory / page-rank approach to find the most influential person in an organization.  Influential person is someone who drives a lot of activity in the  organization. When he assigns a task, most people do it. When he sends  a mail, most people reply to him. When he assigns some training, most  people do it.But there is one drawback to using graph theory here. For example, say Person A is very influential, and Person B has just joined an organization and works under A. B puts in a leave request and sends it to A for approval. A approves the leave request. According to graph theory, B is very influential because A responded to B. But this is not correct. This approach would give very bad results.How can i overcome this limitation? Can anyone suggest an alternative approach?I am using number of interactions to calculate an affinity scores between two users. Based on this, I suggest what content should be shown to a user (like FaceBook does). User B responds to User A most of the time. so (B-\u003eA) has a high affinity score. So, in user B's newsfeed I intend to show content coming from User A.Obviously different type of interactions will have different weights. But is there a better way than just counting number of interactions? If two users A \u0026amp; C have same affinity scores with B, if A is more influential than B, A's content will be shown first to user A.","Creater_id":98374,"Start_date":"2015-12-21 20:30:56","Question_id":187848,"Tags":["graph-theory","social-network"],"Answer_count":2,"Last_activity":"2016-08-01 12:43:21","Link":"http://stats.stackexchange.com/questions/187848/are-there-any-alternatives-to-graph-theory","Creator_reputation":55}
{"_id":{"$oid":"5837a588a05283111e4d6a02"},"View_count":34,"Display_name":"Lars Ahnland Nordfors","Question_score":-1,"Question_content":"Do all variables in a VAR/VEC need to be normally distributed, or only the target variable? It is very hard to get all of them to meet criteria of normality without deleting too many outliers.","Creater_id":56706,"Start_date":"2016-05-30 13:45:42","Question_id":215452,"Tags":["normal-distribution","assumptions","var","cointegration","vecm"],"Answer_count":1,"Last_activity":"2016-08-01 12:40:28","Link":"http://stats.stackexchange.com/questions/215452/do-all-variables-in-a-var-vec-need-to-be-normally-distributed-or-only-the-targe","Creator_reputation":34}
{"_id":{"$oid":"5837a588a05283111e4d6a0e"},"View_count":67,"Display_name":"Andres Azqueta","Question_score":0,"Question_content":"I am trying to understand the coefficients retrieved from running auto.arima in R on my monthly time series of the annual change in House prices. When doing so, I obtain the following outcome:Series: AC.HousePrices ARIMA(1,1,1)(0,0,1)[12] with drift         Coefficients:         ar1      ma1     sma1   drift      0.3243  -0.6592  -0.7892  -6e-04s.e.  0.1733   0.1333   0.1161   4e-04sigma^2 estimated as 0.0008257:  log likelihood=275.22AIC=-540.44   AICc=-539.96   BIC=-526.07To be honest I do not understand why I have two sets of parameters (p,d,q) and (P,D,Q)? The first set (1,1,1) seems to indicate that the series is first-order autoregressive model, nonstationary and with a simple exponential smoothing with drift? What are the second set of values (0,0,1)[12], is it telling me that my series looks yearly seasonal [12]?","Creater_id":111750,"Start_date":"2016-07-12 02:03:30","Question_id":223297,"Tags":["r","time-series","arima","interpretation"],"Answer_count":1,"Last_activity":"2016-08-01 12:35:11","Link":"http://stats.stackexchange.com/questions/223297/interpretation-of-the-arima-coefficients-in-a-time-series","Creator_reputation":117}
{"_id":{"$oid":"5837a588a05283111e4d6a1b"},"View_count":31,"Display_name":"Jeannie","Question_score":1,"Question_content":"I would like to estimate the variance of a time series. Say, if the time series has a period of 24, and I want to estimate the variance using  \\sigma_t^2 = \\frac{1}{2k+1} \\sum^k_{-k} (y_{t+24k} - \\bar{y}_t)^2 where .I found some posts suggest using roll apply, but it seems to only work when estimating variance using neighbourhood data, which does not take the period into account. Does anyone which function to use in this case?","Creater_id":114130,"Start_date":"2016-07-30 12:00:07","Question_id":226479,"Tags":["r","time-series","variance","seasonality","moving-window"],"Answer_count":0,"Last_activity":"2016-08-01 12:29:44","Link":"http://stats.stackexchange.com/questions/226479/use-sliding-window-to-find-variance-for-seasonal-time-series-in-r","Creator_reputation":59}
{"_id":{"$oid":"5837a588a05283111e4d6a1d"},"View_count":95,"Display_name":"Jeannie","Question_score":1,"Question_content":"I have got hourly data from 2012.01.01 to 2016.06.13 and the data shows daily seasonality. I am trying to create a time series object with only daily seasonality in R.data.ts\u0026lt;-ts(mydata,frequency=24,start=2012)However, it seems that R created a time series object from 2012 to 2116. \u0026gt; str(ts3)Time-Series [1:37968] from 2012 to 2116: -6632 -6989 -8344 -8660 -11247 ...I don't know how it works. Can anyone show me what to do with this and how to set the frequency in ts()?","Creater_id":114130,"Start_date":"2016-07-26 12:51:51","Question_id":225774,"Tags":["r","time-series","seasonality","frequency"],"Answer_count":0,"Last_activity":"2016-08-01 12:26:20","Link":"http://stats.stackexchange.com/questions/225774/time-series-object-for-hourly-data-with-daily-seasonality-in-r","Creator_reputation":59}
{"_id":{"$oid":"5837a588a05283111e4d6a1f"},"View_count":12,"Display_name":"jakab922","Question_score":0,"Question_content":"So I would like to see how Scheffé's test after an ANOVA works. It says that we should reject   at significance level  if F_s \u0026gt; (m - 1) F_{\\alpha}(m - 1, N - m)whereF_{s} = \\frac{(\\overline{Y}_{i \\bullet} - \\overline{Y}_{k \\bullet})^2}{MS_{W} \\left( \\frac{1}{n_{i}} + \\frac{1}{n_k} \\right)}where\\overline{Y}_{i \\bullet} = \\sum_{j = 1}^{n_{i}}Y_{i j}where . And lastly MS_{W} = \\frac{\\sum_{i = 1}^{m}\\sum_{j = 1}^{n_i}(Y_{i j} - \\overline{Y}_{i \\bullet})^2}{N - m}Also we have  groups with  sizes in the ANOVA such that .So what I know is that  and (assuming  is true) since( is normal and the difference is also normal) it's the sum of 1 normal variable squared. Also  is the ratio of 2  variables each divided by their own degree of freedom which means that . Even if the F distribution has the  property which I'm not aware of the above formulation makes no sense to me. Could someone explain?EDIT: The above formulation comes from http://www.math.louisville.edu/~pksaho01/teaching/Math662TB-09S.pdf and the original statement for  can be found on page 654.","Creater_id":117721,"Start_date":"2016-08-01 11:59:27","Question_id":226735,"Tags":["anova"],"Answer_count":0,"Last_activity":"2016-08-01 11:59:27","Link":"http://stats.stackexchange.com/questions/226735/scheff%c3%a9-test-has-f-distribution","Creator_reputation":21}
{"_id":{"$oid":"5837a588a05283111e4d6a21"},"View_count":19,"Display_name":"dhalsim2","Question_score":0,"Question_content":"I have an organization that many people would love it join if they only knew of its existence.  People learn of its existence by word of mouth.  This is clearly a model that experiences exponential growth.  I'd like to extrapolate membership based on our historic growth.  What is the best way (formulas, tools, methods, etc.) to accomplish this?  (I have a college degree in computer science, but haven't done anything regarding regression Below are the number of new members added per day since the organization's inception.10/28/2014  1010/29/2014  110/30/2014  110/31/2014  011/1/2014   111/2/2014   011/3/2014   111/4/2014   411/5/2014   211/6/2014   111/7/2014   211/8/2014   011/9/2014   011/10/2014  011/11/2014  011/12/2014  011/13/2014  111/14/2014  011/15/2014  011/16/2014  011/17/2014  011/18/2014  011/19/2014  111/20/2014  011/21/2014  011/22/2014  111/23/2014  011/24/2014  011/25/2014  111/26/2014  011/27/2014  011/28/2014  011/29/2014  011/30/2014  012/1/2014   012/2/2014   012/3/2014   012/4/2014   012/5/2014   112/6/2014   012/7/2014   012/8/2014   112/9/2014   012/10/2014  012/11/2014  012/12/2014  012/13/2014  012/14/2014  112/15/2014  012/16/2014  012/17/2014  012/18/2014  012/19/2014  012/20/2014  012/21/2014  012/22/2014  012/23/2014  012/24/2014  012/25/2014  012/26/2014  012/27/2014  012/28/2014  012/29/2014  012/30/2014  012/31/2014  01/1/2015    01/2/2015    01/3/2015    01/4/2015    01/5/2015    11/6/2015    01/7/2015    01/8/2015    01/9/2015    01/10/2015   01/11/2015   01/12/2015   01/13/2015   01/14/2015   01/15/2015   01/16/2015   21/17/2015   01/18/2015   01/19/2015   01/20/2015   01/21/2015   01/22/2015   01/23/2015   01/24/2015   01/25/2015   01/26/2015   01/27/2015   11/28/2015   01/29/2015   01/30/2015   11/31/2015   02/1/2015    02/2/2015    02/3/2015    12/4/2015    02/5/2015    12/6/2015    12/7/2015    12/8/2015    02/9/2015    02/10/2015   12/11/2015   02/12/2015   02/13/2015   02/14/2015   12/15/2015   02/16/2015   02/17/2015   02/18/2015   02/19/2015   32/20/2015   02/21/2015   12/22/2015   02/23/2015   02/24/2015   02/25/2015   02/26/2015   02/27/2015   02/28/2015   13/1/2015    03/2/2015    03/3/2015    03/4/2015    03/5/2015    03/6/2015    03/7/2015    13/8/2015    03/9/2015    13/10/2015   03/11/2015   13/12/2015   03/13/2015   03/14/2015   03/15/2015   03/16/2015   03/17/2015   03/18/2015   13/19/2015   03/20/2015   03/21/2015   03/22/2015   03/23/2015   03/24/2015   53/25/2015   63/26/2015   13/27/2015   03/28/2015   13/29/2015   03/30/2015   03/31/2015   14/1/2015    04/2/2015    04/3/2015    04/4/2015    04/5/2015    04/6/2015    04/7/2015    04/8/2015    04/9/2015    04/10/2015   14/11/2015   04/12/2015   04/13/2015   04/14/2015   04/15/2015   14/16/2015   04/17/2015   04/18/2015   04/19/2015   14/20/2015   04/21/2015   04/22/2015   04/23/2015   04/24/2015   04/25/2015   04/26/2015   14/27/2015   04/28/2015   04/29/2015   04/30/2015   15/1/2015    15/2/2015    05/3/2015    05/4/2015    05/5/2015    05/6/2015    15/7/2015    05/8/2015    05/9/2015    15/10/2015   05/11/2015   05/12/2015   05/13/2015   05/14/2015   05/15/2015   15/16/2015   05/17/2015   05/18/2015   05/19/2015   05/20/2015   05/21/2015   05/22/2015   05/23/2015   05/24/2015   05/25/2015   15/26/2015   05/27/2015   05/28/2015   05/29/2015   05/30/2015   15/31/2015   06/1/2015    06/2/2015    06/3/2015    06/4/2015    06/5/2015    06/6/2015    06/7/2015    06/8/2015    06/9/2015    06/10/2015   06/11/2015   06/12/2015   06/13/2015   06/14/2015   06/15/2015   06/16/2015   36/17/2015   06/18/2015   06/19/2015   06/20/2015   06/21/2015   06/22/2015   06/23/2015   16/24/2015   06/25/2015   06/26/2015   06/27/2015   06/28/2015   16/29/2015   06/30/2015   27/1/2015    17/2/2015    07/3/2015    07/4/2015    07/5/2015    07/6/2015    07/7/2015    07/8/2015    07/9/2015    07/10/2015   07/11/2015   17/12/2015   07/13/2015   07/14/2015   07/15/2015   07/16/2015   17/17/2015   07/18/2015   07/19/2015   07/20/2015   07/21/2015   07/22/2015   07/23/2015   17/24/2015   07/25/2015   07/26/2015   07/27/2015   07/28/2015   07/29/2015   07/30/2015   07/31/2015   08/1/2015    08/2/2015    08/3/2015    08/4/2015    08/5/2015    08/6/2015    08/7/2015    08/8/2015    18/9/2015    08/10/2015   08/11/2015   18/12/2015   18/13/2015   08/14/2015   08/15/2015   08/16/2015   08/17/2015   08/18/2015   08/19/2015   08/20/2015   18/21/2015   18/22/2015   08/23/2015   08/24/2015   08/25/2015   08/26/2015   18/27/2015   08/28/2015   08/29/2015   08/30/2015   08/31/2015   09/1/2015    09/2/2015    19/3/2015    09/4/2015    09/5/2015    19/6/2015    19/7/2015    09/8/2015    09/9/2015    19/10/2015   09/11/2015   19/12/2015   09/13/2015   09/14/2015   09/15/2015   09/16/2015   09/17/2015   09/18/2015   09/19/2015   09/20/2015   09/21/2015   09/22/2015   09/23/2015   09/24/2015   09/25/2015   09/26/2015   09/27/2015   09/28/2015   09/29/2015   09/30/2015   010/1/2015   010/2/2015   010/3/2015   110/4/2015   010/5/2015   010/6/2015   010/7/2015   010/8/2015   010/9/2015   010/10/2015  010/11/2015  010/12/2015  010/13/2015  110/14/2015  110/15/2015  010/16/2015  010/17/2015  010/18/2015  010/19/2015  010/20/2015  010/21/2015  110/22/2015  010/23/2015  010/24/2015  010/25/2015  010/26/2015  010/27/2015  010/28/2015  010/29/2015  010/30/2015  010/31/2015  011/1/2015   011/2/2015   011/3/2015   011/4/2015   011/5/2015   011/6/2015   011/7/2015   111/8/2015   011/9/2015   011/10/2015  011/11/2015  011/12/2015  011/13/2015  011/14/2015  011/15/2015  111/16/2015  011/17/2015  011/18/2015  011/19/2015  011/20/2015  011/21/2015  011/22/2015  011/23/2015  011/24/2015  011/25/2015  011/26/2015  011/27/2015  011/28/2015  011/29/2015  011/30/2015  012/1/2015   012/2/2015   012/3/2015   112/4/2015   012/5/2015   012/6/2015   012/7/2015   012/8/2015   012/9/2015   112/10/2015  012/11/2015  012/12/2015  012/13/2015  012/14/2015  012/15/2015  112/16/2015  012/17/2015  012/18/2015  012/19/2015  012/20/2015  012/21/2015  012/22/2015  012/23/2015  012/24/2015  112/25/2015  012/26/2015  112/27/2015  012/28/2015  012/29/2015  112/30/2015  012/31/2015  01/1/2016    01/2/2016    01/3/2016    01/4/2016    01/5/2016    01/6/2016    01/7/2016    01/8/2016    11/9/2016    01/10/2016   01/11/2016   01/12/2016   01/13/2016   01/14/2016   01/15/2016   01/16/2016   11/17/2016   01/18/2016   01/19/2016   01/20/2016   01/21/2016   01/22/2016   01/23/2016   01/24/2016   11/25/2016   01/26/2016   01/27/2016   01/28/2016   01/29/2016   01/30/2016   11/31/2016   02/1/2016    12/2/2016    02/3/2016    02/4/2016    02/5/2016    12/6/2016    02/7/2016    12/8/2016    02/9/2016    02/10/2016   02/11/2016   02/12/2016   02/13/2016   12/14/2016   02/15/2016   02/16/2016   12/17/2016   02/18/2016   22/19/2016   02/20/2016   02/21/2016   02/22/2016   02/23/2016   02/24/2016   02/25/2016   02/26/2016   02/27/2016   12/28/2016   02/29/2016   03/1/2016    03/2/2016    13/3/2016    13/4/2016    03/5/2016    03/6/2016    03/7/2016    03/8/2016    03/9/2016    03/10/2016   13/11/2016   03/12/2016   03/13/2016   03/14/2016   03/15/2016   03/16/2016   03/17/2016   03/18/2016   03/19/2016   23/20/2016   03/21/2016   23/22/2016   23/23/2016   13/24/2016   03/25/2016   03/26/2016   23/27/2016   13/28/2016   03/29/2016   03/30/2016   13/31/2016   24/1/2016    04/2/2016    14/3/2016    04/4/2016    04/5/2016    14/6/2016    04/7/2016    04/8/2016    04/9/2016    14/10/2016   14/11/2016   04/12/2016   04/13/2016   04/14/2016   04/15/2016   04/16/2016   04/17/2016   04/18/2016   04/19/2016   04/20/2016   04/21/2016   04/22/2016   04/23/2016   04/24/2016   04/25/2016   04/26/2016   04/27/2016   04/28/2016   04/29/2016   04/30/2016   05/1/2016    05/2/2016    05/3/2016    05/4/2016    35/5/2016    15/6/2016    15/7/2016    05/8/2016    05/9/2016    15/10/2016   55/11/2016   45/12/2016   115/13/2016   35/14/2016   105/15/2016   05/16/2016   05/17/2016   55/18/2016   75/19/2016   25/20/2016   15/21/2016   25/22/2016   25/23/2016   25/24/2016   25/25/2016   45/26/2016   25/27/2016   15/28/2016   15/29/2016   35/30/2016   05/31/2016   06/1/2016    16/2/2016    16/3/2016    26/4/2016    06/5/2016    06/6/2016    16/7/2016    26/8/2016    16/9/2016    26/10/2016   06/11/2016   06/12/2016   06/13/2016   16/14/2016   16/15/2016   16/16/2016   16/17/2016   06/18/2016   06/19/2016   16/20/2016   06/21/2016   06/22/2016   46/23/2016   56/24/2016   26/25/2016   16/26/2016   26/27/2016   16/28/2016   36/29/2016   56/30/2016   17/1/2016    27/2/2016    57/3/2016    17/4/2016    57/5/2016    67/6/2016    37/7/2016    67/8/2016    117/9/2016    47/10/2016   17/11/2016   37/12/2016   57/13/2016   147/14/2016   57/15/2016   27/16/2016   67/17/2016   47/18/2016   37/19/2016   67/20/2016   167/21/2016   77/22/2016   97/23/2016   107/24/2016   67/25/2016   147/26/2016   67/27/2016   127/28/2016   117/29/2016   32","Creater_id":125279,"Start_date":"2016-08-01 11:57:29","Question_id":226734,"Tags":["time-series","forecasting","count-data"],"Answer_count":0,"Last_activity":"2016-08-01 11:58:41","Link":"http://stats.stackexchange.com/questions/226734/extrapolation-based-on-exponential-growth","Creator_reputation":101}
{"_id":{"$oid":"5837a588a05283111e4d6a23"},"View_count":4,"Display_name":"RTrain3K","Question_score":0,"Question_content":"I have a binary matrix that I would like to segment. Each row is a respondent and the columns are Yes/No questions. Every respondent is asked the same set questions and then asked an additional subset of questions from a larger set. The scheme for asking the additional subset of questions ensures that questions co-occur equally and appear the same amount of times across the data set. For the sake of example, suppose respondents are all asked the same 3 questions and then each respondent answers an additional 3 questions from a set of 6 questions based upon the aforementioned scheme. So essentially the matrix has missing observations (I believe the term is \"Missing at Random\" (MAR)).My current approach would be to use a latent class model and maximize the likelihood function using the available data, i.e., no observations would be delete. I believe that non-negative matrix factorization may be a second approach but I have yet to read about how it deals with observations MAR. I came across it from a blog post by Joel Cadwell ","Creater_id":82576,"Start_date":"2016-08-01 11:48:08","Question_id":226733,"Tags":["binary-data","matrix","segmentation","latent-class","nnmf"],"Answer_count":0,"Last_activity":"2016-08-01 11:48:08","Link":"http://stats.stackexchange.com/questions/226733/best-approach-to-segment-binary-matrix-with-observations-missing-at-random","Creator_reputation":57}
{"_id":{"$oid":"5837a588a05283111e4d6a25"},"View_count":35,"Display_name":"Bowtiesarecool900","Question_score":2,"Question_content":"I am taking my first Biostatistics class and I have a question about sample size. If I for example, need to detect mean increases in BMD of 0.04 or more in the proportional change from baseline in BMD, with 80% power and a previous study indicates that proportional change from baseline in BMD at two months has a standard deviation . How many subjects do I need to enroll?I am trying to decipher what my  is, since I am assuming that my  is 0.04.BMD= Bone Mass Density","Creater_id":125175,"Start_date":"2016-07-31 19:01:39","Question_id":226605,"Tags":["self-study","sample"],"Answer_count":1,"Last_activity":"2016-08-01 11:36:09","Link":"http://stats.stackexchange.com/questions/226605/sample-size-question-from-a-newbie","Creator_reputation":11}
{"_id":{"$oid":"5837a588a05283111e4d6a32"},"View_count":65,"Display_name":"Philip O\u0026#39;Brien","Question_score":2,"Question_content":"I have a dataset of Total Sales from 2008-2015. I have an entry for each day, and so I have a created a pandas DataFrame with a DatetimeIndex and a column for sales. So it looks like thisThe problem is that I am missing data for most of 2010. These missing values are currently represented by 0.0 so if I plot the DataFrame I getI want to try forecast values for 2016, possibly using an ARIMA model, so the first step I took was to perform a decomposition of this time seriesObviously if I leave 2010 in the DataFrame any attempted prediction will be skewed by the apparent, albeit erroneous, drop in sales. What is the recommended approach in this situation? I think I should just drop 2010 altogether, but then I don't know if my time series is valid going from 2009 to 2011. I don't want to fill the missing values, because I don't believe I can do so accurately.If I just delete 2010, however, the plot 'fills in' 2010 which doesn't help mesales = sales.drop(sales['2010'].index)","Creater_id":54787,"Start_date":"2016-07-01 04:06:18","Question_id":221658,"Tags":["time-series","forecasting","predictive-models"],"Answer_count":1,"Last_activity":"2016-08-01 11:34:14","Link":"http://stats.stackexchange.com/questions/221658/pandas-time-series-dataframe-missing-values","Creator_reputation":111}
{"_id":{"$oid":"5837a588a05283111e4d6a3f"},"View_count":93,"Display_name":"Emiliano A. Carlevaro","Question_score":1,"Question_content":"I have a monthly unbalanced data panel of balance sheet data of 70 banks (on average 170 observations per bank) over 20 years.I have autocorrelation and heteroskedasticity within panels.I am trying to test the hyphotesis that there is a relation between transactional deposits and credit commitment (limit of line of credit). I would like to say \"all being equal, on average a bank with more transactional deposits would offer more credit commitments to their clients\". Namely, the argument is that in order to \"produce\" credit commitments a bank needs transactional depostis.In another step of the research the final goal is to build some kind of \"production function\" for credit commitments. My dependent variable is the ratio of credit commitments to total loans (comitRatio). My independent variable (regressor) is the ratio of transactional deposits to total deposits (depRatio). I am using natural logarithms of these variable because I would like to interpret estimated coefficients as elasticities. So I have ln(comitRatio) and ln(depRatio).where  are control variables, most of them time-invariant.I chose to use moving average of 3, 6 and 12 months for independent variable ln(depRatio). This is because the hypothesis states that, at point  in time, commitments offer (comitRatio) is decided base on depRatio from previous periods.Also, since the regressor depRatio for some banks (particulary the smaller ones) varies too much from one month to the other I suppose a moving average would show me a better picture.My main concern is the ceteris paribus effect of the independent variable over the dependent.I am working with a fixed effect model with autocorrelation (AR(1) model) and robust standard error (xtpcse command in Stata).The model works better (higher , higher coefficient estimates, higher -statistics) when I use moving average than when I use the original values (even when I use lagged values such us L.depRatio). My question is, given this setup, can I use moving average only on independent variable?Shall I smooth independent as well as dependent variable, or would it not be advisable to smooth the dependent variable?","Creater_id":124100,"Start_date":"2016-07-29 18:15:45","Question_id":226391,"Tags":["panel-data","smoothing","moving-average"],"Answer_count":1,"Last_activity":"2016-08-01 11:11:02","Link":"http://stats.stackexchange.com/questions/226391/can-i-smooth-the-dependent-variable-with-a-moving-average-in-an-ols-or-feasible","Creator_reputation":8}
{"_id":{"$oid":"5837a588a05283111e4d6a4a"},"View_count":93,"Display_name":"user4137941","Question_score":1,"Question_content":"(Please note I have read through \"How to handle changing input vector length with neural networks\" but somehow this is different)As most of us know, financial trade data exhibit different volume and velocity depending on predictable and unpredictable factors, from national holidays to wars. The fine detail of these financial signals is mostly considered \"noise\" and a lot of conclusions can be drawn by so-called timeframes, i.e. averages and aggregates over 1 minute, 1 day, or even 1 year. I would like to hunt for the \"signal in the noise\" with some deeplearning package, but variable length vectors do not seem to be an option. I can think of workarounds, and simply adding the length of the vector (the number of ticks) to a standardized timeframe is not bad at all, but I am not happy with my workarounds. Certainly a spectral parametric description would go a long way towards capturing the waveform in a fixed vector length, but I would be happy to hear of a \"natural way\"-  ","Creater_id":125275,"Start_date":"2016-08-01 10:57:20","Question_id":226729,"Tags":["neural-networks","deep-learning","finance"],"Answer_count":0,"Last_activity":"2016-08-01 10:57:20","Link":"http://stats.stackexchange.com/questions/226729/training-neural-networks-on-variable-length-vectors","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d6a4c"},"View_count":55,"Display_name":"ruthy_gg","Question_score":0,"Question_content":"Following the post here, I came with another issue that is related to another topic. I am using PCA to use K number of principal components as exogenous regressors to use in an auto.arima model in R. These principal components are input in the parameter \"xreg.\" The main problem I am having now is that the number of variables is larger than the number of observations of the data I am trying to fit. So when I choose, say 5 PCA, the number of rows is bigger than the number of observations in auto.arima and an error appears as:Error in model.frame.default(formula = x ~ xreg, drop.unused.levels = TRUE) :   variable lengths differ (found for 'xreg')In addition: Warning message:In !is.na(x) \u0026amp; !is.na(rowSums(xreg)) :  longer object length is not a multiple of shorter object lengthOne way of solving this problem is to choose the number of rows with higher absolute values, so that they match the number of observations in the auto.arima. What do you think?","Creater_id":124847,"Start_date":"2016-07-31 13:51:33","Question_id":226587,"Tags":["pca","arima","error"],"Answer_count":1,"Last_activity":"2016-08-01 10:45:19","Link":"http://stats.stackexchange.com/questions/226587/exogenous-regressors-using-pca-variable-lengths-differ-in-auto-arima","Creator_reputation":31}
{"_id":{"$oid":"5837a588a05283111e4d6a59"},"View_count":59,"Display_name":"Mari","Question_score":1,"Question_content":"I am working on developing a particle filter to improve the results of an Unscented Kalman filter (UKF) to satellite attitude determination. The UKF outputs a 12-dimensional joint normal distribution (the proposal distribution), from which I randomly sample N points (particles) (currently N=100). Then the particle filter uses the observation data to assign weights to the particles (more likely to produce the observations =\u003e more weight to the particle), and then I calculate the new vector of weighted means and the weighted unbiased sample covariance matrix using the formula given here: https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Weighted_samplesThen the vector of means and the covariance matrix are fed into the UKF (it treats them as if they came from a normal distribution) and it produces a new joint normal distribution output, and the cycle is repeated for each timestep.The particle filter seems to be working very well and it's improving the results of the Kalman filter (the results are similar, but more smooth). However, thanks to the weights assigned during the particle filter step, the variances and covariances become very small, until they're approximated as 0 by the code and the whole thing crashes after a certain number of timesteps. (The variance/covariance becoming too small is also an inherent problem, as the filter can never be 100% exact.)In a sense, this is to be expected, since the weights assigned by the particle filter often give more importance to the points closer to the mean (since ideally the mean is close to the observation data), which then contribute less to the variance/covariance. Is there a way to artificially keep the covariance matrix from becoming too small? Or is there something wrong with using the particle filter this way? Thoughts?","Creater_id":124981,"Start_date":"2016-07-29 06:47:35","Question_id":226291,"Tags":["covariance-matrix","joint-distribution","kalman-filter","multivariate-normal","particle-filter"],"Answer_count":1,"Last_activity":"2016-08-01 10:10:55","Link":"http://stats.stackexchange.com/questions/226291/particle-filter-reducing-the-covariance-matrix-of-joint-normal-distribution-too","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d6a5c"},"View_count":223,"Display_name":"user75138","Question_score":12,"Question_content":"I've recently reviewed some old papers by Nancy Reid, Barndorff-Nielsen, Richard Cox and, yes, a little Ronald Fisher on the concept of \"conditional inference\" in the frequentist paradigm, which appears to mean that inferences are based considering only the \"relevant subset\" of the sample space, not the entire sample space.As a key example, it is known that the confidence intervals based on the t-statistic can be improved (Goutis \u0026amp; Casella, 1992) if you also consider the sample's coefficient of variation (referred to as an ancillary statistic).  As someone who regularly uses likelihood-based-inference, I have assumed that when I form an asymptotic %-confidence interval, I am performing (approximate) conditional inference, since the likelihood is conditional on the observed sample.My question is that, apart from conditional logistic regression, I have not seen much use of the idea of conditioning on ancillary statistics prior to inference. Is this type of inference restricted to exponential families, or is it going by another name nowadays, so that it only appears to be limited.   I found a more recent article (Spanos, 2011) that seems to cast serious doubt about the approach taken by conditional inference (i.e., ancillarity). Instead, it proposes the very sensible, and less mathematically convoluted suggestion that parametric inference in \"irregular\" cases (where the support of the distribution is determined by the parameters) can be solved by truncating the usual, unconditional sampling distribution.Fraser (2004) gave a nice defense of conditionality, but I am still left with the feeling that more than just a little luck and ingenuity are required to actually apply conditional inference to complex cases...certainly more complex than invoking the chi-squared approximation on the likelihood ratio statistic for \"approximate\" conditional inference.Welsh (2011, p. 163) may have answered my question (3.9.5, 3.9.6).They point out Basu's well-known result (Basu's theorem) that there can be more than one ancillary statistic, begging the question as to which \"relevant subset\" is most relevant. Even worse, they show two examples of where, even if you have a unique ancillary statistic, it does not eliminate the presence of other relevant subsets.They go on to conclude that only Bayesian methods (or methods equivalent to them) can avoid this problem, allowing unproblematic conditional inference.References:Goutis, Constantinos, and George Casella. \"Increasing the confidence in Student's  interval.\" The Annals of Statistics (1992): 1501-1513.Spanos, Aris. \"Revisiting the Welch Uniform Model: A case for Conditional Inference?.\" Advances and Applications in Statistical Science 5 (2011): 33-52.Fraser, D. A. S. \"Ancillaries and conditional inference.\" Statistical Science 19.2 (2004): 333-369.Welsh, Alan H. Aspects of statistical inference. Vol. 916. John Wiley \u0026amp; Sons, 2011.","Creater_id":null,"Start_date":"2016-07-19 21:43:20","Question_id":224653,"Tags":["confidence-interval","maximum-likelihood","inference"],"Answer_count":1,"Last_activity":"2016-08-01 10:08:29","Link":"http://stats.stackexchange.com/questions/224653/is-frequentist-conditional-inference-still-being-used-in-practice","Creator_reputation":null}
{"_id":{"$oid":"5837a588a05283111e4d6a5e"},"View_count":21,"Display_name":"MattCrow","Question_score":0,"Question_content":"I have a data set with zip codes, number of diagnostic tests, and number of procedures for each zip code. I'm interested in quantifying the \"overlap\" between the zip codes that do lots of tests and lots of procedures, as well as a distance-between spatial analysis.Approach:Determine zip centroids for each zip code.Compare distances between zip codesPerform cluster analysis on zip codes \u0026amp; counts of diagnostics and procedures.Spatial analysis?I'm getting started with R, and was thinking of doing a cluster analysis to see if this would be one way to report on this potential count and spatial relationship. Would that be the best approach?EDIT:By overlap, I mean:1)the volume of diagnostics and procedures for a given zip code. I.e. if a given zip code has a a high number of diagnostics, do they also have a high number of procedures? I thought I could do a simple persons or spearman correlation for this. 2)Are there clustered zip codes close in proximity that seem to have either high diagnostics, high procedures, or both?","Creater_id":125265,"Start_date":"2016-08-01 09:50:00","Question_id":226714,"Tags":["r","spatial","biostatistics"],"Answer_count":0,"Last_activity":"2016-08-01 09:59:01","Link":"http://stats.stackexchange.com/questions/226714/zip-codes-diagnostic-testing-versus-surgical-procedure-counts-best-way-to-ana","Creator_reputation":26}
{"_id":{"$oid":"5837a588a05283111e4d6a60"},"View_count":2711,"Display_name":"ash","Question_score":2,"Question_content":"I have three variables (,, and ), each of which is normally distributed. Is there a way to show the overlap between each of the pairs using the mean and the standard deviation? An independent t-test shows that the three groups are very much significantly different, which is good, but I want a way to show that one pair is most different (while two other pairs are equally different)To show what I mean graphically I've plotted the distributions below;i.e. red and green are very different, while blue and green are less different. Is there a good way to quantify this difference in overlap? The t-statistic for red and blue is 16.6789, while the t-statistic for green and red is 38.9686. I do find this kind of odd, given how much overlap red and blue seem to have though...What I'd like is to be able to directly calculate the information presented in the nomogram here. However, I guess I'd thought there would be standard implementations in matlab/R to do this, but couldn't find any.","Creater_id":11038,"Start_date":"2013-07-07 21:48:32","Question_id":63604,"Tags":["normal-distribution","t-test"],"Answer_count":2,"Last_activity":"2016-08-01 09:57:01","Link":"http://stats.stackexchange.com/questions/63604/get-overlap-between-two-normally-distributed-variables","Creator_reputation":111}
{"_id":{"$oid":"5837a588a05283111e4d6a62"},"View_count":107,"Display_name":"Tim K.","Question_score":1,"Question_content":"I have a question concerning the coding of Effect Sizes for a large (educational) Meta Analysis with mostly latent outcome variables. Some studies provide \"Intent to Treat\" Data from which I calculate Effect Sizes (Hedges g) and some studies only provide (local average treatment effect) LATE or TOT (treatment on treated) Data. Some studies report both. Is it okay to mix ITT and LATE Treatment-Effects-Data or should I stick to one type of treatment operationalization? My meta analysis is concerned with educational interventions and since we seek to explain the variability in treatment success of these interventions afterwards through Intervention characteristics I usually would prefer to include the estimates of the subjects who really completed the intervention (rather than who were assigned to the treatment).","Creater_id":62013,"Start_date":"2014-12-03 06:47:32","Question_id":126452,"Tags":["meta-analysis","effect-size","treatment-effect","meta-regression","systematic"],"Answer_count":1,"Last_activity":"2016-08-01 09:55:04","Link":"http://stats.stackexchange.com/questions/126452/mixing-itt-and-tot-late-effect-sizes-in-meta-analysis","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d6a64"},"View_count":128,"Display_name":"rg255","Question_score":0,"Question_content":"I am using mixed effects Cox models in the R package coxme, with the model SurvObj ~ Sex*NE + (1|Year)where Sex is a categorical fixed effect with two levels each (M and F), NE is a continuous predictor of particular interest, and Year the sampling year used as a random effect.Here's dummy data/script with longer lifespan in Sex = M, and a positive effect of NE on lifespan:set.seed(1239)dumDF = data.frame(    \"Sex\" = rep(c(\"M\",\"F\"), each = 1000),     \"Year\" = sample(c(\"A\",\"B\"), replace = T),     \"NE\" = rnorm(2000,0,5),     \"Status\" = sample(c(1,2,2,2), 2000, replace = T),     \"Life\" = rnorm(2000, 50, 5))dumDFSex==\"M\", dumDFNE, dumDFNE)dumDF coef_{F} = b_1 + b_3 \\times x coef_{M} = (b_1 + b_2) + (b_3 + b_4) \\times xb_1b_2b_1b_3b_4$ is the M specific slope (\"SexM:NE\") relative to the F specific slope.Also how does one interpret the number (coef), given that SurvObj is the response, would a negative coefficient for NE mean that there is negative relationship between NE and survival (higher NE = short lifespan/higher rate of mortality) or negative relationship between NE and mortality (higher NE = long lifespan/lower rate of mortality)? Following from my dummy data, where I know NE is positively associated with lifespan, then I believe the coefficient would represent mortality rate, because NE is negative in the example, thus mortality rate decreases with increasing NE.","Creater_id":16542,"Start_date":"2016-08-01 02:36:02","Question_id":226649,"Tags":["regression","cox-model","hazard"],"Answer_count":1,"Last_activity":"2016-08-01 09:32:16","Link":"http://stats.stackexchange.com/questions/226649/intercept-in-mixed-effects-cox-regression-coxme-in-r","Creator_reputation":344}
{"_id":{"$oid":"5837a588a05283111e4d6a66"},"View_count":52,"Display_name":"amit_kumar","Question_score":1,"Question_content":"I am new to machine learning and I am trying to implement LDA using Spark Mllib but I am confused about corpus.What exactly is a corpus of a document? Is it created document-wise or as a whole for all documents?","Creater_id":108424,"Start_date":"2016-07-31 11:49:23","Question_id":226579,"Tags":["terminology","discriminant-analysis","topic-models","spark-mllib"],"Answer_count":1,"Last_activity":"2016-08-01 09:24:42","Link":"http://stats.stackexchange.com/questions/226579/what-is-a-corpus-in-topic-modeling","Creator_reputation":108}
{"_id":{"$oid":"5837a588a05283111e4d6a68"},"View_count":37,"Display_name":"Annamarie","Question_score":1,"Question_content":"This is sort of a repost of this question, but there weren't any answers so I will ask it again and I will try to make it clear what my problem is.My eventual aim is to find out the relationship between one independent variable and many dependent variables. Because I have many of these dependent variables I decided to use an Exploratory Facor Analysis to first reduce the number of items to fewer latent variables. Also, all these dependent variable are questionaires investigating similar, but not the same thing. And then I wanted to check whether my independent variable  can predict some of the latent variables yielded by the EFA. Now, I wonder whether I can throw my independent variable into the EFA as well and see on which latent variable it has the highest loading on? Or is that entirely unusual to do? And would it mess up my latent variables? ","Creater_id":73581,"Start_date":"2016-08-01 08:43:24","Question_id":226705,"Tags":["factor-analysis"],"Answer_count":1,"Last_activity":"2016-08-01 09:16:52","Link":"http://stats.stackexchange.com/questions/226705/exploratory-factor-analysis-with-additional-variable","Creator_reputation":11}
{"_id":{"$oid":"5837a588a05283111e4d6a6a"},"View_count":1471,"Display_name":"Fuca26","Question_score":3,"Question_content":"I have found many useful posts about standardized independent variables and centered independent variables on stats.stackexchange.com, but I am still a bit confused. I am asking you an evaluation of what I have understood. Also, if what follows is not correct, could you please correct me?How to standardize. Standardized variables are obtained by subtracting the mean of the variable and by dividing by the standard deviation of that same variable. How to center. Centered independent variables are obtained just by subtracting the mean of the variable.The reason for standardizing. You standardize variables to facilitate the interpretation of the estimated coefficients when the variables in your regression have different units of measurement. When you want to standardize, you have to standardize all the variables in the regression--which implies you won't get an estimate of the constant (i.e., the B0 or intercept).The reason for centering. You center variables if you want to gain a meaningful interpretation of the estimated constant. In this case, you can center the amount of variables you want to; you do not need to center all the independent variables in the model.The independent variable, Y. (plain question) Do you ever center or standardize the Y?Natural logarithm utilization. If one or more of your variables are not normally distributed, you can transform them using the natural logarithm. Only AFTER this transformation you may either standardize all the variables or center those that you need to center. In general, whatever transformation of a variable has to happen before standardizing or centering (here I speak about natural logarithm, but you could square a variable or divide a variable by another one, e.g., population/km2)Interpretation coefficients standardized variables. \"An increase by 1 standard deviation in X1 will increase (or decrease) Y by -number-.\"Interpretation coefficients centered variables. Coefficients of random variables: \"An increase in X1 by -number- from its mean will increase (or decrease) Y by -number-.\" Constant: \"It represents the expected value of Y when the non-centered variables are zero and when the centered variables are at their mean.\"Interaction terms. The interpretation of the coefficient of an interaction term should not be problematic, whether you have standardized your variables, or centered them (either only one variable of the interaction or both of them). Basically, the interpretation is that that you normally give to an interaction term (e.g., you are interested in the effect of X1 on Y and X1 is interacted with X2, the total effect of X1 is given by its coefficient + coeff. of the interaction term when X2 is fixed), just remember to contextualize the interpretation following either point 7 or 8, depending on the type of transformation you did.","Creater_id":20921,"Start_date":"2015-06-13 05:49:47","Question_id":156791,"Tags":["regression","data-transformation","interpretation","standardization","centering"],"Answer_count":0,"Last_activity":"2016-08-01 09:07:21","Link":"http://stats.stackexchange.com/questions/156791/standardized-vs-centered-variables","Creator_reputation":107}
{"_id":{"$oid":"5837a588a05283111e4d6a6c"},"View_count":58,"Display_name":"Toney Shields","Question_score":2,"Question_content":"I have read the definitions and basics of random graphs, but I don't really understand what is a random graph, and what's a \"non-random\" graph.A non mathematical explanation would be of great help.","Creater_id":109437,"Start_date":"2016-08-01 06:58:55","Question_id":226679,"Tags":["graph-theory","social-network"],"Answer_count":1,"Last_activity":"2016-08-01 08:32:05","Link":"http://stats.stackexchange.com/questions/226679/what-is-meant-by-random-graph","Creator_reputation":197}
{"_id":{"$oid":"5837a588a05283111e4d6a6e"},"View_count":115,"Display_name":"www3","Question_score":1,"Question_content":"I have about 7000000 patents that I would like to do find the document similarity of.  Obviously with a sample set that big it will take a long time to run.  I am just taking a small sample of about 5600 patent documents and I am preparing to use Doc2vec to find similarity between different documents.  From many of the examples and the Mikolov paper he uses Doc2vec on 100000 documents that are all short reviews.  My documents are much longer than reviews, like 3000+ words each, but I have way fewer of them.  Should I still use Doc2vec on this limited sample set?  Or should I use something like Word Mover Distance and Word2Vec since I have perhaps almost as many words as Mikolov's paper but fewer documents.  Gensim has pre-trained Word2vec.  I don't really understand Doc2vec/Word2vec very well, but can I use that corpus to train Doc2vec?  Anyone have any suggestions?Note:  I have already implemented LDA/LSI and cosine sim of: TF-IDF.  I'm looking to see which method gets the most accurate similarity measure so I can test similarity measures over time.   ","Creater_id":93111,"Start_date":"2016-08-01 07:54:10","Question_id":226692,"Tags":["machine-learning","python","natural-language","doc2vec"],"Answer_count":0,"Last_activity":"2016-08-01 08:14:59","Link":"http://stats.stackexchange.com/questions/226692/doc2vec-for-large-documents","Creator_reputation":67}
{"_id":{"$oid":"5837a588a05283111e4d6a70"},"View_count":86,"Display_name":"AmirSalameh","Question_score":2,"Question_content":"Let's say I have a model in R, a regression tree created by glm,using the data1 dataframe:Model1 = glm(DepVar ~ . ,data=data1,family=\"binomial\")Is there a way to train the model with additional new data that will be coming, say data2 , without having data1 ?In other words, is there a way to create a new model Model2 that is based only on Model1 and data2, without keeping data1? ","Creater_id":48441,"Start_date":"2014-06-16 04:38:42","Question_id":103536,"Tags":["r","regression","machine-learning"],"Answer_count":0,"Last_activity":"2016-08-01 08:08:54","Link":"http://stats.stackexchange.com/questions/103536/online-model-training-in-r-without-a-static-data","Creator_reputation":14}
{"_id":{"$oid":"5837a588a05283111e4d6a72"},"View_count":61,"Display_name":"Anthony W","Question_score":1,"Question_content":"I have random variables  that are IID, and are normally distributed with mean  and variance . I want to find the distribution of the minimum value, in the set , let's call it Y.I'm aware of the answer as listed here : How is the minimum of a set of random variables distributed?My problem is that for a normally distributed variable , its cdf is given by: which has no analytical solution. Furthermore, even if I did have the answer to an approximation to the integral, I'm not sure how to calculate  the distribution of .","Creater_id":125239,"Start_date":"2016-08-01 07:07:08","Question_id":226682,"Tags":["normal-distribution","random-variable","order-statistics"],"Answer_count":1,"Last_activity":"2016-08-01 07:51:16","Link":"http://stats.stackexchange.com/questions/226682/minimum-of-a-set-of-random-variables-that-are-normally-distributed","Creator_reputation":106}
{"_id":{"$oid":"5837a588a05283111e4d6a74"},"View_count":14,"Display_name":"Kuti","Question_score":0,"Question_content":"A (temporal/spatial) bispectrum is defined as a Fourier transform of a (temporal/spatial) triple correlation function (for temporal one):where  is the Fourier transform,  the temporal correlation function for a single component , and  the Fourier transform of the signal , namely a time-series of the single component.Herein, what is the units of the (temporal) bispectrum?","Creater_id":116458,"Start_date":"2016-08-01 07:43:38","Question_id":226688,"Tags":["correlation","spectral-analysis"],"Answer_count":0,"Last_activity":"2016-08-01 07:43:38","Link":"http://stats.stackexchange.com/questions/226688/what-is-the-units-of-a-temporal-or-spatial-bispectrum","Creator_reputation":23}
{"_id":{"$oid":"5837a588a05283111e4d6a76"},"View_count":48,"Display_name":"KevinKim","Question_score":1,"Question_content":"I have seen two ways to conduct randomization in an experiment. I am confused about the difference between them and which one is the correct one.Assume we have an even number of subjects (say 20).Method 1. let each subject flip a coin, if it is Head, then go to group A, otherwise, go to group B. This procedure stops until one group has 10 people, then the rest subjects who haven't flip the coin, all go to the other group.This method will guarantee that each group will have exactly the same number of people. Think about the extreme case, when n = 2Method 2. let all subjects flip a coin, if it is Head, then go to group A, otherwise, go to group B.This method will very likely result in the case that group A and B do not have same number of subjects.Which method is the correct randomization in experiment? And why the other method is wrong?My hunch is that the Method 2 is correct, but I don't know what's wrong with Method 1. Especially, if n=2 (for theoretical purpose), then I would favor Method 1.My idea is the following: in order to claim causality in the end, we have to make sure that each subject has same probability of being assigned to group A and B. The Method 2 can guarantee this. However, the situation for Method 1 is tricky. Namely, before the first guy has flipped the coin in Method 1, it is indeed that all subjects have same probability of being assigned to Group A and B. However, once the first guy has flipped the coin and is assigned to one group, say Group A, then the last guy's chance of being assigned to Group A is less than the chance of being assigned to Group B. ","Creater_id":66461,"Start_date":"2016-07-31 19:02:09","Question_id":226606,"Tags":["experiment-design","randomization"],"Answer_count":1,"Last_activity":"2016-08-01 07:16:53","Link":"http://stats.stackexchange.com/questions/226606/randomization-in-experiment-design","Creator_reputation":1640}
{"_id":{"$oid":"5837a588a05283111e4d6a78"},"View_count":20,"Display_name":"Frederick1214","Question_score":0,"Question_content":"Is it considered correct to estimate the absolute risk from a poisson model as exp(B0+B1) where B0 is the intercept and B1 is the coefficient of a binary variable?It seems logical that in a model with a single binary independent variable with that exp(B0) would estimate the absolute risk in the reference group and exp(B0+B1) the absolute risk in the modeled group. It also makes sense that this would not extend to models with 2 or more predictors unless we assume that the absolute risk for the reference group of all independent variables is equal.Please tell me if I'm wrong and if there is a better or standard way of assessing the absolute risk even if that is the crude calculation.","Creater_id":103554,"Start_date":"2016-08-01 07:16:16","Question_id":226684,"Tags":["poisson-regression","absolute-risk"],"Answer_count":0,"Last_activity":"2016-08-01 07:16:16","Link":"http://stats.stackexchange.com/questions/226684/is-it-possible-to-estimate-absolute-risk-from-a-poisson-model","Creator_reputation":106}
{"_id":{"$oid":"5837a588a05283111e4d6a7a"},"View_count":49,"Display_name":"Mo Bro","Question_score":0,"Question_content":"I try to find additive and innovative outliers in the German Stock Index (DAX) using the method Doornik \u0026amp; Ooms explained in 2002:Estimate the baseline GARCH model to obtain log-likelihood () and residuals.Find the largest (in absolute value) standardized residual at .Estimate the extended GARCH model with dummy  if  in the mean, and  in the variance.This gives estimates for the added parameters and log-likelihood ().If  then terminate: no further outliers are present with.Here  and  is the number of observations.The data is the DAX (Deutscher Aktienindex) from 2014-06-02 till 2016-01-01 and I got it via Datastream cause pdfetch did not work proper at that time.My question is, how do I distinguish between the  dummy in the mean model and the  dummy in the variance model within the extended GARCH model?My code so far:    # Preparation:    library(\"rugarch\")    library(\"tseries\")    library(\"xts\")    dax \u0026lt;-read.csv2(\"~/Bachelorarbeit/Daten/DAXINDX_Time_Series_010114_010116_final.csv\", stringsAsFactors=FALSE)    dax_xts\u0026lt;-xts(dax, order.by=as.Date.character(daxDate=NULL #Remove \"Date\"-Column    storage.mode(dax_xts)\u0026lt;- \"numeric\"    colnames(dax_xts)\u0026lt;-c(\"Dax\") #Rename Column-Names    dax.logs.prep\u0026lt;-diff(log(daxDate[-1]    dax.logs\u0026lt;-data.frame(dax.date,dax.logs.prep)    dax_ret\u0026lt;-xts(dax.logs, order.by=as.Date.character(dax.logsDate=NULL #Remove \"Date\"-Column    storage.mode(dax_ret)\u0026lt;- \"numeric\"    colnames(dax_ret)\u0026lt;-c(\"Index Returns\") #Rename Column-Names    # Step 1: Estimate baseline GARCH model to obtain log-likelihood and residuals    dax_mod\u0026lt;-garch(dax_ret, order = c(1,1))    l.b\u0026lt;-dax_modresiduals)    # Step 2: Find largest absolute standardized residual    max(abs(dax_mod.resdax_mod.residuals,    na.rm = TRUE)), na.rm = TRUE)    specgarch \u0026lt;- ugarchspec(variance.model=list(model=\"sGARCH\", external.regressors=dummy), mean.model=list(external.regressor=dummy), distribution=\"norm\")    garchfit \u0026lt;- ugarchfit(data=dax_ret, spec=specgarch)","Creater_id":124835,"Start_date":"2016-07-28 05:09:01","Question_id":226085,"Tags":["r","outliers","garch","finance"],"Answer_count":1,"Last_activity":"2016-08-01 06:19:26","Link":"http://stats.stackexchange.com/questions/226085/outlier-detection-in-garch1-1-in-r-by-doornik-ooms-2002","Creator_reputation":1}
{"_id":{"$oid":"5837a588a05283111e4d6a7c"},"View_count":69,"Display_name":"Jon","Question_score":1,"Question_content":"I have a single IV (X) and two DVs (Y1, Y2). All variables are continuous. Y1 and Y2 are significantly and moderately correlated. I would like to know if X is more strongly correlated to Y1 or Y2 (or vice verse). What I think I have to do is to \"control for\" or \"partial out\" the correlation between Y1 and Y2. First, I don't think I want a partial correlation, as that controls for the shared variance between the control variable (let's say it's Y2 here) and both the IV (X) and the DV (Y1). I only want to control for the shared variance between Y1 and Y2. Second, I don't think I want a semi-partial correlation in a multiple regression, as that only controls for the variance shared by the IV (X) and the control (Y2). Third, I don't think I want a MANOVA, as I don't want to know what the effect of X on Y1 and Y2 (linearly combined) is. I want to know how the two correlation differ.I seem to want a semi-partial correlation in which the variance shared by Y1 and Y2 is shared. So, it seems that I should compute residuals for both Y1 and Y2. To do this, I'll regress Y1 on Y2 to save the residuals for Y1 (Y1*), and vice verse for Y2 (Y2*). Then, the correlation between X and Y1* will be the correlation between X and Y1 controlling for the variance shared between Y1 and Y2; vice verse for X and Y2. My questions:1. Is this right?2. Is there a simple way to do this in a statistical package, or do I have to compute the residuals first, then run my correlational or regression analysis with X?3. How do I think compare the effects of X in Y1 v. Y2? If I just run two correlational analyses (X-Y1* and X-Y2*), I can just do an r-to-Z test. But what if I run regressions instead? Can I compare betas in this case?","Creater_id":60644,"Start_date":"2016-07-31 02:19:10","Question_id":226523,"Tags":["regression","multiple-regression","residuals","partial-correlation"],"Answer_count":1,"Last_activity":"2016-08-01 06:12:49","Link":"http://stats.stackexchange.com/questions/226523/how-to-compare-the-effect-of-x-on-y1-and-y2-when-y1-and-y2-are-correlated","Creator_reputation":198}
{"_id":{"$oid":"5837a588a05283111e4d6a7e"},"View_count":1367,"Display_name":"user2322784","Question_score":6,"Question_content":"Is there any package in R that's commonly used for semi-supervised learning ? I have a dataset where I manually labeled 100 data points so I'd like to use semi-supervise learning for the rest of the data sets.","Creater_id":55983,"Start_date":"2015-04-06 20:47:13","Question_id":145087,"Tags":["r","machine-learning","svm","semi-supervised"],"Answer_count":2,"Last_activity":"2016-08-01 06:10:30","Link":"http://stats.stackexchange.com/questions/145087/is-there-any-package-in-r-thats-commonly-used-for-semi-supervised-learning","Creator_reputation":33}
{"_id":{"$oid":"5837a588a05283111e4d6a80"},"View_count":20,"Display_name":"Gong  Lei","Question_score":1,"Question_content":"I am new in R. I want to draw a figure visualizing the value of eigenvectors. Could someone help me? An example can be found in figure 1 in enter link description here","Creater_id":125225,"Start_date":"2016-08-01 05:39:34","Question_id":226668,"Tags":["r","eigenvalues"],"Answer_count":0,"Last_activity":"2016-08-01 05:39:34","Link":"http://stats.stackexchange.com/questions/226668/how-to-visualize-eigenvector-related-to-spatial-autocorrelation-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d6a82"},"View_count":4536,"Display_name":"lindsey","Question_score":2,"Question_content":"I've been doing a research paper - Effects of Workplace Bullying on Employees' Productivity,Self-Confidence and Self-Esteem.  So I have 1 IV (Workplace bullying) and 3 DVs. what can I use to interpret it?  Another thing: if it's MANOVA, can I execute it without having to use factors? ","Creater_id":29887,"Start_date":"2013-09-03 22:19:36","Question_id":69145,"Tags":["multiple-regression","manova"],"Answer_count":3,"Last_activity":"2016-08-01 05:02:26","Link":"http://stats.stackexchange.com/questions/69145/what-particular-measure-to-use-multiple-regression-or-manova","Creator_reputation":11}
{"_id":{"$oid":"5837a588a05283111e4d6a84"},"View_count":22,"Display_name":"Mon u","Question_score":0,"Question_content":"i am just wandering what test to use for the following trial:I have a one treatment at 9 different rates (untreated, 100g, 200g etc) and I want to see how 4 continuous dependent variables and 2 categorical dependent variables respond. Should I do a pca first and see what response variable I should focus on? Can you do this for the 9 different treatment rates?I do not know if it is a one-way manova that I need to preform, or multiple regression? I get confused with the R code when I have to seperate the treatments (DV) into the different rates to be compared to the IV.Thanks for the help!","Creater_id":103988,"Start_date":"2016-03-09 15:36:08","Question_id":200857,"Tags":["multiple-regression","manova"],"Answer_count":1,"Last_activity":"2016-08-01 05:00:10","Link":"http://stats.stackexchange.com/questions/200857/what-stat-test-to-use-and-help-with-code-in-r-for-a-beginner","Creator_reputation":1}
{"_id":{"$oid":"5837a588a05283111e4d6a86"},"View_count":24,"Display_name":"Annimirrll","Question_score":3,"Question_content":"I asked participants of a survey to rate the competence of 10 stimuli and want to see whether the stimuli differ significantly. I conducted a repeated measurements ANOVA and tested whether competence ratings significantly differs as a function of stimuli. It was not significant. Then I excluded two stimuli from the ANOVA and it became significant. How is that possible? ","Creater_id":125216,"Start_date":"2016-08-01 03:04:30","Question_id":226652,"Tags":["anova","psychology"],"Answer_count":0,"Last_activity":"2016-08-01 04:14:34","Link":"http://stats.stackexchange.com/questions/226652/anova-is-not-significant-if-i-exclude-one-factor-level-it-gets-significant","Creator_reputation":16}
{"_id":{"$oid":"5837a588a05283111e4d6a88"},"View_count":50,"Display_name":"T. Beige","Question_score":1,"Question_content":"I'm afraid I basically missunderstand something in the Ljung-Box-Pierce test. I estimate an ARMAX model with  as seasonal response variable with periodicity in lag 144, and ARMA(3,1) process for modeling the errors, Fourier series in the xreg argument for modeling seasonality (because of long length and multiple wiggly seasonality) and an exogenous variable . The model works almost great for forecast and a -test proves  as significant influence on  as it should be on preliminary considerations.The model is estimated with R and the package forecast in this way: Freg = forecast::fourier(y, K=45)  # optimal K estimated with gcv  Exo = cbind(Freg, x) fit \u0026lt;- Arima(y[1:2000], order=c(3,0,1), xreg=Exo[1:2000,])You see, the forecast (blue lines with grey confidence interval) isn't perfect, but it is a good match in general with estimating the model with  observations and forecasting  (red line original time series). Now model diagnostics:A test for white-noise by Bartlett with  passed with a -value of 0.939. The graphics for serial correlations look OK in my mind (maybe except the PACF in very high lags), as you can see in the graphic below.Now I want to prove the good impression of a well-predicting model with a Ljung-Box-Pierce test.The test is based on the statisticQ = N(N+2) \\sum_{t=1}^{h} \\frac{1}{N-t} \\cdot \\tilde{\\rho}_{t}^{2} where  is the number of observations (number of residuals here),  is the test lag,  the correlation of observations (residuals) in lag . The  statistic follows a  distribution with  degrees of freedom. The  in  comes from the estimation of the ARMA parameters. Because of the long length seasonality I want to check the -values of the Ljung-Box-Pierce for the lags 5 to 288 as Rob J. Hyndman suggests in his blog post \"Thoughts on the Ljung-Box test\".p.values = numeric(2*144)for (i in 1:(2*144)){  p.pvalues[i] = Box.test(fitp.value}-values are greater 0.05 for  but smaller 0.05 for .Question: How do the results of the Ljung-Box-Pierce test and the plotted ACF match?I thought most test results should have a -value greater than 0.05 as I saw the ACF. Maybe something is wrong with the degrees of freedom, because of the 91 estimated coefficients in the xreg argument?I'm sorry, but I'm not allowed to publish data.EDIT : As advised by @RichardHardy I perform a Breusch-Godfrey-Test in addition to the Ljung-Box-Pierce-Test and create plots showing the p-values to their corresponding lags. You can see, for small lags the Ljung-Box-Pierce test favors the null hypothesis, because of its biased estimation. But essentially both tests converge against a p-value of null for very high lags, eventhough the calculated serial correlation is not higher as in smaller lags. p.values.bgtest = numeric(2*144) for (i in 1:(2*144)){   p.values.bgtest[i] = bgtest(fitp.value }","Creater_id":124127,"Start_date":"2016-07-28 03:34:25","Question_id":226076,"Tags":["time-series","autocorrelation","diagnostic"],"Answer_count":0,"Last_activity":"2016-08-01 03:57:34","Link":"http://stats.stackexchange.com/questions/226076/ljung-box-statistic-doesnt-match-to-acf-of-arimax","Creator_reputation":8}
{"_id":{"$oid":"5837a588a05283111e4d6a8a"},"View_count":61,"Display_name":"zelanix","Question_score":0,"Question_content":"I have a (hopefully simple) question about notation when defining a DP. I have read a lot of papers on DPs, but this is not clear to me, or at least I have not noticed a convention.Say that I am sampling from a simple Gaussian mixture DP, defined as the following generative model:\\begin{align}x~|~\\mu_i\u0026amp;\\sim N(\\mu_i, 1)\\\\\\mu_i~|~G\u0026amp;\\sim G\\\\G\u0026amp;\\sim\\text{DP}(\\alpha,G_0)\\end{align} will also be associated with a generative distribution, in this case let's assume , but obviously this can be arbitrarily complex depending on the application, perhaps with it's own hyper-parameters.If I'm performing inference, say using Neal's (2000) Algorithm 8, then I use samples from , in this case , for the potential new classes.My question therefore: is there a standard (and succinct) way to define the distribution of ? Ideally I feel that it would be sensible and logical to define it as part of the generative model so that the whole model definition is in one place, but writing something like  isn't quite accurate, and doesn't naturally extend to more complex models, say where  contains samples of both unknown mean and variance.","Creater_id":53985,"Start_date":"2016-07-29 05:27:11","Question_id":226280,"Tags":["model","notation","dirichlet-process"],"Answer_count":1,"Last_activity":"2016-08-01 02:07:02","Link":"http://stats.stackexchange.com/questions/226280/notation-for-base-distribution-in-dirichlet-process","Creator_reputation":118}
{"_id":{"$oid":"5837a588a05283111e4d6a97"},"View_count":47,"Display_name":"An old man in the sea.","Question_score":2,"Question_content":"From what I could gatherMixture: if , then W is a mixture with . This definition could also be for the CDF instead of the density.Convolution: To make it simpler, lets assume if , then . We could write this in terms of densities.What I don't get is the practical intuition for these definitions.For example: We have two machines, each producing observations .If there's probability  that machine 1 is chosen, how do we model our final observation ? As a convolution or a mixture? And what changes should we do to our problem/situation to model it as the other possibility?Any help would be appreciated.","Creater_id":40252,"Start_date":"2016-07-31 15:01:34","Question_id":226592,"Tags":["probability","distributions","mixture"],"Answer_count":1,"Last_activity":"2016-08-01 02:05:50","Link":"http://stats.stackexchange.com/questions/226592/difference-between-a-mixture-of-distributions-and-a-convolution-interpretation","Creator_reputation":911}
{"_id":{"$oid":"5837a588a05283111e4d6aa4"},"View_count":24,"Display_name":"user310374","Question_score":0,"Question_content":"When exploring the relationship between independent variables and a dependent variable using multiple regression, it is very common to adjust for confounders.  Most of the models I see in the literature of various fields are linear models, e.g. logistic regression or linear regression.  Is it possible for the relationship between an independent variable and its confounder to be non-linear, and hence not be effectively \"controlled for\" in the model?  My question is, when we use linear regression to understand the relationship between A and an outcome B, and then \"control for C\" by including it in the model as another independent variable, how confident can we be that it is being effectively \"controlled for?\"  ","Creater_id":103007,"Start_date":"2016-08-01 01:46:42","Question_id":226644,"Tags":["regression","multiple-regression"],"Answer_count":0,"Last_activity":"2016-08-01 01:46:42","Link":"http://stats.stackexchange.com/questions/226644/linearity-assumption-when-adjusting-for-confounders","Creator_reputation":35}
{"_id":{"$oid":"5837a588a05283111e4d6aa6"},"View_count":87,"Display_name":"Teresa","Question_score":0,"Question_content":"I am fitting GLMM's (using a binary variable as response variable and continuous variables as explanatory variables [family = binomial(link=\"logit\")]), and I am interested in obtaining the effect sizes for each explanatory variable.I obtain the effect size value by calculating odds ratios:(Effect size in GLMM).However, my variables are standardized, so how do I interpret the odds ratios ?","Creater_id":117281,"Start_date":"2016-07-31 09:39:30","Question_id":226568,"Tags":["effect-size","glmm","odds-ratio"],"Answer_count":1,"Last_activity":"2016-08-01 01:43:23","Link":"http://stats.stackexchange.com/questions/226568/interpretation-of-odds-ratios-when-variables-are-standardized","Creator_reputation":59}
{"_id":{"$oid":"5837a588a05283111e4d6ab3"},"View_count":28,"Display_name":"bla345","Question_score":0,"Question_content":"You tried several models and tuned them appropriately and found the best performing model. What is the typical approach taken after this model selection step to improve the solution? E.g., is it feature engineering? If so, what are typical approaches for feature engineering? Please provide links to relevant resources.thanks.","Creater_id":124606,"Start_date":"2016-07-29 13:53:37","Question_id":226360,"Tags":["machine-learning","feature-construction"],"Answer_count":1,"Last_activity":"2016-08-01 01:29:04","Link":"http://stats.stackexchange.com/questions/226360/after-tuning-different-models-and-comparing-them-what-is-the-next-step-for-impr","Creator_reputation":21}
{"_id":{"$oid":"5837a588a05283111e4d6ac0"},"View_count":41,"Display_name":"Salam Abbas","Question_score":0,"Question_content":"I have data of daily observations for 35 years and I have modeled data for the same period. The coefficient of determination () between them is zero (no correlation at all!). I want to correct the bias of modeled data, particularly adjust the trend of the modeled data to improve the coefficient of determination. Can anyone please suggest any statistical technique to do that?The data is daily rainfall for a selected area and the observation is from 1970-2004.... I got also climate model data for rainfall for the same area for historical and future period and I want to correct the data on a historical basis and then implement the parameter of that correction for future data..... but most bias correction such as quantile mapping correct the mean error and percent of bias not trend of data ... so I'm looking for method the can adjust trend of modeled data (bias correction) ","Creater_id":83457,"Start_date":"2016-07-29 08:46:04","Question_id":226321,"Tags":["time-series","trend","bias-correction"],"Answer_count":1,"Last_activity":"2016-08-01 01:22:52","Link":"http://stats.stackexchange.com/questions/226321/improve-fit-by-trend-adjustment","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d6acc"},"View_count":54,"Display_name":"kanimbla","Question_score":0,"Question_content":"Not sure if this post belongs here or if stack overflow would be more appropriate.I am starting to familiarize with the caret package in R which seems very powerful for the purpose of optimizing and implementing various machine learning methods. According to my understanding the key idea of the package is to train a model across different parameter sets and resampling methods and to select the optimal calibration based on a certain performance measure. This optimized model can subsequently be used to compute predictions on the test data.Does the package also allow computing predictions for all trained models other than the optimal model?If this is possible a minimum working example would be nice, but not essential.  The reason for my question is that I am interested in checking the predictive performance of the optimal model relative to the other trained models on the test data. Moreover, I would like to evaluate the performance of forecast combination schemes based on multiple model calibrations.","Creater_id":79243,"Start_date":"2016-07-31 09:41:56","Question_id":226569,"Tags":["machine-learning","caret"],"Answer_count":1,"Last_activity":"2016-08-01 01:19:37","Link":"http://stats.stackexchange.com/questions/226569/caret-package-is-it-possible-to-compute-predictions-for-non-optimal-models","Creator_reputation":74}
{"_id":{"$oid":"5837a588a05283111e4d6ad9"},"View_count":47,"Display_name":"BeStats","Question_score":2,"Question_content":"In relation to my previous question on different output results and their interpretation based on a model with or without an interaction term, this is a follow up question on how to report such results. Based on the following model formula and its output (see below), am I correct at interpreting this no-interaction model as follows;For treatment1 at time6, we have amp.sqrt = 115.184For treatment2 at time6, we have amp.sqrt = 115.184 + 2.644For treatment3 at time6, we have amp.sqrt = 115.184 + 23.365For treatment1 at time7, we have amp.sqrt = 115.184 + 13.958For treatment2 at time7, we have amp.sqrt = 115.184 + 13.958 + 2.644For treatment3 at time7, we have amp.sqrt = 115.184 + 13.958 + 23.365etc..I had thought of reporting the results as: \"there was a positive effect of treatment, with amp.sqrt increasing from treatment 1, to 2 (2.644) and 3 (23.365). Also a positive effect of time was observed, increasing from time 1 to 2 (13.958) and 3 (21.799). The main effect of axis increased from 1 to 2 and 3 as well.\"However, now that I have my model with the interaction term, and the main effects on their own seem strange to report (as I wrote above), should I instead report the overall trends as seen by your plot (i.e. adding the fixed effects and the interaction term and then presenting that value) or should I still present the results as above, even if the main effect outputs on their own are counterintuitive unless specified in relation with the interaction term results?(NO-INTERACTION TERM MODEL)mTEST1\u0026lt;- lmer(amp.sqrt~ time + treatment + axis + (1+treatment|ID))Fixed effects:        Estimate Std. Error      df t value Pr(\u0026gt;|t|)    (Intercept)  115.184      7.546  36.300  15.265  \u0026lt; 2e-16 ***time7         13.958      4.707 474.800   2.965  0.00318 ** time8         21.799      4.787 478.500   4.554  6.7e-06 ***treatment2     2.644      8.571  18.400   0.308  0.76117    treatment3    23.365      6.139  19.200   3.806  0.00117 ** axis2         60.458      4.746 474.800  12.737  \u0026lt; 2e-16 ***axis3        128.456      4.746 474.800  27.063  \u0026lt; 2e-16 ***---(INTERACTION-TERM MODEL)mTEST2\u0026lt;- lmer(amp.sqrt~ time * treatment + axis + (1+treatment|ID))Fixed effects:                 Estimate Std. Error      df t value Pr(\u0026gt;|t|)    (Intercept)       130.587      8.417  55.500  15.515  \u0026lt; 2e-16 ***time7              -7.697      8.120 471.000  -0.948   0.3436    time8              -2.628      8.120 471.000  -0.324   0.7464    treatment2         -3.766     10.713  44.500  -0.352   0.7269    treatment3        -14.929      8.851  83.600  -1.687   0.0954 .  axis2              60.458      4.569 471.000  13.232  \u0026lt; 2e-16 ***axis3             128.456      4.569 471.000  28.113  \u0026lt; 2e-16 ***time7:treatment2    9.697     11.206 471.000   0.865   0.3873    time8:treatment2    8.554     11.396 473.700   0.751   0.4532    time7:treatment3   53.206     11.206 471.000   4.748 2.73e-06 ***time8:treatment3   62.411     11.289 473.300   5.528 5.35e-08 ***","Creater_id":121550,"Start_date":"2016-07-31 08:11:00","Question_id":226559,"Tags":["mixed-model","interaction","interpretation","reporting"],"Answer_count":1,"Last_activity":"2016-08-01 00:58:54","Link":"http://stats.stackexchange.com/questions/226559/how-should-i-report-the-results-from-a-mixed-model-with-vs-without-an-interactio","Creator_reputation":35}
{"_id":{"$oid":"5837a588a05283111e4d6ae6"},"View_count":34,"Display_name":"rec","Question_score":0,"Question_content":"For SVMs, they operate best when the data is in the ranges of  or . Naturally you want to normalize data so that your model works well.My question is: what do you do about features derived from normalized data? For example, if I have a data frame with a column called \"wait times\" in seconds, and I want to add a 50 day moving average to this data to smooth it, do I:Normalize the \"wait times\" to the range  and then apply the moving average functionApply the moving average function and then normalize the \"wait times\" and \"ma\" column to the range .I'm having trouble finding any sort of literature on this. I feel like (2) is correct because (1) causes information loss, but I am honestly not sure because I cannot prove it to myself handily. It would be very helpful if someone could explain to me which is right so that I know in the future how best to handle this.Thank you!","Creater_id":124589,"Start_date":"2016-07-26 09:21:30","Question_id":225727,"Tags":["svm","normalization","libsvm"],"Answer_count":1,"Last_activity":"2016-08-01 00:17:08","Link":"http://stats.stackexchange.com/questions/225727/understanding-data-normalization-for-svms","Creator_reputation":43}
{"_id":{"$oid":"5837a588a05283111e4d6af3"},"View_count":26,"Display_name":"Adham","Question_score":2,"Question_content":"As we know mixture models are important tools in density estimation and in general in statistical machine learning. I have always used nonparametric Bayesian mixture models to avoid the problem of finding the optimal number of mixture ecomponents. Considering the fact that mixture models with nonparametric priors are quite old and well known, why is that I still see papers that use finite mixture models and then use a model selection method to find the best model. Do these methods offer other advantages over nonparametric models?In particular, could anyone please let me know if the Minimum Message Length method have an advantage over using a nonparametric prior such as Dirichlet process?","Creater_id":48062,"Start_date":"2016-07-31 22:32:05","Question_id":226626,"Tags":["information-theory","dirichlet-process","nonparametric-bayes","finite-mixture-model"],"Answer_count":0,"Last_activity":"2016-07-31 23:22:46","Link":"http://stats.stackexchange.com/questions/226626/bayesian-nonparametrics-vs-model-selection-using-minimum-message-length","Creator_reputation":60}
{"_id":{"$oid":"5837a588a05283111e4d6af5"},"View_count":247,"Display_name":"user86126","Question_score":0,"Question_content":"I have a training dataset which has two columns which has around 70 values.“PNRNo” whose values like UT767G, CADA, 4I9I59, 4BH5TW…(typical PNR number patterns)I have created one more factor variable mentioning (IsPNR) – so all the values are 1 (true)My first objective is to create a model on this training set which would recognize the text pattern.Second objective: The model would then be used to predict IsPNR with new set of test values like “Anshuk”, “4EL58S”…as 0 and 1…Which model would be best for recognizing such kind of pattern and having decent accuracy? I tried naiveBayes, but I don’t think it is all doing a good job. Its predicting all the test values as true. I suppose naive Bayes is not meant for this.","Creater_id":86126,"Start_date":"2015-08-17 08:09:16","Question_id":167495,"Tags":["r","data-mining","pattern-recognition","artificial-intelligence"],"Answer_count":2,"Last_activity":"2016-07-31 23:18:53","Link":"http://stats.stackexchange.com/questions/167495/text-pattern-recognition-model-building-using-r","Creator_reputation":1}
{"_id":{"$oid":"5837a588a05283111e4d6b02"},"View_count":56,"Display_name":"Christian Bueno","Question_score":3,"Question_content":"Let  be a random unit vector in  such that  is uniformly distributed on the unit sphere . Next, let  be a random unit vector that is orthogonal to  and then define . Finally, define  to be the rotation matrix that maps , , . Is  a uniformly random orientation-preserving rotation matrix? In more technical parlance, is the probability measure induced by  the normalized Haar measure on ? And if so (or not), why?","Creater_id":28055,"Start_date":"2016-07-31 16:26:28","Question_id":226598,"Tags":["probability","sampling","matrix","random-matrix"],"Answer_count":1,"Last_activity":"2016-07-31 22:11:16","Link":"http://stats.stackexchange.com/questions/226598/does-this-method-uniformly-sample-3x3-rotation-matrices","Creator_reputation":233}
{"_id":{"$oid":"5837a588a05283111e4d6b0f"},"View_count":7,"Display_name":"arshad","Question_score":0,"Question_content":"I have incidence rates of a disease in 1992-1994 (IR=13.2), 1995-1997 (IR=10.2) and 2011-2013 (IR=22.7). How do I calculate the annual increase in the incidence rate ? Thanks","Creater_id":20213,"Start_date":"2016-07-31 21:55:15","Question_id":226623,"Tags":["poisson-regression"],"Answer_count":0,"Last_activity":"2016-07-31 21:55:15","Link":"http://stats.stackexchange.com/questions/226623/how-to-calculate-annual-increase-in-incidence-rate","Creator_reputation":49}
{"_id":{"$oid":"5837a588a05283111e4d6b11"},"View_count":31,"Display_name":"hans-t","Question_score":0,"Question_content":"I'm sure that Design of Experiments (DoE) is appropriate to a system when it has low noise and high reproducibility, like a natural system. For instance, finding out the effects of temperature and strength on glass strength.But is DoE applicable to a high noise, low reproducibility system like an economic system? For instance, effects of print and radio marketing on sales.If DoE is not applicable, what statistical tools are applicable to find effects and/or optimize such system?","Creater_id":40010,"Start_date":"2016-07-30 22:16:18","Question_id":226511,"Tags":["experiment-design"],"Answer_count":1,"Last_activity":"2016-07-31 21:43:50","Link":"http://stats.stackexchange.com/questions/226511/what-are-situations-that-make-doe-inappropriate","Creator_reputation":189}
{"_id":{"$oid":"5837a588a05283111e4d6b1e"},"View_count":53,"Display_name":"lior_","Question_score":0,"Question_content":"I study linguistics, and I'm conducting a research which compares the ratio between two categories of words in 2 different text corpuses.Let's say I have corpuses A and B. I created 2 categories of words, Let's call them C and D. I want to calculate the ratio between the count of words in category C and the count of words in category D, and see if the ratio in corpus A is significantly larger than the ratio in corpus B.What statistical test can I use to test this hypothesis?It's worth mentioning that the count of words in category C should theoretically depend on the count of words in category D and vice versa.Thanks in advance","Creater_id":87509,"Start_date":"2016-07-31 15:21:24","Question_id":226594,"Tags":["hypothesis-testing","proportion"],"Answer_count":1,"Last_activity":"2016-07-31 21:38:40","Link":"http://stats.stackexchange.com/questions/226594/how-to-test-for-significant-difference-between-2-proportions","Creator_reputation":22}
{"_id":{"$oid":"5837a588a05283111e4d6b2b"},"View_count":64,"Display_name":"priyanka","Question_score":5,"Question_content":" is a random sample from the random variable whose pdf is,\\begin{align*}f(x)=\\lambda e^{-\\lambda(x-\\mu)},\\mu\u0026lt;x\u0026lt;\\infty\\end{align*}How can we find , if We know that,Here,It is difficult to compute, I have also tried to find, Similar problem arises in case of  too.","Creater_id":117161,"Start_date":"2016-05-31 04:47:58","Question_id":215540,"Tags":["self-study","expected-value","order-statistics"],"Answer_count":2,"Last_activity":"2016-07-31 20:46:17","Link":"http://stats.stackexchange.com/questions/215540/expected-value-of-difference-of-two-order-statistics","Creator_reputation":392}
{"_id":{"$oid":"5837a588a05283111e4d6b39"},"View_count":24,"Display_name":"user3022875","Question_score":0,"Question_content":"Let's say you 20 means for 20 independent distributions and you want to know if the means are significantly different but essentially a higher level you want to know if the 20 means are different.One common method to test is an anova and then a post hoc Tukey test or something similar. Or if you do non parametric testing the kruskall wallis and then some post hoc dunn tests. assuming the initial test (anova or kruskall) is significant there are (N x N-1) /2 pairwise test statistics. If there are 20 groups that 190 test stats.My question is:(1) That's a lot of test stats to look at consider.  How to practitioners handle this when they want to say the means or numbers are or are not statistically meaningfull.  Analyzing 190 pairs seems tedious.  Do practitioners just look at the 20 box plots and eyeball if the distributions look the same or different?(2) If you do somehow process the pairwise test how do you do it to communicate it to an audience because not all groups will be significantly differnt from ALL the other grooups in most cases..etc..(2) I was thinking find the groups that are most present in a pair and indicating those groups that have the most differnces as the most significant(3) are there any visual aids in this endevour?Thank you","Creater_id":40579,"Start_date":"2016-07-31 19:49:01","Question_id":226610,"Tags":["probability","anova","t-test","kruskal-wallis"],"Answer_count":0,"Last_activity":"2016-07-31 20:01:34","Link":"http://stats.stackexchange.com/questions/226610/anova-multiple-comparison-post-hoc-test-vs-histogram","Creator_reputation":98}
{"_id":{"$oid":"5837a588a05283111e4d6b3b"},"View_count":29,"Display_name":"mkstreet","Question_score":2,"Question_content":"I want to create a simple project to show about ESP.I started with the idea of guessing whether a coin would be heads or tails.I first wanted to show some values by pure guessing.So, using random numbers of zero (tails) or heads (one), I generated 1000 pairs of these and put them in the first two columns of a spreadsheet.  If the two values in the pair matched, I recorded it as a correct guess (a one in the third column of the spreadsheet) and if they didn't match, a zero.I found the mean of the third column.  It was approximately 0.5.Then I took the standard deviation of this third column also.  It's value was also close to 0.5 (about 0.49 or so).I then took the mean plus two times the standard deviation which got me a value of about 1.5.  I did this because I was thinking this would show more than two standard deviations from the mean, which would be 95%.  My idea was if someone could guess correctly above two standard deviations from the mean, that this would suggest unusual ability better than randomly guessing.Then I realized that, for example, if someone guessed correctly ten times, that would be an average of 1 (from 10 / 10).  But that will be less than 1.5 (the value from the mean of the random numbers plus two times their standard deviation).  It would always be impossible to guess heads/tails and get 1.5 or better.My friend suggested that I have to take the standard deviation value divided by the square root of 1000 (the number of trials).  I don't know if this suggestion is right or not.  If I do this, double it, and add it to the mean, I get a value of about 0.55.  This suggests if a person guesses more than 55% correctly, this is above two standard deviations.  I don't know if this is the right way or not, and why.Thanks","Creater_id":125177,"Start_date":"2016-07-31 17:14:22","Question_id":226600,"Tags":["statistical-significance","standard-deviation","randomization"],"Answer_count":1,"Last_activity":"2016-07-31 19:29:14","Link":"http://stats.stackexchange.com/questions/226600/using-standard-deviation-and-flipping-coins","Creator_reputation":11}
{"_id":{"$oid":"5837a588a05283111e4d6b47"},"View_count":50,"Display_name":"BoltzmannBrain","Question_score":0,"Question_content":"As time progresses in an experiment I'm running, I have two time-dependent variables, A and B, that are calculated at varying time steps. After running the length of the experiment trial, I would like to find a correlation function between them such that in the future when I'm given a new set of times and values for B I can predict the values for A. What is a good approach here?Implementation details: A and B are Python lists of two-tuples (time in seconds, value), e.g.:A = [(0.00466, 2.0227), (0.06979, 2.0408), (0.12991, 2.0623), ...]B = [(0.0, 0.0), (0.04, 14.974), (0.08, 6.482), ...]","Creater_id":123449,"Start_date":"2016-07-31 18:53:43","Question_id":226604,"Tags":["correlation","prediction"],"Answer_count":0,"Last_activity":"2016-07-31 18:53:43","Link":"http://stats.stackexchange.com/questions/226604/correlation-between-two-lines-for-future-predictions","Creator_reputation":103}
{"_id":{"$oid":"5837a588a05283111e4d6b49"},"View_count":32,"Display_name":"Naseer Ahmed","Question_score":0,"Question_content":"In many papers and articles related to Recurrent Neural networks(RNN) with specific (LSTM)Long short term memory technique I have seen that in training phase labels and ground truth data are fed with the input data.Whereas In most of the classification techique only the labels are required with the data in the training phase.Please help me understand why we need both labels and ground truth in training phase. Reference LSTM Paper","Creater_id":108743,"Start_date":"2016-07-31 05:45:43","Question_id":226543,"Tags":["classification","lstm","rnn","labeling"],"Answer_count":1,"Last_activity":"2016-07-31 17:30:42","Link":"http://stats.stackexchange.com/questions/226543/ground-truth-vs-labels-in-lstm-classifications","Creator_reputation":22}
{"_id":{"$oid":"5837a588a05283111e4d6b55"},"View_count":75,"Display_name":"David Zwicker","Question_score":2,"Question_content":"I use a Monte-Carlo algorithm to estimate a target value y=f(x) that depends on a single input parameter x. Because the Monte-Carlo algorithm is stochastic, the target value y fluctuates. I now want to find the value of x that minimizes y. Obviously, I cannot use most of the standard optimization algorithms (like Newton's), because I cannot calculate gradients of y with respect to x.What optimization algorithms would be useful in this kind of situation?Here is a simple python code the illustrates the situationimport matplotlib.pyplot as pltimport numpy as npfrom scipy import optimizedef f(x):    return -np.sin(x + np.pi/2 + 0.5 * np.random.random())xs = np.linspace(-2, 2)ys = [f(x) for x in xs]plt.plot(xs, ys, 'o')optimize.minimize_scalar(f)which results in the following 'minimum' fun: -0.99996139255316596nfev: 38 nit: 37  success: True   x: -0.22358360754884515The function is constructed such that its minimum is at x=0, see the figure produced by the above code:","Creater_id":88762,"Start_date":"2016-07-31 07:46:58","Question_id":226554,"Tags":["optimization","algorithms"],"Answer_count":2,"Last_activity":"2016-07-31 16:55:02","Link":"http://stats.stackexchange.com/questions/226554/minimizing-approximate-function","Creator_reputation":113}
{"_id":{"$oid":"5837a588a05283111e4d6b63"},"View_count":93,"Display_name":"RaviTej310","Question_score":1,"Question_content":"I have come across this brilliant site where I finally understood BPTT for RNN's and want to implement it. The code is given in python but I want to implement it in torch using lua. I have understood the code and have translated most of it to lua with torch functions. However, I have often come across packages for torch on github like this. How can we use these packages? If we were to use them, would we not need to type even a single line of code? Isn't it better if I first completely code the RNN myself to understand and then go on to the packages?","Creater_id":116447,"Start_date":"2016-05-31 03:39:26","Question_id":215528,"Tags":["lstm","rnn","torch"],"Answer_count":1,"Last_activity":"2016-07-31 16:46:59","Link":"http://stats.stackexchange.com/questions/215528/how-to-use-torch-rnn-and-lstm-packages-and-are-they-necessary","Creator_reputation":106}
{"_id":{"$oid":"5837a588a05283111e4d6b6f"},"View_count":7639,"Display_name":"jjepsuomi","Question_score":7,"Question_content":"I started a discussion with a collague of mine and we started to wonder, when should one apply feature normalization / scaling to the data? Lets say that we have a set of features with some of the features having a very broad range of values and some features having not so broad range of values. If I'd be doing principal component analysis I would need to normalize the data, this is clear, but lets say we are trying to classify the data by using plain and simple k-nearest neighbor / linear regression method. Under what conditions should or shouldn't I normalize the data and why? A short and simple example highlighting the point added to the answer would be perfect.","Creater_id":18528,"Start_date":"2014-10-29 02:00:48","Question_id":121886,"Tags":["machine-learning","normalization","scales","k-nearest-neighbour"],"Answer_count":5,"Last_activity":"2016-07-31 16:15:13","Link":"http://stats.stackexchange.com/questions/121886/when-should-i-apply-feature-scaling-for-my-data","Creator_reputation":1112}
{"_id":{"$oid":"5837a588a05283111e4d6b80"},"View_count":43,"Display_name":"ngiann","Question_score":0,"Question_content":"I need to validate whether one or two templates/shapes are present in an image.Fitting two templates has a better maximum likelihood then fitting one template which is a clear symptom of overfitting. What I would like to do is set aside some test set and then check whether one template or two templates generalise better.When working with iid data items in regression or classification tasks, we can leave some data at side to use for testing and this can help with model selection. (Cross-validation is of course is also an alternative option to keeping a holdout set.)My question is: can I do something similar for my template problem? I.e. could I randomly  leave some pixels out of training and use them to test whether one template or two templates generalise better? Somehow this doesn't sit well with me as in contrast to the regression/classification problem, here the pixels are not exactly iid (that is neighbouring pixels must be correlated).","Creater_id":25839,"Start_date":"2015-01-27 16:46:23","Question_id":135256,"Tags":["cross-validation","maximum-likelihood","model-selection","image-processing","out-of-sample"],"Answer_count":1,"Last_activity":"2016-07-31 15:26:45","Link":"http://stats.stackexchange.com/questions/135256/holdout-set-for-image-task","Creator_reputation":229}
{"_id":{"$oid":"5837a588a05283111e4d6b8d"},"View_count":151,"Display_name":"user2543622","Question_score":1,"Question_content":"I have JPG images like below and each one is less than 200 KB. My webcam and the watch are in a fixed position. I want to analyze images and tell the exact time. I have huge number of images and training my algorithm wont be a problem. Any suggestions about packages that i can use? I am thinking about getting pixel color density values for each pixel and then building a neural network or a support vector machine. But i felt that there might be a better package or an algorithm to do the same. ","Creater_id":29065,"Start_date":"2015-01-19 15:57:14","Question_id":134097,"Tags":["r","classification","image-processing"],"Answer_count":1,"Last_activity":"2016-07-31 15:26:30","Link":"http://stats.stackexchange.com/questions/134097/image-classification-r","Creator_reputation":161}
{"_id":{"$oid":"5837a588a05283111e4d6b9a"},"View_count":503,"Display_name":"Opt","Question_score":2,"Question_content":"Given that images can be of vastly different resolutions, but neural networks are usually presented as having a fixed number of inputs, what are the standard techniques used to handle the difference between the number of NN inputs vs the number of pixels in the image?","Creater_id":250,"Start_date":"2014-10-10 16:42:12","Question_id":118654,"Tags":["neural-networks","deep-learning","image-processing","deep-belief-networks","scale-invariance"],"Answer_count":1,"Last_activity":"2016-07-31 15:26:02","Link":"http://stats.stackexchange.com/questions/118654/scale-invariance-for-images","Creator_reputation":258}
{"_id":{"$oid":"5837a588a05283111e4d6ba7"},"View_count":278,"Display_name":"inulinux12","Question_score":0,"Question_content":"I was recently trying out caffe and learning about CNN. So far I have seen that the model used by Krizhevsky performs really well in natural images. However I wanted to know how these models or CNN based models would behave in domains such as biological images or medical images.","Creater_id":56374,"Start_date":"2014-09-24 10:27:46","Question_id":116639,"Tags":["neural-networks","pattern-recognition","image-processing","conv-neural-network"],"Answer_count":1,"Last_activity":"2016-07-31 15:25:45","Link":"http://stats.stackexchange.com/questions/116639/how-well-do-convolutional-neural-networks-in-other-image-domains","Creator_reputation":103}
{"_id":{"$oid":"5837a588a05283111e4d6bb4"},"View_count":180,"Display_name":"John Yetter","Question_score":1,"Question_content":"I know that RBM's have been used on image data for pre-training neural nets, but all I can find are RBM's on black and white images. How do you apply them to say 256 grayscale?","Creater_id":31135,"Start_date":"2014-08-29 04:36:03","Question_id":113686,"Tags":["neural-networks","image-processing","computer-vision","rbm"],"Answer_count":1,"Last_activity":"2016-07-31 15:25:33","Link":"http://stats.stackexchange.com/questions/113686/restricted-boltzmann-machine-for-grayscale-images","Creator_reputation":547}
{"_id":{"$oid":"5837a588a05283111e4d6bc1"},"View_count":1233,"Display_name":"BCLC","Question_score":1,"Question_content":"If the EACF of my TS suggests ARMA(0,0) and the Box-Ljung test does not suggest my TS has correlation, can I conclude that my TS is white noise or merely that there is no reason to suspect that it is not white noise?Conversely, if my TS is white noise, will I necessarily get ARMA(0,0) from EACF, ACF, PACF, etc?","Creater_id":44339,"Start_date":"2014-05-08 07:25:41","Question_id":96930,"Tags":["time-series","arima","autocorrelation","arma","white-noise"],"Answer_count":1,"Last_activity":"2016-07-31 15:22:22","Link":"http://stats.stackexchange.com/questions/96930/is-arma0-0-equivalent-to-white-noise","Creator_reputation":702}
{"_id":{"$oid":"5837a588a05283111e4d6bce"},"View_count":49,"Display_name":"Mahdieh","Question_score":1,"Question_content":"I have conducted PCA as a variant of EFA to develop a scale. I'm a student of applied linguistics and the method of rotation was promax. A questionnare of 60 items has been answered by 200 participants. The items of the questionnaire were prepared based on 15 components extracted from interviews and literature reviews.There are two problems. First, some items have loaded under wrong components. So I prepared the item for component one but it is loaded under component 3. What do I do with the chaotic findings?  Second, there are 4 components with only 2 loaded factors. Should I discard that component?  Now that the items appear to be disorganized how can I make sure that component 3 on SPSS output is component 3 on my own list of themes? ","Creater_id":125092,"Start_date":"2016-07-30 12:39:03","Question_id":226487,"Tags":["pca","spss","factor-analysis","confirmatory-factor"],"Answer_count":1,"Last_activity":"2016-07-31 15:18:08","Link":"http://stats.stackexchange.com/questions/226487/why-have-my-items-loaded-under-wrong-non-hypothesized-components-in-pca-fa","Creator_reputation":6}
{"_id":{"$oid":"5837a588a05283111e4d6bdb"},"View_count":265,"Display_name":"Firebug","Question_score":12,"Question_content":"Some penalty functions and approximations are well studied, such as the LASSO () and the Ridge () and how these compare in regression.I've been reading about the Bridge penalty, which is the  generalized penalty. Compare that to the LASSO, which has , and the Ridge, with , making them special cases. Wenjiang [1] compared the Bridge penalty when  to the LASSO, but I couldn't find a comparison to the Elastic Net regularization, a combination of the LASSO and the Ridge penalties, given as .This is an interesting question because the Elastic Net and this specific Bridge have similar constraint forms. Compare these unit circles using the different metrics ( is the power of the Minkowski distance): corresponds to the LASSO,  to the Ridge, and  to one possible Bridge. The Elastic Net was generated with equal weighting on  and  penalties. These figures are useful to identify sparsity, for example (which Bridge clearly lacks while Elastic Net preserves it from LASSO).So how does the Bridge with  compares to Elastic Net regarding regularization (other than sparsity)? I have special interest in supervised learning, so perhaps a discussion about feature selection/weighting is pertinent. Geometric argumentation is welcome as well.Perhaps, more important, is the Elastic Net always more desirable in this case?[1] Fu, W. J. (1998). Penalized regressions: the bridge versus the lasso. Journal of computational and graphical statistics, 7(3), 397-416.EDIT: There's this question How to decide which penalty measure to use ? any general guidelines or thumb rules out of textbook which superficially mentions LASSO, Ridge, Bridge and Elastic Net, but there are no attempts to compare them.","Creater_id":60613,"Start_date":"2016-07-19 07:33:37","Question_id":224531,"Tags":["regression","lasso","regularization","ridge-regression","elastic-net"],"Answer_count":1,"Last_activity":"2016-07-31 15:11:24","Link":"http://stats.stackexchange.com/questions/224531/bridge-penalty-vs-elastic-net-regularization","Creator_reputation":2542}
{"_id":{"$oid":"5837a588a05283111e4d6be8"},"View_count":1090,"Display_name":"Matt","Question_score":11,"Question_content":"The general consensus on a similar question, Is it wrong to refer to results as being \u0026quot;highly significant\u0026quot;? is that \"highly significant\" is a valid, though non-specific, way to describe the strength of an association that has a p-value far below your pre-set significance threshold. However, what about describing p-values that are slightly above your threshold? I have seen some papers use terms like \"somewhat significant\", \"nearly significant\", \"approaching significance\", and so on. I find these terms to be a little wishy-washy, in some cases a borderline disingenuous way to pull a meaningful result out of a study with negative results. Are these terms acceptable to describe results that \"just miss\" your p-value cutoff?","Creater_id":76825,"Start_date":"2015-09-17 07:15:43","Question_id":172928,"Tags":["hypothesis-testing","statistical-significance","p-value","terminology"],"Answer_count":5,"Last_activity":"2016-07-31 14:16:46","Link":"http://stats.stackexchange.com/questions/172928/is-it-wrong-to-refer-to-results-as-nearly-or-somewhat-significant","Creator_reputation":726}
{"_id":{"$oid":"5837a588a05283111e4d6bf9"},"View_count":1509,"Display_name":"Zenit","Question_score":20,"Question_content":"I've been reading up on -values, type 1 error rates, significance levels, power calculations, effect sizes and the Fisher vs Neyman-Pearson debate. This has left me feeling a bit overwhelmed. I apologise for the wall of text, but I felt it was necessary to provide an overview of my current understanding of these concepts, before I moved on to my actual questions.From what I've gathered, a -value is simply a measure of surprise, the probability of obtaining a result at least as extreme, given that the null hypothesis is true. Fisher originally intended for it to be a continuous measure.In the Neyman-Pearson framework, you select a significance level in advance and use this as an (arbitrary) cut-off point. The significance level is equal to the type 1 error rate. It is defined by the long run frequency, i.e. if you were to repeat an experiment 1000 times and the null hypothesis is true, about 50 of those experiments would result in a significant effect, due to the sampling variability. By choosing a significance level, we are guarding ourselves against these false positives with a certain probability. -values traditionally do not appear in this framework.If we find a -value of 0.01 this does not mean that the type 1 error rate is 0.01, the type 1 error is stated a priori. I believe this is one of the major arguments in the Fisher vs N-P debate, because -values are often reported as 0.05*, 0.01**, 0.001***. This could mislead people into saying that the effect is significant at a certain -value, instead of at a certain significance value.I also realise that the -value is a function of the sample size. Therefore, it cannot be used as an absolute measurement. A small -value could point to a small, non-relevant effect in a large sample experiment. To counter this, it is important to perform an power/effect size calculation when determining the sample size for your experiment. -values tell us whether there is an effect, not how large it is. See Sullivan 2012.My question:How can I reconcile the facts that the -value is a measure of surprise (smaller = more convincing) while at the same time it cannot be viewed as an absolute measurement?What I am confused about, is the following: can we be more confident in a small -value than a large one? In the Fisherian sense, I would say yes, we are more surprised. In the N-P framework, choosing a smaller significance level would imply we are guarding ourselves more strongly against false positives. But on the other hand, -values are dependent on sample size. They are not an absolute measure. Thus we cannot simply say 0.001593 is more significant than 0.0439. Yet this what would be implied in Fisher's framework: we would be more surprised to such an extreme value. There's even discussion about the term highly significant being a misnomer: Why is it wrong to refer to results as being \u0026quot;highly significant\u0026quot;?I've heard that -values in some fields of science are only considered important when they are smaller than 0.0001, whereas in other fields values around 0.01 are already considered highly significant.Related questions:Is the \u0026quot;hybrid\u0026quot; between Fisher and Neyman-Pearson approaches to statistical testing really an \u0026quot;incoherent mishmash\u0026quot;?When to use Fisher and Neyman-Pearson framework?Is the exact value of a \u0026#39;p-value\u0026#39; meaningless?Frequentist properties of p-values in relation to type I errorConfidence intervals vs P-values for two meansWhy are lower p-values not more evidence against the null? Arguments from Johansson 2011 (as provided by @amoeba)","Creater_id":62518,"Start_date":"2015-02-14 11:35:41","Question_id":137702,"Tags":["hypothesis-testing","statistical-significance","confidence-interval","p-value","effect-size"],"Answer_count":4,"Last_activity":"2016-07-31 14:11:21","Link":"http://stats.stackexchange.com/questions/137702/are-smaller-p-values-more-convincing","Creator_reputation":406}
{"_id":{"$oid":"5837a588a05283111e4d6c09"},"View_count":471,"Display_name":"wildetudor","Question_score":10,"Question_content":"Why do statisticians discourage us from referring to results as \"highly significant\" when the -value is well below the conventional -level of ?Is it really wrong to trust a result that has 99.9% chance of not being a Type I error () more than a result that only gives you that chance at 99% ()?","Creater_id":41307,"Start_date":"2014-07-11 11:28:02","Question_id":107640,"Tags":["hypothesis-testing","statistical-significance","p-value","terminology"],"Answer_count":3,"Last_activity":"2016-07-31 14:10:28","Link":"http://stats.stackexchange.com/questions/107640/is-it-wrong-to-refer-to-results-as-being-highly-significant","Creator_reputation":271}
{"_id":{"$oid":"5837a589a05283111e4d6c18"},"View_count":28,"Display_name":"SwingingStrawberry","Question_score":0,"Question_content":"With regard to categorical / dummy variables entered into a multiple regression to predict a continuous outcome variable, I'm a bit unsure about which of the assumptions are not relevant due to the 1,0 nature of the variable input - I previously asked about the linearity being required and that one is not required thanks to an answer I got. Of the four assumptions below, are any of them still required?  1 – Independence of errors (residuals).2 – there should be homoscedasticity of residuals (equal error variances) 3 – there should be no multicollinearity 4 – The errors (residuals) should be approximately normally distributed","Creater_id":116661,"Start_date":"2016-07-31 12:52:31","Question_id":226584,"Tags":["regression","multiple-regression"],"Answer_count":0,"Last_activity":"2016-07-31 13:10:36","Link":"http://stats.stackexchange.com/questions/226584/regression-assumptions-not-required-for-categorical-dummy-variables","Creator_reputation":32}
{"_id":{"$oid":"5837a589a05283111e4d6c1a"},"View_count":26,"Display_name":"Curlew","Question_score":0,"Question_content":"Suppose I am fitting a Generalized mixed effects model (R lme4 package ) for a number of permutations (randomized subsets of the same dataset).I am obtaining a model fit for each permutation iteration (N = 100) and thus end up with 100 seperate model fits.My goal is to average these individual fits and to calculate some kind of uncertainty boundary (standard error) across all models.What would be the theoretical steps to take? Currently I am just calculating the average within the predictors value range and the standard deviation of the individual average models. But I would prefer to take the standard error into account too. Would it make sense to average standard errors?Note:Because each model is fitted on a different permutation (\"different dataset\"), they are thus not comparable via AIC. So any kind of model averaging via AIC weights would not work...","Creater_id":16891,"Start_date":"2016-07-31 12:44:22","Question_id":226583,"Tags":["regression","mixed-model","permutation","randomization","model-averaging"],"Answer_count":0,"Last_activity":"2016-07-31 12:44:22","Link":"http://stats.stackexchange.com/questions/226583/average-multiple-model-predictions-and-standard-errors","Creator_reputation":61}
{"_id":{"$oid":"5837a589a05283111e4d6c1c"},"View_count":27,"Display_name":"Baz","Question_score":2,"Question_content":"I have a data set where n participants were asked to give a yes or no response to of 12 different (and distinct) stimuli. Each stimulus fits into 1 of 3 categories and an item for each category appears 4 times.This all fits neatly into a 3x2 contingency table, but each participant is contributing to the table 12 times! I would compare categories with a chi-square test, but the assumption of independence is broken.Are there alternatives that would account for this? Should I be looking at totaling the responses into 0 to 4 scores?","Creater_id":73369,"Start_date":"2016-07-31 12:12:03","Question_id":226581,"Tags":["chi-squared","contingency-tables"],"Answer_count":0,"Last_activity":"2016-07-31 12:34:28","Link":"http://stats.stackexchange.com/questions/226581/alternative-to-chi-square-where-independence-assumption-is-broken","Creator_reputation":11}
{"_id":{"$oid":"5837a589a05283111e4d6c1e"},"View_count":3878,"Display_name":"user42174","Question_score":3,"Question_content":"My data has a binary response (correct/incorrect), one continuous predictor score, three categorical predictors (race, sex, emotion) and a random intercept for the random factor subj. All predictors are within-subject. One of the categorical factor has 3 levels, the other have two. I need advice on obtaining \"global\" p-values for each categorical factor (in an \"ANOVA like\" way)Here is how I proceed :I fitted a binomial GLMM using 'glmer' from the lme4 package (because 'glmmML' doesn't compute on my data and glmmPQL does not provide AIC) and did model selection using drop1 repeatedly until no more terms can be dropped. Here is the final model (let's assume it has been validated):library(lme4)M5 \u0026lt;- glmer(acc ~ race + sex + emotion + sex:emotion + race:emotion + score +(1|subj),         family=binomial, data=subset)# apparently using family with lmer is deprecated drop1(M5, test=\"Chisq\")summary(M5)drop1 gives p-values for the higher level terms only (the two 2-way interactions + score). summarygives p-values for every term, but separates the different levels of each categorical factor.How can I get \"global\" p-values for each factor? I need to report them even if they are not the most relevant or meaningful estimates of signifiance here. How should I proceed? I tried searching on the web and ended up reading about likelihood ratios or the \"Wald test\" but I am not sure if or how this would apply here.(PS: This is a duplicate from my \"anonymous\" post here that needed editing: Binomial mixed model with categorical predictors: model selection and getting p-values Sorry about that.)","Creater_id":42174,"Start_date":"2014-03-18 15:58:35","Question_id":90511,"Tags":["r","mixed-model","binomial","p-value","glmm"],"Answer_count":1,"Last_activity":"2016-07-31 11:48:15","Link":"http://stats.stackexchange.com/questions/90511/binomial-glmm-with-categorical-predictors-p-values","Creator_reputation":63}
{"_id":{"$oid":"5837a589a05283111e4d6c2b"},"View_count":17,"Display_name":"user53020","Question_score":0,"Question_content":"I have a count response across 33 years which I am analyzing using quasipoisson regression.mdl\u0026lt;- glm(y ~ year, family=\"quasipoisson\")Now since y is a temporal data, I need to account for temporal autocorrelation at lag(1). I could do this in using glsmdl \u0026lt;- gls(y ~ year,correlation = corAR1(form=~year))but in gls how do I specify the error family which is \"quasipoisson\"?Thanks","Creater_id":53020,"Start_date":"2016-07-31 11:42:32","Question_id":226577,"Tags":["r","time-series","autocorrelation"],"Answer_count":0,"Last_activity":"2016-07-31 11:42:32","Link":"http://stats.stackexchange.com/questions/226577/r-specify-autocorrelation-at-lag1-in-a-poisson-regression","Creator_reputation":63}
{"_id":{"$oid":"5837a589a05283111e4d6c2d"},"View_count":54,"Display_name":"funda","Question_score":3,"Question_content":"This is my first post, so let me know if I break any rules. I'm trying to learn about neural networks and I have implemented some toy examples; now I'm trying with a real data set. My data set has about 70000 points  where  has about  dimensions and  is binary variable. I'm trying to train a feedforward multilayer NN with ReLu nonlinearities and softmax loss via stochastic gradient descent. The problem I'm running into is that overall only ~1% of training cases has , so my batches need to be pretty big to get any positive samples. My question is generally what kind of things should I be paying attention to or tweak in order to deal with this kind of improbable event data.Thanks!","Creater_id":125105,"Start_date":"2016-07-30 16:29:38","Question_id":226503,"Tags":["neural-networks"],"Answer_count":1,"Last_activity":"2016-07-31 10:46:54","Link":"http://stats.stackexchange.com/questions/226503/neural-network-with-improbable-events","Creator_reputation":18}
{"_id":{"$oid":"5837a589a05283111e4d6c3a"},"View_count":83,"Display_name":"Andrew Olney","Question_score":2,"Question_content":"As far as I can tell, PBmodcomp is doing some straight up bootstrapping, similar to this Faraway example for lmer models mmod and rmod (maximal and reduced models):lrstat \u0026lt;- numeric(1000)for(i in 1:1000){rmath \u0026lt;- unlist(simulate(rmod))bmod \u0026lt;- refit(mmod, rmath)smod \u0026lt;- refit(rmod, rmath)lrstat[i] \u0026lt;- 2*(logLik(bmod)-logLik(smod))}pvalue \u0026lt;- mean(lrstat \u0026gt; olrt)That Faraway pvalue is a bit off, see http://www.ncbi.nlm.nih.gov/pmc/articles/PMC379178/The PBmodcomp documentation is in line with the above link as far as calculating the pvalue http://www.jstatsoft.org/v59/i09/paper.However, in the code for PBmodcomp something else is going on as far as calculating the pvalue:refpos \u0026lt;- ref[ref\u0026gt;0]nsim \u0026lt;- length(ref)npos \u0026lt;- length(refpos)n.extreme \u0026lt;- sum(tobs \u0026lt; refpos)p.PB  \u0026lt;- (1+n.extreme) / (1+npos)Instead of using all iterations in the denominator, the code is only using the iterations that resulted in a positive value.This biases the pvalue up. In some tests I'm running, I'm getting .12 instead of .08, which is quite a difference.I'm wondering if someone can explain this different scaling factor and provide a reference. I've seen many examples of bootstrapping like the Faraway above, but never this.My guess is that the justification may be that the bootstrapped test statistic here should always be positive because it should be asymptotically chi square distributed. Even so, using that to correct the p-value in such a strong way makes me uncomfortable without more justification (especially since it departs from what appears to be standard practice).","Creater_id":77273,"Start_date":"2015-08-24 19:26:07","Question_id":168632,"Tags":["r","p-value","bootstrap","lmer"],"Answer_count":1,"Last_activity":"2016-07-31 10:24:55","Link":"http://stats.stackexchange.com/questions/168632/bootstrap-pvalues-strange-adjustment-pbkrtest-pbmodcomp-lrt","Creator_reputation":111}
{"_id":{"$oid":"5837a589a05283111e4d6c47"},"View_count":45,"Display_name":"webNash","Question_score":1,"Question_content":"I have implemented backpropagation algorithm for neural network. The neural network is trained using online stochastic gradient descent. (with regularization). I have used a separate training and validation data sets. would like to know the how to determine the optimum number of epochs.Thanks","Creater_id":95075,"Start_date":"2016-07-31 04:39:39","Question_id":226539,"Tags":["machine-learning","neural-networks","gradient-descent"],"Answer_count":1,"Last_activity":"2016-07-31 10:15:14","Link":"http://stats.stackexchange.com/questions/226539/backpropagation-neural-network-stoppping-criteria-with-online-training-with-stoc","Creator_reputation":30}
{"_id":{"$oid":"5837a589a05283111e4d6c54"},"View_count":2204,"Display_name":"tan","Question_score":5,"Question_content":"I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?Here is my code:library('glmnet')data \u0026lt;- read.csv('datafile.csv', header=T)mat = as.matrix(data)X = mat[,1:ncol(mat)-1] y = mat[,ncol(mat)]fit \u0026lt;- cv.glmnet(X,y, family=\"binomial\")Another question is:I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?","Creater_id":12245,"Start_date":"2012-12-08 15:16:41","Question_id":45449,"Tags":["r","multiple-regression","lasso","glmnet"],"Answer_count":1,"Last_activity":"2016-07-31 09:56:47","Link":"http://stats.stackexchange.com/questions/45449/when-using-glmnet-how-to-report-p-value-significance-to-claim-significance-of-pr","Creator_reputation":217}
{"_id":{"$oid":"5837a589a05283111e4d6c61"},"View_count":46,"Display_name":"Arran","Question_score":0,"Question_content":"Whereby no interaction terms or main effects are significant in a model, should model simplification be carried out and the interaction terms removed sequentially until the remaining variables are either significant/insignificant?  Info on the model I am running: I am currently carrying out a GLMM with negative binomial distribution looking at the effects of different factors on the number of farmland birds along the edges of fields. The main fixed effects included in the model are: Crop type (3 level factor, which was experimentally manipulated) Hedgerow structure (continuous) Percentage gaps (continuous)Month (2 level factor) with all 2 way interactions and 3 way interactions with month included. Field is included as a random factor to account for non-independence and offset is field length. This is the first time i have deal with complex models and as i am partly hypothesis testing (whether crop type has an effect) I am unsure what the correct protocol is.The model output is included below. Thanks ","Creater_id":124974,"Start_date":"2016-07-31 01:04:14","Question_id":226516,"Tags":["hypothesis-testing","interaction","model-selection","glmm","stepwise-regression"],"Answer_count":1,"Last_activity":"2016-07-31 09:37:50","Link":"http://stats.stackexchange.com/questions/226516/should-model-simplification-be-carried-out-where-none-of-the-variables-included","Creator_reputation":3}
{"_id":{"$oid":"5837a589a05283111e4d6c6e"},"View_count":133,"Display_name":"Michael M","Question_score":5,"Question_content":"Is there any bootstrap technique available to compute prediction intervals for point predictions obtained e.g. from linear regression or other regression method (k-nearest neighbour, regression trees etc.)?Somehow I feel that the sometimes proposed way to just bootsrap the point prediction (see e.g. Prediction intervals for kNN regression) is not providing a prediction interval but a confidence interval.An example in R# STEP 1: GENERATE DATAset.seed(34345)n \u0026lt;- 100 x \u0026lt;- runif(n)y \u0026lt;- 1 + 0.2*x + rnorm(n)data \u0026lt;- data.frame(x, y)# STEP 2: COMPUTE CLASSIC 95%-PREDICTION INTERVALfit \u0026lt;- lm(y ~ x)plot(fit) # not shown but looks fine with respect to all relevant aspects# Classic prediction interval based on standard error of forecastpredict(fit, list(x = 0.1), interval = \"p\")# -0.6588168 3.093755# Classic confidence interval based on standard error of estimationpredict(fit, list(x = 0.1), interval = \"c\")# 0.893388 1.54155# STEP 3: NOW BY BOOTSTRAPB \u0026lt;- 1000pred \u0026lt;- numeric(B)for (i in 1:B) {  boot \u0026lt;- sample(n, n, replace = TRUE)  fit.b \u0026lt;- lm(y ~ x, data = data[boot,])  pred[i] \u0026lt;- predict(fit.b, list(x = 0.1))}quantile(pred, c(0.025, 0.975))# 0.8699302 1.5399179Obviously, the 95% basic bootstrap interval matches the 95% confidence interval, not the 95% prediction interval. So my question: How to do it properly?","Creater_id":30351,"Start_date":"2016-07-31 09:09:54","Question_id":226565,"Tags":["bootstrap","prediction-interval"],"Answer_count":0,"Last_activity":"2016-07-31 09:09:54","Link":"http://stats.stackexchange.com/questions/226565/bootstrap-prediction-interval","Creator_reputation":4134}
{"_id":{"$oid":"5837a589a05283111e4d6c70"},"View_count":67,"Display_name":"BeStats","Question_score":2,"Question_content":"I run two lmer tests, one with and one without the interaction term between fixed effects. The problem is that the former gives an output result that makes no sense to the actual data (i.e. negative slope instead of positive), whereas the latter shows the expected output. Why does this happen and even though the interaction is significant (and also makes sense) does it mean that I should not include it in the model due to wrong output? Would it be better to run a model with only the fixed factors and another with the interaction term alone?Below is the models and their outputs. Thank you!(WITHOUT INTERACTION TERM)mTEST\u0026lt;- lmer(amp.sqrt~ treatment + time + axis + (1+treatment|ID))summary(mTEST)Linear mixed model fit by REML t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']Formula: amp.sqrt ~ treatment + time + axis + (1 + treatment | ID)REML criterion at convergence: 5682.2Scaled residuals:     Min      1Q  Median      3Q     Max -2.2769 -0.7678 -0.0236  0.6049  3.5182 Random effects: Groups   Name        Variance Std.Dev. Corr        ID       (Intercept)  602.8   24.55                         treatment2  1028.9   32.08    -0.14                treatment3   283.2   16.83    -0.03  0.52 Residual             2027.6   45.03               Number of obs: 540, groups:  ID, 21Fixed effects:            Estimate Std. Error      df t value Pr(\u0026gt;|t|)    (Intercept)  115.184      7.546  36.300  15.265  \u0026lt; 2e-16 ***treatment2     2.644      8.571  18.400   0.308  0.76117    treatment3    23.365      6.139  19.200   3.806  0.00117 ** time7         13.958      4.707 474.800   2.965  0.00318 ** time8         21.799      4.787 478.500   4.554  6.7e-06 ***axis2         60.458      4.746 474.800  12.737  \u0026lt; 2e-16 ***axis3        128.456      4.746 474.800  27.063  \u0026lt; 2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Correlation of Fixed Effects:           (Intr) trtmn2 trtmn3 time7  time8  axis2 treatment2 -0.287                                   treatment3 -0.299  0.506                            time7      -0.312  0.000  0.000                     time8      -0.314  0.013  0.008  0.492              axis2      -0.315  0.000  0.000  0.000  0.000       axis3      -0.315  0.000  0.000  0.000  0.000  0.500(WITH INTERACTION TERM)mTEST2\u0026lt;- lmer(amp.sqrt~ treatment * time + axis + (1+treatment|ID))summary(mTEST2)Linear mixed model fit by REML t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']Formula: amp.sqrt ~ treatment * time + axis + (1 + treatment | ID)REML criterion at convergence: 5615.6Scaled residuals:     Min      1Q  Median      3Q     Max -2.7117 -0.7237 -0.0390  0.6140  3.3017 Random effects: Groups   Name        Variance Std.Dev. Corr        ID       (Intercept)  619.0   24.88                         treatment2  1061.1   32.58    -0.16                treatment3   296.4   17.22    -0.06  0.54 Residual             1879.0   43.35               Number of obs: 540, groups:  ID, 21Fixed effects:                 Estimate Std. Error      df t value Pr(\u0026gt;|t|)    (Intercept)       130.587      8.417  55.500  15.515  \u0026lt; 2e-16 ***treatment2         -3.766     10.713  44.500  -0.352   0.7269    treatment3        -14.929      8.851  83.600  -1.687   0.0954 .  time7              -7.697      8.120 471.000  -0.948   0.3436    time8              -2.628      8.120 471.000  -0.324   0.7464    axis2              60.458      4.569 471.000  13.232  \u0026lt; 2e-16 ***axis3             128.456      4.569 471.000  28.113  \u0026lt; 2e-16 ***treatment2:time7    9.697     11.206 471.000   0.865   0.3873    treatment3:time7   53.206     11.206 471.000   4.748 2.73e-06 ***treatment2:time8    8.554     11.396 473.700   0.751   0.4532    treatment3:time8   62.411     11.289 473.300   5.528 5.35e-08 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Correlation of Fixed Effects:            (Intr) trtmn2 trtmn3 time7  time8  axis2  axis3  trt2:7 trt3:7 trt2:8treatment2  -0.448                                                               treatment3  -0.479  0.515                                                        time7       -0.482  0.379  0.459                                                 time8       -0.482  0.379  0.459  0.500                                          axis2       -0.271  0.000  0.000  0.000  0.000                                   axis3       -0.271  0.000  0.000  0.000  0.000  0.500                            trtmnt2:tm7  0.349 -0.523 -0.332 -0.725 -0.362  0.000  0.000                     trtmnt3:tm7  0.349 -0.275 -0.633 -0.725 -0.362  0.000  0.000  0.525              trtmnt2:tm8  0.344 -0.514 -0.327 -0.356 -0.712  0.000  0.000  0.492  0.258       trtmnt3:tm8  0.347 -0.272 -0.628 -0.360 -0.719  0.000  0.000  0.261  0.496  0.512","Creater_id":121550,"Start_date":"2016-07-30 18:19:31","Question_id":226505,"Tags":["r","interaction"],"Answer_count":1,"Last_activity":"2016-07-31 08:55:04","Link":"http://stats.stackexchange.com/questions/226505/why-is-there-an-odd-output-result-when-adding-an-interaction-term-to-a-linear-mo","Creator_reputation":35}
{"_id":{"$oid":"5837a589a05283111e4d6c7d"},"View_count":22085,"Display_name":"Graham Cookson","Question_score":26,"Question_content":"Difference in differences has long been popular as a non-experimental tool, especially in economics. Can somebody please provide a clear and non-technical answer to the following questions about difference-in-differences.What is a difference-in-difference estimator?Why is a difference-in-difference estimator any use?Can we actually trust difference-in-difference estimates?","Creater_id":215,"Start_date":"2010-07-23 09:57:50","Question_id":564,"Tags":["regression","econometrics","difference-in-difference"],"Answer_count":3,"Last_activity":"2016-07-31 08:48:44","Link":"http://stats.stackexchange.com/questions/564/what-is-difference-in-differences","Creator_reputation":4126}
{"_id":{"$oid":"5837a589a05283111e4d6c8b"},"View_count":627,"Display_name":"Jason","Question_score":1,"Question_content":"I have a question about the \"weights\" and \"prior\" in R's rpart function. This question has been asked before here, but the answer doesn't quite make sense.Currently I have very unbalanced data where the target is only 0.0066% of the whole dataset, which has over 2 million rows. I want to know if either the \"weights\" or the \"prior\" can help me with this biased dataset, and how they would be used.I tried oversampling the target and downsampling the noise and then producing an ensemble of my predictions, but I did not achieved the desired result.","Creater_id":82824,"Start_date":"2015-07-20 15:51:45","Question_id":162372,"Tags":["r","rpart"],"Answer_count":1,"Last_activity":"2016-07-31 08:12:44","Link":"http://stats.stackexchange.com/questions/162372/difference-between-weights-and-prior-in-rpart-and-how-to-use-them","Creator_reputation":6}
{"_id":{"$oid":"5837a589a05283111e4d6c98"},"View_count":46,"Display_name":"slazien","Question_score":0,"Question_content":"First, a short introduction. I am predicting latitude/longitude using Random Forests and XGBoost based on several environmental variables and custom features such as cluster IDs (there are obvious spatial clusters in data). Based on validation set, predictions from RF are slightly better than XGBoost (I am minimising RMSE). I came up with an idea of taking a (weighted) average of these raw predictions and calculating RMSE based on these. My reasoning behind this is that if we assign a higher weight to a better model (in this case RF) and a lower one to weaker (XGBoost), the predictions could improve. Surprisingly, RMSE is indeed lower when I combine the predictions.I also attached 2D function plots for both latitude and longitude. Clearly the functions look almost identical. What's interesting is that the results follow the intuition: assigning bigger weight to the better model (RF) results in better RMSE compared to assigning a bigger weight to the weaker model (XGBoost). This can be seen in plots below.The best weights were found to be as follows:Now, my question is - is that a valid approach from a statistical/machine learning point of view? If not, are there any approaches of combining model results (apart from model ensembles) which I could try?Code for analysis:weight_rf \u0026lt;- seq(0, 1, by = 0.01)weight_xgb \u0026lt;- seq(0, 1, by = 0.01)len_rf \u0026lt;- length(weight_rf)len_xgb \u0026lt;- length(weight_xgb)lat_grid \u0026lt;- matrix(nrow = len_rf, ncol = len_xgb)lon_grid \u0026lt;- lat_gridpb \u0026lt;- txtProgressBar(min = 0, max = len_rf * len_xgb, style = 3)iter = 1for(i in 1:len_rf) {      for(j in 1:len_xgb) {            mean.lat.pred \u0026lt;- (weight_rf[i] * rf.lat.pred + weight_xgb[j] * xgb.lat.pred)/(weight_rf[i] + weight_xgb[j])            mean.lon.pred \u0026lt;- (weight_rf[i] * rf.lon.pred + weight_xgb[j] * xgb.lon.pred)/(weight_rf[i] + weight_xgb[j])            lat_grid[i, j] \u0026lt;- return_metrics(mean.lat.pred, mean.lon.pred, tzlon_deg[-train])lat_deg[-train], tzRMSE_lon            iter = iter + 1            setTxtProgressBar(pb, iter)      }}image(lat_grid, xlab = \"RF weight\", ylab = \"XGBoost weight\", main = \"RMSE for Latitude\", col = color_ramp, axes = F)axis(1, at = seq(0, 1, length.out = len_rf), labels = weight_rf)axis(2, at = seq(0, 1, length.out = len_xgb), labels = weight_xgb)image(lon_grid, xlab = \"RF weight\", ylab = \"XGBoost weight\", main = \"RMSE for Longitude\", col = color_ramp, axes = F)axis(1, at = seq(0, 1, length.out = len_rf), labels = weight_rf)axis(2, at = seq(0, 1, length.out = len_xgb), labels = weight_xgb)lat_best \u0026lt;- arrayInd(which.min(lat_grid), dim(lat_grid))lon_best \u0026lt;- arrayInd(which.min(lon_grid), dim(lon_grid))lat_rf_best \u0026lt;- weight_rf[lat_best[1]]lat_xgboost_best \u0026lt;- weight_rf[lat_best[2]]lon_rf_best \u0026lt;- weight_rf[lon_best[1]]lon_xgboost_best \u0026lt;- weight_rf[lon_best[2]]best_df \u0026lt;- data.frame(rf = c(lat_rf_best, lon_rf_best), xgboost = c(lat_xgboost_best, lon_xgboost_best))rownames(best_df) \u0026lt;- c(\"lat\", \"lon\")best_df","Creater_id":121401,"Start_date":"2016-07-31 07:47:52","Question_id":226555,"Tags":["machine-learning","random-forest","geography","xgboost"],"Answer_count":0,"Last_activity":"2016-07-31 07:47:52","Link":"http://stats.stackexchange.com/questions/226555/averaging-predictions-from-two-different-models","Creator_reputation":58}
{"_id":{"$oid":"5837a589a05283111e4d6c9a"},"View_count":67,"Display_name":"Teresa","Question_score":0,"Question_content":"In the output of a GLMM, using a binary variable as response variable and continuous variables as explanatory variables [family = binomial(link=\"logit\")], I obtain, for each variable, an estimate value,  standard error, a z-value and a Pr(\u003e|z|). 1) Is the z-value simmilar to the effect size?2) If not, how can I obtain the effect size for each variable?","Creater_id":117281,"Start_date":"2016-07-31 05:51:02","Question_id":226545,"Tags":["effect-size","glmm"],"Answer_count":1,"Last_activity":"2016-07-31 07:25:48","Link":"http://stats.stackexchange.com/questions/226545/effect-size-in-glmm","Creator_reputation":59}
{"_id":{"$oid":"5837a589a05283111e4d6ca6"},"View_count":22,"Display_name":"Dang Manh Truong","Question_score":1,"Question_content":"I'm working on a project about using Eigenfaces for classification: http://courses.cs.washington.edu/courses/cse455/10wi/projects/p2/ . But in here: http://www.cs.cmu.edu/~pmuthuku/mlsp_page/assignments/assignment2_hints.html it says that the image needs to be converted to grayscale. Is this true? The first link does not say anything about that.","Creater_id":120103,"Start_date":"2016-07-31 01:57:41","Question_id":226522,"Tags":["pca","image-processing"],"Answer_count":0,"Last_activity":"2016-07-31 07:16:10","Link":"http://stats.stackexchange.com/questions/226522/in-eigenfaces-decomposition-should-the-images-be-converted-to-grayscale","Creator_reputation":62}
{"_id":{"$oid":"5837a589a05283111e4d6ca8"},"View_count":20,"Display_name":"user53020","Question_score":0,"Question_content":"I was interested in knowing if for a quasipoisson regression, the residuals are on a log scaleIf this is my modelmdl\u0026lt;-glm(y ~ x1 +x1, family=\"quasipoisson\")To get the slope on the response scale, I do thisexp(mdlresiduals)-1","Creater_id":53020,"Start_date":"2016-07-31 04:17:24","Question_id":226535,"Tags":["r","generalized-linear-model","poisson-regression"],"Answer_count":1,"Last_activity":"2016-07-31 06:20:10","Link":"http://stats.stackexchange.com/questions/226535/are-the-residuals-of-quasipoisson-regression-in-logged-scale","Creator_reputation":63}
{"_id":{"$oid":"5837a589a05283111e4d6cb5"},"View_count":27,"Display_name":"AliceJ","Question_score":2,"Question_content":"I'm doing a study looking at the association between poor food hygiene  (exposure) and stunting (outcome).Maternal short stature and low infant birth weight are both risk factors for stunting - however, in my opinion they are not related to the exposure.Should these factors be controlled for?Sorry, i'm new to statistics and really appreciate any advice!Many thanks.","Creater_id":125109,"Start_date":"2016-07-30 22:31:38","Question_id":226512,"Tags":["epidemiology","confounding"],"Answer_count":1,"Last_activity":"2016-07-31 05:48:53","Link":"http://stats.stackexchange.com/questions/226512/should-all-risk-factors-for-a-disease-be-treated-as-confounders-even-if-they-are","Creator_reputation":13}
{"_id":{"$oid":"5837a589a05283111e4d6cc1"},"View_count":17,"Display_name":"Hamid Oskorouchi","Question_score":0,"Question_content":"I am running a multivariate linear regression model by means of OLS.I have checked for normality of the errors using my dependent variable transformed (log, square, square-root). I have noticed that using the square root gives the errors distributing exactly on the normal line of the qqnorm plot.However, using this transformation of my DV would result in difficulties when interpreting the results.I have thus run the same regression, with the DV not transformed, but with weights equal to sqrt(DV). This gives exactly the same coefficients of the model with sqrt(DV) as regressand.model 1 \u0026lt;- lm(I(sqrt(y)) ~ x)model 2 \u0026lt;- lm(y ~ x, weights = I(sqrt(y)) )Could someone give me some hints on the reason why the two model are equivalent?Am I legitimated to use model 2?What are the implication and underling assumptions of using model 2?","Creater_id":123092,"Start_date":"2016-07-31 05:24:48","Question_id":226542,"Tags":["multiple-regression","weighted-regression"],"Answer_count":0,"Last_activity":"2016-07-31 05:24:48","Link":"http://stats.stackexchange.com/questions/226542/use-function-of-the-dependent-variable-as-weights-in-ols","Creator_reputation":65}
{"_id":{"$oid":"5837a589a05283111e4d6cc3"},"View_count":26,"Display_name":"loganecolss","Question_score":0,"Question_content":"Given a user-item rating matrix , matrix factorization are usually used to learn latent factors for user  and items .However, no matter how we train the model (SGD or ALS) and how we regularize the parameters, we still might get negative predicted ratings, right?How do we solve this issue other than resorting to non-negative matrix factorization (NMF)? ","Creater_id":30540,"Start_date":"2016-07-31 05:05:53","Question_id":226541,"Tags":["recommender-system","matrix-decomposition"],"Answer_count":0,"Last_activity":"2016-07-31 05:05:53","Link":"http://stats.stackexchange.com/questions/226541/negative-output-of-matrix-factorization-on-user-item-rating-matrix","Creator_reputation":523}
{"_id":{"$oid":"5837a589a05283111e4d6cc5"},"View_count":12,"Display_name":"julypraise","Question_score":0,"Question_content":"Let  be a kernel. And let us be given data samples  and a feature map  such that . Now letS_B = \\sum_c (\\mu_c - \\overline{\\phi(x)})(\\mu_c - \\overline{\\phi(x)})^Twhere . Now I need to prove, assuming , w^T S_B w = \\alpha^T S^{\\phi}_B \\alphawhere S^{\\phi}_B = \\sum_c N_c[\\kappa_c \\kappa_c^T - \\kappa \\kappa^T].Is the formula (13) correct in the note?For , I getS^{\\Phi}_B =\\sum_c (\\kappa_c-\\kappa)(\\kappa_c-\\kappa)^T, not the one mentioned in the note.","Creater_id":102712,"Start_date":"2016-07-31 01:42:01","Question_id":226520,"Tags":["discriminant-analysis"],"Answer_count":0,"Last_activity":"2016-07-31 04:53:06","Link":"http://stats.stackexchange.com/questions/226520/kfda-kernel-fisher-da-formula-check","Creator_reputation":101}
{"_id":{"$oid":"5837a589a05283111e4d6cc7"},"View_count":20,"Display_name":"GRS","Question_score":0,"Question_content":"Let  and The likelihood function is (without constant term):-\\frac{n}{2}\\log{\\theta_1}-\\frac{1}{2\\theta_1}\\sum^n(x_i -\\mu_1)^2-\\frac{m}{2}\\log{\\theta_2}-\\frac{1}{2\\theta_2}\\sum^m(y_i -\\mu_2)^2I found the MLEs to be:Now I need to test that  and  have common variance: Using likelihood ratio: I need to test:What would be ? Is ,since we have  estimated?As for the test,l(\\theta)=-\\frac{n}{2}\\log{\\theta}-\\frac{1}{2\\theta}\\sum^n(x_i -\\mu_1)^2-\\frac{m}{2}\\log{\\theta}-\\frac{1}{2\\theta}\\sum^m(y_i -\\mu_2)^2l(\\hat\\theta)=-\\frac{n}{2}\\log{\\hat\\theta_1}-\\frac{1}{2\\hat\\theta_1}\\sum^n(x_i -\\mu_1)^2-\\frac{m}{2}\\log{\\hat\\theta_2}-\\frac{1}{2\\hat\\theta_2}\\sum^m(y_i -\\mu_2)^2Can I replace  by I am asked to show that the likelihood ratio is a function of R=\\frac{\\sum (x_i-\\bar x)^2}{\\sum(y_i-\\bar y)^2}l(\\theta)-l(\\hat\\theta)=-\\frac{n}{2}\\log{\\frac{\\theta}{\\hat\\theta_1}}-\\frac{m}{2}\\log{\\frac{\\theta}{\\hat\\theta_2}}-\\frac{1}{2\\theta}\\sum^n(x_i -\\mu_1)^2-\\frac{1}{2\\theta}\\sum^m(y_i -\\mu_2)^2+\\frac{1}{2\\hat\\theta_1}\\sum^n(x_i -\\mu_1)^2+\\frac{1}{2\\hat\\theta_2}\\sum^m(y_i -\\mu_2)^2Another small question:What is the distribution  under the hypotesis they have equal variances. I know that  with  degress of freedom.","Creater_id":109646,"Start_date":"2016-07-31 04:33:56","Question_id":226537,"Tags":["chi-squared","maximum-likelihood","likelihood","likelihood-ratio"],"Answer_count":0,"Last_activity":"2016-07-31 04:45:05","Link":"http://stats.stackexchange.com/questions/226537/likelihood-ratio-testing-h-0-theta-1-theta-2-theta-normally-distributed","Creator_reputation":111}
{"_id":{"$oid":"5837a589a05283111e4d6cc9"},"View_count":34,"Display_name":"annie","Question_score":3,"Question_content":"I am comparing three groups of participants, who gave ratings during a certain test on a scale from  to  (with  steps). I was planning to do an ANOVA, but realized that a thing such as the mean of ratings , ,  is   and does not actually exist on my original scale.Is my data ordinal then? Should I do a Friedman test instead?I should maybe add that I consider the distance from  to  to be the same as from  to  etc. EDIT: actually, I am not sure if I should do that. Would the answer be any different?","Creater_id":125130,"Start_date":"2016-07-31 04:11:23","Question_id":226533,"Tags":["ordinal","scales","friedman-test"],"Answer_count":1,"Last_activity":"2016-07-31 04:31:59","Link":"http://stats.stackexchange.com/questions/226533/is-my-data-ordinal","Creator_reputation":18}
{"_id":{"$oid":"5837a589a05283111e4d6cd6"},"View_count":117,"Display_name":"half-pass","Question_score":3,"Question_content":"I need to realistically simulate study effect sizes and within-study variances for a random-effects meta-analysis in which the outcome is a relative risk. My question: why does this simulation lead to severe underestimation of , and how can I fix it?Simulation approachSimulate total sample sizes for each of  studies. Assume equal numbers in control and treatment groups for all studies.Draw a population effect, , for each study from a Normal with a specified mean and variance . (Keep your eye on the variance -- it is the heart of my woes.) This is on the log-RR scale. Draw the number of successes in the control group from a binomial with success probability  (fixed across all studies). Draw the number of success in the treatment group from a binomial with success probability .Compute observed effect sizes, , for each study from the above data. Fit a random-effects meta-analysis using Viechtbauer's metaforpackage.Become upset that .Explanations I've consideredNote that metafor by default estimates  via REML. Using other options, such as Dersimonian-Laird, does not help. Also, the estimated SE of  is also small, so it's not a precision problem. Finally, fiddling with the parameters doesn't help either. Reproducible example# set parameters.k = c(1000)  # huge number of studies to eliminate concerns about asymptotics.Mt = log(1.5)  # mean of true effects (log-RR).V = 0.3  # variance of true effectsp0=0.08  # P(success) in control groupseed = 131457# simulate total N for each study# right-skewed with minimum 20N = round( rchisq(.k, df=2) * 30 + 20 )# simulate population effect for each studyMi = rnorm( n=.k, mean=.Mt, sd=sqrt(.V) )##### Simulate Control Groups ##### # assume equal numbers in each treatment arm (so denom is N/2)n0 = floor(N/2)# simulate d0, number of successes in group 0d0 = rbinom( n=.k, size=n0, prob=p0 )##### Simulate Treatment Groups ##### # calculate n for this groupn1 = N - n0# figure out p(success) in this group using population RR = exp(Mi)# use pmin to ensure probability isn't above 1# I used a small p0 and smallish .Mt to ensure that there is very little truncationp1 = pmin( exp(Mi) * p0, 1 )  # simulate d1, number of successes in group 1d1 = rbinom( n=.k, size=n1, prob=p1 )# calculate true ES using metaforrequire(metafor)temp = escalc( measure=\"RR\", ai=d1, bi=n1-d1, ci=d0, di=n0-d0)#head( log( (d1/n1) / (d0/n0) ) ); head(temp)  # yup, matches manual approach# get observed effect sizes and within-study variances for each studyth.t = tempvi# see if true ES are hitting correct tau^2RE = rma.uni( yi=th.t, vi=vyi, measure=\"RR\")# these should be the same (var of true ES):print(REn\\tau^{2}$ is unbiased.Generating huge sample sizes from a Normal works:N = round( rnorm(.k, mean=10000, sd=800)   )Or even from a uniform:N = round( runif(.k, 1000, 3000) )But not generating small sample sizes from a uniform:N = round( runif(.k, 20, 200) )I'd still very much appreciate a theoretical explanation for this behavior. As far as I know, the random-effects model does not make any assumptions on the within-study variances, so it's disturbing that this seems to matter so much.","Creater_id":11511,"Start_date":"2016-07-23 16:49:55","Question_id":225311,"Tags":["r","mixed-model","simulation","meta-analysis","power-analysis"],"Answer_count":1,"Last_activity":"2016-07-31 03:41:16","Link":"http://stats.stackexchange.com/questions/225311/simulation-of-random-effects-meta-analysis-yields-biased-tau2","Creator_reputation":1070}
{"_id":{"$oid":"5837a589a05283111e4d6ce3"},"View_count":397,"Display_name":"sponge_knight","Question_score":3,"Question_content":"Suppose that I make a point estimate, 0.7, with a 90% CI: [0.6, 0.8].Can I say that in the worst case, the true parameter is 0.6 and in the best case it's 0.8?","Creater_id":46925,"Start_date":"2016-07-30 05:35:58","Question_id":226436,"Tags":["confidence-interval"],"Answer_count":3,"Last_activity":"2016-07-31 03:35:49","Link":"http://stats.stackexchange.com/questions/226436/can-you-say-that-confidence-intervals-measure-worst-case-and-best-case","Creator_reputation":1522}
{"_id":{"$oid":"5837a589a05283111e4d6cf2"},"View_count":32,"Display_name":"user123423","Question_score":3,"Question_content":"My background: I have completed an introductory course in statistics and done some further reading on my own. I have 3 months of R experience.I have been asked to design an experiment to measure the effect of removing ads on user spending and retention in a mobile game.The main criterion is to keep the test group small to minimize the amount of revenue at risk.For retention (measured as percent retained for a given day), I chose a level of confidence I wanted, and used the Clopper-Pearson method to find the necessary test group size, which was not prohibitively big.For spending, however, the distribution is exponential, with a long tail of outrageously big spenders, making the test group size needed to get enough accuracy prohibitively large. Has anybody run into this problem before? Are there any design techniques or stats tricks I can use to get the desired accuracy? I suspect there might a better confidence interval I than the standard CI for a continuous variable, which I am using. I read that bootstrapping can increase accuracy, but I don't really understand how it works.","Creater_id":123423,"Start_date":"2016-07-29 14:11:33","Question_id":226367,"Tags":["confidence-interval","sample-size","skewness"],"Answer_count":0,"Last_activity":"2016-07-31 03:35:16","Link":"http://stats.stackexchange.com/questions/226367/confidence-interval-for-mean-of-highly-skewed-distribution","Creator_reputation":16}
{"_id":{"$oid":"5837a589a05283111e4d6cf4"},"View_count":5,"Display_name":"Hussain","Question_score":1,"Question_content":"Could any one please tell me how to calculate the percent variance just like in this Table of PCA components,","Creater_id":125124,"Start_date":"2016-07-31 03:30:29","Question_id":226529,"Tags":["variance"],"Answer_count":0,"Last_activity":"2016-07-31 03:30:29","Link":"http://stats.stackexchange.com/questions/226529/percent-variance-for-pca-components","Creator_reputation":6}
{"_id":{"$oid":"5837a589a05283111e4d6cf6"},"View_count":102,"Display_name":"Mr.Morgan","Question_score":0,"Question_content":"I would like to write you beacause of the following issue: I´m estimating an IV-model with the following common structure: . I´ve found also a promising instrumental variable for , . In order to check overall robustness I used the original OLS and OLS vce robust specification and several 2SLS estimators. In general and beside some minor changes in coefficients and significance levels (probably due to the adequacy of IV-Regression) the theoretically hypothesized effects keep in place.  But as soon, as I modify my model to an interaction model: some really odd things happen: There is a very notable and thus confusing structural change in the values of coefficients and further significance related statistics between the classical OLS estimators and the several 2SLS estimators. In detail, every prior (in OLS) significant realtionship cancels out (e.g    and ) and the coefficients even change signs. As literature suggested in my first stage equation I´ve used the variable (() and ()) as an instrument itself for the newly added endogenous interaction terms (in stata notation e.g. ivregress Y (Xend (Xend*X1) (Xend*X2) = Xinstr. (Xinstr. * X1) (Xinstr. * X2)) X1 X2 Xcontrols). What is going on here? Why is this change happening?Here are some actual quick and dirty examples of my work on car sales and marketing strategies (please forgive me the formatting issues; i also shortened the actual output and the variations in estimators in the interest of time). As you can see in the original regressions (non-interaction) there is no big difference....but in the interaction model the obtained effects via OLS cancel out (especially for the two strategy related variables of main interest).quietly regress lnsales car_quality marketing_strategy1 marketing_strategy2 sourcing car_type1 car_type2 (+\"List of additional control variables\")estimates store OLS  quietly regress lnsales car_quality marketing_strategy1 marketing_strategy2sourcing car_type1 car_type2 (+\"List of additional control variables\"), robustestimates store OLS_robust global ivmodel lnsales (car_quality = peer_quality) marketing_strategy1   marketing_strategy2 sourcing car_type1 car_type2 (+\"List of additional control variables\")quietly ivregress 2sls ivmodel , vce(robust)estimates store TwoSLS__2quietly ivregress gmm ivmodel , wmatrix(robust) igmmestimates store IGMMquietly ivregress liml ivmodelestimates store TwoSLS_defquietly ivregress 2sls ivmodel , wmatrix(robust)estimates store GMM_hetestimates table OLS OLS_robust TwoSLS_def TwoSLS__2 GMM_het IGMM LIML, b se   p stats(N r2) ------------------------------------------------------------------------------    Variable |     OLS       OLS_robust   TwoSLS_def   TwoSLS__2      GMM_het -------------+----------------------------------------------------------------    car_~y   |  .30626371    .30626371    .40466472    .40466472    .40466472               |  .06639855    .08737882    .17734552    .14822445    .14822445              |     0.0000       0.0005       0.0225       0.0063       0.0063               |marketing_~1 | -2.7663962   -2.7663962    -1.022544    -1.022544    -1.022544              |  .87427115    .87740022     3.468728     3.021177     3.021177               |     0.0018       0.0018       0.7682       0.7350      0.7350               |marketing_~1#|    c.car~y  |          1  |  .40964628    .40964628    .14894708    .14894708    .14894708               |  .12788375    .12954421    .51333938    .44914179    .44914179              |     0.0015       0.0018       0.7717       0.7402       0.7402 marketing_~2 | -1.6974189   -1.6974189   -.81075049   -.81075049   -.81075047              |  1.2256574    1.0156041    4.4093988    3.5747531    3.5747531              |     0.1674       0.0960       0.8541       0.8206       0.8206             |marketing_~2#|    c.car~y  |          1  |  .20617457    .20617457    .07077817    .07077817    .07077817              |  .18004716    .14488011    .65063831    .53051219    .53051219              |     0.2533       0.1560       0.9134       0.8939       0.8939              |sourcing     |  .02814061    .02814061    .01454754    .01454754    .01454754              |  .15052717    .13857819    .17351094    .14787563    .14787563              |     0.8519       0.8393       0.9332       0.9216       0.9216    car_~1   | -.23592028   -.23592028   -.28205832   -.28205832   -.28205832              |  .26452637    .23238727    .26379489    .24610577    .24610577              |     0.3734       0.3110       0.2850       0.2518       0.2518    car_~2   | -.02415081   -.02415081   -.03596115   -.03596115   -.03596115             |  .33585648    .37759328    .33613488    .36136989    .36136989             |     0.9427       0.9491       0.9148       0.9207       0.9207....................................... ","Creater_id":124739,"Start_date":"2016-07-27 10:02:33","Question_id":225947,"Tags":["interaction","least-squares","stata","endogeneity","2sls"],"Answer_count":1,"Last_activity":"2016-07-31 03:20:11","Link":"http://stats.stackexchange.com/questions/225947/including-several-endogenous-interaction-terms","Creator_reputation":1}
{"_id":{"$oid":"5837a589a05283111e4d6d02"},"View_count":68,"Display_name":"wildetudor","Question_score":2,"Question_content":"This** study asked subjects to rate musical excerpts along 5 subjective dimensions (valence, tension, energy, consonance, preference); each excerpt had a certain chord type and a certain register. The authors did 5 separate ANOVAs, one for each DV, to look for the effect of chord type and register on the 5 rating dimensions. Would it not have been \"more correct\" to do one MANOVA, given that (as they report) there was moderate correlation between them? Or is it the case that doing multiple ANOVAs presents its own advantages?Screenshots below.** Lahdelma, I., \u0026amp; Eerola, T. (2016). Mild Dissonance Preferred Over Consonance in Single Chord Perception. I-Perception, 7(3), 2041669516655812. http://doi.org/10.1177/2041669516655812","Creater_id":41307,"Start_date":"2016-07-30 10:41:43","Question_id":226470,"Tags":["hypothesis-testing","anova","multiple-comparisons","manova"],"Answer_count":1,"Last_activity":"2016-07-31 02:40:34","Link":"http://stats.stackexchange.com/questions/226470/pros-and-cons-of-5-anovas-for-5-dvs-instead-of-1-manova","Creator_reputation":271}
{"_id":{"$oid":"5837a589a05283111e4d6d0f"},"View_count":53,"Display_name":"BeStats","Question_score":1,"Question_content":"I'm not very good with technical details or equations in stats as I'm quite new to running these in R. So I hope that you won't mind me asking these perhaps simple questions and also I would appreciate if someone could attempt to reply in layman terms.What part of the model output can I use, which denotes the % of variance explained in the model? Is it ? i.e. if I have  or  (for regression and correlation tests respectively) = 0.65, does this mean that my model explains 65% of the variance? How can I do this with mixed models (e.g. lmer())? I've tried to find ways to calculate  for lmer() but found much discussion on the topic, without a clearcut way, other than 'not necessary as not too reliable'. Hence, I will simply report the results of the model without the  but rather with the likelihood ratio test results. Does this lack of  for lmer then mean that there's no way for me to say how much variance is explained by my model? Or is this something that the random effect coefficients represent and if not then how can you interpret them or what kind of statement would you write regarding your random effects coefficients?","Creater_id":121550,"Start_date":"2016-07-30 19:07:44","Question_id":226509,"Tags":["mixed-model","variance","lme4","r-squared"],"Answer_count":1,"Last_activity":"2016-07-31 02:32:46","Link":"http://stats.stackexchange.com/questions/226509/how-to-find-variance-in-explained-within-the-output-coefficients","Creator_reputation":35}
{"_id":{"$oid":"5837a589a05283111e4d6d1c"},"View_count":15,"Display_name":"B Calder","Question_score":0,"Question_content":"I am new to using SPSS and am getting confused when inputting variables. I am looking at how latitude impacts tree diversity, and how they differ between hemispheres, so want to look at the interaction. My data is non-normal, and I am using a general linear model. So my understanding is that tree diversity is the dependent variable. Would hemisphere (N or S) be the fixed factor or covariate? And then where does latitude come in in the model? Any guidance would be hugely appreciated :) ","Creater_id":125102,"Start_date":"2016-07-30 15:48:30","Question_id":226498,"Tags":["multiple-regression"],"Answer_count":1,"Last_activity":"2016-07-31 02:31:23","Link":"http://stats.stackexchange.com/questions/226498/which-is-the-covariate-dependent-and-fixed-factor-in-glm","Creator_reputation":1}
{"_id":{"$oid":"5837a589a05283111e4d6d29"},"View_count":82,"Display_name":"dudu","Question_score":1,"Question_content":"The core of the question is: Can I estimate the parameters of a gaussian mixture model (with EM or Dirichlet Process) from a mixture density directly, that is, without using data drawn from such mixture?Long storyI have data that is a mixture density (spectroscopic data) that looks like thisThe raw instrument data is binned, measuring how much light is reflected at various wavelengths. Anyways, I want to decompose that density using a Gaussian Mixture Model GMM. For a tractable example, I simulate a mixture of two gaussians with the code below:mu     = c(100, 200)sd     = c(5, 10)weight = c(0.25, 0.75)xvec   = seq(0, 300)mix_mat = mapply(function(x, y, z){    dnorm(xvec, x, y)}, mu, sd)mix = mix_mat %*% weight mix_f = splinefun(mix)          # So the data is a real functionSo, mix looks like a pretty simple mixture Now, I've not been able to fit a gaussian mixture model directly to mix (or the function mix_f) in R. I end up having to draw a bunch of samples from the mixture and fit them instead. e.g.:library(mixtools)n      = 1000000sampl  = runif(n, min(xvec), max(xvec))s_prob = mix_f(sampl) + 1e-12  # Adding a small baseline so the                                 # prob isn't zero anywherekeep   = rbinom(n, 1, s_prob)draws  = sampl[ which(keep == 1) ]clust = mixtools::normalmixEM(draws, k = 2)Which seems like a weird workaround. Is there a way I can avoid doing this?","Creater_id":31528,"Start_date":"2016-07-30 17:01:02","Question_id":226504,"Tags":["r","gaussian-mixture"],"Answer_count":0,"Last_activity":"2016-07-31 01:44:24","Link":"http://stats.stackexchange.com/questions/226504/fit-gaussian-mixture-model-directly-to-the-mixture-density","Creator_reputation":106}
{"_id":{"$oid":"5837a589a05283111e4d6d2b"},"View_count":45,"Display_name":"RobM","Question_score":0,"Question_content":"I am looking for some statistical solution to the problem of testingthe similarity of curves. I am working with multiply time series (sortof survival curves). The curves are calculated as a separate curvesfor different values of a categorized continuous variable. Forexample:I want to test which of the curves are equal and can be combined inone curve. I know that there are some r packages that use distancemeasure to cluster curves, such as tsclust.What i need is an algorithm that takes into account that only curvesform adjacent intervals of a class variables can be matched. It needsto be automatic as i have many curves and many class variables.","Creater_id":122620,"Start_date":"2016-07-08 01:10:51","Question_id":222717,"Tags":["r","clustering","survival","multiple-comparisons"],"Answer_count":2,"Last_activity":"2016-07-31 01:42:56","Link":"http://stats.stackexchange.com/questions/222717/algorithm-for-ordered-curves-clustering","Creator_reputation":3}
{"_id":{"$oid":"5837a589a05283111e4d6d38"},"View_count":140,"Display_name":"Stephen","Question_score":0,"Question_content":"Does anyone know if there's a heuristic for a high or low adjusted rand index? I understand this is rather subjective - and probably depends on the type of network data you're using. However, if anyone has any intuition for ranges of ARI scores, that would be very helpful! I know that in the social sciences a correlation of 0 to .3 is considered low, .3 to .5 is considered medium, and .5 to 1 is considered high. I know this isn't perfect. But, it does give researchers some intuition for the effect their finding.  ","Creater_id":74716,"Start_date":"2016-07-29 11:59:11","Question_id":226345,"Tags":["clustering","heuristic"],"Answer_count":1,"Last_activity":"2016-07-31 01:25:57","Link":"http://stats.stackexchange.com/questions/226345/what-is-a-high-adjusted-rand-index-score","Creator_reputation":135}
{"_id":{"$oid":"5837a589a05283111e4d6d45"},"View_count":83,"Display_name":"Mark Verheyden","Question_score":1,"Question_content":"I have the following problem: There is 1 group of students (student 1, student 2, ..., student n) This group of students takes several tests (math, French, Biology) - all scored as percentages (e.g.: student 1 has 60% on math, 70% on French, etc.) Finding: the average score for math is 55% while for Biology it is 76 %. Question: I want to know whether these average scores are statistically differentMy answer would be: paired samples t-test However: in most explanations and examples I get in textbooks as well as online about the paired samples t-test a time element is involved (eg: comparing test scores for the same group before and after introducing a new education method). In case I am right and it is the paired samples t-test, what is my independent variable? (second part of my question)","Creater_id":111121,"Start_date":"2016-04-05 14:33:58","Question_id":205702,"Tags":["t-test","repeated-measures","paired-comparisons","paired-data"],"Answer_count":2,"Last_activity":"2016-07-31 01:23:35","Link":"http://stats.stackexchange.com/questions/205702/comparing-mean-test-scores-different-tests-same-group","Creator_reputation":11}
{"_id":{"$oid":"5837a589a05283111e4d6d53"},"View_count":39675,"Display_name":"Flying pig","Question_score":53,"Question_content":"We have a multivariate normal vector . Consider partitioning  and  into \\boldsymbol\\mu=\\begin{bmatrix} \\boldsymbol\\mu_1 \\\\ \\boldsymbol\\mu_2\\end{bmatrix}{\\boldsymbol Y}=\\begin{bmatrix}{\\boldsymbol y}_1 \\\\ {\\boldsymbol y}_2 \\end{bmatrix}with a similar partition of  into \\begin{bmatrix}\\Sigma_{11} \u0026amp; \\Sigma_{12}\\\\\\Sigma_{21} \u0026amp; \\Sigma_{22}\\end{bmatrix}Then, , the conditional distribution of the first partition given the second, is , with mean\\overline{\\boldsymbol\\mu}=\\boldsymbol\\mu_1+\\Sigma_{12}{\\Sigma_{22}}^{-1}({\\boldsymbol a}-\\boldsymbol\\mu_2)and covariance matrix\\overline{\\Sigma}=\\Sigma_{11}-\\Sigma_{12}{\\Sigma_{22}}^{-1}\\Sigma_{21}Actually these results are provided in Wikipedia too, but I have no idea how the  and  is derived. These results are crucial, since they are important statistical formula for deriving Kalman filters. Would anyone provide me a derivation steps of deriving  and  ? Thank you very much!","Creater_id":3525,"Start_date":"2012-06-16 11:09:15","Question_id":30588,"Tags":["normal-distribution","conditional-probability"],"Answer_count":1,"Last_activity":"2016-07-31 01:07:40","Link":"http://stats.stackexchange.com/questions/30588/deriving-the-conditional-distributions-of-a-multivariate-normal-distribution","Creator_reputation":1459}
{"_id":{"$oid":"5837a589a05283111e4d6d5f"},"View_count":52,"Display_name":"Robert","Question_score":2,"Question_content":"Experiment: Given mice of two genetic backgrounds (normal, mutant), is there a difference in brain inflammation as determined by fluorescence intensity of an inflammatory marker?Data: 19 mice (12 in genotype = normal, 7 in genotype = mutant) across 90 observations (3-7 observations per mouse) were analyzed.Model (code in R): model = lme(inflammation ~ genotype + (1|mouse)) inflammation = continuous dependent variablegenotype = categorical independent variable (binary), fixed effectsmouse = arbitrary number, random effectsDoes this sound right to you? ","Creater_id":125035,"Start_date":"2016-07-29 14:35:24","Question_id":226371,"Tags":["mixed-model","lme4"],"Answer_count":1,"Last_activity":"2016-07-31 00:33:18","Link":"http://stats.stackexchange.com/questions/226371/new-to-lme4-in-r-am-i-modeling-this-correctly","Creator_reputation":13}
{"_id":{"$oid":"5837a589a05283111e4d6d6c"},"View_count":39,"Display_name":"James White","Question_score":0,"Question_content":"I want to calculate the multivariate dispersion of community abundances (and not their traits). I can do this via 'betadisper' in the vegan package, but I would also like to account for abundant species via the fdisp function within the FD package. Is the method below the correct mathematical procedure for approaching this? Also, would this be possible to this via Bray-Curtis dissimilarity? library(FD)dist \u0026lt;- gowdis(t(dummyabun)ex1$FDis","Creater_id":81660,"Start_date":"2016-07-30 08:59:43","Question_id":226461,"Tags":["r","multivariate-analysis","dispersion"],"Answer_count":0,"Last_activity":"2016-07-31 00:03:10","Link":"http://stats.stackexchange.com/questions/226461/calculating-multivariate-functional-dispersion-without-trait-information","Creator_reputation":18}
{"_id":{"$oid":"5837a589a05283111e4d6d6e"},"View_count":216,"Display_name":"B K","Question_score":1,"Question_content":"I am trying to find the significance of predictors while using different linear regression models (I am using Python scikit-learn). Scikit-learn does not provide the pvalues of predictors( at least i did not find a way). So i would like to know how to calculate pvalues?Puporpose is to find which all predictors are significant.Thanks in Advance","Creater_id":100764,"Start_date":"2016-07-26 22:24:11","Question_id":225828,"Tags":["regression","p-value","python","scikit-learn"],"Answer_count":0,"Last_activity":"2016-07-30 23:05:27","Link":"http://stats.stackexchange.com/questions/225828/how-to-calculate-p-value-for-significance-test-of-variables-in-linear-regression","Creator_reputation":26}
{"_id":{"$oid":"5837a589a05283111e4d6d70"},"View_count":45,"Display_name":"roachsinai","Question_score":2,"Question_content":"My question is:Why the incomplete-data likelihood equal to formula 1?Why it should not equal to formula 2?I apologize for not being word-perfect in English.I'm reading the book Pattern Recognition And Machine Learning. And I was confused at the derivation of EM algorithm in General at Page 467.If we denote all of the observed variables by X and all of the hidden variables by Z. Our goal is to maximize the likelihood function: .Why our goal is equal to \\sum_Z p(X,Z \\mid \\theta) \\qquad (1)Where,   \\sum_z p(X,Z \\mid \\theta) = \\sum_z \\prod_n \\sum_k z_n^k p(x_n\\mid\\mu_k, \\Sigma_k)\\pi_kfrom cos 513: mixture models and em algorithm page 4. As Gaussian mixture model,  means instance  with probability  generated by kth Gaussian component.And  is a one of K variable, if instance  was generated by kth Gaussian component, ; otherwise, .In my mind,p(X\\mid\\theta) = \\prod_n \\sum_z \\sum_k z_n^k p(x_n\\mid\\mu_k, \\Sigma_k)\\pi_k \\qquad (2)So, I think this was the marginal likelihood. Cause, likelihood should be the product of probabilities of observations.","Creater_id":117271,"Start_date":"2016-07-29 22:31:31","Question_id":226404,"Tags":["machine-learning","mathematical-statistics","expectation-maximization","pattern-recognition"],"Answer_count":1,"Last_activity":"2016-07-30 22:27:52","Link":"http://stats.stackexchange.com/questions/226404/derivation-of-expectation-maximization-in-general-prml","Creator_reputation":18}
{"_id":{"$oid":"5837a589a05283111e4d6d7d"},"View_count":86,"Display_name":"AlphaOmega","Question_score":3,"Question_content":"Wikipedia:\"In statistics, family-wise error rate (FWER) is the probability of making one or more false discoveries, or type I errors, among all the hypotheses when performing multiple hypotheses tests.\"\"The false discovery rate (FDR) is one way of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons.\"I don't understand the difference between these two concepts. How do they not mean the same?Perhaps you can help me by further elaborating the following example:Say the probability for an unbiased coin to substantially deviate from a 50/50 head/tail-distribution in a sequence of 1,000 tosses is 0.001.If I want to find out if one coin is biased I throw it 1,000 times and if it shows heads ~500 times I can be quite sure it is not biased.However if I throw a million coins 1,000 times and deem those biased who don't show a 50/50-distribution of heads and tails, I will categorize unbiased coins as biased, because the probability of an unbiased coin showing deviating from the 50/50-distribution is multiplied by the number of coins (1 million). Thus from a set of one million unbiased coins, I have to expect about 1,000,000*0.001=1,000 coins to deviate substantially from the 50% tails, 50% heads-distribution.As far as I understood this is multiple hypotheses testing (synonymous: multiple comparisons?) as I am testing the hypothesis \"coin is unbiased\" a million times, and the false discovery rate FDR is 1,000 in this example.But what, then, is the FWER (family wise error rate)?","Creater_id":120194,"Start_date":"2016-07-26 05:45:14","Question_id":225688,"Tags":["multiple-comparisons","false-discovery-rate","familywise-error"],"Answer_count":1,"Last_activity":"2016-07-30 22:05:00","Link":"http://stats.stackexchange.com/questions/225688/fwer-fdr-and-multiple-comparisons-for-beginners","Creator_reputation":163}
{"_id":{"$oid":"5837a589a05283111e4d6d8a"},"View_count":41,"Display_name":"pellis","Question_score":1,"Question_content":"I am trying to understand the asymptotic distribution of the Wald test statistic, specifically under the alternative hypothesis which I've found little reference to.For clarity, the binary hypothesis for an unknown parameter vector  of size  is:\\begin{equation}  \u0009\\text{H}_0  : \\theta = \\theta_0 \\nonumber \\\\\u0009\\text{H}_1  : \\theta \\neq \\theta_0\\end{equation}and the resulting log likelihood ratio, when assuming the asymptotic PDF of the MLE is attained (), can be modified to yield the Wald Test Statistic:\\begin{equation} 2 \\ln L_G({\\bf x} ) = T_W({\\bf x}) = (\\hat{\\theta}_1 - \\theta_0)^T {\\bf I}(\\hat{\\theta}_1) (\\hat{\\theta}_1 - \\theta_0)\\end{equation}where  denotes the unrestricted MLE and  is the FIM.  Now, as we seek to attain the asymptotic distribution of this test statistic we can note:So that as , under the null hypothesis () :\\begin{equation}  T_W({\\bf x}) = (\\hat{\\theta}_1 - \\theta_0)^T {\\bf I}(\\theta_0) (\\hat{\\theta}_1 - \\theta_0) \\sim \\chi^2_r\\end{equation}But it is the distribution under  I get confused.  A variety of texts/proofs (i.e. Kay Vol II) say the following, as :\\begin{equation}  {\\bf I}(\\hat{\\theta}_1) (\\hat{\\theta}_1 - \\theta_0) = {\\bf I}(\\theta_0) (\\hat{\\theta}_1 - \\theta_0) = {\\bf I}(\\theta_1) (\\hat{\\theta}_1 - \\theta_0) \\end{equation}The above equation I have no clue how to validate.  Anyways, the proof continues so that under :\\begin{equation}  T_W({\\bf x}) = (\\hat{\\theta}_1 - \\theta_0)^T {\\bf I}(\\theta_1) (\\hat{\\theta}_1 - \\theta_0) \\sim \\chi^2_r (\\lambda)\\end{equation}where the non centrality parameter is equal to:\\begin{equation} \u0009\\lambda = ( \\theta_1 - \\theta_0  )^T{\\bf I}(\\theta_1) ( \\theta_1 - \\theta_0  ) \\end{equation}or equivalently, as :\\begin{equation}\u0009\\lambda = ( \\theta_1 - \\theta_0  )^T{\\bf I}(\\theta_0) ( \\theta_1 - \\theta_0  )\u0009\\end{equation}How can this possibly be true?  Unless there's a strong assumption taken here that I am unaware of, there is no reason, even asymptotically, the variances of these two values ( and ) should be equal.  What am I missing?  Something inherent in the Wald Test Statistic?Any input on this would be appreciated as I'm completely vexed.  Thanks!","Creater_id":124877,"Start_date":"2016-07-28 10:56:31","Question_id":226151,"Tags":["distributions","mathematical-statistics","asymptotics","log-likelihood"],"Answer_count":1,"Last_activity":"2016-07-30 21:46:27","Link":"http://stats.stackexchange.com/questions/226151/asymptotic-distribution-of-the-wald-test-statistic","Creator_reputation":6}
{"_id":{"$oid":"5837a589a05283111e4d6d97"},"View_count":319,"Display_name":"dimebucker91","Question_score":5,"Question_content":"I have fit the following linear model, I tested the response by looking at a qq plot and it is almost perfectly linear. When i fit the model though, and study the predicted vs observed plot, It looks like this:What does this tell me exactly? It seems to me that a better fitting line would be gained if i pivoted the model slightly to match the slope of the points. I'm not sure what I can do to gain a better predictive model.editi trained the linear model on my training set. The 'predicted' in the plot are the result of applying that model to an independent 'test' set, and comparing it wth the observed values for that test set. The line is found by abline(0,1)","Creater_id":55946,"Start_date":"2016-07-27 04:56:10","Question_id":225882,"Tags":["regression","multiple-regression","diagnostic"],"Answer_count":3,"Last_activity":"2016-07-30 20:36:54","Link":"http://stats.stackexchange.com/questions/225882/what-does-this-plot-tell-me-about-my-linear-model","Creator_reputation":176}
{"_id":{"$oid":"5837a589a05283111e4d6da6"},"View_count":8,"Display_name":"Tina","Question_score":0,"Question_content":"I have data from my experiment where two groups participated (monolinguals vs. bilinguals). There was a language switching as well as a task switching part to the experiment where the data tells how fast the participants responded as well as whether or not their answer was correct (correct vs. incorrect). How should I analyse my data?In addition\u003e I HAVE looked at methods including: Two-way repeated measures ANOVA, IRT and some others but I don't feel like they apply to my case and I am therefore seeking inspiration and NOT for someone to do my homework as implied below.PS: I am trying to see whether bilinguals perform better in task switching than monolinguals. ","Creater_id":124524,"Start_date":"2016-07-25 14:19:51","Question_id":226506,"Tags":["methodology","dataset"],"Answer_count":2,"Last_activity":"2016-07-30 18:44:39","Link":"http://stats.stackexchange.com/questions/226506/what-statistical-method-to-compare-bilinguals-and-monolinguals-on-a-task-switchi","Creator_reputation":3}
{"_id":{"$oid":"5837a589a05283111e4d6db4"},"View_count":505,"Display_name":"merch","Question_score":3,"Question_content":"I have several time-series in a VAR(1) and, due to some of them haven't the same unit of measure, I'd like to estimate the RMSE in percentage. I know that it could be done in several ways (see below) but I don't know precisely which is the one that fits better a forecast evaluation problem. I hope you could help me. Examples of normalized RMSE:RMSE_1 = \\sqrt{\\frac{1}{n}\\sum_i\\left(\\frac{Y_{forecast_i}-Y_i}{Y_i}\\right)^2} \\\\RMSE_2 = \\sqrt{\\frac{1}{n}\\sum_i\\left(\\frac{Y_{forecast_i}-Y_i}{Y_{forecast_i}}\\right)^2} \\\\RMSE_3 = \\frac{\\sqrt{\\frac{1}{n}\\sum_i\\left(Y_{forecast_i}-Y_i\\right)^2}}{mean(Y)}","Creater_id":40899,"Start_date":"2014-09-21 12:09:14","Question_id":116238,"Tags":["time-series","mse","rms"],"Answer_count":2,"Last_activity":"2016-07-30 17:10:15","Link":"http://stats.stackexchange.com/questions/116238/normalized-rmse","Creator_reputation":162}
{"_id":{"$oid":"5837a589a05283111e4d6dc2"},"View_count":23,"Display_name":"Gaius Augustus","Question_score":1,"Question_content":"I am helping with a paper where a co-author got Spearman correlations between each of 6 variables.I've seen that permutation is likely the best way to increase power for these (if I'm understanding it correctly), but is a Bonferroni correction a valid way to do this?I don't have access to the data, and the person working on this is not a statistician, so I'm trying to offer an alternative to just spitting out 30+ correlations without multiple testing corrections (which is the current state of things).","Creater_id":78773,"Start_date":"2016-07-30 16:02:41","Question_id":226501,"Tags":["multiple-comparisons","bonferroni","spearman-rho"],"Answer_count":0,"Last_activity":"2016-07-30 16:02:41","Link":"http://stats.stackexchange.com/questions/226501/is-multiple-testing-correction-e-g-bonferroni-necessary-for-multiple-spearman","Creator_reputation":106}
{"_id":{"$oid":"5837a589a05283111e4d6dc4"},"View_count":84,"Display_name":"TokyoUrban","Question_score":1,"Question_content":"This started as a simple discussion where everyone thought they new the answer, and ended up with arguments and dozens of paper quotations. Thoughts appreciated.We have 10 samples: Control, A, B, C, ..., I, with good enough sample size, and the groups have similar variances.The following comparisons were made using unpaired two-sided t-tests:A - ControlB - ControlC - Control...I - ControlWe obtained 9 p-values. Do we have to adjust these p-values to account for family-wise issues ?Thanks!","Creater_id":31003,"Start_date":"2014-11-16 19:07:38","Question_id":124307,"Tags":["multiple-comparisons","adjustment"],"Answer_count":2,"Last_activity":"2016-07-30 15:42:58","Link":"http://stats.stackexchange.com/questions/124307/adjust-the-p-values-or-not","Creator_reputation":11}
{"_id":{"$oid":"5837a589a05283111e4d6dd1"},"View_count":26,"Display_name":"qualiaMachine","Question_score":-1,"Question_content":"I am trying to employ the silhouette index in order to decide what number of clusters produced from my dataset is most realistic/probable.  I'm aware of how to calculate the SI as well as the general theory behind it, but I am wondering how to apply such an algorithm if my clustering procedure produces K clusters along with a \"junk\" cluster.  The junk cluster, in this case, simply refers to data that would not fit well into any of the k clusters (decided by a distance threshold).  If my data only produced k \u003e 1 clusters plus the junk cluster, there would be no issue.  However, often K = 1.  In such a scenario, Bi (avg dissimilarity to any other clusters) is going to be very large given that I would be comparing a well categorized set of data to a mess of odds and ends in the junk cluster (thus producing a rather large average silhouette).  On top of that, when more than one K cluster is produced along with the junk cluster, there is going to be at least some degree of similarity between all K clusters (much smaller Bi).  This makes it quite difficult for K \u003e 1 to ever produce an average SI that is above that of K = 1 (or so I am thinking)... Then again, I also need to consider the fact that when K increases, the junk cluster is going to become smaller, which means that it is possible for the internal distance of the junk cluster (average Ai) to improve.  I can't decide if these effects will even out, or if I am being completely inappropriate by including the junk cluster at all...  Anyone with more experience in the clustering biz have any words of advice?EDIT: I am using wave_clus, which utilizes something called superparamagnetic clustering to sort the data. The junk cluster contains datapoints that would show some similarity to each other, but not nearly as much consistency as you would see in a true cluster.  If enough points in the junk cluster are similar to each other based on the feature extraction method wave_clus uses, those points would be more likely (but not always) to be extracted as an additional 'real' cluster and become separate from the junk cluster.  This is exactly what happens when the clustering algorithm produces two 'real' clusters rather than one.  In some cases, however, it seems that the clustering algorithm lands on a boundary--where it sometimes produces two real clusters, sometimes one.  I would like to evaluate which case is more likely by calculating the silhouette index for each case.  If it makes any difference, my actual data consists of waveforms/neural spikes recorded from MEA's.  The goal is to determine which spikes came from which cell (since each cell typically produces a characteristic waveform).","Creater_id":92522,"Start_date":"2016-07-29 16:23:11","Question_id":226385,"Tags":["machine-learning","clustering"],"Answer_count":1,"Last_activity":"2016-07-30 15:40:07","Link":"http://stats.stackexchange.com/questions/226385/is-it-appropriate-to-use-a-silhouette-index-to-evaluate-clustering-validity-when","Creator_reputation":3}
{"_id":{"$oid":"5837a589a05283111e4d6dde"},"View_count":33,"Display_name":"thisisnothappening","Question_score":1,"Question_content":"There is sequence of vectors  passed as input to deep learning regression model .As observed, the order of this sequence is irrelevant, so  etc.So instead of passing raw sequence of vectors and forcing a model to learn commutativity of input data, it should be better to map input by commutative function to the single vector at the beginning and then pass the result to the learning part of a model.What is the standard / best way to do this as part of differentiable graph?Multiplying or summing vectors will not fit, because it could produce same result for different sequences.I have no higher education in math or statistics, so please be forgiving if this is silly question. For my experiments I'm using python + theano.","Creater_id":124873,"Start_date":"2016-07-28 08:58:52","Question_id":226130,"Tags":["neural-networks","python","deep-learning","theano"],"Answer_count":1,"Last_activity":"2016-07-30 14:51:30","Link":"http://stats.stackexchange.com/questions/226130/commutative-function-mapping-sequence-of-vectors-to-vector","Creator_reputation":6}
{"_id":{"$oid":"5837a589a05283111e4d6deb"},"View_count":146,"Display_name":"user20069","Question_score":3,"Question_content":"I need help for choosing a proper test for inter-rater reliability. I have three raters (experts) who have analysed the quality of images (10 images from different location). The data is ordinal (image quality is assessed numerically from 1 to 5).","Creater_id":20069,"Start_date":"2013-01-25 03:46:22","Question_id":48498,"Tags":["ordinal","reliability","inter-rater"],"Answer_count":1,"Last_activity":"2016-07-30 14:01:12","Link":"http://stats.stackexchange.com/questions/48498/what-is-the-proper-test-for-inter-rater-reliability-with-three-raters-and-ordina","Creator_reputation":16}
{"_id":{"$oid":"5837a589a05283111e4d6df8"},"View_count":18,"Display_name":"SwingingStrawberry","Question_score":0,"Question_content":"My research question is based on a lot of previous research suggesting that a certain predictor explains a lot of the variance in an outcome, my hypothesis is that certain additional predictors account for the association and am planning a hierarchical multiple regression to test this with a sample \u003e6,000 participants.A previous question/answer mentions that choosing predictors based purely on correlation is not always the way to go, however is there a citable reference for this? Something my supervisor mentioned was that my entire research can only go ahead if my main predictor correlates with my outcome, which it does (.202) but is a weak correlation using spearman's rank correlation as the 'main' predictor is ordinal. I was hoping there might be something citable that states that a predictor that has a weak bivariate correlation is still ok to use as long as there are good theoretical grounds to use it.","Creater_id":116661,"Start_date":"2016-07-30 13:48:18","Question_id":226490,"Tags":["regression","multiple-regression"],"Answer_count":0,"Last_activity":"2016-07-30 13:48:18","Link":"http://stats.stackexchange.com/questions/226490/reference-for-not-selecting-predictors-purely-based-on-correlation","Creator_reputation":32}
{"_id":{"$oid":"5837a589a05283111e4d6dfa"},"View_count":19,"Display_name":"rksh","Question_score":0,"Question_content":"I have two sets of data, one set of data is the calculated values from a hypothesis and the other set of data represents the real values observed.I want to compare the two sets of data to see if there is a statistical significance between them or there is no significance between them (whether it's pure chance) Is chi squared test is suitable for this? or is there any other test that is good? The empty rows are waited to be filled with observed data. Which is not available yet.Thanks","Creater_id":123274,"Start_date":"2016-07-30 13:24:12","Question_id":226488,"Tags":["probability","statistical-significance","chi-squared"],"Answer_count":0,"Last_activity":"2016-07-30 13:24:12","Link":"http://stats.stackexchange.com/questions/226488/compareing-the-statistical-significance-between-two-distributions","Creator_reputation":101}
{"_id":{"$oid":"5837a589a05283111e4d6dfc"},"View_count":103,"Display_name":"Max","Question_score":1,"Question_content":"I have a question related to Granger Causality testing.Is it okay to use a lag-length of lag=1 in my Granger-test? The optimum lag length selection in my R VARselect(data,lag=maxlag,type=trend) model says that lag=1 shows the best and most stable information criteria values according to AIC, BIC and FPE.I have a 30-year set of quarterly data and I'm using a maxlag of 4.","Creater_id":100392,"Start_date":"2016-01-11 13:29:41","Question_id":190224,"Tags":["feature-selection","model-selection","var","lags","granger-causality"],"Answer_count":1,"Last_activity":"2016-07-30 13:09:09","Link":"http://stats.stackexchange.com/questions/190224/is-it-ok-to-use-lag-1-for-granger-causality-test","Creator_reputation":6}
{"_id":{"$oid":"5837a589a05283111e4d6e09"},"View_count":1256,"Display_name":"majom","Question_score":10,"Question_content":"When modeling time series one has the possibility to (1) model the correlational structure of the error terms as e.g. an AR(1) process(2) include the lagged dependent variable as an explanatory variable (on the right hand side)I understand that their are sometimes substantial reasons to go for (2). However, what are the methodological reasons to do either (1) or (2) or even both?","Creater_id":13212,"Start_date":"2014-08-05 11:10:49","Question_id":110757,"Tags":["time-series","autocorrelation","residuals","lags"],"Answer_count":3,"Last_activity":"2016-07-30 13:04:04","Link":"http://stats.stackexchange.com/questions/110757/residual-autocorrelation-versus-lagged-dependent-variable","Creator_reputation":386}
{"_id":{"$oid":"5837a589a05283111e4d6e18"},"View_count":935,"Display_name":"eski","Question_score":0,"Question_content":"When building a VAR-model with six variables and 117 observation, I had the following situation: after building a VAR(1), the overall portmanteau test says that the residuals are OK (, ). But when I have a look at the single residuals the ACFs all look white noise except one of the six in my case. For this only one it seems like I need a VAR(2) -- and portmanteau test for this single residual shows: , .I'm unsure what to do in this situation. I experienced the same situation in another VAR where one residual looked like including 5 lags. So the first question is, how do I decide if 1 lag is enough or if I need 2 lags? Next: If I decide that it takes 2 lags, do I then have to let the model estimate the whole matrix even if there is only one variable that needs the second lag? Or is it reasonable to allow for the matrix of the second lags only the specific row (and/or column) belonging to this variable? Like in the picture below - imagine only variable E shows elevated autocorrelation at lag 2; the picture is only for showing the problem, normally there is a constant included and deterministic trend terms, too.What to do in the case of one residual showing only lag 5 to include additionally? Then I would not include the matrices for lags 2, 3 and 4 and only for 5?What goes in the same direction: if I first include only intercepts and deterministic trends and see that some residuals are already WN, then does it make sense to include any predictors for them? Should I include higher lags only for the variables where the residuals show it?NOTE:  and  are calculated as follows:","Creater_id":32091,"Start_date":"2015-03-20 09:10:12","Question_id":142618,"Tags":["time-series","var","lags"],"Answer_count":1,"Last_activity":"2016-07-30 12:43:19","Link":"http://stats.stackexchange.com/questions/142618/how-many-lags-should-i-include-in-a-var-model","Creator_reputation":51}
{"_id":{"$oid":"5837a589a05283111e4d6e25"},"View_count":89,"Display_name":"wildetudor","Question_score":0,"Question_content":"In most of the times when effect size is reported, it seems to me that there is a clear inverse proportionality with p-value. I know that effect sizes bring information that is independent from significance, which is obvious when one considers the extreme cases of a very small and very large sample size - when large effects are difficult to found significant, and small effects can be found significant respectively. What would help is to see a plot of effect size as a function of p-value, with N as a curve parameter, but I haven't been able to find such a plot. Can anyone suggest where to find / how to create such a plot, or otherwise shed light on the problem?","Creater_id":41307,"Start_date":"2016-07-30 11:50:47","Question_id":226477,"Tags":["statistical-significance","effect-size"],"Answer_count":3,"Last_activity":"2016-07-30 12:33:22","Link":"http://stats.stackexchange.com/questions/226477/relationship-between-effect-size-and-statistical-significance","Creator_reputation":271}
{"_id":{"$oid":"5837a589a05283111e4d6e34"},"View_count":17,"Display_name":"cgreen","Question_score":0,"Question_content":"I would like to generate some kind of plot from my data to see if my covariance kernel (square exponential) is a good one. I only have one data sample, so how do I calculate the spatial covariance to see if it looks like the kernel?","Creater_id":59667,"Start_date":"2016-07-30 12:28:27","Question_id":226484,"Tags":["gaussian-process"],"Answer_count":0,"Last_activity":"2016-07-30 12:28:27","Link":"http://stats.stackexchange.com/questions/226484/how-to-check-if-gaussian-process-covariance-model-is-good","Creator_reputation":138}
{"_id":{"$oid":"5837a589a05283111e4d6e36"},"View_count":101,"Display_name":"Pedestrian","Question_score":0,"Question_content":"In Patton (2011) the author finds that both the MSE and the QLIKE loss function are robust when used to compare rivalling volatility forecasting models, which means that using a proxy for volatility gives the same ranking as using the true (unobservable) volatility of an asset. In my current project I am comparing a family of GARCH/AGARCH models and while the MSE suggests that nothing outperforms a GARCH(1,1), the QLIKE statistic suggests that an APARCH(1,1) model performs significantly better.Is this caused by the two loss functions penalising deviations differently?Specifically, what do the two loss functions place the highest penalty on, i.e. how do I interpet this?I am hoping this is not down to some trivial coding error.#MSE MSE\u0026lt;-function(sigmafc,RV){  MSE=1/length(sigmafc)*sum((sigmafc^2-RV)^2)  return(MSE)}#QLIKEQLIKE\u0026lt;-function(sigmafc,RV){  varfc=sigmafc^2  QLIKE=sum(    (RV/varfc-log(RV/varfc)-1)    )  return(QLIKE)}I gather that the MSE depends on forecast errors, while QLIKE depends on standardised errors, but how would I interpret this?","Creater_id":122805,"Start_date":"2016-07-27 04:18:09","Question_id":225877,"Tags":["r","garch","loss-functions","volatility-forecasting"],"Answer_count":0,"Last_activity":"2016-07-30 12:08:38","Link":"http://stats.stackexchange.com/questions/225877/interpreting-qlike-and-mse-loss-function-patton-2011","Creator_reputation":16}
{"_id":{"$oid":"5837a589a05283111e4d6e38"},"View_count":20,"Display_name":"wildetudor","Question_score":0,"Question_content":"I've often seen the following approach to how people report parameters of best-fit curves: a correlation between X and Y is weak and/or non-significant, so because the data points seem to be placed on what appears to be a parabola, try a 2nd-order polynomial fit, and report those params if the fit is good enough.I just wonder if it is correct for the curve type to be eye-balled and its parameters then computed. Isn't there a problem of multiple comparisons also apparent? Is there a systematic way to find the curve of best fit, one that eliminates the subjective \"hunch\" element, or is that element actually desirable?","Creater_id":41307,"Start_date":"2016-07-30 11:43:22","Question_id":226474,"Tags":["multiple-comparisons","curve-fitting"],"Answer_count":1,"Last_activity":"2016-07-30 12:00:05","Link":"http://stats.stackexchange.com/questions/226474/multiple-comparisons-in-the-process-of-finding-the-best-fit-curve","Creator_reputation":271}
{"_id":{"$oid":"5837a589a05283111e4d6e45"},"View_count":112,"Display_name":"Guillermo Guardastagno","Question_score":1,"Question_content":"I've building a model to predict count variables, i. e. the quantity I'm predicting is a positive integer.I know that for regression a usual metric of model quality is the R-squared coefficient, but I'm not sure if this is a good metric for a discrete output. What's the usual metric for model evaluation for a discrete regression?","Creater_id":105859,"Start_date":"2016-07-27 09:10:54","Question_id":225940,"Tags":["regression","r-squared","model-evaluation"],"Answer_count":2,"Last_activity":"2016-07-30 11:36:25","Link":"http://stats.stackexchange.com/questions/225940/model-evaluation-for-discrete-regression","Creator_reputation":141}
{"_id":{"$oid":"5837a589a05283111e4d6e53"},"View_count":11,"Display_name":"user1058210","Question_score":0,"Question_content":"From what I understand, when we apply a scaling transformation (zero-mean and unit variance) on the training set, any new data we hope to apply our learnt model to also needs to be scaled using the exact same scaling transformation?I am trying to build a predictive model that predicts whether an area will improve based on some factors e.g. crime, health, transportation etc.  My dataset consists of a list of neighbourhoods with a breakdown by variable of each factor e.g.  with crime, we could have:  Robbery = 20%, Fraud = 70%, Murder=10%, for a particular neighbourhood.I want to score these areas based on their relative performance so for each attribute I have decided to scale the data by removing the mean and scaling it to unit variance.  I am training this model on data in 2000 and predicting 2010, and then want to use the same model on 2010 to predict to 2020.There is an assumption in my research that the relative performance on these attributes is the only thing that matters.  So in 2010 it only matters how neighbourhoods compare to each other, so does this mean that I can scale the data in 2010 independently?The type of rules I am expecting to mine are of the form \"if crime \u0026lt; 0.4, then class = improved\".   If it only matters about how areas in 2010 compare with each other then is treating them independently valid? ","Creater_id":99445,"Start_date":"2016-07-30 11:28:52","Question_id":226473,"Tags":["data-transformation","python","multidimensional-scaling"],"Answer_count":0,"Last_activity":"2016-07-30 11:28:52","Link":"http://stats.stackexchange.com/questions/226473/measuring-relative-performance-does-the-scaling-transformation-on-the-training","Creator_reputation":121}
{"_id":{"$oid":"5837a589a05283111e4d6e55"},"View_count":141,"Display_name":"St\u0026#233;phane Laurent","Question_score":5,"Question_content":"Consider the following Deming model with independent replicates :x_{i,j} \\mid \\theta_{i} \\sim {\\cal N}(\\theta_{i}, \\gamma_X^2), \\quady_{i,j} \\mid \\theta_{i} \\sim {\\cal N}(\\alpha+\\beta\\theta_{i}, \\gamma_Y^2), \\\\ i \\in 1:I, \\quad j \\in 1:J,all observations being independent, all parameters being unknown.Let's fix the parameters and the design and write a function that simulates the data:N \u0026lt;- 25  # number of groupsJ \u0026lt;- 4  # number of repetitions within each grouptheta0 \u0026lt;- 1:N  # true theta_igammaX0 \u0026lt;- sqrt(4); lambda \u0026lt;- 1gammaY0 \u0026lt;- sqrt(lambda)*gammaX0alpha \u0026lt;- 0beta \u0026lt;- 1simulations \u0026lt;- function(){  X \u0026lt;- Y \u0026lt;- array(NA,dim=c(N,J))  for(i in 1:N){     for(j in 1:J){      X[i,j] \u0026lt;- rnorm(1,theta0[i],gammaX0)      Y[i,j] \u0026lt;- rnorm(1,alpha+beta*theta0[i],gammaY0)      }  }return(list(X=X,Y=Y))}set.seed(666)sims \u0026lt;- simulations()X \u0026lt;- simsYWhen the ratio  is known, one can get good estimates of  and  by considering only the group means  \\bar{x}_{i\\bullet} \\sim {\\cal N}(\\theta_i, \\gamma_X^2/J), \\quad  \\bar{y}_{i\\bullet} \\sim {\\cal N}(\\alpha + \\beta\\theta_i, \\gamma_Y^2/J)and then by taking the maximum likelihood estimates:MethComp::Deming(rowMeans(X), rowMeans(Y), vr=lambda)[1:2]##  Intercept      Slope ## -0.3207636  1.0154527On the other hand, the least-squares estimates:coefficients(lm(rowMeans(Y)~rowMeans(X)))## (Intercept) rowMeans(X) ##  0.04505284  0.98709755are biased and unconsistent.Now, using a Bayesian approach with the following naive and independent priors:\\gamma_X^2 \\sim {\\cal IG}(0^+,0^+), \\qquad   \\gamma_Y^2 \\sim {\\cal IG}(0^+,0^+), \\\\\\theta_i  \\sim {\\cal N}(0, \\infty), \\\\\\alpha \\sim {\\cal N}(0, \\infty), \\qquad \\beta \\sim {\\cal N}(0, \\infty),then the Bayesian estimates (posterior means or medians) of  and  are close to the least-squares estimates. And I wonder why. I know there is no reason to get a good coincidence between maximum likelihood estimates and Bayesian estimates by using \"naive\" non-informative priors, but I am surprised by the coincidence between the least-squares estimates and the Bayesian estimates. Is there an intuitive reason to expect this result ?This is what I discovered using JAGS:library(rjags)modelfile \u0026lt;- \"gibbs1.txt\" jmodel \u0026lt;- function(){  for(i in 1:N){    for(j in 1:J){      X[i,j] ~ dnorm(theta[i], inv.gammaX2)      Y[i,j] ~ dnorm(nu[i], inv.gammaY2)    }    theta[i] ~ dnorm(0, 0.00001)     nu[i] \u0026lt;- alpha+beta*theta[i]  }  alpha ~ dnorm(0,0.001)  beta ~ dnorm(0,0.001)  #  inv.gammaX2 ~ dgamma(0.01,0.01)  inv.gammaY2 ~ dgamma(0.01,0.01)  gammaX2 \u0026lt;- 1/inv.gammaX2  gammaY2 \u0026lt;- 1/inv.gammaY2  lambda \u0026lt;- gammaY2/gammaX2}writeLines(paste(\"model\", paste(body(jmodel), collapse=\"\\n\"), \"}\"), modelfile)# run Gibbs samplerdata \u0026lt;- list(X=X, Y=Y, N=N, J=J)inits1 \u0026lt;- list(alpha=0, beta=1, inv.gammaX2=1, inv.gammaY2=1, theta=rowMeans(X))inits2 \u0026lt;- lapply(inits1, function(x) x*rnorm(length(x),1,.01))inits \u0026lt;- list(inits1,inits2)jags \u0026lt;- jags.model(modelfile,                   data = data,                    inits=inits,                    n.chains = 2,                   n.adapt = 1000,                   quiet=TRUE)update(jags, 5000, progress.bar=\"none\")samples \u0026lt;- coda.samples(jags, c(\"alpha\", \"beta\"), 10000, progress.bar=\"none\")summary(samples)1000\\theta_i(\\alpha, \\beta)\\theta_i(\\hat\\alpha, \\hat\\beta)\\lambda=\\gamma^2_Y/\\gamma_X^2(\\alpha, \\beta)=y_{ij}\\theta_i\\theta_iJ\\lambda=\\infty$). What is wrong in this intuition ?  ","Creater_id":8402,"Start_date":"2015-03-25 09:45:03","Question_id":143378,"Tags":["bayesian","maximum-likelihood","gibbs","uninformative-prior","deming-regression"],"Answer_count":0,"Last_activity":"2016-07-30 09:57:47","Link":"http://stats.stackexchange.com/questions/143378/bayesian-estimates-for-deming-regression-coinciding-with-least-squares-estimates","Creator_reputation":9097}
{"_id":{"$oid":"5837a589a05283111e4d6e57"},"View_count":46,"Display_name":"Dwaipayan Gupta","Question_score":1,"Question_content":"How do I prove the following inequality :\\frac{2}{\\alpha^2} \\bigg( e^{\\alpha y} - e^{\\alpha x} \\bigg) +  e^{\\alpha x} \\bigg( x^2 - y^2 \\bigg) \u0026gt; 0 \\; \\;?Here, .Additional information : both  and  are strictly greater than 0 !","Creater_id":88754,"Start_date":"2016-07-29 14:01:35","Question_id":226364,"Tags":["inequality"],"Answer_count":2,"Last_activity":"2016-07-30 09:55:30","Link":"http://stats.stackexchange.com/questions/226364/inequality-problem-involving-exponential-expression","Creator_reputation":154}
{"_id":{"$oid":"5837a589a05283111e4d6e65"},"View_count":16,"Display_name":"TalG","Question_score":0,"Question_content":"I'm trying to build a confidence interval for a metric called \"pages per visit\"'Pages per visit' means the number of pages a user views when reaches to a site. Make sense to immediately think about poisson as the relevant distribution but I believe this is not correct, the denominator is discrete. Will appreciate your help here,Thanks!!Tal.","Creater_id":86294,"Start_date":"2016-07-30 09:42:26","Question_id":226465,"Tags":["distributions","confidence-interval"],"Answer_count":0,"Last_activity":"2016-07-30 09:42:26","Link":"http://stats.stackexchange.com/questions/226465/how-pages-per-visit-distributes","Creator_reputation":16}
{"_id":{"$oid":"5837a589a05283111e4d6e67"},"View_count":156,"Display_name":"sponge_knight","Question_score":1,"Question_content":"Say that I had a bootstrap distribution, can I return the mode as a point estimate? It's right-skewed, so the mean does not accurately summarize the distribution.","Creater_id":46925,"Start_date":"2016-07-30 05:11:39","Question_id":226434,"Tags":["bootstrap","mode"],"Answer_count":2,"Last_activity":"2016-07-30 09:41:13","Link":"http://stats.stackexchange.com/questions/226434/why-not-take-the-mode-of-a-bootstrap-distribution","Creator_reputation":1522}
{"_id":{"$oid":"5837a589a05283111e4d6e75"},"View_count":22,"Display_name":"sheppa28","Question_score":0,"Question_content":"Not sure if I'm confusing terminology, hence asking the question. I believe it is sufficient statistic may be referring too.Given some data wish to model with a probability distribution, and the estimates of parameters are found via MLE. What properties of the distribution, using MLE found parameters, match exactly with similar function on the data itself?I'm specifically looking at the Beta-Binomial Negative-Binomial distribution which is given in this paper \"A note on modeling underreported Poisson counts\" paper. Is there a way to tell if a function of the parameters relate to a computable function of the data, and that it is still equal at the maximum likelihood estimates?Example:Does there exist a function of the parameters  such that evaluated at MLE,  it equals some function of the raw data, The Normal distribution has  as a parameter, and at MLE, The Negative Binomial has  and at MLEIs this the same as stating that  is a sufficient statistic in these cases? What if the \"conserved\" quantity is some non-obvious formula such as   for example, how would that quantity be determined? ","Creater_id":16623,"Start_date":"2016-07-30 09:40:19","Question_id":226464,"Tags":["maximum-likelihood","sufficient-statistics"],"Answer_count":0,"Last_activity":"2016-07-30 09:40:19","Link":"http://stats.stackexchange.com/questions/226464/sufficient-statistic-terminology-confusion","Creator_reputation":441}
{"_id":{"$oid":"5837a589a05283111e4d6e77"},"View_count":89,"Display_name":"JerryTheForester","Question_score":1,"Question_content":"I am using the M5 model implemented in the RWeka package for predicting a continues variable based on several independent, ecological variables.model  \u0026lt;- M5P(T_apr  ~ ., data=train)I would like to use this to further build an ensemble model in R, but I'm having difficulties finding a way how to do this. Therefore my question: how to build an ensemble using M5 models in R?","Creater_id":111222,"Start_date":"2016-07-05 01:18:52","Question_id":222178,"Tags":["r","regression","machine-learning","ensemble"],"Answer_count":1,"Last_activity":"2016-07-30 09:25:03","Link":"http://stats.stackexchange.com/questions/222178/m5-model-in-ensemble","Creator_reputation":11}
{"_id":{"$oid":"5837a589a05283111e4d6e84"},"View_count":213,"Display_name":"jerome","Question_score":2,"Question_content":"First of all sorry for the questions that can be basics but I am very new in the field of forecasting. I am currently working on a problem where I have a time Serie of datas sampled each seconds for 4 days. (86400*4=345600 points).The blue curve in the graph below presents one day of datas but the same pattern repeats every day.By doing decompose on R, it appears that I have a seasonality for every day and one for every 30min (you might guess it in the graph)Question 1 : Which frequency should I choose. The daily one or the 30min ? I would intuitively choose the 30min one but when I do in R HoltWinters(timeseries), it seems that I have better results with a higher frequency.Question 2 : I tried with ARIMA but, as I understood, it seems that I have too many datas (or to big seasonality). Is it correct ?Question 3 : When I try to plot the forecasted values (for the next day or the next 30min) there are not very accurate although the plot from HoltWinters(timeserie) seems very good (SSE is low)Do not hesitate if you have any questions or need more precisions.Thanks by advance.Jerome","Creater_id":90691,"Start_date":"2015-09-28 23:31:03","Question_id":174644,"Tags":["forecasting"],"Answer_count":1,"Last_activity":"2016-07-30 09:02:44","Link":"http://stats.stackexchange.com/questions/174644/holt-winters-forecast","Creator_reputation":11}
{"_id":{"$oid":"5837a589a05283111e4d6e91"},"View_count":42,"Display_name":"sponge_knight","Question_score":0,"Question_content":"Is it possible to get a confidence interval on K-fold cross-validation without bootstrapping?Thoughts?","Creater_id":46925,"Start_date":"2016-07-29 15:38:13","Question_id":226377,"Tags":["bootstrap"],"Answer_count":0,"Last_activity":"2016-07-30 08:40:38","Link":"http://stats.stackexchange.com/questions/226377/getting-a-confidence-interval-on-cross-validation","Creator_reputation":1522}
{"_id":{"$oid":"5837a589a05283111e4d6e93"},"View_count":4199,"Display_name":"Tim","Question_score":9,"Question_content":"t-test for testing whether the mean of a normally distributed sample equals a constant is said to be a Wald test, by estimating the standard deviation of the sample mean by the fisher's information of the normal distribution at the sample mean. But the test statistic in the t test has a student t distribution, while the test staistic in a Wald test asymptotically has a chi-square distribution. I wonder how to explain that?In one-way ANOVA, the test statistic is defined as the ratio between between-class variance and within-class variance. I was wondering if it is also a Wald test? But the test statistic in one-way ANOVA has a F distribution, and the test statistic in a Wald test asymptotically has a chi-square distribution.  I wonder how to explain that?Thanks and regards!","Creater_id":1005,"Start_date":"2013-05-30 04:09:37","Question_id":60438,"Tags":["hypothesis-testing","anova"],"Answer_count":2,"Last_activity":"2016-07-30 08:26:19","Link":"http://stats.stackexchange.com/questions/60438/are-t-test-and-one-way-anova-both-wald-tests","Creator_reputation":5527}
{"_id":{"$oid":"5837a589a05283111e4d6ea1"},"View_count":77,"Display_name":"Enzo D\u0026#39;Innocenzo","Question_score":3,"Question_content":"In the Breusch-Godfrey test we use a model e_t = \\varepsilon_t + \\beta_1 \\varepsilon_{t-1} + \\dots+ \\beta_p \\varepsilon_{t-p}. If we reject the null hypotesis of no serial auotocorrelation of the error, it means that the residuals follow an auto-regressive model of order ().If I want to avoid this problem, I must add a certain number of lags of the response variable  as regressors in the original model. However, in some cases this method is not useful. Are there other options to consider? ","Creater_id":124731,"Start_date":"2016-07-29 05:19:38","Question_id":226279,"Tags":["time-series","autocorrelation","residuals"],"Answer_count":2,"Last_activity":"2016-07-30 08:18:54","Link":"http://stats.stackexchange.com/questions/226279/how-can-i-handle-autocorrelated-residuals","Creator_reputation":33}
{"_id":{"$oid":"5837a589a05283111e4d6eaf"},"View_count":33,"Display_name":"Rohit","Question_score":0,"Question_content":"I am looking to know what to do with bagging when n (observations) \u0026lt;\u0026lt; p (features), or we have wide data. Note that each of the features are useful/significant/required. So I cannot subset my features.Bagging, as it stands, addresses the problem of high variance in decision trees by creating multiple trees from bootstrapped observations. So when n \u0026lt;\u0026lt; p, we really do not care whether each individual tree overfits, right? So bagging should work just fine with wide data. Or is there something we need to do, be careful about?Appreciate your discussion on this idea.This is not a textbook question","Creater_id":78206,"Start_date":"2015-11-02 10:39:57","Question_id":179836,"Tags":["dataset","bagging"],"Answer_count":1,"Last_activity":"2016-07-30 08:09:56","Link":"http://stats.stackexchange.com/questions/179836/bagging-on-data-when-observations-features","Creator_reputation":115}
{"_id":{"$oid":"5837a589a05283111e4d6ebc"},"View_count":18,"Display_name":"jf328","Question_score":1,"Question_content":"Suppose everyday I want to predict the temperature reading taken somewhere in a city. I have some explanatory variables, say humidity and wind speed. And I have data for  cities. So the data look likedate       city   humidity    wind    temperature2000-09-01 London 70          12      15.02000-09-01 London 50          6       15.32000-09-01 London 80          7       15.52000-09-01 Paris  60          4       16.02000-09-02 London 80          1       18.02000-09-02 London 81          2       18.22000-09-02 Paris  60          4       16.0Number of readings per day is random. Suppose  and some cities have 900+ rows and some cities have only 30 rows.The model I want to build is to have a hidden \"normalised\" temperature of each day for each city which follows an AR process \\tilde{T}_d^{city} = \\alpha\\tilde{T}_{d-1}^{city} + \\epsilon,and the observed temperature is T_d^{city} = \\mu+X\\beta+\\tilde{T}_d^{city}+\\eta,where  are my explanatory variables,  are to be fitted and are independent of day or city.  are iid Normal (and I'm not sure if I should include  here).Is there any literature on such model or similar ones? How can I fit this model?NB The temperature example is for illustration only. I know it's wrong to have a constant  for the whole year, but let's just say my period goes from summer to winter.This is not a regression with AR error but similar. Regression-AR deals with one long time series while I have 250 time series with different lengths.","Creater_id":20551,"Start_date":"2016-07-28 13:41:23","Question_id":226181,"Tags":["time-series","references","autoregressive","latent-variable","state-space-models"],"Answer_count":0,"Last_activity":"2016-07-30 07:56:06","Link":"http://stats.stackexchange.com/questions/226181/linear-model-with-ar-hidden-state","Creator_reputation":416}
{"_id":{"$oid":"5837a589a05283111e4d6ebe"},"View_count":96,"Display_name":"Octoplus","Question_score":2,"Question_content":"I am reading the book Artificial Intelligence a Modern Approach and I have trouble understanding why the SVM needs to keep support vectors.From the book:  SVMs are a nonparametric method -- they retain training examples an  potentially need to store them all. On the other hand, in practice  they often end up retaining only a small fraction of the number of  examples.And then:  A final important property is that the weights associated with each data point are zero except for the support vectors -- the points closest to the separator. Because there are usually many fewer support vectors than examples SVMs gain some of the advantages of parametric models.  Source: Artificial Intelligence a Modern Approach  p746As the SVMs separator is defined by a hyperplane w.x + b = 0 we only need to know w and b to make predictions. Why should it keep all the support vectors?  ","Creater_id":76570,"Start_date":"2016-05-31 15:08:27","Question_id":215641,"Tags":["classification","svm"],"Answer_count":2,"Last_activity":"2016-07-30 07:37:39","Link":"http://stats.stackexchange.com/questions/215641/why-does-svm-needs-to-keep-support-vectors","Creator_reputation":27}
{"_id":{"$oid":"5837a589a05283111e4d6ecb"},"View_count":61,"Display_name":"Arsl\u0026#225;n","Question_score":2,"Question_content":"I have time series data about sales/day, but I also want to include other data (static/dynamic) to forecast the time series. Is it possible to combine ARIMA model and regression models to achieve the goal?","Creater_id":106704,"Start_date":"2016-07-25 07:22:19","Question_id":225525,"Tags":["regression","time-series","forecasting","arima"],"Answer_count":1,"Last_activity":"2016-07-30 07:33:11","Link":"http://stats.stackexchange.com/questions/225525/combining-arima-model-with-regression","Creator_reputation":27}
{"_id":{"$oid":"5837a589a05283111e4d6ed8"},"View_count":42,"Display_name":"Arsl\u0026#225;n","Question_score":1,"Question_content":"I'm new to time series modeling and am trying to do seasonal ARIMA modeling here. I have figured out the p,d,q values but im not sure how to select the period  in the below formula. There seem to be troughs in the data during summer months and winter holidays, what does it suggest the value of period should be, what is the concept.arima(time_series,c(2,1,4),seasonal=list(order=c(2,1,4),period=\u0026lt;??\u0026gt;))My data looks like thisEDIT : I have 365 data points for a year","Creater_id":106704,"Start_date":"2016-07-27 14:07:55","Question_id":225995,"Tags":["r","time-series","arima","seasonality"],"Answer_count":2,"Last_activity":"2016-07-30 07:30:13","Link":"http://stats.stackexchange.com/questions/225995/deciding-the-value-of-period-in-seasonal-arima-r","Creator_reputation":27}
{"_id":{"$oid":"5837a589a05283111e4d6ee6"},"View_count":28,"Display_name":"Alexander Maas","Question_score":0,"Question_content":"I have a pretty basic question for those of you familiar with finite mixture models.  The simple question is:  How come the coefficent estimates are so different between FMM and my OLS models.  Specifically, if I partition my sample based on the FMM classification and estimate coefficents via OLS for each \"class\", the coefficients are quite different from the ones that are estimated directly with the finite mixture model.  Does this suggest that one of the estimates must be bias? Does this result suggest something else of which I am not aware?  Any help or references to investigate this issue would be greatly appreciated. Thanks","Creater_id":124191,"Start_date":"2016-07-29 17:08:54","Question_id":226388,"Tags":["least-squares","finite-mixture-model"],"Answer_count":1,"Last_activity":"2016-07-30 07:15:50","Link":"http://stats.stackexchange.com/questions/226388/finite-mixture-models-versus-segmented-ols","Creator_reputation":1}
{"_id":{"$oid":"5837a589a05283111e4d6ef3"},"View_count":31,"Display_name":"Ralf Pierson","Question_score":1,"Question_content":"I searched already for similiar problems but couldn't find an answer.I split the file in SPSS by a two-level between group variable (sequence) and then want to perform two ANOVAS on each of those splitted parts. Do I need a correction of significance level as a result of multiple comparison problem? Like Bonferroni? Oder isn't it multiple cmparison because I do the two tests on seperate data? Thanks so much! Ralf ","Creater_id":124820,"Start_date":"2016-07-28 01:44:27","Question_id":226063,"Tags":["anova","spss","multiple-comparisons","bonferroni"],"Answer_count":0,"Last_activity":"2016-07-30 07:05:51","Link":"http://stats.stackexchange.com/questions/226063/does-a-split-file-datasat-need-a-correction-for-multiple-comparison","Creator_reputation":6}
{"_id":{"$oid":"5837a589a05283111e4d6ef5"},"View_count":46,"Display_name":"Euphe","Question_score":3,"Question_content":"I have a big task at hand - building a predictive model that predicts the amount of fires in a certain region of a certain city tomorrow based on historical data.Currently the problem was narrowed down to constructing a time series of amount of fires per date and inspecting it.What should I do to the time series to pick the right predictive algorithm?I have done an autocorrelation check which showed little to no autocorrelation.My question is purely about approaching the problem. What actions should one take first when inspecting a time series?","Creater_id":114581,"Start_date":"2016-07-29 02:21:41","Question_id":226242,"Tags":["time-series","predictive-models"],"Answer_count":2,"Last_activity":"2016-07-30 06:26:02","Link":"http://stats.stackexchange.com/questions/226242/approaching-a-time-series-before-building-a-predictive-model","Creator_reputation":140}
{"_id":{"$oid":"5837a589a05283111e4d6f02"},"View_count":22,"Display_name":"Teresa","Question_score":0,"Question_content":"I constructed GLM's to compare a set of variables before constructing GLMM (to model habitat selection). I also wanted to see if there were linear relationships between the response variable and the explanatory variables.I have a response binary variable, which correspond to used or available (1/0) locations of several individuals (which is the reason I will construct GLMM, to include them as a random effect).First question: since GLMM can handle non-linear variables (correct me if I am wrong), is it important to test linear relationships before constructing GLMMs?Second question: I plotted one of the models and I don't know how to interpretate the plot. I present the summary and the plot below.Here is the summary of the model:Deviance Residuals:     Min       1Q   Median       3Q      Max  -0.5364  -0.5364  -0.4028  -0.2793   2.6059  Coefficients:            Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept) -2.42919    0.02177 -111.58   \u0026lt;2e-16 ***LC2_z       -0.57874    0.02361  -24.51   \u0026lt;2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 19905  on 32669  degrees of freedomResidual deviance: 19191  on 32668  degrees of freedomAIC: 19195Here is the plot:How do I interpretate this? Is it normal to have these two lines?","Creater_id":117281,"Start_date":"2016-07-30 05:56:32","Question_id":226440,"Tags":["categorical-data","generalized-linear-model","glmm"],"Answer_count":1,"Last_activity":"2016-07-30 06:17:25","Link":"http://stats.stackexchange.com/questions/226440/glm-r-plot-linear-relationship","Creator_reputation":59}
{"_id":{"$oid":"5837a589a05283111e4d6f0f"},"View_count":38,"Display_name":"sxanus","Question_score":4,"Question_content":"I have a data set of customer purchases from the day of their registration to 120 days. There is a time series for each customer. However, some new customers do not have a history of 120 days yet. I want to predict how many purchases they will do by the time their history reaches 120 days.I have created a feature set including frequency of purchase, recency and monetary, and product category (https://en.wikipedia.org/wiki/RFM_(customer_value)).How can I train the model from the time series to make a regression for each customer?","Creater_id":124987,"Start_date":"2016-07-29 07:32:17","Question_id":226304,"Tags":["regression","time-series","forecasting"],"Answer_count":0,"Last_activity":"2016-07-30 05:45:50","Link":"http://stats.stackexchange.com/questions/226304/how-to-do-regression-on-a-time-series-by-learning-from-historical-time-series","Creator_reputation":21}
{"_id":{"$oid":"5837a589a05283111e4d6f11"},"View_count":68,"Display_name":"becko","Question_score":1,"Question_content":"According to Wikipedia, the th cumulant  is related to the central-moments  by the following recurrence:\\kappa_n = \\theta_n - \\sum_{m=1}^{n-1} \\binom{n-1}{m-1} \\kappa_m \\theta_{n-m}which allows one to compute the cumulants sequentially, given the central-moments. The Wikipedia page offers no reference or proof of this formula. What's the proof of this formula?","Creater_id":5536,"Start_date":"2016-07-28 09:57:23","Question_id":226141,"Tags":["moments","cumulants"],"Answer_count":1,"Last_activity":"2016-07-30 05:40:35","Link":"http://stats.stackexchange.com/questions/226141/proof-of-recurrence-between-cumulants-and-central-moments","Creator_reputation":712}
{"_id":{"$oid":"5837a589a05283111e4d6f1e"},"View_count":50,"Display_name":"D. Adams","Question_score":0,"Question_content":"My dissertation is about funds seasonality. The model that I am using is an OLS regression with dummies to check if January has a return greater than the remaining period:R_t = B_0 + B_1 D_{mt} + U_t is the return on funds is the intercept is the dummy variable. The value 1 corresponding to January and 0 otherwise is error termWhat tests do I have to run?I am checking for heteroscedasticty (White test) and serial correlation (Durbin-Watson test).Do I need to run other tests?","Creater_id":124788,"Start_date":"2016-07-27 17:45:44","Question_id":226025,"Tags":["time-series","panel-data","seasonality","finance"],"Answer_count":2,"Last_activity":"2016-07-30 05:27:10","Link":"http://stats.stackexchange.com/questions/226025/model-to-test-seasonality-of-funds","Creator_reputation":6}
{"_id":{"$oid":"5837a589a05283111e4d6f2c"},"View_count":175,"Display_name":"user3125","Question_score":2,"Question_content":"I am not very clear about some technical details in implementing Fully Convolutional Networksfor Semantic Segmentation.  The paper discusses three models: fcn32, fcn16 and fcn18. According to this description, for fcn16, looks like the last deconvolutional layer has stride 16. But what is the stride for the skip layer from pool 4.Similarly, for fcn8, looks like the last deconvolutional layer has stride 8. But what is the stride for the skip layer from pool3 and poo4?In the Tensorflow implementation of this model, author uses stride=2 for all these skip level cases? Are there any justifications for this?Moreover, for deconvolutional kernel, we also need to know the kernel size. The paper does not mention that. The above implementation using “kernel size = 4”, which can be found from the following definition, where ksize=4 is setup in defining the _upscore_layer.  What should be the criteria for setting up this kernel size.","Creater_id":3125,"Start_date":"2016-07-27 21:41:24","Question_id":226047,"Tags":["machine-learning","deep-learning","conv-neural-network","computer-vision","tensorflow"],"Answer_count":1,"Last_activity":"2016-07-30 05:15:54","Link":"http://stats.stackexchange.com/questions/226047/kernel-size-and-stride-value-for-fully-convolutional-network-for-semantic-segmen","Creator_reputation":766}
{"_id":{"$oid":"5837a589a05283111e4d6f39"},"View_count":26,"Display_name":"Cyurmt","Question_score":1,"Question_content":"What is the intuition behind the finite sample adjustments in the Ljung-Box test: ?Degrees of freedom adjustments usually involve subtraction, so where does the  come from? Also, why are farther lags weighted more in the  adjustment?","Creater_id":36542,"Start_date":"2016-07-29 09:51:47","Question_id":226334,"Tags":["time-series","arima","autocorrelation","small-sample","theory"],"Answer_count":1,"Last_activity":"2016-07-30 05:11:00","Link":"http://stats.stackexchange.com/questions/226334/ljung-box-finite-sample-adjustments","Creator_reputation":16}
{"_id":{"$oid":"5837a589a05283111e4d6f46"},"View_count":56,"Display_name":"TTZ","Question_score":0,"Question_content":"I have two groups (healthy and disease, different sample sizes) of data, pooled from the individual subject's measurements of a certain region of the brain.  The size of the brain region may be different from each individual and the sample sizes were different in these two groups.  I have just carried out the ks.test using R on the data.  They are not normally distributed. As such, I ended up having 2500 samples for the healthy group and 1000 samples for the disease group. I have plotted the histogram of these two groups (using normalized counts for y axis) and they appeared to be two different distributions, with overlapping. I would like to determine the cut-off value between two groups.  What would be the correct way to go about it? ","Creater_id":125065,"Start_date":"2016-07-30 01:55:34","Question_id":226420,"Tags":["sample-size","method-comparison"],"Answer_count":1,"Last_activity":"2016-07-30 05:10:53","Link":"http://stats.stackexchange.com/questions/226420/determining-a-cut-off-value-from-two-populations-of-unequal-sample-sizes","Creator_reputation":8}
{"_id":{"$oid":"5837a589a05283111e4d6f53"},"View_count":48,"Display_name":"Dieter Menne","Question_score":2,"Question_content":"We have measured a physiological parameter describing a gastric mechanism in 26 healthy volunteers. To establish a reference range of the parameter, we use the Harrell-Davis estimator, hdquantile in R.For 13 subjects, we have measured the parameter twice on separate occasions; carry-over effects can be ruled out with certainty, but there is a relevant within-subject correlation (about r = 0.7).If the endpoint were some between-treatment comparison, a mixed-model would be adequate. But how do we take into account correlation when reference ranges are to be estimated?Can someone point me to a paper or other reference?","Creater_id":8433,"Start_date":"2016-07-29 07:57:21","Question_id":226311,"Tags":["mixed-model","clinical-trials"],"Answer_count":1,"Last_activity":"2016-07-30 05:04:44","Link":"http://stats.stackexchange.com/questions/226311/mixed-model-harrell-davis-estimator-for-reference-range","Creator_reputation":337}
{"_id":{"$oid":"5837a589a05283111e4d6f60"},"View_count":42,"Display_name":"user3669801","Question_score":0,"Question_content":"I am conducting a survey analysis of 5-point scale questions. I have 17 questions and for each question I have response from responders on 5-point scale. My 5-point scale is \"Strongly Disagree\", \"Somewhat Disagree\", \"Neutral\", \"Somewhat Agree\" and \"Strongly Agree\". I am confused about value of x. What value should I use for x?.I did some reading and found out that value of x should be 0.8*5 = 4 on some blog. I would appreciate any help in guiding me using proper value of x.","Creater_id":124514,"Start_date":"2016-07-30 03:57:53","Question_id":226424,"Tags":["survey","descriptive-statistics","summary-statistics","z-score"],"Answer_count":0,"Last_activity":"2016-07-30 04:19:45","Link":"http://stats.stackexchange.com/questions/226424/value-of-x-in-z-score-in-5-point-scale","Creator_reputation":18}
{"_id":{"$oid":"5837a589a05283111e4d6f62"},"View_count":64,"Display_name":"Guido167","Question_score":1,"Question_content":"This problem has held me up for three days now, so I really hope somebody here has a solution for the problem.I have a model with an excessive number of zeros, so I use a zero-inflated poisson regression model with the following code and summary.cr_f1 = formula(cr ~ depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year | depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2)summary(zeroinfl(cr_f1, dist = \"poisson\", link = \"logit\", data = allUVCdata))Call:zeroinfl(formula = cr_f1, data = allUVCdata, dist = \"poisson\", link = \"logit\")Pearson residuals:    Min      1Q  Median      3Q     Max -1.6430 -0.5680 -0.2893  0.1426 16.8090 Count model coefficients (poisson with log link):                            Estimate Std. Error z value Pr(\u0026gt;|z|)   (Intercept)                -4.515522   2.182503  -2.069  0.03855 * depth                       0.108941   0.072278   1.507  0.13175   habtype2Pinnacles           0.879765   0.791166   1.112  0.26614   habtype2Unexposed          -0.604246   0.786129  -0.769  0.44211   month2                      0.628468   0.380450   1.652  0.09855 . month3                      0.309282   0.367690   0.841  0.40026   month4                      0.649411   0.371667   1.747  0.08059 . month5                      0.758717   0.364079   2.084  0.03717 * month6                      0.467611   0.341024   1.371  0.17031   month7                      0.523043   0.343363   1.523  0.12768   month8                      0.563272   0.356843   1.578  0.11445   month9                      0.204509   0.400398   0.511  0.60952   month10                     0.662415   0.341616   1.939  0.05249 . month11                     0.934844   0.335077   2.790  0.00527 **month12                     0.252216   0.360512   0.700  0.48417   year2013                   -1.271010   1.282158  -0.991  0.32154   year2014                    1.221887   0.753644   1.621  0.10495   year2015                   -0.463176   0.771131  -0.601  0.54808   lightregimeLight            2.754925   1.948779   1.414  0.15746   depth:month2               -0.019864   0.008906  -2.230  0.02572 * depth:month3               -0.014157   0.008106  -1.747  0.08071 . depth:month4               -0.020553   0.008332  -2.467  0.01364 * depth:month5               -0.021213   0.008373  -2.533  0.01129 * depth:month6               -0.013561   0.007393  -1.834  0.06663 . depth:month7               -0.015043   0.007544  -1.994  0.04615 * depth:month8               -0.017383   0.008011  -2.170  0.03003 * depth:month9               -0.012340   0.008990  -1.373  0.16988   depth:month10              -0.019631   0.007629  -2.573  0.01008 * depth:month11              -0.024101   0.007611  -3.167  0.00154 **depth:month12              -0.014319   0.007952  -1.801  0.07174 . depth:lightregimeLight     -0.079860   0.071024  -1.124  0.26084   depth:habtype2Pinnacles    -0.006819   0.011178  -0.610  0.54182   depth:habtype2Unexposed     0.014857   0.011103   1.338  0.18086   habtype2Pinnacles:year2013  1.351509   1.277930   1.058  0.29025   habtype2Unexposed:year2013  1.538282   1.256047   1.225  0.22069   habtype2Pinnacles:year2014 -1.213233   0.754305  -1.608  0.10775   habtype2Unexposed:year2014 -0.495275   0.726863  -0.681  0.49563   habtype2Pinnacles:year2015  0.389117   0.775476   0.502  0.61582   habtype2Unexposed:year2015  0.659117   0.750396   0.878  0.37975   Zero-inflation model coefficients (binomial with logit link):                        Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)             -4.61555    7.04621  -0.655 0.512442    depth                    0.28728    0.28211   1.018 0.308524    habtype2Pinnacles        9.41037    3.82210   2.462 0.013813 *  habtype2Unexposed        2.11213    1.46465   1.442 0.149282    month2                   8.67847    3.91193   2.218 0.026523 *  month3                   7.12210    3.86428   1.843 0.065320 .  month4                   4.10296    2.41285   1.700 0.089044 .  month5                  12.76919    4.28035   2.983 0.002852 ** month6                   3.57695    2.49820   1.432 0.152198    month7                   5.85534    3.27394   1.788 0.073700 .  month8                   5.59503    3.33054   1.680 0.092974 .  month9                   4.22953    3.76919   1.122 0.261807    month10                  6.35022    3.59424   1.767 0.077265 .  month11                  5.92079    3.36405   1.760 0.078404 .  month12                  4.36214    3.17233   1.375 0.169113    year2013                -0.18722    0.42651  -0.439 0.660688    year2014                -1.50194    0.45263  -3.318 0.000906 ***year2015                -9.79773    4.87536  -2.010 0.044469 *  lightregimeLight         0.79826    5.62419   0.142 0.887133    depth:month2            -0.39212    0.16795  -2.335 0.019557 *  depth:month3            -0.36363    0.16695  -2.178 0.029397 *  depth:month4            -0.21521    0.10211  -2.108 0.035059 *  depth:month5            -0.57543    0.16933  -3.398 0.000678 ***depth:month6            -0.24336    0.10398  -2.341 0.019256 *  depth:month7            -0.33704    0.13975  -2.412 0.015874 *  depth:month8            -0.35343    0.14683  -2.407 0.016082 *  depth:month9            -0.31787    0.16903  -1.881 0.060026 .  depth:month10           -0.37550    0.16021  -2.344 0.019087 *  depth:month11           -0.34650    0.14821  -2.338 0.019397 *  depth:month12           -0.29639    0.14221  -2.084 0.037142 *  depth:lightregimeLight   0.08117    0.21795   0.372 0.709571    depth:habtype2Pinnacles -0.57765    0.17049  -3.388 0.000704 ***depth:habtype2Unexposed -0.17897    0.06252  -2.863 0.004200 ** ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Number of iterations in BFGS optimization: 146 Log-likelihood: -3977 on 72 DfSo I included the interaction 'habtype2*year' in the count part of the formula, but now want to include it in the second model aswel (the binomial), but if I do I get the following error:cr_f1 = formula(cr ~ depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year | depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year)summary(zeroinfl(cr_f1, dist = \"poisson\", link = \"logit\", data = allUVCdata))Error in solve.default(as.matrix(fityear = as.numeric(as.character(allUVCdatahessian)) :   system is computationally singular: reciprocal condition number = 6.23277e-26\u0026gt; allUVCdatayear))\u0026gt; table(allUVCdatacr)          0    1    2    3    4    5    6    7  2012  750  149   25   12    3    0    0    0  2013 1133  209   69   16    4    1    1    0  2014  844  387  142   42   11    7    0    1  2015  833  401  125   31    5    3    1    2\u0026gt; table(allUVCdatacr)       0   1   2   3   4   5   6   7  1  299  53  18   7   1   1   1   2  10 346 104  40   9   4   4   0   0  11 328 114  43  17   5   0   0   0  12 350 112  29  10   2   0   0   0  2  248  80  16   1   1   0   0   1  3  303  82  24   6   0   0   1   0  4  329  93  32   4   0   0   0   0  5  277 105  28   9   1   1   0   0  6  312 111  36  12   3   2   0   0  7  362 113  46  14   5   2   0   0  8  213 100  25   8   1   1   0   0  9  193  79  24   4   0   0   0   0\u0026gt; table(allUVCdatayear)     2012 2013 2014 2015  1    41  129   72  140  10   91  149  152  115  11  112  121  150  124  12  112  124  154  113  2    35  108   79  125  3    33  149  101  133  4    88  105  150  115  5   101  108   95  117  6    94  115  142  125  7   118  133  153  138  8    61  114  100   73  9    53   78   86   83table(allUVCdatayear)            2012 2013 2014 2015  Exposed     93  138  120  144  Pinnacles  274  386  339  338  Unexposed  572  909  975  919","Creater_id":101294,"Start_date":"2016-07-28 02:34:56","Question_id":226066,"Tags":["r","regression","multiple-regression","generalized-linear-model","zero-inflation"],"Answer_count":1,"Last_activity":"2016-07-30 03:54:53","Link":"http://stats.stackexchange.com/questions/226066/computational-error-running-regression-model","Creator_reputation":30}
{"_id":{"$oid":"5837a589a05283111e4d6f6f"},"View_count":69,"Display_name":"Lfppfs","Question_score":3,"Question_content":"I’m interested in understanding if a count variable (abundance of insects caught in traps) differs between sites (I’ll call this “difference effect”), and assessing if this difference may appear in one area but not another one. My experimental design is:- 2 areas (categorical: area_1; area_2)- 5 transects (3 in one area and 2 in the other one) (categorical: \"A\";\"B\";\"C\")- 10 sites (2 sites for each transect) (dummy: 0;1)- 80 traps (8 in each site) (counts)My response variable is sites, and my predictors are groups of insects, each of them a count variable.I can only compare sites within transects (I cannot compare site 1 in transect A to site 0 in transect B; not sure if this means sites are paired), and, as the areas are distinct from one another, I cannot say the difference effect is the same in both areas, I have to separate this effect somehow.I’m running logistic regressions in order to know if there is a “difference effect” (a group of insects being more abundant) in one of the sites compared to the other one (within transects and in the same area). It is necessary that transects be integrated as random effects in my models.I can think of two options for modelling this: one option is to run separate analysis for each area, like so:model_area_1 \u0026lt;- glmer(Sites ~ group1 + group 2 + group3 + group4 + (1|Transect),family = binomial(link = \"logit\"), data = df_area_1)model_area_2 \u0026lt;- glmer(Sites ~ group1 + group 2 + group3 + group4 + (1|Transect), family = binomial(link = \"logit\"), data = df_area_2)The first model has N = 48 and group Transect = 3, while the second one has N = 32 and group Transect = 2.In the other option, transects would be included as nested within areas in one model:model_both_areas \u0026lt;- glmer(Sites ~ group1 + group 2 + group3 + group4 + (1|Area/Transect), family = binomial(link = \"logit\"), data = df_both_areas)This model has N = 80 and groups Transect/Area = 5 and Area = 2.I have two questions:1) is one of the options better than the other? (maybe I should compare them using AIC?). Again, my objective is to understand if a group of insects is more abundant in one site instead of the other, taking into consideration that this might happen in one area, but not in the other.2) is there any reason for me to use another link function? I saw this answer which says that extensive variables should be dealt with using log link function, so I think this is correct in my models.Hope this question is pertinent and I have been clear.","Creater_id":124701,"Start_date":"2016-07-29 13:39:29","Question_id":226359,"Tags":["logistic","mixed-model","nested"],"Answer_count":1,"Last_activity":"2016-07-30 01:46:03","Link":"http://stats.stackexchange.com/questions/226359/choosing-one-mixed-effects-model-in-a-logistic-regression","Creator_reputation":18}
{"_id":{"$oid":"5837a589a05283111e4d6f7c"},"View_count":416,"Display_name":"Sergey Zykov","Question_score":10,"Question_content":"I am reading an article whose method is fully based on the likelihood ratio test. The author says that the LR test against one sided alternatives is UMP. He proceeds by claiming that \"...even when it [the LR test] can not be shown to be uniformly most powerful, the LR test often has desirable statistical properties.\"I am wondering what statistical properties are meant here. Given that the author refers to those in passing, I assume they are common knowledge among statisticians. The only desirable property I have managed to find so far is the asymptotic chi-squared distribution of  (under some regularity conditions), where  is the LR ratio. I would also be thankful for a reference to a classical text where one can read about those desired properties.  ","Creater_id":31275,"Start_date":"2015-08-31 09:31:37","Question_id":169524,"Tags":["hypothesis-testing","power-analysis","power","likelihood-ratio","neyman-pearson-lemma"],"Answer_count":1,"Last_activity":"2016-07-30 00:31:28","Link":"http://stats.stackexchange.com/questions/169524/what-are-the-desirable-statistical-properties-of-the-likelihood-ratio-test","Creator_reputation":193}
{"_id":{"$oid":"5837a589a05283111e4d6f89"},"View_count":162,"Display_name":"Marat Zakirov","Question_score":4,"Question_content":"If you open any SVM guide you will see that 1/||w|| is proportional to margin size (which is meant to be maximized by SVM). But how did you get this result?On the picture below you may see 2 plots. One is   and second is   according to my understanding of comments margin should have size equal to 10 in this case. But if you look at plots you will see its is slightly less than 2.","Creater_id":86706,"Start_date":"2015-08-24 05:22:07","Question_id":168531,"Tags":["svm"],"Answer_count":1,"Last_activity":"2016-07-30 00:30:40","Link":"http://stats.stackexchange.com/questions/168531/svm-why-do-we-maximize-2-w","Creator_reputation":84}
{"_id":{"$oid":"5837a589a05283111e4d6f96"},"View_count":24,"Display_name":"Adrian Skeete","Question_score":-1,"Question_content":"I have rank data from a survey of Impact Assessments (IA). Each IA is ranked from A to F on a particular aspect (such as quality of introduction). These IAs come from 7 different sectors, but I don't have an equal number from each sector. The sectors and the number of IAs for each are as followsMining: 19Transport: 9Infrastructure: 10Waste: 10Energy:8Manufacturing: 5Environmental: 5I ran 21 Mann-Whit U tests in SPSS to determine if the ranks received by each sector were significantly different from one another. Many of the tests produce a significant difference at a 0.05 significance level. My question is, with such a big difference in the number number of observations for each IA sector, and in some cases only 5 observations (such as manufacturing and environment), would it be better for me to be to be only using results of a higher confidence interval such as 0.01? Especially considering that some aspects are not applicable to particular IA (approximately 7% of ranks were not applicable)","Creater_id":107731,"Start_date":"2016-07-29 16:02:09","Question_id":226382,"Tags":["statistical-significance","spss","ordinal","mann-whitney-u-test"],"Answer_count":1,"Last_activity":"2016-07-30 00:18:00","Link":"http://stats.stackexchange.com/questions/226382/mann-whitney-u-significance-levels-on-ordinal-data","Creator_reputation":4}
{"_id":{"$oid":"5837a589a05283111e4d6fa1"},"View_count":132,"Display_name":"user20160","Question_score":1,"Question_content":"The questionHow can I calculate p values for individual transitions in a Markov chain? I want to test the null hypothesis that the probability of entering state  from previous state  is less than or equal to the overall probability of being in state .DetailsI have an observed sequence of discrete states and am fitting a first order, stationary Markov chain. I'd like to calculate a matrix of p values (one for each possible transition). Say the state at time  is . Given states  and , I want to test the null hypothesis that  against the alternative hypothesis that .There are a couple hundred states. The 'true' transition matrix of the data-generating process is very sparse, meaning that each state can transition to only a few other states. All states transition to themselves with high probability, but there are no absorbing states. The data contain ~10,000 time points, but some transitions may only be observed several times. So, I'm probably not in the asymptotic regime.The literatureThe closest method I've found is described in:  Vautard et al. (1990). Statistical significance test for transition matrices of atmospheric Markov chains.They randomly permute the sequence of observed states and estimate transition probabilities from the shuffled data. For each pair of states , they calculate a p value as the fraction of shuffles for which the estimated  transition probability exceeds that of the original data. My hesitation about this method is that 1) They don't clearly state a null hypothesis. 2) The permutation destroys temporal dependence between all states. But, I can imagine many more models where some states exhibit temporal dependence and others don't. The method doesn't seem to be counting these cases, but I don't know whether it matters.Another paper:  Anderson and Goodman (1957). Statistical inference about Markov chains.They give a  test for the hypothesis that particular transition probabilities have particular values. Maybe I could use this to compare  to the estimated marginal probability of state , but this doesn't seem to take into account the uncertainty of estimating  from the data.","Creater_id":116440,"Start_date":"2016-06-03 11:06:38","Question_id":216190,"Tags":["hypothesis-testing","statistical-significance","p-value","markov-process","transition-matrix"],"Answer_count":0,"Last_activity":"2016-07-29 23:48:13","Link":"http://stats.stackexchange.com/questions/216190/significance-testing-for-markov-chain-transition-probabilities","Creator_reputation":3832}
{"_id":{"$oid":"5837a589a05283111e4d6fa3"},"View_count":35,"Display_name":"Parth Raghav","Question_score":0,"Question_content":"I have been lately researching and working with ANNs and I found that, despite of being used widely on our machines, we are unable to implement such networks in other paradigms: at a molecular level and so on. While in this case Boolean Networks show a promise.Though this question may make no sense at first, it is perhaps an interesting problem to consider:How to convert a neural network to boolean network?Simply constructing a neural network by connecting basic logic gates AND, NAND, NOR, XOR, XNOR, YES et cetera.I want to research on the topic, kindly help me as to where should I begin with because I am surely missing some important keywords here.[Kindly disregard the training/learning process]Thanks in advance","Creater_id":110367,"Start_date":"2016-07-29 22:56:39","Question_id":226406,"Tags":["neural-networks"],"Answer_count":1,"Last_activity":"2016-07-29 23:26:00","Link":"http://stats.stackexchange.com/questions/226406/neural-network-vs-boolean-network","Creator_reputation":28}
{"_id":{"$oid":"5837a58aa05283111e4d6fb0"},"View_count":620,"Display_name":"Daniel Falbel","Question_score":4,"Question_content":"How does randomForest package estimate class probabilities when I use predict(model, data, type = \"prob\")?I was using ranger for training random forests using the probability = T argument to predict probabilities. ranger says in documentation that it:  Grow a probability forest as in Malley et al. (2012).I simulated some data and tried both packages and obtained very different results (see code below)So I know that it uses a different technique (then ranger) to estimate probabilities. But which one?simulate_data \u0026lt;- function(n){  X \u0026lt;- data.frame(matrix(runif(n*10), ncol = 10))  Y \u0026lt;- data.frame(Y = rbinom(n, size = 1, prob = apply(X, 1, sum) %\u0026gt;%                               pnorm(mean = 5)                             ) %\u0026gt;%                     as.factor()  )   dplyr::bind_cols(X, Y)}treino \u0026lt;- simulate_data(10000)teste \u0026lt;- simulate_data(10000)library(ranger)modelo_ranger \u0026lt;- ranger(Y ~., data = treino,                                 num.trees = 100,                                 mtry = floor(sqrt(10)),                                 write.forest = T,                                 min.node.size = 100,                                 probability = T                                )modelo_randomForest \u0026lt;- randomForest(Y ~., data = treino,                                    ntree = 100,                                     mtry = floor(sqrt(10)),                                    nodesize = 100                                    )pred_ranger \u0026lt;- predict(modelo_ranger, teste)$predictions[,1]pred_randomForest \u0026lt;- predict(modelo_randomForest, teste, type = \"prob\")[,2]prob_real \u0026lt;- apply(teste[,1:10], 1, sum) %\u0026gt;% pnorm(mean = 5)data.frame(prob_real, pred_ranger, pred_randomForest) %\u0026gt;%  tidyr::gather(pacote, prob, -prob_real) %\u0026gt;%  ggplot(aes(x = prob, y = prob_real)) + geom_point(size = 0.1) + facet_wrap(~pacote)","Creater_id":44359,"Start_date":"2016-07-28 07:01:01","Question_id":226109,"Tags":["r","random-forest"],"Answer_count":2,"Last_activity":"2016-07-29 23:22:08","Link":"http://stats.stackexchange.com/questions/226109/how-does-predict-randomforest-estimate-class-probabilities","Creator_reputation":198}
{"_id":{"$oid":"5837a58aa05283111e4d6fbd"},"View_count":21,"Display_name":"Kironide","Question_score":2,"Question_content":"I am dealing with a problem wherein an object (say, a dart) is thrown at an area with many circular targets. These targets are all uniform radius and do not overlap, but are not necessarily touching or organized in any way. The throw is assumed to follow a bi-variate normal distribution, although I am also interested in other distributions potentially.I am trying to figure out the optimal aiming location, as in where the center of the distribution should be. With one target this is simple: the middle of that target. But with multiple, it can be more challenging, like in the picture below (red are shots, grey are targets):In this picture the center is the average x,y coordinates of each target. but it is clear that this is not the optimal location.Is there a way to figure out the best location for it? I am using python for this so solutions relating to that are preferred, but R or math are welcome.","Creater_id":100223,"Start_date":"2016-07-29 21:22:05","Question_id":226400,"Tags":["r","distributions","normal-distribution","python"],"Answer_count":1,"Last_activity":"2016-07-29 22:53:16","Link":"http://stats.stackexchange.com/questions/226400/calculating-ideal-shot-at-multiple-off-centered-targets","Creator_reputation":111}
{"_id":{"$oid":"5837a58aa05283111e4d6fca"},"View_count":39,"Display_name":"psych 101","Question_score":0,"Question_content":"If I set up an interaction term between two dichotomous variables, let's say experiment (1= control, 2 = experiment group)  and gender (1 = male , 2 = female). If the effect of the interaction term (experiment*gender) on happiness is .46 using regression, does that mean the experiment increased happiness in females more than males?","Creater_id":125032,"Start_date":"2016-07-29 14:10:35","Question_id":226366,"Tags":["regression","interaction","interaction-variable"],"Answer_count":1,"Last_activity":"2016-07-29 21:58:39","Link":"http://stats.stackexchange.com/questions/226366/interpret-effect-of-interaction-term-between-two-binary-variables-on-continuous","Creator_reputation":5}
{"_id":{"$oid":"5837a58aa05283111e4d6fd7"},"View_count":116,"Display_name":"newbie","Question_score":1,"Question_content":"I have a short panel dataset that I want to analyze. It contains panel data of multiple individuals. Each individual is assigned to 1 of 4 treatment groups and I am interested in the effect of these treatments. Basically I want to run this regression:Y = X + Z + Condition(X, Z vary over time; Condition is a dummy variable and time-constant)I can't use the fixed effects \"within\" model, because the variable I'm interested in will drop out - since it is time-invariant.reformulated: I think the proper way to analyze this dataset regarding my variable of interest (Condition) is random effects. However, I am not sure - is there a better way?","Creater_id":124778,"Start_date":"2016-07-27 14:44:15","Question_id":226006,"Tags":["r","categorical-data","random-effects-model","fixed-effects-model","plm"],"Answer_count":1,"Last_activity":"2016-07-29 21:20:39","Link":"http://stats.stackexchange.com/questions/226006/plm-keep-dummy-variable-in-fixed-effects-random-effects-analysis","Creator_reputation":6}
{"_id":{"$oid":"5837a58aa05283111e4d6fe4"},"View_count":28,"Display_name":"shionlau","Question_score":1,"Question_content":"The standard Bayesian parameter estimation deals with the problem of estimating , where  is a constant but unknown parameter and  are observations/data. I know there exists some convergence results showing the point estimate, say MLE , converges to the true parameter value .My question is that, if  is time variant and we know its deterministic dynamics, e.g. , can we obtain some similar result that the error between the MLE  and the true value , i.e. , converges to  asymptotically?","Creater_id":83841,"Start_date":"2016-07-29 19:57:37","Question_id":226399,"Tags":["time-series","bayesian","estimation","convergence"],"Answer_count":0,"Last_activity":"2016-07-29 19:57:37","Link":"http://stats.stackexchange.com/questions/226399/bayesian-parameter-estimation-with-time-variant-parameter","Creator_reputation":23}
{"_id":{"$oid":"5837a58aa05283111e4d6fe6"},"View_count":31,"Display_name":"Paparazzi","Question_score":0,"Question_content":"I am writing a tool that calculates the chance of winning a poker hand.    Without going into the rules each player has 2 hole cards and there are 5 shared cards on the board.  From 2 cards in the hand plus the 5 on the board you make the  best 5 card hand and best hand wins.   For two known hands I can just iterate through all the possible boards (combinations) and calculate the chance of winning. Against an unknown hand there are too many combinations to run them all in a reasonable period of time.  Hand is not order dependent. Only have to consider the combinations.  I have loops that delivers the combinations.Is this a reasonable / proper approach?Is there a better approach?remove the two known cards from the deck  shuffle the remaining 50 cardsFisher Yates take the first 5 cards for the board (shared)   take the next two cards for the unknown hand  determine the winner   discards the first card rotate first card from unknown hand to board unknown hand gets next combination card from shuffle  determine winner back to discard  Can get though about 1/4 of the combination in 10 seconds.  Most users would rather have a good answer in 10 seconds than wait a minute. Loop is good as I used it on  to brute force the chances of hands and matches the answer on wiki exactly.The zero based loop for all the  combinationsIt creates the correct number of combinations  for(int i = 49; i \u0026gt;= 6; i--)   for(int j = i-1; j \u0026gt;= 5; j--)       for(int k = j-1; k \u0026gt;= 4; k--)          for(int m = k-1; , \u0026gt;= 3; m--)               for(int n = m-1; n \u0026gt;= 2; n--)                 for(int p = n-1; p \u0026gt;= 1; p--)                    for(int q = p-1; q \u0026gt;= 0; q--)Not sure if this is Monte Carlo but I still felt like it is was proper tag.But would like to know if it is a valid Monte Carlo.  ","Creater_id":54616,"Start_date":"2016-07-28 12:04:37","Question_id":226165,"Tags":["monte-carlo","games"],"Answer_count":0,"Last_activity":"2016-07-29 17:41:40","Link":"http://stats.stackexchange.com/questions/226165/poker-equity-calulator","Creator_reputation":196}
{"_id":{"$oid":"5837a58aa05283111e4d6fe8"},"View_count":53,"Display_name":"R. Carlos","Question_score":0,"Question_content":"I am wondering how to derive the formula for the standard error of Pearson's correlation coefficient which is given in Zar for example as\\newcommand{\\cov}{{\\rm Cov}}\\newcommand{\\var}{{\\rm Var}}\\newcommand{\\sd}{{\\rm SD}}SE_r =\\sqrt{\\frac{1-r^2}{n-2}}I tried to get it from estimating the variance of r when  r =\\frac{\\cov(x,y)}{\\sd(x)\\sd(y)}and  so we get . But from here I don't know how to continue since  would have to be  to get finally to\\var(r) =\\frac{1-r^2}{n-2}Any suggestions or references where I could look this up?","Creater_id":125041,"Start_date":"2016-07-29 15:52:51","Question_id":226380,"Tags":["correlation","standard-error","pearson"],"Answer_count":0,"Last_activity":"2016-07-29 17:12:43","Link":"http://stats.stackexchange.com/questions/226380/derivation-of-the-standard-error-for-pearsons-correlation-coefficient","Creator_reputation":61}
{"_id":{"$oid":"5837a58aa05283111e4d6fea"},"View_count":72,"Display_name":"bla345","Question_score":2,"Question_content":"intuitively it seems that low variance features are not useful and are just noise to the model. is it important to remove the features though? i.e., does the model performance improve significantly by removing irrelevant features?I'm particularly interested in the effect on binary classification models.","Creater_id":124606,"Start_date":"2016-07-29 15:54:30","Question_id":226381,"Tags":["feature-selection","feature-construction"],"Answer_count":1,"Last_activity":"2016-07-29 16:15:08","Link":"http://stats.stackexchange.com/questions/226381/what-is-the-effect-of-low-variance-features-on-machine-learning-models","Creator_reputation":21}
{"_id":{"$oid":"5837a58aa05283111e4d6ff7"},"View_count":91,"Display_name":"12kate34","Question_score":-1,"Question_content":"In this paper giving an overview of machine learning, the author writes:  Generalizing correctly becomes exponentially harder as the dimensionality (number of features) of the examples grows, because a fixed-size training set covers a dwindling fraction of the input space. Even with a moderate dimension of  and a huge training set of a trillion examples, the latter covers only a fraction of about  of the input space. This is what makes machine learning both necessary and hard. I also don't understand what he means by the input space in this context. I know he's referring to a vector space. I think he might be referring to a vector space which somehow represents all of the parameters. But I don't understand how this relates to the training set examples, and where he's getting the number  from  features and 1 trillion training examples - is there some way of calculating this, or is it some kind of estimate somehow? I have the impression that the fraction of the input space covered by the training set relates to the idea that one cannot solve a system of equation with more columns than rows, and I can vaguely see how in machine learning there could generally be issues if one just didn't have many columns compared to rows. But I'm not sure, and I wish I understood why this mattered but I don't even know where to look to find this information. ReferencesDomingos, P. (2012). A few useful things to know about machine learning. Communications of the ACM, 55(10), 78-87.","Creater_id":125009,"Start_date":"2016-07-29 14:16:13","Question_id":226369,"Tags":["machine-learning","feature-selection","terminology","overfitting"],"Answer_count":1,"Last_activity":"2016-07-29 16:10:00","Link":"http://stats.stackexchange.com/questions/226369/what-is-an-input-space-and-why-does-the-fraction-of-the-input-space-covered-by","Creator_reputation":3}
{"_id":{"$oid":"5837a58aa05283111e4d7004"},"View_count":88,"Display_name":"Bajcz","Question_score":3,"Question_content":"This is going to look like a duplicate of a common question--something along the lines of \"Should/can I remove insignificant regression terms?\" That type of question has been asked--and answered--here and here and many other places besides.My question is more specific and, I hope, grapples more with the nuance at the core of the issue.Let's say I have a simple linear regression of the form:Y ~ X1 + X2 + Z1 + Z2In this case, X1 and X2 are binary factors variables (i.e. \"treated\" versus \"untreated\") representing two different treatments whose potential impacts on some outcome variable Y I have concrete hypotheses about. Z1 and Z2, meanwhile, are some kind of continuous measures of how relatively intense the corresponding treatment was for each unit studied. For example, if X2 is \"leaf removal,\" Z2 might be the exact number of leaves removed from each plant, which maybe varied for some unavoidable reason. Now, I'm not interested in knowing whether Y dropped more for plants that had 40 leaves removed versus 38 leaves removed (Z2), though I could believe that that could happen. That within-treatment variability is not my focus. I'm interested in knowing simply if leaf removal in general resulted in a significant change in Y (X2). I include Z1 and Z2 only as a way to account for a possible additional source of variance in Y that I know about and would like to account for, if it matters, but that I am not particularly interested in. Now, when I run my regression, I find that Z1 and Z2 are not remotely significant, nor are their effect sizes large. X1 and X2 aren't significant either. But if I remove Z1 and Z2, suddenly X1 and X2 become significant. So, obviously, the inclusion of those two terms affects my conclusions.Now, here are my thoughts:Z1 and Z2 are not apparently accounting for as much variance inthe data as I thought they would. Maybe my estimate of them is tooimprecise to be useful and may actually be reducing my power todetect a treatment effect? Removing them increases the simplicity ofmy model, making it easier to describe.Z1 and Z2 are notcentral to my hypotheses, nor are they particularly germane in theliterature available on this question. That is, other, similarstudies don't generally include them, and no one will think it isstrange if I don't either.All of that said, do I really have a reasonable justification to remove Z1 and Z2 from this model for value Y when, for value K in a different model of mine, they would be predictive and thus should be left in?My conclusion is, so far, that I either have to conclude they are not useful predictors and remove them from all my models or conclude that they may be useful predictors at least some of the time and leave them in all my models. Otherwise, I feel as though I am somehow having my cake and eating it also. Is this a fair conclusion to reach? Or are there additional ethical and practical considerations that need to be made here that I am unaware of? ","Creater_id":61175,"Start_date":"2016-07-29 12:28:15","Question_id":226352,"Tags":["regression","hypothesis-testing","multiple-regression","covariate"],"Answer_count":2,"Last_activity":"2016-07-29 15:19:34","Link":"http://stats.stackexchange.com/questions/226352/if-a-regression-term-doesnt-do-what-it-was-intended-to-do-is-it-alright-to-rem","Creator_reputation":130}
{"_id":{"$oid":"5837a58aa05283111e4d7012"},"View_count":74,"Display_name":"RegalPlatypus","Question_score":1,"Question_content":"Like the title says.  I'm using the latest versions of both packages.  My model is fairly simple to begin with:model \u0026lt;- glmer(binary~factor1*continuous + (1|factor2), data=my.data, family=binomial)Both packages should be using a Laplace estimator, but I get a convergence warning for the lme4 package that I don't get in glmmADMB.  Estimates are very similar for both packages.lme4 outputBT20.glmer \u0026lt;- glmer(survival ~ tree * pctrans + (1|trayid), data=BT20.trimmed, family=binomial)Warning message:In checkConv(attr(opt, \"derivs\"), optcheckConv,  :  Model failed to converge with max|grad| = 0.00103723 (tol = 0.001, component 1)summary(BT20.glmer)Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod'] Family: binomial  ( logit )Formula: survival ~ tree * pctrans + (1 | trayid)   Data: BT20.trimmed     AIC      BIC   logLik deviance df.resid    775.7    833.1   -374.8    749.7      598 Scaled residuals:     Min      1Q  Median      3Q     Max -2.0280 -0.7618 -0.4364  0.8399  2.2683 Random effects: Groups Name        Variance Std.Dev. trayid (Intercept) 0.1577   0.3971  Number of obs: 611, groups:  trayid, 42Fixed effects:                   Estimate Std. Error z value Pr(\u0026gt;|z|)(Intercept)        -0.39192    0.57742  -0.679    0.497treeBC3F3           1.26660    0.86633   1.462    0.144treeD54            -0.23165    0.83908  -0.276    0.782treeD58            -1.19841    0.88656  -1.352    0.176treeEllis1         -1.28773    0.87973  -1.464    0.143treeQing           -0.62360    0.74321  -0.839    0.401pctrans             1.58330    1.45625   1.087    0.277treeBC3F3:pctrans  -1.00729    2.02795  -0.497    0.619treeD54:pctrans     0.09817    2.12836   0.046    0.963treeD58:pctrans     0.43424    2.42337   0.179    0.858treeEllis1:pctrans -0.57459    2.30508  -0.249    0.803treeQing:pctrans    0.47065    1.64346   0.286    0.775Correlation of Fixed Effects:            (Intr)    tr1     tr2    tr3    tr4    tr5 pctrns   tr1:    tr2:   tr3:    tr4:tree1       -0.666                                                                        tree2       -0.687  0.459                                                                 tree3       -0.651  0.434   0.448                                                         tree4       -0.657  0.437   0.450  0.427                                                  tree5       -0.776  0.518   0.536  0.506  0.509                                           pctrans     -0.896  0.597   0.615  0.583  0.589  0.695                                    tr1:pct      0.643 -0.894  -0.442 -0.419 -0.422 -0.499 -0.718                             tr2:pctrn    0.612 -0.409  -0.887 -0.399 -0.401 -0.477 -0.683  0.491                      tr3:pctrn    0.537 -0.359  -0.372 -0.896 -0.352 -0.420 -0.599  0.431   0.412              tr4:pct      0.565 -0.377  -0.391 -0.369 -0.897 -0.441 -0.630  0.453   0.433  0.381       tr5:pctrn    0.793 -0.529  -0.548 -0.517 -0.520 -0.871 -0.885  0.635   0.607  0.533  0.560convergence code: 0Model failed to converge with max|grad| = 0.00103723 (tol = 0.001, component 1)glmmADMB\u0026gt; BT20.admb \u0026lt;- glmmadmb(survival ~ tree*pctrans + (1|trayid), data=BT20.trimmed, family=\"binomial\")\u0026gt; summary(BT20.admb)Call:glmmadmb(formula = survival ~ tree * pctrans + (1 | trayid),     data = BT20.trimmed, family = \"binomial\")AIC: 775.7 Coefficients:                   Estimate Std. Error z value Pr(\u0026gt;|z|)(Intercept)         -0.3915     0.5777   -0.68     0.50tree1                1.2660     0.8675    1.46     0.14tree2               -0.2320     0.8394   -0.28     0.78tree3               -1.1993     0.8873   -1.35     0.18tree4               -1.2882     0.8808   -1.46     0.14tree5               -0.6242     0.7435   -0.84     0.40pctrans              1.5821     1.4572    1.09     0.28tree1:pctrans       -1.0056     2.0310   -0.50     0.62tree2:pctrans        0.0995     2.1292    0.05     0.96tree3:pctrans        0.4365     2.4250    0.18     0.86tree4:pctrans       -0.5735     2.3078   -0.25     0.80tree5:pctrans        0.4722     1.6445    0.29     0.77Number of observations: total=611, trayid=42 Random effect variance(s):Group=trayid            Variance StdDev(Intercept)   0.1577 0.3971Log-likelihood: -374.835 lme4 optimizer outputstart par. =  1 fn =  761.1452 At returneval:  17 fn:      749.70460 par: 0.394784(NM) 20: f = 749.704 at  0.394784  -0.38221   1.23536  -0.22496   -1.1676  -1.25786 -0.611638   1.54437 -0.981101 0.0945286  0.422298 -0.559069  0.466482(NM) 40: f = 749.704 at  0.394784  -0.38221   1.23536  -0.22496   -1.1676  -1.25786 -0.611638   1.54437 -0.981101 0.0945286  0.422298 -0.559069  0.466482(NM) 60: f = 749.704 at  0.394784  -0.38221   1.23536  -0.22496   -1.1676  -1.25786 -0.611638   1.54437 -0.981101 0.0945286  0.422298 -0.559069  0.466482(NM) 80: f = 749.693 at  0.403214 -0.398286   1.27238 -0.229772   -1.1895  -1.28617 -0.611556    1.5764 -0.934195  0.131065  0.427062 -0.577178  0.503229(NM) 100: f = 749.683 at  0.398067 -0.386183   1.25362 -0.224924  -1.17371  -1.26664 -0.607996   1.53317 -0.956612  0.117117  0.431422 -0.552243   0.48632(NM) 120: f = 749.677 at  0.403184 -0.384828    1.2495 -0.237432   -1.1928  -1.27768 -0.630575   1.56274 -0.962548  0.140994  0.409612 -0.558911  0.489161(NM) 140: f = 749.676 at  0.401892 -0.391983   1.25148 -0.231979  -1.17635  -1.28301 -0.621995   1.55619 -0.955253  0.148741  0.419699 -0.551909   0.49057(NM) 160: f = 749.673 at  0.401711 -0.389823   1.24776 -0.248815  -1.18684  -1.29189 -0.625875    1.5646 -0.957093  0.149736   0.40804 -0.553094  0.488138(NM) 180: f = 749.672 at  0.400601 -0.385288   1.25068 -0.242765  -1.19045  -1.29892 -0.629416   1.56929 -0.974233  0.122587  0.398086 -0.542567  0.483422(NM) 200: f = 749.671 at  0.400013 -0.385172   1.25169 -0.243985  -1.19194  -1.30175 -0.630152    1.5716 -0.975104  0.120825  0.396965 -0.542863  0.482651(NM) 220: f = 749.671 at  0.398195 -0.385741    1.2534 -0.245356  -1.19137  -1.30133  -0.62962   1.57661  -0.98824  0.119274  0.404967 -0.548629  0.478899(NM) 240: f = 749.671 at  0.396846 -0.386089   1.25599  -0.23783  -1.18838  -1.29705 -0.627903   1.56999 -0.983097  0.112611  0.412207 -0.550918  0.482465(NM) 260: f = 749.671 at  0.396791 -0.386088    1.2563 -0.237798  -1.18869  -1.29718 -0.627977   1.57024 -0.983812  0.112462  0.412037 -0.550764  0.482539(NM) 280: f = 749.67 at  0.396502 -0.385236   1.25722 -0.238627  -1.19152  -1.29885 -0.629215   1.56901 -0.989994  0.110833  0.409519 -0.545798  0.484243(NM) 300: f = 749.67 at  0.396416 -0.386106   1.26028 -0.236847  -1.19112  -1.29556  -0.62914   1.56895 -0.992995  0.107319  0.413889 -0.551313  0.484151(NM) 320: f = 749.67 at  0.395913 -0.387481   1.25938 -0.237382  -1.19242  -1.29511 -0.628587   1.57013  -0.98866  0.110182  0.414504 -0.548281  0.484826(NM) 340: f = 749.67 at  0.395808 -0.387955   1.26093 -0.236321  -1.19224  -1.29443 -0.629328   1.57052 -0.993145  0.107512  0.419233 -0.551661  0.486564(NM) 360: f = 749.67 at  0.396856 -0.388637   1.26064 -0.236805  -1.19136  -1.29248 -0.629241   1.57061 -0.990447  0.115044  0.418231 -0.554314  0.487459(NM) 380: f = 749.67 at  0.396118 -0.389248   1.26173 -0.236795  -1.19326   -1.2919 -0.629308   1.57276 -0.993918  0.110452  0.422509 -0.555278  0.487294(NM) 400: f = 749.67 at  0.396621 -0.389002   1.26128 -0.235907  -1.19246  -1.29135  -0.63008   1.57381 -0.995446  0.110692  0.422069 -0.558286  0.486561(NM) 420: f = 749.67 at  0.396519 -0.388862    1.2613  -0.23713  -1.19317  -1.29216 -0.629661   1.57408 -0.994968  0.112797  0.419094 -0.557443  0.486028(NM) 440: f = 749.67 at  0.396248  -0.38993   1.26353 -0.235109  -1.19359   -1.2903 -0.630125   1.57579  -1.00009  0.108874  0.425007 -0.561882   0.48679(NM) 460: f = 749.67 at  0.396277 -0.389883   1.26476 -0.233885  -1.19395  -1.29026 -0.630761   1.57728  -1.00328  0.105159  0.424146 -0.563362  0.485087(NM) 480: f = 749.67 at  0.396348 -0.389975   1.26565 -0.234128  -1.19478  -1.29018 -0.630783   1.57876  -1.00724  0.104176  0.423989 -0.566655  0.483861(NM) 500: f = 749.67 at  0.396215 -0.390091   1.26585 -0.233658  -1.19439  -1.28956 -0.630044   1.57901   -1.0065  0.103831  0.423434 -0.567486  0.482324(NM) 520: f = 749.67 at  0.396484 -0.391154   1.26807 -0.233121  -1.19536  -1.28886 -0.629592   1.58241  -1.01112   0.10188  0.423351 -0.573978  0.479046(NM) 540: f = 749.67 at  0.396361 -0.390515   1.26651 -0.234651  -1.19448  -1.28952 -0.628941   1.58035   -1.0062  0.105582  0.422028 -0.570506   0.47979(NM) 560: f = 749.67 at  0.396698 -0.390748   1.26668 -0.234277  -1.19455   -1.2887 -0.628991   1.58135  -1.00742  0.105631  0.421337 -0.573112   0.47857(NM) 580: f = 749.67 at  0.396857 -0.391228   1.26664  -0.23428  -1.19452  -1.28799 -0.628566    1.5812   -1.0071  0.107476  0.422354  -0.57374  0.479248(NM) 600: f = 749.67 at  0.396639 -0.391074   1.26688 -0.233833  -1.19452  -1.28761 -0.627606   1.58228  -1.00798  0.105107  0.421428 -0.576241  0.475988(NM) 620: f = 749.67 at  0.396514 -0.391446   1.26679 -0.233815  -1.19457  -1.28567 -0.625137     1.583  -1.00846  0.105126   0.42157 -0.580426  0.471714(NM) 640: f = 749.67 at  0.396445 -0.391424   1.26699 -0.233415  -1.19449  -1.28578 -0.625553   1.58222  -1.00829  0.103624  0.422954 -0.578994  0.472756(NM) 660: f = 749.67 at  0.396452 -0.392158   1.26703 -0.233823   -1.1947  -1.28512  -0.62531   1.58277  -1.00754  0.104789  0.425069 -0.579177  0.473394(NM) 680: f = 749.67 at  0.396455 -0.392274    1.2671  -0.23388  -1.19477  -1.28495 -0.625224     1.583  -1.00758  0.104935  0.425272 -0.579541  0.473252(NM) 700: f = 749.67 at  0.396481 -0.392492    1.2674 -0.233511  -1.19516  -1.28439  -0.62484   1.58438  -1.00936  0.104054  0.425045 -0.582091  0.471637(NM) 720: f = 749.67 at   0.39644 -0.392519   1.26742 -0.233029   -1.1955  -1.28441 -0.624835   1.58457  -1.01027  0.101949  0.425569 -0.582011  0.471336(NM) 740: f = 749.67 at   0.39652 -0.392395   1.26728 -0.232842  -1.19564  -1.28487 -0.625022    1.5845  -1.01085  0.100923  0.425203 -0.581493  0.471482(NM) 760: f = 749.67 at  0.396412 -0.392344   1.26728 -0.233176  -1.19569  -1.28522 -0.625116   1.58465     -1.01  0.101249   0.42496  -0.58113  0.471918(NM) 780: f = 749.67 at  0.396497 -0.392331   1.26719 -0.232923  -1.19593  -1.28524 -0.624906   1.58523  -1.01089 0.0996949  0.424681 -0.582026  0.470846(NM) 800: f = 749.67 at   0.39655 -0.392371   1.26689 -0.233195  -1.19585   -1.2853  -0.62467   1.58501  -1.00981  0.100425   0.42451 -0.581754  0.471202(NM) 820: f = 749.67 at  0.396567 -0.392335   1.26671 -0.232965  -1.19587  -1.28512 -0.624433   1.58508  -1.00973 0.0997577  0.424518 -0.582178  0.470849(NM) 840: f = 749.67 at  0.396732 -0.392229   1.26611 -0.233071  -1.19556   -1.2851 -0.624647   1.58446  -1.00836  0.100429  0.424319 -0.581106  0.471419(NM) 860: f = 749.67 at  0.396703 -0.392226   1.26609 -0.232815  -1.19551  -1.28519   -0.6244   1.58497  -1.00843  0.099104  0.424076  -0.58195  0.470471(NM) 880: f = 749.67 at  0.396746  -0.39207   1.26541 -0.232133   -1.1953  -1.28533 -0.624674   1.58472   -1.0068 0.0971924  0.424188 -0.581183  0.471429(NM) 900: f = 749.67 at  0.396766 -0.392408   1.26572 -0.232447  -1.19564  -1.28481 -0.624072   1.58574  -1.00791 0.0975808   0.42433 -0.582941  0.469748(NM) 920: f = 749.67 at  0.396699   -0.3925   1.26588 -0.232031  -1.19554   -1.2844 -0.624127   1.58569  -1.00812 0.0970635  0.425244  -0.58326  0.470014(NM) 940: f = 749.67 at  0.396713 -0.392349   1.26587 -0.232277  -1.19553  -1.28479 -0.624183   1.58556  -1.00817 0.0974172  0.424577 -0.582925  0.469947(NM) 960: f = 749.67 at  0.396687 -0.392355   1.26594 -0.232175  -1.19552  -1.28485 -0.624305    1.5855  -1.00822 0.0972216  0.424832   -0.5828   0.47036(NM) 980: f = 749.67 at  0.396698 -0.392454   1.26589 -0.232042  -1.19551  -1.28465 -0.624227   1.58578  -1.00815 0.0968939  0.425079 -0.583228  0.470127(NM) 1000: f = 749.67 at  0.396682 -0.392514   1.26604 -0.232122  -1.19546  -1.28455 -0.624167   1.58579  -1.00828 0.0972957  0.425222  -0.58347  0.470069(NM) 1020: f = 749.67 at  0.396685 -0.392423   1.26596 -0.232176  -1.19541  -1.28462 -0.624197   1.58546  -1.00801  0.097574  0.425059 -0.583059  0.470341(NM) 1040: f = 749.67 at  0.396686 -0.392564    1.2661 -0.232132  -1.19548   -1.2845 -0.624159   1.58586   -1.0083 0.0974297  0.425244 -0.583608  0.470085(NM) 1060: f = 749.67 at  0.396728 -0.392532   1.26598 -0.232147  -1.19547  -1.28453 -0.624109   1.58582  -1.00817 0.0972738  0.425098 -0.583576  0.469964(NM) 1080: f = 749.67 at  0.396728 -0.392496   1.26605 -0.232187  -1.19543   -1.2846 -0.624203   1.58561  -1.00813 0.0976207  0.425015 -0.583363  0.470273(NM) 1100: f = 749.67 at   0.39677 -0.392593   1.26611 -0.232074  -1.19545  -1.28443 -0.624014   1.58572  -1.00816 0.0973341  0.425115 -0.583941  0.470047(NM) 1120: f = 749.67 at  0.396793 -0.392563   1.26632 -0.231778  -1.19539  -1.28445 -0.624179   1.58544   -1.0081 0.0970493   0.42508 -0.584049  0.470614(NM) 1140: f = 749.67 at    0.3968 -0.392604   1.26623 -0.231627  -1.19539  -1.28427 -0.623946   1.58554  -1.00805 0.0964774  0.425111 -0.584385  0.470109(NM) 1160: f = 749.67 at  0.396831 -0.392569   1.26627 -0.231529  -1.19541  -1.28426 -0.624149   1.58536  -1.00792 0.0965457  0.425083 -0.584181  0.470591(NM) 1180: f = 749.67 at  0.396825 -0.392629   1.26628  -0.23151  -1.19551  -1.28419 -0.624008   1.58565  -1.00813 0.0962724  0.425088 -0.584545  0.470113(NM) 1200: f = 749.67 at  0.396825 -0.392629   1.26628  -0.23151  -1.19551  -1.28419 -0.624008   1.58565  -1.00813 0.0962724  0.425088 -0.584545  0.470113(NM) 1220: f = 749.67 at  0.396817 -0.392608   1.26629 -0.231538  -1.19549  -1.28418 -0.624036   1.58555    -1.008 0.0965452  0.424986 -0.584406   0.47019(NM) 1240: f = 749.67 at  0.396804 -0.392681   1.26634  -0.23157  -1.19547  -1.28408 -0.623942    1.5857  -1.00806  0.096697  0.425128 -0.584691   0.47001(NM) 1260: f = 749.67 at  0.396802  -0.39264   1.26626 -0.231461  -1.19547  -1.28412 -0.623967   1.58558   -1.0079 0.0964204  0.425118 -0.584497  0.470173(NM) 1280: f = 749.67 at  0.396807 -0.392638   1.26625 -0.231381   -1.1955   -1.2841 -0.623893    1.5856  -1.00785 0.0962084  0.425024  -0.58459  0.470034(NM) 1300: f = 749.67 at  0.396771 -0.392722   1.26622 -0.231369  -1.19554  -1.28403 -0.623729   1.58583  -1.00779 0.0961983  0.425128 -0.584769  0.469716(NM) 1320: f = 749.67 at  0.396768 -0.392784   1.26617 -0.231366  -1.19562  -1.28404 -0.623649    1.5859  -1.00762 0.0963004  0.425202 -0.584636  0.469723(NM) 1340: f = 749.67 at  0.396767 -0.392806   1.26622 -0.231389   -1.1956  -1.28401 -0.623607   1.58579  -1.00759 0.0965361  0.425325 -0.584578  0.469763(NM) 1360: f = 749.67 at  0.396866 -0.392762   1.26624  -0.23137  -1.19555  -1.28417 -0.623728   1.58533  -1.00711 0.0970542  0.425119 -0.584159  0.470329(NM) 1380: f = 749.67 at  0.396826  -0.39282    1.2661 -0.231337  -1.19567  -1.28435 -0.623529    1.5855  -1.00691 0.0969928  0.425347 -0.583752  0.470006(NM) 1400: f = 749.67 at  0.396828 -0.392809   1.26587 -0.230523  -1.19567  -1.28454 -0.623204   1.58518  -1.00592 0.0958208  0.425714 -0.583203  0.469847(NM) 1420: f = 749.67 at   0.39682 -0.392772   1.26583 -0.231355  -1.19569  -1.28484 -0.623301   1.58509  -1.00597  0.097786  0.425473 -0.582301  0.469858(NM) 1440: f = 749.67 at  0.396817 -0.392632   1.26561 -0.230757   -1.1958  -1.28514 -0.623166   1.58496  -1.00593 0.0960783  0.425831 -0.581773  0.469414(NM) 1460: f = 749.67 at  0.396878 -0.392651   1.26552 -0.231005  -1.19575   -1.2856 -0.622971   1.58465  -1.00551 0.0972454  0.426505 -0.580689  0.469065(NM) 1480: f = 749.67 at  0.396877  -0.39262   1.26542 -0.230338  -1.19594  -1.28565 -0.623137     1.585  -1.00615 0.0947591  0.427144 -0.580886  0.469078(NM) 1500: f = 749.67 at  0.396912 -0.392667   1.26537 -0.230245  -1.19599  -1.28571 -0.623118   1.58511  -1.00615 0.0944749  0.427491  -0.58083  0.468978(NM) 1520: f = 749.67 at  0.396907 -0.392819   1.26548 -0.230669  -1.19601  -1.28547 -0.623182   1.58553  -1.00624  0.095432  0.427372 -0.581363  0.469012(NM) 1540: f = 749.67 at  0.396907 -0.392819   1.26548 -0.230669  -1.19601  -1.28547 -0.623182   1.58553  -1.00624  0.095432  0.427372 -0.581363  0.469012(NM) 1560: f = 749.67 at  0.396846 -0.392872   1.26549 -0.230975  -1.19603  -1.28571 -0.623079   1.58562  -1.00611 0.0965022  0.427656 -0.580792  0.468764(NM) 1580: f = 749.67 at  0.396871 -0.392731   1.26548 -0.230966  -1.19629  -1.28622 -0.623358   1.58536  -1.00663 0.0961115  0.428155 -0.579597   0.46913(NM) 1600: f = 749.67 at   0.39686  -0.39284   1.26563 -0.231135  -1.19639  -1.28626 -0.623407    1.5856  -1.00688 0.0965733  0.428763 -0.579568  0.469227(NM) 1620: f = 749.67 at  0.396932 -0.392808   1.26574 -0.231183  -1.19665  -1.28668 -0.623765   1.58549  -1.00711 0.0967714  0.429543 -0.578619  0.469839(NM) 1640: f = 749.67 at  0.396889 -0.392691   1.26572 -0.230796  -1.19694  -1.28691 -0.623865    1.5856  -1.00733 0.0955276  0.429535 -0.578312  0.469878(NM) 1660: f = 749.67 at  0.396886 -0.392516   1.26562 -0.230521  -1.19706  -1.28715 -0.623744   1.58529  -1.00658 0.0953614  0.429132  -0.57776  0.469805(NM) 1680: f = 749.67 at  0.396993 -0.392586   1.26569 -0.230802   -1.1969  -1.28694 -0.623985   1.58509  -1.00626 0.0965312  0.429345 -0.577799  0.470392(NM) 1700: f = 749.67 at  0.396936 -0.392258   1.26567 -0.230714  -1.19731  -1.28786 -0.624302   1.58469  -1.00631 0.0965168  0.429975 -0.575631  0.470725(NM) 1720: f = 749.67 at  0.396981 -0.392454   1.26553 -0.230937  -1.19695  -1.28732 -0.623964   1.58477  -1.00581 0.0972308  0.429443  -0.57663  0.470304(NM) 1740: f = 749.67 at  0.396973 -0.392387   1.26563 -0.230911  -1.19706  -1.28728 -0.624211   1.58489  -1.00612 0.0969993  0.429718 -0.576671  0.470485(NM) 1760: f = 749.67 at  0.396943 -0.392263   1.26568 -0.230933  -1.19699  -1.28745 -0.624264   1.58456  -1.00628 0.0970891   0.42972 -0.576271  0.470648(NM) 1780: f = 749.67 at  0.396939 -0.392186   1.26572 -0.230985    -1.197  -1.28738 -0.624367   1.58447  -1.00643 0.0970968  0.429719 -0.576269  0.470732(NM) 1800: f = 749.67 at  0.396948 -0.392171   1.26577 -0.230764   -1.1971  -1.28732  -0.62441   1.58452  -1.00673  0.096286  0.429841 -0.576488  0.470805(NM) 1820: f = 749.67 at  0.396971 -0.392141    1.2659 -0.230947  -1.19698  -1.28718 -0.624475   1.58422  -1.00674 0.0969716  0.429722 -0.576586  0.471092(NM) 1840: f = 749.67 at  0.396945 -0.392139   1.26597 -0.230962  -1.19701  -1.28707  -0.62448   1.58437  -1.00708 0.0966966   0.42988 -0.576925  0.471011(NM) 1860: f = 749.67 at  0.396972 -0.392213   1.26593 -0.231074  -1.19723  -1.28721 -0.624402   1.58453  -1.00711 0.0968097  0.430196  -0.57657   0.47081(NM) 1880: f = 749.67 at  0.397024 -0.392183   1.26595 -0.231329  -1.19731  -1.28733 -0.624461   1.58427  -1.00698  0.097675  0.430796 -0.575895  0.470995(NM) 1900: f = 749.67 at  0.396991 -0.392182     1.266 -0.231467  -1.19731  -1.28704 -0.624205   1.58434  -1.00729 0.0974862  0.430715 -0.576544  0.470548(NM) 1920: f = 749.67 at  0.397039 -0.392191     1.266 -0.231734  -1.19793  -1.28731 -0.623951   1.58448   -1.0074 0.0977164  0.431918  -0.57564  0.470021(NM) 1940: f = 749.67 at  0.397027 -0.392047   1.26626 -0.231944  -1.19794  -1.28729 -0.624103   1.58408  -1.00784 0.0982291  0.432216 -0.575468  0.470392(NM) 1960: f = 749.67 at  0.397066 -0.392037   1.26612 -0.231914  -1.19802  -1.28724 -0.623767   1.58399   -1.0072 0.0983412  0.432213 -0.575423  0.469928(NM) 1980: f = 749.67 at  0.397087 -0.391862   1.26629 -0.231432  -1.19829  -1.28781 -0.623692   1.58353  -1.00703 0.0975409  0.433466 -0.574501  0.470314(NM) 2000: f = 749.67 at  0.397087 -0.391862   1.26629 -0.231432  -1.19829  -1.28781 -0.623692   1.58353  -1.00703 0.0975409  0.433466 -0.574501  0.470314(NM) 2020: f = 749.67 at  0.397087 -0.391862   1.26629 -0.231432  -1.19829  -1.28781 -0.623692   1.58353  -1.00703 0.0975409  0.433466 -0.574501  0.470314(NM) 2040: f = 749.67 at  0.397077 -0.391939   1.26646 -0.231486  -1.19841  -1.28789 -0.623662   1.58361  -1.00741 0.0975635  0.433883 -0.574515  0.470425(NM) 2060: f = 749.67 at   0.39712 -0.391966   1.26646 -0.231625  -1.19838  -1.28778  -0.62354   1.58344  -1.00716 0.0980888  0.434002 -0.574528  0.470418(NM) 2080: f = 749.67 at   0.39712 -0.391966   1.26646 -0.231625  -1.19838  -1.28778  -0.62354   1.58344  -1.00716 0.0980888  0.434002 -0.574528  0.470418(NM) 2100: f = 749.67 at  0.397107 -0.391912   1.26662  -0.23165  -1.19843  -1.28786  -0.62364   1.58331  -1.00743 0.0981731  0.434137 -0.574402  0.470663(NM) 2120: f = 749.67 at  0.397106 -0.391911    1.2666 -0.231627  -1.19847  -1.28781 -0.623645   1.58341  -1.00742 0.0980212  0.434218 -0.574503  0.470601(NM) 2140: f = 749.67 at  0.397119  -0.39194   1.26659 -0.231646  -1.19842  -1.28776 -0.623553   1.58336   -1.0073 0.0981474  0.434229 -0.574597  0.470556(NM) 2160: f = 749.67 at  0.397123 -0.391945    1.2666 -0.231625  -1.19835  -1.28768 -0.623633   1.58336  -1.00732 0.0981253  0.434178 -0.574749  0.470696","Creater_id":121418,"Start_date":"2016-07-28 12:29:58","Question_id":226171,"Tags":["lme4","glmm"],"Answer_count":1,"Last_activity":"2016-07-29 13:56:40","Link":"http://stats.stackexchange.com/questions/226171/convergence-warning-in-lme4-not-in-glmmadmb","Creator_reputation":13}
{"_id":{"$oid":"5837a58aa05283111e4d701e"},"View_count":10,"Display_name":"Norman Simon","Question_score":0,"Question_content":"I've read that Randomised Controlled Trials (RCTs) give unbiased estimates of the effect when comparing a treatment group with a control group, but those estimates are the expected effects, that is, mean effects. For example, I could say that the effect of a treatment of iron sachets in African kids was an improvement in school tests of 10% on average.My question is: Can't we find the distribution of the effects and consider it unbiased? For instance, to know that the effect of the iron sachets had a symmetrical distribution (even maybe a normal distribution?) so that we could tell the policymaker what proportion of people is estimated to have an effect size of, say, at least 5%?Please go easy on me, I'm not a statistician, and I'm not an expert in making questions here. Thanks for your help.","Creater_id":86081,"Start_date":"2016-07-29 13:01:53","Question_id":226355,"Tags":["distributions","effect-size","randomization"],"Answer_count":0,"Last_activity":"2016-07-29 13:01:53","Link":"http://stats.stackexchange.com/questions/226355/can-we-estimate-the-shape-of-the-distribution-of-the-effect-in-a-randomised-cont","Creator_reputation":85}
{"_id":{"$oid":"5837a58aa05283111e4d7020"},"View_count":36,"Display_name":"RegalPlatypus","Question_score":0,"Question_content":"I have a model of the form:model \u0026lt;- glmer(binary ~ continuous*categorical + (1|random), data=my.data, family=binomial)I know that \"continuous\" and \"categorical\" are multicollinear, in fact, it's of interest later in the analysis that they are. \"Continuous\" is slightly significantly dependent on the levels of \"categorical.\"  However, it's also of interest whether there's a significant interaction between them, i.e., whether \"binary\" varies between levels of \"categorical\" at a given level of \"continuous.\"My instinct is to first reduce multicollinearity by centering \"continuous\" and testing the significance of the interaction with a likelihood ratio test:anova(model, update(model . ~ continuous + categorical + (1|random), test=\"Chisq\")Then, perform a separate test for multicollinearity:model2 \u0026lt;- glmer(continuous ~ categorical + (1|random), data=my.data, family=binomial)anova(model2, update(model2 . ~ 1 + (1|random)), test=\"Chisq\")Is this reasonable, or am I making a mess of everything?","Creater_id":121418,"Start_date":"2016-07-29 12:56:23","Question_id":226354,"Tags":["interaction","multicollinearity","glmm"],"Answer_count":0,"Last_activity":"2016-07-29 12:56:23","Link":"http://stats.stackexchange.com/questions/226354/interaction-between-two-multicollinear-predictor-variables","Creator_reputation":13}
{"_id":{"$oid":"5837a58aa05283111e4d7022"},"View_count":56,"Display_name":"Layla","Question_score":0,"Question_content":"I am planning to use a neural network for prediction. For example, to predict whether a student will pass a course based on his previous academic records or characteristics. I was wondering how to choose meaningful data as input for my neural network, as opposed to irrelevant data.I remember reading that it's better if the input data is not strongly correlated. Is that true?","Creater_id":69395,"Start_date":"2016-07-28 21:44:14","Question_id":226218,"Tags":["machine-learning","neural-networks","predictive-models","feature-selection"],"Answer_count":1,"Last_activity":"2016-07-29 12:54:17","Link":"http://stats.stackexchange.com/questions/226218/what-characteristics-should-the-input-data-have-for-a-neural-network","Creator_reputation":105}
{"_id":{"$oid":"5837a58aa05283111e4d702f"},"View_count":1394,"Display_name":"user35169","Question_score":4,"Question_content":"I am trying to run an econometric panel data (fixed effects) model with about 4000 observations (so not a small dataset). My data consists of transactions (buy and sell). The transactions are linked to a rank (1 to 5) based on the B/M ratio for the company related to the transaction. However, I am having enormous amount of trouble with the following aspect: According to the kdensity plot and the qnorm plot, the residuals does not pass the test of normality. What does this implicate? Are the p-values not to be trusted? What are the minimum requirements to a OLS regression run on panel data? ","Creater_id":35169,"Start_date":"2013-11-22 02:56:45","Question_id":77332,"Tags":["regression","panel-data","normality"],"Answer_count":2,"Last_activity":"2016-07-29 12:54:00","Link":"http://stats.stackexchange.com/questions/77332/panel-data-ols-assumptions","Creator_reputation":21}
{"_id":{"$oid":"5837a58aa05283111e4d703d"},"View_count":11780,"Display_name":"Kochede","Question_score":30,"Question_content":"Given all good properties of state-space models and KF, I wonder - what are disadvantages of state-space modelling and using Kalman Filter (or EKF, UKF or particle filter) for estimation? Over let's say conventional methodologies like ARIMA, VAR or ad-hoc/heuristic methods.Are they hard to calibrate? Are they complicated and hard to see how a change in a model's structure will affect predictions?Or, put another way - what are advantages of conventional ARIMA, VAR over state-space models?I can think only of advantages of a state-space model: It easily handles structural breaks, shifts, time-varying parameters of some static model - just make those parameters dynamic states of a state-space model and model will automatically adjust to any shifts in parameters;It handles missing data very naturally, just do transition step of KF and don't do update step;It allows to change on-a-fly parameters of a state-space model itself (covariances of noises and transition/observation matrices) so if your current observation came from a little different source than others - you can easily incorporate it into estimation without doing anything special;Using above properties it allows easily handle irregular-spaced data: either change a model each time according to interval between observations or use regular interval and treat intervals without observations as missing data;It allows to use data from different sources simultaneously in the same model to estimate one underlying quantity;It allows to construct a model from several interpretable unobservable dynamic components and estimate them;Any ARIMA model can be represented in a state-space form, but only simple state-space models can be represented exactly in ARIMA form.","Creater_id":31774,"Start_date":"2013-12-02 03:53:24","Question_id":78287,"Tags":["time-series","arima","kalman-filter","var"],"Answer_count":5,"Last_activity":"2016-07-29 12:40:44","Link":"http://stats.stackexchange.com/questions/78287/what-are-disadvantages-of-state-space-models-and-kalman-filter-for-time-series-m","Creator_reputation":887}
{"_id":{"$oid":"5837a58aa05283111e4d704e"},"View_count":3327,"Display_name":"Tim","Question_score":2,"Question_content":"I am confused about the difference between period prevalence and incidence rate. Following are from Wikipedia:  The incidence rate is the number of new cases per population in a  given time period.    Period prevalence is the proportion of the population with a given  disease or condition over a specific period of time.Thanks!","Creater_id":1005,"Start_date":"2013-08-08 22:30:39","Question_id":66894,"Tags":["epidemiology"],"Answer_count":2,"Last_activity":"2016-07-29 12:30:24","Link":"http://stats.stackexchange.com/questions/66894/what-is-the-difference-between-period-prevalence-and-incidence-rate","Creator_reputation":5527}
{"_id":{"$oid":"5837a58aa05283111e4d705c"},"View_count":13,"Display_name":"An old man in the sea.","Question_score":0,"Question_content":"I'm having some difficulties in understanding the definitions in the following example:How do we interpret the pmf and the likelihood? is it always 1/3? If so, then why the branching out for values of theta? Berger and Wolpert (1988) is compilation of lecture notes, and I have no access to it. Also what's the meaning of ? I've never seen this notation. This example is from 'The Bayesian Choice' by Christian Robert.Any help would be appreciated.","Creater_id":40252,"Start_date":"2016-07-29 12:22:56","Question_id":226349,"Tags":["bayesian"],"Answer_count":0,"Last_activity":"2016-07-29 12:22:56","Link":"http://stats.stackexchange.com/questions/226349/how-to-interpret-this-pmf-and-likelihood-from-the-bayesian-choice-by-christian","Creator_reputation":911}
{"_id":{"$oid":"5837a58aa05283111e4d705e"},"View_count":59,"Display_name":"oli","Question_score":0,"Question_content":"I've posted this on stack overflow a few days ago, but since nobody replied, I assume that it was the wrong forum for this kind of issue. And probably it's way to basic... but as I seriously need some help with this, I re-post it here (with some extensions) and I appreciate any comments or answers.My datset looks like this:'data.frame':   124 obs. of  28 variables:  origin                           : Factor w/ 2 levels \"F\",\"S\": 2 2 2 2 2 2 2 2 2 2 ...  species                          : Factor w/ 10 levels \"AGR\",\"BET\",\"FEX\",..: 1 1 1 1 1 1 1 1 1 1 ...  log                              : Factor w/ 6 levels \"A\",\"B\",\"C\",\"CX\",..: 2 1 3 6 5 6 5 2 5 5 ... origin, Loglife$species))HSD.test(y = glm(mass.loss.a.pct ~ origin + species + origin.spp,          data = Loglife.interaction),'origin.spp',console = T)returns this:Groups, Treatments and meansa    S.PTR   41.56 a    F.FSY   36.19 a    F.BET   34.33 a    F.QRO   31.01 ab   F.POP   29.29 abc      S.PAB   24.59 abcd     F.FEX   23.05 bcde     S.AGR   10.51 cde      F.PAB   5.92 cde      S.LKA   4.878 de   S.QRO   3.92 e    S.PME   2.33 I would assume, that R is using the formula from the best model to decide how these groups are split up, so is it already taking the effect of origin and the interaction into consideration when I am looking at only species. But the fact, that origin is listed here makes me insecure about that. Again, I don't what exactly these values mean. So just as an example to make clear what I think they mean: Origin site S and the species specific traits from PTR have different effects on the mass loss in wood. Significantly different to these are the effects from origin site S and the species specific traits from PME on the mass loss in wood (41.56 \u0026lt;-\u003e 2.33). Is that true?I am convinced I could find all the answers in books and on websites, but actually I am so tired from searching and stumbling upon all those terms which raise even more questions, and also I am running a bit out of time and can't invest much more.Thanks a lot for reading all this, and I hope there's somebody willing to help me out :) ","Creater_id":124534,"Start_date":"2016-07-29 10:12:09","Question_id":226337,"Tags":["generalized-linear-model","interaction","intercept"],"Answer_count":1,"Last_activity":"2016-07-29 12:22:08","Link":"http://stats.stackexchange.com/questions/226337/interpretation-of-glm-and-hsd-test-output","Creator_reputation":1}
{"_id":{"$oid":"5837a58aa05283111e4d706a"},"View_count":106,"Display_name":"Kal","Question_score":0,"Question_content":"I need to compute a seasonal ARIMA model and make forecasts using Kalman filter.I do not understand how to feed the output of SARIMA((p,d,q)(P,D,Q)s) to Kalman filter.Also, there are many R packages such as \"dlm\", \"Stats\", \"FKF\", etc. Which one should I use? I know how to use SARIMA like this: k \u0026lt;- arima(data, order = c(2, 1, 2),      seasonal = list(order = c(1, 1, 1), period = 12),      xreg = NULL, include.mean = T ) But, I do not understand how to use Kalman filter accurately. Is the following is correct? result \u0026lt;- KalmanForecast(\u0026lt;ahead\u0026gt;, k$model)Here, the issue is how the Kalman filter can handle the seasonality?","Creater_id":61538,"Start_date":"2016-07-29 06:22:04","Question_id":226286,"Tags":["r","forecasting","arima","kalman-filter"],"Answer_count":0,"Last_activity":"2016-07-29 12:03:40","Link":"http://stats.stackexchange.com/questions/226286/how-to-add-kalman-filter-forecast-to-seasonal-arima-model-in-r","Creator_reputation":13}
{"_id":{"$oid":"5837a58aa05283111e4d706c"},"View_count":56,"Display_name":"birdnerd_j","Question_score":0,"Question_content":"I am having difficulty figuring out how to calculate a dispersion parameter to calculate QAICc for a GLMM with a binomial fit.  I have tested for overdispersion using this code: overdisp_fun \u0026lt;- function(model) {  ## number of variance parameters in   ##   an n-by-n variance-covariance matrix  vpars \u0026lt;- function(m) {  nrow(m)*(nrow(m)+1)/2  }  model.df \u0026lt;- sum(sapply(VarCorr(model),vpars))+length(fixef(model))  rdf \u0026lt;- nrow(model.frame(model))-model.df  rp \u0026lt;- residuals(model,type=\"pearson\")  Pearson.chisq \u0026lt;- sum(rp^2)  prat \u0026lt;- Pearson.chisq/rdf  pval \u0026lt;- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)}With this code, I have found some of my candidate models show signs of overdispersion, while some do not. I have tried QAICc in MuMin, but I am having difficulty figuring out how to calculate c-hat properly. Could anyone point me in the right direction? Also, using just AICc, I found that I have two candidate models, one that shows signs of overdispersion and one that does not. Therefore, how does one average candidate models if one should be assessed by QAICc and the other AICc? ","Creater_id":119847,"Start_date":"2016-07-29 08:51:33","Question_id":226322,"Tags":["lme4","glmer","overdispersion","model-averaging","quasi-binomial"],"Answer_count":1,"Last_activity":"2016-07-29 11:53:19","Link":"http://stats.stackexchange.com/questions/226322/calculating-qaicc-and-averaging-glmm-models-with-various-overdispersion","Creator_reputation":8}
{"_id":{"$oid":"5837a58aa05283111e4d7079"},"View_count":135,"Display_name":"Davide","Question_score":1,"Question_content":"I have a panel data in the following form:\u0026lt;id\u0026gt; \u0026lt;city\u0026gt; \u0026lt;treated\u0026gt; \u0026lt;time\u0026gt; \u0026lt;after\u0026gt;where id identifies the individuals in my panel, city is the location where the individual live (non-time varying), treated is a dummy indicating those individual that are eventually treated (0: non-treated, 1: treated), time is a year-month variable, and after is a dummy (0: before, 1: after) indicating the period in which the treated unit are under treatment.With this data, I am running a simple difference-in-differences model. I am including individual fixed effects and year-month fixed effects. To relax the parallel assumption, I include a treatment-city specific time trend -- people often include treatment specific trends, but in my settings the outcome can vary a lot across cities for treatment and control units -- so my specification should be:xtset id timextreg depvar i.treated i.after c.treated#c.after i.time i.city#c.treated#c.time, fe cluster(id)wherei.city#c.treated#c.timeis the treatment-city specific linear trend.Generally, in this kind of models I use to include only a treatment-specific trend, and not a 3-way interaction. The first question is whether this approach makes sense.Second, I thought that adding: i.city#c.treated#c.time or i.city#i.treated#c.timewas exactly the same (note that the variable treated in coded as 0/1), but apparently is not. Can someone explain me the statistical difference between including one term or the other in my regression?Morever, some tests I did suggest that adding i.treated#c.time or c.treated#c.timeis the same. Why with a 2-way interaction, treating the variable treated as continuous or as a factor does not matter? P.S. Thanks to Dimitriy for the current answer! ","Creater_id":124287,"Start_date":"2016-07-23 17:14:14","Question_id":225312,"Tags":["stata","difference-in-difference"],"Answer_count":1,"Last_activity":"2016-07-29 11:24:00","Link":"http://stats.stackexchange.com/questions/225312/3-way-interaction-stata","Creator_reputation":20}
{"_id":{"$oid":"5837a58aa05283111e4d7086"},"View_count":64,"Display_name":"Levi Thatcher","Question_score":1,"Question_content":"Overall, I'd like to be able to say that, for the logistic prediction for this row, ColA was more influential in driving up the resultant probability (ie, y_hat) than ColB. (We'll use y_hat as it's usually defined for logistic.) But is this possible? Some data scientists I've talked to say yes, but I've also seen push-back.From what I've read, it seems that GLMs make it easiest to get at a per-row variable importance (see this limited discussion on logit in particular, including push-back). But can they actually do it?If B1 and B2 are coefficients and the cols in X represent our features, it would seem that if B1*X1 is greater than B2*X2 then B1*X1 would drive the resultant probability towards 1 more than B2*X2. Here's an example (which brings in a factor col, for a full treatment).We create features X1 and X2, where X1 is random and X2 (I think we can agree) has a large positive impact on y:set.seed(33)X1 \u0026lt;- runif(10, 0.0, 1.0)X2 \u0026lt;- c(1,0,1,0,1,0,1,0,1,0)y \u0026lt;-  c(1,1,1,0,1,0,1,0,1,0)df \u0026lt;- data.frame(X1,X2,y)dforig \u0026lt;- df #Need a copy bc multiplying below doesn't work with factorsdfX2)Now we create the logit model:fit.logit = glm(formula = y~.,data = df,family = binomial(link = \"logit\"))                         X1          X21  Coefficients:       -1.2353      22.0041Wald statistic:      -0.267        0.003Now if we multiply B1 and B2 by X1 and X2 respectively and print the results:coefftemp \u0026lt;- fit.logitX2 \u0026lt;- as.factor(dfcoefficientscoefficients \u0026lt;- coefftemp[2:length(coefftemp)] # drop interceptmultiply_res \u0026lt;- sweep(dforig[,1:2], 2, coefficients, `*`)multiply_res            X1        X21  -0.55087679   0.000002  -0.48751729 -22.004113  -0.59755734   0.000004  -1.13510089 -22.004115  -1.04245907   0.000006  -0.63908954 -22.004117  -0.53998690   0.000008  -0.42395777 -22.004119  -0.01916833   0.0000010 -0.14575621 -22.00411Overall, for logistic, can we accurately say (for example) that feature A drives y_hat toward 1 more than feature B, for this individual prediction? Thanks, all!","Creater_id":124897,"Start_date":"2016-07-29 09:37:06","Question_id":226330,"Tags":["r","logistic","generalized-linear-model","interpretation","importance"],"Answer_count":1,"Last_activity":"2016-07-29 11:20:30","Link":"http://stats.stackexchange.com/questions/226330/logistic-model-variable-importance-on-a-per-row-basis","Creator_reputation":6}
{"_id":{"$oid":"5837a58aa05283111e4d7093"},"View_count":35,"Display_name":"yuxu zi","Question_score":2,"Question_content":"The estimates by function Arima from the \"forecast\" package in R are as follows:ARIMA(1,0,0) with non-zero mean   Coefficients:             ar1   intercept  SEASONAL.. 1MO_LIBOR... GDP_GOODS... CORP...                           0.3950   0.0464    -0.0783    -0.0220       1.8730       0.0679     s.e.  0.1463   0.0068     0.0083     0.0115       0.8527       0.0323 Meanwhile, the EViews estimates areWhy do the standard error differ in EViews vs. R even though the coefficients coincide?By the way, could any body help me to calculate the -values of the coefficients? Is this function correct?P_Value \u0026lt;- function(fit){  if(inherits(fit,\"lm\")){    res=summary(fit)xreg)    n=nobs(fit)    df=nobs(fit)-length(fitcoef)/sqrt(diag(fit$var.coef)*n/df),df))*2  }  return (res[vars])}","Creater_id":125000,"Start_date":"2016-07-29 09:12:52","Question_id":226323,"Tags":["r","arima","standard-error","eviews"],"Answer_count":0,"Last_activity":"2016-07-29 11:16:34","Link":"http://stats.stackexchange.com/questions/226323/why-do-arimax-standard-error-differ-in-eviews-vs-r-while-coefficients-coincide","Creator_reputation":11}
{"_id":{"$oid":"5837a58aa05283111e4d7095"},"View_count":2052,"Display_name":"user2305193","Question_score":18,"Question_content":"This may be a simple question for many but here it is: Why isn't variance defined as the difference between every value following each other instead of the difference to the average of the values? This would be the more logical choice to me, I guess I'm obviously overseeing some disadvantages. ThanksEDIT:Let me rephrase as clearly as possible. This is what I mean:Assume you have a range of numbers, ordered: 1,2,3,4,5Calculate and sum up (the absolute) differences (continuously, between every following value, not pairwise) between values (without using the average).Divide by number of differences(Follow-up: would the answer be different if the numbers were un-ordered)-\u003e What are the disadvantages of this approach compared to the standard formula for variance?","Creater_id":107356,"Start_date":"2016-07-26 09:46:29","Question_id":225734,"Tags":["variance"],"Answer_count":8,"Last_activity":"2016-07-29 11:07:13","Link":"http://stats.stackexchange.com/questions/225734/why-isnt-variance-defined-as-the-difference-between-every-value-following-each","Creator_reputation":93}
{"_id":{"$oid":"5837a58aa05283111e4d70a9"},"View_count":79,"Display_name":"Ben","Question_score":2,"Question_content":"Suppose a student sits for an exam and in a question, there are 5 choices where one is correct. There is also a 1/4 chance that the student may have seen the answer to the question previously. If the student randomly selects an answer for the question if he hadn't seen the answer and selects the correct one if he had seen the answer, what is the probability that the student selects the correct answer? What I think is that this is a conditional probability whereby. the probability of selecting the correct answer is the probability of selecting the correct answer given he had seen the answer or probability of selecting the correct one given he hadn't seen the answer. However, adding the two probabilities gives me a value greater than one. This is because, to me, the probability that he selects the correct answer given he had seen the answer is 1. In what other way can I approach this question?","Creater_id":124985,"Start_date":"2016-07-29 07:13:18","Question_id":226300,"Tags":["conditional-probability"],"Answer_count":1,"Last_activity":"2016-07-29 10:54:29","Link":"http://stats.stackexchange.com/questions/226300/probability-of-selecting-a-correct-answer-given-that-there-is-a-chance-the-stude","Creator_reputation":18}
{"_id":{"$oid":"5837a58aa05283111e4d70b6"},"View_count":80,"Display_name":"Greg Ver Steeg","Question_score":2,"Question_content":"Suppose that  are some random variables. I'd like to do multiple linear regression to learn to predict any of these variables from the others. My model for the reconstructed variables is the following. X_i = \\sum_{j=1}^n w_{i,j} X_i + \\epsilon_i, \\forall i=1,\\ldots,n A simple thing to do would be to tune the weights, , to minimize the squared error between the reconstruction and the original. Obviously, there is a perfect but trivial solution! We have to enforce that  so that we can't predict  from itself. After imposing this constraint, we have what I'd call a self-regression problem. Has this simple model been studied and can anyone point me in the right direction? A related problem would be to use dimensionality reduction to carry out this reconstruction. E.g., we could use an autoencoder with linear encoder and decoder, or simply use PCA. One problem with this point of view is that it is not completely straightforward to predict variable  from the other variables . Why not? Because the PCA vectors depend on  in the first place. Again, I believe this context must be well-studied. We are just doing linear dimensionality reduction with missing variables and we want to know how well we can reconstruct the missing variables. Has this situation been studied?","Creater_id":88727,"Start_date":"2016-06-29 16:31:45","Question_id":221348,"Tags":["regression","multiple-regression","pca","linear-model","dimensionality-reduction"],"Answer_count":2,"Last_activity":"2016-07-29 10:32:49","Link":"http://stats.stackexchange.com/questions/221348/linear-self-regression-terminology-and-references","Creator_reputation":336}
{"_id":{"$oid":"5837a58aa05283111e4d70c4"},"View_count":32,"Display_name":"Annamarie","Question_score":0,"Question_content":"I have a continuous variable which is skewed (possibly because of a ceiling effect) in a very large sample. My colleague argues that due to the central limit theorem we can treat this as normally distributed. It is data from a population study with over 1000 subjects. However, if some of the variables indeed have ceiling effects, does the central limit theorem apply? The means would be normally distributed, but they would not be the real means for this cohort.","Creater_id":73581,"Start_date":"2016-07-29 09:42:31","Question_id":226333,"Tags":["central-limit-theorem"],"Answer_count":1,"Last_activity":"2016-07-29 10:07:57","Link":"http://stats.stackexchange.com/questions/226333/central-limit-theorem-ceiling-effect","Creator_reputation":11}
{"_id":{"$oid":"5837a58aa05283111e4d70d1"},"View_count":70,"Display_name":"milowang","Question_score":-1,"Question_content":"(This is taken from a class.) I was given a generalized exponential distribution: . As follows and calculate the expected value of the distribution:\\begin{eqnarray}f(t) \u0026amp;=\u0026amp; \\color{red}{\\alpha}\\frac{_1}{^\\beta}e^{-\\frac{t}{\\beta}}+\\color{red}{C}\\\\ \u0026amp;=\u0026amp; (3.34\\times 10^{-1})\\cdot(2.172\\times 10^{-3})\\cdot \\,e^{-2.172\\times 10^{-3}\\,t}\\,+\\:\\text{(negligible)}\\\\\\mathbb{E}[t]\u0026amp;\\approx\u0026amp; 153.83 \\text{ seconds}\\end{eqnarray}But what confused me is the way he calculate the expected value. Here's what he do in the python code:  # define fit functiondef fitFunc_gen(t, a, b, c):    return a*(b)*numpy.exp(-b*t)+c# find fit parameters of a,b,cfitParams_gen, fitCov_gen = curve_fit(fitFunc_gen, division[0:len(division)-1],                                       count, p0=[0, 3e-4, 0])#expect valueev = (1/fitParams_gen[1])*fitParams_gen[0]+fitParams_gen[1]# ev= 153.8330951411821As can be seen from the code, the formula he used for expected value is: . However I did the calculation myself:\\begin{eqnarray}E(X) \u0026amp;=\u0026amp; \\int_{-\\infty}^\\infty x f(x) dx\\\\\u0026amp;\u0026amp;\\hspace{2.5cm}\\:\\downarrow\\:c\\approx 0\\\\E(X) \u0026amp;=\u0026amp; \\int_{-\\infty}^\\infty x  \\frac{\\alpha}{\\beta}\\,e^{-\\frac{x}{\\beta}} dx\\\\\u0026amp;=\u0026amp;\\frac{\\alpha}{\\beta}\\int_{0}^\\infty x \\,e^{-\\frac{x}{\\beta}} dx\\\\\u0026amp;=\u0026amp;-\\alpha\\int_{0}^\\infty x\\,(-\\frac{1}{\\beta}) \\,e^{-\\frac{x}{\\beta}} dx\\\\\u0026amp;=\u0026amp;-\\alpha\\int_{0}^\\infty x\\, de^{-\\frac{x}{\\beta}}\\\\\u0026amp;=\u0026amp;-\\alpha \\left(xe^{-\\frac{x}{\\beta}}|_{0}^\\infty  -\\int_{0}^\\infty e^{-\\frac{x}{\\beta}}dx \\right)\\\\\u0026amp;=\u0026amp;-\\alpha\\left(0+\\beta\\int_{0}^\\infty (-\\frac{1}{\\beta})e^{-\\frac{x}{\\beta}}dx\\right)\\\\\u0026amp;=\u0026amp;-\\alpha\\beta\\int_{0}^\\infty (-\\frac{1}{\\beta})e^{-\\frac{x}{\\beta}}dx\\\\\u0026amp;=\u0026amp;-\\alpha\\beta e^{-\\frac{x}{\\beta}}|_{0}^\\infty=\\alpha\\beta\\end{eqnarray}I think it should be .Can anyone correct me?","Creater_id":124917,"Start_date":"2016-07-28 18:29:26","Question_id":226205,"Tags":["self-study","expected-value","exponential"],"Answer_count":1,"Last_activity":"2016-07-29 09:49:14","Link":"http://stats.stackexchange.com/questions/226205/expected-value-of-a-generalized-exponential-distribution","Creator_reputation":1}
{"_id":{"$oid":"5837a58aa05283111e4d70de"},"View_count":38,"Display_name":"tluh","Question_score":0,"Question_content":"As I understand it, the output of a t-SNE graph has arbitrary axis directions. That is, the axis can be easily flipped by multiplying one/both of the output coordinates by -1, and the output graph can still be visually interpreted equally validly.This article, in section \"Materials and Methods\" \u003e \"PCA vs. t-SNE\", describes t-SNE as using a \"method/optimization-criteria [that] is rotation and scale-invariant\", which suggests that t-SNE outputs can also be rotated by, say, 45 degrees in either direction. This could be carried out by any algorithm for rotation about a point.Is this accurate, and if so, does it matter which point is chosen as the center of rotation of a t-SNE map?","Creater_id":83496,"Start_date":"2016-07-29 09:30:07","Question_id":226328,"Tags":["data-visualization","dimensionality-reduction","tsne"],"Answer_count":1,"Last_activity":"2016-07-29 09:41:58","Link":"http://stats.stackexchange.com/questions/226328/t-sne-output-graph-rotation-flip-transformation","Creator_reputation":28}
{"_id":{"$oid":"5837a58aa05283111e4d70eb"},"View_count":29,"Display_name":"metathor","Question_score":0,"Question_content":"I have an adjacency matrix that contains physical contact rates between different age groups in a population. This graphic is exactly the type and style of data that I have and is the standard method of presenting contact-matrix data in my field. I have several thousand iterations of these contact matrices for a hypothetical population in a non-linear model and would like to combine that data into a graphic that demonstrates both 1) the intensity of contact as is given in the graphic above via the heat map and 2) the uncertainty surrounding the contact rate for each (i,j) age group combination. I thought, perhaps, a 3d graph with 3 planes might be a simple way to display the data: the upper plane would represent the upper bound, the middle the median, and the lower plane the lower bound (basically, like a sandwich with the bread as the confidence limits). However, I'm not a statistician and do not know if there is a simple and standard way of depicting this type of data? If not, suggestions for a clean and intuitive graphic would be extremely helpful. Thank you. ","Creater_id":114016,"Start_date":"2016-07-29 09:37:53","Question_id":226331,"Tags":["confidence-interval","data-visualization","nonlinear"],"Answer_count":0,"Last_activity":"2016-07-29 09:37:53","Link":"http://stats.stackexchange.com/questions/226331/suggestions-on-graphing-adjacency-matrix-with-uncertainty-confidence-limits","Creator_reputation":18}
{"_id":{"$oid":"5837a58aa05283111e4d70ed"},"View_count":11,"Display_name":"Simone","Question_score":1,"Question_content":"I have sample A (17'123 observations) and sample B (23'136 observations) of intervall data. Visual inspection, i.e. histogram, QQplot and boxplot showed that the data is not perfectly normally distributed and has outliers. The reason why I don't use a normality test is because my samples are large.  The kurtosis values are as following:sample A = -.4786 / sample B = -.3769The data is skewed to the left:sample A = -.7669 / sample B = -.7499The calculations are based on G2 / G1 as adopted in SPSS or SAS and explained in D. N. Joanes and C. A. Gill (1998), Comparing measures of sample skewness and kurtosis. The Statistician, 47 , 183–189. The values are close to 0. In light of the big samples, does this still mean that the data is not too skewed and platykurtic? Implications on t-test results? In addition my samples are not equal. According to these posts (1 and 2) unequal sample sizes don't bias the t-test results, but reduce its power. In other samples I have gathered, sample C is more than 3 times bigger than sample D. Therefore, the statistical power is reduced markably see here. Is there any way to improve statistical power? A different test? I want to see whether the scores of group A (or C) are significantly lower than in group B(or D).I don't like to just delete data. That is somewhat hard to justify to reviewers and editors. Many thanks!!","Creater_id":115835,"Start_date":"2016-07-29 09:19:15","Question_id":226325,"Tags":["sample-size","sample","skewness","kurtosis"],"Answer_count":0,"Last_activity":"2016-07-29 09:33:00","Link":"http://stats.stackexchange.com/questions/226325/impact-of-large-slighly-unequal-samples-on-kurtosis-and-skew","Creator_reputation":21}
{"_id":{"$oid":"5837a58aa05283111e4d70ef"},"View_count":148,"Display_name":"Sanja","Question_score":4,"Question_content":"In our randomized controlled trial, we used linear mixed effects models to test differences between groups in changes from baseline to six months while adjusting for important covariates. We ran separate analyses for each outcome. We had 6 covariates that were considered, and we used a stepwise method using Akaike information criterion for selection of the best variable set. A reviewer came back to us saying that we should have addressed multiplicity and adjusted our p-values due to a possibility of inflating type I error.I am not sure if this is true or not, but I don't think that we should do this, because we did not conduct post hoc analysis. Also, in our analysis we did not have multiple levels of treatment. So, I don't really think we should adjust our p-values. The only thing that we should adjust for, I believe, is for selecting the covariates. But, that was taken care of by using iterative model selection technique, namely stepwise variable selection based on AIC (stepAIC). P.S. I searched the site for possible answers, and couldn't find any that fits. ","Creater_id":41258,"Start_date":"2016-07-27 08:55:50","Question_id":225937,"Tags":["mixed-model","multiple-comparisons","aic"],"Answer_count":1,"Last_activity":"2016-07-29 09:19:17","Link":"http://stats.stackexchange.com/questions/225937/linear-mixed-effects-model-and-multiplicity-issue-and-adjusting-for-p-values","Creator_reputation":23}
{"_id":{"$oid":"5837a58aa05283111e4d70fc"},"View_count":26,"Display_name":"kilojoules","Question_score":0,"Question_content":"For , would latin-hypercube sampling be associated with less error than random sampling? Here N is the normal distribution.","Creater_id":83526,"Start_date":"2016-07-29 09:17:57","Question_id":226324,"Tags":["sampling"],"Answer_count":0,"Last_activity":"2016-07-29 09:17:57","Link":"http://stats.stackexchange.com/questions/226324/does-lhs-sampling-have-less-error-than-random-sampling-for-zx-y-nx-sigma2","Creator_reputation":85}
{"_id":{"$oid":"5837a58aa05283111e4d70fe"},"View_count":8704,"Display_name":"kaji331","Question_score":2,"Question_content":"I am a biology student. We do many Enzyme Linked Immunosorbent Assay (ELISA) experiments and Bradford detection. A 4-parametric logistic regression (reference) is often used for regression these data following this function:F(x) = \\left(\\frac{A-D}{1+(x/C)^B}\\right) + D How can I do this in R? I want to get the , ,  and  values and plot the curve.PS. If I have some data, how can I use the calculated function  to get the value? I mean how do I go from \"data -\u003e F(x) -\u003e value\"?","Creater_id":26342,"Start_date":"2013-06-07 02:45:37","Question_id":61144,"Tags":["r","regression","logistic","biostatistics","bioinformatics"],"Answer_count":4,"Last_activity":"2016-07-29 08:43:34","Link":"http://stats.stackexchange.com/questions/61144/how-to-do-4-parametric-regression-for-elisa-data-in-r","Creator_reputation":16}
{"_id":{"$oid":"5837a58aa05283111e4d710e"},"View_count":4771,"Display_name":"qqqwww","Question_score":26,"Question_content":"I have a dataset. There are lots of missing values. For some columns, the missing value was replaced with -999, but other columns, the missing value was marked as 'NA'. Why would we use -999 to replace the missing value? ","Creater_id":96184,"Start_date":"2016-07-22 12:47:48","Question_id":225175,"Tags":["missing-data","coding"],"Answer_count":5,"Last_activity":"2016-07-29 08:28:58","Link":"http://stats.stackexchange.com/questions/225175/why-do-some-people-use-999-or-9999-to-replace-missing-values","Creator_reputation":146}
{"_id":{"$oid":"5837a58aa05283111e4d711f"},"View_count":7,"Display_name":"user3403745","Question_score":0,"Question_content":"I did an experiment where I subjected birds to two different treatments. I measured a continuous variable (physiological) and an ordinal variable (behavioural). The data are non-parametric. I did a Spearman's Rank Correlation and found a correlation between my variables with all the data pooled. I'm now interested in knowing if this correlation is just driven by the treatment groups or if it's also observed within treatment groups. Should I just subset the data and do Spearman's Rank Correlations on both to see if the correlation holds up within treatment groups or is there a more sophisticated test I can use to look at the correlations across and within treatments at the same time?","Creater_id":94946,"Start_date":"2016-07-29 08:11:24","Question_id":226315,"Tags":["correlation","nonparametric","spearman-rho"],"Answer_count":0,"Last_activity":"2016-07-29 08:11:24","Link":"http://stats.stackexchange.com/questions/226315/test-to-examine-correlation-across-treatments-and-within-treatments","Creator_reputation":20}
{"_id":{"$oid":"5837a58aa05283111e4d7121"},"View_count":20,"Display_name":"Jiang Xiang","Question_score":0,"Question_content":"As we unfold the network by time, the gradients of each time step is calculated. Are the recurrent gradients averaged or summed over the time steps in the Tensorflow implementation?","Creater_id":53144,"Start_date":"2016-07-29 08:04:12","Question_id":226314,"Tags":["deep-learning","lstm","tensorflow"],"Answer_count":0,"Last_activity":"2016-07-29 08:04:12","Link":"http://stats.stackexchange.com/questions/226314/how-do-we-compute-the-gradients-of-recurrent-connections-in-tensorflow","Creator_reputation":143}
{"_id":{"$oid":"5837a58aa05283111e4d7123"},"View_count":141,"Display_name":"Sudharsan","Question_score":0,"Question_content":"I am trying to find the variable importance on a credit scoring database.I have categorical inputs as well as numerical inputs. My question is does the random forest algorithm works the same way when I case (1) : give only categorical variables as inputscase (2): both categorical and numerical variables as inputs.By this what I am actually asking is only categorical variables are enough or numerical variables should be included along with categorical variables as inputs to the algorithm...???Thanks in advance...","Creater_id":105289,"Start_date":"2016-05-22 23:04:42","Question_id":214031,"Tags":["r","random-forest"],"Answer_count":1,"Last_activity":"2016-07-29 08:00:22","Link":"http://stats.stackexchange.com/questions/214031/are-numerical-variables-must-for-random-forest-algorithm","Creator_reputation":27}
{"_id":{"$oid":"5837a58aa05283111e4d7130"},"View_count":38,"Display_name":"user124975","Question_score":0,"Question_content":"Can a hypothesis test be performed if I have a non-normal population, small sample size, but population standard deviation is known? We are testing if the mean differs from the given mean.","Creater_id":124975,"Start_date":"2016-07-29 05:49:24","Question_id":226282,"Tags":["hypothesis-testing"],"Answer_count":2,"Last_activity":"2016-07-29 07:56:04","Link":"http://stats.stackexchange.com/questions/226282/hypothesis-testing-options-on-non-normal-populations","Creator_reputation":1}
{"_id":{"$oid":"5837a58aa05283111e4d713e"},"View_count":40,"Display_name":"michek","Question_score":2,"Question_content":"I am looking for an intuitive reasoning behind the Marchenko Pastur law, which is described as a law of large numbers analog for random matrices. I know the law gives the probability density function of eigenvalues values of a large Wishart matrix whose dimensions tend to infinity, but I am curious of the corollaries for covariance matrices of large dimensional  matrices  with correlation between columns.Let . How does MP law provide the lower  and upper bound  of the eigenvalues for  derived from an  Wishart matrix, without knowing the variance of its random variates?  Secondly, given  and , why is it that the eigenvalues of  which exceed  are considered to be the signal eigenvalues but anything below are considered noise? What do eigenvalues to the left of the interval  indicate?","Creater_id":102689,"Start_date":"2016-07-28 18:27:35","Question_id":226204,"Tags":["mathematical-statistics","random-matrix"],"Answer_count":0,"Last_activity":"2016-07-29 07:49:51","Link":"http://stats.stackexchange.com/questions/226204/intuitive-explanation-for-marchenko-pastur-law","Creator_reputation":121}
{"_id":{"$oid":"5837a58aa05283111e4d7140"},"View_count":36,"Display_name":"CMUEngineer","Question_score":1,"Question_content":"I have two distinct texture classes of 1000 images each with varying amounts of blur and noise (but with a reasonable threshold on both so that even if both are at their highest levels the class of the image is reasonably discernible to the human eye). I successfully trained a Linear Support Vector Machine on the classification task, but moving to a non-linear SVM (in sci-kit learn) or a convolutional neural network both yield no success. More specifically, they are all stuck predicting everything as the same class. Training the CNN has the same effect: accuracy is consistently stuck at 50%, despite my multiple initializations with different optimization and loss functions. For the sake of the question, I'll provide an example of each class:The class that the non-linear SVMs are favoring is the \"striped\" class.  Does anyone have an idea why this might be occurring? ","Creater_id":124469,"Start_date":"2016-07-29 07:45:44","Question_id":226308,"Tags":["machine-learning","classification","svm","conv-neural-network","computer-vision"],"Answer_count":0,"Last_activity":"2016-07-29 07:45:44","Link":"http://stats.stackexchange.com/questions/226308/why-does-my-linear-svm-on-sift-features-converge-but-not-a-non-linear-svm-or-a","Creator_reputation":11}
{"_id":{"$oid":"5837a58aa05283111e4d7142"},"View_count":19,"Display_name":"jonc","Question_score":1,"Question_content":"If I compute the cumulative binomial probability for sliding windows across a larger sequence, then is it accurate to state something on the lines of:\"The cumulative binomial probability was calculated to test if the observed frequency of XYZ in each window was significantly lower than that expected given the total number of XYZ in each sequence\" ","Creater_id":72703,"Start_date":"2016-07-29 07:45:12","Question_id":226307,"Tags":["probability","binomial","terminology"],"Answer_count":0,"Last_activity":"2016-07-29 07:45:12","Link":"http://stats.stackexchange.com/questions/226307/binomial-probability-terminology","Creator_reputation":25}
{"_id":{"$oid":"5837a58aa05283111e4d7144"},"View_count":140,"Display_name":"Paul Rougieux","Question_score":1,"Question_content":"Is there an R package or Stata command that implements panel smooth transition regression as detailed in this 2005 paper: Panel Smooth Transition Regression Models - Andrés González, Timo Teräsvirta and Dick van Dijk?There already is a question related to smooth transition regression models in time series. But the time series I want to use are rather short (30 years or less) and I would like to pool data for 40 or more countries. I'm looking for a multi-variate estimator that would allow the use of panel data.","Creater_id":68318,"Start_date":"2016-07-29 07:40:08","Question_id":226306,"Tags":["r","panel-data","stata","cointegration"],"Answer_count":0,"Last_activity":"2016-07-29 07:40:08","Link":"http://stats.stackexchange.com/questions/226306/is-there-a-way-to-estimate-panel-smooth-transition-regression-using-r-or-stata","Creator_reputation":121}
{"_id":{"$oid":"5837a58aa05283111e4d7146"},"View_count":85,"Display_name":"Andreas","Question_score":2,"Question_content":"I want to explore the effect of the average prenatal maternal stress (cortisol level, continuous measure) on offspring growth during a linear growth period (monthly body size measure, N = 17 infants, 16.7+-1.3 body size measures per infant).  So my basic model islm(BodySize ~ PrenatalStress + Age + PrenatalStress*Age)but this one does not control for repeated measurements/dependent data. In theory, I could add infant-ID as a random slope factor, but this would take away the variance between the infants which is what I´m interested in (indeed, “PrenatalStress” has only on value per infant, and R informs me that the mixed model (lme4) is nearly unidentifiably etc.).Therefore, I´m now looking for a method that controls for repeated measurements without taking away the between-subject variance. I was thinking about running a GEE with geepack or a GLS (package nlme) with infant-ID as “repeated statement” similar to SPSS but I´m not sure if this is what I need. All 3 methods bring up very different results although the direction of the estimated coefficients is identical. So the formulas are actually:GEE (geepack): geeglm(formula = BodySize ~ PrenatalStress + Age + PrenatalStress*Age, data = xdata, id = ID, corstr = \"ar1\")GLS (nlme): gls(BodySize ~ PrenatalStress + Age + PrenatalStress*Age, data=xdata,corr=corAR1(,form=~Age|ID))Thank you so very much!Andreas","Creater_id":124989,"Start_date":"2016-07-29 07:35:35","Question_id":226305,"Tags":["mixed-model","repeated-measures","gee","generalized-least-squares","marginal-model"],"Answer_count":0,"Last_activity":"2016-07-29 07:35:35","Link":"http://stats.stackexchange.com/questions/226305/how-to-control-for-repeated-measurements-without-controlling-for-between-cluster","Creator_reputation":11}
{"_id":{"$oid":"5837a58aa05283111e4d7148"},"View_count":23,"Display_name":"Sandro","Question_score":2,"Question_content":"we are trying to throw up warnings for when customers buying behavior might have changed. Our current indicator is the number of purchases in a given time period and we believe that this process satisfies the condition for assuming a Poisson distribution. The idea is that when the likelihood for the exact number of purchases (probability mass function) in the current timeframe is lower than x%, we throw a warning and somebody should take a look at this.If the number of purchases gets too large and technical limitations make it impossible to calculate the Poisson distribution, we are estimating it with a normal distribution with SD = sqrt(number of purchases in current timeframe).We are getting results though, which seem intuitively wrong. Example: 229 purchases last year, 225 this year. Intuitively I would say, that this is close enough to discard as normal fluctuation. But the likelihood for this result is (appr. with normal dist.) just 2,55%.Questions:Am I right in thinking that the cumulative distribution function would be more appropriate to use? If so, why? My line of thinking would be that we have one result and we want the likelihood for that result not the likelihood of that result and all that are worse.Thanks! :-)","Creater_id":62179,"Start_date":"2016-07-29 07:12:07","Question_id":226298,"Tags":["p-value","poisson"],"Answer_count":1,"Last_activity":"2016-07-29 07:24:05","Link":"http://stats.stackexchange.com/questions/226298/finding-unloyal-customers-with-a-poisson-distribution","Creator_reputation":23}
{"_id":{"$oid":"5837a58aa05283111e4d7155"},"View_count":22,"Display_name":"Emilio Calvano","Question_score":0,"Question_content":"Consider a binary state of the world {high,low} with  prior probability that the state is high. Bayesian updating implies that, given a random variable X whose conditional distribution depending on the state, the expected posterior is equal to the prior: or, equivalently, I conjecture that bayesian updating also implies that the expected posterior conditional on knowing the state being low should be lower than the prior:Although it seems very intuitive I haven't been able to show it. any clues?","Creater_id":35124,"Start_date":"2016-07-29 07:13:14","Question_id":226299,"Tags":["bayesian"],"Answer_count":0,"Last_activity":"2016-07-29 07:13:14","Link":"http://stats.stackexchange.com/questions/226299/bayesian-updating-expected-conditional-posterior-lower-than-the-prior","Creator_reputation":33}
{"_id":{"$oid":"5837a58aa05283111e4d7157"},"View_count":24,"Display_name":"Theonlyone","Question_score":0,"Question_content":"Since I didn't found a question close enough to my problem I would very much appreciate your help on the following. (Due to reasons I changed the subject)Let's say I want to make a prediction on the number of crimes committed per year. I have data for the last 12 years. The problem is now, that since I have for the crimes and some of the independent variables (e.g. average temperature) the monthly values and hence would like to do a monthly model, for others I have only one value per year (e.g. population).First I simply interpolated*  the yearly ones and did normal regression. But then I started thinking: there are rules of thumb how many times more data you need than variables (I found the numbers 10 to 30 times more and various formulas). I know they are only rules of thumb but let's assume for a moment they are not completely wrong: I have 12 years, i.e. 144 months. Does this mean I should only include one of the yearly variables (since I have only 12 years), but can include several of the monthly ones, or can I \"trust\" the interpolation (see *) and use them as if they would be also monthly? (Or do I have to take a completely different approach?)The main reason I ask is that I want to avoid that at the end, someone reads what I have done and simply says \"obviously unscientific and hence wrong\"._* I know this is usually not a good idea, but here they are all rather \"stable\" ones like the population (assume it's not a touristic place with huge seasonal changes).","Creater_id":124977,"Start_date":"2016-07-29 06:33:31","Question_id":226288,"Tags":["regression","prediction"],"Answer_count":1,"Last_activity":"2016-07-29 06:51:21","Link":"http://stats.stackexchange.com/questions/226288/number-of-variables-if-some-are-interpolated","Creator_reputation":3}
{"_id":{"$oid":"5837a58aa05283111e4d7164"},"View_count":66,"Display_name":"Wonne","Question_score":1,"Question_content":"Using the Easyfit add-in on excel I can calculate the Kolmogorov Smirnov and the Anderson Darling statistics. But is it correct to compare those value?","Creater_id":124982,"Start_date":"2016-07-29 06:51:20","Question_id":226292,"Tags":["normal-distribution","poisson","goodness-of-fit","continuous-data","discrete-data"],"Answer_count":0,"Last_activity":"2016-07-29 06:51:20","Link":"http://stats.stackexchange.com/questions/226292/is-there-a-goodness-of-fit-test-that-i-can-use-to-check-if-my-data-best-fits-a-n","Creator_reputation":6}
{"_id":{"$oid":"5837a58aa05283111e4d7166"},"View_count":10240,"Display_name":"Ram Ahluwalia","Question_score":16,"Question_content":"Are there any measures of similarity or distance between two symmetric covariance matrices (both having the same dimensions)?I am thinking here of analogues to KL divergence of two probability distributions or the Euclidean distance between vectors except applied to matrices. I imagine there would be quite a few similarity measurements.Ideally I would also like to test the null hypothesis that two covariance matrices are identical.","Creater_id":8101,"Start_date":"2011-08-22 19:40:04","Question_id":14673,"Tags":["distributions","hypothesis-testing","covariance-matrix","kullback-leibler","information-theory"],"Answer_count":4,"Last_activity":"2016-07-29 06:48:39","Link":"http://stats.stackexchange.com/questions/14673/measures-of-similarity-or-distance-between-two-covariance-matrices","Creator_reputation":1123}
{"_id":{"$oid":"5837a58aa05283111e4d7176"},"View_count":30,"Display_name":"maple","Question_score":3,"Question_content":"I my opinion, EM algorithm is used to estimate the parameters of some complex log likelihood function. Because sometimes, it's hard to get the derivative, we can use EM algorithm. But if we have some auto differentiate tools such as theano, we can compute the derivative automatically. Can these tools replace EM algorithm?","Creater_id":62208,"Start_date":"2016-07-29 02:17:34","Question_id":226241,"Tags":["expectation-maximization","derivative","theano"],"Answer_count":1,"Last_activity":"2016-07-29 06:40:04","Link":"http://stats.stackexchange.com/questions/226241/if-we-have-auto-differentiate-tool-do-we-also-need-em-algorithm","Creator_reputation":136}
{"_id":{"$oid":"5837a58aa05283111e4d7183"},"View_count":50,"Display_name":"CoffeeSurfer","Question_score":2,"Question_content":"I am trying to compare a group's performance on a test over the course of three measurements*. Because of the nature of the testing on the 3rd measurement, I cannot directly compare data across all measurements. Therefore, I just standardized my data in SPSS and will now work with the \"z-scores\"* [The grading scale for the test is from 1 to 100 points in the first two measurements, and for the third one is from 1 to 125 (this is an example)]I want to see how their performance varies with age, i.e. if there is a correlation between age and performance. However, I have no clue what kind of correlation analysis I can perform since as far as I know Pearson's  makes comparisons by making use of z-scores and I have deleted the effect of those.  ","Creater_id":124157,"Start_date":"2016-07-29 05:49:57","Question_id":226283,"Tags":["correlation","standardization"],"Answer_count":1,"Last_activity":"2016-07-29 06:20:39","Link":"http://stats.stackexchange.com/questions/226283/how-to-perform-correlation-analysis-on-standardized-data","Creator_reputation":13}
{"_id":{"$oid":"5837a58aa05283111e4d7190"},"View_count":23,"Display_name":"gabboshow","Question_score":0,"Question_content":"I have 2 populations A and B.I would like to test whether the difference in 100 features are significant between population A and population B.For each feature I compute the p value. Should I correct the p value since I am testing different features?","Creater_id":35660,"Start_date":"2016-07-29 05:04:51","Question_id":226275,"Tags":["p-value","multiple-comparisons"],"Answer_count":1,"Last_activity":"2016-07-29 05:58:05","Link":"http://stats.stackexchange.com/questions/226275/p-value-in-case-of-comparing-several-features","Creator_reputation":212}
{"_id":{"$oid":"5837a58aa05283111e4d719d"},"View_count":66,"Display_name":"petrichor","Question_score":0,"Question_content":"I am trying to estimate a linear regression model (in the context of econometrics) using Bayesian approach (Gibbs sampler). The choice of the explanatory variables and model specification can be backed up by the literature and are intuitively reasonable. Simulated samples look like all converged from trace plots (MC iterations 20000+). However, I computed 95% highest posterior density and the results show that all of the estimated coefficients in the model are not significantly different from 0. It is micro level data and the sample size is around 300.Could anyone help to explain how to understand such situation and how to deal with such \"problem\" please?Specifically,Model estimated is demand system: expenditure share = intercept + log(price) + log(real expenditure) + log(real expenditure)^2 + sociodemographic variables + errorHave tried 5 and 6 groups (5 or 6 equations to be estimated), and each equation contains at least 8 or 9 (where without any sociodemograghic variables added for testing) explanatory variables. I am kind of lost in experimenting with data (e.g trying out different number of groups and including different sociodem variables). Is there a systematic framework to approach this question please? e.g. From the perspective of the nature of data, the model specification and the sampling method?","Creater_id":124910,"Start_date":"2016-07-28 17:15:20","Question_id":226197,"Tags":["regression","statistical-significance","bayesian","econometrics","gibbs"],"Answer_count":2,"Last_activity":"2016-07-29 05:48:57","Link":"http://stats.stackexchange.com/questions/226197/why-are-all-of-the-estimated-coefficients-in-a-linear-regression-model-not-signi","Creator_reputation":1}
{"_id":{"$oid":"5837a58aa05283111e4d71ab"},"View_count":54,"Display_name":"Johann","Question_score":1,"Question_content":"I'm not sure how to interpret the value of the bandwidth parameter in kernel density estimations. Let's say I if the values range from 1 to 20. How would I need to set the bandwidth, so that each kernel ranges over two. For example, if I want to set the kernel above the point 10, then the kernel should range from [9,11], if above 15 then [14,16]. Would that simply be the bandwidth of 2?The goal is to attach some meaning to the bandwidth.","Creater_id":124943,"Start_date":"2016-07-29 00:38:53","Question_id":226232,"Tags":["kernel-smoothing","density-estimation"],"Answer_count":1,"Last_activity":"2016-07-29 05:25:27","Link":"http://stats.stackexchange.com/questions/226232/how-to-interpret-the-bandwidth-value-in-a-kernel-density-estimation","Creator_reputation":6}
{"_id":{"$oid":"5837a58aa05283111e4d71b8"},"View_count":51715,"Display_name":"Paul Vogt","Question_score":59,"Question_content":"I'm looking for a non-technical definition of the lasso and what it is used for.","Creater_id":6927,"Start_date":"2011-10-18 21:24:43","Question_id":17251,"Tags":["regression","lasso","regularization","shrinkage"],"Answer_count":2,"Last_activity":"2016-07-29 05:22:51","Link":"http://stats.stackexchange.com/questions/17251/what-is-the-lasso-in-regression-analysis","Creator_reputation":296}
{"_id":{"$oid":"5837a58aa05283111e4d71c6"},"View_count":64,"Display_name":"Ali Turab Lotia","Question_score":1,"Question_content":"I have fit two models on a count variable.The first model is based on the assumption that the response variable is Poisson and the other is based on the assumption it is a negative binomial.The AIC for the model where a Poisson family is assumed is 476497, whereas the AIC for the negative binomial assumption is 339581. Furthermore, the data has a large number of 0s and the mean of the count variable (response) is 1.974 while the variance is 12.86011 breaking the Poisson mean = variance assumption.However, Micheal Friendly's distplot in R suggests the count variable follows a Poisson process. (Micheal Friendly's plot is essentially an equivalent form of the Q-Q plot for discrete data. It is interpreted in an identical manor as a Q-Q plot)Which distribution should be applied to the glm?Lastly, I could fit a quasipoisson model but the AIC appears as NA for the model and I do not know any methods of testing a quasipoisson assumption so I do not know how to compare that model with these ones.Thank you for your input.","Creater_id":124010,"Start_date":"2016-07-29 01:25:38","Question_id":226238,"Tags":["r","distributions","aic","count-data"],"Answer_count":0,"Last_activity":"2016-07-29 05:01:22","Link":"http://stats.stackexchange.com/questions/226238/is-michael-friendlys-distplot-or-aic-a-superior-indicator-of-which-model-to-fit","Creator_reputation":69}
{"_id":{"$oid":"5837a58aa05283111e4d71c8"},"View_count":50,"Display_name":"Alison Fairbrass","Question_score":0,"Question_content":"I would like to ask the question 'How do different taxonomic groups respond to different types of land cover?'. I am using the 'lme4' package v.1.1-7 in R v.3.1.2 to conduct a GLMER analysis of the form:M1 \u0026lt;- glmer(Ecological response ~ Agriculture + Taxon + (Agriculture | Taxon) + Forest + (Forest | Taxon) + Water + (Water | Taxon) + (1|Study) + (1|Site), na.action = \"na.fail\", family = poisson, data=dat)The response data is Poisson distributed and over-dispersed hence my use of the observer-level Site random effect.I have included the (Land cover type | Taxon) terms so that I am able to extract taxon-specific responses to different types of land cover using ranef(M1)['Taxon'], for example these results for Agriculture:$Taxon             (Intercept)          AgricultureArthropoda   -1.782759e-08        -2.326447e-06Ascomycota    2.820254e-08         2.267939e-06Chordata      1.792752e-09        -5.783661e-08Tracheophyta  3.688819e-10        -1.507536e-09But these are the results from the full model, what I would like are these results after conducting model-averaging.I am conducting model-averaging using the 'MuMIn' v.1.12.1 package using to following code:allModels \u0026lt;- dredge(M1)topModels \u0026lt;- model.avg(allModels, subset = delta \u0026lt; 4)summary(topModels)I would like to know how to obtain the taxon-specific coefficients, such as is obtained using ranef(M1)['Taxon'] from the full model, for each land cover type in relation to the averaged top models?Many thanks in advance for your assistance.","Creater_id":61376,"Start_date":"2016-07-29 04:53:35","Question_id":226273,"Tags":["regression","mixed-model","lme4","model-averaging"],"Answer_count":0,"Last_activity":"2016-07-29 04:53:35","Link":"http://stats.stackexchange.com/questions/226273/obtaining-coefficients-of-fixed-effect-random-effect-in-mixed-effects-analys","Creator_reputation":21}
{"_id":{"$oid":"5837a58aa05283111e4d71ca"},"View_count":452,"Display_name":"jeremy radcliff","Question_score":8,"Question_content":"Our prof is not getting into the math or even geometric representation of multiple linear regression and this has me slightly confused.On the one hand it's still called multiple linear regression, even in higher dimensions. On the other hand, if we have for example  and we can plug in any values we'd like for  and , wouldn't this give us a plane of possible solutions and not a line?In general, isn't our surface of prediction going to be a  dimensional hyperplane for  independent variables?","Creater_id":60672,"Start_date":"2016-07-28 12:31:34","Question_id":226172,"Tags":["multiple-regression","high-dimensional"],"Answer_count":2,"Last_activity":"2016-07-29 04:49:16","Link":"http://stats.stackexchange.com/questions/226172/is-multiple-linear-regression-in-3-dimensions-a-plane-of-best-fit-or-a-line-of-b","Creator_reputation":375}
{"_id":{"$oid":"5837a58aa05283111e4d71d7"},"View_count":59,"Display_name":"jf328","Question_score":0,"Question_content":"\u0026gt; dt = data.table(x = rnorm(100))\u0026gt; dt[, y := 1+0.2*x + rnorm(100)]\u0026gt; \u0026gt; fit = lm(y~x, data = dt)\u0026gt; summary(fit)Call:lm(formula = y ~ x, data = dt)Residuals:     Min       1Q   Median       3Q      Max -2.19493 -0.75218 -0.03459  0.64181  2.38214 Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)   0.8561     0.1036   8.266 6.86e-13 ***x             0.3025     0.1056   2.865   0.0051 ** ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 1.022 on 98 degrees of freedomMultiple R-squared:  0.07731,   Adjusted R-squared:  0.06789 F-statistic: 8.211 on 1 and 98 DF,  p-value: 0.005095\u0026gt; dt[, 1 - sum(fit$residuals^2) / sum(y^2)][1] 0.419261Shouldn't R-squared be 0.419? Or at least close to it, not 0.07?","Creater_id":20551,"Start_date":"2016-07-29 04:18:20","Question_id":226260,"Tags":["r","r-squared","lm"],"Answer_count":2,"Last_activity":"2016-07-29 04:47:35","Link":"http://stats.stackexchange.com/questions/226260/cant-understand-this-multiple-r-squared-value-in-r-lm","Creator_reputation":416}
{"_id":{"$oid":"5837a58aa05283111e4d71e5"},"View_count":235,"Display_name":"poperin","Question_score":0,"Question_content":"I want to test my pair matching method based on this evaluation metric but unsure how to do this? I only have positive (matching) test samples.","Creater_id":14645,"Start_date":"2014-01-23 20:42:30","Question_id":83201,"Tags":["roc"],"Answer_count":1,"Last_activity":"2016-07-29 04:45:21","Link":"http://stats.stackexchange.com/questions/83201/how-to-compute-for-the-verification-rate-at-0-001-far","Creator_reputation":108}
{"_id":{"$oid":"5837a58aa05283111e4d71f2"},"View_count":26,"Display_name":"Jef Van Alsenoy","Question_score":0,"Question_content":"In the context of designing a multiple mixed model with categorical and continuous variables and running into multicollinearity problems, I'm now checking which of my categorical variables correlate.I started with chisq.tests on pairs of categorical variables, but chisq.test returned the smallest possible p-value (\u0026lt; 2.2e-16) for almost all couples with the warning \"Chi-squared approximation may be incorrect\". Bad news: I don't want all my categorical variables to correlate, because then I have none left to include in my model.I then investigated why all categorical variables correlated according to chisq.test, and found out that the tables of expected values often had many values \u0026lt;5 (e.g. 70% of the cells contained values \u0026lt;5).As proposed in thread (What to do when I have expected count \u0026lt;5 warning for a chi squared test?) and in thread (Warning in R - Chi-squared approximation may be incorrect) I turned to fisher.test but ran into the error \"Error in fisher.test(TEXTUUR, DRAINKL) : FEXACT error 5.\".As proposed in thread (Warning in R - Chi-squared approximation may be incorrect) I left the Fisher test for what is was and returned the chi-squared test, using the simulate.p.value = TRUE alternative. Where I first got p-value \u0026lt; 2.2e-16, now I get p-value = 0.0004998.My questions are:Why does the Fisher test fail?I'm prone to using the chisq.test with simulate.p.value = TRUE, but as Glen_B states in thread (Warning in R - Chi-squared approximation may be incorrect) it implies that I'm \"willing to condition on my margins\". What does this mean? I don't understand. I will be reporting on this extensively, and I don't just want to use the test that returns the best p-value.Is there a better strategy to investigate correlations of categorical variables than 2-by-2 comparisons using chisq.test(simulate.p.value=T)?An example to illustrate the process for a couple of categorical variables:TEXTDRAIN\u0026lt;-chisq.test(TEXTUUR,DRAINKL)Warning message:In chisq.test(TEXTUUR, DRAINKL) :Chi-squared approximation may be incorrectTEXTDRAINPearson's Chi-squared testdata:  TEXTUUR and DRAINKLX-squared = 2442.3, df = 104, p-value \u0026lt; 2.2e-16TEXTDRAINexpected,0)       DRAINKLTEXTUUR a a-d  b  c c-d   d d-e  e e-f  f g  h h-i i      A 2   1 12 40   2  71   1 30   2 19 1  9   1 0      E 2   1  9 29   2  53   1 22   1 14 1  7   1 0      L 3   1 19 64   4 115   2 48   3 31 1 14   2 1      P 2   1 13 44   3  79   1 33   2 21 1 10   1 1      S 4   2 24 82   5 146   2 61   4 39 2 18   2 1      U 0   0  2  8   0  15   0  6   0  4 0  2   0 0      V 0   0  0  0   0   0   0  0   0  0 0  0   0 0      X 0   0  1  4   0   7   0  3   0  2 0  1   0 0      Z 4   2 24 80   5 143   2 59   4 38 2 18   2 1\u0026gt;   (length(TEXTDRAINexpected \u0026lt; 5]))/(length(TEXTDRAIN$expected))[1] 0.6825397\u0026gt;   TEXTDRAIN2\u0026lt;-table(TEXTUUR,DRAINKL)\u0026gt;   fisher.test(TEXTDRAIN2)Error in fisher.test(TEXTDRAIN2) : FEXACT error 5.The hash table key cannot be computed because the largest keyis larger than the largest representable int.The algorithm cannot proceed.Reduce the workspace size or use another algorithm.\u0026gt;   chisq.test(TEXTDRAIN2,simulate.p.value = TRUE)    Pearson's Chi-squared test with simulated p-value (based on 2000 replicates)data:  TEXTDRAIN2X-squared = 2442.3, df = NA, p-value = 0.0004998","Creater_id":123928,"Start_date":"2016-07-29 04:44:21","Question_id":226270,"Tags":["correlation","categorical-data","chi-squared","fishers-exact"],"Answer_count":0,"Last_activity":"2016-07-29 04:44:21","Link":"http://stats.stackexchange.com/questions/226270/choosing-the-appropriate-test-to-check-correlation-in-categorical-variables","Creator_reputation":26}
{"_id":{"$oid":"5837a58aa05283111e4d71f4"},"View_count":14,"Display_name":"Henry David Thorough","Question_score":0,"Question_content":"I have two separate treatments, which do not interact (though a few units receive both treatments). How do I test a hypothesis test that one has a larger coefficient than the other? In other words, if I pool the sample and run the following regression...Y = B_0 + B_1 Treatment1 + B_2 Treatment2How do I test that B_1 is larger than B_2?","Creater_id":73913,"Start_date":"2016-07-29 04:43:44","Question_id":226269,"Tags":["regression","hypothesis-testing"],"Answer_count":0,"Last_activity":"2016-07-29 04:43:44","Link":"http://stats.stackexchange.com/questions/226269/testing-hypothesis-that-effect-of-one-treatment-is-greater-than-the-other","Creator_reputation":129}
{"_id":{"$oid":"5837a58aa05283111e4d71f6"},"View_count":13,"Display_name":"rbx","Question_score":0,"Question_content":"Are there any strategies to learn a model that can classify data from one domain using only data from a different domain for training?For example, suppose I have a bunch of data from two different domains (Da and Db), and I want to train a model using data from those two domains that can then turn around and classify data from a third domain (Dc). Some of the features are shared between Da and Db, and some are not. Some of the features from Dc appear in Da and Db, but there are also some that do not. However, the labels in each domain are the same. So, I want to classify each instance as either class X or class Y, and I have labeled instances of class X and Y for Da and Db, and each instance I get from Dc will also either be of class X or class Y.With the only information on Dc being which features are used and the classes to classify, is it even possible to determine which features specific to Dc are useful for classification, using the data and features from Da and Db? I know there are transfer learning techniques for cases where you have access to training data from Dc, labeled or no, but are there any techniques that can perform classification without any data from Dc? Intuitively I suspect not, because there is no way to know anything about the features specific to Dc without any training data from Dc. With some training data from Dc (labeled or not), it seems like current transfer learning techniques would be able to construct a mapping from features specific to Dc to features from Da or Db.If it is impossible as I suspect, are there ways to incrementally learn the mapping as each instance is introduced from Dc?","Creater_id":37276,"Start_date":"2016-07-29 04:40:02","Question_id":226268,"Tags":["machine-learning","transfer-learning"],"Answer_count":0,"Last_activity":"2016-07-29 04:40:02","Link":"http://stats.stackexchange.com/questions/226268/heterogeneous-domain-adaptation-without-training-data-from-target-domain","Creator_reputation":1086}
{"_id":{"$oid":"5837a58aa05283111e4d71f8"},"View_count":36,"Display_name":"Crops","Question_score":0,"Question_content":"What are the differences and similarities between the R packages lme4 and varComp for fitting linear mixed models ? ","Creater_id":37661,"Start_date":"2016-07-29 04:38:49","Question_id":226267,"Tags":["r","mixed-model","lmer","lme4"],"Answer_count":0,"Last_activity":"2016-07-29 04:38:49","Link":"http://stats.stackexchange.com/questions/226267/choosing-between-lme4-and-varcomp","Creator_reputation":202}
{"_id":{"$oid":"5837a58aa05283111e4d71fa"},"View_count":38,"Display_name":"Crops","Question_score":2,"Question_content":"I am trying to fit some mixed models for unbalanced data as follows.library(easyanova)data(data13)1) genotypes as randomfrmla \u0026lt;- \"yield ~ 1 + (1|blocks) + (1|genotypes)\"model1 \u0026lt;- lmer(formula(frmla), data = data13)# adjusted means - BLUPs for genotypesnewdata13 \u0026lt;- expand.grid(genotypes = levels(data13blocks))newdata13pred, newdata13genotypes), blocks = levels(data13pred \u0026lt;- predict(model2, newdata=newdata13)tapply(newdata13genotypes, mean)For further calculations I need to compute mean variance of difference of adjusted means (BLUPs or BLUEs). A method is given (https://static-content.springer.com/esm/art%3A10.1186%2F1471-2164-14-860/MediaObjects/12864_2013_5591_MOESM1_ESM.doc)to compute it from variance-covariance matrix of adjusted means.How to get the variance-covariance matrix of adjusted means for model1 and model2 ?When genotypes are fixed in model2 does vcov(model2) give the variance-covariance matrix of adjusted means ?","Creater_id":37661,"Start_date":"2016-07-29 04:36:00","Question_id":226266,"Tags":["r","mixed-model","lme4"],"Answer_count":0,"Last_activity":"2016-07-29 04:36:00","Link":"http://stats.stackexchange.com/questions/226266/compute-variance-covariance-matrix-of-adjusted-means-in-r-lme4","Creator_reputation":202}
{"_id":{"$oid":"5837a58aa05283111e4d71fc"},"View_count":36,"Display_name":"user3275222","Question_score":1,"Question_content":"I need advice regarding an analysis I wish to perform. I have 10 data files, each represents a different year (2003-2012). Each file, contains a filed of family ID, and a field of Subject ID within the family ID. In addition, I have a variable of interest: the monthly income, and explanatory variables, from which the one of main interest is the age. I wish to find:1. The relationships between income and age (which theoretically is expected to be quadratic).2. The change of this relationship over the years.I need your advice on which model I need for answering these two questions. But it ain't as simple as that. Most families has only 1 subject. A few has 2, and only a minority has more. In addition, a minority of the families (and subjects) appear on more than one file (year), while no subject appears on all 10 files, and there are many families and subjects that appears on 1 file only (data is messy!). From numbers point of view, each file contains around 10,000 subjects, which is good and bad. While modeling, I need to be very careful, and produce more than just a P-Value, since I expect P-Values to be low just due to the small sample size. I am attaching a sketch of the table for one representative year, and a plot of income vs. age in this year, not taking into account the family. There are many families and I am afraid of estimating too many parameters. Do you think it's possible to model each year separately and somehow compare the coefficients?Thank you! ","Creater_id":81477,"Start_date":"2016-07-28 23:13:18","Question_id":226225,"Tags":["panel-data","nonlinear-regression"],"Answer_count":1,"Last_activity":"2016-07-29 04:20:42","Link":"http://stats.stackexchange.com/questions/226225/how-to-analyze-non-linear-unbalanced-yearly-data","Creator_reputation":159}
{"_id":{"$oid":"5837a58aa05283111e4d7208"},"View_count":25,"Display_name":"Tessa Visser","Question_score":2,"Question_content":"I'm studying the electrophysiological response of mosquitoes towards different odours at different concentrations. Because the set-up and the environment gives so much noise, I have to normalise my data to a control odour. Now I want to know if the normalised means of my tested odours at different concentrations differ from the control, i.e. do the mosquitoes respond differently towards the odours than they do to the control? I thought I could do a one-way t-test, and compare the mean of each odour and concentration to 0. However, then I'm not taking into account that there are several odours and concentrations, so a ANOVA with my control set to 0 seems to be more appropriate, but doesn't seem like a good way either. How should I approach this?I hope I've explained it clearly. Thank you in advance!","Creater_id":124961,"Start_date":"2016-07-29 04:02:18","Question_id":226254,"Tags":["anova","dataset","normalization"],"Answer_count":1,"Last_activity":"2016-07-29 04:06:20","Link":"http://stats.stackexchange.com/questions/226254/what-statistical-test-to-use-for-testing-means-of-normalised-data-against-contro","Creator_reputation":13}
{"_id":{"$oid":"5837a58aa05283111e4d7215"},"View_count":21,"Display_name":"Purav Agrawal","Question_score":1,"Question_content":"I am doing forecasting for six month daily data for item sales of which increases on friday and saturday.  I am using Holts Winters method and have done the forecasting getting Thiel's U Statistics value of 0.62.  Plotting the ACF graph for the error, i find that the peaks occur in 7, 14, 21 periods which indicates that some seasonality error is present in my calculations.I have below two questions:Is my forecast good enough? Some parameters are mentioned below:ME  0.25MAE 3.38MAPE    23.77MSE 19.20U Stat  0.623038794Alpha   0.494203004Beta    0.691687311Gamma   0.00617242Which approach would provide better results?  I am doing the exercise in excel.What would you suggest looking into the image of the ACF attached herewith?  How can the forecast be improved?  There are 176 data points.[![ACF graph for errors of forecasting][1]][1]Thanks in advance,Best regards,PuravData for the forecasting:Date    Yt3/2/2015    153/3/2015    193/4/2015    183/5/2015    143/6/2015    233/7/2015    263/8/2015    243/9/2015    153/10/2015   233/11/2015   163/12/2015   173/13/2015   223/14/2015   243/15/2015   193/16/2015   123/17/2015   123/18/2015   113/19/2015   153/20/2015   213/21/2015   253/22/2015   253/23/2015   183/24/2015   153/25/2015   203/26/2015   173/27/2015   233/28/2015   253/29/2015   193/30/2015   183/31/2015   144/1/2015    124/2/2015    94/3/2015    314/4/2015    224/5/2015    254/6/2015    94/7/2015    104/8/2015    284/9/2015    164/10/2015   184/11/2015   174/12/2015   224/13/2015   204/14/2015   154/15/2015   184/16/2015   194/17/2015   174/18/2015   224/19/2015   334/20/2015   114/21/2015   104/22/2015   134/23/2015   174/24/2015   234/25/2015   224/26/2015   244/27/2015   114/28/2015   134/29/2015   124/30/2015   245/1/2015    205/2/2015    135/3/2015    245/4/2015    85/5/2015    145/6/2015    195/7/2015    125/8/2015    155/9/2015    225/10/2015   155/11/2015   125/12/2015   135/13/2015   145/14/2015   175/15/2015   205/16/2015   205/17/2015   235/18/2015   165/19/2015   85/20/2015   235/21/2015   205/22/2015   155/23/2015   135/24/2015   135/25/2015   65/26/2015   155/27/2015   155/28/2015   145/29/2015   225/30/2015   315/31/2015   246/1/2015    96/2/2015    146/3/2015    166/4/2015    146/5/2015    146/6/2015    276/7/2015    226/8/2015    146/9/2015    176/10/2015   206/11/2015   156/12/2015   96/13/2015   156/14/2015   256/15/2015   136/16/2015   176/17/2015   156/18/2015   136/19/2015   226/20/2015   246/21/2015   206/22/2015   106/23/2015   106/24/2015   226/25/2015   246/26/2015   236/27/2015   256/28/2015   196/29/2015   146/30/2015   197/1/2015    127/2/2015    157/3/2015    277/4/2015    257/5/2015    147/6/2015    117/7/2015    207/8/2015    167/9/2015    117/10/2015   167/11/2015   317/12/2015   167/13/2015   117/14/2015   147/15/2015   227/16/2015   147/17/2015   227/18/2015   197/19/2015   217/20/2015   167/21/2015   167/22/2015   187/23/2015   97/24/2015   297/25/2015   257/26/2015   167/27/2015   127/28/2015   147/29/2015   267/30/2015   117/31/2015   188/1/2015    258/2/2015    228/3/2015    98/4/2015    208/5/2015    158/6/2015    168/7/2015    348/8/2015    258/9/2015    218/10/2015   138/11/2015   128/12/2015   188/13/2015   188/14/2015   248/15/2015   278/16/2015   158/17/2015   188/18/2015   148/19/2015   198/20/2015   178/21/2015   208/22/2015   208/23/2015   228/24/2015   148/25/2015   188/26/2015   118/27/2015   228/28/2015   228/29/2015   338/30/2015   228/31/2015   129/1/2015    89/2/2015    149/3/2015    179/4/2015    159/5/2015    299/6/2015    109/7/2015    119/8/2015    199/9/2015    179/10/2015   109/11/2015   369/12/2015   199/13/2015   179/14/2015   129/15/2015   129/16/2015   239/17/2015   139/18/2015   209/19/2015   159/20/2015   139/21/2015   159/22/2015   179/23/2015   149/24/2015   219/25/2015   219/26/2015   309/27/2015   199/28/2015   189/29/2015   119/30/2015   1610/1/2015   1910/2/2015   2410/3/2015   2110/4/2015   1610/5/2015   1410/6/2015   1210/7/2015   1310/8/2015   1410/9/2015   2010/10/2015  2910/11/2015  2010/12/2015  910/13/2015  810/14/2015  610/15/2015  1510/16/2015  1810/17/2015  2210/18/2015  1710/19/2015  910/20/2015  1310/21/2015  1610/22/2015  1210/23/2015  1210/24/2015  2310/25/2015  2710/26/2015  1010/27/2015  1510/28/2015  1810/29/2015  1210/30/2015  1710/31/2015  1611/1/2015   811/2/2015   1211/3/2015   1311/4/2015   1111/5/2015   1211/6/2015   1511/7/2015   1811/8/2015   1311/9/2015   611/10/2015  2011/11/2015  1411/12/2015  1311/13/2015  1811/14/2015  811/15/2015  911/16/2015  911/17/2015  1411/18/2015  1211/19/2015  611/20/2015  1711/21/2015  1611/22/2015  1711/23/2015  1111/24/2015  1611/25/2015  1411/26/2015  1311/27/2015  1511/28/2015  2011/29/2015  1611/30/2015  712/1/2015   1112/2/2015   712/3/2015   1212/4/2015   2512/5/2015   2012/6/2015   1712/7/2015   1112/8/2015   1512/9/2015   1012/10/2015  2312/11/2015  1712/12/2015  1312/13/2015  1712/14/2015  1312/15/2015  1812/16/2015  1212/17/2015  1212/18/2015  1712/19/2015  1212/20/2015  1912/21/2015  1512/22/2015  1512/23/2015  1612/24/2015  1712/25/2015  2712/26/2015  1512/27/2015  1912/28/2015  812/29/2015  1212/30/2015  1812/31/2015  481/1/2016    131/2/2016    181/3/2016    141/4/2016    121/5/2016    111/6/2016    61/7/2016    141/8/2016    171/9/2016    211/10/2016   161/11/2016   71/12/2016   121/13/2016   71/14/2016   121/15/2016   131/16/2016   151/17/2016   141/18/2016   81/19/2016   111/20/2016   111/21/2016   151/22/2016   101/23/2016   231/24/2016   131/25/2016   141/26/2016   161/27/2016   141/28/2016   131/29/2016   141/30/2016   171/31/2016   192/1/2016    72/2/2016    92/3/2016    72/4/2016    82/5/2016    182/6/2016    152/7/2016    142/8/2016    82/9/2016    122/10/2016   162/11/2016   142/12/2016   162/13/2016   172/14/2016   82/15/2016   72/16/2016   92/17/2016   62/18/2016   82/19/2016   122/20/2016   122/21/2016   202/22/2016   42/23/2016   112/24/2016   142/25/2016   102/26/2016   162/27/2016   112/28/2016   132/29/2016   143/1/2016    9","Creater_id":124853,"Start_date":"2016-07-28 07:07:25","Question_id":226110,"Tags":["forecasting"],"Answer_count":0,"Last_activity":"2016-07-29 04:01:01","Link":"http://stats.stackexchange.com/questions/226110/forecast-done-by-holts-winter-method-has-seasonality-in-error-auto-correlation","Creator_reputation":6}
{"_id":{"$oid":"5837a58aa05283111e4d7217"},"View_count":26,"Display_name":"Dirk Nachbar","Question_score":0,"Question_content":"I have some normally distributed data x of length N. I want to test 2 hypotheses: that =0 or that . I can calculate the likelihood for all N data points for each hypothesis (for model 2 I use numerical integration to get it). Now I have N likelihoods under each model, how do I condense this to one Bayes Factor? I could avg it or count the number of times each model has a higher factor? ","Creater_id":105596,"Start_date":"2016-07-28 05:27:24","Question_id":226088,"Tags":["probability","bayesian","likelihood"],"Answer_count":1,"Last_activity":"2016-07-29 03:57:42","Link":"http://stats.stackexchange.com/questions/226088/bayes-factor-with-data","Creator_reputation":101}
{"_id":{"$oid":"5837a58aa05283111e4d7224"},"View_count":32,"Display_name":"user86569","Question_score":0,"Question_content":"I want to pose a question regarding auditing the output of different regression the methodologies:Let's say we have a hypothetical data set where we are trying to determine the relationship that several (10+) continuous explanatory variables have on one continuous response variable (aka a classic regression problem).  Here's the catch, the explanatory variables display traits of both multi-colinearity and heteroskedasticity.  Using a frequentist regression model that includes all the explanatory variables introduces the risk of having the coefficients be biased, inflated..., etc.  If I were looking to use a new~ish Bayesian model using MCMC instead, how could I account for factors such as heteroskedasticity and multi-colinearity?  Using frequentist regression, I would typically go through the process of checking the residuals against a set of assumptions, adjusting/removing variables, rerunning the model, checking the assumptions again, and again.  But when it comes do diagnosing a Bayesian model, I'm at a loss.  Does anyone have any recommendations as to how to repeat this validation process but in a Bayesian/MCMC manner?  Also, when I say Bayesian, I'm learning through the work of John Kruschke and his book Doing Bayesian Data Analysis using the R programming language.  Thank you! ","Creater_id":86569,"Start_date":"2016-07-27 06:29:53","Question_id":225899,"Tags":["regression","bayesian"],"Answer_count":1,"Last_activity":"2016-07-29 03:21:55","Link":"http://stats.stackexchange.com/questions/225899/diagnosing-baysian-regression-models","Creator_reputation":10}
{"_id":{"$oid":"5837a58aa05283111e4d722f"},"View_count":18,"Display_name":"luchonacho","Question_score":0,"Question_content":"There are two questions asking about the random nature of a subsample (here and here). However, they do not provide a formal proof of the result. I am looking for a proof to support my selection mechanism.The specific problem is: Say I have a random sample of size , drawn from a given population. One of the dimensions of that population is, for example, gender. Now, from the above sample, I selects only those individuals who are female, obtaining a subsample of size  (naturally, ). The question is: is that subsample representative of the population of females? ","Creater_id":100369,"Start_date":"2016-07-29 02:34:06","Question_id":226248,"Tags":["sample","randomness"],"Answer_count":0,"Last_activity":"2016-07-29 02:34:06","Link":"http://stats.stackexchange.com/questions/226248/formal-proof-that-a-subsample-of-a-random-sample-is-representative","Creator_reputation":586}
{"_id":{"$oid":"5837a58aa05283111e4d7231"},"View_count":204,"Display_name":"Chirayu Chamoli","Question_score":1,"Question_content":"I would like to use gradient descent for hyper parameter tuning for RF. Does any one has any insights on this, any merits and demerits would be appreciated? Also It would be great if some can share R code.","Creater_id":100552,"Start_date":"2016-07-29 00:18:50","Question_id":226230,"Tags":["r","random-forest","gradient-descent"],"Answer_count":2,"Last_activity":"2016-07-29 02:31:40","Link":"http://stats.stackexchange.com/questions/226230/random-forest-hyperparmeter-tuning-using-gradient-descent","Creator_reputation":150}
{"_id":{"$oid":"5837a58aa05283111e4d723f"},"View_count":8,"Display_name":"kosmos","Question_score":0,"Question_content":"I have a data  and D2=D1 + noise.I learn a model  using  and model  using . Now suppose I lose  But I have ,  and . How to learn the noise distribution?","Creater_id":38235,"Start_date":"2016-07-29 02:31:34","Question_id":226246,"Tags":["distributions","learning","noise"],"Answer_count":0,"Last_activity":"2016-07-29 02:31:34","Link":"http://stats.stackexchange.com/questions/226246/reverse-learning-a-noise-distribution-using-existing-model","Creator_reputation":81}
{"_id":{"$oid":"5837a58aa05283111e4d7241"},"View_count":24,"Display_name":"David J","Question_score":2,"Question_content":"I am trying to build a toy model of school student misbehaviour to compare with data I've taken from my classes.A element of the models I'm considering is, when students choose an action, they may either choose to follow rules or break them. To begin with, each student has a probability to break the rules when they make a choice, between 0 and 1.I want to know what sorts of distributions to consider for the students' probability to break rules. Ideally, it would be concentrated near 0 (most students follow rules).My first thought is to try a truncated Gaussian, but my only reason for this is because it's such a common distribution.","Creater_id":124780,"Start_date":"2016-07-27 15:53:48","Question_id":226015,"Tags":["distributions"],"Answer_count":1,"Last_activity":"2016-07-28 23:39:53","Link":"http://stats.stackexchange.com/questions/226015/distributions-of-inclination-to-choose-the-first-of-two-options","Creator_reputation":13}
{"_id":{"$oid":"5837a58aa05283111e4d724d"},"View_count":14,"Display_name":"Alex","Question_score":0,"Question_content":"I have taken many things for granted while fitting GLMs, whose mechanism I find very ably described here, which I quote (emphasis is mine):  A standard linear model (e.g., a simple regression model) can be  thought of as having two 'parts'. These are called the structural  component and the random component. For example:    Y=\\beta_0+\\beta_1X+\\varepsilon  \\\\ \\text{where }  \\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)  The first two terms (that  is, ) constitute the structural component, and the   (which indicates a normally distributed error term) is  the random component. When the response variable is not normally  distributed (for example, if your response variable is binary) this  approach may no longer be valid. The generalized linear model  (GLiM) was developed to address such cases, and logit and probit  models are special cases of GLiMs that are appropriate for binary  variables (or multi-category response variables with some adaptations  to the process). A GLiM has three parts, a structural component, a  link function, and a response distribution. For example:    g(\\mu)=\\beta_0+\\beta_1X  Here  is again the  structural component,  is the link function, and  is a mean  of a conditional response distribution at a given point in the  covariate space. The way we think about the structural component here  doesn't really differ from how we think about it with standard linear  models; in fact, that's one of the great advantages of GLiMs. Because  for many distributions the variance is a function of the mean, having  fit a conditional mean (and given that you stipulated a response  distribution), you have automatically accounted for the analog of the  random component in a linear model (N.B.: this can be more complicated  in practice).So, the error referenced in my question title is the  from the first equation. However, I am not sure I really understand the second bold statement, with regards to where the error term has gone. Here is my interpretation:Given a covariate/response observations , we assume that  is a random variate of a random variable  with some distribution with mean  conditional on . The difference between the random variates of  and  is what I understand to be the \"error\", thus  is one particular observation of this \"error\" term.Thus, this leads me to the second part of my question, why is the significance of the error  when specifying the distribution of ? I can only think that you penalise the magnitude of the error differently, depending on the distribution. I.e. between possible distributions of , some values for the observed  would be more extreme/improbable than others.Well if this is correct, and bearing in mind that I am fitting my GLMs with maximum likelihood, then is it not an incomplete statement to say \" has Poisson distribution\". Here, I use the Poisson as an example, as you can replace it with any other distribution you choose. Surely what we mean here is that we will use the log-likelihood function for the Poisson to penalise error terms .Thus, is it not more descriptive to say: \"We will penalise our model means based on a Poisson log-likelihood applied to the observed responses\".","Creater_id":22199,"Start_date":"2016-07-28 22:22:48","Question_id":226221,"Tags":["generalized-linear-model"],"Answer_count":0,"Last_activity":"2016-07-28 22:22:48","Link":"http://stats.stackexchange.com/questions/226221/where-did-the-error-term-go-and-what-is-its-significance-in-specifying-the-distr","Creator_reputation":727}
{"_id":{"$oid":"5837a58aa05283111e4d724f"},"View_count":27,"Display_name":"isosceleswheel","Question_score":0,"Question_content":"I have what seems like a very simple question, but for some reason I cannot find any references for my specific problem.I have a predictive model for a system I am studying. At each point  in the system where I make an observation, my model makes a prediction. I make a series of observations  and my model makes a series of predictions . I would like to assume the linear model  (instead of fitting a regression line) and then calculate  directly from my observed and predicted data as follows:R^2 = 1 - \\frac{\\sum_i (y_i - x_i)^2}{\\sum_i (y_i - \\bar{y})^2}(source: https://en.wikipedia.org/wiki/Coefficient_of_determination)My questions are 1.) is there a specific name for calculating  in this way and is the above quantity still called  ? 2.) how would I go about evaluating the statistical significance of  ?","Creater_id":90032,"Start_date":"2016-07-28 20:27:59","Question_id":226211,"Tags":["predictive-models","linear-model"],"Answer_count":0,"Last_activity":"2016-07-28 21:49:27","Link":"http://stats.stackexchange.com/questions/226211/calculating-r2-and-signifiance-when-the-linear-model-y-x-is-assumed","Creator_reputation":101}
{"_id":{"$oid":"5837a58aa05283111e4d7251"},"View_count":11,"Display_name":"Naveenan","Question_score":0,"Question_content":"customer cohort data analysis - Tracking customer revenue over period time for each cohort. A cohort is customers acquired during the month. So For 2015 I have cohorts like 201501,201502,201503 etc. I track revenue for customers by the cohorts and see how each cohort perform. I want to plot chart with cumulative average revenue (total revenue/number of customers in the cohort). I want to add confidence interval around this plot. How would I approach the problem.","Creater_id":76660,"Start_date":"2016-07-28 21:36:35","Question_id":226216,"Tags":["confidence-interval","sampling"],"Answer_count":0,"Last_activity":"2016-07-28 21:36:35","Link":"http://stats.stackexchange.com/questions/226216/confidence-interval-for-cummulative-data","Creator_reputation":18}
{"_id":{"$oid":"5837a58aa05283111e4d7253"},"View_count":1173,"Display_name":"Oswy","Question_score":11,"Question_content":"There seem to be many machine learning algorithms that rely on kernel functions. SVMs and NNs to name but two. So what is the definition of a kernel function and what are the requirements for it to be valid?","Creater_id":null,"Start_date":"2012-02-16 09:25:28","Question_id":23386,"Tags":["svm","terminology","kernel-trick"],"Answer_count":3,"Last_activity":"2016-07-28 20:44:46","Link":"http://stats.stackexchange.com/questions/23386/what-is-a-kernel-and-what-sets-it-apart-from-other-functions","Creator_reputation":null}
{"_id":{"$oid":"5837a58aa05283111e4d7262"},"View_count":34,"Display_name":"chrisw","Question_score":0,"Question_content":"I want to use the R implementation of t-distributed stochastic neighbor embedding Rtsne to sort out a large number of sets of numbers of cDNA reads from different cell lines. I am interested in the degree of clustering of both cells and genes. I have a table of reads with columns being cells and rows being genes, and the table values are expressed in a standardized way. Unfortunately, not all genes in all cells have identifiable reads that match them, so there are lots of zeros in the table. The zeros have many sources, and I would like to confine the analysis to cases in which there are attributable reads.Can Rtsne be persuaded to ignore the zero values? I have tried turning them into NA and NaN without success. It may be that Rtsne requires a completely regular data matrix. ","Creater_id":117616,"Start_date":"2016-06-01 14:59:13","Question_id":215844,"Tags":["clustering"],"Answer_count":0,"Last_activity":"2016-07-28 20:21:57","Link":"http://stats.stackexchange.com/questions/215844/zero-values-in-rtsne","Creator_reputation":1}
{"_id":{"$oid":"5837a58aa05283111e4d7264"},"View_count":10438,"Display_name":"Paul","Question_score":30,"Question_content":"Besides gnuplot and ggobi, what open source tools are people using for visualizing multi-dimensional data?Gnuplot is more or less a basic plotting package. Ggobi can do a number of nifty things, such as:animate data along a dimension or among discrete collectionsanimate linear combinations varying the coefficientscompute principal components and other transformationsvisualize and rotate 3 dimensional data clustersuse colors to represent a different dimensionWhat other useful approaches are based in open source and thus freely reusable or customizable?Please provide a brief description of the package's abilities in the answer.","Creater_id":87,"Start_date":"2010-07-19 19:17:24","Question_id":196,"Tags":["data-visualization","open-source"],"Answer_count":8,"Last_activity":"2016-07-28 19:59:10","Link":"http://stats.stackexchange.com/questions/196/open-source-tools-for-visualizing-multi-dimensional-data","Creator_reputation":915}
{"_id":{"$oid":"5837a58aa05283111e4d7278"},"View_count":21,"Display_name":"Ben Donovan","Question_score":1,"Question_content":"To fit a given a set of data points (x,y) to a circle, one can use a least squares fit and obtain values for the center of the circle (xc, yc) and the circle's radius (R).However, each of the data points (x,y) has their own uncertainty in both x and y. How would you propagate the uncertainties in the individual data points to uncertainties in the center of the circle (xc +/- xc_err, yc +/- yc_err) and the circle's radius (R +/- R_err)?","Creater_id":121476,"Start_date":"2016-07-28 19:26:40","Question_id":226207,"Tags":["least-squares","error","uncertainty","error-propagation"],"Answer_count":0,"Last_activity":"2016-07-28 19:26:40","Link":"http://stats.stackexchange.com/questions/226207/uncertainties-on-fitted-parameters-in-least-squares-circle-fit","Creator_reputation":11}
{"_id":{"$oid":"5837a58aa05283111e4d727a"},"View_count":331,"Display_name":"hxd1011","Question_score":10,"Question_content":"We know that some objective functions are easier to optimize and some are hard. And there are many loss functions that we want to use but hard to use, for example 0-1 loss. So we find some proxy loss functions to do the work. For example, we use hinge loss or logistic loss to \"approximate\" 0-1 loss. Following plot is coming from Chris Bishop's PRML book. The Hinge Loss is plotted in blue, the Log Loss in red, the Square Loss in green and the 0/1 error in black.I understand the reason we have such design (for hinge and logistic loss) is we want the objective function to be convex. By looking at hinge loss and logistic loss, it penalize more on strongly misclassified instances, and interestingly, it also penalize correctly classified instances if they are weakly classified. It is a really strange design. My question is what are the prices we need to pay by using different \"proxy loss functions\", such as hinge loss and logistic loss?","Creater_id":113777,"Start_date":"2016-07-07 05:58:38","Question_id":222585,"Tags":["machine-learning","classification","optimization","loss-functions"],"Answer_count":2,"Last_activity":"2016-07-28 18:52:10","Link":"http://stats.stackexchange.com/questions/222585/what-are-the-impacts-of-choosing-different-loss-functions-in-classification-to-a","Creator_reputation":4461}
{"_id":{"$oid":"5837a58aa05283111e4d7288"},"View_count":24,"Display_name":"Bg1850","Question_score":0,"Question_content":"I have a dataset of customer transactions (multiple customers,multiple transactions) and based on the historical data, I want to know when a new credit(+ve) transaction arrives if its unusual for that particular customer .The transaction data comes with 4 fieldscustomer_no, transaction_type, date_vale,amountsome transactions_types such as salary,interest (with similar amounts ,not exactly same everytime ) occur in regular interval and other transaction_types dont have any regular behaviour for any particular customer .for now what I am doing is calculating mean,median,standard deviation and mad for each customer from the historical data and calculating z score and robust z score for each transaction (for each customer_no ) .Few of the caveats that I want to mentionall customers dont have same no of data and dont have data spanning everyday of the time period (you can think of this like a normal bank statement for a person , you dont have transactions everyday )The historical data is not labelled ,that is no field pointing which transaction are unusual and which are notso my goal is to find characteristic of unusual credit transactions from historical data and find if new transactions are unusual or not based on those characteristicsI am looking for suggestions on building this model .Please assist","Creater_id":89008,"Start_date":"2016-07-27 16:23:30","Question_id":226020,"Tags":["anomaly-detection"],"Answer_count":1,"Last_activity":"2016-07-28 18:20:27","Link":"http://stats.stackexchange.com/questions/226020/find-abnormal-credit-transactions-based-on-historical-data","Creator_reputation":56}
{"_id":{"$oid":"5837a58aa05283111e4d7291"},"View_count":19,"Display_name":"Daniel Pinto","Question_score":0,"Question_content":"When one has variables that are already in percentage (returns, volatility,etc), what is the interpretation of an irf?e.g. suppose a one unit s.d. shock to x (say 0.04) causes y to respond by 0.05. I would say that this means that a 4 percentage point increase in x leads to a 5 percentage point increase in y. However, I've seen the interpretation of a 4% increase in x leads to a 5% increase in y. I do not understand why this is the case... In which contests would one or the other interpretation be right? I would say that the 4%/5% interpretation (as opposed to percentage point intepretation) only makes sense if both x and y are in logsAlso, I would say that a one percentage point increase in x would lead to a 0.05/0.04=1.25 percentage point increase in y. Does it make sense?","Creater_id":99392,"Start_date":"2016-07-28 17:46:19","Question_id":226200,"Tags":["impulse-response"],"Answer_count":0,"Last_activity":"2016-07-28 17:46:19","Link":"http://stats.stackexchange.com/questions/226200/interpretation-impulse-response-function","Creator_reputation":44}
{"_id":{"$oid":"5837a58aa05283111e4d7293"},"View_count":42,"Display_name":"Ashish Singhal","Question_score":0,"Question_content":"I am using Elasticnet from scikit-learn in python, I've also used Glmnet package in R for prototyping. I want to use weights in Elasticnet which apparently is not available as an option/argument in  Elasticnet in scikit-learn. However it is available in glmnet/elnet in R.Has anyone used weights with elasticnet in python? Any insight on how it can be done.Thanks,Ashish","Creater_id":87183,"Start_date":"2016-07-06 08:07:59","Question_id":222435,"Tags":["regression","machine-learning","python","glmnet","elastic-net"],"Answer_count":1,"Last_activity":"2016-07-28 17:21:48","Link":"http://stats.stackexchange.com/questions/222435/how-to-use-weights-with-elasticnet-regression-in-python","Creator_reputation":6}
{"_id":{"$oid":"5837a58aa05283111e4d72a0"},"View_count":80,"Display_name":"SwingingStrawberry","Question_score":5,"Question_content":"This relates to the use of a continuous variable as a predictor in a multiple regression.If a continuous variable (e.g. age) was measured in a questionnaire but the datafile has placed 'cutoffs' on the variable (it has been censored at the lowest and highest ends) can it still be used as continuous variable?For example, I have a data file of a large dataset and I think for ethical reasons the data collectors had to use \"21 years or below\" as the lowest measure of age and \"60 years or above\" as the higher measure. So someone who was 18 years of age isn't in the file as 18, they are in the file as \"21 years or below\". So my frequencies look like this:\"21 years or below\" - n=102.\"22 years\" - n=28.\"23 years\" - n=16....\"58 years\" - n=8.\"59 ears\" - n=11.\"60 years or above\" - n=62.Can this variable really be considered a continuous variable anymore? Or do I have to create ordinal groups to account for the 'groupings' at the low and high end of data?","Creater_id":116661,"Start_date":"2016-07-28 06:23:43","Question_id":226098,"Tags":["regression","multiple-regression","continuous-data"],"Answer_count":1,"Last_activity":"2016-07-28 16:47:31","Link":"http://stats.stackexchange.com/questions/226098/does-a-continuous-censored-predictor-have-to-be-treated-as-ordinal","Creator_reputation":32}
{"_id":{"$oid":"5837a58aa05283111e4d72ad"},"View_count":8,"Display_name":"mic","Question_score":0,"Question_content":"I am looking for a document (paper, preprint, blog, whatever) that lists the parametric copulas whose densities are known in closed-form (and gives their expressions), and also which mentions which ones have no known closed-form or cannot be expressed in a closed-form.I have not found such document yet.Anyone?","Creater_id":67168,"Start_date":"2016-07-28 16:19:25","Question_id":226193,"Tags":["distributions","multivariate-analysis","references","pdf","copula"],"Answer_count":0,"Last_activity":"2016-07-28 16:19:25","Link":"http://stats.stackexchange.com/questions/226193/closed-form-expressions-for-density-of-parametric-copulas","Creator_reputation":1040}
{"_id":{"$oid":"5837a58aa05283111e4d72af"},"View_count":39,"Display_name":"redgem","Question_score":0,"Question_content":"Let's say I have sales data for each month of the year for the last few years. Sales tend to dip in the summer, and I want to determine if the dip this year (or for a given month) is abnormal.   month   2014     2015     2016       1   7775     8454     7700       2  12122    16229    20978       3  11631    16308    21051       4  11840    16004    21045       5  12108    16800    21807    ...Would an F-test/ANOVA be appropriate for this kind of year-over-year data? If not, what might be?","Creater_id":124902,"Start_date":"2016-07-28 14:00:55","Question_id":226183,"Tags":["time-series","hypothesis-testing","anova","f-test"],"Answer_count":1,"Last_activity":"2016-07-28 15:32:52","Link":"http://stats.stackexchange.com/questions/226183/testing-whether-a-seasonal-dip-in-a-time-series-this-year-is-abnormal-compared-t","Creator_reputation":103}
{"_id":{"$oid":"5837a58aa05283111e4d72bc"},"View_count":32,"Display_name":"ngiann","Question_score":1,"Question_content":"I have real valued function, let's call it , which I would like to maximise the function wrt  and minimise it wrt . After a while I realised that I am looking for a solution that is a saddle point. I have no experience whatsoever with such kind of problems. Could anyone inform me what algorithms there are for dealing with such problems? Also: I am working in Julia so in case anyone knows some implementation in Julia that would help me even further, thanks.","Creater_id":25839,"Start_date":"2016-07-28 14:12:26","Question_id":226185,"Tags":["optimization","maximum","minimum","julia"],"Answer_count":1,"Last_activity":"2016-07-28 15:09:14","Link":"http://stats.stackexchange.com/questions/226185/looking-for-saddle-point-in-scalar-function-with-multiple-parameters","Creator_reputation":229}
{"_id":{"$oid":"5837a58aa05283111e4d72c9"},"View_count":60,"Display_name":"Frank49","Question_score":0,"Question_content":"I'm a mathematical engineering student in Norway and I'm trying to calculate if there has been a change in risk over time, during 10 years of follow-up, for young adults to develop depression.We have a population of 3500 patients that carries a risk gene, which increases the probability of depression in a younger age. We also have controls randomly selected from the general population, and who are matched for age and sex to each patient with the risk gene, but the controls does not have the gene which increases susceptibility to early depression. We follow all patients and matched controls until their first depression and then estimate the hazard ratios for the decline over a total of 10 years of follow-up. I've successfully calculated the hazard ratio and 95% CI for each group.The hazard ratio for the patients with the risk gene is = HR1 0.42 (95% CI, 0.37 to 0.48)The hazard ratio for the matched controls is= HR2 0.50 (95% CI, 0.40 to 0.64)Now, if I were to calculate the difference between these two hazard ratios including the 95% confidence intervals, how would I proceed?According to my statistician, I can simply obtain the point estimate for the new hazard ratio by dividing HR2/HR1=1.19. This means that the point estimate for the difference (or Ratio of Hazard ratios) between the two hazard ratios is 1.19, however, I must also calculate the 95% CI for the new point estimate, which necessitates the standard error i think.  The problem is that I don't know how to calculate the standard error for the new point estimate.All help is much appreciated!","Creater_id":52455,"Start_date":"2016-07-22 04:48:26","Question_id":225099,"Tags":["confidence-interval","cox-model","hazard","ratio"],"Answer_count":0,"Last_activity":"2016-07-28 14:42:45","Link":"http://stats.stackexchange.com/questions/225099/how-to-estimate-the-difference-between-two-different-hazard-ratios-from-the-sam","Creator_reputation":103}
{"_id":{"$oid":"5837a58aa05283111e4d72cb"},"View_count":43,"Display_name":"sambajetson","Question_score":2,"Question_content":"Pretend we have a list of items, e.g. names, and we know it is sorted ALPHANUMERICALLY let's say in ascending order. Given a small set of the first K items, how can we estimate the length of the full list?E.g.: for K=5:The subset:AdrianAnnaCarlDerrickEric...would probably be from a shorter list than the subset:AaronAdalineAdamAdenAlmasince the average alphanumeric distance between names is a lot less in the 2nd subset than the first subset, and since we know there is the same potential finite length for each full list (there is a last possible name, since the last name on the list will start with Z and be something like Zebulon, etc.).I can think of 2 approaches.Use average alphanumeric distance between the items of the subset to estimate the full length, e.g. using some metric like Hamming distance or better, Levenshtein distance.Also include priors by using known name frequencies (a lot more complicated, but probably provides a better estimate).So, can someone please help me to decide on a way to estimate N, the cardinality of the full list, given a subset of the first K names in the alphanumerically sorted list?","Creater_id":83546,"Start_date":"2016-07-28 14:38:01","Question_id":226190,"Tags":["probability","estimation","frequency","puzzle"],"Answer_count":0,"Last_activity":"2016-07-28 14:38:01","Link":"http://stats.stackexchange.com/questions/226190/modified-german-tank-problem-involving-string-distance-metrics","Creator_reputation":136}
{"_id":{"$oid":"5837a58aa05283111e4d72cd"},"View_count":60,"Display_name":"bluepole","Question_score":0,"Question_content":"I have some data as shown below:subject   QM    emotion     yi  s1   75.1017   neutral  -75.928276  s2  -47.3512   neutral -178.295990  s3  -68.9016   neutral -134.753906  s4 -193.6777   neutral -137.988266  s5  -89.8085   neutral  357.732239  s6  151.6949   neutral -342.555511  s7  -66.7561   neutral  -55.791832  s8  176.7803   neutral    1.458443  s9   35.2962   neutral -196.741882 s10  -93.8680   neutral   11.772915 s11   22.2184   neutral  224.331467 s12  -57.7316   neutral  -33.035969 s13  -24.6816   neutral   -8.723662 s14   37.4021   neutral  -66.085762 s15   -8.3483   neutral   87.531853 s16   66.6684   neutral  -98.155365 s17   85.9628   neutral    2.060247  s1   17.2099  negative -104.168312  s2  -53.1114  negative -182.373474  s3  -33.0322  negative -137.420410  s4 -210.9767  negative -144.443954  s5  -19.8764  negative  408.798706  s6  112.5601  negative -375.723450  s7 -138.0572  negative  -68.359787  s8  204.2617  negative   -9.281775  s9   79.2678  negative -197.376678 s10 -135.2844  negative   -9.184753 s11  -62.4264  negative  180.171204 s12  -77.7733  negative  -28.135830 s13   57.8981  negative   20.544859 s14   11.4957  negative  -65.592827 s15   18.5995  negative  124.318390 s16  122.0963  negative  -76.976395 s17  107.1490  negative  -28.010180When I perform the following model with the nlme package in R:mydata \u0026lt;- read.table('clipboard', header=T)summary(lme(yi~QM, random=~1|subject, data=mydata))I see a positive QM effect, which is also statistically significant at the 0.05 level:Fixed effects: yi ~ QM                 Value Std.Error DF   t-value p-value...QM            0.36209   0.08492 16  4.263942  0.0006Similar results for the QM effect can be obtained with two other models:summary(lme(yi~QM*emotion, random=~1|subject, data=mydata))summary(lme(yi~QM*emotion, random=~0+emotion|subject, data=mydata))However, the result from a linear model tells a different story (negative QM effect, which is not statistically significant at the 0.05 level):summary(lm(yi~QM, data=aggregate(mydata[,c('yi', 'QM')], by=list(mydataemotion=='negative',]))Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)...QM           -0.1766     0.4055  -0.436    0.669summary(lm(yi ~ QM, data = mydata[mydata$emotion=='neutral',]))Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)...QM           -0.3584     0.4251  -0.843    0.412Admittedly the data is messy (see the attached plot). But still, what is causing the different assessment between lme() and lm()?","Creater_id":1513,"Start_date":"2016-07-27 14:20:43","Question_id":226001,"Tags":["lme","lm"],"Answer_count":2,"Last_activity":"2016-07-28 13:42:30","Link":"http://stats.stackexchange.com/questions/226001/inconsistent-results-between-linear-model-and-linear-mixed-effects-model","Creator_reputation":1003}
{"_id":{"$oid":"5837a58aa05283111e4d72db"},"View_count":63,"Display_name":"fbrundu","Question_score":1,"Question_content":"I am searching for a classification score, preferably provided by Python scikit-learn, to evaluate classification in a cross-validation routine.This classification score must be suitable for:strong class imbalancemulticlass classificationThe cardinality of the classes is the following:         N Class1  19Class2  34Class3   8Class4  17UpdateI defined a custom scorer based on ROC AUC score from sklearn. Basically I extended it to the multi-class problem by averaging the different scores for each class in a one-vs-all fashion. Is this feasible? Are there drawbacks in this approach?Here is the Python/sklearn code:from sklearn.metrics import roc_auc_scorefrom sklearn.preprocessing import LabelBinarizerdef custom_avg_roc_auc_score(truth, pred):    lb = LabelBinarizer()    lb.fit(truth)    truth = lb.transform(truth)    pred = lb.transform(pred)    return roc_auc_score(truth, pred, average=\"macro\")avg_roc_auc_scorer = make_scorer(custom_avg_roc_auc_score)","Creater_id":29911,"Start_date":"2016-06-28 07:11:15","Question_id":221039,"Tags":["cross-validation","scikit-learn","multi-class","scores","imbalanced"],"Answer_count":1,"Last_activity":"2016-07-28 13:30:14","Link":"http://stats.stackexchange.com/questions/221039/score-for-classification-of-dataset-composed-by-different-class-with-class-imbal","Creator_reputation":289}
{"_id":{"$oid":"5837a58aa05283111e4d72e7"},"View_count":36,"Display_name":"deepseas","Question_score":1,"Question_content":"Not sure if the question is suitable to ask here. Calculating odds ratio with interaction term in R using exp() function:For example, If I take exponent of the coefficients which would give me odds ratio. It works fine for the predictors with no interaction. However, my question is if the interaction term is included and we take exp() function on the coefficient, is the odds ratio for the interaction term (y:z) correct? Thanksx \u0026lt;- sample( c(0,1), 20, replace=TRUE, prob=c(0.1, 0.52))y \u0026lt;- sample( c(0,1), 20, replace=TRUE, prob=c(0.3, 0.52))z \u0026lt;- sample( c(0,1), 20, replace=TRUE, prob=c(0.15, 0.04))df\u0026lt;-data.frame(cbind(x,y,z))model=glm(x~y*z,data=df,family=binomial(link=\"logit\"))summary(model)exp(cbind(Odds_and_OR=coef(model), confint(model)))###################################################output:Call:glm(formula = x ~ y * z, family = binomial(link = \"logit\"), data = df)Deviance Residuals:     Min       1Q   Median       3Q      Max  -1.8930   0.4530   0.6360   0.6681   0.9005  Coefficients:             Estimate Std. Error z value Pr(\u0026gt;|z|)(Intercept)    1.6094     1.0954   1.469    0.142y             -0.2231     1.3509  -0.165    0.869z             -0.9163     1.6432  -0.558    0.577y:z           17.0961  3956.1807   0.004    0.997(Dispersion parameter for binomial family taken to be 1)    Null deviance: 20.016  on 19  degrees of freedomResidual deviance: 19.234  on 16  degrees of freedomAIC: 27.234Number of Fisher Scoring iterations: 16\u0026gt; \u0026gt; exp(cbind(Odds_and_OR=coef(model), confint(model)))Waiting for profiling to be done...            Odds_and_OR         2.5 %   97.5 %(Intercept)         5.0  8.063919e-01 95.79539y                   0.8  3.208724e-02 10.71957z                   0.4  1.106609e-02 13.61758y:z          26590507.7 6.838652e-265       NA","Creater_id":124490,"Start_date":"2016-07-28 12:09:48","Question_id":226167,"Tags":["r","logistic","odds-ratio"],"Answer_count":1,"Last_activity":"2016-07-28 13:23:50","Link":"http://stats.stackexchange.com/questions/226167/in-r-odds-ratio-calculation-using-function-expcoef-is-correct-or-not-for-the","Creator_reputation":16}
{"_id":{"$oid":"5837a58aa05283111e4d72f4"},"View_count":100,"Display_name":"user124716","Question_score":1,"Question_content":"Hello I'm going to be a sophomore in college and planning on taking exam P for actuarial science. One of my weakest topic is probability. What \"entry level\" book would you recommend before I start studying exam P material.I've included a copy of they syllabus, taken from the Society of Actuaries website:General ProbabilitySet functions including set notation and basic elements of probabilityMutually exclusive eventsAddition and multiplication rulesIndependence of eventsCombinatorial probabilityConditional probabilityBayes Theorem / Law of total probabilityRandom Variables with univariate probability distributions (including binomial, negativebinomial, geometric, hypergeometric, Poisson, uniform, exponential, gamma, normal,and mixed)Probability functions and probability density functionsCumulative distribution functionsSums of Independent Random Variables (Poisson and normal)Mode, median, percentiles, and momentsVariance and measures of dispersion (including coefficient of variation)Moment generating functionsTransformationsRandom Variables with multivariate probability distributions (including the bivariatenormal)Joint probability functions and joint probability density functionsJoint cumulative distribution functionsCentral Limit TheoremConditional and marginal probability distributionsMoments for joint, conditional, and marginal probability distributionsJoint moment generating functionsVariance and measures of dispersion for conditional and marginal probabilitydistributionsCovariance and correlation coefficientsTransformations and order statisticsProbabilities and moments for linear combinations of independent random -variables","Creater_id":124716,"Start_date":"2016-07-27 07:25:15","Question_id":225911,"Tags":["probability"],"Answer_count":2,"Last_activity":"2016-07-28 13:23:45","Link":"http://stats.stackexchange.com/questions/225911/what-books-are-good-for-exam-p-for-actuarial-science","Creator_reputation":1}
{"_id":{"$oid":"5837a58aa05283111e4d72f6"},"View_count":43,"Display_name":"siddharth ravi","Question_score":0,"Question_content":"I frequently come across authors in reinforcement learning papers mentioning that some or the other algorithm converges. Do they mean a local convergence or a global convergence? What do they specifically refer to, when they just mention convergence? Take the example of this paper 'Convergence of a Q-learning Variant for Continuous States and Actions' (https://www.jair.org/media/4271/live-4271-7865-jair.pdf). ","Creater_id":124899,"Start_date":"2016-07-28 12:49:56","Question_id":226174,"Tags":["machine-learning","convergence","reinforcement-learning","q-learning"],"Answer_count":1,"Last_activity":"2016-07-28 13:14:50","Link":"http://stats.stackexchange.com/questions/226174/authors-frequently-mention-the-convergence-of-their-reinforcement-learning-algor","Creator_reputation":1}
{"_id":{"$oid":"5837a58aa05283111e4d72f8"},"View_count":51,"Display_name":"ultimate8","Question_score":1,"Question_content":"I am trying to solve the equation AX = B in R.I have two matrices, A and B:A = matrix(c(1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,     0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,     0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,     0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,     0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,     0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,     0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,     0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,     0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,     1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0), byrow = T, nrow = 10, ncol = 16)B = matrix(c(1900,2799,3096,3297,3782,4272,7783,10881,7259,30551), nrow = 10, ncol = 1)My question is, how can I solve AX = B and be guaranteed a non-negative solution? The values I am solving for (X1, X2,...X15, X16) are population figures so they cannot be negative. Ideally, they would be integer values as well but one thing at a time.Is there an easy way to do this in R?I have found one way to do it here but it doesn't yield a positive result for all X which is what I'm after.","Creater_id":77249,"Start_date":"2016-07-27 14:08:26","Question_id":225997,"Tags":["r","matrix","linear-algebra"],"Answer_count":1,"Last_activity":"2016-07-28 12:41:33","Link":"http://stats.stackexchange.com/questions/225997/how-to-get-non-negative-solutions-to-matrix-in-r","Creator_reputation":21}
{"_id":{"$oid":"5837a58aa05283111e4d72fa"},"View_count":45,"Display_name":"MasterPValue","Question_score":2,"Question_content":"When setting up the Granger causality (Wald) test, I determine the optimum lag using information criteria AIC, BIC, FPE to test for causality. What does the lag selection really mean?Does the selected lag describe the delay of the Granger cause effect? That is, if I select , does that mean that  Granger-causes  with a delay of 6 time units (say, weeks)?","Creater_id":124871,"Start_date":"2016-07-28 08:55:08","Question_id":226128,"Tags":["var","lags","granger-causality"],"Answer_count":1,"Last_activity":"2016-07-28 12:29:55","Link":"http://stats.stackexchange.com/questions/226128/lag-selection-and-granger-causality-is-the-maximum-lag-the-causal-lag","Creator_reputation":13}
{"_id":{"$oid":"5837a58aa05283111e4d72fc"},"View_count":36,"Display_name":"Simon","Question_score":0,"Question_content":"Let's say that we have 2 data samples  and , containing  samples each, which are generated by two random variables  and . If I apply the student's T-test and apply a value below 0.01, can I formulate the following statement?Data samples  and  are significantly different by a threshold of .Otherwise, how can I correctly interpret this with a statement?","Creater_id":5228,"Start_date":"2016-07-28 11:30:14","Question_id":226160,"Tags":["t-test","p-value"],"Answer_count":2,"Last_activity":"2016-07-28 12:18:19","Link":"http://stats.stackexchange.com/questions/226160/how-to-interpret-the-students-t-test","Creator_reputation":166}
{"_id":{"$oid":"5837a58aa05283111e4d72fe"},"View_count":11,"Display_name":"Grace Carroll","Question_score":0,"Question_content":"I have a continuous outcome variable (levels of cortisol) with several categorical predictor variables (e.g. injury severity level). I am checking that my data meets the assumptions of a mixed model analysis. How do I check for normality of residuals in a mixed model analysis when all predictor variables are categorical? Can categorical variables have residuals? The only demos I can find on this use continuous predictors in the examples and I can't find any clear answers from other posts on the site. I am using SPSS to analyse the data.  ","Creater_id":30409,"Start_date":"2016-07-28 12:02:12","Question_id":226164,"Tags":["mixed-model","residuals","assumptions","normality"],"Answer_count":0,"Last_activity":"2016-07-28 12:02:12","Link":"http://stats.stackexchange.com/questions/226164/normality-of-residuals-in-a-mixed-model-analysis-when-all-predictor-variables-ar","Creator_reputation":71}
{"_id":{"$oid":"5837a58ba05283111e4d7300"},"View_count":69,"Display_name":"SKM","Question_score":2,"Question_content":"Let  be realizations of a random variable . Let  be realizations of a random variable .   The distribution of  and  is  and  and they are independent of each other. How can I find the pdf of the euclidean distance /difference between the two initial conditions i.e., ","Creater_id":21160,"Start_date":"2016-07-27 17:18:29","Question_id":226024,"Tags":["probability","random-variable"],"Answer_count":1,"Last_activity":"2016-07-28 11:36:38","Link":"http://stats.stackexchange.com/questions/226024/how-to-find-the-pdf-of-the-difference-beween-random-variables","Creator_reputation":48}
{"_id":{"$oid":"5837a58ba05283111e4d7302"},"View_count":28,"Display_name":"Matthias","Question_score":1,"Question_content":"Since I run into complete separation with logistic regression I try to run a penalized logistic regression for a binomial response variable. It doesn't seem to work for my data. In an example that works a plot of the deviance vs. the lambda value looks like this:The green point shows the point of minimum deviance, and the blue one shows the minimum deviance plus no more than one standard deviation.In this case I can choose one of the two points. However in my data set it looks like this:1) What could be the reason that shrinkage doesn't work? Apparently the model suggests to use lambda = 0. Unfortunately I have no clue why this is the case.For the interested reader I provide more data below:Data used: my data is highly imbalanced, shows complete separation and has two predictors that are linear correlated. These two are the ones that lead to complete separation.Another plot to analyse is to look at a trace plot below for an example where it works. The trace plot shows nonzero model coefficients as a function of the regularization parameter Lambda. Because there are 32 predictors and a linear model, there are 32 curves. As Lambda increases to the left, lassoglm sets various coefficients to zero, removing them from the model.However again my data having four features (two correlated) doesn't yield a useful plot:","Creater_id":121353,"Start_date":"2016-07-28 05:42:05","Question_id":226091,"Tags":["generalized-linear-model","convergence","regularization","hauck-donner-effect"],"Answer_count":0,"Last_activity":"2016-07-28 11:36:25","Link":"http://stats.stackexchange.com/questions/226091/regularized-generalized-linear-model","Creator_reputation":50}
{"_id":{"$oid":"5837a58ba05283111e4d7304"},"View_count":11,"Display_name":"Lena","Question_score":1,"Question_content":"I am currently reviewing a paper and would like to reproduce their statistical tests in order to see if what they do is accurate.The experiment has 2 factors: four soil types and five levels of glucose addition, which results in 20 different treatments. Each treatments is replicated 3 times (total of 60 replicates). They do provide the mean and standard error for soil organic matter respiration (the variable I'm interested in) for each of the 20 treatments. My question is: can I reproduce the ANOVA results based on this summary data?So far I'm trying to do it manually: I have already calculated the sum of squares for the first factor (soil type), the sum of squares for the second factor (glucose addition), and the sum of squares within - but I'm not sure whether I can also calculate the Total sum of squares without the original data.If anyone knows about an R package that can do this, that would of course also be appreciated.","Creater_id":124893,"Start_date":"2016-07-28 11:32:07","Question_id":226162,"Tags":["anova","summary-statistics"],"Answer_count":0,"Last_activity":"2016-07-28 11:32:07","Link":"http://stats.stackexchange.com/questions/226162/reproducing-two-way-anova-results-based-on-treatment-means-and-standard-errors","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d7306"},"View_count":69,"Display_name":"user112638","Question_score":0,"Question_content":"so for my research project I am investigating effect a blood pressure diagnosis has on alcohol consumption frequency and physical activity. I have constructed 2 different models for both and I will be using xtologit command as I have panel data.Please see my output below. My DV is alcohol_freq which is an ordinal variable of 4, which measures days per week the individual drinks. (0-1 1-2- 2 3-5 -3 6-7 -4). My independent variables are gender (1 is female), net financial wealth, education level, waves( 3 4 5 6) (baseline wave is 2).. xtologit alcohol_freq ragender i.wave i.wealth_group i.raeduc_e   if r2hibps==1, vce(robust)------------------------------------------------------------------------------------------                         |               Robust            alcohol_freq |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------------------+----------------------------------------------------------------                ragender |  -1.779655   .4354061    -4.09   0.000    -2.633036    -.926275                         |                    wave |                      3  |  -.2776735   .1835768    -1.51   0.130    -.6374775    .0821305                      4  |  -.3572538    .193667    -1.84   0.065    -.7368342    .0223266                      5  |  -.7242535   .2117997    -3.42   0.001    -1.139373   -.3091337                      6  |  -.8055891   .2289829    -3.52   0.000    -1.254387   -.3567908                         |            wealth_group |                      2  |   1.923992   .6102358     3.15   0.002     .7279517    3.120032                      3  |   3.108671   .7094336     4.38   0.000     1.718207    4.499135                      4  |   4.251541   .7972072     5.33   0.000     2.689043    5.814038                         |                raeduc_e | 3.high-school graduate  |   1.213114   .5658486     2.14   0.032     .1040715    2.322157         4.some college  |   1.909918   .5798655     3.29   0.001     .7734019    3.046433    5.college and above  |   2.508408   .7991029     3.14   0.002     .9421947    4.074621-------------------------+----------------------------------------------------------------                   /cut1 |  -1.474018   .8762496    -1.68   0.093    -3.191435    .2434001                   /cut2 |   1.684593   .8791786     1.92   0.055    -.0385653    3.407752                   /cut3 |   4.320586   .8989051     4.81   0.000     2.558764    6.082408-------------------------+----------------------------------------------------------------               /sigma2_u |    14.7487   2.156123                      11.07429    19.64226------------------------------------------------------------------------------------------So from my output I guess I can conclude as time went on the no. of days of drinking per week decreased for my sample, and this became stat significant in wave 5.However I am confused by the positive coefficients for wealth group, education. Does this mean having a higher net wealth would increase frequency of drinking per week over the study? Or I am interpreting this wrong? Any help would be appreciated. Thanks","Creater_id":112638,"Start_date":"2016-07-28 10:45:03","Question_id":226148,"Tags":["panel-data","ordered-logit"],"Answer_count":1,"Last_activity":"2016-07-28 11:31:50","Link":"http://stats.stackexchange.com/questions/226148/how-to-interpret-xtologit-output","Creator_reputation":31}
{"_id":{"$oid":"5837a58ba05283111e4d7308"},"View_count":18,"Display_name":"becko","Question_score":3,"Question_content":"The central moments of a probability distribution  are defined as:\\theta_n = \\langle (x - \\langle x \\rangle)^n \\rangle while the non-central moments are the standard:\\mu_n = \\langle x^n \\rangle By the binomial theorem, we have:\\theta_n = \\sum_{k=0}^n \\binom{n}{k}(-1)^{n-k} \\mu_k \\mu_1^{n-k}which allows us to compute the central moments from the non-central moments. Is there an inverse to this expression, giving the non-central moments  from the central moments ?","Creater_id":5536,"Start_date":"2016-07-28 09:52:04","Question_id":226138,"Tags":["moments"],"Answer_count":1,"Last_activity":"2016-07-28 11:25:37","Link":"http://stats.stackexchange.com/questions/226138/converting-central-moments-to-non-central-moments-and-back","Creator_reputation":712}
{"_id":{"$oid":"5837a58ba05283111e4d730a"},"View_count":17,"Display_name":"Qwertford","Question_score":0,"Question_content":"Suppose I recorded the data for the stock price movements every second for 1 minute. I will end up with 60 samples of 1 second price movements. But Now, if i become interested instead, in the stock movements every 10 seconds, can I use (1st second result-11th second result), (2nd second result - 12th second result),... to get about 30 10 second samples?  Is this wrong, because even though there are more samples, the samples are showing identical information, rather than bringing more information? What would be the best way of capturing 10 second data? sampling every 10 seconds or sampling 10 seconds of data at random times?","Creater_id":86977,"Start_date":"2016-07-28 11:21:58","Question_id":226158,"Tags":["sampling","sample-size"],"Answer_count":0,"Last_activity":"2016-07-28 11:21:58","Link":"http://stats.stackexchange.com/questions/226158/how-many-10-second-datas-samples-are-there-in-a-60-1-second-sample","Creator_reputation":38}
{"_id":{"$oid":"5837a58ba05283111e4d730c"},"View_count":31,"Display_name":"user2917781","Question_score":0,"Question_content":"Does anybody know of an R package that produces a p-value for point biserial correlations? I've tried all of the major packages that I know (with some help from Google) and haven't found any. If a package doesn't come to mind, is there some way that I can intuitively calculate the p-value?Thanks everyone!","Creater_id":82681,"Start_date":"2016-07-28 11:20:21","Question_id":226157,"Tags":["r","correlation"],"Answer_count":0,"Last_activity":"2016-07-28 11:20:21","Link":"http://stats.stackexchange.com/questions/226157/p-value-for-point-biserial-corrleation-in-r","Creator_reputation":46}
{"_id":{"$oid":"5837a58ba05283111e4d730e"},"View_count":19,"Display_name":"Vindication09","Question_score":3,"Question_content":"If Cook's distance can only be used for least squares regression, what are some alternatives that will give me a similar plot for a Gamma model or any regression model from the exponential family? ","Creater_id":116069,"Start_date":"2016-07-28 06:46:08","Question_id":226105,"Tags":["regression","computational-statistics","diagnostic","exponential-family"],"Answer_count":1,"Last_activity":"2016-07-28 11:20:09","Link":"http://stats.stackexchange.com/questions/226105/can-cooks-distance-plot-only-be-used-for-least-squares-regression","Creator_reputation":23}
{"_id":{"$oid":"5837a58ba05283111e4d7310"},"View_count":758,"Display_name":"pir","Question_score":3,"Question_content":"Say that I use an RNN/LSTM to do sentiment analysis, which is a many-to-one approach (see this blog). The network is trained through a truncated backpropagation through time (BPTT), where the network is unrolled for only 30 last steps as usual. In my case each of my text sections that I want to classify are much longer than the 30 steps being unrolled (~100 words). Based on my knowledge BPTT is only run a single time for a single text section, which is when it has passed over the entire text section and computed the binary classification target, , which it then compares to the loss function to find the error.The gradients will then never be computed with regards to the first words of each text section. How can the RNN/LSTM then still adjust its weights to capture specific patterns that only occur within the first few words? For instance, say that all sentences marked as  start with \"I love this\" and all sentences marked as  start with \"I hate this\". How would the RNN/LSTM capture that when it is only unrolled for the last 30 steps when it hits the end of a 100-step long sequence?","Creater_id":29025,"Start_date":"2015-08-17 06:57:24","Question_id":167482,"Tags":["neural-networks","deep-learning","natural-language","backpropagation"],"Answer_count":1,"Last_activity":"2016-07-28 11:18:23","Link":"http://stats.stackexchange.com/questions/167482/capturing-initial-patterns-when-using-truncated-backpropagation-through-time-rn","Creator_reputation":638}
{"_id":{"$oid":"5837a58ba05283111e4d7312"},"View_count":25,"Display_name":"tluh","Question_score":1,"Question_content":"Let's say that I have one population that can be divided by a 2-level factor.  I run MDS twice (using prcomp() {stats} in R), using the 2-level factor to separate my subjects.If I reduce to two dimensions, I now have two plots. Let's say that on each of the MDS plots, I have 5 points (with the same labels). I can calculate a total of 10 (5x4/2) distances for plot 1, and 10 distances for plot 2.Is it meaningful to quantitatively compare the distances of plot 1 and plot 2?","Creater_id":83496,"Start_date":"2016-07-28 11:01:13","Question_id":226153,"Tags":["multidimensional-scaling"],"Answer_count":1,"Last_activity":"2016-07-28 11:16:57","Link":"http://stats.stackexchange.com/questions/226153/can-the-distances-of-two-mds-plots-be-quantitatively-compared","Creator_reputation":28}
{"_id":{"$oid":"5837a58ba05283111e4d7314"},"View_count":1372,"Display_name":"a.powell","Question_score":21,"Question_content":"In a question I asked recently, I was told that it was a big \"no-no\" to extrapolate with loess. But, in Nate Silver's most recent article on FiveThirtyEight.com he discussed using loess for making election predictions.He was discussing the specifics of aggressive versus conservative forecasts with loess but I am curious as to the validity of making future predictions with loess? I am also interested in this discussion and what other alternatives there are that might have similar benefits to loess.","Creater_id":119338,"Start_date":"2016-07-28 06:57:40","Question_id":226108,"Tags":["time-series","forecasting","predictive-models","loess","politics"],"Answer_count":1,"Last_activity":"2016-07-28 10:45:01","Link":"http://stats.stackexchange.com/questions/226108/explanation-of-what-nate-silver-said-about-loess","Creator_reputation":538}
{"_id":{"$oid":"5837a58ba05283111e4d7316"},"View_count":44,"Display_name":"Ramkishore Swaminathan","Question_score":0,"Question_content":"I am trying to model data with multivariate Hawkes distribution. Take the below example. I am able to compute likelihood but dont know how to maximize it.library(hawkes)lambda0 \u0026lt;- c(0.2,0.2)alpha   \u0026lt;- matrix(c(0.5,0,0,0.5),byrow=TRUE,nrow=2)beta    \u0026lt;- c(0.7,0.7)history \u0026lt;- simulateHawkes(lambda0,alpha,beta,3600)l       \u0026lt;- likelihoodHawkes(lambda0,alpha,beta,history)How do I maximize this likelihood so that I can find the best lambda, alpha and beta parameters?I am not able to find any library or function calls for doing this. Can anyone help?","Creater_id":115714,"Start_date":"2016-07-28 07:42:52","Question_id":226117,"Tags":["r","maximum-likelihood","optimization","expectation-maximization"],"Answer_count":1,"Last_activity":"2016-07-28 10:42:55","Link":"http://stats.stackexchange.com/questions/226117/em-algorithm-for-maximizing-the-likelihood-of-multivariate-hawkes-process","Creator_reputation":1}
{"_id":{"$oid":"5837a58ba05283111e4d7318"},"View_count":885,"Display_name":"PascalvKooten","Question_score":3,"Question_content":"Imagine we regress y on x1...x4. Now, we want to find out if x5 is a stronger predictor than x6 (given the other variables). Note that all variables are scaled.Would it be okay to use the residuals to see which one would be a stronger predictor?y \u0026lt;- scale(rnorm(1000))x \u0026lt;- scale(replicate(6, rnorm(1000)))# Method 1:res = lm(y ~ x[,1:4])$residuals lm(res ~ x[,5] - 1)lm(res ~ x[,6] - 1)The goal here is to identify which variable is a stronger predictor (taking into account the other variables). As far as I can see, this indeed delivers different results from simply correlating x5 and x6 with y (method 2) in turn.The benefit of doing it this way is that it would be less computationally expensive (with high amount of predictors) to compute rather than computing the whole equation. Also, the results still differ a bit from when we would compute them all at once, that is lm(y ~ x[,1:5]) and lm(y ~ x[,c(1:4,6)]) separately (method 3).results                   x5           x6explain residuals    -0.003126777 -0.008349196cor(x[,5:6], y)      -0.003499607 -0.006773532explain at once      -0.003137124 -0.008407007So: is there any kind of a shortcut that could produce the latter model without having to compute the large model?What would be the advice for feature selection? Is explaining the residuals a good approximation of how good the model would be including x5 or x6 from the start?Added some benchmark results (10000x1002 matrix):              x1001      x1002     time takenmethod1     -0.01515   -0.00967       16s  method2     -0.01690   -0.01170    0.001smethod3     -0.01689   -0.01068       32sThis might actually suggest that cor() might be good enough, or does this have to do with the fact that here all x's are independent of each other, while in reality this is most likely not the case?","Creater_id":16175,"Start_date":"2014-02-19 03:49:57","Question_id":87130,"Tags":["regression","feature-selection","residuals"],"Answer_count":1,"Last_activity":"2016-07-28 10:39:52","Link":"http://stats.stackexchange.com/questions/87130/linear-regression-for-feature-selection","Creator_reputation":708}
{"_id":{"$oid":"5837a58ba05283111e4d731a"},"View_count":28,"Display_name":"Kenny","Question_score":1,"Question_content":"I have a warehouse that packages units and ships them. Any number of units can go into the same package, including only 1 unit. I have a forecast for number of units and units per package (UPP). From it, I calculate the total number of expected packages I will ship. As the forecast and actuals vary each week, when the expected number of packages differs from actual packages shipped, how do I determine how much blame to put on each of the two forecasts? Obviously, if one forecast has 0% error, then the other forecast is 100% to blame. But when they both have error, then I get stuck. Sample data:150 = Units Forecast100 = Units Actual2 = UPP Forecast2.5 = UPP Actual75 = Packages Forecast (calcuated, Units Forecast / UPP Forecast)40 = Packages Actual (calculated, Units Actual / UPP Actual)Here's a google sheet with the sample data. Please feel free to add answers to it.","Creater_id":124812,"Start_date":"2016-07-27 23:36:24","Question_id":226052,"Tags":["forecasting","error","weights"],"Answer_count":1,"Last_activity":"2016-07-28 10:39:13","Link":"http://stats.stackexchange.com/questions/226052/root-causes-of-error-in-a-forecast-consisting-of-two-multiplicative-factors","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d731c"},"View_count":58,"Display_name":"NoChance","Question_score":0,"Question_content":"I was in the process of analyzing the data below. The first 2 columns are correct. They represent a point in time and a value of a stock at that point.I decided to calculate the probability surrounding each value and calculated the average to be the Sum(price)/16 =4.6875. I calculated the mean as Sum(price * P(value=price)=E(y) and took the average of that last column. I expected that these to values to be close but E(y) was found to be 0.7617.I want to know the following:  Do the calculation look right (just take a single random row, I don't expect anyone to waste time checking all the numbers of course).Why is E(y) so different from AVG(y)? What is the statistical significance of that?The list of y values (2nd column) is:  3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 3, 1, 3, 7, 4, 6The entire table is:Time Price.y N.gt.y N.eq.y N.eq.y.1 P.gt.y P.eq.y P.eq.y.1    E.y   1       3     11      4        1 68.75% 25.00%    6.25% 0.7500   2       4      8      3        5 50.00% 18.75%   31.25% 0.7500   3       5      6      2        8 37.50% 12.50%   50.00% 0.6250   4       6      3      3       10 18.75% 18.75%   62.50% 1.1250   5       7      1      2       13  6.25% 12.50%   81.25% 0.8750   6       8      0      1       15  0.00%  6.25%   93.75% 0.5000   7       3     11      4        1 68.75% 25.00%    6.25% 0.7500   8       4      8      3        5 50.00% 18.75%   31.25% 0.7500   9       5      6      2        8 37.50% 12.50%   50.00% 0.6250  10       6      3      3       10 18.75% 18.75%   62.50% 1.1250  11       3     11      4        1 68.75% 25.00%    6.25% 0.7500  12       1     15      1        0 93.75%  6.25%    0.00% 0.0625  13       3     11      4        1 68.75% 25.00%    6.25% 0.7500  14       7      1      2       13  6.25% 12.50%   81.25% 0.8750  15       4      8      3        5 50.00% 18.75%   31.25% 0.7500  16       6      3      3       10 18.75% 18.75%   62.50% 1.1250 ","Creater_id":124872,"Start_date":"2016-07-28 09:08:56","Question_id":226131,"Tags":["expected-value","average","basic-concepts"],"Answer_count":2,"Last_activity":"2016-07-28 10:30:57","Link":"http://stats.stackexchange.com/questions/226131/what-is-the-implication-of-having-the-simple-mean-very-different-from-the-expect","Creator_reputation":103}
{"_id":{"$oid":"5837a58ba05283111e4d731e"},"View_count":17,"Display_name":"SwingingStrawberry","Question_score":0,"Question_content":"This relates to running a multiple regression. In similar studies to mine a descriptive stats table shows odds ratios for each categorical predictor to display the odds of being in that category.To give a small example, say I'm predicting cognitive ability based on breastfeeding (breastfed vs not breastfed), with an additional predictor included being maternal education (no highschool, highschool, degree) - a table in another study would have details on the number of participants' whose mothers had no highschool, highschool or degree with an odds ratio to show whether a kid being in any of these three categories was more/less likely to have been breastfed.An example of a study with such a table is this one available through researchgate - specifically Table 1 in that study.I'm trying to work out what statistical test is used to generate these odds ratios? This is probably very simple but I cannot work it out.","Creater_id":116661,"Start_date":"2016-07-28 10:18:45","Question_id":226143,"Tags":["regression","multiple-regression","odds-ratio"],"Answer_count":0,"Last_activity":"2016-07-28 10:18:45","Link":"http://stats.stackexchange.com/questions/226143/what-statistic-is-used-when-showing-odds-ratios","Creator_reputation":32}
{"_id":{"$oid":"5837a58ba05283111e4d7320"},"View_count":17,"Display_name":"goro","Question_score":0,"Question_content":"Is it a valid statistical procedure to compare the difference between two groups which are different in populations?For example:let';s say we want to study the difference in blood glucose level between two groups of diabetic patients:Group #1 is consisted of 20 patientGroup #2 is consisted of 100 patientsIf t-test or non-parametric tests were able to detect a difference between these two groups in blood glucose level. How can we investigate that this difference is really a real difference between the groups, and it is not just because of the difference in the population.","Creater_id":58684,"Start_date":"2016-07-28 09:37:16","Question_id":226137,"Tags":["t-test"],"Answer_count":0,"Last_activity":"2016-07-28 10:13:05","Link":"http://stats.stackexchange.com/questions/226137/the-difference-in-samples-size","Creator_reputation":367}
{"_id":{"$oid":"5837a58ba05283111e4d7322"},"View_count":24,"Display_name":"VitorH","Question_score":1,"Question_content":"Van der Vaart's \"Asymptotic Statistics\" (Ch 12) contains the quote:  Given a known function , consider the estimation of the \"parameter\" \\theta = Eh(X_{1},\\cdots,X_{r}) In order to simplify the formulas, it is assumed that the function  is permutation-symmetric in its  arguments. (A given  can always be replaced by a symmetric one).Given any function , how can we find its permutation-symmetric equivalent? ","Creater_id":105559,"Start_date":"2016-07-27 10:50:32","Question_id":225955,"Tags":["mathematical-statistics","asymptotics"],"Answer_count":1,"Last_activity":"2016-07-28 09:34:50","Link":"http://stats.stackexchange.com/questions/225955/how-to-symmetrize-a-given-function-u-statistics","Creator_reputation":23}
{"_id":{"$oid":"5837a58ba05283111e4d7324"},"View_count":26,"Display_name":"MikeSimpson","Question_score":2,"Question_content":"I'm looking at capital costs of different battery technologies, with the aim of conducting a meta-analysis of both the published literature and known commercial installations. It's quite common for a range to be quoted, though some authors provide a single-point estimate, and installations provide one value. My task is to provide a single-point descriptor of the population with an associated measure of uncertainty. This will be an input into later modelling work.One option is to collapse the ranges to mid-ranges and treat them as additional single-points, but it seems a shame to throw away information on spread. On the other hand, the number of samples used to produce the ranges aren't quoted, so perhaps I can't do any better. Is this the case, and if not, what methods should I consider? I'm open to both Bayesian and frequentist approaches. I appreciate it's far from ideal conditions for rigorous statistics, and there are a number of other factors working against me (sparse data, nuisance parameters, outliers) but some insights into this aspect alone would be great.","Creater_id":124857,"Start_date":"2016-07-28 09:32:33","Question_id":226134,"Tags":["meta-analysis","interval","point-estimation"],"Answer_count":0,"Last_activity":"2016-07-28 09:32:33","Link":"http://stats.stackexchange.com/questions/226134/combining-single-point-estimates-and-two-point-range-estimates","Creator_reputation":11}
{"_id":{"$oid":"5837a58ba05283111e4d7326"},"View_count":57,"Display_name":"Preeti Ranaware","Question_score":0,"Question_content":"I am using MLP neural network. My question is for training the neural network and testing it how much splitting of data is needed like is there any rule that I always have to split data 70% for training and 30% for testing when I did this my accuracy was not good as when I split it into 10% for training and 90% for testing I got more accuracy... Is this valid?","Creater_id":123778,"Start_date":"2016-07-28 07:48:10","Question_id":226118,"Tags":["machine-learning","neural-networks"],"Answer_count":2,"Last_activity":"2016-07-28 09:28:55","Link":"http://stats.stackexchange.com/questions/226118/how-many-data-should-we-choose-for-training-and-testing-the-neural-network","Creator_reputation":4}
{"_id":{"$oid":"5837a58ba05283111e4d7328"},"View_count":43,"Display_name":"neelshiv","Question_score":3,"Question_content":"I have two groups, \"in\" and \"out,\" and item categories that can be split up among the groups. For example, I can have item category A that is 99% \"in\" and 1% \"out,\" and item B that is 98% \"in\" and 2% \"out.\"For each of these items, I actually have the counts that are in/out. For example, A could have 99 items in and 1 item out, and B could have 196 items that are in and 4 that are out.I would like to rank these items based on the percentage that are \"in,\" but I would also like to give some priority to items that have larger overall populations. This is because I would like to focus on items that are very relevant to the \"in\" group, but still have a large number of items in the \"out\" group that I could pursue.Is there some kind of score that could do this?edit: I should add that I also know the total number of items that are in and out across all categories. I have a pretty complete picture of how many items I have, what categories they are in, and whether or not they are in/out.","Creater_id":64776,"Start_date":"2016-07-27 12:15:39","Question_id":225975,"Tags":["ranking","information-theory","information-retrieval"],"Answer_count":2,"Last_activity":"2016-07-28 09:20:06","Link":"http://stats.stackexchange.com/questions/225975/is-there-a-ranking-metric-based-on-percentages-that-favors-larger-magnitudes","Creator_reputation":160}
{"_id":{"$oid":"5837a58ba05283111e4d732a"},"View_count":82,"Display_name":"sponge_knight","Question_score":1,"Question_content":"Regression to the Mean is a concept in sampling not regression. Why is it not called Sampling to the Mean?","Creater_id":46925,"Start_date":"2016-07-28 06:37:51","Question_id":226104,"Tags":["regression"],"Answer_count":1,"Last_activity":"2016-07-28 08:32:01","Link":"http://stats.stackexchange.com/questions/226104/why-does-regression-to-the-mean-have-nothing-to-do-with-regression","Creator_reputation":1522}
{"_id":{"$oid":"5837a58ba05283111e4d732c"},"View_count":37,"Display_name":"Max Rolfes","Question_score":1,"Question_content":"I have panel data that is structured like the example below only with more variables. I am using R and my goal is pretty straight forward - estimate the effect of the independent variables on my dependent variable.        country   date        dependent     independent type1  independent type2        Germany   01/01/2006  70            30                 0.754        Germany   01/02/2006  72            36                 0.821        ...          Germany   12/31/2016  70            16                 1.214        Italy     01/01/2006  54            30                 0.213        Italy     01/02/2006  59            36                 0.343        ...I assume that there are country specific effects that probably also vary across time in the long run which is why I wanted to include fixed effects and then split my panel into 4 time periods and run a regression that is simply like this:time period 1 lm(dependent ~ independent1+independent2+independent3 ....+country)time period 2 lm(dependent ~ independent1+independent2+independent3 ....+country)...However when skimming trough some books  I became increasingly unsure (and confused) if this is an adequate approach. Thus my two questions would be:Is what I intended to do the (or a) suitable approach to achieve my objective?             Can you recommend some other ways to estimate this? I am also very interested in trying some \"creative\" things as long as they are not way over my head.One more remark.I indicated type 1 and type 2 for the independent variables. While both vary across time the first one does not vary across countries. I am not sure if that is important on the other hand I feel like I am not sure about anything anymore after looking trough those statistics books.Thank you.EDIT: What I mean with \"fixed effects change over time\" There are (for me) unobservable variables. The effect of these variables is of different magnitude for every country however it will also change to some extend over time. I thought that fixed effects might be able to improve my estimation in that they would capture something like \"the average\" effect of these variables for every country. It might be difficult to explain without the economic context. I left it out before, because I thought it might be clearer when I express it in general terms but maybe this helps. I look at the credit default swap (CDS) bond basis which is a spread so simply the difference between the two \"yields\". Now some of this spread I can explain with variables that I am able to proxy for like counter party risk that is involved in CDS etc. However some other parts like \"embedded options in CDS contracts\" I can not observe or proxy for. The impact of these variables will likely be large and also be different for every country. So for example the option value is connected to the default risk of the country thus it will vary over time (as the default probability will vary over time) but especially it will be different for say Greece and Germany.","Creater_id":124823,"Start_date":"2016-07-28 03:06:41","Question_id":226073,"Tags":["r","multiple-regression","panel-data","fixed-effects-model"],"Answer_count":1,"Last_activity":"2016-07-28 08:20:36","Link":"http://stats.stackexchange.com/questions/226073/panel-data-basic-question-on-right-best-approach","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d732e"},"View_count":21,"Display_name":"ziweiguan","Question_score":0,"Question_content":"I can use box-plot to examine data and detect outliers based on box-plot. Most often Box-plot uses a q-range (q3-q1) to define the data boundary and then display \"outliers\". Sometimes I see people use mean (or median) +/- 2*SD (or 3*SD) to restrain the data to be used for analysis. Mean, SD, and Q-range, are all biased by \"outliers\". One way to reduce the bias is to use mean and SD calculated from trimmed data (see, trim the data at both ends, 5% or 10% or other percents). My question is: is using mean and SD from tail trimmed data a good practice? ","Creater_id":114489,"Start_date":"2016-07-28 08:20:09","Question_id":226122,"Tags":["outliers","boxplot","data-cleaning","trimmed-mean"],"Answer_count":0,"Last_activity":"2016-07-28 08:20:09","Link":"http://stats.stackexchange.com/questions/226122/outlier-detection-use-trimmed-data-to-calculate-std","Creator_reputation":36}
{"_id":{"$oid":"5837a58ba05283111e4d7330"},"View_count":37,"Display_name":"bla345","Question_score":2,"Question_content":"When evaluating a model, for example a binary classifier, should the train and test set have 50% + and 50% - label distribution or could the distribution be random? If the distribution is biased in the train/test sets e.g., 80% + and 20%-, the precision/recall scores may not be representative. For example, the model may do well on classifying positive points but may misclassify a lot of negative points. It's recall is high but its precision could still be high because there aren't too many false positives because there are less negative points in the dataset. Is AUC robust metric against such imbalanced distributions? Or is it best to balance the distribution in train/test data in order to compute more accurate precision and recall values? I read this Kaggle forum post: Precision-recall AUC vs ROC AUC for class imbalance problems, but it doesn't discuss the issue I'm raising about dataset distribution.","Creater_id":124606,"Start_date":"2016-07-26 11:50:23","Question_id":225761,"Tags":["unbalanced-classes","precision-recall","auc"],"Answer_count":1,"Last_activity":"2016-07-28 07:56:25","Link":"http://stats.stackexchange.com/questions/225761/what-is-the-effect-of-training-a-model-on-an-imbalanced-dataset-using-it-on-a","Creator_reputation":21}
{"_id":{"$oid":"5837a58ba05283111e4d7332"},"View_count":454,"Display_name":"user75138","Question_score":15,"Question_content":"For a given inference problem, we know that a Bayesian approach usually differ in both form and results from a fequentist approach. Frequentists (usually includes me) often point out that their methods don't require a prior and hence are more \"data driven\" than \"judgement driven\". Of course, Bayesian's can point to non-informative priors, or, being pragmatic, just use a really diffuse prior.My concern, especially after feeling a hint of smugness at my fequentist objectivity, is that perhaps my purportedly \"objective\" methods can be formulated in a Bayesian framework, albeit with some unusual prior and data model. In that case, am I just blissfully ignorant of the preposterous prior and model my frequentist method implies?If a Bayesian pointed out such a formulation, I think my first reaction would be to say \"Well, that's nice that you can do that, but that's not how I think about the problem!\". However, who cares how I think about it, or how I formulate it. If my procedure is statistically/mathematically equivalent to some Bayesian model, then I am implicitly (unwittingly!) performing Bayesian inference.Actual Question BelowThis realization substantially undermined any temptation to be smug. However, I'm not sure if its true that the Bayesian paradigm can accommodate all frequentist procedures (again, provided the Bayesian chooses a suitable prior and likelihood). I know the converse is false.I ask this because I recently posted a question about conditional inference, which led me to the following paper: here (see 3.9.5,3.9.6)They point out Basu's well-known result that there can be more than one ancillary statistic, begging the question as to which \"relevant subset\" is most relevant. Even worse, they show two examples of where, even if you have a unique ancillary statistic, it does not eliminate the presence of other relevant subsets.They go on to conclude that only Bayesian methods (or methods equivalent to them) can avoid this problem, allowing unproblematic conditional inference.It may not be the case that Bayesian Stats  Fequentist Stats -- that's my question to this group here. But it does appear that a fundamental choice between the two paradigms lies less in philosophy than in goals: do you need high conditional accuracy or low unconditional error: High conditional accuracy seems applicable when we have to analyze a singular instance -- we want to be right for THIS particular inference, despite the fact that this method may not be appropriate or accurate for the next dataset (hyper-conditionality/specialization). Low unconditional error is appropriate when if we are willing make conditionally incorrect inferences in some cases, so long as our long run error is minimized or controlled. Honestly, after writing this, I'm not sure why I would want this unless I were strapped for time and couldn't do a Bayesian analysis...hmmm.I tend to favor likelihood-based fequentist inference, since I get some (asymptotic/approximate) conditionality from the likelihood function, but don't need to fiddle with a prior - however, I've become increasingly comfortable with Bayesian inference, especially if I see the prior a a regularization term for small sample inference.Sorry for the aside. Any help for my main problem is appreciated.","Creater_id":null,"Start_date":"2016-07-21 14:25:48","Question_id":225002,"Tags":["bayesian","inference","likelihood","likelihood-ratio","frequentist"],"Answer_count":2,"Last_activity":"2016-07-28 07:54:53","Link":"http://stats.stackexchange.com/questions/225002/are-we-frequentists-really-just-implicit-unwitting-bayesians","Creator_reputation":null}
{"_id":{"$oid":"5837a58ba05283111e4d7334"},"View_count":9825,"Display_name":"Thea","Question_score":38,"Question_content":"Can you suggest some good movies which involve math, probabilities etc? One example is 21. I would also be interested in movies that involve algorithms (e.g. text decryption). In general \"geeky\" movies with famous scientific theories but no science fiction or documentaries. Thanks in advance!","Creater_id":4504,"Start_date":"2011-05-07 04:13:51","Question_id":10459,"Tags":["probability","references"],"Answer_count":20,"Last_activity":"2016-07-28 07:29:47","Link":"http://stats.stackexchange.com/questions/10459/are-there-any-good-movies-involving-mathematics-or-probability","Creator_reputation":1}
{"_id":{"$oid":"5837a58ba05283111e4d7336"},"View_count":123,"Display_name":"Mik meadow","Question_score":1,"Question_content":"I have paired data.The response variable is categorical with levels 1-5.And I have used both McNemar's test and Wilcoxon rank sign test and I get different p-values. For McNemar's test a p-value of 0.0396; and for Wilcoxon rank sign test a p-value of 0.538. before=c(4,3, 5, 3, 5, 4, 3, 3, 3, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5)after=c(4, 4, 5, 5, 4, 3, NA, 5, 3, 4, 5, 3, 4, 4, 5, 5, 5, 4, 4, 5, 4, 5)","Creater_id":57138,"Start_date":"2016-07-28 04:52:04","Question_id":226084,"Tags":["wilcoxon","mcnemar-test"],"Answer_count":2,"Last_activity":"2016-07-28 07:25:57","Link":"http://stats.stackexchange.com/questions/226084/which-test-to-choose-when-wilcoxon-test-and-mcnemars-test-are-different","Creator_reputation":118}
{"_id":{"$oid":"5837a58ba05283111e4d7338"},"View_count":28,"Display_name":"carlogambino","Question_score":1,"Question_content":"Let  and  denote the respective pdf and cdf of a probabilitydistribution on . Consider any natural  and any real such that , and .We want to prove that:These expressions are related to the expected values of order statistics, in particular the highest and second highest order statistics of n-1 entities. We know (by some exogenous theorem) that when  the expressions (LHS and RHS) are identical. So it is sufficient to prove that the expression is increasing in . Taking the derivative of the inequality with respect to , we have:or, rearranging,Attempts to show that this inequality was true proved to be futile. Could someone suggest a strategy as to how one would go about this proof?","Creater_id":49683,"Start_date":"2016-07-28 04:26:39","Question_id":226081,"Tags":["probability","distributions","integral"],"Answer_count":0,"Last_activity":"2016-07-28 07:15:19","Link":"http://stats.stackexchange.com/questions/226081/proving-that-a-complex-expression-of-probabilistic-integrals-is-increasing-in-a","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d733a"},"View_count":24,"Display_name":"cgo","Question_score":2,"Question_content":"I am reading a survey paper on Multiagent reinforcement learning (MARL) by Busoniu, Babuska, De Schutter (IEEE Transactions on Systems, Man and Cybernetics March 2008). Under 'Challenges in MARL', one of the things they put forward as a challenge/difficulty is how to 'specify a goal'. I am not sure what 'goal' in this case means. \"Specifying a good MARL goal in the general stochastic game is a difficult challenge, as the agents' returns are correlated and cannot be maximized independently. Several types of MARL goals have been proposed in the literature, which consider stability of the agents' learning dynamics, adaptation to the changing behavior of the other agents, or both.\" What then is a 'goal'? ","Creater_id":67413,"Start_date":"2016-07-21 09:34:29","Question_id":224964,"Tags":["machine-learning","supervised-learning","reinforcement-learning","game-theory"],"Answer_count":1,"Last_activity":"2016-07-28 07:09:35","Link":"http://stats.stackexchange.com/questions/224964/specifying-a-good-goal-in-multi-agent-rl","Creator_reputation":596}
{"_id":{"$oid":"5837a58ba05283111e4d733c"},"View_count":18,"Display_name":"Folli","Question_score":0,"Question_content":"I'm working with a large dataset (~100k rows) of the following type:Predictor: consists of 200 columns, categorical type, 20 different categories in totalResponse: a pair of integers (can be thought of as a point on a 2D-plot)Example (the three letter words are the predictors [20 different amino acids] and the (-138/-67)-tuple is the response I'd like to predict):ARG     TYR     HIS     THR     (...200 columns...)     ILE     ASP     HIS     ASN     -138/-67I tried using a slightly modified regression random forest approach to feed my data into:Decision nodes are split based on categorical dataMean squared error is used as penalty to choose the best splitBecause I'm working with a pair of integers, I'm using the euclidean distance to calculate the mean and the distance to the mean of all entries within a given split to calculate the mean squared errorThis approach worked quite well using an artificial dataset with a very clearly defined relationship between Predictor variables and response and with few noise.Using my actual dataset (~100k entries), following problem appears (I found this by checking how a single tree will do the splitting on a bootstrapped sample): the dataset will be split in a very unevenly fashion, i.e. every node will split the dataset into approximately 1-5 entries vs the remaining ~100k entries, so that every leaf will only hold 1 or so entry with the exception of the last created leaf that will hold all the remaining ~80k entries (depending on when the splitting is finished due to having reached max. depth).If this random forest is used for prediction of the hold-out-data, virtually all of these entries will match to the last leaves of the trees and thus get assigned the mean of the dataset, rendering the prediction useless.Does anyone have any pointers on what to change, how to fix the problem? Any better suited methods (which still can handle this amount of data)?","Creater_id":80284,"Start_date":"2016-07-28 07:08:56","Question_id":226112,"Tags":["regression","random-forest"],"Answer_count":0,"Last_activity":"2016-07-28 07:08:56","Link":"http://stats.stackexchange.com/questions/226112/regression-on-categorical-variables-using-random-forest-results-in-unbalanced-tr","Creator_reputation":18}
{"_id":{"$oid":"5837a58ba05283111e4d733e"},"View_count":246,"Display_name":"alepfu","Question_score":0,"Question_content":"Given this joint probability distribution table     |   A   |   B   |   C  X    | 0.15  | 0.03  | 0.07notX | 0.05  | 0.25  | 0.45I've to calc the following probabilitiesP(B or X) = ?P(C or notX) = ?So far I got this solution:P(B or X) = 0.15 + 0.03 + 0.07 + 0.25 P(C or notX) = 0.07 + 0.45 + 0.05 + 0.25 but I'm very unsure...","Creater_id":11965,"Start_date":"2012-06-13 10:19:26","Question_id":30391,"Tags":["probability","self-study"],"Answer_count":1,"Last_activity":"2016-07-28 06:39:46","Link":"http://stats.stackexchange.com/questions/30391/simple-joint-probability-distribution-table-calculations","Creator_reputation":1}
{"_id":{"$oid":"5837a58ba05283111e4d7340"},"View_count":15,"Display_name":"N-K Pham","Question_score":1,"Question_content":"I want to do an regression analysis based on a questionnaire survey regarding taking risks. I have 80 respondents from the questionnaire.My dependent variable is a gamble question: where options are such as 1000, 5000 and 10 000 to gamble.My independent variables are such as gender, education, age and income with different answer options.I will put up each individual answers on a excel sheet. When I put them up how should I interpret them? Should I example rank the answer option with number? For example Female=1 Male=2 and for education the same bachelor=1, master=2 and Phd=3 and so on to make it easier? or is it possible to just write the answer as it is. indnividual 1 chose male, bachelor, gambled 1000. Individual 2 chose xxxxxIs it possible to make an multiple regression analysis using this structure? or are there any other structure I can set up the data from the respondents to make and multipple regression analysis?","Creater_id":124697,"Start_date":"2016-07-28 06:31:35","Question_id":226101,"Tags":["multiple-regression","survey"],"Answer_count":0,"Last_activity":"2016-07-28 06:31:35","Link":"http://stats.stackexchange.com/questions/226101/regression-analyis-based-on-questionaire-surveys","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d7342"},"View_count":5,"Display_name":"Daniel Pinto","Question_score":0,"Question_content":"In Baltagi's Econometrics it is said that the number of parameters in a VAR is mg^2+g where g is the number of equations and m is the number of lags. It further says that if the sample size T is small relative to mg^2+g, then you have a problem with degrees of freedom. But since each equation in the var is estimated separately via ols, shouldn't it just matter whether T is small relative to the number of coefficient in each regression as opposed to the total number of parameters in the VAR?","Creater_id":99392,"Start_date":"2016-07-28 06:30:55","Question_id":226100,"Tags":["var"],"Answer_count":0,"Last_activity":"2016-07-28 06:30:55","Link":"http://stats.stackexchange.com/questions/226100/sample-size-number-of-parameters-trade-off-in-a-var","Creator_reputation":44}
{"_id":{"$oid":"5837a58ba05283111e4d7344"},"View_count":16,"Display_name":"Hima Varsha","Question_score":0,"Question_content":"I have the below sample code in which I am using the sklearn(scikit-learn=0.16.1) 20newsgroup dataset:import pandas as pdimport numpy as npfrom sklearn.datasets import fetch_20newsgroupsfrom sklearn.linear_model import LogisticRegressionfrom sklearn.multiclass import OneVsRestClassifierfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics.classification import precision_recall_fscore_supportfrom sklearn import metricsnewsgroups_train = fetch_20newsgroups(subset='train')newsgroups_test = fetch_20newsgroups(subset='test')X_train = newsgroups_train.dataX_test = newsgroups_test.datay_train = list(newsgroups_train.target)y_test = list(newsgroups_test.target)# y_train = [[x] for x in y_train]# y_test = [[x] for x in y_test]vectorizer = TfidfVectorizer(ngram_range = (1,1))X_train = vectorizer.fit_transform(X_train)X_test = vectorizer.transform(X_test)clf = OneVsRestClassifier(LogisticRegression(C=(6), penalty='l1')).fit(X_train, y_train)pred = clf.predict(X_test)overall_precision,overall_recall,overall_fscore,overall_support = precision_recall_fscore_support(y_test,pred,average='weighted')print \"Precision \" + str(overall_precision) + \" \" + \"Recall \" + str(overall_recall)The output obtained is Precision 0.809072982209 Recall 0.806691449814. Now when I uncomment the comments in the above code and convert y_train and y_test from list to list of lists, the output changes to Precision 0.881659860014 Recall 0.631439192777. I understand that by doing it, I am claiming it to be a MultiLabel problem but I do not understand why that should change the precision or recall. My questions:Why is there a change of precision and recall?Which of the both are correct?How do I get a simple single data structure to use for y_train and y_test which can handle both MultiLabel and Single Labelled dataset.","Creater_id":121920,"Start_date":"2016-07-25 05:41:23","Question_id":225505,"Tags":["scikit-learn","natural-language","precision-recall","multi-class"],"Answer_count":1,"Last_activity":"2016-07-28 06:15:39","Link":"http://stats.stackexchange.com/questions/225505/varying-outputs-for-the-same-train-and-test-data","Creator_reputation":103}
{"_id":{"$oid":"5837a58ba05283111e4d7346"},"View_count":66,"Display_name":"ruthy_gg","Question_score":3,"Question_content":"I would like to conduct a forecast based on a time series ARIMA-model with multiple exogenous variables. My time series is monthly unemployment data (in percentage) during several years and my regressors are continuous values of viewership Wikipedia traffic data on several Wikipedia articles. Both, the time series and the regressors, have the same length. How to choose the right regressors to include in the model? Using auto.arima and forecast functions from the \"forecast\" package in R, my first attempt was to order the regressors according to the best resulting MAE when using each one individually. So, I start by using only 1 regressor (the best MAE), then I add the second best regressor, etc. Nevertheless, this post suggests to choose regressors according to significance but this post by Rob Hyndman suggests using AIC. How should I proceed? How do I accept/reject regressors?","Creater_id":124847,"Start_date":"2016-07-28 06:01:16","Question_id":226093,"Tags":["regression","time-series","feature-selection","arima","model-selection"],"Answer_count":1,"Last_activity":"2016-07-28 06:11:44","Link":"http://stats.stackexchange.com/questions/226093/choosing-regressors-for-inclusion-in-regression-with-arma-errors","Creator_reputation":31}
{"_id":{"$oid":"5837a58ba05283111e4d7348"},"View_count":32,"Display_name":"mackbox","Question_score":0,"Question_content":"Suppose X and Y are binary and we assume P(X=1|Y=0)=0;P(X=0|Y=0)=1;P(X=1|Y=1)=theta; and P(X=0|Y=1)=1-thetaDoes the likelihood function look like this?L(theta|X,Y)=0^{#instances where X=1 and Y=0} x 1^{# instances where X=0 and Y=0} x theta^{#instances where X=1 and Y=1) x (1-theta)^(#instances where X=0 and Y=1}If this is correct, how can I deal with the situation when I observe some instances {X=1 and Y=0}? Does the MLE of theta remain the same given that the likelihood function becomes 0?    ","Creater_id":73733,"Start_date":"2016-07-28 04:36:14","Question_id":226083,"Tags":["self-study","mathematical-statistics","maximum-likelihood","likelihood"],"Answer_count":0,"Last_activity":"2016-07-28 06:02:47","Link":"http://stats.stackexchange.com/questions/226083/what-is-the-likelihood-in-this-situation","Creator_reputation":128}
{"_id":{"$oid":"5837a58ba05283111e4d734a"},"View_count":543,"Display_name":"MartinGalilee","Question_score":4,"Question_content":"I have conducted an analysis of covariance, ANCOVA, followed by Bonferroni post-hoc tests, on SPSS. I want to report the differences between my groups in text (APA style) and in a corresponding bar graph. My covariate (a different variable from the DV and IV, not a pretest) was found significant, so the descriptive means and the estimated marginal means (EMM) are different.I want to know which of the means (\"descriptive\" or EMM) is better to report and put in the graph. One way to think about it is that the adjusted means are not real data but simply estimations from the software --nothing my article should be about. However my analysis will be drawn on the conclusions of the ANCOVA, comparing the EMM, so I should focus on the EMM.I have explored various manuals and articles without finding a clear answer --I found everything and the opposite.Should I graph the actual means as presented in the \"descriptives\" table, or should I graph the estimated marginal means?","Creater_id":64195,"Start_date":"2014-12-19 20:48:46","Question_id":129836,"Tags":["ancova","bonferroni","reporting"],"Answer_count":1,"Last_activity":"2016-07-28 05:35:11","Link":"http://stats.stackexchange.com/questions/129836/report-in-ancova-descriptive-means-or-estimated-marginal-means","Creator_reputation":21}
{"_id":{"$oid":"5837a58ba05283111e4d734c"},"View_count":256,"Display_name":"felix000","Question_score":4,"Question_content":"PREFACE: I don't care about the merits of using a cutoff or not, or how one should choose a cutoff. My question is purely mathematical and due to curiosity.Logistic regression models the posterior conditional probability of class A versus class B and it fits a hyperplane where the posterior conditional probabilities are equal. So in theory, I understood that a 0.5 classification point will minimize total errors regardless of set balance, since it models the posterior probability (assuming you consistently encounter the same class ratio). In my real life example, I obtain very poor accuracy using P \u003e 0.5 as my classifying cutoff (about 51% accuracy). However, when I looked at the AUC it is above 0.99. So I looked at some different cutoff values and found that P \u003e 0.6 gave me 98% accuracy (90% for the smaller class and 99% for the bigger class) - only 2% of cases misclassified. The classes are heavily unbalanced (1:9) and it is a high-dimensional problem. However, I allocated the classes equally to each cross-validation set so that there should not be a difference between the balance of classes between model fit and then prediction. I also tried using the same data from the model fit and in predictions and the same issue occurred. I'm interested in the reason why 0.5 would not minimize errors, I thought this would be by design if the model is being fit by minimizing cross-entropy loss. Does anyone have any feedback as to why this happens? Is it due to adding penalization, can someone explain what is happening if so?","Creater_id":70157,"Start_date":"2016-07-27 01:13:45","Question_id":225843,"Tags":["logistic","predictive-models","unbalanced-classes"],"Answer_count":2,"Last_activity":"2016-07-28 05:28:50","Link":"http://stats.stackexchange.com/questions/225843/why-p0-5-cutoff-is-not-optimal-for-logistic-regression","Creator_reputation":38}
{"_id":{"$oid":"5837a58ba05283111e4d734e"},"View_count":22,"Display_name":"kushy","Question_score":1,"Question_content":"I wanted to ask if anyone knows a good book for citing, which handles the topic of \"calculating / approximating the standard deviation for an estimated standard deviation from a finite sample\". It is explained wonderfully in this PDF (https://web.eecs.umich.edu/~fessler/papers/files/tr/stderr.pdf), but I'm not sure if this article is eligible for citation.","Creater_id":124841,"Start_date":"2016-07-28 05:16:58","Question_id":226086,"Tags":["estimation","standard-deviation"],"Answer_count":0,"Last_activity":"2016-07-28 05:16:58","Link":"http://stats.stackexchange.com/questions/226086/citable-source-for-standard-deviation-of-sample-standard-deviation","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d7350"},"View_count":2486,"Display_name":"RDJ","Question_score":8,"Question_content":"While doing some EDA I decided to use a box plot to illustrate the difference between two levels of a factor.The way ggplot rendered the box plot was satisfactory, but slightly simplistic (first plot below). Whilst researching the characteristics of box plots I started experimenting with notches. I understand notches display the CI around the median, and that if two boxes' notches don't overlap there's ‘strong evidence’ – at a 95% confidence level – that the medians differ.In my case (second plot), the notches don't meaningfully overlap. But why does the bottom of the box on the right hand side take that strange form?Plotting the same data in a violin plot didn't indicate anything unusual about the probability density of the corresponding violin. ","Creater_id":72600,"Start_date":"2015-05-09 12:02:16","Question_id":151580,"Tags":["data-visualization","ggplot2","eda"],"Answer_count":1,"Last_activity":"2016-07-28 04:45:33","Link":"http://stats.stackexchange.com/questions/151580/how-to-interpret-notched-box-plots","Creator_reputation":205}
{"_id":{"$oid":"5837a58ba05283111e4d7352"},"View_count":198,"Display_name":"what","Question_score":3,"Question_content":"I guess this is a very basic question, but I find myself unable to find an answer.I want to ask a sample of probands (e.g. N = 467) which one of three presidential candidates they would elect, A, B, or C. I will then have a vector of votes containing a certain number of \"A\"s, \"B\"s and \"C\"s each, that is, something like this: A, B, A, C, C, B, A, A, B, ....When I count the votes, a possible result could be:A: 345B: 52C: 70How do I test whether these vote counts are significantly different from each other? And if possible, how do I calculate effect size?I would be especially happy if you could provide an R function for your answer, if it exists.","Creater_id":14650,"Start_date":"2016-07-25 00:45:41","Question_id":225458,"Tags":["statistical-significance","effect-size"],"Answer_count":2,"Last_activity":"2016-07-28 03:05:55","Link":"http://stats.stackexchange.com/questions/225458/election-result-how-to-test-for-significance","Creator_reputation":96}
{"_id":{"$oid":"5837a58ba05283111e4d7354"},"View_count":1840,"Display_name":"user1137731","Question_score":10,"Question_content":"One of the motivations for the elastic net was the following limitation of LASSO:\"In the p \u003e n case, the lasso selects at most n variables before it saturates, because of the nature of the convex optimization problem. This seems to be a limiting feature for a variable selection method. Moreover, the lasso is not well defined unless the bound on the L1-norm of the coefficients is smaller than a certain value.\" (http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x/full)I understand that LASSO is a quadratic programming problem but also can be solved via LARS or element-wise gradient descent. But I do not understand where in these algorithms I encounter a problem if p \u003e n where p is the number of predictors and n is the sample size. And why is this problem solved using elastic net where I augment the problem to p+n variables which clearly exceeds p.","Creater_id":12733,"Start_date":"2012-09-29 19:53:07","Question_id":38299,"Tags":["regression","optimization","feature-selection","lasso"],"Answer_count":2,"Last_activity":"2016-07-28 03:05:04","Link":"http://stats.stackexchange.com/questions/38299/if-p-n-the-lasso-selects-at-most-n-variables","Creator_reputation":117}
{"_id":{"$oid":"5837a58ba05283111e4d7356"},"View_count":31,"Display_name":"Nick","Question_score":1,"Question_content":"I have a question for a study involving psychology, linguistics, and criminology.  All me to walk you through the labyrinth...Let's say we have a criminal whose original name is John Howard Smith (or to use letters, A B C).That criminal then selects the alias Howard Smith (B C).What statistical test can I use to determine the probability that another offender whose real name matches the pattern of (A B C) will select the same alias pattern (B C)?And extending the question to a group of offenders....Let's now say I have data set with four offenders with the following real names and name patterns: OFFENDERS                         ORIGINAL PATTERNS1.) John Howard Smith        A B C2.) Gregory Thomas             A C3.) Carlos Gomez-Rodriguez  A B-C4.)  A.G. Keating                     A. B. CAnd then I look at the aliases that the above set of offenders used to form my base observations1.) A B C                     becomes        B C    (i.e. Howard Smith)2.) AC                        becomes        C A     (i.e. Thomas Gregory)3.) A B-C                     becomes        A C     (i.e. Carlos Rodriguez)4.) A. B. C                   becomes        A.  C    (i.e. A. Keating)Now I expand my data-set to include hundreds of offenders with their accompanying aliases; and I now determine the frequencies with which offenders whose original names fall within a particular pattern select aliases with particular patterns.  For example, amongst the offenders whose real name has the pattern A B C, I find that50% select an alias with the pattern B C (i.e. Howard Smith) TRANSFORMATION TYPE 110% select an alias with the pattern A C  (i.e. John Smith) TRANFORMATION TYPE 230% select an alias with the pattern B A C  (i.e. Howard John Smith) TRANSFORMATION TYPE 310% select an alias with the pattern B. C   (i.e. H. Smith) TRANSFORMATION TYPE 4I now do precisely the same for all of the different original name patterns I find in my original data-set.Once I have this set of observations, what analysis would I need to use to determine the probability that a  new offender, who was not a part of my originally observed set, will take on an alias that follows Transformation Type 1 as opposed to Transformation Type 2, etc.??? Cheers,Nick","Creater_id":124829,"Start_date":"2016-07-28 03:01:23","Question_id":226071,"Tags":["probability","prediction"],"Answer_count":0,"Last_activity":"2016-07-28 03:01:23","Link":"http://stats.stackexchange.com/questions/226071/statistical-creativity-needed-for-a-study-in-criminal-behaviour-help","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d7358"},"View_count":5340,"Display_name":"MarvMind","Question_score":12,"Question_content":"In most Tensorflow Code I have seen Adam Optimizer is used with a constant Learning Rate of 1e-4 (i.e. 0.0004). The Code usually looks the following:...build the model...# Add the optimizertrain_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)# Add the ops to initialize variables.  These will include # the optimizer slots added by AdamOptimizer().init_op = tf.initialize_all_variables()# launch the graph in a sessionsess = tf.Session()# Actually intialize the variablessess.run(init_op)# now train your modelfor ...:  sess.run(train_op)I am wondering, whether it is useful to use exponential decay when using adam optimizer, i.e. use the following Code:...build the model...# Add the optimizerstep = tf.Variable(0, trainable=False)rate = tf.train.exponential_decay(0.15, step, 1, 0.9999)optimizer = tf.train.AdamOptimizer(rate).minimize(cross_entropy, global_step=step)# Add the ops to initialize variables.  These will include # the optimizer slots added by AdamOptimizer().init_op = tf.initialize_all_variables()# launch the graph in a sessionsess = tf.Session()# Actually intialize the variablessess.run(init_op)# now train your modelfor ...:  sess.run(train_op)Usually people use some kind of decay, for Adam it seems uncommon. Is there any theoretical reason for this? Can it be useful to combine Adam optimizer with decay?","Creater_id":107535,"Start_date":"2016-03-05 01:22:01","Question_id":200063,"Tags":["machine-learning","neural-networks","gradient-descent","tensorflow"],"Answer_count":2,"Last_activity":"2016-07-28 02:51:17","Link":"http://stats.stackexchange.com/questions/200063/tensorflow-adam-optimizer-with-exponential-decay","Creator_reputation":161}
{"_id":{"$oid":"5837a58ba05283111e4d735a"},"View_count":27,"Display_name":"ihadanny","Question_score":0,"Question_content":"I have data that consists of lab test results for patients, cancer dates for those who got cancer, and time till death/censor from that cancer. This cancer is both rare (most of the patients never get it) and mostly non-lethal (even for the patients that do get it, most live with it for many years).I would like to predict a risk score for patients before they have cancer, which will answer the question: \"according to my lab tests, what's the risk for me to get a lethal cancer?\". I thought of 2 ways to get the most from my data, and using all populations (very lethal cancer, medium and non-lethal cancer and healthy):Survival analysis (training a model such as a random survival forest). For the cancerous patients I would enter their true death/censor time, and for healthy patients I would enter censor with the maximum follow-up time.Regression (training a model such as xgboost). The label would be some risk level such as: healthy=0, death-from-cancer-within-more-than-3-years=1, death-from-cancer-within-0-3-years=2. This method would require estimating time-to-death for censored-out cancer patients, e.g. by using a mean death time for the cancer patients with a recorded death-time.Which of these 2 ways is preferable (if any)? Is it valid at all to mix healthy with cancer and ask that risk/survival question? And most importantly - how can I measure which model is better? By using concordance index? - is it valid to assume healthy patients are censored-out in max follow-up time, as they are substantially different from patients that got cancer but lived with it for max follow-up time..","Creater_id":37793,"Start_date":"2016-07-28 00:38:51","Question_id":226056,"Tags":["predictive-models","survival","concordance"],"Answer_count":1,"Last_activity":"2016-07-28 02:43:22","Link":"http://stats.stackexchange.com/questions/226056/using-non-cancerous-patients-to-predict-survival-time-from-cancer","Creator_reputation":280}
{"_id":{"$oid":"5837a58ba05283111e4d735c"},"View_count":123,"Display_name":"BarocliniCplusplus","Question_score":3,"Question_content":"I am trying to build a multilinear regression with predictor variables that likely are correlated. I understand that this is a problem, due to overlapping explanations of data. I think I have a method that may get around this, but would like to see if it is valid.The idea, simply put, is to do the following:Perform a linear regression and find the residuals\\hat{y_1}=m_1\\hat{x_1}+b_1 r_1=y_1-\\hat{y_1}Calculate a least squares regression between the residual of the prior regression and the next predictor variable. For example, \\hat{r_1}=m_2\\hat{x_2}+b_2 r_2=r_1-\\hat{r_1}Repeat the idea behind step 2 for  variables. This can be written as \\hat{r}_{i-1}=m_i\\hat{x}_i+b_i r_i=r_{i-1}-\\hat{r}_{i-1}The idea behind this is that the multilinear regression could be written as \\hat{y}_{MLR}=\\sum\\limits_{j=1}^M m_j\\hat{x}_j+b_jMy questions behind this is:Will it work?Does it truly bypass the problem of multicollinearity?Is there any unforeseen problems with this that are being overlooked?","Creater_id":85843,"Start_date":"2016-07-27 09:43:09","Question_id":225945,"Tags":["regression","multiple-regression","multicollinearity"],"Answer_count":1,"Last_activity":"2016-07-28 02:20:59","Link":"http://stats.stackexchange.com/questions/225945/multilinear-regression-with-multicollinearity-residual-regression","Creator_reputation":118}
{"_id":{"$oid":"5837a58ba05283111e4d735e"},"View_count":96,"Display_name":"Mark Heckmann","Question_score":4,"Question_content":"I have elicited 10 attributes from  subjects. Each subject rank ordered his own 10 attributes from the most to the least important one. I am interested in the relation between the order of elicitation (i.e. was it the 1st, 2nd, etc. elicited attribute) and the importance ranking (1, ..., 10). The hypothesis is, that attributes elicited early have higher importance ranks.As the importance rankings are nested within subjects, I am not sure which models would be suitable to test this hypothesis. Any ideas?Update 1: Sample dataBelow I create some sample data. The sample has N=60 individuals. For each individual, 10 attributes were elicited and ranked with regard to importance (no ties).library(reshape2)library(dplyr)set.seed(1)N \u0026lt;- 60d \u0026lt;- data.frame(id=rep(1:N, each=10),     # subject ID                position = 1:10)          # position of attributed \u0026lt;- d %\u0026gt;%   group_by(id) %\u0026gt;%                        # generate rank based on position + noise  mutate(importance_rank = rank(position + rnorm(n(), sd=2)))head(d)     id position importance_rank1     1        1               12     1        2               33     1        3               24     1        4               65     1        5               56     1        6               4Tabulating the data shows the dependency which I want to model/test.dcast(d, position ~ importance_rank)   position  1  2  3  4  5  6  7  8  9 101         1 30 16  7  3  3  1  0  0  0  02         2 18 11 14 11  2  3  1  0  0  03         3  5 14 16 13  7  5  0  0  0  04         4  4 13 11 11 14  5  0  2  0  05         5  1  4  7 13  9 10  8  4  2  26         6  2  0  2  5 10 10 17  5  8  17         7  0  2  3  3  5 17 15  4  8  38         8  0  0  0  1  5  4 10 18 16  69         9  0  0  0  0  4  5  7 17 13 1410       10  0  0  0  0  1  0  2 10 13 34Update 2: A model suggestionThis (mostly mathematical) book covers a variety of models for rank data. It appears, that the rank-ordered logit model (ROL) AKA exploded logit model is one model option to cover such scenarios. A more gentle article on ROL can be found here, and a nice blog post here. The model can be estimated using the mlogit R package. The vignette also has an ROL example (p.25ff.) What I tried: library(mlogit)md \u0026lt;- mlogit.data(d, shape = \"long\", choice = \"importance_rank\",                  alt.var = \"position\", ranked = TRUE)summary(mlogit(importance_rank ~ position | 0 , md,               reflevel = \"1\"))Coefficients :           Estimate Std. Error  t-value  Pr(\u0026gt;|t|)    position2   0.19272    0.24995   0.7711   0.44068    position3  -0.15338    0.24391  -0.6288   0.52945    position4  -0.41315    0.24921  -1.6578   0.09735 .  position5  -1.29994    0.25856  -5.0276 4.966e-07 ***position6  -1.78532    0.26232  -6.8058 1.005e-11 ***position7  -1.92077    0.26615  -7.2168 5.320e-13 ***position8  -2.49722    0.27249  -9.1645 \u0026lt; 2.2e-16 ***position9  -2.76654    0.27952  -9.8974 \u0026lt; 2.2e-16 ***position10 -3.53177    0.27635 -12.7801 \u0026lt; 2.2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Log-Likelihood: -662.47What I get are tests with position 1 as reference, if I see this correctly. Now the question remains, does this answer my questiondo I need to respecify the model, andhow to properly interpret the results?What I feel is missing is a single estimate for the effect of position. My understanding of the mode is still very rudimentary. So any suggestions how to model it better or correctly are very welcome :)","Creater_id":7952,"Start_date":"2016-07-22 09:07:45","Question_id":225141,"Tags":["multilevel-analysis","ranking","nested","mlogit"],"Answer_count":1,"Last_activity":"2016-07-28 01:59:49","Link":"http://stats.stackexchange.com/questions/225141/model-relation-between-two-rank-variables-where-ranks-are-nested-within-subjects","Creator_reputation":136}
{"_id":{"$oid":"5837a58ba05283111e4d7360"},"View_count":37,"Display_name":"traggatmot","Question_score":0,"Question_content":"I am examining the use of underground pipe network information in the prediction of soil moisture in metropolitan areas; drinking water pipes leak water to the surrounding soil, storm sewer and wastewater pipes receive water from the surrounding soil.  I am assuming certain leakage rates based on the size and age of the pipes - older pipes leak/receive more, larger pipes leak/receive more.  I am missing information for certain pipes on their age and size.Because this is a first pass at building a working model (set of mathematical equations predicting soil moisture), I don't need the model to be perfect - but I do need to replace my NODATA values with some estimate of age and size.  What is the best method for replacing my NODATA values?  With the mean, median, or a random set of values that follow the distribution of the original data?Total number of pipes: 10779, with 1034 = NODATA.  Age range: 1858 - 1992.  ","Creater_id":36954,"Start_date":"2015-10-05 18:31:57","Question_id":175618,"Tags":["missing-data"],"Answer_count":1,"Last_activity":"2016-07-28 01:43:42","Link":"http://stats.stackexchange.com/questions/175618/replacing-nodata-values-with-mean-median-or-random-values-from-current-distrib","Creator_reputation":136}
{"_id":{"$oid":"5837a58ba05283111e4d7362"},"View_count":122,"Display_name":"skan","Question_score":2,"Question_content":"Is there any important distribution with Median=Mode=Mean,  apart from the Gaussian and Student's distribution?","Creater_id":23802,"Start_date":"2015-02-26 06:11:43","Question_id":139421,"Tags":["distributions","mean","median","mode"],"Answer_count":1,"Last_activity":"2016-07-28 01:42:09","Link":"http://stats.stackexchange.com/questions/139421/distributions-with-median-mode-average","Creator_reputation":270}
{"_id":{"$oid":"5837a58ba05283111e4d7364"},"View_count":560,"Display_name":"ToNoY","Question_score":1,"Question_content":"the question is trivial, but is there any statistical test (can be done in R) to show that the median is changing in a time-series?? For example, if you go to the following link, you would notice that, after the first few samples (6 or 7), the time series takes a sharp rise. I was wondering if there is a formal statistical way of saying it!https://docs.google.com/file/d/0B6GUNg-8d30vMzB3cUxJM1dBbXc/edit?usp=sharing","Creater_id":21918,"Start_date":"2013-05-09 10:52:32","Question_id":58598,"Tags":["r","time-series","median","change-point"],"Answer_count":3,"Last_activity":"2016-07-28 01:36:57","Link":"http://stats.stackexchange.com/questions/58598/test-to-identify-change-in-median-in-a-time-series","Creator_reputation":180}
{"_id":{"$oid":"5837a58ba05283111e4d7366"},"View_count":41,"Display_name":"user89073","Question_score":0,"Question_content":"I am estimating a Vector Autoregression where the independent variables are all interacted by a dummy variable and represents economic regimes.The VAR looks like this:Y_t=\\mu+\\Phi Y_{t-1}+\\epsilon_t where I estimate the VAR by Maximum Likelihood and have in the set of the parameters  parameters where Nreg is the number of regimes (here, 2), Ny is the number of dependent variables (here, 7) and Nx is the number of independent variables. The first term is thus the number of regression coefficients and the second is the number of variance-covariance matrix elements.I want to estimate the whole set of parameters given that the variance-covariance matrices are identical.For this, I am using MATLAB fmincon.In the documentation, I read that I have to use the Aeq and beq matrices such that .Since I want some of my parameters to be equal to each other, I have to set up the constraint such that the sum of 1 parameter and the opposite of the corresponding parameter in the other regime is zero. For example,  or , which works fine for two regimes but breaks down if I have more than 2. I doubt that I have to add every combination of the constraint two by two in order to obtain a constraint that would look like this: The alternative is to penalize the Likelihood value if the matching parameters are not identical:if sigma_1(1,1)~=sigma_2(1,1)llk=10^12;endWhat would be the best strategy?Thank you for your answers.","Creater_id":32279,"Start_date":"2016-07-28 01:30:27","Question_id":226061,"Tags":["maximum-likelihood","matlab","likelihood","constraint","fmincon"],"Answer_count":0,"Last_activity":"2016-07-28 01:30:27","Link":"http://stats.stackexchange.com/questions/226061/constraint-equality-of-parameters-in-matlab-fmincon","Creator_reputation":554}
{"_id":{"$oid":"5837a58ba05283111e4d7368"},"View_count":42,"Display_name":"vgaasj","Question_score":0,"Question_content":"I have heard people using the term background class in the following to scenarios: For a class which has a very high number of instances compared to other classes in a classification problemSometimes just creating a class like \"others\" and adding all those instances for which enough features are not available to discriminatively decide the class which they belong to. For example, in Section 2 of this paper, https://www.robots.ox.ac.uk/~vgg/publications/2015/Cimpoi15/cimpoi15.pdf (this paper deals with building features for performing texture/material recognition in images). I have quoted the particular text which talks about a background class below:  In particular, we build on the Open Surfaces (OS) dataset that was  recently introduced by Bell et al. [4] in computer graphics. OS  comprises 25,357 images, each containing a number of high-quality  texture/material segments. Many of these segments are annotated with  additional attributes such as the material name, the viewpoint, the  BRDF, and the object class. Not all segments have a complete set of  annotations; the experiments in this paper focus on the 58,928 that  contain material names. Since material classes are highly unbalanced,  only the materials that contain at least 400 examples are considered.  This result in 53,915 annotated material segments in 10,422 images  spanning 23 different classes.   Images are split evenly into  training, validation, and test subsets with 3,474 images each. Segment  sizes are highly variable, with half of them being relatively small,  with an area smaller than 64 × 64 pixels. While the lack of exhaustive  annotations makes it impossible to define a complete background class,  several less common materials (including for example segments that  annotators could not assign to a material) are merged in an “other”  class that acts as pseudo-background.Are there other scenarios where this term is used? Also, why exactly is it called a background class?","Creater_id":124457,"Start_date":"2016-07-26 07:16:06","Question_id":225701,"Tags":["machine-learning","classification","terminology"],"Answer_count":1,"Last_activity":"2016-07-28 00:52:21","Link":"http://stats.stackexchange.com/questions/225701/what-exactly-is-a-background-class-in-a-classification-problem","Creator_reputation":1}
{"_id":{"$oid":"5837a58ba05283111e4d736a"},"View_count":33,"Display_name":"user6321","Question_score":0,"Question_content":"I was reading these slides about Bag of Features (BoF). At slide 23 you can read:  each image is represented by a vector, typically 1000-4000 dimension,  normalization with L1/L2 normWhy we should normalize the feature histogram vector for image-classification/retrieval applications?In addition, the distance used for normalize histograms should be the same for computing the distance between them? Because the standard distance used for BoF histograms is   distance, not L1/L2.","Creater_id":116375,"Start_date":"2016-07-20 08:07:11","Question_id":224757,"Tags":["chi-squared","normalization","histogram","euclidean"],"Answer_count":1,"Last_activity":"2016-07-28 00:50:58","Link":"http://stats.stackexchange.com/questions/224757/why-should-we-normalize-bag-of-features-histograms","Creator_reputation":125}
{"_id":{"$oid":"5837a58ba05283111e4d736c"},"View_count":93,"Display_name":"Kevin Zhang","Question_score":1,"Question_content":"I'm building a ridge regression model in scikit-learn and trying to find the optimal degree polynomial to use. The data I'm working with is a fairly predictable time series of hourly traffic volumes, and I'm predicting said volumes from the date, hour, and day of the week. R-squared values increase for both my train and test sets as I generate higher degree polynomial features, but suddenly drop from .91 to -1.4 when I go from degree 8 to 9, signifying that the 9th-order model is worse than the 0-order model. Any idea why this happens? ","Creater_id":124726,"Start_date":"2016-07-27 08:44:22","Question_id":225931,"Tags":["regression","machine-learning","nonlinear-regression","scikit-learn"],"Answer_count":1,"Last_activity":"2016-07-28 00:35:12","Link":"http://stats.stackexchange.com/questions/225931/why-is-my-high-degree-polynomial-regression-model-suddenly-unfit-for-the-data","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d736e"},"View_count":111,"Display_name":"chandresh","Question_score":1,"Question_content":"I was thinking to solve LASSO via vanilla subgradient methods. But,I have read people suggesting to use Proximal GD. Can somebody highlight why proximal GD instead of vanilla subgradient methods be used for LASSO?","Creater_id":54448,"Start_date":"2015-10-20 04:38:44","Question_id":177800,"Tags":["machine-learning","optimization","gradient-descent"],"Answer_count":1,"Last_activity":"2016-07-27 23:17:58","Link":"http://stats.stackexchange.com/questions/177800/why-proximal-gradient-instead-of-plain-subgradient-methods-for-lasso","Creator_reputation":110}
{"_id":{"$oid":"5837a58ba05283111e4d7370"},"View_count":21,"Display_name":"RegalPlatypus","Question_score":0,"Question_content":"My dataset has two explanatory fixed effect variables; one, tree, is categorical and the other, pctrans, is continuous (actually, a proportion as a decimal on the (0,1) interval. Trays \"trayid\" are a random effect.  The model also has one explanatory random effect variable: trayid. My response variable, survival, is a Bernoulli distribution of 0's (died) and successes (1's). I'm using the lme4 R packageI have three main questions I want to answer.Is there a significant interaction between tree and pctrans?Is there a significant effect of tree on survival?Is there a significant effect of pctrans on survival?Because I'm interested in each of these questions individually, is it fair to evaluate separate models for each?  If I model them together (survival~tree*pctrans + (1|trayid)) and run anova() with test=\"Chisq\" on the model, tree and pctrans are significant but the interaction is not.Model: binomial, link: logitResponse: survivalTerms added sequentially (first to last)             Df Deviance Resid. Df Resid. Dev  Pr(\u0026gt;Chi)    NULL                           610     845.65              tree          5   77.297       605     768.35 3.083e-15 ***pctrans       1   14.584       604     753.77 0.0001341 ***tree:pctrans  5    1.257       599     752.51 0.9392616    However, if I test the interaction independently, then that model is significant:\u0026gt; drop1(interaction.glmer, test=\"Chisq\")Single term deletionsModel:survival ~ tree:pctrans + (1 | trayid)             Df    AIC    LRT   Pr(Chi)    \u0026lt;none\u0026gt;          776.12                     tree:pctrans  6 795.58 31.454 2.075e-05 ***I think, mentally, I'm trying to separate out main effects and interaction effects that one would normally encounter in a two-way ANOVA by running separate models for each. My emphasis is on effects, not creating one model that best fits the data.Tangent:  If the interaction in this model can be modeled independently, I'm going to need some help making sense of the slope estimates.  Any resources that I could be pointed towards would be very helpful!","Creater_id":121418,"Start_date":"2016-07-27 23:09:31","Question_id":226049,"Tags":["generalized-linear-model","chi-squared","lme4","glmm","likelihood-ratio"],"Answer_count":0,"Last_activity":"2016-07-27 23:09:31","Link":"http://stats.stackexchange.com/questions/226049/evaluating-interaction-in-a-generalized-linear-mixed-model","Creator_reputation":13}
{"_id":{"$oid":"5837a58ba05283111e4d7372"},"View_count":9,"Display_name":"Alex","Question_score":2,"Question_content":"Suppose that  is a stochastic process, such that  is modelled as a Poisson random variable with rate , where  is a function of the previous state . What conditions must be imposed on this function before we are able to derive a closed form for the probability distribution function of  for any  when given the state ?For example, in a population of animals, the number of offspring in a year is modelled as a Poisson random variable with mean proportional to the population. Ignoring deaths, this mean will increase as the population increases each year.","Creater_id":22199,"Start_date":"2016-07-27 22:33:08","Question_id":226048,"Tags":["poisson","random-variable","formula"],"Answer_count":0,"Last_activity":"2016-07-27 22:33:08","Link":"http://stats.stackexchange.com/questions/226048/closed-form-for-poisson-stochastic-process-with-mean-dependent-on-the-state-spac","Creator_reputation":727}
{"_id":{"$oid":"5837a58ba05283111e4d7374"},"View_count":56,"Display_name":"bones.felipe","Question_score":1,"Question_content":"I'm performing visualization of a dataset clustered with k-means. I compute a weight for each cluster and I draw a circle as big as its weight. But it seems like after the clustering some values are too high respect to the data set. For example the biggest weight is 1117797 while the smallest is just 2.75, I've performed normalization between [0,1] and the visualization due to that dissimilarity is not good.Should I normalize data in a different way?, I've read about z-score but I'm not sure how to apply it in order to give less relevance to this big clusters.Additional info:Average: 16213Standard deviation: 110985.9The problem I'm solving: I have around 500k text comments and they are represented in a vector space model using a Term frequency – Inverse document frequency matrix. In the end every document is represented as a vector where each dimension represents the weight of a term in the corpus.Edit:So far I've got the following: I consider a cluster an \"outlier\" of the data if its weight is greater than the average plus the standard deviation multiplied by 3. Formally is an outlier if satisfies:Then I do a scaling in [0,1] of the set of \"outliers\" and then multiply every element by . It is, take the max of the \"non outliers\" and use it as a baseline to put another weight to the outlier points such that they remain bigger but not too much big. I've set  because it gives me nice graphical results but I am not sure this \"experimental\" method would work for different types of data (which may be the case in my problem).Edit2:Based on the Anony-Mousse suggestion I used the log function to smooth the differences between the weights, this gave some interesting results because indeed the clusters does not have such huge differences but they seem to have a very similiar size between them, I've added images to make it more clear:Without any data standarization:With log scale:As Anony-Mousse suggested this is a \"Zipfian\" scenario but the log scale seems to smooth way to much the differences between the weigts, for example the big blue cluster should be much more big but allowing the rest of the clusters to be seen.","Creater_id":124290,"Start_date":"2016-07-23 18:10:29","Question_id":225313,"Tags":["clustering","data-visualization","normalization","k-means","standardization"],"Answer_count":1,"Last_activity":"2016-07-27 22:11:25","Link":"http://stats.stackexchange.com/questions/225313/normalization-standarization-for-clustering-visualization","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d7376"},"View_count":16,"Display_name":"dayum","Question_score":0,"Question_content":"I am reading up on spectrum analysis and am confused about the following terms (I have included the relevant language of different sources in brackets to further clarify the terms)Scatterplots of paired eigenvectors (pure sine and cosine series with equal frequencies, amplitudes, and phases create the scatterplot with the points lying on a circle. If period is an integer, then this points are vertices of regular p-vertex polygon)Periodogram of the paired eigentriples (If the periodograms of the eigenvector have sharp spark around some frequencies, then the corresponding eigentriples must be regarded as those related to the signal component.The usual google search etc hasn't been fruitful so any help is appreciated!","Creater_id":92890,"Start_date":"2016-07-27 21:05:15","Question_id":226042,"Tags":["time-series","scatterplot","eigenvalues","spectral-analysis"],"Answer_count":0,"Last_activity":"2016-07-27 21:05:15","Link":"http://stats.stackexchange.com/questions/226042/periodogram-paired-eigenvectors-scatterplot","Creator_reputation":16}
{"_id":{"$oid":"5837a58ba05283111e4d7378"},"View_count":87,"Display_name":"JacKeown","Question_score":1,"Question_content":"It's pretty neat that Tensorflow allows you to define and do math on arbitrary tensors, but for supervised learning applications, is there any reason you would want to define your inputs and outputs as anything other than a vector?Are there any common reasons for needing the extra abstraction that tensors allow?","Creater_id":122137,"Start_date":"2016-07-27 16:37:56","Question_id":226021,"Tags":["tensorflow","shape"],"Answer_count":1,"Last_activity":"2016-07-27 20:07:52","Link":"http://stats.stackexchange.com/questions/226021/is-tensorflow-shape-just-for-convenience","Creator_reputation":27}
{"_id":{"$oid":"5837a58ba05283111e4d737a"},"View_count":295,"Display_name":"Eddie","Question_score":0,"Question_content":"I'm looking for an approach that would fill null values in a dataframe for discrete and continuous values such that the nulls would be replaced by randomly generated numbers that reflect the relative frequency of the values in the column.  It would looks something like this for a categorical variable:IN: dataframe['fruit']OUT:applebananaappleapplenanappleGiven this data, it would fill the nan with apple approximately 4/5 of the time and banana 1/5 of the timeIN: dataframe['fruit'].fillna('randomwalk')OUT:applebananaappleappleappleapple","Creater_id":96963,"Start_date":"2015-12-02 23:01:02","Question_id":184781,"Tags":["python","pandas"],"Answer_count":1,"Last_activity":"2016-07-27 19:36:30","Link":"http://stats.stackexchange.com/questions/184781/how-to-fill-null-values-in-a-pandas-dataframe-using-random-sampling-to-generate","Creator_reputation":21}
{"_id":{"$oid":"5837a58ba05283111e4d737c"},"View_count":33,"Display_name":"SwingingStrawberry","Question_score":0,"Question_content":"This question I think was already asked here but I can't fully understand the answer. I have a number of ordinal predictors that I'm transforming into dummy variables and I'm wondering whether the hierarchical multiple regression linear relationship assumption (linear relationship between each predictor and the outcome variable - also the composite and outcome) needs to be met for each dummy variable?","Creater_id":116661,"Start_date":"2016-07-27 18:30:56","Question_id":226031,"Tags":["multiple-regression"],"Answer_count":1,"Last_activity":"2016-07-27 19:00:24","Link":"http://stats.stackexchange.com/questions/226031/linear-relationship-assumption-with-dummy-variable","Creator_reputation":32}
{"_id":{"$oid":"5837a58ba05283111e4d737e"},"View_count":38,"Display_name":"Omar","Question_score":1,"Question_content":"I have two normal distributions fg and bg with mean (mu) and standard deviations (sd) as follows:set.seed(100)fg = rnorm(10000, mean=11.00, sd=3.77)bg = rnorm(10000, mean=-0.508, sd=1.04)If I fit an LDA model like this:library(MASS)mydata = data.frame(label = c(rep(1, 10000), rep(0,10000)),                     score = c(fg, bg))fit = lda(label~score, data=mydata)And try and to predict some new values:newvals = seq(-7, 25, 0.1)pred = predict(fit, data.frame(score=newvals))# Plot posteriormatplot(newvals, pred$posterior, type='l', col=c('red', 'blue'), lty=1)I get posteriors which look like this:At a value of 5, the posterior for belonging to either class is 0.5, but looking at the density plots above, you can see that at 5 it almost always belongs to the fg distribution. I would expect the posterior to be 0.5 closer to 2.5-3, where both density curves cross eachother.Can anyone please explain why the lda posteriors are behaving this way - or if I'm doing something wrong?Thanks!","Creater_id":16887,"Start_date":"2016-07-27 18:36:08","Question_id":226032,"Tags":["r","probability","classification","linear-model","posterior"],"Answer_count":1,"Last_activity":"2016-07-27 18:58:21","Link":"http://stats.stackexchange.com/questions/226032/linear-discriminant-analysis-posterior-not-giving-expected-values-in-r","Creator_reputation":187}
{"_id":{"$oid":"5837a58ba05283111e4d7380"},"View_count":31,"Display_name":"user3022875","Question_score":0,"Question_content":"I have 10 means and I want to test if there is a significant difference between the groups. The means are composed of cost values that can be negative and positive.I read here the kruskal test can be used on \"ratio\" variables but everywhere I read like on this page: https://statistics.laerd.com/spss-tutorials/kruskal-wallis-h-test-using-spss-statistics.php  It seems they must be \"positive\" ratio scale. The example at that link says you can run the kruskal test on a variable 0-100.    Assumption #1: Your dependent variable should be measured at the ordinal or continuous level (i.e., interval or ratio).    ...continuous variables include revision time (measured in hours), intelligence (measured using IQ score), exam performance (measured from 0 to 100), But can you run Kruskal wallis on NEGATIVE and positive variables?  Say each of my 10 variables have a range of ~ -5 to +10?######## UPDATE::::  THEY defined \"ratio\" as  Ratio variables are interval variables, but with the added condition that 0 (zero) of the measurement indicates that there is none of that variable. So, temperature measured in degrees Celsius or Fahrenheit is not a ratio variable because 0C does not mean there is no temperature. However, temperature measured in Kelvin is a ratio variable as 0 Kelvin (often called absolute zero) indicates that there is no temperature whatsoever. from this link https://statistics.laerd.com/statistical-guides/types-of-variable.phpIn my case 0 means 0 cost so I can use Kruskal wallis. ","Creater_id":40579,"Start_date":"2016-07-27 14:51:13","Question_id":226008,"Tags":["anova","t-test","mann-whitney-u-test","manova","kruskal-wallis"],"Answer_count":1,"Last_activity":"2016-07-27 15:54:34","Link":"http://stats.stackexchange.com/questions/226008/kruskal-wallis-post-hoc-test-for-negative-and-positive-values","Creator_reputation":98}
{"_id":{"$oid":"5837a58ba05283111e4d7382"},"View_count":101,"Display_name":"Sahil Talwar","Question_score":3,"Question_content":"Suppose I have a data which has 90% values in  range, 98.8% values in  range and 99.9% values in the  range. Can I refute that this data is distributed normally?I have around 7000 observations (edited in from comments)Is it okay from a normality perspective to have more than \"prescribed\" percentage of observations in these ranges?","Creater_id":93688,"Start_date":"2016-07-26 05:27:46","Question_id":225687,"Tags":["normal-distribution","goodness-of-fit","normality"],"Answer_count":1,"Last_activity":"2016-07-27 15:52:21","Link":"http://stats.stackexchange.com/questions/225687/can-the-68-95-99-7-rule-be-used-to-test-normality","Creator_reputation":26}
{"_id":{"$oid":"5837a58ba05283111e4d7384"},"View_count":13,"Display_name":"AlphaOmega","Question_score":0,"Question_content":"I'd like to do an ANOVA on the following problem:The only dependent variable is the number of children a person has.The two independent variables are the person's age and the person's income. Of course there might be a non-negligible interaction between age and income. We want to neglect the fact that neither age nor income (I presume) are actually normally distributed as I learned that ANOVA is quite robust to violation of that condition (or is it?).Let's consider persons aged 20 to 70 with incomes from 0 to 250k/year.I am wondering how the sizes of the factor levels can change the results of the analysis, i.e. can I expect analyses that consider 10 age groups (20 to 25, 25 to 30,..., 60 to 70) to yield \"better/worse\" results (be more/less statistically significant) than analyses that use only 5 age groups (20 to 30, 30 to 40,...)?","Creater_id":120194,"Start_date":"2016-07-27 15:27:42","Question_id":226011,"Tags":["anova"],"Answer_count":0,"Last_activity":"2016-07-27 15:27:42","Link":"http://stats.stackexchange.com/questions/226011/what-influence-do-the-sizes-of-the-factor-levels-have-in-anova","Creator_reputation":163}
{"_id":{"$oid":"5837a58ba05283111e4d7386"},"View_count":49,"Display_name":"Robust","Question_score":1,"Question_content":"I'm trying to get aquainted with robust regression methods and there's something about M-estimators that I don't understand.In \"Robust statistics\" (Maronna, Martin, Yohai) it is said that if both our x's and y's are random, the M-estimator of regression is given as a solution to\\sum \\rho (\\frac{r_i (\\hat{\\beta})}{\\hat{\\sigma}}) = minwhere  is a bounded  function and  is a \"high breakdown preliminary scale\". What is that \"preliminary scale\" supposed to be? How do I understand preliminary in that context? ","Creater_id":24594,"Start_date":"2013-04-20 09:18:46","Question_id":56684,"Tags":["regression","robust","estimators"],"Answer_count":0,"Last_activity":"2016-07-27 15:23:46","Link":"http://stats.stackexchange.com/questions/56684/what-is-the-preliminary-scale-in-m-estimates-for-regression","Creator_reputation":6}
{"_id":{"$oid":"5837a58ba05283111e4d7388"},"View_count":5999,"Display_name":"Qroid","Question_score":55,"Question_content":"I saw this article in the Economist about a seemingly-devastating paper [1] casting doubt on \"something like 40,000 published [fMRI] studies.\" The error, they say, is because of \"erroneous statistical assumptions.\" I read the paper and see it's partly a problem with multiple comparison corrections, but I'm not an fMRI expert and am finding it difficult to follow. What are the erroneous assumptions the authors are talking about? Why are those assumptions made? What are ways around making these assumptions?A back of the envelope calculation says that 40,000 fMRI papers is well over a $billion in funding (grad student salary, operating costs, etc.).[1] Eklund et al., Cluster failure: Why fMRI inferences for spatial extenthave inflated false-positive rates, PNAS 2016","Creater_id":90427,"Start_date":"2016-07-25 10:09:30","Question_id":225557,"Tags":["hypothesis-testing","multiple-comparisons","spatial","neuroimaging","neuroscience"],"Answer_count":1,"Last_activity":"2016-07-27 15:04:53","Link":"http://stats.stackexchange.com/questions/225557/40-000-neuroscience-papers-on-fmri-might-be-wrong","Creator_reputation":896}
{"_id":{"$oid":"5837a58ca05283111e4d73e1"},"View_count":22,"Display_name":"Ari Leshno","Question_score":2,"Question_content":"This might be a very basic question, but I've failed in finding an explanation on the web...How is it possible that a variable (Var1) that significantly correlates with two different variables (Var2 and Var3), would not correlated also with a fourth variable (Var4) which is the result of dividing the later two variables (Var4 = Var2/Var3)?For example:I have an image in which one of the properties is \"Quality\".I use this image to obtain two measurements: \"M1\" and \"M2\". From \"M1\" and \"M2\" I calculate \"F1\" (which is my endpoint) by dividing \"M1\" by \"M2\" (F1 = M1/M2).Although I get a significant correlation between \"Quality\" to both \"M1\" and \"M2\" (Spearman's -0.625, P\u0026lt;0.0001 and -0.636, P\u0026lt;0.0001 respectively), the correlation between \"F1\" and \"Quality\" is non-significant (-0.095, p=0.565).It should also be noted that \"M1\" and \"M2\" strongly correlate with each other (0.692, P\u0026lt;0.0001).Any help would be greatly appreciated","Creater_id":124774,"Start_date":"2016-07-27 14:24:55","Question_id":226002,"Tags":["correlation"],"Answer_count":0,"Last_activity":"2016-07-27 14:24:55","Link":"http://stats.stackexchange.com/questions/226002/correlation-of-a-calculated-variable","Creator_reputation":11}
{"_id":{"$oid":"5837a58ca05283111e4d73e3"},"View_count":32,"Display_name":"Moe","Question_score":1,"Question_content":"I have data about patient hospital readmission within one year after surgery. About two thirds of patients were readmitted, and the number of readmissions for those readmitted vary from 1 to 8 times. In addition, I have data about the dates of these readmissions and their duration, so I can calculate the duration between any two readmissions for all patients. My question is, how could I test whether these readmissions are close together or are scattered throughout the year and not closely related? Closely related readmissions are likely more preventable through improved care coordination, quality of care delivered, etc.","Creater_id":114479,"Start_date":"2016-07-27 14:11:58","Question_id":225999,"Tags":["r","repeated-measures","survival","stata","recurrent-events"],"Answer_count":0,"Last_activity":"2016-07-27 14:11:58","Link":"http://stats.stackexchange.com/questions/225999/how-to-statistically-make-inferences-about-how-close-are-repeated-events-to-each","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d73e5"},"View_count":20,"Display_name":"machinery","Question_score":0,"Question_content":"I have run some machine learning experiments and now I have created some ROC and Precision-Recall curves (with the help of a toolbox).Unfortunately, I'm not familiar with these two things. Of course, in the web there is plenty of material describing it but I did not find some good explanation based on an example.Is there somebody who can show how one can analyse classifiers and also compare them based on the ROC and Precision-Recall curve? Perhaps this could easier be understood when following an example.I would highly appreciate if somebody can bring light into the dark.","Creater_id":72747,"Start_date":"2016-07-27 14:08:04","Question_id":225996,"Tags":["roc","precision-recall"],"Answer_count":0,"Last_activity":"2016-07-27 14:08:04","Link":"http://stats.stackexchange.com/questions/225996/analysis-of-roc-and-precision-recall-curve","Creator_reputation":258}
{"_id":{"$oid":"5837a58ca05283111e4d73e7"},"View_count":18,"Display_name":"K23","Question_score":0,"Question_content":"I am trying to quantify error of the MLE for the following model using Fisher's information:where the 's and 's are known.Looking at some old notes from my statistics class, I see that Fisher's information for a single observation  with parameter  is equal to the negative ofIn my case, I have calculated bothbut I am unsure how to deal with the integral because each observation is multivariate with a  from each binomial. Do I have to use multiple integrals with respect to each ?","Creater_id":69505,"Start_date":"2016-07-26 14:05:56","Question_id":225785,"Tags":["maximum-likelihood","fisher-information"],"Answer_count":1,"Last_activity":"2016-07-27 14:03:40","Link":"http://stats.stackexchange.com/questions/225785/fishers-information-for-multiple-binomials","Creator_reputation":56}
{"_id":{"$oid":"5837a58ca05283111e4d73f4"},"View_count":26,"Display_name":"FKG","Question_score":0,"Question_content":"I have never worked with dyadic data before but need to do that now. So my question touches upon the very structure of dyadic data. The subject of the study is countries and their ties to each other. I am particularly interested in 3 dimensions/types of ties. All three types of ties can be at play, but this is not always the case. Let's say that these three types of ties are called Y1, Y2 and Y3. Now what I want to know for sure is that the way I structure the data is correct before I go further with the coding. So far I have been coding according to the sample below:  countryT    year    countryS    Y1  Y2  Y3Argentina   2007    Afghanistan 0   0   0Argentina   2007    Albania     0   0   0Argentina   2007    Algeria     0   0   0Argentina   2007    Angola      0   0   0Argentina   2007    Argentina   0   1   1Ecuador     2008    Brazil      0   0   1Ecuador     2008    Ecuador     1   1   1Ecuador     2008    Sudan       0   0   0Ecuador     2008    France      1   0   1Ecuador     2008    Germany     0   1   1Note that country X (in this case Argentina, as displayed above) can have ties with itself. This is because the Y variable captures non-state actors' ties to countries.   Anyway, I have been told that the best way to code this data is as in the following example belowcountryT    year    countryS    Y1  Y2  Y3Argentina   2007    Afghanistan 0   0   0Argentina   2007    Afghanistan 0   0   0Argentina   2007    Afghanistan 0   0   0Argentina   2007    Albania     0   0   0Argentina   2007    Albania     0   0   0Argentina   2007    Albania     0   0   0Argentina   2007    Algeria     0   0   0Argentina   2007    Algeria     0   0   0Argentina   2007    Algeria     0   0   0The  difference is that I now have three rows for each country (i.e. countryS. What is the correct way to code this? What should be considered? I would also appreciate books or articles touching upon the data structure of dyadic data. ","Creater_id":53160,"Start_date":"2016-07-27 13:58:50","Question_id":225994,"Tags":["regression","logistic","dataset","dyadic-data"],"Answer_count":0,"Last_activity":"2016-07-27 13:58:50","Link":"http://stats.stackexchange.com/questions/225994/how-to-structure-dyadic-data","Creator_reputation":50}
{"_id":{"$oid":"5837a58ca05283111e4d73f6"},"View_count":19,"Display_name":"Tom","Question_score":1,"Question_content":"I am a teacher in a language program at a university, and I was interested in investigating whether or not there is a correlation between the number of sessions a student spends in our program and their performance on the TOEFL (an English proficiency test). I have test scores from students who took this test before entering our program, and test scores from the same students who took the test again after spending 1 - 4 sessions in our program. To be clear, these students did not take the test again after each session, so for any given student, I only have their pre-program test score and an additional score from when they took the test again (either after 1, 2, 3, or 4 sessions in our program).Is there a way to analyze this data to learn whether or not the time spent in our program correlates to an improve in performance on the TOEFL test? My experience with statistics of this kind is limited, but I thought I could use the Kruskal-Wallis test to analyze the gain scores between pre and post tests between the four groups of students (students who spent either 1, 2, 3, or 4 sessions in our program) to see if there is a meaningful connection. A co-worker thought I should use Kendall's tau coefficient instead, which I have not used before.  Would either of these statistics be appropriate for this situation? This is not data from an experiment, and the students are all from different populations and took this test in different locations after studying with different teachers in our program, so I realize that there are many confounding variables. Any guidance would be greatly appreciated! ","Creater_id":124768,"Start_date":"2016-07-27 13:17:51","Question_id":225985,"Tags":["r","correlation","statistical-significance","kruskal-wallis","kendall-tau"],"Answer_count":1,"Last_activity":"2016-07-27 13:56:46","Link":"http://stats.stackexchange.com/questions/225985/analyzing-gain-scores-and-or-correlation-in-a-non-experimental-setup","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d7403"},"View_count":40,"Display_name":"Ryan","Question_score":1,"Question_content":"I have a massive data set of about 25000 pancreatic cancer patients; extremely quick and depressing mortality rate. I'm interested in survival differences between three groups--no treatment, chemotherapy, and chemo-radiation. Below is the survival function produced by PROC LIFETEST.I have some evidence that the proportionality of hazards assumption is violated. Statistically, the estimated group*time interaction (programmed within PROC PHREG) is statistically significant. This makes sense given the large N. However, the graphical approach is a bit more subjective; see the log-negative-log survival function below. If I choose to retain the group*time interaction, the survival differences estimated by this extended Cox model diverge from what I would expected based on what I see in the Kaplan-Meier analysis. Specifically, both chemotherapy and chemo-radiation show significantly lower risk of death (or better survival) than the no treatment group until about 12-months post-diagnosis, at which point the effect switches, and by 18-months post-diagnosis the no treatment group has significantly lower risk of death than either treatment group. Knowing who I am working with (err, for...), I'm going to have a hard time explaining this result. I'm guessing it has to do with the interaction. Do you think that I mis-specified time function? Any thoughts on this? Has anyone else seen this?Thanks!Ryan","Creater_id":124771,"Start_date":"2016-07-27 13:50:41","Question_id":225991,"Tags":["spss","sas","cox-model","kaplan-meier"],"Answer_count":0,"Last_activity":"2016-07-27 13:50:41","Link":"http://stats.stackexchange.com/questions/225991/estimated-survival-discrepancies-kaplan-meier-vs-an-extended-cox-regression","Creator_reputation":11}
{"_id":{"$oid":"5837a58ca05283111e4d7405"},"View_count":87,"Display_name":"Frey","Question_score":3,"Question_content":"For i.i.d. random variables, we may write the CDF of  as F_t(t)=F_{t_i}(x)^nand the CDF of  as F_x(x)=1-(1-F_{x_i}(x))^nWhen we have , we may combine above results if every entry in  are independent to each other in each  term.However, I have some common terms in  terms. For example: X=\\max(\\min(x_1,x_2,x_3),\\min(x_1,x_4,x_5),\\min(x_5,x_6,x_7),\\min(x_3,x_6,x_8))I understand that they are correlated (and dependent?), but I am not sure how I can apply correlation and/or dependency, specially for the general case. Basically, 's are exponentially distributed. Can someone guide me to derive ? ","Creater_id":124679,"Start_date":"2016-07-27 03:13:15","Question_id":225867,"Tags":["probability","distributions","pdf","cdf","order-statistics"],"Answer_count":1,"Last_activity":"2016-07-27 13:48:43","Link":"http://stats.stackexchange.com/questions/225867/pdf-cdf-of-max-min-type-random-variable","Creator_reputation":116}
{"_id":{"$oid":"5837a58ca05283111e4d7412"},"View_count":30,"Display_name":"Zenvega","Question_score":1,"Question_content":"I am trying to fit a model for a loan portfolio which have been acquired over several years, each of the loan has one of the several term periods (different performance window) over which the loan needs to be paid off. A few number of loans have charged off before the term expired and a few after the term. The event is chargeoff (1) or not (0).How should I define time to event? For chargeoffs, the time to event is Time to chargeoff. For non-chargeoffs, should the time be till the end of their respective term or end of the maximum term of all the terms? I tried former definition, and used Cox proportional hazard model in SAS, but my expect vs actual survival curves are way off. Any help is appreciated.Thanks.","Creater_id":102353,"Start_date":"2016-07-27 13:29:11","Question_id":225986,"Tags":["survival","analysis"],"Answer_count":0,"Last_activity":"2016-07-27 13:43:44","Link":"http://stats.stackexchange.com/questions/225986/survival-analysis-time-to-event","Creator_reputation":106}
{"_id":{"$oid":"5837a58ca05283111e4d7414"},"View_count":12,"Display_name":"Phil","Question_score":1,"Question_content":"I'm working on a survey that is already partway through the implementation phase. It is a web survey, and unfortunately, one of my colleagues noticed that there was an error in the skip logic, leading to some questions not being answered by early respondents. It is impractical to restart the survey at this point, as many respondents have already taken the survey, but we do not have any way of identifying them (only some of the surveys were administered incorrectly). The bug has now been fixed, so we now have an inconsistent implementation problem affecting a particular subgroup of the survey respondents (and obviously this was non-random).Obviously there is no \"optimal\" solution to this problem, but can anyone point to any helpful guidelines or ways of estimating the amount of error introduced by this problem?Thanks!","Creater_id":124770,"Start_date":"2016-07-27 13:34:14","Question_id":225988,"Tags":["survey","error","methodology","bias-correction","non-response"],"Answer_count":0,"Last_activity":"2016-07-27 13:34:14","Link":"http://stats.stackexchange.com/questions/225988/what-do-you-do-when-you-discover-a-mistake-in-survey-implementation-partway-thro","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d7416"},"View_count":197,"Display_name":"Michael Luu","Question_score":2,"Question_content":"I'm currently working on building a predictive model for a binary outcome on a dataset with ~300 variables and 800 observations. I've read much on this site about the problems associated with stepwise regression and why not to use it.I've been reading into LASSO regression and its ability for feature selection and have been successful in implementing it with the use of the \"caret\" package and \"glmnet\". I am able to extract the coefficient of the model with the optimal lambda and alpha from \"caret\"; however, I'm unfamiliar with how to interpret the coefficients. Are the LASSO coefficients interpreted in the same method as logistic regression?Would it be appropriate to use the features selected from LASSO in logistic regression?EDITInterpretation of the coefficients, as in the exponentiated coefficients from the LASSO regression as the log odds for a 1 unit change in the coefficient while holding all other coefficients constant.http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm  ","Creater_id":120225,"Start_date":"2016-07-27 11:56:46","Question_id":225970,"Tags":["multiple-regression","predictive-models","interpretation","regression-coefficients","lasso"],"Answer_count":2,"Last_activity":"2016-07-27 13:30:40","Link":"http://stats.stackexchange.com/questions/225970/interpretation-of-lasso-regression-coefficients","Creator_reputation":28}
{"_id":{"$oid":"5837a58ca05283111e4d7424"},"View_count":41,"Display_name":"goro","Question_score":1,"Question_content":"I want to use k-means clustering algorithm to cluster my data into two clusters, tight as possible. I have the following questions:Are there any roles to choose the variables needed to cluster the data. e.g I have 100 patients who diagnosed of having a disease. I have the rate of disease progression and the disease duration since diagnosis. The formula that calculate the rate of disease progression depends on a clinical scale for disease evaluation, and disease duration as follow : rate of disease progression= (total clinical scale - current clinical scale) / disease durationI aim to divide the patients into two groups (slow disease progression and fast disease progression) by feeding the rate of disease progression and disease duration as variables into k-means. Is this choice, for the variables, correct?. In other words the rate of disease progression was calculated depending in part on disease duration. Can this affect clustering the data using two variables one derived from the other one? Or I need,just, any two or more variables that can make dividing the patients make sense?When clustering the data, using k-means, I need to choose the clusters tight as possible ( i.e. the data points are close as possible to the centroid of each cluster). Is it advisable to remove the outiers from the clusters.  For example:In the figure bellow, there are three clusters. the data points are concentrated in almost 90% around the centroids ( within each cluster). Do I need to remove the data points that are so far away from the centroid of the clusters ( #1 and #3).","Creater_id":58684,"Start_date":"2016-07-27 12:31:51","Question_id":225978,"Tags":["clustering"],"Answer_count":0,"Last_activity":"2016-07-27 13:09:34","Link":"http://stats.stackexchange.com/questions/225978/how-to-choose-k-means-clustering-variables","Creator_reputation":367}
{"_id":{"$oid":"5837a58ca05283111e4d7426"},"View_count":22,"Display_name":"stat_joe","Question_score":2,"Question_content":"I am using Canay's R program to explore quantile regression for panel data. In the example given, Grunfeld data is used. Following the Example.R file everything goes smoothly. However, when I delete some rows to create an unbalanced panel from the Grunfeld data I obtain errors. I am using the following command to remove some rows. Grunfeld \u0026lt;- Grunfeld[-c(190:194),] I wonder if it is not possible to use any unbalanced panel data with Canay's two step approach. Could you point me into the right direction on how to conduct a quantile regression with unbalanced panel data on R. Thanks.","Creater_id":124764,"Start_date":"2016-07-27 12:56:20","Question_id":225984,"Tags":["panel-data","quantile-regression"],"Answer_count":0,"Last_activity":"2016-07-27 12:56:20","Link":"http://stats.stackexchange.com/questions/225984/quantile-regression-for-unbalanced-panel-data-in-r","Creator_reputation":11}
{"_id":{"$oid":"5837a58ca05283111e4d7428"},"View_count":24,"Display_name":"Simon","Question_score":0,"Question_content":"If I have two regressors that should theoretically have opposite signed coefficients, should I switch the sign (i.e. multiply by -1) of one of the variables before creating an interaction term?In my work, I found the 'correct' signs for the main regressors, one positive and one negative in the non-interacted regression. When I created an interaction term between the two, the main effects kept their original signs but the interaction term was insignificant.I switched the signs of one of the variables (multiplied each observation by -1) and re-ran the regressions. Obviously, without the interaction term, the absolute value of the coefficient on that variable remained the same but the sign had switched. However, when I added the new interaction term, the coefficient on the interaction term was now significant.Note that these are all continuous variables and interaction terms were based on mean-centered variables as in Balli \u0026amp; Sorensen (2013). I'm not sure why this would happen. Is this acceptable? Is it normal? Should I be concerned about my data?","Creater_id":124749,"Start_date":"2016-07-27 11:37:38","Question_id":225965,"Tags":["regression","interaction","regression-coefficients"],"Answer_count":1,"Last_activity":"2016-07-27 12:52:45","Link":"http://stats.stackexchange.com/questions/225965/should-i-switch-the-signs-of-a-variable-before-interaction","Creator_reputation":1}
{"_id":{"$oid":"5837a58ca05283111e4d7435"},"View_count":58,"Display_name":"user124644","Question_score":3,"Question_content":"Let's say I have a finite set of binary random variables. Does there exist, for any such set, another finite set of binary random variables that carries approximately the same information, but whose members are all statistically independent of each other? By \"carry approximately the same information\", I mean that any sample of the dependent set could be approximately determined given the corresponding sample of the independent set. In other words: It's my understanding that when we have statistical dependencies, it means we have redundant information. For any given set of binary variables, does there exist another set which carries the same information but with all the redundancies eliminated?If so, is it possible to find such sets? Are there any known techniques for doing so?","Creater_id":null,"Start_date":"2016-07-26 18:24:45","Question_id":225815,"Tags":["independence","non-independent"],"Answer_count":1,"Last_activity":"2016-07-27 12:47:16","Link":"http://stats.stackexchange.com/questions/225815/eliminating-statistical-dependency-from-a-set-of-random-variables","Creator_reputation":null}
{"_id":{"$oid":"5837a58ca05283111e4d7442"},"View_count":56,"Display_name":"user102426","Question_score":0,"Question_content":"My situation is as follows: as a teacher, I've given students the option to make 5 sets of homework during the year, which does not count for their grade, but solely to practice and receive feedback; in hopes to prepare them better for their exam.For every student I now have two lists: how many of the sets of homework they have made (so an integer from 0 to 5) and their exam grade (between 0 and 10). The number of homework made is severely skewed: 66% of the values are 5, 20% are 4, the rest is 0,1,2 or 3. The grades aren't rounded so can (probably?) be treated as continuous and according to Shapiro-Wilk they are distributed normally. I'd like a good way to measure the impact of the number of homework made on the final grade, and if possible apply regression. I applied linear regression and calculated the Pearson R-coefficient, but is this a good method considering the ordinal variable? Or should I apply a transformation to the homework, or use a different kind of regression altogether?Thanks in advance.","Creater_id":102426,"Start_date":"2016-01-30 10:05:39","Question_id":193257,"Tags":["regression","correlation","ordinal","continuous-data"],"Answer_count":1,"Last_activity":"2016-07-27 12:44:21","Link":"http://stats.stackexchange.com/questions/193257/regression-for-continuous-dependent-variable-with-independent-ordinal-variable","Creator_reputation":101}
{"_id":{"$oid":"5837a58ca05283111e4d744f"},"View_count":12,"Display_name":"Sara S","Question_score":1,"Question_content":"I am running a linear regression looking at the effect of long term rainfall averages and rainfall shocks on district welfare (Y). I have measures of rainfall long term averages (rain_lt) as well as measures of rainfall level in the year preceding the survey where welfare is computed (Lag1_rain). In one of the specifications I define the rainfall shock as the difference between the year-specific level and the long term average (Lag1_rain-rain_lt), which I call Lag1_rain_diff. Since I am interested in seeing what is the effect of much higher rain than usual (flooding) as well as much lower rain than usual (drought), I split Lag1_rain_diff into two different variables:1) Lag1_rain_diff_flood = Lag1_rain-rain_lt if Lag1_rain\u003e=rain_lt,    Lag1_rain_diff_flood = 0 otherwise2) Lag1_rain_diff_drought  = -(Lag1_rain-rain_lt)  if Lag1_rainAnd the obtained linear regression specification is the following:Y = rain_lt + Lag1_rain_diff_flood + Lag1_rain_diff_drought  + Controls + errorI was wondering whether the introduction of two regressors that are artificially censored at 0 introduces any bias in the regression, and in the case they do, how to redefine the specification in order to avoid the issue.Thank you very much in advance!","Creater_id":124756,"Start_date":"2016-07-27 12:42:27","Question_id":225980,"Tags":["regression","censoring"],"Answer_count":0,"Last_activity":"2016-07-27 12:42:27","Link":"http://stats.stackexchange.com/questions/225980/artificially-censored-regressors-in-linear-model","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d7451"},"View_count":108,"Display_name":"Anton","Question_score":0,"Question_content":"I'm trying to create a multilevel ordinal logistic regression model in Stan and the following converges:   stanmodel \u0026lt;- 'data {  int\u0026lt;lower=2\u0026gt; K;  int\u0026lt;lower=0\u0026gt; N;  int\u0026lt;lower=1,upper=K\u0026gt; y[N];  int\u0026lt;lower=0\u0026gt; Ntests;  int\u0026lt;lower=1,upper=Ntests\u0026gt; tests[N];}parameters {  ordered[K-1]  CutpointsMean;  real\u0026lt;lower=0\u0026gt; CutpointsSigma[K-1];  ordered[K-1]  Cutpoints[Ntests];}model {  CutpointsSigma ~ exponential(1);  CutpointsMean  ~ normal(2, 3);  for (i in 1:Ntests) {    Cutpoints[i][1] ~ normal(CutpointsMean[1] , CutpointsSigma[1]);    Cutpoints[i][2] ~ normal(CutpointsMean[2] , CutpointsSigma[2]);    Cutpoints[i][3] ~ normal(CutpointsMean[3] , CutpointsSigma[3]);  }  for (i in 1:N)    y[i] ~ ordered_logistic(0, Cutpoints[tests[i]]);}'CutpointsMean' and 'CutpointsSigma' define the population global ordinal response while Cutpoints[i][1:3] is the ordinal response for group i.As Cutpoints[i] is an ordered vector of 3 elements, what happens when I write directly into  Cutpoints[i][2] ? Is the write operation rejected if the constraints are not satisfied or simply the entry in written in the vector and and the results is sorted?Is this  the correct way of modelling a multilevel ordinal response in Stan?","Creater_id":95411,"Start_date":"2016-07-27 11:30:26","Question_id":225963,"Tags":["regression","multilevel-analysis","stan","ordered-logit"],"Answer_count":1,"Last_activity":"2016-07-27 12:37:56","Link":"http://stats.stackexchange.com/questions/225963/multilevel-ordered-logistic-regression-in-stan","Creator_reputation":87}
{"_id":{"$oid":"5837a58ca05283111e4d745e"},"View_count":38,"Display_name":"Gordon","Question_score":1,"Question_content":"I've been looking into using Bayes to classify incoming emails to one of several distinct \"owners\" (so more complex than a spam filter that only has two outcomes).I don't have a stats background, so my nomenclature is likely to be poor. Although it'll make this post long, I'll start with some of my understanding below in the hope that puts things in context.I (think I've) understood the simple applications, such as:I have two bowls. Both bowls have 40 biscuits. Bowl 1 has 30 vanilla and 10 chocolate. Bowl 2 has 20 vanilla and 20 chocolate. If I pick a chocolate biscuit, what's the probability I chose from bowl 1?p(B1 | v) = The probability of Bowl 1, given we chose a vanillabiscuitp(B1) = The probability of Bowl 1 (there are two bowls, so 1/2 = 0.5)p(v | B1) = The probability we'd get vanilla if we did chose fromBowl 1 (30/40 = 0.75)p(v) = The probability of choosing vanilla from all bowls (30+20 /40+40 = 0.625)p(B1 | v) = p(B1) * p(v | B1)             -----------------                   p(v)= 0.5 * 0.75  ----------     0.625= 0.6=\u003e 60% chance that if we chose vanilla it came from Bowl 1If I run the numbers for p(B2 | v) I get 0.4, so this works nicely.I note that if the input data had bowl 1 with 20 biscuits (4 vanilla, 16 chocolate) and bowl 2 with 80 biscuits (1 vanilla, 79 chocolate) I get some odd output numbers (that don't add up to 1):p(B1 | v) = 2p(B2 | v) = 0.125However, I see from Naive Bayes classifier gives a probability greater than 1 that this is likely because these are probability densities?I note that there's a 16:1 ratio, which seems about right, and if I divide both by the sum (2 / 2.125 and 0.125 / 2.125) I get ~0.94 and ~0.06 respectively (which adds up to 1, and still has the same ratio). I assume this is all \"ok\".If I understand correctly, I can add a third bowl (or more), with the only changes being that p(B1) would now be 1/3, or 1/4 etc (depending on the number of bowls), and p(v) would also change as I'd have to sum all vanilla biscuits across all bowls, divided by all biscuits across all bowls.Assuming I haven't made a complete mess of the above... onto the actual problem...What I want to do is classify the probability that a word in a candidate email belongs to one of my prior known \"bowls\". I build a word frequency database for each bowl (from prior emails), and test in the same way. E.g. if bowl 1 contains 30 instances of \"the\" and 10 of something else, and my candidate word is \"the\" I can check the probability that \"the\" came from bowl 1 (or bowl 2 etc).The bit I'm struggling with is how to then integrate the results from every word in the incoming email.With a bit of empirical testing, it seems that multiplying the resulting probability value with the previous is mathematically correct (e.g. if I throw two sixes with dice, the probability is 1/6 x 1/6 = 1/36). Thus I'd multiply the probability for each word for each bowl, and the bowl that has the largest final probability is the winner. E.g.:Final probability of bowl 1 = p(B1 | \"the\") * p(B1 | \"cat\") * p(B1 | \"sat\") ...Note that if \"the\" appeared in my candidate email twice, I'd use p(B1 | \"the\")^2 instead, and p(B1 | \"the\")^3 if it appeared 3 times, and so on.However, it's likely that a candidate email may contain a word that doesn't exist in a bowl, in which case I get an output of 0 (meaning that the total result for that bowl would be 0).Mathematically that's correct - if I had bowls with 3 types of biscuits (e.g. chocolate, vanilla, strawberry) and took a handful from one bowl (e.g. vanilla, vanilla, strawberry), then if any of those bowls didn't contain any strawberry biscuits there's 0 probability I could have taken the biscuits from that bowl - regardless of the probabilities of the other bowls.In this instance however, I'm looking for which bowl was the \"best fit\" (i.e. on balance, the most likely).I could just sum the probabilities (p(B1 | \"the\") + p(B1 | \"cat\") + p(B1 | \"sat\")) and choose the bowl with the largest number, but I'm aware I'm now out of my depth, and likely doing the \"wrong\" thing from a stats point of view.Any re-education on my understanding of Bayes above, and advice on how to sum the result for each candidate word would be greatly appreciated. Preferably in layman's terms - because that's what I am ;)","Creater_id":124717,"Start_date":"2016-07-27 08:26:30","Question_id":225923,"Tags":["naive-bayes","filter"],"Answer_count":0,"Last_activity":"2016-07-27 12:25:28","Link":"http://stats.stackexchange.com/questions/225923/how-could-i-use-a-bayes-classifier-to-categorise-emails","Creator_reputation":41}
{"_id":{"$oid":"5837a58ca05283111e4d7460"},"View_count":100,"Display_name":"b_pcakes","Question_score":4,"Question_content":"I've been reading about k-fold validation, and I want to make sure I understand how it works.I know that for the holdout method, the data is split into three sets, and the test set is only used at the very end to assess the performance of the model, while the validation set is used for tuning hyperparameters, etc. In the k-fold method, do we still hold out a test set for the very end, and only use the remaining data for training and hyperparameter tuning, i.e. we split the remaining data into k folds, and then use the average accuracy after training with each fold (or whatever performance metric we choose to tune our hyperparameters)? Or do we not use a separate test set at all, and simply split the entire dataset into k folds (if this is the case, I assume that we just consider the average accuracy on the k folds to be our final accuracy)? ","Creater_id":124743,"Start_date":"2016-07-27 10:30:34","Question_id":225949,"Tags":["cross-validation","validation","out-of-sample"],"Answer_count":1,"Last_activity":"2016-07-27 12:15:49","Link":"http://stats.stackexchange.com/questions/225949/do-we-need-a-test-set-when-using-k-fold-cross-validation","Creator_reputation":133}
{"_id":{"$oid":"5837a58ca05283111e4d746d"},"View_count":44,"Display_name":"SpeedBirdNine","Question_score":2,"Question_content":"When a dataset is needed to be modeled, the process is to take a part of it out as holdout set which is \"unseen\" by the training method and is used to test the performance of models created using various techniques. Also while training, cross validation divides the data into parts for training and evaluation. My question is that after a model has been selected using its performance in k-fold cross validation as well as on the holdout set which training methods have not seen at all, is there any need to train a new model on the entire set (including the holdout data) with the selected technique when putting it in actual use? For example if, after evaluation of different techniques, logistic regression model is selected with some subset of variables. When putting it to use, it is better to use the selected model as it is, or is it better to train a new model using logistic regression (which produced the best model) with same variables set and same other parameters, but with entire data (training and holdout sets combined)?The reason against training a new model are:The selected model has gone through cross validation and other selection processes which the new model has not, and it might be overfitted. Its performance on holdout set is unknown becuase now there is no holdout set. Depending on modeling technique, the new model might be different than the selected model. For example if lasso is used, on the larger set it might give a different set of variables in the final model, putting more doubts on its real world performance. On the other hand, using the model that has been selected and then verified using holdout set have the advantage of an evaluated performance on holdout set, but It might have less information because it has been trained on training set which is subset of the entire data. I understand if the dataset is large enough, there is less chance of this and new model being very different but it is still a possibility. Another scenario that makes the choice even less clear is if the dataset is huge, beyond the capability of the development machine, and therefore a small sample is taken from it. And then the analyst uses multiple techniques, and selects the best technique and model. Now considerable time and effort is required to create a new model on the larger set to prepare it for actual use. Is this step of training a new model on entire set necessary, or the previously selected model should be put to final use. A small additional question from this question that comes to my mind is: According to bias-variance tradeoff concept, does training on a larger set compared to a smaller sample add more variance and reduces bias from the model, and does this become a factor to consider in my original question? ","Creater_id":56105,"Start_date":"2016-07-26 18:58:03","Question_id":225820,"Tags":["cross-validation","model-selection","validation","subset"],"Answer_count":2,"Last_activity":"2016-07-27 12:14:12","Link":"http://stats.stackexchange.com/questions/225820/is-it-needed-to-train-the-selected-model-again-on-entire-data-before-putting-in","Creator_reputation":304}
{"_id":{"$oid":"5837a58ca05283111e4d747b"},"View_count":76,"Display_name":"Steve","Question_score":4,"Question_content":"It is known that for independent sub-exponential random variables, the following Bernstein-type inequality holds:\\begin{align}\\mathbb{P}\\biggl(\\biggl|  \\sum_{i=1}^N a_i X_i\\biggr| \u0026gt;t \\biggr) \\leq 2 \\exp\\left[ -c\\min \\left(\\frac{t^2}{K^2 \\| \\vec{a}\\|_2^2}, \\frac{t}{ K \\| \\vec{a}\\|_{\\infty}} \\right)\\right],\\end{align}where  and . I wonder if similar concentration inequality holds for heavy-tailed random variables where  satisfies  for .","Creater_id":92438,"Start_date":"2015-11-19 03:14:49","Question_id":182549,"Tags":["probability","heavy-tailed","inequality"],"Answer_count":1,"Last_activity":"2016-07-27 11:59:22","Link":"http://stats.stackexchange.com/questions/182549/bernsteins-inequality-for-heavy-tailed-random-variables","Creator_reputation":172}
{"_id":{"$oid":"5837a58ca05283111e4d7488"},"View_count":43,"Display_name":"Sara S","Question_score":0,"Question_content":"I am running a linear regression with a quadratic regressor of this form:Y= X + X^2 + W + errorMy X variable is a measure of difference (rainfall for the year of Y - long term rainfall average), it therefore can take both positive and negative values. I was wondering whether I should enter the quadratic term as is (therefore containing only positive values) or whether I should manually replace the square values with their negative for the X that are negative.Thank you in advance for your answer!","Creater_id":124756,"Start_date":"2016-07-27 11:42:56","Question_id":225966,"Tags":["regression","least-squares","quadratic-form"],"Answer_count":2,"Last_activity":"2016-07-27 11:51:25","Link":"http://stats.stackexchange.com/questions/225966/regression-with-a-quadratic-term-of-a-variable-that-has-both-negative-and-positi","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d7496"},"View_count":66887,"Display_name":"PhD","Question_score":117,"Question_content":"...assuming that I'm able to augment their knowledge about variance in an intuitive fashion ( Understanding \u0026quot;variance\u0026quot; intuitively ) or by saying: It's the average distance of the data values from the 'mean' - and since variance is in square units, we take the square root to keep the units same and that is called standard deviation.Let's assume this much is articulated and (hopefully) understood by the 'receiver'. Now what is covariance and how would one explain it in simple English without the use of any mathematical terms/formulae? (I.e., intuitive explanation. ;)Please note: I do know the formulae and the math behind the concept. I want to be able to 'explain' the same in an easy to understand fashion, without including the math; i.e., what does 'covariance' even mean?","Creater_id":4426,"Start_date":"2011-11-07 12:41:45","Question_id":18058,"Tags":["variance","covariance","intuition"],"Answer_count":9,"Last_activity":"2016-07-27 11:46:24","Link":"http://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean","Creator_reputation":3705}
{"_id":{"$oid":"5837a58ca05283111e4d74ab"},"View_count":96,"Display_name":"user3269","Question_score":3,"Question_content":"I am collecting a group of multivariate time sequences. For example, there are 2000 time series. Each time series is of 12 dimensions. Are there any systematic models/algorithms that can cluster multivariate time series?  For instance, I would like to identify some time series that are very different with others.Moreover, for the online monitoring, I may run this algorithm in an on-time fashion. For instance, every 10 minutes, I run this kind of algorithm  against the time series covering 10 minutes.  Are there any efficient algorithms with respect to this?","Creater_id":3269,"Start_date":"2016-07-26 20:31:42","Question_id":225822,"Tags":["machine-learning","time-series","clustering","multivariate-analysis","sequential-pattern-mining"],"Answer_count":1,"Last_activity":"2016-07-27 11:43:41","Link":"http://stats.stackexchange.com/questions/225822/multivariate-time-series-clustering","Creator_reputation":1194}
{"_id":{"$oid":"5837a58ca05283111e4d74b8"},"View_count":34,"Display_name":"ahmedmar","Question_score":0,"Question_content":"Here is an example: Previous research reported that the prevalence of HIV testing among gays is 19.2% . Sample size required to detect this percentage with 95% confidence and .05 precision is 273 (using n = (Z^2 × P(1 – P))/e^2  ).e is precision and Z is the Z score corresponding to 95% i.e. 1.96Have I done it correctly?Now what's the importance of having such power: Is it to claim that the sample is representative of that population regarding HIV testing and hence HIV prevalence in my study should be trusted even if it was different from previous research? ","Creater_id":100365,"Start_date":"2016-07-24 07:20:38","Question_id":225357,"Tags":["sample-size","power","cross-section"],"Answer_count":1,"Last_activity":"2016-07-27 11:34:58","Link":"http://stats.stackexchange.com/questions/225357/how-do-i-calculate-sample-size-and-what-is-the-is-the-meaning-of-power-in-a-cros","Creator_reputation":216}
{"_id":{"$oid":"5837a58ca05283111e4d74c5"},"View_count":180,"Display_name":"John Tarr","Question_score":2,"Question_content":"I've got some data that represents two groups, with a before and after result for both groups. When I plot the data together, it looks as though the after is much tighter around the mean. However, when I plot the groups individually, that picture changes drastically. If anyone could help me understand why this is so, I would greatly appreciate it. I've pasted the R code below.library(dplyr)library(ggplot2)my_df \u0026lt;- structure(list(Group = c(1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1), Status = c(\"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"Before\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"Before\", \"After\", \"Before\", \"Before\", \"Before\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\", \"After\"), Result = c(2.39000010490417, 2.00999999046326, 2.26999998092651, 2.4300000667572, 4.19999980926514, 4.01999998092651, 4.26999998092651, 4.01999998092651, 4.1399998664856, 4.05000019073486, 4.30999994277954, 4.01999998092651, 4.01999998092651, 2.47000002861023, 2.25, 2.39000010490417, 4.28999996185303, 4.09000015258789, 4.1399998664856, 4, 4.23999977111816, 4.17000007629395, 4.21000003814697, 4.1399998664856, 4.57999992370605, 2.24000000953674, 2.44000005722046, 2.42000007629395, 3.83999991416931, 4.01000022888184, 3.75, 4.01000022888184, 3.78999996185303, 3.85999989509583, 3.94000005722046, 3.96000003814697, 4.17000007629395, 4, 4.32000017166138, 4.07999992370605, 2.46000003814697, 2.60999989509583, 2.23000001907349, 2.13000011444092, 4.46999979019165, 4.09000015258789, 4.1100001335144, 4.17000007629395, 3.86999988555908, 4.5, 3.9300000667572, 2.15000009536743, 2.35999989509583, 4.46999979019165, 4.48000001907349, 4.3600001335144, 4.19000005722046, 4.28000020980835, 4.82999992370605, 4.15000009536743, 4.42000007629395, 4.15000009536743, 4.19999980926514, 4.44000005722046, 4.21999979019165, 4.38000011444092, 3.94000005722046, 4.57000017166138, 2.32999992370605, 2.44000005722046, 2.09999990463257, 2.17000007629395, 2.17000007629395, 2.61999988555908, 4.09999990463257, 3.85999989509583, 4.15999984741211, 4.19000005722046, 4.09999990463257, 3.97000002861023, 4.19999980926514, 4.32999992370605, 4.07999992370605, 3.8199999332428, 4.01999998092651, 4.15999984741211, 3.9300000667572, 4.1399998664856, 3.77999997138977, 4.11999988555908, 4.53999996185303, 4.07000017166138, 2.54999995231628, 2.50999999046326, 2.4300000667572, 2.32999992370605, 3.85999989509583, 3.92000007629395, 4.3600001335144, 4.30000019073486, 4.34000015258789, 4.1399998664856, 4.25, 4.13000011444092, 4.03999996185303, 4.26999998092651, 4.32000017166138, 4.11999988555908, 4.05000019073486, 4.44000005722046, 4.1100001335144, 4.19000005722046, 4.28000020980835, 4.51999998092651, 4.07999992370605, 4.07000017166138, 4.05000019073486, 4.46000003814697, 4.05000019073486, 2.52999997138977)), class = \"data.frame\", row.names = c(NA, -120L), .Names = c(\"Group\", \"Status\", \"Result\"))my_df %\u0026gt;%   ggplot() +  geom_density(aes(Result, fill=Status), alpha=.3) my_df %\u0026gt;%   filter(Group == 1) %\u0026gt;%   ggplot() +  geom_density(aes(Result, fill=Status), alpha=.3) my_df %\u0026gt;%   filter(Group == 2) %\u0026gt;%   ggplot() +  geom_density(aes(Result, fill=Status), alpha=.3) ","Creater_id":121843,"Start_date":"2016-07-26 12:53:39","Question_id":225776,"Tags":["r","distributions"],"Answer_count":1,"Last_activity":"2016-07-27 11:34:38","Link":"http://stats.stackexchange.com/questions/225776/why-does-this-distribution-look-completely-different-after-splitting-into-groups","Creator_reputation":25}
{"_id":{"$oid":"5837a58ca05283111e4d74d2"},"View_count":22,"Display_name":"Artem Oboturov","Question_score":0,"Question_content":"The question comes from Kevin Murphy's book, Ch 5, Ex 5.6. Could somebody suggest a solution?Let  be the bayes factor in favor of model 1. Suppose we plot two ROC curves, one computed by thresholding , and the other computed by thresholding . Will they be the same or different? Why?","Creater_id":11238,"Start_date":"2016-06-23 13:43:06","Question_id":220383,"Tags":["bayesian","roc","threshold"],"Answer_count":1,"Last_activity":"2016-07-27 11:30:37","Link":"http://stats.stackexchange.com/questions/220383/bayes-factors-and-roc-curves","Creator_reputation":108}
{"_id":{"$oid":"5837a58ca05283111e4d74df"},"View_count":53,"Display_name":"user3024069","Question_score":0,"Question_content":"I have two columns of data: Observed (Obs) and Predicted (Pred), each column having 23 data. I have plotted Observed on y-axis and Predicted on x-axis (as pointed by Pineiro et al., 2008). On deriving a best fit linear model for the data, I have obtained:Obs = 0.21 + 1.09 * PredIt is well known that in the ideal case, the equation should have been:Obs = 0.00 + 1.00 * PredPineiro et al. (2008) [Link available on top], on page 4 [Eq. (9)], suggest:  We tested the hypothesis of slope = 1 and intercept = 0 to assess statistically the significance of regression  parameters. This test can be performed easily with statistical  computer packages with the model: Pred - Obs = a + b* Pred + \\epsilon The significance of the regression parameters of thismodels corresponds to the tests: b = 1 and a = 0. Please help on how do I conduct these tests so that I can compare the slope () with 1 and intercept () with 0. Also I am not able to get the basic concept behind proposing the model shown above. Thanks for your time and effort, all ears!","Creater_id":97916,"Start_date":"2016-07-27 10:52:34","Question_id":225956,"Tags":["regression"],"Answer_count":1,"Last_activity":"2016-07-27 11:12:15","Link":"http://stats.stackexchange.com/questions/225956/how-do-i-test-hypothesis-of-slope-1-and-intercept-0-for-observed-vs-predict","Creator_reputation":1}
{"_id":{"$oid":"5837a58ca05283111e4d74ec"},"View_count":32,"Display_name":"Tajana S","Question_score":1,"Question_content":"I would have a question about using Structural Equation Modeling for comparing repeated measures results. I have a one DV which has been measured 4 times and corresponds to 4 different experimental conditions (A, B, C, D). The DV was measured on the same scale across the 4 conditions. My question is whether there is a way to model this in SEM. As my variable is not dependent upon time but just a different experimental condition, I guess latent growth would not be appropriate - do you have suggestions about possible alternatives?Many thanks!","Creater_id":112782,"Start_date":"2016-07-27 11:10:42","Question_id":225961,"Tags":["experiment-design","sem","group-differences","growth-model","amos"],"Answer_count":0,"Last_activity":"2016-07-27 11:10:42","Link":"http://stats.stackexchange.com/questions/225961/repeated-measures-in-sem","Creator_reputation":16}
{"_id":{"$oid":"5837a58ca05283111e4d74ee"},"View_count":36,"Display_name":"DataMiner","Question_score":0,"Question_content":"I have 22 companies response about 22 questions/parameters in a 22x22 matrix. I applied clustering technique which gives me different groups with similarities.Now I would like to find correlations between parameters and companies preferences. Which technique is more suitable in R?Normally we build Bayesian network to find a graphical relationship between different parameters from data. As this data is very limited, how i can build Bayesian Network for it?Any suggestion to analyze this data.","Creater_id":16098,"Start_date":"2016-07-27 11:09:40","Question_id":225960,"Tags":["correlation","data-mining","weighted-data","qualitative"],"Answer_count":0,"Last_activity":"2016-07-27 11:09:40","Link":"http://stats.stackexchange.com/questions/225960/analysis-of-small-set-of-interview-data-using-data-mining-techniques","Creator_reputation":28}
{"_id":{"$oid":"5837a58ca05283111e4d74f0"},"View_count":19218,"Display_name":"coffeinjunky","Question_score":23,"Question_content":"This seems to be a basic issue, but I just realized that I actually don't know how to test equality of coefficients from two different regressions. Can anyone shed some light on this? More formally, suppose I ran the following two regressions: y_1 = X_1\\beta_1 + \\epsilon_1and y_2 = X_2\\beta_2 + \\epsilon_2where  refers to the design matrix of regression , and  to the vector of coefficients in regression . Note that  and  are potentially very different, with different dimensions etc. I am interested in for instance whether or not . If these came from the same regression, this would be trivial. But since they come from different ones, I am not quite sure how to do it. Does anyone have an idea or can give me some pointers? My problem in detail: My first intuition was to look at the confidence intervals, and if they overlap, then I would say they are essentially the same. This procedure does not come with the correct size of the test, though (i.e. each individual confidence interval has , say, but looking at them jointly will not have the same probability). My \"second\" intuition was to conduct a normal t-test. That is, take \\frac{\\beta_{11}-\\beta_{21}}{sd(\\beta_{11})}where  is taken as the value of my null hypothesis. This does not take into account the estimation uncertainty of , though, and the answer may depend on the order of the regressions (which one I call 1 and 2). My third idea was to do it as in a standard test for equality of two coefficients from the same regression, that is take \\frac{\\beta_{11}-\\beta_{21}}{sd(\\beta_{11}-\\beta_{21})} The complication arises due to the fact that both come from different regressions. Note that Var(\\beta_{11}-\\beta_{21}) = Var(\\beta_{11}) + Var(\\beta_{21}) -2 Cov(\\beta_{11},\\beta_{21})but since they are from different regressions, how would I get ?This led me to ask this question here. This must be a standard procedure / standard test, but I cound not find anything that was sufficiently similar to this problem. So, if anyone can point me to the correct procedure, I would be very grateful!","Creater_id":31634,"Start_date":"2014-04-12 05:51:14","Question_id":93540,"Tags":["hypothesis-testing","inference"],"Answer_count":2,"Last_activity":"2016-07-27 11:03:48","Link":"http://stats.stackexchange.com/questions/93540/testing-equality-of-coefficients-from-two-different-regressions","Creator_reputation":656}
{"_id":{"$oid":"5837a58ca05283111e4d74fe"},"View_count":97,"Display_name":"Laura","Question_score":0,"Question_content":"I asked this question in stackoverflow but closed it after 4 days since I didn't get any answer. I am using this example as a basis for my work: I am trying to use pymc MCMC to fit the parameters of a deterministic model. THe model has 8 equations and 7 parameters, and describes an outbreak of an infectious disease transmitted through vectors (mosquitoes). The data I have are the number of new cases in any given day, but only a fraction of the infections become cases, and out of those, only a fraction of them are reported. So, for the days that I have no cases reported, I replaced the 0 entries with \"None\" so that PYMC knows that these are missing values.So, I have an ODE system that models the transmission and dynamics of the outbreak with 7 unknown parameters, and 2 additional parameters for the fraction of cases ()  and the fraction of reported cases (). The idea is that the ODE gives me the count of the number of new infections, anddata ~ NegativeBinomial with .I gave priors to some of the parameters of interest:#priors:betah = pc.Uniform('betah', 0.0, 1e6, value= 0.001) #force of infection for humansbetamos = pc.Uniform('betamos', 0.0, 1e6, value= 0.001) #force of infection for mosquitoesmos2humanRatio = pc.Uniform('mos2humanRatio', 0, 1e6, value=10.0) #mosquito density symfraction = pc.Uniform('symfraction', 0.0, 1.0, value=0.2) #proportion of infections that become casesrepRate = pc.Uniform('repRate', 0.0, 1.0, value=0.1) #proportion of cases reporteddisper_par = pc.Uniform('disper_par', 0, 1e6, value= 1)and I defined my likelihood as:A = pc.NegativeBinomial('A', mu=incidence, alpha=disper_par, value=masked_values, observed=True)My problem is that I keep getting this error for my likelihood:pymc.Node.ZeroProbability: Stochastic A's value is outside its support, or it forbids its parents' current values.I have read all over the place to try to understand this, and I just cannot fix it. So I guess I have 3 questions, one statistical, the other 2 for pymc experts:Is this the correct way of setting up the MCMC? Am I handling the missing data correctly?Why I keep getting this error? I just don't understand why the likelihood is outside its support... or why I would not allow the values of the parents...I don't quite understand if the problem resides with the missing data or with one of the priors...I am completely new at pymc and MCMC, so any help would be highly appreciated!The following is a mwe:from __future__ import divisionimport pymc as pcimport numpy as npfrom matplotlib import pyplot as pltfrom scipy.integrate import odeintdef my_eqs(y,t,params):    #parameters:    [betah, betamos, bmos, delta,  gammamos, mu, N, psi] = params    S_h, E_h, Inf_h, R_h, C_h = y[0], y[1], y[2], y[3], y[4]    S_mos, E_mos, Inf_mos = y[5], y[6], y[7]    #human equations:    dS_h = -betah*(Inf_mos)*S_h    dE_h = betah*S_h*Inf_mos - delta*E_h    dInf_h = delta*E_h - psi*Inf_h    dR_h = psi*Inf_h    dC_h = delta*E_h    #mosquito equations:    dS_mos = bmos -betamos*(Inf_h/N)*S_mos - mu*S_mos    dE_mos = betamos*(Inf_h/N)*S_mos - mu*E_mos -gammamos*E_mos    dInf_mos = gammamos*E_mos -mu*Inf_mos    dydt = [dS_h, dE_h, dInf_h, dR_h, dC_h, dS_mos, dE_mos, dInf_mos]    return dydt#create some fake data:mydata = [0.0, 0.0, 0.0, 3.0, 8.0, 7.0, 1.0, 5.0, 3.0, 1.0, 0.0, 6.0, 9.0, 2.0, 0.0, 3.0, 8.0, 8.0, 6.0, 0.0, 2.0, 4.0, 4.0, 9.0, 8.0, 6.0, 8.0, 0.0, 2.0, 1.0, 0.0, 7.0, 5.0, 9.0, 1.0, 4.0, 6.0, 7.0, 4.0, 10.0, 4.0, 2.0, 12.0, 6.0, 4.0, 5.0, 9.0, 12.0, 10.0, 10.0, 12.0, 8.0, 15.0, 17.0, 15.0, 30.0, 32.0, 39.0, 44.0, 51.0, 57.0, 61.0, 31.0, 32.0, 25.0, 39.0, 45.0, 54.0, 65.0, 56.0, 60.0, 57.0, 48.0, 47.0, 66.0, 48.0, 39.0, 48.0, 32.0, 39.0, 45.0, 41.0, 59.0, 38.0, 41.0, 41.0, 48.0, 40.0, 35.0, 38.0, 27.0, 29.0, 20.0, 14.0, 17.0, 17.0, 16.0, 11.0, 10.0, 11.0, 22.0, 21.0, 6.0, 12.0, 9.0, 14.0, 4.0, 7.0, 15.0, 14.0, 13.0, 6.0, 12.0, 49.0, 22.0, 9.0, 6.0, 8.0, 0.0, 5.0, 12.0, 5.0, 10.0, 8.0, 11.0, 15.0, 5.0, 9.0, 6.0, 3.0, 3.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]#replace the days where mydata=0 with None to handle missing data:cases = np.array(mydata, dtype=object)cases[1:][cases[1:] == 0] = None# plt.plot(cases,'o')masked_values = np.ma.masked_equal(cases, value=None)###### MCMC setup#deterministic parameters:tspan = range(0,len(mydata),1)delta = 1/4.5gammamos = 0.1mymu = 1/15.0psi = 1/5.5N = 100000#priors:betah = pc.Uniform('betah', 0.0, 1e6, value= 0.001)betamos = pc.Uniform('betamos', 0.0, 1e6, value= 0.001)mos2humanRatio = pc.Uniform('mos2humanRatio', 0, 1e6, value=10.0)symfraction = pc.Uniform('symfraction', 0.0, 1.0, value=0.2) #proportion of infections that become symptomaticrepRate = pc.Uniform('repRate', 0.0, 1.0, value=0.1) #proportion of cases reporteddisper_par = pc.Uniform('disper_par', 0, 1e6, value= 1)@pc.deterministicdef bmos(mos2humanRatio=mos2humanRatio):    out = mos2humanRatio*(N)*mymu    return out####deterministic model@pc.deterministicdef mymodel(repRate=repRate,symfraction=symfraction, betah=betah,              betamos=betamos,mos2humanRatio=mos2humanRatio,bmos=bmos):    initCond = [N-1, 0, 1, 0, 0, mos2humanRatio*N, 0, 0]    params = [betah, betamos, bmos, delta,  gammamos, mymu, N, psi]    soln = odeint(my_eqs, initCond, tspan, args = (params, ))    newcases = np.zeros(np.size(soln[:, 4]))    for t in range(1,len(tspan)):        newcases[t] = (soln[t, 4] - soln[t-1, 4])    reported_cases = repRate*symfraction*newcases    # plt.plot(reported_cases, 'r')    return reported_casesincidence = pc.Lambda('incidence', lambda mymodel=mymodel: mymodel)A = pc.NegativeBinomial('A', mu=incidence, alpha=disper_par, value=masked_values, observed=True)","Creater_id":114579,"Start_date":"2016-05-04 14:06:19","Question_id":210933,"Tags":["mcmc","pymc"],"Answer_count":1,"Last_activity":"2016-07-27 11:01:17","Link":"http://stats.stackexchange.com/questions/210933/pymc-errorpymc-node-zeroprobability-stochastic-as-value-is-outside-its-suppor","Creator_reputation":101}
{"_id":{"$oid":"5837a58ca05283111e4d7509"},"View_count":19,"Display_name":"sambajetson","Question_score":0,"Question_content":"When using the Neyman-Fisher Factorization theorem to get sufficient statistics in the case of 2 or more parameters, how do know which sufficient statistic corresponds to which parameter? For textbook cases with a few parameters and using distributions that everyone knows well like the normal distribution, it's obvious which statistic matches which parameter. E.g. the following example using the normal distribution:https://onlinecourses.science.psu.edu/stat414/node/285But in general, how can you find the correct pairing of sufficient statistics with parameters?","Creater_id":83546,"Start_date":"2016-07-27 10:48:07","Question_id":225953,"Tags":["parametric","parameterization","sufficient-statistics"],"Answer_count":0,"Last_activity":"2016-07-27 10:48:07","Link":"http://stats.stackexchange.com/questions/225953/sufficient-statistics-for-n-parameters","Creator_reputation":136}
{"_id":{"$oid":"5837a58ca05283111e4d750b"},"View_count":147,"Display_name":"AlphaOmega","Question_score":6,"Question_content":"The -distro is the distribution of the sum  where the  are drown from a standard normal distribution.However, the standard normal distribution is a Gaussian bell curve, i.e. so squaring this would give another bell curve:.So why, then, is this not the -distribution?","Creater_id":120194,"Start_date":"2016-07-27 08:00:19","Question_id":225914,"Tags":["chi-squared"],"Answer_count":3,"Last_activity":"2016-07-27 10:47:55","Link":"http://stats.stackexchange.com/questions/225914/how-is-the-chi2-1-distribution-not-a-gaussian","Creator_reputation":163}
{"_id":{"$oid":"5837a58ca05283111e4d751a"},"View_count":65,"Display_name":"Jarad","Question_score":0,"Question_content":"When are you supposed to use the pooled formula vs the unpooled formula? My understanding is that you use the pooled formula when testing a hypothesis where the null hypothesis is  and the alternative hypothesis is . My confusion arises when doing hypothesis testing for \"greater than\" or \"less than\" scenarios.  Do you pool it to create a  or do you use  and  seen in the unpooled formula below?Which formula would I use if I were testing:Or:Or other scenarios like:Or:Pooled Formula:Where Unpooled Formula:Where  and  are simply the proportions in decimal format.I see so many confusing posts on this subject:[1] Video maker tests null=0, alternative\u003e0 and uses pooled[2] This StackExchange post suggests there is not a consensus among stats practicioners[3] This StackExchange post suggests you use unpooled when you \"believe\" the population proportions are different - I have a problem with the word \"believe\" as I'm looking for a concrete definitive rule to guide my decision framework.[4] Makes the case that pooled estimates give us the best estimate for variability - but doesn't have examples of \"greater than\" or \"less than\" scenarios which makes it hard for me to understand[5] StackExchange post provides good examples of the difference in calculation but also states 'the final statistical decision does not quite exist on this'[6] YouTube video showing a two proportion left-tail z-test - Video maker uses pooled formula when  .[7] YouTube video shows using pooled formula in one-sided test[8] YouTube video computes  but uses in unpooled formula[9] Blog writer uses unpooled formula to calculate the standard error for a one-tailed test... I'll stop there.","Creater_id":94117,"Start_date":"2016-07-27 10:30:45","Question_id":225950,"Tags":["hypothesis-testing","proportion","pooling"],"Answer_count":0,"Last_activity":"2016-07-27 10:36:24","Link":"http://stats.stackexchange.com/questions/225950/two-sample-hypothesis-test-for-difference-between-two-proportions-pooled-vs-unp","Creator_reputation":98}
{"_id":{"$oid":"5837a58ca05283111e4d751c"},"View_count":12,"Display_name":"godspeed","Question_score":0,"Question_content":"I have two questionsQuestion 1:I have a longitudinal data with extreme class imbalance: 9 cases of the event of interest out of a total of 3000. I believe I have 3 options and this is the of preference I have and want your opinion/critique:Keeping all 9 positive cases, randomly sample 191 negative cases. There is a name for this type of design, right?Give up on analysis of the data due to possibly biased results given the extreme class imbalanceAnalyze the data without trying to correct for the extreme class imbalanceQuestion 2: Coding the response variableAssume that the experiment ran for 3 days and that the event of interest only occurred on the third day. I may have two options in coding the response variable:Code the response variable as 0 for day 1 and 2, and as 1 for day 3. Code the response variable as 1 for all 3 days since the event occurred at the end of the experiment for this individualThe first option is under the assumption that each individual's value for the response variable can be allowed to change in GEE models.","Creater_id":103245,"Start_date":"2016-07-27 10:31:19","Question_id":225951,"Tags":["logistic","sampling","small-sample","unbalanced-classes","gee"],"Answer_count":0,"Last_activity":"2016-07-27 10:31:19","Link":"http://stats.stackexchange.com/questions/225951/logistic-gee-models-with-extreme-class-imbalance","Creator_reputation":133}
{"_id":{"$oid":"5837a58ca05283111e4d751e"},"View_count":42,"Display_name":"Marius Jonsson","Question_score":1,"Question_content":"Let . And assume that there are exactly  observations  for each integer  such that . For each , I create Bernoulli distributed random variables , such that  if  and  otherwise. I proceed to do logistic regression on each of the  groups using:P(Y_{i_j} = 1 | x_j) = \\frac{\\exp(\\beta_{i_0} + \\beta_{i_1} x_j)}{1 + \\exp(\\beta_{i_0} + \\beta_{i_1} x_j)} \\qquad \\text{for each .}Thus I complete exactly one logistic regression for each . Now fix . How can I construct a  confidence interval for  such that  for each ? The purpose is that I want to test whether there is statistically significant difference between -values. I suppose I need to use maximum likelihood estimation, but I am not sure where to begin.","Creater_id":124155,"Start_date":"2016-07-22 06:14:42","Question_id":225107,"Tags":["hypothesis-testing","logistic","statistical-significance","confidence-interval"],"Answer_count":0,"Last_activity":"2016-07-27 10:22:38","Link":"http://stats.stackexchange.com/questions/225107/logistic-regression-is-there-a-significant-difference-for-probability-p-between","Creator_reputation":108}
{"_id":{"$oid":"5837a58ca05283111e4d7520"},"View_count":2200,"Display_name":"Joe King","Question_score":7,"Question_content":"I would like to know how the treatment of weights differs between svyglm and glmI am using the twang package in R to create propensity scores which are then used as weights, as follows (this code comes from the twang documentation):library(twang)library(survey)set.seed(1)data(lalonde)ps.lalonde \u0026lt;- ps(treat ~ age + educ + black + hispan + nodegree + married + re74 + re75, data = lalonde)lalonde$w \u0026lt;- get.weights(ps.lalonde, stop.method=\"es.mean\")design.ps \u0026lt;- svydesign(ids=~1, weights=~w, data=lalonde)glm1 \u0026lt;- svyglm(re78 ~ treat, design=design.ps)summary(glm1)...Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)   6685.2      374.4  17.853   \u0026lt;2e-16 ***treat         -432.4      753.0  -0.574    0.566    Compare this to:glm11 \u0026lt;- glm(re78 ~ treat, weights=w , data=lalonde)summary(glm11)Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)   6685.2      362.5  18.441   \u0026lt;2e-16 ***treat         -432.4      586.1  -0.738    0.461  So the parameter estimates are the same but the standard errors for the treatment are quite different.How does the treatment of weights differ between svyglm and glm ?","Creater_id":11405,"Start_date":"2013-04-24 10:35:06","Question_id":57107,"Tags":["r","survey"],"Answer_count":2,"Last_activity":"2016-07-27 10:01:15","Link":"http://stats.stackexchange.com/questions/57107/use-of-weights-in-svyglm-vs-glm","Creator_reputation":811}
{"_id":{"$oid":"5837a58ca05283111e4d752e"},"View_count":61,"Display_name":"Joram","Question_score":2,"Question_content":"I collected the following variables, as I thought they might be in some relationship, but without any strict hypothesis. Note: this is a repeated measures design.phy = continuous DV (a physiological measure)lag = interval, quadratic, IV (a setting of the experiment, ranges from -5 to +5 seconds)group = factor, 2 level, IV (two different conditions, say males VS females)quest1 = continuous covariate (a questionnaire related to the measured stuff)quest2 = continuous covariate (another scale of the same questionnaire)id = factor subject idI collected 15 subjects, each measured for 11 lags, for a total of 165 data points.My goal is to decide wether there is a credible difference between the two groups in the phy response, controlling for all the other variables, and to describe such difference.So my logic was to build a full model:MODEL1 \u0026lt;- lmer(phy ~ lag * I(lag^2) * group * quest1 * quest2 + (1|id))and then to stepwise remove the interactions and to compare the models with likelihood ratio test, AIC and BIC, trough the anova() function.My results are:        Df    AIC    BIC  logLik deviance   Chisq Chi Df Pr(\u0026gt;Chisq)    MODEL8  10 385.70 416.76 -182.85   365.70                              MODEL4  13 390.16 430.54 -182.08   364.16  1.5380      3  0.6735395    MODEL5  13 420.45 460.82 -197.22   394.45  0.0000      0  1.0000000    MODEL6  18 365.84 421.74 -164.92   329.84 64.6106      5   1.35e-12 ***MODEL7  18 397.84 453.75 -180.92   361.84  0.0000      0  1.0000000    MODEL9  18 390.63 446.53 -177.31   354.63  7.2144      0  \u0026lt; 2.2e-16 ***MODEL10 18 465.24 521.14 -214.62   429.24  0.0000      0  1.0000000    MODEL11 18 413.73 469.64 -188.87   377.73 51.5046      0  \u0026lt; 2.2e-16 ***MODEL2  19 408.63 467.64 -185.31   370.63  7.1007      1  0.0077053 ** MODEL3  19 367.78 426.79 -164.89   329.78 40.8490      0  \u0026lt; 2.2e-16 ***MODEL1  34 355.60 461.20 -143.80   287.60 42.1812     15  0.0002108 ***Where MODEL1 is the full model, and the higher numbers are increasingly simple models. So except BIC, which is obviously penalizing the model complexity, AIC and likelihood test are telling me to keep a 5-way interaction!?My questions are:Is my logic right?Should I believe in such a complex model?Is it ok with my relatively few datapoints 15 subjects x 11 repetitionsHow can I even begin to interpret or to plot the effects?thank you for any suggestion on how to proceed!","Creater_id":80184,"Start_date":"2016-07-27 07:01:52","Question_id":225904,"Tags":["r","mixed-model","modeling","model-comparison","interaction-variable"],"Answer_count":2,"Last_activity":"2016-07-27 09:44:24","Link":"http://stats.stackexchange.com/questions/225904/does-it-makes-sense-to-keep-a-5-way-interaction-model","Creator_reputation":83}
{"_id":{"$oid":"5837a58ca05283111e4d753c"},"View_count":4753,"Display_name":"E L M","Question_score":20,"Question_content":"Since my first probability class I have been wondering about the following.Calculating probabilities is usually introduced via the ratio of the \"favored events\" to the total possible events. In the case of rolling two 6-sided dice, the amount of possible events is , as displayed in the table below.\\begin{array} {|c|c|c|c|c|c|c|}\\hline \u0026amp;1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \\\\\\hline1 \u0026amp; (1,1) \u0026amp; (1,2) \u0026amp; (1,3) \u0026amp; (1,4) \u0026amp; (1,5) \u0026amp; (1,6) \\\\\\hline2 \u0026amp; (2,1) \u0026amp; (2,2) \u0026amp; (2,3) \u0026amp; (2,4) \u0026amp; (2,5) \u0026amp; (2,6) \\\\\\hline3 \u0026amp; (3,1) \u0026amp; (3,2) \u0026amp; (3,3) \u0026amp; (3,4) \u0026amp; (3,5) \u0026amp; (3,6) \\\\\\hline4 \u0026amp; (4,1) \u0026amp; (4,2) \u0026amp; (4,3) \u0026amp; (4,4) \u0026amp; (4,5) \u0026amp; (4,6) \\\\\\hline5 \u0026amp; (5,1) \u0026amp; (5,2) \u0026amp; (5,3) \u0026amp; (5,4) \u0026amp; (5,5) \u0026amp; (5,6) \\\\\\hline6 \u0026amp; (6,1) \u0026amp; (6,2) \u0026amp; (6,3) \u0026amp; (6,4) \u0026amp; (6,5) \u0026amp; (6,6) \\\\\\hline\\end{array}If we therefore were interested in calculating the probability of the event A \"rolling a  and a \", we would see that there are two \"favored events\" and calculate the probability of the event as . Now, what always made me wonder is: Let's say it would be impossible to distinguish between the two dice and we would only observe them after they were rolled, so for example we would observe \"Somebody gives me a box. I open the box. There is a  and a \". In this hypothetical scenario we would not be able to distinguish between the two dice, so we would not know that there are two possible events leading to this observation. Then our possible events would like that:\\begin{array} {|c|c|c|c|c|c|}\\hline (1,1) \u0026amp; (1,2) \u0026amp; (1,3) \u0026amp; (1,4) \u0026amp; (1,5) \u0026amp; (1,6) \\\\\\hline \u0026amp; (2,2) \u0026amp; (2,3) \u0026amp; (2,4) \u0026amp; (2,5) \u0026amp; (2,6) \\\\\\hline  \u0026amp;  \u0026amp; (3,3) \u0026amp; (3,4) \u0026amp; (3,5) \u0026amp; (3,6) \\\\\\hline  \u0026amp;  \u0026amp;  \u0026amp; (4,4) \u0026amp; (4,5) \u0026amp; (4,6) \\\\\\hline  \u0026amp;  \u0026amp;  \u0026amp; \u0026amp; (5,5) \u0026amp; (5,6) \\\\\\hline  \u0026amp;  \u0026amp;  \u0026amp;  \u0026amp;  \u0026amp; (6,6) \\\\\\hline\\end{array}and we would calculate the probability of event A as . Again, I am fully aware of the fact that the first approach will lead us to the correct answer. The question I am asking myself is:  How do we know that  is correct?The two answers I have come up with are:We can empirically check it. As much as I am interested in this, I need to admit that I haven't done this myself. But I believe it would be the case. In reality we can distinguish between the dice, like one is black and the other one blue, or throw one before the other or simply know about the  possible events and then all the standard theory works.My questions to you are:What other reasons are there for us to know that  is correct? (I am pretty sure there must be a few (at least technical) reasons and this is why I posted this question)Is there some basic argument against assuming that we cannot distinguish between the dice at all?If we assume that we cannot distinguish between the dice and have no way to check the probability empirically, is  even correct or did I overlook something?Thank you for taking your time to read my question and I hope it is specific enough.","Creater_id":124036,"Start_date":"2016-07-25 09:44:46","Question_id":225552,"Tags":["probability","dice"],"Answer_count":9,"Last_activity":"2016-07-27 09:39:52","Link":"http://stats.stackexchange.com/questions/225552/how-do-we-know-that-the-probability-of-rolling-1-and-2-is-1-18","Creator_reputation":358}
{"_id":{"$oid":"5837a58ca05283111e4d7551"},"View_count":20,"Display_name":"Ginotitant Cubing","Question_score":2,"Question_content":"I've seen a lot of examples of neural networks where when they introduce the bias, they treat it as a node that always outputs 1, and then the nodes each have an individual weight for it, instead of each having a number that they add with the rest of their inputs before being activated. Is there anything wrong with this form, like maybe how it loses the point of being modeled after cells in the brain?","Creater_id":124725,"Start_date":"2016-07-27 08:30:56","Question_id":225925,"Tags":["neural-networks","intercept","bias-node"],"Answer_count":1,"Last_activity":"2016-07-27 09:25:59","Link":"http://stats.stackexchange.com/questions/225925/is-there-anything-wrong-with-treating-neural-network-bias-as-a-node","Creator_reputation":13}
{"_id":{"$oid":"5837a58ca05283111e4d755e"},"View_count":20,"Display_name":"Woeitg","Question_score":1,"Question_content":"I want to compare two plots and see how similar they are in shape.The question is similar to this one where comparison between two histograms are discussed. However the main difference is the length of two plots are not the same.I like using Chi-Square Goodness of Fit Test in question mentioned above. To make it work, the main idea is to normalized two plots to have the same length. Here comes the main challenge: When normalizing the plots, plots are may not corresponding pairwise so in this case the similarity won't be detected accurately. I show my idea in thee picture below:BLue and Red figure are normalized and they are similar (have similar shape). Pairwise operation in Chi-Square Goodness of Fit Test is shown with green line and it is not accurate. Because even plots have similar shapes, they are distributed in different manner. The question is how can I fix this issue?","Creater_id":96725,"Start_date":"2016-07-27 09:22:31","Question_id":225943,"Tags":["regression","chi-squared"],"Answer_count":0,"Last_activity":"2016-07-27 09:22:31","Link":"http://stats.stackexchange.com/questions/225943/how-to-assess-similarity-of-two-plots","Creator_reputation":61}
{"_id":{"$oid":"5837a58ca05283111e4d7560"},"View_count":82,"Display_name":"J. Sweet","Question_score":2,"Question_content":"Say I have 300 samples from a population containing two groups, A and B, and data for several variables. I have 150 from Group A and 150 from Group B. However, I know that Group A makes up roughly 20% of the population and group B makes up 80% and the two groups differ on the variables in question.Is there a way to weight the PCA by cases to make it more representative of the population?Would it be enough to just to do a weighted standardization?","Creater_id":112237,"Start_date":"2016-04-14 11:24:29","Question_id":207368,"Tags":["pca","weighted-data"],"Answer_count":1,"Last_activity":"2016-07-27 09:21:11","Link":"http://stats.stackexchange.com/questions/207368/is-there-such-thing-as-a-case-weighted-pca","Creator_reputation":13}
{"_id":{"$oid":"5837a58ca05283111e4d756d"},"View_count":24,"Display_name":"Leo","Question_score":0,"Question_content":"I am testing double EWMA smoothing on a time series of financial data to attempt to forecast the next point, xt+1 at time t. The original method I used (from wikipedia) is below:for t = 1 :s1 = x1b1 = x1 - x0and for t \u003e 1 :st = \u0026alpha;xt + (1 - \u0026alpha;)(st-1 + bt-1)bt = \u0026beta;(st - st-1) + (1 - \u0026beta;)bt-1However, by using the observable data to calculate bt rather than the smoothed values (as shown here)...for t = 1 :s1 = x1b1 = x1 - x0and for t \u003e 1 :st = \u0026alpha;xt + (1 - \u0026alpha;)(st-1 + bt-1)bt = \u0026beta;(xt - xt-1) + (1 - \u0026beta;)bt-1I get results that forecast the next point more accurately. I realize this might be due to characteristics of my data set. But I am wondering what broader implications using this method may have if I am using st to attempt to forecast xt+1. ","Creater_id":56094,"Start_date":"2016-07-27 09:20:55","Question_id":225942,"Tags":["time-series","forecasting","finance","exponential-smoothing"],"Answer_count":0,"Last_activity":"2016-07-27 09:20:55","Link":"http://stats.stackexchange.com/questions/225942/double-exponential-smoothing-alternative","Creator_reputation":101}
{"_id":{"$oid":"5837a58ca05283111e4d756f"},"View_count":74,"Display_name":"Baron Yugovich","Question_score":0,"Question_content":"I want to run PCA on a set of data, but I'd like to weigh each row of the input matrix(i.e. each data point) based on how recent it is. In other words, in my calculations of the PCs, I'd like more recent data points to be more important. How can I achieve this?","Creater_id":78063,"Start_date":"2015-10-05 20:49:43","Question_id":175640,"Tags":["pca","weighted-data"],"Answer_count":1,"Last_activity":"2016-07-27 09:20:13","Link":"http://stats.stackexchange.com/questions/175640/pca-loading-with-weights-on-samples","Creator_reputation":76}
{"_id":{"$oid":"5837a58ca05283111e4d757c"},"View_count":20,"Display_name":"Asher11","Question_score":0,"Question_content":"essentially I'd like to \"find \", given a heatmap - like the one attached - , all the \"red values\", aka all the output values of a f(x,y) function for which the value exceeds a varying threshold AND for which the nearby points are not \"blue\". the f(x,y) in question is of a non linear, non continuos in fashion.essentially, I want to find all the \"robust clusters\" for which the output value (the color in the figure) is similar and there are no areas where the (x,y) result does not greatly differ in the area (meaning finding a \"red\" cluster and then a nice \"blue\" smaller cluster in the middle). I have tried a variety of methods involving clustering (both Kmeans and hierarchical) slices involving custom distances, numerically approximating gradients but to no avail.any advice on how to proceed since literature seems a little thin on the matter (or most likely I am not looking in the right places) ?","Creater_id":68970,"Start_date":"2016-07-27 09:09:34","Question_id":225939,"Tags":["clustering","k-means","hierarchical-clustering","gradient","heatmap"],"Answer_count":0,"Last_activity":"2016-07-27 09:09:34","Link":"http://stats.stackexchange.com/questions/225939/turn-heatmap-into-a-quantitative-reading","Creator_reputation":25}
{"_id":{"$oid":"5837a58ca05283111e4d757e"},"View_count":24,"Display_name":"user3203370","Question_score":1,"Question_content":"When lift charts are generated in a Multiple Linear Regression model, for example, in predicting a continuous variable such as price of a car, how can they be explained in evaluating the performance of the model. Lot of examples on the web are related to logistic regression but it is not clear as to why and how lift charts can be used for linear regression models.","Creater_id":124730,"Start_date":"2016-07-27 08:58:53","Question_id":225938,"Tags":["multiple-regression","data-visualization","data-mining"],"Answer_count":0,"Last_activity":"2016-07-27 08:58:53","Link":"http://stats.stackexchange.com/questions/225938/lift-charts-in-multiple-linear-regression","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d7580"},"View_count":29,"Display_name":"teucer","Question_score":0,"Question_content":"Assume that we have a population of  credit card owners and we want to determine the number of frauds. We review manually  cases and detect % of frauds. It turns out  is rather small.Given the costs related to manual reviews, we want to keep  as small as possible without noticeable loss of accuracy for . This appears to be classical sample size determination problem. We are using the following approach:n = \\dfrac{z_{\\alpha}^2𝑁𝑝(1−𝑝)}{𝐸^2(𝑁−1)+z_{\\alpha}^2𝑝(1−𝑝)}where  is the desired confidence level (e.g. 95%),  the corresponding normal quantile,  the desired margin of error and  is the assumed population proportion of the frauds (set to 50% yielding the maximum ).The formula is based on the hypergeometric distribution with a normal approximation. I have some questions regarding this approach:Under which conditions is the normal approximation accurate? Can one apply the same rules (of thumb) as the binomial distribution? Are there alternative approaches? The approach will be used by non technical people, the simpler the better.","Creater_id":1443,"Start_date":"2016-07-27 08:42:52","Question_id":225929,"Tags":["confidence-interval","sampling"],"Answer_count":0,"Last_activity":"2016-07-27 08:42:52","Link":"http://stats.stackexchange.com/questions/225929/sampling-without-replacement-and-fraud-detection","Creator_reputation":850}
{"_id":{"$oid":"5837a58ca05283111e4d7582"},"View_count":93,"Display_name":"jean","Question_score":0,"Question_content":"I am using a FCN to produce a localization heat-map for each class I am interested in. The images are big 500x500 pixels and targets are very small around 20x20 pixels. The heatmaps look good I get blobs of high density in the locations of the targets.However I am only interested in extracting point(s) located inside the contours of the targets.  The first approach I tried was to extract the maximum of the heatmap (store it) and cancel it and its neighbors (its 8 nearest neighbors or even a wider range if needed). Then extract the second maximum and so forth (NMS).  However as the shape and size of the blobs of location (hot zones in the heatmap) I get in my heatmap are approximate I get a lot of maximums close to the targets but still outside of it, which I want to avoid at all cost when the target was detected (if the blobs don't look too bad of course).  So my idea was that even if the shape was approximate its mean was almost always inside the target. So I use my first approach and then apply a clustering algorithm on the points extracted to get the sets of all the means of the clusters.I get very good result if I know the number of the clusters (of course I don't), I also tried MeanShift algorithm but it did not work quite as well.But this process involves a lot of choice of hyper parameters (size of the neighborhood number of clusters bandwith for mean shift).Those algorithms were the results of me trying (very badly I am sure) to deal this very new problem that I could not find anywhere in the litterature and that I have not encountered before. Do you have some solutions that would work better, links to articles that deal with similar issues, new ideas, etc.Doing the NMS in one step would be great.(I work with python and tensorflow but would accept anything)","Creater_id":80720,"Start_date":"2016-03-17 10:38:01","Question_id":202229,"Tags":["deep-learning","computer-vision","heatmap"],"Answer_count":0,"Last_activity":"2016-07-27 08:25:13","Link":"http://stats.stackexchange.com/questions/202229/extracting-centers-from-probabilistic-heatmaps","Creator_reputation":201}
{"_id":{"$oid":"5837a58ca05283111e4d7584"},"View_count":7,"Display_name":"Marco Mene","Question_score":0,"Question_content":"I am facing a problem with mobile-application data.I want to build a model to predict the number of daily downloads of an application on the App Store, using as predictors the daily rankings of the app for a set of N relevant searches possibly used by an App Store user.Example:the app X allows you to color and personalize your phone keyboardsearch \"keyboard personalization\": ranks 2nd out of 100 --\u003e will yield many downloadssearch \"color keyboard\" ranks 1st out of 100 --\u003e will yield many downloadssearch \"paintings\": ranks 90th out of 100 --\u003e will yield few downloadsFor sure this is a multi-dimensional problems that will require regularization, as most of the searches are correlated.I have historical time series of downloads and rankings.Ideally, beside having good prediction power, I would like to be able to spot individual search contribution to the total downloads too. I would go with an additive model..Any ideas/suggestions on how to approach the problem? Any reference to something similar already done?","Creater_id":94865,"Start_date":"2016-07-27 08:23:02","Question_id":225920,"Tags":["prediction","inference","regularization","high-dimensional","application"],"Answer_count":0,"Last_activity":"2016-07-27 08:23:02","Link":"http://stats.stackexchange.com/questions/225920/predict-app-downloads-based-on-a-set-of-search-rankings","Creator_reputation":21}
{"_id":{"$oid":"5837a58ca05283111e4d7586"},"View_count":45,"Display_name":"Ruggero","Question_score":0,"Question_content":"I'm reading different papers about Neural Networks, which express results on certain datasets in a percentage value called mAP. Could anyone explain me  the meaning of mAP?","Creater_id":124687,"Start_date":"2016-07-27 06:15:05","Question_id":225897,"Tags":["neural-networks","terminology"],"Answer_count":1,"Last_activity":"2016-07-27 08:22:43","Link":"http://stats.stackexchange.com/questions/225897/what-does-map-mean","Creator_reputation":3}
{"_id":{"$oid":"5837a58ca05283111e4d7593"},"View_count":62,"Display_name":"Brash Equilibrium","Question_score":0,"Question_content":"I have some data on prices that I would like to predict as a function of covariates. One of those covariates is the predicted rating of that item as estimated from totally separate item rating data. I would like to treat this covariate as an out-of-sample prediction. So I would load the separate price and ratings data in the data step, estimate the parameters of the rating distribution in the model step, and then estimate the parameters of the price model given a proposal from the posterior predictive distribution of ratings.Is this possible in Stan? Thank you.P.S. Let me know if you need more details to formulate an answer. I hope this is enough but maybe not.","Creater_id":7616,"Start_date":"2016-07-26 16:43:17","Question_id":225804,"Tags":["stan"],"Answer_count":1,"Last_activity":"2016-07-27 08:17:39","Link":"http://stats.stackexchange.com/questions/225804/using-posterior-predicted-values-in-estimation-in-stan","Creator_reputation":1523}
{"_id":{"$oid":"5837a58ca05283111e4d75a0"},"View_count":25,"Display_name":"Numrok","Question_score":2,"Question_content":"This question is about how to interpret a likelihood function obtained from Bayesian methods. I will give quite a bit of background and thoughts, but the question in the end is about how to interpret a very skewed and long tailed likelihood distribution. I hope that all the physics is not distracting/too long, I just wanted to make the example well defined enough to be referred to. I am also a bit of an amateur in data analysis, just know a tiny bit of Bayesian stuff.Quick backgroundMost of this does not really matter for the question, but just a quick overview of where the data comes from. In my undergraduate course (that I just finished) we had to do an experiment whose aim was to measure the mass of the  particle (whose true mass is , all masses in this post are in ) from a number of projected momentum measurements after a stationary decay. The instructions we got suggested a method to analyse the data which I though was overly simplistic, since it completely ignored individual error bars. I then came up with a Bayesian method to get a likelihood function for the mass of the particle. The method is essentially a Bayesian distribution fitting that also works for asymmetric error bars, which is significant (in my opinion) in this case.As usual, this analysis yielded a likelihood function, in this case for the mass of the particle (see graph below).Graph descriptionThe graph below shows the likelihood function against the mass  in . The function is correctly normalised to integrate to unity over the finite domain. In addition I calculated a number of potentially useful parameters and put them on the plot:Maximum at Mean at Then I also calculated a variety of possible width measures:1 confidence interval (I hope that is correct terminology): the integrated probability between the red boundary is equal to the 1 probability.The Süssmann measure, which is often used for distributions with a long tailThe standard deviation of a Gaussian that locally fits the distribution at the maximumQuestionThis distribution has 2 problems:it is very skewedit has a long tailMy question is how to express the result from this distribution. For the value of the mass we would usually give . The width or error is what I am concerned about. A priori I would have said that the confidence interval would represent the error well. The problem is that from observation I know that it massively underestimates how good the measurement is. As you can see  is very close to the true value, which might be luck of course, but was the case in a variety of such experiments (I analysed a number of students' data). The confidence interval however has a large width compared to width.So my observations don't quite fit the results I get from Bayesian methods. That might of course be a problem with my model in the first place, my intuition however is that this comes from the long tail of the distribution. E.g. the local Gaussian approximation (statistically probably not very useful, but to understand this distribution it is) gives a much smaller width estimate. So my question is: how to deal with such a distribution in general? And if you would like to work with the example I gave: is there anything wrong with my analysis of it or do I have to go back to my model to search for the mistake.","Creater_id":117145,"Start_date":"2016-07-27 07:57:57","Question_id":225913,"Tags":["bayesian","maximum-likelihood"],"Answer_count":1,"Last_activity":"2016-07-27 08:15:28","Link":"http://stats.stackexchange.com/questions/225913/how-to-interpret-long-tailed-likelihood-distributions-and-their-measurement-unce","Creator_reputation":111}
{"_id":{"$oid":"5837a58ca05283111e4d75ad"},"View_count":25,"Display_name":"Jonalyn","Question_score":1,"Question_content":"Can someone help me on Sparse PCA? I am using the \"elasticnet\" package to perform sparse PCA. I am having a hard time in figuring out how many nonzero values should a component contain? For example, In this code: sparse.pca.result \u0026lt;-  spca(X, K = 2, type = \"predictor\", sparse = \"varnum\", para = c(4, 4))(para = c(4, 4))indicates the number of non-zero components for each of the two PC’s respectively.So the question is, how to identify the number of non-zero components?I hope that someone could help me on this. Thank you :) ","Creater_id":124722,"Start_date":"2016-07-27 08:15:16","Question_id":225916,"Tags":["pca","sparse"],"Answer_count":0,"Last_activity":"2016-07-27 08:15:16","Link":"http://stats.stackexchange.com/questions/225916/sparse-pca-using-elasticnet-package-in-r-how-to-know-how-many-number-of-nonzer","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d75af"},"View_count":32,"Display_name":"jf328","Question_score":0,"Question_content":"What is a goodness of fit measure for quantile regression? I'm not too familiar with the theory but in R the AIC and log-likelihood are given in quantreg package. So I can use that to do variable selection.But if I want to compare two fitting methods, eg whether quantreg does better than quantregForest, what shall I do? Maybe trimmed residual mean square?","Creater_id":20551,"Start_date":"2016-07-27 07:17:43","Question_id":225908,"Tags":["r","quantile-regression"],"Answer_count":0,"Last_activity":"2016-07-27 07:17:43","Link":"http://stats.stackexchange.com/questions/225908/goodness-of-fit-measure-for-quantile-regression","Creator_reputation":416}
{"_id":{"$oid":"5837a58ca05283111e4d75b1"},"View_count":109,"Display_name":"radek","Question_score":2,"Question_content":"Starting with a toy example data (http://www.stata-press.com/data/r14/hbp.dta)use http://www.stata-press.com/data/r14/hbp, cleardrop if mi(sex)drop if mi(age_group)sort city sex age_group year, stableby city sex age_group: egen obs = total(hbp)by city sex age_group:  gen pop = _Nby city sex age_group:  keep if _n == 1drop year race hbpreplace city = 4 if city == 5 // used by the the loop later, so sequential betterbysort city (sex age): egen obs_city = total(obs)bysort city (sex age): egen pop_city = total(pop)gen crude = obs_city / pop_cityWe have age \u0026amp; sex specific counts of events obs and population denominator pop in 5 different cities. Crude rate in a city is given by crude variable simply dividing observations by population within city and ignoring age structure.  Now, I'd like to use information from all cities to create age \u0026amp; sex specific rates and using that - calculated expected number of events given the study population. I'm trying to achieve that by using city specific standardized rates:dstdize obs pop age sex, by(city) print saving( \"temp\", replace )mat b = r(adj)matrix list bgen adjusted = .gen exp_city_ds = .forv city = 1/4 {    local bw = b[1, `city']    display `bw'    replace adjusted = `bw' if city == `city'    replace exp_city_ds = pop_city * `bw' if city == `city'}So far so good, but when I look at results I'd obtain from simple poisson model - the numbers do not add up:poisson obs i.sex i.age, exp(pop) irr predict exp_p, nbysort city (sex age): egen exp_city_poi = total(exp_p)drop exp_pgen dif0 = obs_city - exp_city_ds   gen dif1 = obs_city - exp_city_poi  gen dif2 = exp_city_ds - exp_city_poisu dif? if sex == 1 \u0026amp; age == 2 // first obs by citysaveold \"temp_old.dta\", v(12) replace // data for RI also tried to calculate from the same dummy dataset observed number of case using expected function of SpatialEpi package:library(foreign)library(SpatialEpi)data \u0026lt;- read.dta(\"temp_old.dta\") #, convert.factors = FALSE)expected(dataobs, 8)seq(1, 4, 1)city \u0026lt;- cbind(seq(1, 4, 1), expected(dataobs, 8))colnames(city) \u0026lt;- c(\"city\", \"E\")data \u0026lt;- merge(data, city, by=\"city\")data$poi_fit \u0026lt;- fitted(glm(obs ~ 1 + offset(log(pop)), data = data, family = \"poisson\"))Again - I get different numbers, very close to Poisson fit (both Stata \u0026amp; R) but not the same.Now, my question: is the reason of such difference lying in my calculations/methods? Or is it expected?","Creater_id":22,"Start_date":"2016-06-16 08:28:55","Question_id":219215,"Tags":["r","stata","epidemiology"],"Answer_count":1,"Last_activity":"2016-07-27 07:10:34","Link":"http://stats.stackexchange.com/questions/219215/expected-number-of-cases-standardizing-with-age-sex-of-the-whole-population","Creator_reputation":573}
{"_id":{"$oid":"5837a58ca05283111e4d75be"},"View_count":26,"Display_name":"Kevin Nowaczyk","Question_score":1,"Question_content":"I was putzing around with Excel and the LOGEST function. This fits your data to . It does this by linearizing the data, and doing a least-squares fit to  It then transforms the parameters back. However, I noticed that when the \"Additional regression parameters\" are returned, they are exactly the same as if I had done a linear fit on  vs , including the Sum Of Squares and standard error of the parameters. The list is stand_err_a, stand_err_b, standard_err_y, r², degree_freedom, F, SSreg, SSres.The only values that are any different are the  and  parameters.Is this the expected result? Should the standard errors be transformed back to the log form or not? What about the sum of squares?Additionally, maybe the bigger question is...is there a better procedure (numeric, or matrix) to fit log data?","Creater_id":82897,"Start_date":"2016-07-27 06:37:33","Question_id":225901,"Tags":["regression","excel","logarithm"],"Answer_count":0,"Last_activity":"2016-07-27 07:05:54","Link":"http://stats.stackexchange.com/questions/225901/are-the-standard-errors-of-parameters-from-regression-of-log-data-in-excel-corre","Creator_reputation":237}
{"_id":{"$oid":"5837a58ca05283111e4d75c0"},"View_count":26,"Display_name":"8bytez","Question_score":1,"Question_content":"I want to test if the means of two populations are different from each other. There are 12 poulations, all with different sample size, ranging from 48.000 to 300. Here is a 20.000 rows sample. The data is the result of quantitative content analysis. str(data)data.frame: 75627 obs. of 2 variables: B: num 10 98 56 72 98.3 ....mean \u0026lt;- tapply(dataB, mean)mean1    2    3    4    5    6    7   ...6.1  5.2  5.2  5.4  5.1. 4.8  5.3 ...stdev \u0026lt;- tapply(dataB, sd)1    2    3    4    5    6    7   ...4.0  4.0  4.1  3.9  3.6  3.8  3.9 ...table(dataB, 5000))Shapiro-Wilk normality testdata: head(dataB ~ data$A, center = median)Levene´s Test for Homogeneity of Variance          DF  F Value  Pr(\u0026gt;F)group     10  9.9826   \u0026lt; 2.2e-16 ***       77219The Breusch-Pagan test gives me a value of 0.0005622, so it seems the data is heteroskedasticMy question: how can i test under these circumstances, if a mean of one group is statistically different from another?The Central Limit Theorem says that I could use a t.test, but the shapiro.test indicates, that it is far from normally distributed. Furthermore the variance seem to be different from each other and heteroskedastic.","Creater_id":124053,"Start_date":"2016-07-27 07:02:24","Question_id":225905,"Tags":["r","statistical-significance","normal-distribution","t-test","heteroscedasticity"],"Answer_count":0,"Last_activity":"2016-07-27 07:02:24","Link":"http://stats.stackexchange.com/questions/225905/how-to-test-differences-in-means-in-a-population-that-is-not-normally-distribute","Creator_reputation":11}
{"_id":{"$oid":"5837a58ca05283111e4d75c2"},"View_count":3482,"Display_name":"DPS","Question_score":12,"Question_content":"I am surveying the use of statistical significance testing (SST) to validate the results of cluster analysis. I have found several papers around this topic, such as\"Statistical Signiﬁcance of Clustering for High-Dimension, Low–Sample Size Data\" by Liu, Yufeng et al. (2008)\"On some significance tests in cluster analysis\", by Bock (1985)But I am interested in finding some literature arguing that SST is NOT appropriate to validate results of cluster analysis. The only source I have found claiming this is a web page of a software vendorTo clarify:I am interested in testing whether a significant cluster structure has been found as a result of cluster analysis, so, I'd like to know of papers supporting or refuting the concern \"about the possibility of post-hoc testing of the results of exploratory data analysis used to find clusters\". I've just found a paper from 2003,  \"Clustering and classification methods\" by Milligan and Hirtle saying, for example, that using ANOVA would be an invalid analysis since data have not have random assignments to the groups.","Creater_id":7508,"Start_date":"2011-11-21 07:02:36","Question_id":18706,"Tags":["hypothesis-testing","clustering","statistical-significance"],"Answer_count":1,"Last_activity":"2016-07-27 06:45:55","Link":"http://stats.stackexchange.com/questions/18706/using-statistical-significance-test-to-validate-cluster-analysis-results","Creator_reputation":101}
{"_id":{"$oid":"5837a58ca05283111e4d75cf"},"View_count":20,"Display_name":"cyzyk","Question_score":0,"Question_content":"I have the following problem:I have results of a survey conducted in multiple subgroups, but I do not have access to raw data (so basically I have results like mean, median, min, max, a sample size in each group).Now I will have new subgroups \u0026amp; new responses. How to correctly compare result if I want to say something about new subgroups without having access to raw data of old subgroups?","Creater_id":21578,"Start_date":"2016-07-27 02:29:12","Question_id":225859,"Tags":["survey"],"Answer_count":0,"Last_activity":"2016-07-27 06:36:25","Link":"http://stats.stackexchange.com/questions/225859/survey-results-comparison-with-only-aggregated-metrics-means-median-etc","Creator_reputation":16}
{"_id":{"$oid":"5837a58ca05283111e4d75d1"},"View_count":66,"Display_name":"William Shakespeare","Question_score":0,"Question_content":"I am trying to translate some logistic regressions from SAS to Mplus.  Some are fairly straightforward, e.g., no random effects, and others are mixed models with random intercepts.  The example here will be a fixed effects model and my question will involve both theoretical and programming issues.  I have a binary outcome and a set of predictors, most of which are binary and two of which are continuous.  My outcome has missing data and two predictors as well (tcodec, nonwhite). I believe I can treat missing data as MAR.  I wish to use FIML to estimate the model.  I'd prefer to use Mplus and not PROC CALIS.  In terms of SAS, the model (without FIML) would look like this:proc glimmix data=have;  class sex ster bwc bncat g1 g2 g3 g4               r6 r7 r8 r9 r10 r11 ;  model y(event='1')=tcodec sex ster bwc bncat g1 g2 g3 g4               r6 r7 r8 r9 r10 r11 nonwhite /dist=bin link=logit ;run;To get this into Mplus takes a bit more work:  data: file=have.csv;  variable: names=y tcodec sex ster bwc bncat g1 g2 g3 g4                   r6 r7 r8 r9 r0 r1 nonwhite ;            usevariables=y tcodec sex ster bwc bncat g1 g2 g3 g4                   r6 r7 r8 r9 r0 r1 nonwhite ;            missing=.;            categorical=y;  analysis: estimator=mlr;            INTEGRATION = MONTECARLO(5000);            processors=4;  model: y on tcodec sex ster bwc bncat g1 g2 g3 g4                   r6 r7 r8 r9 r0 r11 nonwhite ;          tcodec sex ster bwc bncat g1 g2 g3 g4                   r6 r7 r8 r9 r10 r11 nonwhite ;So, my understanding is that the second line in the model statement creates latent variables for the exogenous variables so that they can essentially be dependent variables and the missing data estimated.  There are several other things going on:Mplus treats all predictors as continuous (at least that's what I was told by their tech support).There are (17*18)/2=153 elements in the covariance matrix.Mplus wants to estimate tau, the threshold of y. This is 1 element in my example.Mplus wants to estimate alpha, the mean/intercept of the continuous latent variables. This is 16 elements in my example.Mplus wants to estimate beta, the slope coefficients for the regressions of latent continuous variables on each other. This is 16 elements in my example.Mplus wants to estimate psi, the covariance matrix of the continuous latent variables.  This should be just the predictors, (16*17)/2=136.So you can see that there is an identification problem: 136+16+16+1=169.  I've seen examples of FIML in Mplus with normal outcomes that seem fairly straightforward, but nothing with binary outcomes.  Are there logical constraints (remember, I'm trying to get this to replicate the SAS code) that I can set to make the model identified? And don't I need to set the lambdas to 1 for the observed to latent loadings (they seem to be set to 0 by default)?EDIT: Output as requested. It's very lengthy, so I'll include only relevant pieces.  I'm including the full variables names which I abbreviated above for the sake of conciseness.  First, the attempted FIML analysis.  SUMMARY OF ANALYSISNumber of groups                                                 1Number of observations                                       76171Number of dependent variables                                    1Number of independent variables                                 16Number of continuous latent variables                            0Observed dependent variables  Binary and ordered categorical (ordinal)   ROPRECObserved independent variables   TCODEC      SEXREC      STEROIDS    BWC         BNCAT       GA1   GA2         GA3         GA4         YR06        YR07        YR08   YR09        YR10        YR11        NONWHITEEstimator                                                      MLRInformation matrix                                        OBSERVEDOptimization Specifications for the Quasi-Newton Algorithm forContinuous Outcomes  Maximum number of iterations                                 100  Convergence criterion                                  0.100D-05Optimization Specifications for the EM Algorithm  Maximum number of iterations                                 500  Convergence criteria    Loglikelihood change                                 0.100D-02    Relative loglikelihood change                        0.100D-05    Derivative                                           0.100D-02Optimization Specifications for the M step of the EM Algorithm forCategorical Latent variables  Number of M step iterations                                    1  M step convergence criterion                           0.100D-02  Basis for M step termination                           ITERATIONOptimization Specifications for the M step of the EM Algorithm forCensored, Binary or Ordered Categorical (Ordinal), UnorderedCategorical (Nominal) and Count Outcomes  Number of M step iterations                                    1  M step convergence criterion                           0.100D-02  Basis for M step termination                           ITERATION  Maximum value for logit thresholds                            15  Minimum value for logit thresholds                           -15  Minimum expected cell size for chi-square              0.100D-01Maximum number of iterations for H1                           2000Convergence criterion for H1                             0.100D-03Optimization algorithm                                         EMAIntegration Specifications  Type                                                  MONTECARLO  Number of integration points                                5000  Dimensions of numerical integration                            4  Adaptive quadrature                                           ON  Monte Carlo integration seed                                   0Link                                                         LOGITCholesky                                                       OFFTHE STANDARD ERRORS OF THE MODEL PARAMETER ESTIMATES MAY NOT BE TRUSTWORTHY FOR SOME PARAMETERS DUE TO A NON-POSITIVE DEFINITE FIRST-ORDER DERIVATIVE PRODUCT MATRIX.  THIS MAY BE DUE TO THE STARTING VALUES BUT MAY ALSO BE AN INDICATION OF MODEL NONIDENTIFICATION.  THE CONDITION NUMBER IS      -0.268D-16.  PROBLEM INVOLVING THE FOLLOWING PARAMETER: Parameter 98, YR07MODEL FIT INFORMATIONNumber of Free Parameters                      169Loglikelihood          H0 Value                     -417496.815          H0 Scaling Correction Factor      0.9936            for MLRInformation Criteria          Akaike (AIC)                  835331.630          Bayesian (BIC)                836893.314          Sample-Size Adjusted BIC      836356.227            (n* = (n + 2) / 24)      PARAMETER SPECIFICATION           TAU              ROPREC1              ________ 1                 17NU is all zeroesLAMBDA is all zeroesTHETA is all zeroesALPHA is all zeroesBETA (just showing first row-all others are zero)           BETA              ROPREC        TCODEC        SEXREC        STEROIDS      BWC              ________      ________      ________      ________      ________ ROPREC             0             1             2             3             4           BETA              BNCAT         GA1           GA2           GA3           GA4              ________      ________      ________      ________      ________ ROPREC             5             6             7             8             9           BETA              YR06          YR07          YR08          YR09          YR10              ________      ________      ________      ________      ________ ROPREC            10            11            12            13            14           BETA              YR11          NONWHITE              ________      ________ ROPREC            15            16PSI is zero","Creater_id":28835,"Start_date":"2016-07-26 10:05:27","Question_id":225740,"Tags":["logistic","missing-data","sem"],"Answer_count":0,"Last_activity":"2016-07-27 06:11:17","Link":"http://stats.stackexchange.com/questions/225740/fiml-using-mplus","Creator_reputation":74}
{"_id":{"$oid":"5837a58ca05283111e4d75d3"},"View_count":28,"Display_name":"Dr ashraf","Question_score":-3,"Question_content":"I want to find the standard error and standard deviationMin. = 8 Х104Max. = 8Х107 -  = 9 Х106n = 36","Creater_id":124700,"Start_date":"2016-07-27 05:33:44","Question_id":225889,"Tags":["statistical-significance"],"Answer_count":2,"Last_activity":"2016-07-27 06:01:32","Link":"http://stats.stackexchange.com/questions/225889/finding-standard-error-using-only-mean-min-max-and-number","Creator_reputation":1}
{"_id":{"$oid":"5837a58ca05283111e4d75e1"},"View_count":33,"Display_name":"Dang Manh Truong","Question_score":0,"Question_content":"I'm working on an assigment about Eigenfaces for facial recognition: http://courses.cs.washington.edu/courses/cse455/10wi/projects/p2/ . Here is the dataset: http://courses.cs.washington.edu/courses/cse455/10wi/projects/p2/class_images.zip , there are 3 folders but I'm focusing on the \"smiling_cropped\" one right now. The formulas are in this page: http://www.vision.jhu.edu/teaching/vision08/Handouts/case_study_pca1.pdf (After some reading I found that it is completely relevant to the assignment). And here is a recap of the algorithm:- Step 1: Calculate the eigenvectors of the images's covariance matrix. The results are called Eigenfaces- Step 2: Given a new face, we project in onto the \"face space\" spanned by the Eigenface vectors, and we do that by multiplying the new face (mean-subtracted) with the Eigenfaces, resulting in a column-vector of coefficients. These coefficients are (supposed to) help us reconstructing the original face. For more information you should check the links above.The problem I'm facing is with step 2: If I use one of the training faces in this step then I should be able to reconstruct the original image. But I was only able to do that by normalizing the coefficient vector (divide every element by the Euclidean norm of the vector). I really don't know why this happens :(Here is the code (main part): clear% Initialization (machine-dependent so you should change this part to suit% yours)numImages = 36;k = 35; % Number of eigenfaces that we needpreDir = 'D:/Hoctap/MachineLearning/CSE_455_Projects/CSE_455_Project_2/class_images/';smileOrNot = 'smiling_cropped';%smileOrNot = 'nonsmiling_cropped';middleDir = '/face';postDir = '.jpg';% Load the imagesfaces = cell(1,numImages);for i = 1 : numImages    dir = strcat(preDir,smileOrNot,middleDir,num2str(i),postDir);        faces{i} = imread(dir);    end% Calculate the eigenfaces[avgface, eigfaces] = eigenfaces(faces,k );% RGB = 3;% eigVals = reshape(double(eigfaces), 57 * 57 * RGB,k);% eigVals(:,1)' * eigVals(:,2)% pause% for i = 1 : k%     imshow(eigfaces(:,:,:,i))%     pause% endcoeffs = project_face(avgface,eigfaces,faces{24}) ;coeffs = coeffs ./ norm(coeffs,2);imshow(faces{24})pauseshit = avgface;%imshow(uint8(round(shit)))%imshow(shit)% pausefor i = 2 : k%      imshow(eigfaces(:,:,:,i))%      pause    shit = shit + coeffs(i) * eigfaces(:,:,:,i) ;%      imshow( uint8(round(shit)))%      pauseendimshow(uint8(round(shit)) )Function to calculate eigenfaces: function [avgface, eigfaces] = eigenfaces(faces,k )% This function uses PCA to generate eigenfaces from the set of user faces% Input: faces -  the set of faces in a cell array of 2xd images (example : 32x32 images) %      : k - the number of eigenfaces to compute% Output: avgface - the average face (in image form)%       : eigface - the eigen faces (in image form)% Written by Dang Manh TruongnumFaces = numel(faces);RGB = 3; % 3 colours% Find the average size of all the imagesavgSize = 0; for i = 1 : numFaces    avgSize = avgSize + size(faces{i},1);endavgSize = round(avgSize / numFaces);faceVec = zeros(avgSize * avgSize * RGB, numFaces); % Resize all images to the average size for i = 1 : numFaces         resizedImage = my_imresize(double(faces{i}),avgSize,avgSize);        faceVec(:,i) = reshape(resizedImage,[],1);    end% Calculate the average faceavgVec = sum(faceVec,2) ./ numFaces;avgface = reshape(avgVec,size(faces{1}));%avgface = uint8(round(avgface));% imshow(avgface)% pause% Find the eigenfaces newFaces = faceVec - repmat(avgVec,1,numFaces);reducedCov = newFaces' * newFaces;[reducedEig,D] = eigs(reducedCov,k);for i = 1 : size(D,1)    D(i,i) = D(i,i) / norm(reducedEig(:,i) ,2);end[~,I] = sort(diag(D),'descend');reducedEig = reducedEig(:, I);eigfaces = newFaces * reducedEig;% eigfaces(:,1)' * eigfaces(:,2)% pause% Reshape back to image% eigfaces = reshape( uint8(round(eigfaces)) , avgSize,avgSize,RGB,k);eigfaces = reshape(eigfaces, avgSize, avgSize, RGB, k);% We can only show the eigenfaces here before they are normalized% for i = 1 : k%     imshow( reshape(uint8(round( eigValues(:,i))),avgSize,avgSize,RGB));%     pause% end% Normalize the eigenfaces. NOTE: DO NOT DO THIS BECAUSE EIGENVALUES WOULD% GO TO ZERO ONCE THEY ARE CONVERTED BACK TO UINT8% for i = 1 : k%     eigValues(:,i) = eigValues(:,i) ./ norm(eigValues(:,i),2);    % endStarter function to help with resizing images with antialiasing:function b = my_imresize(a,m,n)%MY_IMRESIZE Resize image with antialiasing.%  B = MY_IMRESIZE(A,M,N) resizes image A to be M-by-N using bilinear %  interpolation.  Before performing the interpolation, A is blurred with %  a Gaussian in each dimension to prevent aliasing.% Credit goes to : http://courses.cs.washington.edu/courses/cse455/10wi/projects/p2/[height, width,  channels] = size(a);% determine the size of the Gaussian blur kernelsigma1 = 0.5 * height / m;sigma2 = 0.5 * width / n;k1 = ceil(3 * sigma1);k2 = ceil(3 * sigma2);% construct the (separable) kerneld1 = linspace(-k1,k1,2*k1+1).^2 / sigma1^2;d2 = linspace(-k2,k2,2*k2+1).^2 / sigma2^2;h1 = exp(-d1)';h2 = exp(-d2);h1 = h1 / sum(h1);h2 = h2 / sum(h2);% allocate space for the resized imageb = zeros(m,n,channels);for i=1:channels  % pad the image by duplicating the boundary  temp = [repmat(a(1,1,i),k1,k2)   repmat(a(1,:,i),k1,1)   repmat(a(1,end,i),k1,k2);          repmat(a(:,1,i),1,k2)    a(:,:,i)                repmat(a(:,end,i),1,k2);          repmat(a(end,1,i),k1,k2) repmat(a(end,:,i),k1,1) repmat(a(end,end,i),k1,k2)];  % blur this image channel with the kernel  temp = conv2(temp,h1,'full');  temp = conv2(temp,h2,'full');  % perform the bilinear interpolation  x = linspace(2*k2+0.5, width+2*k2+0.5, 2*n+1);  y = linspace(2*k1+0.5, height+2*k1+0.5, 2*m+1);  [x y] = meshgrid(x(2:2:end-1),y(2:2:end-1));  b(:,:,i) = interp2(temp,x,y,'linear');endFunction to calculate coefficients in step 2: function coeffs = project_face(avgface,eigfaces,newface) % This function projects a new face onto the \"face space\" spanned by the% eigenfaces previously computed , in order to generate a vector of% \"k\" coefficients, one for each of the \"k\" eigenfaces% Inputs:% Outputs: % Written by Dang Manh Truong% No matter what kind of image we are dealing with, the number of principal% components is always at the end of the \"size\"numComponents = size(eigfaces, numel(size(eigfaces)) ); % Resize the new face to be of the same size as the average faceavgSize = size(avgface,1);newface =  double(my_imresize(double(newface),avgSize,avgSize));% Vectorize the eigenfaces and the new facenewface = reshape(newface,[],1);eigfaces = reshape(eigfaces,[], numComponents);avgface = reshape(avgface,[],1);%The eigenfaces need to be normalized to have a norm of 1for i = 1 : numComponents    eigfaces(:,i) = eigfaces(:,i) ./ norm(eigfaces(:,i),2);    end% Calculate the coefficients by projecting them onto the \"face space\"% spanned by the eigenfacescoeffs = eigfaces' * (newface - avgface);%coeffs = eigfaces' * newface;% coeffs% pauseend","Creater_id":120103,"Start_date":"2016-07-27 05:55:07","Question_id":225893,"Tags":["pca","eigenvalues"],"Answer_count":0,"Last_activity":"2016-07-27 05:55:07","Link":"http://stats.stackexchange.com/questions/225893/facial-recognition-phase-in-eigenfaces-requires-normalizing-coefficients","Creator_reputation":62}
{"_id":{"$oid":"5837a58ca05283111e4d75e3"},"View_count":21,"Display_name":"ruth","Question_score":1,"Question_content":"I have t and beta (the estimate of the standardized coefficient), but I didn't write down the coefficients or their standard errors.  How can I work out the unstandardized coefficients?","Creater_id":124631,"Start_date":"2016-07-26 16:23:26","Question_id":225802,"Tags":["multiple-regression"],"Answer_count":0,"Last_activity":"2016-07-27 05:47:31","Link":"http://stats.stackexchange.com/questions/225802/can-i-work-out-the-regression-coefficients-from-the-betas-and-the-t-values","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d75e5"},"View_count":85,"Display_name":"LoveMeow","Question_score":0,"Question_content":"I am using Scikit learn to perform PCA and Feature Agglomeration of my survey data. I have done the following :#PCApca = PCA(n_components=10)pca.fit(adf_nan_norm)hello = pca.get_covariance()adf_nan_norm is a dataframe. My data frame has 181 rows and 36 columns. The columns are my features. All data has 0 mean and std 1.I would like to get the features (or) combination of features that have maximum variance(indices or the names?), I am only able to get the variance of those features which are maximum, but not which features were selected or combined.In addition, I am trying to find which features are very similar to each other, I tried the following example from http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.htmlI don't understand how I can use my dataframe adf_nan_norm in this code and obtainthe feature that are similar? I would like to have the index or names of the columns so I can infer similarities?digits = datasets.load_digits()\u0026gt;\u0026gt;\u0026gt; images = digits.images\u0026gt;\u0026gt;\u0026gt; X = np.reshape(images, (len(images), -1))\u0026gt;\u0026gt;\u0026gt; connectivity = grid_to_graph(*images[0].shape)\u0026gt;\u0026gt;\u0026gt; agglo = cluster.FeatureAgglomeration(connectivity=connectivity,...                                      n_clusters=32)\u0026gt;\u0026gt;\u0026gt; agglo.fit(X) FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',...\u0026gt;\u0026gt;\u0026gt; X_reduced = agglo.transform(X)\u0026gt;\u0026gt;\u0026gt; X_approx = agglo.inverse_transform(X_reduced)\u0026gt;\u0026gt;\u0026gt; images_approx = np.reshape(X_approx, images.shape)When I replace my dataframe it does not work. Furthermore I am unable to obtain the columns that are similar.Thank you!","Creater_id":53787,"Start_date":"2016-07-27 05:37:34","Question_id":225890,"Tags":["pca","feature-selection","python"],"Answer_count":0,"Last_activity":"2016-07-27 05:37:34","Link":"http://stats.stackexchange.com/questions/225890/scikit-learn-python-pca-and-feature-agglomeration-feature-indices","Creator_reputation":121}
{"_id":{"$oid":"5837a58ca05283111e4d75e7"},"View_count":1353,"Display_name":"Arjun Aletty","Question_score":8,"Question_content":"I've recently entered the realm of machine learning and a project I am working on requires me to cluster users based on the order they visited webpages on a website. I have data in the form of:['user_id', 1, 2, 4, 6, 3, 7, 3, 2, 4...]Where each number is a category/page that the user visited. In addition the length of data for each user is not the same i.e. some users visit more pages than others.I realize this is really vague and defining similarity it hard. I tried following the example in this research paper and to be honest a lot of it went over my head.I need help in how to approach this problem and am open to new ideas and suggestions.","Creater_id":12499,"Start_date":"2012-07-09 16:38:27","Question_id":31972,"Tags":["clustering","unsupervised-learning","model-based-clustering"],"Answer_count":2,"Last_activity":"2016-07-27 05:27:42","Link":"http://stats.stackexchange.com/questions/31972/cluster-clickstream-data","Creator_reputation":41}
{"_id":{"$oid":"5837a58ca05283111e4d75f5"},"View_count":27,"Display_name":"Irit Sella","Question_score":1,"Question_content":"I have drawing data from 10 subjects drawing 8 shapes (and many repetitions of every shape). I fitted a model to this data Matlab, separately for each subject and each shapes, such that I have 2 model- parameters and an r^2 measure of the goodness of fit, in an 8x10 matrix (8 shapes, 10 subjects).For each of these 3 variables I would like to know:Does the parameter differ between shapes?Do subjects differ in the parameter?I think I need a linear mixed-model, where shapes are fixed and subjects are random, but I’m not sure (shapes are categorical, but this can still be modeled by linear models, right?). I tried the following:gamma.model = lmer(gamma ~ Shape + (1|Subject), data = gammatable, REML = FALSE)summary(gamma.model) shows me a p-value for shape1 vs. every other shape, but not the relation between shape2 and shape 8 (for instance).There is also no mentioning at all of random effects...? Can someone please help me understand -If indeed I need a mixed-model (or if not – what do I need?)How to define it correctly in lme4 (or other)How to perform multiple comparisons and get p-values for fixed, as well as for random effects?Thanks a lot! ","Creater_id":124248,"Start_date":"2016-07-25 08:56:15","Question_id":225543,"Tags":["mixed-model","categorical-data","lme4"],"Answer_count":1,"Last_activity":"2016-07-27 05:21:32","Link":"http://stats.stackexchange.com/questions/225543/mixed-model-on-1-level-8-catagory-design-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d7602"},"View_count":98,"Display_name":"Ferenc Beleznay","Question_score":2,"Question_content":"In the textbooks I have access to (and that discuss hypothesis testing for correlation), I only met examples, where the null-hypothesis was , and the alternative hypothesis was . My question is about using a one-sided alternative hypothesis . Is this meaningful? This question has been asked before, but it has not been answered. There was a comment next to the linked question, that said that the null-hypothesis should be  in case we would like a one-sided alternative hypothesis, but I have problems with this comment. As I understand, the t-distribution that is used for testing the correlation coefficient is only valid when , so we have no choice, but using this as the null-hypothesis.So, to summarize: can we test  against  using  and the t-distribution with degree of freedom ?","Creater_id":124605,"Start_date":"2016-07-27 04:06:18","Question_id":225874,"Tags":["hypothesis-testing","correlation"],"Answer_count":1,"Last_activity":"2016-07-27 05:01:09","Link":"http://stats.stackexchange.com/questions/225874/one-sided-hypothesis-test-for-correlation","Creator_reputation":11}
{"_id":{"$oid":"5837a58ca05283111e4d760f"},"View_count":7,"Display_name":"Ji Soo Song","Question_score":1,"Question_content":"I've measured ADC values of liver and spleen in 50 patients, and these patients underwent three rounds of DWI at different time points (so there's total of 150 ADC values measured each for liver and spleen). What I want to do is compare the variability of liver ADC and spleen ADC at different time points. At first, I calculated coefficient of variance (CV) on the basis of three data points per patient, and then compared the CV of liver and CV of spleen using Wilcoxon test. But the reviewer recommended the following method:\"by dividing each individual data point by the mean over the entire sample, they derive measures having a standard deviation equal to the CV of the original data. Likelihood ratio tests or F tests in a repeated measures or mixed model regression analysis could then be used to compare the variances of the rescaled data, which is equivalent to comparing the CV values of the unscaled data.\"Can anyone give me some more detailed information about this recommendation? How can I compare the variability of ADC values at three different time points?","Creater_id":124693,"Start_date":"2016-07-27 05:00:44","Question_id":225883,"Tags":["likelihood","f-test"],"Answer_count":0,"Last_activity":"2016-07-27 05:00:44","Link":"http://stats.stackexchange.com/questions/225883/how-can-i-compare-the-variability-of-adc-values-at-three-different-time-points","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d7611"},"View_count":28,"Display_name":"Pamela Andrews","Question_score":0,"Question_content":"I am working on a systematic review looking at the global prevalence of pain, I am trying to do a meta regression to examine what effect gender, age, etc has on the prevalence of pain. I have no prior knowledge of regression analysis and I am struggling to figure out my next move, I can use either SPSS or r (prefer SPSS) i have 23 prevalence estimates, any help would be appreciated.","Creater_id":124678,"Start_date":"2016-07-27 03:03:02","Question_id":225865,"Tags":["regression","meta-regression","systematic-review"],"Answer_count":1,"Last_activity":"2016-07-27 04:42:42","Link":"http://stats.stackexchange.com/questions/225865/performing-a-meta-regression-of-prevalence-data","Creator_reputation":6}
{"_id":{"$oid":"5837a58ca05283111e4d761e"},"View_count":3722,"Display_name":"pir","Question_score":7,"Question_content":"I'm creating a convolutional neural network (CNN), where I have a convolutional layer followed by a dropout layer and I want to apply dropout to reduce overfitting. I have this feeling that the dropout layer should be applied after the pooling layer, but I don't really have anything to back that up. Where is the right place to add the dropout layer? Before or after the pooling layer?","Creater_id":29025,"Start_date":"2015-04-22 17:05:29","Question_id":147850,"Tags":["deep-learning","conv-neural-network","dropout"],"Answer_count":2,"Last_activity":"2016-07-27 04:05:53","Link":"http://stats.stackexchange.com/questions/147850/are-pooling-layers-added-before-or-after-dropout-layers","Creator_reputation":648}
{"_id":{"$oid":"5837a58ca05283111e4d762c"},"View_count":2049,"Display_name":"mfrmn","Question_score":4,"Question_content":"I am trying to solve for the parameters in the maximum entropy problem in R using the nonlinear system:where  and  represent the lower and upper bound on the support of the distribution, and  are the parameters of interest.I wrote an implementation based on minimising the sum of the squared differences, and then optimising using BBoptim, but this method is very hit-or-miss, and depends heavily upon me inputting reasonable starting parameters, and quite tight lower and upper bounds on the range over which to search. I have included the code below:MaxEnt \u0026lt;- function(y) {G \u0026lt;- function(x) exp(y[1]+y[2]*x+y[3]*x^2) ;H \u0026lt;- function(x) x*exp(y[1]+y[2]*x+y[3]*x^2) ;I \u0026lt;- function(x) x^2*exp(y[1]+y[2]*x+y[3]*x^2) ;(integrate(G, lower=l, upper=u)value - mu)^2+(integrate(I, lower=l, upper=u)\\ y= \\frac{x-l}{u-l}\\ dx = (u-l)dy\\ x= (u-l)y+l\\ a+bx+cx^2 = q+ry+cy^2\\ q=a+bl+cl^2,r=(u-l)(b+2cl),s=c(u-l)^2\\ \\int_0^1 e^{q+ry+sy^2}dy=\\frac{1}{u-l}\\ \\int_0^1 ((u-l)y+l) e^{q+ry+sy^2}dy=\\frac{\\mu}{u-l}\\ \\int_0^1 ((u-l)y+l)^2 e^{q+ry+sy^2}dy=\\frac{\\mu^2+\\sigma^2}{u-l}\\ e^{q+ry+sy^2}\\ \\nu = \\frac{-r}{2s}\\ \\tau = \\sqrt{\\frac{-1}{2s}}\\ e^{q+ry+sy^2} = e^{q-\\frac{r^2}{2s}}e^{-0.5\\left(\\frac{y-\\nu}{\\tau}\\right)^2}\\ \\tau^2 \u0026gt;0 \\ s\u0026lt;0\\ s \u0026lt;0\\ c \u0026lt;0c\u0026gt;0$. I am probably going wrong somewhere, and would be grateful if somebody could point out where. Thanks!","Creater_id":8562,"Start_date":"2012-01-16 09:21:45","Question_id":21173,"Tags":["r","optimization","moments","maximum-entropy"],"Answer_count":1,"Last_activity":"2016-07-27 04:03:25","Link":"http://stats.stackexchange.com/questions/21173/max-entropy-solver-in-r","Creator_reputation":67}
{"_id":{"$oid":"5837a58ca05283111e4d7639"},"View_count":48,"Display_name":"cqs","Question_score":3,"Question_content":"I've been working on the following problem and I'm confused about how to get started:Let  denote i.i.d. real valued random variables, each absolutely continuous with an exponential distribution with parameter . Let  denote real non-zero constants and let . Specify the conditions on  under which  and  are independent. There is a suggestion to consider the gamma distribution. Based on that I've played around with the Moment Generating functions (mgfs) of  (which resembles the gamma function) and of , which as the sum of exponentials has a gamma distribution, and my plan was to show that under certain conditions the joint mgf  =  but I'm not sure how to get a joint distribution of  and . Can anyone help me see how this is done? Is this even the right way to go about solving this? I feel like I'm not really making use of the suggestion in more than a trivial way at present, but I'm not sure how to go about using it.","Creater_id":113980,"Start_date":"2016-04-29 12:16:42","Question_id":210074,"Tags":["probability","self-study","independence","gamma-distribution","joint-distribution"],"Answer_count":0,"Last_activity":"2016-07-27 04:00:27","Link":"http://stats.stackexchange.com/questions/210074/showing-independence-between-two-functions-of-a-set-of-random-variables","Creator_reputation":16}
{"_id":{"$oid":"5837a58ca05283111e4d763b"},"View_count":37,"Display_name":"lte__","Question_score":0,"Question_content":"I have the following problem: I want to predict an event's occurrence by investigating the steps a user goes through. Fe. I want to analyse which webpages a user visited that lead him to buy or not buy a certain product (I'm interested also in predicting just from the data of the buying customers: the webpages the buying user visited and what he bought is the input). I'm thinking this would be ideally solved by some sort of a graph analysis algorithm (ofc, correct me if I'm wrong!), but I'm not sure where to start. I think regression is not the best choice, since we can have information about which page has links to another one, etc. Do you think you could set me up in a direction? Recommend a few algorithms I can check out? I'd like a solution that can improve over time when new data is added (~machine learning).Thank you!","Creater_id":124682,"Start_date":"2016-07-27 03:45:00","Question_id":225871,"Tags":["predictive-models","prediction","algorithms","graph-theory","events"],"Answer_count":0,"Last_activity":"2016-07-27 03:50:05","Link":"http://stats.stackexchange.com/questions/225871/event-prediction-through-path-analysis","Creator_reputation":101}
{"_id":{"$oid":"5837a58ca05283111e4d763d"},"View_count":27,"Display_name":"superciccio14","Question_score":1,"Question_content":"From Wikipedia we can read:  In statistics, dependence or association is any statistical  relationship, whether causal or not, between two random variables or  two sets of data. Correlation is any of a broad class of statistical  relationships involving dependence, though in common usage it most  often refers to the extent to which two variables have a linear  relationship with each other. Familiar examples of dependent phenomena  include the correlation between the physical statures of parents and  their offspring, and the correlation between the demand for a product  and its price.They introduce some concepts: dependence (or association), causality, correlation and statistical relationship.Ok, (linear) correlation does not imply causation and that's fine, but I do not clearly understand the differences between aforementioned concepts, introduced in that wikipedia article.I read a lot of post on this community but I did not find a complete answer to this question.","Creater_id":124677,"Start_date":"2016-07-27 02:45:22","Question_id":225862,"Tags":["mathematical-statistics","independence","causality"],"Answer_count":0,"Last_activity":"2016-07-27 03:37:48","Link":"http://stats.stackexchange.com/questions/225862/differences-among-dependence-statistical-relationship-and-causation","Creator_reputation":106}
{"_id":{"$oid":"5837a58ca05283111e4d763f"},"View_count":1061,"Display_name":"winerd","Question_score":2,"Question_content":"In the following example\u0026gt; m = matrix(c(3, 6, 5, 6), nrow=2)\u0026gt; m     [,1] [,2][1,]    3    5[2,]    6    6\u0026gt; (OR = (3/6)/(5/6))    #1[1] 0.6\u0026gt; fisher.test(m)        #2    Fisher's Exact Test for Count Datadata:  m p-value = 0.6699alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 0.06390055 5.07793271 sample estimates:odds ratio  0.6155891 I calculated the odds ratio (#1) \"manually\", 0.600; then (#2) as one of the outputs of the Fisher's exact test, 0.616. Why didn't I get the same value?","Creater_id":14202,"Start_date":"2013-03-28 05:28:07","Question_id":54530,"Tags":["r","odds-ratio","fishers-exact"],"Answer_count":1,"Last_activity":"2016-07-27 03:16:22","Link":"http://stats.stackexchange.com/questions/54530/different-odds-ratios-from-formula-and-rs-fisher-test","Creator_reputation":187}
{"_id":{"$oid":"5837a58ca05283111e4d764c"},"View_count":73,"Display_name":"user3639557","Question_score":0,"Question_content":"I am reading Escobar\u0026amp;West paper and in particular am interested in their Gibbs sampler for the concentration parameter of Dirichlet Process (eq 13, eq 14). Given this,p(\\alpha,\\eta|k)\\propto p(\\alpha)\\alpha^{k-1}(\\alpha+n)\\eta^\\alpha(1-\\eta)^{n-1} If we replace , with it's  distribution replacement, we get, p(\\alpha,\\eta|k)\\propto \\large(\\frac{1}{\\Gamma(a)b^a}\\alpha^{a-1}e^{\\frac{-\\alpha}{b}}\\large)\\alpha^{k-1}(\\alpha+n)\\eta^\\alpha(1-\\eta)^{n-1} which can be re-written as,p(\\alpha,\\eta|k)\\propto \\frac{1}{\\Gamma(a)b^a}e^{\\frac{-\\alpha}{b}}\\alpha^{a+k-2}(\\alpha+n)e^{\\log\\eta^\\alpha}(1-\\eta)^{n-1} and can be simplified again as,p(\\alpha,\\eta|k)\\propto \\frac{1}{\\Gamma(a)b^a}\\alpha^{a+k-2}(\\alpha+n)e^{(-\\frac{\\alpha}{b}+\\alpha\\log\\eta)}(1-\\eta)^{n-1} and to convert this to , we must take the integral,p(\\alpha|\\eta,k)\\propto \\frac{1}{\\Gamma(a)b^a}\\alpha^{a+k-2}(\\alpha+n)\\int e^{(-\\frac{\\alpha}{b}+\\alpha\\log\\eta)}(1-\\eta)^{n-1}d\\eta but I don't understand how from that we can get to the following relation mentioned in the paper,p(\\alpha|\\eta,k)\\propto \\alpha^{a+k-2}(\\alpha+n)e^{\\alpha(-b+\\log(\\eta))} Assuming that they dropped redundant terms that do not depend on , for example,  and , still it is not clear how they computed the integral. Also, the power of  must be  unless I am skipping a step.","Creater_id":56676,"Start_date":"2016-07-26 23:55:09","Question_id":225834,"Tags":["bayesian","sampling","hierarchical-bayesian","dirichlet-process"],"Answer_count":1,"Last_activity":"2016-07-27 03:14:37","Link":"http://stats.stackexchange.com/questions/225834/sampling-concentration-parameter-of-dp-according-to-escobar-and-west","Creator_reputation":310}
{"_id":{"$oid":"5837a58ca05283111e4d7659"},"View_count":12,"Display_name":"Jack","Question_score":0,"Question_content":"I have data on people's return times which I wanted to fit a distribution to using maximum likelihood estimation.I was planning on using a Weibull or Gamma but there are a high number of return times that are very small leading to a distribution with a huge peak near zero, but then tailing off quite slowly. This means a fit Weibull/Gamma doesn't describe the tails very well.I was wondering if there are any better distribution options, or should I just use a mixture?","Creater_id":24763,"Start_date":"2016-07-27 03:13:26","Question_id":225868,"Tags":["maximum-likelihood","survival","modeling","queueing","interarrival-time"],"Answer_count":0,"Last_activity":"2016-07-27 03:13:26","Link":"http://stats.stackexchange.com/questions/225868/distribution-for-modelling-return-times-with-inflated-zeros","Creator_reputation":51}
{"_id":{"$oid":"5837a58ca05283111e4d765b"},"View_count":110,"Display_name":"PietroB","Question_score":0,"Question_content":"I am analyzing a segregating population of plants coming from an hybridization process. The experiment consists in several field plots (according to an augmented design). In each plot a segregating population coming from an hybrid plant was seeded. Therefore, the plants into each plot are segregating. I defined several traits corresponding to morphological characteristics of the plant (eg. leaves colour, flower colour, ...). The plants in each plots are segregating. Therefore the shows different characteristics for each of those morphological traits (eg. red or green leaves) and I counted the number of plants in each plot for each of those classes. Therefore I may express the data in my data-set as 'number of plants' or as percentage of e.g green/red plants on the total number of plants in each plots. Since the genetic background of the original hybrids is not known, I would like to run a PCA and a cluster analysis in order to see which populations cluster together according to those traits. Can PCA be applied to such a data-set? Which package can be used for running such an analysis in R?","Creater_id":48492,"Start_date":"2014-07-16 07:53:10","Question_id":108173,"Tags":["pca","count-data","discrete-data"],"Answer_count":1,"Last_activity":"2016-07-27 02:30:20","Link":"http://stats.stackexchange.com/questions/108173/can-principal-component-analyses-be-applied-to-a-counting-trait","Creator_reputation":8}
{"_id":{"$oid":"5837a58da05283111e4d7667"},"View_count":22,"Display_name":"d3vid","Question_score":0,"Question_content":"We are measuring weekly staff performance, so that management can make informed decisions about task assignment. [1] To the end we have measured performance as follows:Workers are assigned batches of widgets to assemble and deliverBatches vary in widget quantity and complexity (complexity is not modelled, at least for now)Worker's performance is measured as number of widgets delivered per weekDelivery happens in batches, so if a batch of 10 is delivered this week, that counts as 10, even if 5 of the widgets were assembled last weekPerformance is visualised as a notched boxplotThe boxplot is annotated with mean performance (total widgets / total weeks) -- outliers are included in this calculationSome workers have been working for 1 week and some for 6 months, so statistical significance varies per box (notch plots reveal this)Assuming this approach is valid (challenges welcome), we are now faced with interpreting the following chart. We are looking to identify:fast, reliable workers: they can always be assigned large/complex batchesworkers who are \"in their groove\": manangement might try assigning them more complex batchesslow and/or underperforming workers: need workload and/or complexity reduced, or need other forms of help or interventionCan we use boxplot measures and the mean to identify these types?To make the question more concrete, here are some sample interpretations for validation/correction (feel free to use or ignore):Ben, Lee and Max have too few measures for statistical significance (weird notches), so take their interpretations lightlyIan and Amy haven't delivered anything yet (we also know this because they've only been working a week)Of the remaining workers, Ivy is the slowest, Ken is the fastestAlmost everyone suffers from too many weeks with no batches complete (Q1 = 0, 25% of weeks with no delivery)Ben, Eli, Sia, Pat and Dom have a bigger delivery problem (Q2 = 0, 50% of weeks with no delivery)Many workers are operating in their \"zone\" (Q2 \u0026lt; mean \u0026lt; Q3, so on average they are delivering in the high end of their IQR)Max is a stable worker (small IQR), who could do better (mean \u0026lt; Q2), but we have to wait and see on this evaluation (weird notches implies low statistical significance)Bob and Pat have roughly equal speed (B mean = P mean), but Bob is more likely to deliver faster (B Q2 \u003e P Q2)Ken is a bit faster Jen (K mean \u003e J mean), but is far more likely to deliver faster (K Q2 much \u003e J Q2)Zoe is slower than Roz (Z mean \u0026lt; R mean), and less likely to deliver faster (Z Q2 \u0026lt; R 2)Jen and Dom are both fast and in their groove, but Jen is much more likely to deliver (50% of Dom's weeks are 0 vs 25% of Jen's)[1] We are aware of the perils of modelling performance on non-trivial, multivariate tasks, and the perils of managing only by the numbers. Our objective is only to make task assignment better-than-random and better-than-hunch.","Creater_id":123034,"Start_date":"2016-07-22 09:00:15","Question_id":225137,"Tags":["boxplot"],"Answer_count":1,"Last_activity":"2016-07-27 02:23:25","Link":"http://stats.stackexchange.com/questions/225137/measuring-performance-with-histogram","Creator_reputation":111}
{"_id":{"$oid":"5837a58da05283111e4d766c"},"View_count":678,"Display_name":"Agus Dwikarna","Question_score":1,"Question_content":"I just found out that machine learning also has logistic regression as one of its methods. Can someone please tell me the differences between logistic regression in statistics and machine learning? I've seen lecture slides on logistic regression from a machine learning course, but I can't see the difference with the coverage of logistic regression in a statistics course.Does logistic regression in machine learning have no need to check for multicollinearity?The reason I asked this is because I've tried to run a dataset through R's glm function with binomial logit, and then I ran the same dataset through Apache Mahout's trainlogistic. But the resulting coefficients are different.This is the command I use in R:w1.glm \u0026lt;- glm(anw ~ cs, data = w1, family = \"binomial\")This is the result of summary(w1.glm):glm(formula = anw ~ cs, family = \"binomial\", data = w1)Deviance Residuals:     Min       1Q   Median       3Q      Max  -2.5400   0.1073   0.1924   1.0047   1.0047  Coefficients:            Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)  0.42077    0.02588   16.26   \u0026lt;2e-16 ***cs           1.89342    0.06427   29.46   \u0026lt;2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 11762.5  on 10660  degrees of freedomResidual deviance:  9250.3  on 10659  degrees of freedomAnd this is the command I use in Mahout:/usr/local/mahout/bin/mahout trainlogistic --input w1.csv --output ./model --target anw --categories 2 --predictors cs --types numeric --features 20 --passes 100 --rate 50Running on hadoop, using /usr/local/hadoop/bin/hadoop and HADOOP_CONF_DIR=MAHOUT-JOB: /usr/local/mahout/mahout-examples-0.8-job.jar20anw ~ -19.553*cs + -7.512*Intercept Term            cs -19.55265      Intercept Term -7.51155    0.000000000     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000   -19.552646543     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000    -7.511546797     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000 13/11/01 02:04:47 INFO driver.MahoutDriver: Program took 22118 ms (Minutes: 0.3686333333333333)Edited: Added the reason I asked the question in the title. Added the commands used to execute glm in R and trainlogistic in Mahout.","Creater_id":31735,"Start_date":"2013-10-31 23:49:31","Question_id":74295,"Tags":["machine-learning","logistic"],"Answer_count":3,"Last_activity":"2016-07-27 02:18:41","Link":"http://stats.stackexchange.com/questions/74295/differences-between-logistic-regression-in-statistics-and-in-machine-learning","Creator_reputation":11}
{"_id":{"$oid":"5837a58da05283111e4d767b"},"View_count":55,"Display_name":"Tal Galili","Question_score":4,"Question_content":"In Wikipedia, the \"Bias–variance tradeoff\" is mentioned in the context of prediction models where one can control the complexity of the model with some tuning parameters, and the more complex the model the more likely it is that the model would be less biased but with more variance.However, there are other cases for Bias–variance tradeoff. For example, when estimating the error rate of a prediction model we are affected by which proportion of the observations we allocate for training vs testing. The more observations we have for training, the more close we will be to predict the error rate of a model which uses all the observations (less bias), but also the more variable would be our prediction of the error rate (higher variance).Are there other classical/new cases which demonstrate some sort of a bias-variance tradeoff?","Creater_id":253,"Start_date":"2016-07-27 01:17:13","Question_id":225845,"Tags":["variance","predictive-models","bias"],"Answer_count":1,"Last_activity":"2016-07-27 01:50:34","Link":"http://stats.stackexchange.com/questions/225845/where-is-there-bias-variance-trade-off-and-why","Creator_reputation":7701}
{"_id":{"$oid":"5837a58da05283111e4d7688"},"View_count":52,"Display_name":"elkbrs","Question_score":0,"Question_content":"I am currently struggling with designing some Deep Learning algorithms that takes as an input sparse sequential data, since the system I try to model signals sparsely (once in a while). I try to predict the next signal (strength, timing, etc).Can you suggest an architecture\\design\\method\\resources for this type of problem?","Creater_id":124385,"Start_date":"2016-07-27 01:48:14","Question_id":225850,"Tags":["machine-learning","neural-networks","deep-learning","autoencoders","rnn"],"Answer_count":0,"Last_activity":"2016-07-27 01:48:14","Link":"http://stats.stackexchange.com/questions/225850/how-to-design-rnn-vae-for-sparse-sequential-data","Creator_reputation":11}
{"_id":{"$oid":"5837a58da05283111e4d768a"},"View_count":76,"Display_name":"Guido167","Question_score":0,"Question_content":"So I have the following model predicting the presence of an animal on a certain spot. As a time unit quarter is initially used, but for one of the species of animals there is some (little) interesting variation within the months. So instead of quarter (factor) I want to include month (factor) into the model as a time variable. But I get an error when I try to run the zero-inflated poisson (and negative binomial also) with the month instead of quarter.The model with quarter as predictor looks like this:cr_f1c = formula(cr ~ depth + habtype2 + quarter + hurseason + year + lightregime + depth*quarter + depth*hurseason + depth*year + depth*lightregime)cr_zipf1c = zeroinfl(cr_f1c, dist = \"poisson\", link = \"logit\", data = allUVCdata)summary(cr_zipf1c)Call:zeroinfl(formula = cr_f1c, data = allUVCdata, dist = \"poisson\", link = \"logit\")Pearson residuals:    Min      1Q  Median      3Q     Max -1.3485 -0.5650 -0.2814  0.1256 14.3362 Count model coefficients (poisson with log link):                        Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)            -5.936049   2.484272  -2.389 0.016874 *  depth                   0.148349   0.088296   1.680 0.092932 .  habtype2Pinnacles       0.466881   0.154337   3.025 0.002486 ** habtype2Unexposed       0.514980   0.147179   3.499 0.000467 ***quarter2                0.074480   0.206863   0.360 0.718814    quarter3                0.104256   0.263334   0.396 0.692174    quarter4                0.280803   0.223813   1.255 0.209611    hurseasonY              0.021838   0.176503   0.124 0.901533    year2013                1.596452   0.664552   2.402 0.016292 *  year2014                3.790261   0.556287   6.814 9.52e-12 ***year2015                2.205248   0.559779   3.939 8.17e-05 ***lightregimeLight        1.482895   2.405802   0.616 0.537642    depth:quarter2         -0.001569   0.004740  -0.331 0.740714    depth:quarter3         -0.002489   0.005999  -0.415 0.678266    depth:quarter4         -0.006888   0.005142  -1.340 0.180349    depth:hurseasonY        0.002535   0.003940   0.643 0.519987    depth:year2013         -0.049598   0.021527  -2.304 0.021222 *  depth:year2014         -0.115424   0.017704  -6.520 7.04e-11 ***depth:year2015         -0.088689   0.017678  -5.017 5.25e-07 ***depth:lightregimeLight -0.031523   0.086388  -0.365 0.715189    Zero-inflation model coefficients (binomial with logit link):                        Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)              2.21642    4.16511   0.532 0.594629    depth                   -0.01364    0.16504  -0.083 0.934156    habtype2Pinnacles      -16.73199  485.47422  -0.034 0.972506    habtype2Unexposed       -1.29454    0.37891  -3.416 0.000634 ***quarter2                 0.20681    1.57207   0.132 0.895337    quarter3                 1.29092    1.75101   0.737 0.460974    quarter4                 0.27225    1.55530   0.175 0.861041    hurseasonY              -0.92972    0.89675  -1.037 0.299846    year2013                 2.32961    1.84284   1.264 0.206179    year2014                 2.60298    1.66907   1.560 0.118869    year2015                 7.02916    3.29527   2.133 0.032916 *  lightregimeLight        -0.74676    3.46865  -0.215 0.829543    depth:quarter2          -0.02925    0.06352  -0.460 0.645160    depth:quarter3          -0.07610    0.07064  -1.077 0.281286    depth:quarter4          -0.04340    0.06337  -0.685 0.493377    depth:hurseasonY         0.01239    0.03503   0.354 0.723684    depth:year2013          -0.09100    0.07004  -1.299 0.193827    depth:year2014          -0.14156    0.06302  -2.246 0.024685 *  depth:year2015          -0.60836    0.22895  -2.657 0.007881 ** depth:lightregimeLight   0.07768    0.14075   0.552 0.580994    ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Number of iterations in BFGS optimization: 132 Log-likelihood: -4007 on 40 DfBut when I run it with month as predictor, I get the following error:cr_f1c = formula(cr ~ depth + habtype2 + month + hurseason + year + lightregime + depth*month + depth*hurseason + depth*year + depth*lightregime)cr_zipf1c = zeroinfl(cr_f1c, dist = \"poisson\", link = \"logit\", data = allUVCdata)Error in optim(fn = loglikfun, gr = gradfun, par = c(startzero,  :   non-finite value supplied by opticWhat does this tell me and how can I possibly include month in my model?","Creater_id":101294,"Start_date":"2016-07-27 01:47:51","Question_id":225849,"Tags":["r","regression","hypothesis-testing","multiple-regression","zero-inflation"],"Answer_count":0,"Last_activity":"2016-07-27 01:47:51","Link":"http://stats.stackexchange.com/questions/225849/zero-inflated-model-non-finite-value-supplied-by-optim","Creator_reputation":30}
{"_id":{"$oid":"5837a58da05283111e4d768c"},"View_count":137,"Display_name":"Tristan","Question_score":1,"Question_content":"I want to perform a nested ANCOVA using the function lme in r. My data concerns the growth rate (sgr) of fish at different temperatures (temp). In the trial there were 3 replicate tanks each housing 8 fish at each level of temperature (15,18,21 and 24 degrees Celsius) so that a total of 12 tanks were included in the trial. I want to control for the effects that individual tanks may have had on growth and since the initial mass of the fish also effects growth rate I want to include it as a covariate.These are the variables I plan to include in the model:sgr= continuous dependent variable temp= fixed main effect mass= covariatetank= random factor (tank coded 1-12) Using lme I have coded my model as such:m1=lme(sgr~temp+mass,random=~1|tank)I want to know two things:If the code I have used to include tank as a random factor has been done correctly?Can a covariate (mass) be added this way using lme (just like a regular ANCOVA) when the model also includes a random effect? Thank you in advance for any assistance provided?","Creater_id":124103,"Start_date":"2016-07-21 19:50:48","Question_id":225036,"Tags":["r","ancova","lme","nested"],"Answer_count":1,"Last_activity":"2016-07-27 01:42:56","Link":"http://stats.stackexchange.com/questions/225036/using-lme-to-perform-a-nested-ancova-in-r","Creator_reputation":8}
{"_id":{"$oid":"5837a58da05283111e4d7699"},"View_count":3605,"Display_name":"Bin","Question_score":6,"Question_content":"In machine learning, people talk about objective function, cost function, loss function. Are they just different names of the same thing? When to use them? If they are not always refer to the same thing, what are the differences?","Creater_id":53799,"Start_date":"2015-10-25 15:03:48","Question_id":179026,"Tags":["machine-learning","artificial-intelligence"],"Answer_count":2,"Last_activity":"2016-07-27 01:16:05","Link":"http://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing","Creator_reputation":136}
{"_id":{"$oid":"5837a58da05283111e4d76a7"},"View_count":91,"Display_name":"Alex","Question_score":1,"Question_content":"I am given  observations of pairs of covariates and response . When the response are non-negative integers, by doing Poisson regression I am modelling  as Poisson random variables with mean , such that  is a linear function of the covariate . A maximum likelihood estimator for the coefficients of  maximises the Poisson log-likelihood:\\sum_{i=1}^N (y_i \\ln(\\mu_i) - \\mu_i)I have seen references to doing Poisson regression with non-negative, non-integers, e.g. How does a Poisson distribution work when modeling continuous data and does it result in information loss?In this case, what log-likelihood function is used? Do you still use the above function but allow  to take the non-integer values?","Creater_id":22199,"Start_date":"2016-07-26 20:52:54","Question_id":225825,"Tags":["regression","poisson"],"Answer_count":1,"Last_activity":"2016-07-27 01:04:50","Link":"http://stats.stackexchange.com/questions/225825/what-log-likelihood-function-do-you-use-when-doing-a-poisson-regression-with-con","Creator_reputation":727}
{"_id":{"$oid":"5837a58da05283111e4d76b4"},"View_count":31,"Display_name":"roel","Question_score":0,"Question_content":"I am trying to build a regression tree that outputs both a mean and a covariance matrix for each leaf of the tree.Ideally I would be able to have a Gaussian Mixture Model at each leaf. A first literature search was not productive. Has anybody seen this modeling structure before?","Creater_id":122083,"Start_date":"2016-07-03 16:23:07","Question_id":221989,"Tags":["regression","normal-distribution","model","cart","mixture"],"Answer_count":1,"Last_activity":"2016-07-27 00:44:45","Link":"http://stats.stackexchange.com/questions/221989/building-a-regression-tree-with-one-gaussian-mixture-model-at-each-node","Creator_reputation":1}
{"_id":{"$oid":"5837a58da05283111e4d76c1"},"View_count":148,"Display_name":"dontloo","Question_score":2,"Question_content":"If the data is 1d, the variance shows the extent to which the data points are different from each other. If the data is multi-dimensional, we'll get a covariance matrix.Is there a measure that gives a single number of how the data points are different from each other in general for multi-dimensional data?I feel that there might be many solutions already, but I'm not sure the correct term to use to search for them. Maybe I can do something like adding up the eigenvalues of the covariance matrix, does that sound sensible?","Creater_id":95569,"Start_date":"2016-07-24 19:37:58","Question_id":225434,"Tags":["variance","covariance","covariance-matrix"],"Answer_count":2,"Last_activity":"2016-07-27 00:41:58","Link":"http://stats.stackexchange.com/questions/225434/a-measure-of-variance-from-the-covariance-matrix","Creator_reputation":2865}
{"_id":{"$oid":"5837a58da05283111e4d76cf"},"View_count":43,"Display_name":"FKG","Question_score":1,"Question_content":"Let's say that I have a full and a restricted model that looks like this:Full\u0026lt;- polr(Y ~ X1+X2+X3+X4, data=data, Hess = TRUE,  method=\"logistic\")Restricted\u0026lt;- polr(Y ~ X1+X2+X3, data=data, Hess = TRUE,  method=\"logistic\")I want to conduct F-tests to determine whether the information from the X4variable statistically improves our understanding of Y. What command is convenient for carrying out this test for logistic regression? Is it aov()? For example: summary(aov(Y ~ X1+X2+X3+X4)) #Full modelsummary(aov(Y ~ X1+X2+X3)) #Restricted modelIn linear regression case this would be the way to do it, I am not sure for ordered logistic regression...","Creater_id":53160,"Start_date":"2016-07-26 08:02:24","Question_id":225713,"Tags":["r","regression","logistic","model-selection","nested"],"Answer_count":1,"Last_activity":"2016-07-26 23:35:30","Link":"http://stats.stackexchange.com/questions/225713/how-to-compare-ordered-logistic-nested-models","Creator_reputation":50}
{"_id":{"$oid":"5837a58da05283111e4d76dc"},"View_count":25,"Display_name":"ts_highbury","Question_score":2,"Question_content":"I have data collected on a meeting-by-meeting basis, where the change in time between two meetings is not constant, i.e., . Are ordinary Augmented Dicky Fuller and Phillips-Perron tests valid in this context?","Creater_id":101137,"Start_date":"2016-07-26 06:58:36","Question_id":225696,"Tags":["regression","time-series","stationarity"],"Answer_count":0,"Last_activity":"2016-07-26 23:07:47","Link":"http://stats.stackexchange.com/questions/225696/stationarity-of-a-variable-measured-at-irregular-intervals","Creator_reputation":45}
{"_id":{"$oid":"5837a58da05283111e4d76de"},"View_count":56,"Display_name":"Abe","Question_score":0,"Question_content":"In the deep learning book by Bengio, Goofellow and Courville (http://www.deeplearningbook.org/) there is paragraph in the regularization chapter.\"Deep learning algorithms are typically applied to extremely complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe.\"Why is this the case?","Creater_id":124645,"Start_date":"2016-07-26 18:44:04","Question_id":225819,"Tags":["machine-learning","neural-networks","deep-learning"],"Answer_count":1,"Last_activity":"2016-07-26 22:49:37","Link":"http://stats.stackexchange.com/questions/225819/why-is-the-true-data-generating-process-similar-to-simulating-the-entire-unive","Creator_reputation":8}
{"_id":{"$oid":"5837a58da05283111e4d76eb"},"View_count":62,"Display_name":"Charlie Parker","Question_score":1,"Question_content":"I was training a singled layered (shallow) neural network as in: f(x) = \\sum^K_{k} c_k\\theta(W_k x+b_k)for regression (using squared error loss) or function approximation.I was wondering, is there a preference on what type of activation one should use? For example, sigmoid, ReLu, tanh, etc? I am aware that ReLu are usually the preferred method when using multiple layers to combat the vanishing gradient problem, however, it doesn't seem the argument there applies to shallow neural networks. Is there a particular advantage of using any activation function compared to another one in this special case?The only things I thought (intuitively) is that sigmoids or tanh seem more non-linear that ReLu's and since its singled layered, I'd probably prefer a more \"non-linear model\" though these are just guesses. If there is better research for this I'd love to know!","Creater_id":28986,"Start_date":"2016-07-26 21:07:14","Question_id":225826,"Tags":["regression","machine-learning","neural-networks","deep-learning"],"Answer_count":0,"Last_activity":"2016-07-26 21:07:14","Link":"http://stats.stackexchange.com/questions/225826/when-doing-regression-with-a-singled-layered-neural-network-what-activation-fun","Creator_reputation":494}
{"_id":{"$oid":"5837a58da05283111e4d76ed"},"View_count":25,"Display_name":"sponge_knight","Question_score":0,"Question_content":"Why favor Sensitivity-Specificity over Precision-recall for model selection?","Creater_id":46925,"Start_date":"2016-07-26 20:50:32","Question_id":225824,"Tags":["model"],"Answer_count":0,"Last_activity":"2016-07-26 20:50:32","Link":"http://stats.stackexchange.com/questions/225824/do-precision-recall-vs-sensitivity-specificity-represent-two-cultures","Creator_reputation":1522}
{"_id":{"$oid":"5837a58da05283111e4d76ef"},"View_count":28,"Display_name":"R Sharada","Question_score":0,"Question_content":"I have a very small sample size of 4 or 5 within a treatment. Can I use the Mann-Whitney to draw any inference of differences between them? Online calculators seem to indicate a requirement of sample size \u003e= 5Sharada","Creater_id":124235,"Start_date":"2016-07-26 07:09:36","Question_id":225698,"Tags":["anova","nonparametric"],"Answer_count":1,"Last_activity":"2016-07-26 19:59:21","Link":"http://stats.stackexchange.com/questions/225698/can-mann-whitney-u-test-be-used-for-n5","Creator_reputation":1}
{"_id":{"$oid":"5837a58da05283111e4d76fc"},"View_count":881,"Display_name":"LRB","Question_score":1,"Question_content":"I need to know how can I combine in stata the DID (difference-in-difference) model with the PSM (Propensity score matching) with cross-sectional data. I had searched and there are tones of answers related to each of those models, but together I haven't been able to find them. Could someone please help me!! ??","Creater_id":29923,"Start_date":"2013-09-04 13:05:39","Question_id":69233,"Tags":["stata","matching","cross-section","difference-in-difference"],"Answer_count":0,"Last_activity":"2016-07-26 18:24:00","Link":"http://stats.stackexchange.com/questions/69233/didm-matching-with-difference-in-difference-using-cross-sectional-data","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d76fe"},"View_count":14,"Display_name":"Landa","Question_score":1,"Question_content":"I am using 2x3 repeated measures ANOVA. I am analyzing three years of test data and comparing two different groups. However, my data is negatively skewed. What is the best way to transform negatively skewed data? What shall I use for repeated measure analysis of non normal data?Thanks for your help.","Creater_id":124639,"Start_date":"2016-07-26 18:06:53","Question_id":225812,"Tags":["repeated-measures"],"Answer_count":1,"Last_activity":"2016-07-26 18:14:00","Link":"http://stats.stackexchange.com/questions/225812/2x3-repeated-measures-for-skewed-data","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d770b"},"View_count":76934,"Display_name":"ustroetz","Question_score":15,"Question_content":"When running a multiple regression model in R, one of the outputs is a residual standard error of 0.0589 on 95,161 degrees of freedom. I know that the 95,161 degrees of freedom is given by the difference between the number of observations in my sample and the number of variables in my model. What is the residual standard error?","Creater_id":22008,"Start_date":"2013-04-30 13:54:11","Question_id":57746,"Tags":["regression","standard-error","residuals"],"Answer_count":3,"Last_activity":"2016-07-26 17:50:59","Link":"http://stats.stackexchange.com/questions/57746/what-is-residual-standard-error","Creator_reputation":246}
{"_id":{"$oid":"5837a58da05283111e4d771a"},"View_count":34,"Display_name":"Aeternia","Question_score":0,"Question_content":"I want to build a chart to compare accessibility of N given front pages of different web sites. For getting the initial data I use AChecker tool which returns me a number of accessibility issues were found on each front page included into my research array.Since the page design of the sites is the same from day to day, the correlation between the amount of issues seems to stay within acceptable error.But the data itself fluctuates a bit, because each page updates daily. Which means it would be wrong to represent the correlation in raw data for a chart to say “site01 has 42 issues, and site02 has 112 issues”, because the next day it can be “48” and “110” respectively.I thought to collect data about the amount of “reported problems” for each page at same day, then find an average, and build a chart around this average. Would it be a valid way to represent a correlation? If no, what should I do to find and show a valid correlation in such case?If the question doesn't belong here, I apologize, and will be grateful if someone will guide me to appropriate sub.","Creater_id":124274,"Start_date":"2016-07-23 13:37:31","Question_id":225294,"Tags":["hypothesis-testing","data-visualization"],"Answer_count":1,"Last_activity":"2016-07-26 17:48:13","Link":"http://stats.stackexchange.com/questions/225294/how-to-compare-data-that-fluctuates","Creator_reputation":103}
{"_id":{"$oid":"5837a58da05283111e4d7727"},"View_count":42,"Display_name":"sponge_knight","Question_score":0,"Question_content":"I'm wondering: Is there a statistical test for determining \"no correlation\"? I would not want to use Fisher's transformation.","Creater_id":46925,"Start_date":"2016-07-26 11:42:37","Question_id":225760,"Tags":["correlation"],"Answer_count":2,"Last_activity":"2016-07-26 17:47:23","Link":"http://stats.stackexchange.com/questions/225760/test-for-no-correlation","Creator_reputation":1522}
{"_id":{"$oid":"5837a58da05283111e4d7735"},"View_count":36,"Display_name":"Marko","Question_score":2,"Question_content":"The reciprocal link function in exponential regression doesn't constrain to positive values, whilst on the other hand it does thoroughly space out points close to zero when fitting the linear predictor. (a) Regarding the lack of constraint of results to non-negative predictions, my thinking is that, as long as we choose an initial trial parameter vector that results in a non-negative linear predictor, the chances of ending up with negative predictions is negligible since the sheer number of points in the neighbourhood of zero will make the predictions very accurate in that region.(b) I am not clear as to how transforming values close to zero to be spread out over the range of the reciprocal function makes the regression more successful. I can see that it reduces leverage compared with an ordinary regression to fit the means, but is there something deeper connected with it being easier to fit a linear function to points that are reasonably far apart?I was wondering how that reasoning sat with more seasoned modellers of exponential regression, and keen to get some more understanding around what makes a good link function over and above that it constrains predictions to appropriate values.Thanks,Marko","Creater_id":113478,"Start_date":"2016-07-26 11:00:50","Question_id":225755,"Tags":["regression","exponential-family","link-function"],"Answer_count":1,"Last_activity":"2016-07-26 17:38:43","Link":"http://stats.stackexchange.com/questions/225755/reasoning-behind-reciprocal-link-function-for-exponential-regression","Creator_reputation":49}
{"_id":{"$oid":"5837a58da05283111e4d7742"},"View_count":1125,"Display_name":"Miroslav Sabo","Question_score":26,"Question_content":"Maybe this question is naive, but:If linear regression is closely related to Pearson's correlation coefficient, are there any regression techniques closely related to Kendall's and Spearman's correlation coefficients?","Creater_id":14730,"Start_date":"2013-07-20 02:25:11","Question_id":64938,"Tags":["regression","correlation","pearson","spearman","kendall-tau"],"Answer_count":3,"Last_activity":"2016-07-26 16:20:27","Link":"http://stats.stackexchange.com/questions/64938/if-linear-regression-is-related-to-pearsons-correlation-are-there-any-regressi","Creator_reputation":1752}
{"_id":{"$oid":"5837a58da05283111e4d7750"},"View_count":32,"Display_name":"humematu","Question_score":0,"Question_content":"I'm working with a dataset of electrical submeter readings taken once per minute for 47 months. I'm trying to create a time series that covers meter readings from the kitchen during the week of January 21-27, 2007. My code at the moment is as follows:myWeek \u0026lt;- filter(hpc_by_datetime, \"2007-01-21 00:00:00\" \u0026lt;= DateTime \u0026amp;    DateTime \u0026lt; \"2007-01-28 00:00:00\")#Create Time Serieskitchenweek \u0026lt;- ts(myWeek$submeter_kitchen, frequency = 525600, start = c(2007, 28800))What would be a reasonable frequency to use in this situation? And is the vector I assign to start affected by this?","Creater_id":111212,"Start_date":"2016-07-26 14:53:33","Question_id":225793,"Tags":["r","time-series"],"Answer_count":1,"Last_activity":"2016-07-26 16:05:51","Link":"http://stats.stackexchange.com/questions/225793/im-using-r-to-create-a-time-series-having-some-difficulty-understanding-frequ","Creator_reputation":1}
{"_id":{"$oid":"5837a58da05283111e4d775d"},"View_count":156,"Display_name":"Filip Haglund","Question_score":4,"Question_content":"I am doing some simulations and I need a good heuristic for when to stop the simulation. The simulation continuously outputs data with a cost in the range [0,1]. The output looks gamma distributed or possibly Poisson.This is a standard Google Sheets histogram; all default values.And here's a scatter plot, with the first attempt at the left side. The orange dots are blue dots which are also the lowest cost seen so far. Those are the ones I need to predict.What I would like to know is a rough probability of the next simulation yielding a new lowest cost, so that I can do a somewhat informed decision of when to stop. This dataset consists of 11,475 values. If you would like the actual data or charts from a bigger dataset or from another run, let me know. This is part of a program I'm writing as a freetime project; it is not part of an assignment.EDIT: I would guess that all values are independent, since they are executed in parallel. They take the same input, but use different random values.","Creater_id":54599,"Start_date":"2016-07-25 06:14:43","Question_id":225511,"Tags":["probability","estimation"],"Answer_count":1,"Last_activity":"2016-07-26 15:31:02","Link":"http://stats.stackexchange.com/questions/225511/probability-of-finding-a-new-smallest-value","Creator_reputation":133}
{"_id":{"$oid":"5837a58da05283111e4d776a"},"View_count":458,"Display_name":"Constantin","Question_score":15,"Question_content":"We are typically introduced to method of moments estimators by \"equating population moments to their sample counterpart\" until we have estimated all of the population's parameters; so that, in the case of a normal distribution, we would only need the first and second moments because they fully describe this distribution. And we could theoretically compute up to  additional moments as:How can I build intuition for what moments really are? I know they exist as a concept in physics and in mathematics, but I find neither directly applicable, especially because I don't know how to make the abstraction from the mass concept to a data point. The term seems to be used in a specific way in statistics, which differs from usage in other disciplines.What characteristic of my data determines how many () moments there are overall?","Creater_id":60577,"Start_date":"2015-01-10 06:54:21","Question_id":132914,"Tags":["moments","method-of-moments"],"Answer_count":4,"Last_activity":"2016-07-26 15:14:03","Link":"http://stats.stackexchange.com/questions/132914/what-exactly-are-moments-how-are-they-derived","Creator_reputation":259}
{"_id":{"$oid":"5837a58da05283111e4d777a"},"View_count":19,"Display_name":"user124531","Question_score":1,"Question_content":"I have an experiment which evaluates decision making processes while buying a given product (wine).I use MouseLab Web to gather the data.Basically the user has multiple attributes, that describe his product:Price, Brand, Type, Country of originThe user can open up the different Attributes (displayed as cards) and have a look at the values. When he opens an attribute, all others are closed.The user may reopen any attribute as often as he wants.That gives me an example output for one user like the following:timestamp | attributelike:00001 | Type00111 | Price01111 | Brand02222 | Price03333 | Country04444 | BrandI am trying to evalute the data with a weighted ranking scale:Type: 6Price: 5Brand: 4Price: 3Country: 2Brand: 1But this way I got the problem that a user that checks a lot of cards will have a huge score for each category comparing to a user that made his decision after only a few cards, so I can't compare the two users.How would you normalize the ranking data to make it comparable?","Creater_id":124531,"Start_date":"2016-07-26 01:43:05","Question_id":225654,"Tags":["weighted-mean"],"Answer_count":0,"Last_activity":"2016-07-26 15:13:07","Link":"http://stats.stackexchange.com/questions/225654/analyzing-a-weighted-ranking-with-repeating-items","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d777c"},"View_count":39,"Display_name":"Footy","Question_score":0,"Question_content":"What is the probability that 13 randomly drawn cards will come ina strictly increasing sequence of ranks in a common 52 card deck (239KA is strictly increasing,but A239K is not, and JJJKK is not)? Ignore suits.How do I go about doing this?","Creater_id":122959,"Start_date":"2016-07-13 11:33:31","Question_id":223614,"Tags":["probability","self-study"],"Answer_count":1,"Last_activity":"2016-07-26 15:06:58","Link":"http://stats.stackexchange.com/questions/223614/probability-of-drawn-cards-coming-in-an-increasing-sequence","Creator_reputation":9}
{"_id":{"$oid":"5837a58da05283111e4d7788"},"View_count":65,"Display_name":"MikeB0317","Question_score":1,"Question_content":"Introduction Editing this post to clarify what I am asking. This is a bit of a vague question to begin with. I am not asking if this particular statistics method with these variables will cause this to happen. Hopefully this post can be used by anybody who is in the beginning stages of learning advanced time series data mining.I have an SQL server filled with exceptions (it does not matter where the data comes from I just thought I would give that bit of information) I would like to use data mining as well as machine learning to create decisions about exceptions that are similar enough but also can correlation to a particular defect. There are tons of exceptions in these logs, a lot of junk that is not going to be what I am seeking. Some exceptions are handled properly, and are not a very big concern. I do not mind suggestions for more advanced methodologies. The math is not what is concerning to me. What worries me is that I will follow a less than ideal path, and the trail will never lead to anything meaningful, hopefully I can learn from somebody with more experience than myself. The purpose of this post is to find the meaningful path towards my goal. BackgroundDefects occur often that directly affect the business, so they must be resolved quickly. We often find ourselves siting through error logs by continually changing the queries.Columns in SQL ServerCreate DateMachine (server)Application Service Class NameMethod NameError TextSomehow I am going to need to query these particular fields to place the data in a desired state. Obviously I do not want to take exceptions that are completely unrelated, and correlate them to another defect just because they happened around the same time, and fit the trend I am seeking. Example A null order ID is causing a huge damage to sales revenue. We know that the null ID causes this issue where an order can not book, and an order is not a cheap cost to lose. We have no idea where to look in software to find this particular defect (This happens quite often). We can make assumptions but unless we have concrete evidence, nothing can be done. Aggregate queries are used to create a time series representation of the production data in this case null order ID using date time, and count(*).ProblemI now would like to find a correlation between this particular time series data set of exceptions that are alike, and a defect which has data that can be gathered from the production information. Thus I have two time series data sets, with a fair deal of flexibility on how exactly they are constructed. Typically what I would do is start executing queries to try to zone in on the problem (this is where I would like to place some sort of AI decision making) After about 6 hours of executing queries with different clauses, I found a slight relationship between an exception in that application service, and the null ID.Below is a time series representation of both the null ID we knew was causing a problem, and an exception in our database.Running a statistical correlation between the two data sets I found a 0.922419934match. The team was searching for an entire week before we finally this match. Without this proof we do not have enough evidence to warrant an emergency code push to production, and thus the never ending cycle of stress anger and frustration. I realized that we could do better than simply blindly searching databases, and graphing by hour in excel for different exceptions until I find a trend. I have some experience with mat lab, so that is the first place I went to for the more advanced mathematics processing / graphing. I realized I was actually manually performing AI tasks but much less efficient that a properly designed system. I really want to start programming new things, and stop digging through database exceptions so that is why I am undertaking this.Advice?Rather than go off on some tangent idea, only to find out that I am not correlating anything but wasting vast amounts of time energy and effort I decided to do some research. I have painfully read through some dissertations which relate these topics. I realized that some of the process that they are following are very similar to what I am actually am manually doing much less precise and formally.I know that time series data interpretation is nothing new, it is a very commonly analyzed metric, used in a vast number of applications, sciences, and disciplines. I am seeking any information to help towards that goal. I am trying to determine what is in existence so that I am not recreating the wheel. I am relatively new to these topics, but I do not taking on a more difficult task than I can handle because I believe that is the best way to learn something new.My questions to the community can now be asked more definitely. 1). Is there a process by which I can follow to help me towards performing this correlation and validation checks between the time series data sets? This process should technically work on any set of time series data sets regardless of the particular domain knowledge. I imagine this process to involve some sort of pre-processing to get the data in a desire form. Next I imagine some form of digital filters being applied to the discrete signal to extract a smooth looking curve, and finally a continuous time signal constructed from the filtered data?2). What are some type of tools that could be leveraged for this kind of task. I have gathered through reading the forums that Python is my best bet for the data. I would think that I need some sort of decision trees, and a way to classify different data sets together (this is where the domain knowledge comes into play)3). Does the sorting window of the the time series affect the outcome of the process to be discovered in my first question? Does adding more data points increase my likelihood of finding correlations? I know that the more data points that are gathered the more accurate the signal will look, but I do not know how that is going to affect the data mining of the time series. Does sorting by minutes or seconds vs hours or days give more insight?4). Is there any sort of starting point that I could reference. Currently I have found dissertation research that shows certain methods and procedures from an abstract level. I am seeking some time series open source AI project that has some similarities which could give a model for how the data logic is performed once the domain checks have been implemented.5). Is there any proprietary software in existence that can aid in my endeavor? Currently I am using mat lab, because it is what I am familiar with. I am forming the data manipulations in Python, and sending that to a text file which a matlab function reads in, graphs.6). Given the information what would be an ideal model for this type of task? What I am actually trying to do is not that difficult considering typically a defect seen by the user will correlate to a particular set of exceptions through similar methods in the stack trace.6). Is there anything else I missed? Please poke holes anywhere you see them, add additional information, insight?I hope that I am not the only has the questions, and that this post may also help somebody else seeking the same information as myself.","Creater_id":124508,"Start_date":"2016-07-25 19:44:56","Question_id":225631,"Tags":["machine-learning","classification","bayesian","data-mining","autocorrelation"],"Answer_count":0,"Last_activity":"2016-07-26 15:01:37","Link":"http://stats.stackexchange.com/questions/225631/time-series-data-mining-and-correlation-guidance","Creator_reputation":14}
{"_id":{"$oid":"5837a58da05283111e4d778a"},"View_count":65,"Display_name":"lior_","Question_score":0,"Question_content":"I'm conducting a research which compares the ratio between two categories of words in 2 different text corpuses.Let's say I have corpuses A and B.I created 2 categories of words, Let's call them C and D.I want to calculate the ratio between the count of words in category C and the count of words in category D, and see if the ratio in corpus A is significantly larger than the ratio in corpus B.What statistical test can I use to check this hypothesis?Your help would be appreciated.","Creater_id":87509,"Start_date":"2016-07-25 21:25:28","Question_id":225638,"Tags":["hypothesis-testing","statistical-significance"],"Answer_count":0,"Last_activity":"2016-07-26 14:39:22","Link":"http://stats.stackexchange.com/questions/225638/help-identifying-the-right-statistical-hypothesis-test","Creator_reputation":22}
{"_id":{"$oid":"5837a58da05283111e4d778c"},"View_count":17,"Display_name":"user3639557","Question_score":0,"Question_content":"I am reading Escobar\u0026amp;West paper and in particular am interested in their Gibbs sampler for the concentration parameter of Dirichlet Process. The issue I have is at the end of their section 6, where all the interesting stuff is done I guess :). So the authors mention that from the following,p(\\alpha|k)\\propto p(\\alpha)\\alpha^{k-1}(\\alpha+n)\\int^1_0x^{\\alpha}(1-x)^{n-1}dxit can be implied that  is a marginal distribution from a joint for  and a continuous quantity  such that p(\\alpha,\\eta|k)\\propto p(\\alpha)\\alpha^{k-1}(\\alpha+n)\\eta^\\alpha(1-\\eta)^{n-1} I don't understand how this is implied from the first equation.","Creater_id":56676,"Start_date":"2016-07-26 10:17:06","Question_id":225745,"Tags":["bayesian","sampling","hierarchical-bayesian","dirichlet-distribution","dirichlet-process"],"Answer_count":1,"Last_activity":"2016-07-26 14:35:42","Link":"http://stats.stackexchange.com/questions/225745/escobar-and-west-sampler-for-dirichlet-process-parameters","Creator_reputation":310}
{"_id":{"$oid":"5837a58da05283111e4d7799"},"View_count":49,"Display_name":"EngrStudent","Question_score":2,"Question_content":"I want to use a Savitzky-Golay filter to smooth some data.  There is a right width to use based on the data that it is smoothing.  A number of papers basically use \"eyeball norm\" on the parameters but that feels like voodoo.How do I get a measure like AIC to tell me which implementation is the proper one to use?When I think about AIC, I do so in terms of RSS, number parameters, and number of degrees of freedom.  I can measure RSS for the smoothed data, but I don't know how to think about parameter count or number of degrees of freedom in this context.  Hints or pointers would be appreciated.  Clear explanation would be delightful.I'm personally using either R, or LabVIEW, but the actual software package isn't important.  Work so far:    This post says \"k is 2 parameters, n is samples, use RSS\"without providing references or derivation.This reference is about the 'PoMoS' library which claimes to use genetic algorithms and information criteria to find optimal polynomial structure of time-series.References:    (R lib) http://www.inside-r.org/packages/cran/pracma/docs/savgol(NI lib) http://zone.ni.com/reference/en-XX/help/371361H-01/lvanls/sgfil/","Creater_id":22452,"Start_date":"2016-04-06 18:37:31","Question_id":205955,"Tags":["aic","smoothing","filter"],"Answer_count":1,"Last_activity":"2016-07-26 14:23:51","Link":"http://stats.stackexchange.com/questions/205955/aic-on-savitzky-golay-width","Creator_reputation":4186}
{"_id":{"$oid":"5837a58da05283111e4d77a6"},"View_count":27,"Display_name":"sambajetson","Question_score":0,"Question_content":"QDA (quadratic discriminant analysis) assumes that the K different classes are generated by K different multivariate Gaussians, each with potentially different mean vector and covariance matrix.If these assumptions hold, will QDA give the same classification results as using EM (expectation maximization) clustering with Gaussian likelihoods?","Creater_id":83546,"Start_date":"2016-07-26 11:12:24","Question_id":225756,"Tags":["normal-distribution","clustering","expectation-maximization","gaussian-mixture","discriminant-analysis"],"Answer_count":0,"Last_activity":"2016-07-26 13:53:48","Link":"http://stats.stackexchange.com/questions/225756/qda-vs-em-with-gaussian-likelihoods","Creator_reputation":136}
{"_id":{"$oid":"5837a58da05283111e4d77a8"},"View_count":124,"Display_name":"H\u0026#233;ctor","Question_score":2,"Question_content":"I try to estimate a location and dispersion model with R, as described Maronna et al (2006, pp. 56). However, my estimate dispersion does not converge to the desired value. Do I have an error in the code?The starting values are the median and the MADN, for the estimates of the location and dispersion, respectively.\\begin{equation}MADN=\\hat{\\sigma}_{0}=k\\cdot Median(|r_{k,i}-Median(r_{k,i})|), \\qquad k\\approx 1.4826\\end{equation}The model is \\begin{equation}x_{i}=\\mu+\\sigma \\varepsilon_{i}, \\qquad \\varepsilon_{i}\\sim \\mathcal{N}(0,1)\\end{equation}The -function is the bisquare (biweight) for the location parameter and  for the dispersion parameter. The -function for the location parameter is \\begin{equation}\\rho(r_{k,i})=\\begin{cases} 1-\\left[1-\\left(\\frac{r_{k,i}}{c}\\right)^{2}\\right]^{3} \u0026amp; \\text{if}\\quad |r_{k,i}|\\leq c \\\\ 1 \u0026amp; \\text{it}\\quad |r_{k,i}|\u0026gt;c \\end{cases}\\end{equation}with  where\\begin{equation}\\psi(r_{k,i})=r_{k,i}\\left[1-\\left(\\frac{r_{k,i}}{c}\\right)^{2}\\right]^{2}I(|r_{k,i}|\\leq c)\\end{equation}with c set out to achive some efficiency value, compared to the Maximum likelihood estimator for the normal distribution.The weight function for the location parameter is\\begin{equation}W_{1}(r_{k,i})=\\begin{cases} \\psi(r_{k,i})/r_{k,i} \u0026amp; \\text{if}\\quad r_{k,i}\\neq 0 \\\\ \\psi'(0) \u0026amp; \\text{if}\\quad r_{k,i}=0\\end{cases}  \\end{equation}For the dispersion (scale) parameter, the rho function is the same for the location parameter, but with . However, the weight function is \\begin{equation}W_{2}(r_{k,i})=\\begin{cases} \\rho(r_{k,i})/r_{k,i}^{2} \u0026amp; \\text{if}\\quad r_{k,i}\\neq 0 \\\\ \\rho''(0) \u0026amp; \\text{if}\\quad r_{k,i}=0 \\end{cases}\\end{equation}In my opinion, the results must converge, to the true parameters  and . Indeed, it is pretty close to mu, but not sigma.p.d: I tried to consult on StackOverflow, it is about programming. However, they pointed me well, that to answer this question it is necessary to know the underlying theory.    #set.seed(12345)    n \u0026lt;- 100    error\u0026lt;-rnorm(n,0,1)    x \u0026lt;- 10+5*error    # psi bisquare function    psi_bisq \u0026lt;- function(y){      k \u0026lt;- 2.2  # c in the text above       ifelse(abs(y)\u0026lt;=k,ind \u0026lt;- 1,ind \u0026lt;- 0)      return(y*ind*(1-(y/k)^2)^2)    }    # derivative of the psi bisquare function    psi_bisq_der \u0026lt;- function(y){      k \u0026lt;- 2.2      ifelse(abs(y)\u0026lt;=k,ind \u0026lt;- 1,ind \u0026lt;- 0)      return(ind*(1-(y/k)^2)*(1-5*(y/k)^2))    }    #  rho_bisquare function for the dispersion (scale) parameter    rho_bisq \u0026lt;- function(y){      k \u0026lt;- 1      ifelse(abs(y)\u0026lt;=k, sal \u0026lt;- 1-(1-(y/k)^2)^3, sal \u0026lt;- 1)      return(sal)    }    # second derivative of the  rho_bisquare function     rho_bisq_der2 \u0026lt;- function(y){      k \u0026lt;- 1      ifelse(abs(y)\u0026lt;=k, sal \u0026lt;- 6*(1-(y/k)^2)*(1-5*(y/k)^2)/k^2, sal \u0026lt;- 0)      return(sal)    }    # W1 weight function    W \u0026lt;- function(dato){      ifelse(dato==0, ww\u0026lt;-psi_bisq_der(0), ww\u0026lt;-psi_bisq(dato)/dato)      return(ww)    }    # W2 weight function    Wdis\u0026lt;-function(dato){      ifelse(dato==0,ww\u0026lt;-rho_bisq_der2(0),ww\u0026lt;-rho_bisq(dato)/dato^2)    }    #======== Simultaneous estimator of location and dispersion     # Initial value for the dispersion parameter    sigma_est_a \u0026lt;- median(abs(x-median(x)))/0.6745    # Initial value for the location parameter    mu_est_a \u0026lt;- median(x)    epsilon \u0026lt;- .00001    err1 \u0026lt;- epsilon*sigma_est_a    err2\u0026lt;-epsilon    error \u0026lt;- 1000    error2\u0026lt;- 1000    delta \u0026lt;- .9 # this value can vary? the text use 1 for the scale parameter    iter\u0026lt;-0    while((error\u0026gt;=err1) \u0026amp;\u0026amp; (error2\u0026gt;=err2)){      pesos_loc \u0026lt;- rep(0,n)      pesos_dis \u0026lt;- rep(0,n)      for(i in 1:n){        r[i] \u0026lt;- (x[i]-mu_est_a)/sigma_est_a        pesos_loc[i] \u0026lt;- W(r[i])        pesos_dis[i]\u0026lt;-Wdis(r[i])      }      iter\u0026lt;-iter+1      mu_est_n \u0026lt;-sum(pesos_loc*x)/sum(pesos_loc)      sigma_est_n\u0026lt;-sqrt(((sigma_est_a^2)/(n*delta))*sum(pesos_dis*r^2))      error \u0026lt;- abs(mu_est_n - mu_est_a)      error2 \u0026lt;- abs(sigma_est_n/sigma_est_a -1)      mu_est_a\u0026lt;-mu_est_n      sigma_est_a\u0026lt;-sigma_est_n    }    sigma_est_a    mu_est_a","Creater_id":77970,"Start_date":"2016-07-23 23:12:09","Question_id":225328,"Tags":["r","outliers","robust"],"Answer_count":0,"Last_activity":"2016-07-26 13:50:10","Link":"http://stats.stackexchange.com/questions/225328/robust-m-estimator-of-location-and-dispersion-by-hand-in-r","Creator_reputation":29}
{"_id":{"$oid":"5837a58da05283111e4d77aa"},"View_count":29,"Display_name":"Adam","Question_score":0,"Question_content":"I'm trying to classify a bit of a strange-looking data set where the input vectors have a variable length. The actual data is pretty boring, but imagine a related problem where you're trying to classify the way somebody will look for their next residence based off of their current attributes and also the kinds of housing they've previously lived in. The input vector might look something like this:  Features      PersonID  Current income, debt responsibility, etc.  Credit rating  Residence 1      Square footage  Apartment, condo, home?  Duration of residence  Monthly rent or mortgage payment    Residence 2      Square footage  Apartment, condo, home?  Duration of residence  Monthly rent or mortgage payment        Classification      Realtor, Zillow, Craigslist, referrals, etc.  Of course, not everybody is going to have lived in the same number of residences. Just summarizing this variable-length housing vector won't do well either, since order matters (is the individual downsizing as children leave the home? going from parent's home to a college apartment? etc.). On a problem like this, my gut reaction is to do a Collaborative Filter (CF) to find other input vectors that look like this one and then make recommendations based on those similar-looking individuals. But how do you set up a model like this when you've got no idea how many houses that person has lived in?Clarification I'm not looking for how to summarize housing statistics in this sample problem. I'm interested in what kind of statistical/machine learning models are good at handling problems like these when your feature vector has an unknown length.","Creater_id":35079,"Start_date":"2016-07-26 13:37:50","Question_id":225782,"Tags":["machine-learning","feature-construction"],"Answer_count":0,"Last_activity":"2016-07-26 13:49:45","Link":"http://stats.stackexchange.com/questions/225782/what-models-are-well-suited-to-varying-length-feature-vectors","Creator_reputation":12}
{"_id":{"$oid":"5837a58da05283111e4d77ac"},"View_count":66,"Display_name":"Mik meadow","Question_score":1,"Question_content":"I have paired data from 9 individuals. The response variable is ordinal with levels 1-5. As the sample is very small and the data are categorical, I thought the appropriate test was McNemar's test. However I get McNemar's chi-squared = NaN and therefore a p-value = NA. What should I do to test whether the effect is significant?My data:before4 3 4 5 4 4 4 5 3after5 4 3 5 5 4 5 5 4","Creater_id":57138,"Start_date":"2016-07-26 09:41:05","Question_id":225732,"Tags":["categorical-data","nonparametric","ordinal","paired-data","mcnemar-test"],"Answer_count":2,"Last_activity":"2016-07-26 13:23:28","Link":"http://stats.stackexchange.com/questions/225732/mcnemars-test-p-value-na","Creator_reputation":118}
{"_id":{"$oid":"5837a58da05283111e4d77ba"},"View_count":13542,"Display_name":"xuexue","Question_score":43,"Question_content":"From what I know, using lasso for variable selection handles the problem of correlated inputs. Also, since it is equivalent to Least Angle Regression, it is not slow computationally. However, many people (for example people I know doing bio-statistics) still seem to favour stepwise or stagewise variable selection. Are there any practical disadvantages of using the lasso that makes it unfavourable?","Creater_id":2973,"Start_date":"2011-03-06 16:21:24","Question_id":7935,"Tags":["regression","feature-selection","lasso"],"Answer_count":8,"Last_activity":"2016-07-26 13:17:21","Link":"http://stats.stackexchange.com/questions/7935/what-are-disadvantages-of-using-the-lasso-for-variable-selection-for-regression","Creator_reputation":758}
{"_id":{"$oid":"5837a58da05283111e4d77ce"},"View_count":1252,"Display_name":"biostats101","Question_score":5,"Question_content":"If I understand correctly, a Linear Discriminant Analysis (LDA) assumes normal distributed data, independent features, and identical covariances for every class for the optimality criterion.Since the mean and variance is estimated from the training data, isn't it already a violation? I found a quotation in an article (Li, Tao, Shenghuo Zhu, and Mitsunori Ogihara. “Using Discriminant Analysis for Multi-Class Classification: An Experimental Investigation.” Knowledge and Information Systems 10, no. 4 (2006): 453–72.)     \"linear discriminant analysis frequently achieves good performances in  the tasks of face and object recognition, even though the assumptions  of common covariance matrix among groups and normality are often  violated (Duda, et al., 2001)\"-- unfortunately, I couldn't find the corresponding section in Duda et. al.  \"Pattern Classification\". Any experiences or thoughts about using LDA (vs. Regularized LDA or QDA) for non-normal data in context of dimensionality reduction?","Creater_id":39663,"Start_date":"2014-08-06 09:27:10","Question_id":110908,"Tags":["dimensionality-reduction","normality","discriminant-analysis"],"Answer_count":1,"Last_activity":"2016-07-26 13:02:13","Link":"http://stats.stackexchange.com/questions/110908/linear-discriminant-analysis-and-non-normally-distributed-data","Creator_reputation":795}
{"_id":{"$oid":"5837a58da05283111e4d77db"},"View_count":28,"Display_name":"PascalvKooten","Question_score":0,"Question_content":"I just had this \"funny\" idea: what about a classifier that not only tries to learn weights for predicting y but actually works with \"deleted\" data (as in sparse data)?Maximum likelihood estimators can work nicely with missing data for example.Let's say you have an arbitrarily big matrix of 1 million samples by 10k features.We start with a completely sparse representation, 10m by 10k.Then we randomly fill the matrix by 1%, and are able to apply a MLE estimator to predict y.Based on calculating error on another part of the data, this matrix could for example keep 0.5% of this data (somehow there should be a mechanism to choose what to keep), and add 1% new data (specifically from certain sample, and specifically from certain features), totalling 1.5%. The speed and the cap of this increase could help as learning rate.I don't know, it just sounds like a lot of fun.The difference is we are not sampling cases, but we are sampling cases + variables (like in some flavors of random forest ensembles).Is there any algorithm that uses sparsity + MLE?","Creater_id":16175,"Start_date":"2016-07-26 12:48:15","Question_id":225773,"Tags":["machine-learning","maximum-likelihood","sparse"],"Answer_count":0,"Last_activity":"2016-07-26 12:54:28","Link":"http://stats.stackexchange.com/questions/225773/sparsity-as-missing-data-mle","Creator_reputation":708}
{"_id":{"$oid":"5837a58da05283111e4d77dd"},"View_count":719,"Display_name":"Adrian","Question_score":4,"Question_content":"I found this awesome thread which shows KL divergence between two univariate Gaussians. I was wondering if the same formula worked for KL divergence b/w 2 univariate Poisson distributions.Or should I use the general KL divergence formula and plug into it the pdf for a Poisson process:\\int { pdf1(x)*{ log(pdf1(x)/pdf2(x))} } where for Poisson is pdf(x) = (\\lambda^x / x!)*e^{-\\lambda} ","Creater_id":32295,"Start_date":"2015-04-10 20:53:29","Question_id":145789,"Tags":["distributions","kullback-leibler"],"Answer_count":1,"Last_activity":"2016-07-26 12:46:49","Link":"http://stats.stackexchange.com/questions/145789/kl-divergence-between-two-univariate-poisson-distributions","Creator_reputation":127}
{"_id":{"$oid":"5837a58da05283111e4d77ea"},"View_count":69,"Display_name":"ML_Pro","Question_score":0,"Question_content":"I want to cluster lat-long data such that all clusters formed will have radius\u0026lt;=1000 metersQuestionsWhat is the actual meaning of eps parameter? Please given an example.Will setting eps=1000 serve my purpose if distance measure is haversine in meters?I understand that minpts parameter is the cluster size.","Creater_id":54214,"Start_date":"2016-07-26 01:44:54","Question_id":225655,"Tags":["clustering","spatial","hierarchical-clustering","dbscan"],"Answer_count":2,"Last_activity":"2016-07-26 12:46:26","Link":"http://stats.stackexchange.com/questions/225655/what-is-the-interpretation-of-eps-parameter-in-dbscan-clustering","Creator_reputation":628}
{"_id":{"$oid":"5837a58da05283111e4d77f8"},"View_count":38,"Display_name":"MG Funs","Question_score":0,"Question_content":"I am trying to determine whether a paired T test or Wilcoxon ranked sum test is more appropriate to test for significance of improvement in the following: Patients were surveyed for pain scores before treatment on 0-10 scalePatients were then given interventionPatients were then surveyed for pain score after treatment on 0-10 scale. The data are effectively paired since datapoints exist for the same patient pre and post treatment. Wilcoxon-signed-rank yields a very small P value over 24 patients. Very, very small (p\u0026lt;0.0001). IS wilcoxon-SR appropriate for this application or should I be using a paired T test or chi-square test?","Creater_id":124591,"Start_date":"2016-07-26 09:41:47","Question_id":225733,"Tags":["wilcoxon"],"Answer_count":1,"Last_activity":"2016-07-26 12:39:23","Link":"http://stats.stackexchange.com/questions/225733/paired-t-test-vs-wilcoxon-signed-rank","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d7805"},"View_count":138,"Display_name":"Arsl\u0026#225;n","Question_score":0,"Question_content":"All of these models seem to be used in predicting an endogenous time series variable, using several lagged exogenous time series variables. If it is so, how do we decide when to use which?","Creater_id":106704,"Start_date":"2016-07-25 13:52:28","Question_id":225597,"Tags":["time-series","arima","var","dynamic-regression","armax"],"Answer_count":2,"Last_activity":"2016-07-26 12:36:02","Link":"http://stats.stackexchange.com/questions/225597/what-is-the-difference-between-var-dynamic-regressive-and-armax-models","Creator_reputation":27}
{"_id":{"$oid":"5837a58da05283111e4d7813"},"View_count":13,"Display_name":"Revious","Question_score":0,"Question_content":"I've got a 2 column table (X, Y). Every row of the table says that the X on that row use the Y specified in the same row.This are the sample data. https://jpst.it/L6I5I'd like to cluster the table splitting it into smaller tables (less rows, same number of columns). I want to group X and Y which are 'similar'.To make an exampleIf 3 fields are used only by a single method they should be together.If two fields are always used together in every method they should betogether. And so on.Is there a method to do something similar?","Creater_id":42074,"Start_date":"2016-07-26 12:30:48","Question_id":225769,"Tags":["data-mining"],"Answer_count":0,"Last_activity":"2016-07-26 12:30:48","Link":"http://stats.stackexchange.com/questions/225769/can-i-split-a-table-through-the-correlation-of-its-2-columns","Creator_reputation":181}
{"_id":{"$oid":"5837a58da05283111e4d7815"},"View_count":18,"Display_name":"j.jerrod.taylor","Question_score":0,"Question_content":"I'm taking a class on data mining and of course, we went over the Apriori algorithm (Fast Algorithms for Mining Association Rules).   Something that I want to know is how do apriori based algorithms avoid finding spurious relationships when generating association rules?Is this a case of being aware of the difference between predictive vs. causal analysis? Meaning there could be some hidden third factor causing the association but as long as the relationship holds we don't care what that third factor is. Association rules don't say that A causes B, they just say that A and B happen together often.Is it because these algorithms are often run on VERY large data sets so the probability of having an excessive number of false positives is very low. Meaning, if you only have a sample size of 50 you could get a associations that complete a low support just by change but that is unlikely to happen when you have 500,000 or 5,000,000 samples.","Creater_id":17566,"Start_date":"2016-07-26 12:28:26","Question_id":225768,"Tags":["correlation","data-mining","association-rules","apriori"],"Answer_count":0,"Last_activity":"2016-07-26 12:28:26","Link":"http://stats.stackexchange.com/questions/225768/how-do-apriori-based-algorithms-avoid-finding-spurious-relationships-in-their-ru","Creator_reputation":129}
{"_id":{"$oid":"5837a58da05283111e4d7817"},"View_count":42,"Display_name":"rconway91","Question_score":2,"Question_content":"I am trying to perform a regression of GPR measurement response (X) vs concrete core density (Y). There is a known, constant error associated both these measurements. How would I incorporate this error into a simple linear regression?","Creater_id":86169,"Start_date":"2016-07-26 07:37:36","Question_id":225706,"Tags":["regression","linear","measurement-error"],"Answer_count":1,"Last_activity":"2016-07-26 12:18:52","Link":"http://stats.stackexchange.com/questions/225706/simple-linear-regression-with-constant-error-in-x-and-y","Creator_reputation":57}
{"_id":{"$oid":"5837a58da05283111e4d7824"},"View_count":66,"Display_name":"Arsl\u0026#225;n","Question_score":0,"Question_content":"I have the following time series dataset (dependent | independent) :Sales | Income,Inflation, Interest Rates etcAll of this is dynamic data pertaining to each of 24 months (month:0 to month:24). For 25th month onward I have no data for the independent variables (Income,Inflation, Interest Rates etc), yet I want to be able to predict sales for month:25 +.I have been trying to figure out models which I can used to implement this scenario including Dynamic Regression and ARMAX/ARIMAX models. However, it seems that to be able to predict sales for the 25th month, i need data for dependent variables (Income,Inflation, Interest Rates etc) for the month (25). Can I create a model using lagged values of the dependent and independent variables, used together in a regression model? I'm not sure if that makes sense.This is my first time series model and im not sure if i am on the right track. Please advise.","Creater_id":106704,"Start_date":"2016-07-25 12:01:58","Question_id":225578,"Tags":["r","regression","time-series","arima","armax"],"Answer_count":1,"Last_activity":"2016-07-26 12:16:46","Link":"http://stats.stackexchange.com/questions/225578/armax-or-dynamic-regression-regression-of-multiple-timeseries","Creator_reputation":27}
{"_id":{"$oid":"5837a58da05283111e4d7831"},"View_count":67,"Display_name":"Goldname","Question_score":1,"Question_content":"If I flip a fair coin 10 times, and all of them land on heads, according to the Law of Large Numbers I am more likely to get tails on the next flip. However, this is clearly the Gambler's Fallacy. How am I misinterpreting the Law of Large Numbers?","Creater_id":117869,"Start_date":"2016-07-26 12:03:10","Question_id":225765,"Tags":["probability"],"Answer_count":1,"Last_activity":"2016-07-26 12:11:37","Link":"http://stats.stackexchange.com/questions/225765/contradiction-between-the-law-of-large-numbers-and-the-gamblers-fallacy","Creator_reputation":115}
{"_id":{"$oid":"5837a58da05283111e4d783e"},"View_count":17,"Display_name":"alice","Question_score":2,"Question_content":"The true DGP is \\begin{equation}y=\\alpha_0 + \\alpha_1 x_1 + \\dots + \\alpha_k x_k +\\epsilon, \\quad \\epsilon\\sim \\mathcal{N}(0,1)\\label{eq:1}\\end{equation}but we instead estimate\\begin{equation}y=\\alpha_0 + \\alpha_1 x_1 + \\dots + \\alpha_k x_k + \\dots + \\alpha_{k'} x_{k'} \\epsilon, \\quad \\epsilon\\sim \\mathcal{N}(0,1)\\label{eq:2}\\end{equation}where . The observations are fixed design.Is it true that the standard error of the out of sample prediction of the estimated function (with spurious regressors) at some fixed  is (weakly) greater than the standard error of the out of sample prediction at  using the correctly specified model? If so, how can I prove this?","Creater_id":87306,"Start_date":"2016-07-26 11:56:49","Question_id":225762,"Tags":["regression","linear-model"],"Answer_count":0,"Last_activity":"2016-07-26 12:07:17","Link":"http://stats.stackexchange.com/questions/225762/impact-of-spurious-regressors-on-out-of-sample-prediction-error","Creator_reputation":26}
{"_id":{"$oid":"5837a58da05283111e4d7840"},"View_count":24,"Display_name":"Eric","Question_score":1,"Question_content":"From a blog post  ... The important thing is why they’re missing. Gartner excluded them this year, “…due to not satisfying the [new] visual composition framework [VCF] inclusion criteria.” VCF is the term they’re using to describe the workflow (also called streams or flowcharts) style of Graphical User Interface (GUI). To be included in the 2016 plot, companies must have offered software that uses the workflow GUI. What Garter is saying is, in essence, advanced analytics software that does not use the workflow interface is not worth following! ...  While menu-driven interfaces such as R Commander, Deducer or SPSS are somewhat easier to learn, the flowchart interface has two important advantages. First, you can often get a grasp of the big picture as you see steps such as separate files merging into one, or several analyses coming out of a particular data set. Second, and more important, you have a precise record of every step in your analysis. This allows you to repeat an analysis simply by changing the data inputs.The blogger appears to be arguing that analytical software needs to have VCF (or a GUI interface) for two reasons.  One is \"grasp the big picture\" and two is that it is self documenting.  I would argue that the \"big picture\" argument is specious logic as having VCF doesn't guarantee that one grasps all the elements that go into an analytics project.  I would argue that the second point is also incorrect as my R scripts are pretty well self documenting without a GUI-like flowchart. Still, the Gartner report which is quoted in the blog makes VCF a requirement for consideration, and analytical software has strongly drifted in that direction.My question is should \"VCF\" be a strong consideration when deciding on analytical software and if so, what features does it bring that regular programming like software (R scripts) do not?","Creater_id":45603,"Start_date":"2016-07-26 11:57:50","Question_id":225763,"Tags":["software","project-management"],"Answer_count":0,"Last_activity":"2016-07-26 11:57:50","Link":"http://stats.stackexchange.com/questions/225763/importance-of-workflow-visualization-in-a-statistical-software-package","Creator_reputation":380}
{"_id":{"$oid":"5837a58da05283111e4d7842"},"View_count":84,"Display_name":"tia_0","Question_score":2,"Question_content":"I'm trying to understant the benefit apported by the step of data augmentation in a classification algorithm. I have a vector of hexadecimal strings and a column vector containing the label associated with the string in the same position. As an optional step in the classification algorithm, a data augmentation process is performed by subsetting the strings in pieces and replating the associated label for the number of split performed. What are the benefit of this process?","Creater_id":124316,"Start_date":"2016-07-24 11:30:59","Question_id":225380,"Tags":["machine-learning","classification","sampling","computational-statistics","overfitting"],"Answer_count":1,"Last_activity":"2016-07-26 11:45:15","Link":"http://stats.stackexchange.com/questions/225380/how-does-data-augmentation-reduce-overfitting","Creator_reputation":13}
{"_id":{"$oid":"5837a58da05283111e4d784f"},"View_count":53,"Display_name":"Daniel Pinto","Question_score":0,"Question_content":"I am selecting the number of lags for a VAR model. Selection criteria and the LR statistic suggest 0 lags. Should I simply drop the VAR altogether, even if this goes against my intuition?","Creater_id":99392,"Start_date":"2016-07-25 17:52:15","Question_id":225624,"Tags":["time-series","model-selection","var","lags"],"Answer_count":1,"Last_activity":"2016-07-26 11:42:45","Link":"http://stats.stackexchange.com/questions/225624/selection-criteria-select-0-lags-for-a-var-model","Creator_reputation":44}
{"_id":{"$oid":"5837a58da05283111e4d785c"},"View_count":27,"Display_name":"Jarad","Question_score":0,"Question_content":"I am highly confused about computing Effect Size for comparing two proportions.Question 1: How does the below formula make sense? Why is it that all I need to pass in is two proportions (without trials info) and it just works?Question 2: Is there a more straight-forward formula? I ask because I've read that Cohen's D seems to be the calculation of choice for effect size... however I'm HIGHLY confused about how to compute the standard deviation of the denominator of Cohen's D when trying to compare two proportions.Effect Size Calculationh = 2 * (arcsin(sqrt(proportion1)) - arcsin(sqrt(proportion2)))Source 1: Calculate effect sizeSource 2: Statsmodels proportion effect sizeSource 3: r pwr package has effect size ES.h","Creater_id":94117,"Start_date":"2016-07-26 10:45:40","Question_id":225752,"Tags":["proportion","effect-size","statsmodels"],"Answer_count":0,"Last_activity":"2016-07-26 11:25:09","Link":"http://stats.stackexchange.com/questions/225752/properly-computing-effect-size-for-comparing-two-proportions","Creator_reputation":98}
{"_id":{"$oid":"5837a58da05283111e4d785e"},"View_count":26,"Display_name":"sponge_knight","Question_score":0,"Question_content":"I want to show that two variables are NOT correlated. When I run spearman on it, I get a very large p-value and a correlation of about -0.1.So, I accept the null hypothesis? I'm right?","Creater_id":46925,"Start_date":"2016-07-26 11:22:27","Question_id":225759,"Tags":["statistical-significance"],"Answer_count":0,"Last_activity":"2016-07-26 11:22:27","Link":"http://stats.stackexchange.com/questions/225759/if-im-running-a-test-to-determine-whether-two-variables-are-not-correlated-wha","Creator_reputation":1522}
{"_id":{"$oid":"5837a58da05283111e4d7860"},"View_count":10569,"Display_name":"Karl Morrison","Question_score":15,"Question_content":"Supervised learning1) A human builds a classifier based on input and output data2) That classifier is trained with a training set of data3) That classifier is tested with a test set of data4) Deployment if the output is satisfactoryTo be used when, \"I know how to classify this data, I just need you(the classifier) to sort it.\"Point of method: To class labels or to produce real numbersUnsupervised learning1) A human builds an algorithm based on input data2) That algorithm is tested with a test set of data (in which the algorithm creates the classifier)3) Deployment if the classifier is satisfactoryTo be used when, \"I have no idea how to classify this data, can you(the algorithm) create a classifier for me?\"Point of method: To class labels or to predict (PDF)Reinforcement learning1) A human builds an algorithm based on input data2) That algorithm presents a state dependent on the input data in which a user rewards or punishes the algorithm via the action the algorithm took, this continues over time3) That algorithm learns from the reward/punishment and updates itself, this continues4) It's always in production, it needs to learn real data to be able to present actions from statesTo be used when, \"I have no idea how to classify this data, can you classify this data and I'll give you a reward if it's correct or I'll punish you if it's not.\"Is this the kind of flow of these practices, I hear a lot about what they do, but the practical and exemplary information is appallingly little!","Creater_id":61886,"Start_date":"2015-03-30 19:04:10","Question_id":144154,"Tags":["machine-learning","unsupervised-learning","supervised-learning","reinforcement-learning"],"Answer_count":1,"Last_activity":"2016-07-26 11:16:38","Link":"http://stats.stackexchange.com/questions/144154/supervised-learning-unsupervised-learning-and-reinforcement-learning-workflow","Creator_reputation":255}
{"_id":{"$oid":"5837a58da05283111e4d786d"},"View_count":55,"Display_name":"hxd1011","Question_score":3,"Question_content":"In this What\u0026#39;s wrong to fit periodic data with polynomials? post, I tried to use Fourier basis expansion and Polynomial basis expansion to fit a toy periodic data (daily temperature data set). I got excellent answer from @Cliff AB and @Aksakal on why the Fourier basis is better for such case.At the same time, @whuber mentioned in the comment, using periodic version of splines is another option. So, what are periodic version of splines and what's the basis expansion looks like?","Creater_id":113777,"Start_date":"2016-07-26 09:30:11","Question_id":225729,"Tags":["regression","time-series","linear-model","splines"],"Answer_count":1,"Last_activity":"2016-07-26 10:57:48","Link":"http://stats.stackexchange.com/questions/225729/what-are-periodic-version-of-splines","Creator_reputation":4461}
{"_id":{"$oid":"5837a58da05283111e4d787a"},"View_count":124,"Display_name":"hxd1011","Question_score":2,"Question_content":"Suppose we want to build a binary classifier with weighted loss, i.e., it penalize different types of errors (false positive and false negative) differently. At the same time, the software we are using does not support a weighted loss.Can I hack it by manipulating my data? For example, suppose we are doing some fraud detection problem (let's assume the prior is 50% to 50% fraud vs. normal here, although most fraud detection are extremely imbalanced), where we can afford some false positives (false alerts on normal transactions), but really want to avoid false negatives (missed detection on fraud transactions). Let's say we want the loss ratio to be 1:5 (false positive : false negative), can we make 5 copies of my fraud transactions? Intuitively, by doing such copy we changed the prior distribution, and the model would more likely to say a transaction is a fraud one. So the false negative will be reduced.My guess is if we are truly minimize 0-1 loss, this can do the trick, but if we are minimizing a proxy/logistic/hinge loss (see this post), then this hack will not work well.Any formal/mathematical explanations?","Creater_id":113777,"Start_date":"2016-07-08 07:27:02","Question_id":222768,"Tags":["machine-learning","classification","optimization","loss-functions"],"Answer_count":1,"Last_activity":"2016-07-26 10:34:10","Link":"http://stats.stackexchange.com/questions/222768/can-i-hack-weighted-loss-function-by-creating-multiple-copies-of-data","Creator_reputation":4461}
{"_id":{"$oid":"5837a58da05283111e4d7887"},"View_count":14,"Display_name":"hansti","Question_score":1,"Question_content":"I have three independent variables and one dependent variable (two separate questions which I want to analyse seperately against the tree independent variables). I want to look at the relationship between the three independent variables on the dependent variable. I have made a mean score on each of the independent variables, and it is negatively skewed. IV 1 = s:-,475     IV 2 = s:-,797      IV 3 = s:-,535For my dependent variable: DV 1 = s:-,874        DV 2 = s: -1,406Can I do a multiple regression with these data's? I know it's probably difficult answering without seeing the data, but I have tried looking on the web and in books for answers, but not figured it out. I have tried to transform them (as it says in the book when it is negatively skewed). Before I transformed them, I reversed the scores. The log10 helped a lot with the independent variables, but not on the dependent variable. My second question is: Since I did not succeed transforming the dependent variable, I can not do a multiple regression with the transformed data's (I guess). Do you think my original data are decent enough to do the analysis with or do I have to do something else?Looking forward hearing your answers. ","Creater_id":124596,"Start_date":"2016-07-26 10:33:03","Question_id":225749,"Tags":["multiple-regression"],"Answer_count":0,"Last_activity":"2016-07-26 10:33:03","Link":"http://stats.stackexchange.com/questions/225749/transform-my-data-or-use-the-original-data-in-my-multiple-regression-analysis","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d7889"},"View_count":31,"Display_name":"username123","Question_score":2,"Question_content":"I am currently working on training a 5-layer neural network, and I got some problems with tanh layer and would like to try ReLU layer.  But I found that it becomes even worse for ReLU layer.  I am wondering if it is due to that I did not find the best parameters or simply because ReLU is only good for deep networks?Thanks!","Creater_id":94799,"Start_date":"2016-07-26 10:28:54","Question_id":225748,"Tags":["neural-networks"],"Answer_count":0,"Last_activity":"2016-07-26 10:28:54","Link":"http://stats.stackexchange.com/questions/225748/does-relu-layer-work-well-for-a-shallow-network","Creator_reputation":16}
{"_id":{"$oid":"5837a58da05283111e4d788b"},"View_count":33,"Display_name":"Randy Welt","Question_score":0,"Question_content":"See this example: convnet quiz Udacity.How to get from input depth = 3  to output depth = 8? My assumption: In this example we have 8 filter (kernels) and each of them slides over the 3 inputs. So in total we have 24 convolutions. That would give me a depth of 24? So how to reduce to eight? Update: I found this mapping table by Yann. LeNet5, see page 8 Table1. However the question is, whether this table is still used in the same fashion as in early convnets or today we might use a different mapping sheme? E.g. just sum 3 filtered maps into one of the 8 output maps?","Creater_id":124315,"Start_date":"2016-07-26 05:22:34","Question_id":225686,"Tags":["machine-learning","conv-neural-network"],"Answer_count":1,"Last_activity":"2016-07-26 10:24:19","Link":"http://stats.stackexchange.com/questions/225686/how-to-get-from-input-depth-to-output-depth-in-convnets","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d7898"},"View_count":32,"Display_name":"Camilo Romero","Question_score":0,"Question_content":"Could anyone share me some good material to start to read How one can prove causation or the steps to prove it using statistical test via code e.g python or R?Thanks in advanced.","Creater_id":101977,"Start_date":"2016-07-26 09:37:24","Question_id":225731,"Tags":["regression","bayesian","python","conditional-probability","causality"],"Answer_count":1,"Last_activity":"2016-07-26 10:05:51","Link":"http://stats.stackexchange.com/questions/225731/prove-causation","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d78a5"},"View_count":24,"Display_name":"fungs","Question_score":0,"Question_content":"I'm trying to get my head around the KB divergence in the context of the sample likelihood under two competing hypotheses, one optimal  and one suboptimal . Roughly speaking, I want to see the \"difference in information\" when  is used instead of the better suited  to describe the data. I write the data likelihood for each datum as  and . I could write the observed KB divergence as\\sum_{i=1}^N p(X_i \\mid H_0) \\log \\frac{p(X_i \\mid H_0)}{p(X_i \\mid H_1)}However, both likelihoods over the sample data  do not sum to one. Therefore, I'm not sure what the interpretation would be. I do consider the two situations (a) where I compare a good hypothesis  to a number of suboptimal alternatives and (b) where I compare two independent pairs  and . All comparisons are done over the same sample data .","Creater_id":48258,"Start_date":"2016-07-26 10:03:19","Question_id":225738,"Tags":["hypothesis-testing","likelihood","entropy"],"Answer_count":0,"Last_activity":"2016-07-26 10:03:19","Link":"http://stats.stackexchange.com/questions/225738/kullback-leibler-divergence-with-sample-data-likelihood","Creator_reputation":44}
{"_id":{"$oid":"5837a58da05283111e4d78a7"},"View_count":36,"Display_name":"sponge_knight","Question_score":4,"Question_content":"Why should I use KL divergence over just giving the abs difference from two PDFs?","Creater_id":46925,"Start_date":"2016-07-26 09:30:45","Question_id":225730,"Tags":["pdf"],"Answer_count":1,"Last_activity":"2016-07-26 09:47:30","Link":"http://stats.stackexchange.com/questions/225730/kl-divergence-vs-absolute-difference-between-two-distributions","Creator_reputation":1522}
{"_id":{"$oid":"5837a58da05283111e4d78b4"},"View_count":31,"Display_name":"Veera","Question_score":0,"Question_content":"Let me give a simple example,set.seed(100)disease = sample(c(0,1),100,replace = TRUE)snp1 = sample(c(\"AA\",\"AB\",\"BB\"),100,replace = TRUE)snp2 = sample(c(\"XX\",\"XY\",\"YY\"),100,replace = TRUE)summary(glm(disease~snp1*snp2, family = binomial))output1Deviance Residuals:      Min        1Q    Median        3Q       Max  -1.55176  -0.94003  -0.00649   0.90052   1.53535  Coefficients:                Estimate Std. Error z value Pr(\u0026gt;|z|)  (Intercept)   -8.109e-01  6.009e-01  -1.349   0.1772  snp1AB         5.232e-01  9.718e-01   0.538   0.5903  snp1BB         1.504e+00  8.580e-01   1.753   0.0796 .snp2XY         4.074e-16  8.498e-01   0.000   1.0000  snp2YY         1.504e+00  9.280e-01   1.621   0.1051  snp1AB:snp2XY  1.135e+00  1.335e+00   0.850   0.3952  snp1BB:snp2XY  1.542e-01  1.254e+00   0.123   0.9022  snp1AB:snp2YY -1.216e+00  1.333e+00  -0.912   0.3616  snp1BB:snp2YY -2.785e+00  1.244e+00  -2.239   0.0252 *---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 138.63  on 99  degrees of freedomResidual deviance: 127.71  on 91  degrees of freedomAIC: 145.71Output2snp12 = interaction(snp1,snp2)summary(glm(disease~snp12, family = binomial))Deviance Residuals:      Min        1Q    Median        3Q       Max  -1.55176  -0.94003  -0.00649   0.90052   1.53535  Coefficients:              Estimate Std. Error z value Pr(\u0026gt;|z|)  (Intercept) -8.109e-01  6.009e-01  -1.349   0.1772  snp12AB.XX   5.232e-01  9.718e-01   0.538   0.5903  snp12BB.XX   1.504e+00  8.580e-01   1.753   0.0796 .snp12AA.XY  -3.990e-16  8.498e-01   0.000   1.0000  snp12AB.XY   1.658e+00  9.150e-01   1.812   0.0700 .snp12BB.XY   1.658e+00  9.150e-01   1.812   0.0700 .snp12AA.YY   1.504e+00  9.280e-01   1.621   0.1051  snp12AB.YY   8.109e-01  8.333e-01   0.973   0.3305  snp12BB.YY   2.231e-01  8.199e-01   0.272   0.7855  ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 138.63  on 99  degrees of freedomResidual deviance: 127.71  on 91  degrees of freedomAIC: 145.71Number of Fisher Scoring iterations: 4So here I did a logistic regression for interaction between, lets say 2 mutations (each with 3 categories). Like shown above I can do it in 2 ways. My questions are,Are both output1 and output2 same ? If same, which one is more appropriate?How to interpret the coefficients (and odds ratios) in each case?","Creater_id":46554,"Start_date":"2016-07-26 07:05:07","Question_id":225697,"Tags":["r","regression","logistic","interaction","regression-coefficients"],"Answer_count":1,"Last_activity":"2016-07-26 09:44:28","Link":"http://stats.stackexchange.com/questions/225697/how-to-interpret-coefficients-of-interaction-terms-in-logistic-regression-in-the","Creator_reputation":67}
{"_id":{"$oid":"5837a58da05283111e4d78c1"},"View_count":56,"Display_name":"pat-s","Question_score":4,"Question_content":"I am modeling binominal data with random effects and spatial autocorrelation using MASS::glmmPQL(). Plotting the residual semivariogram of a model fit without accounting for spatial autocorrelation leads me to the following semivariogram:Hence, I set up a corSpatial object to account for the spatial autocorrelaton within my data.correl = corSpatial(value = c(1911, 0.03), form = ~ry + rx, nugget = TRUE,                    fixed = FALSE, type = \"spherical\")After inspecting the fit of the model which accounts for spatial autorcorrelation using the correl object introduced above, I get the following output when calling summary() (note that \"date\" is a random effect):Correlation Structure: Spherical spatial correlation Formula: ~ry + rx | date  Parameter estimate(s):      range      nugget 329.0934685   0.2516632I visualized the residuals(?) again in a variogram using nlme::Variogram()plot(nlme::Variogram(fit5, form = ~rx + ry, data = d, maxDist = 20000,                breaks = c(50, 200, 400, 600, 1000, 1500, 2000, seq(2700,29700,by = 1000)),                robust = FALSE), smooth = TRUE, span = 0.65, showModel = FALSE)Leaving me confused. How do I interpret the summary() outcome?How to deal with the nlme::Variogram() output showing a totally different shape compared to the resid. svgm/summary() output?What did I do wrong?","Creater_id":101464,"Start_date":"2016-07-26 09:17:41","Question_id":225726,"Tags":["r","mixed-model","interpretation","autocorrelation","random-effects-model"],"Answer_count":0,"Last_activity":"2016-07-26 09:17:41","Link":"http://stats.stackexchange.com/questions/225726/interpretation-of-glmmpql-spatial-autocorrelation-output","Creator_reputation":36}
{"_id":{"$oid":"5837a58da05283111e4d78c3"},"View_count":15,"Display_name":"George Sovetov","Question_score":2,"Question_content":"I expect that, in Python with numpy and scipy,scipy.stats.chisquare(numpy.bincount(numpy.random.randint(100, size=1000000)))will return P-value which is very close to 1. In reality, I constantly get different P-values which are not close to 1 at all. P-values of different runs are scattered from 0 to 1, no matter how many categories I have and how big sample is.It seems I don't understand something very basic. But what? Is chi2 test applicable here?If not, how can I test uniformity of pseudo-random values generated by machine? (I know that there are a lot of tests for random number generators. I'm looking for test for uniformity.)What I didn't understandIndeed, P-value is distributed uniformly if null-hypothesis is true. Looks obvious when I understood it.My mistake was that I expected to have P-value to be close to 1. If distribution is really uniform, P-value should be uniformly scattered over [0, 1]. And I should check that P-value greater than some value close to 0. If distribution weren't uniform, I would get values close to 0, since there is very low (close to 0) probability that I would get more extreme results if distribution were uniform.","Creater_id":123268,"Start_date":"2016-07-26 07:58:50","Question_id":225710,"Tags":["random-generation","uniform","scipy","numpy"],"Answer_count":0,"Last_activity":"2016-07-26 09:08:38","Link":"http://stats.stackexchange.com/questions/225710/chi2-test-on-big-uniformly-random-sample","Creator_reputation":111}
{"_id":{"$oid":"5837a58da05283111e4d78c5"},"View_count":314,"Display_name":"Po Ning","Question_score":6,"Question_content":"I am trying to create a Block Toeplitz matrix likes this:where ,  is a  matrix. I found the toeplitz() function in R only deals with the case when s are scalars. Currently I only know to use loops to create the matrix , which is not efficient in R. If someone have any suggestions to vectorize the code, could you please help me out.For example, I am interested in creating a matrix like this:Let ,, and let  for .The R code I wrote is,U0 \u0026lt;- matrix(c(0.8, 0, 1, 0.8), 2, 2) # create U0phi \u0026lt;- matrix(c(0.9, 0, 1, 0.8), 2, 2) # create phin \u0026lt;- 100 # n is the number of U(i)k \u0026lt;- dim(U0)[1]U \u0026lt;- matrix(NA, k*n, k*n) # create an empty matrix to receive valuefor (i in 1:n) {   for (j in 1:n) {     if (i \u0026gt;= j) {       U[((i-1)*k+1):(i*k), ((j-1)*k+1):(j*k)] \u0026lt;- phi^(i-j) %*% U0     } else {        U[((i-1)*k+1):(i*k), ((j-1)*k+1):(j*k)] \u0026lt;- t(phi)^(j-i) %*% U0     }   } }Is there anyway to avoid the loops?Thanks.","Creater_id":79036,"Start_date":"2015-09-06 11:20:20","Question_id":171342,"Tags":["r","matrix","toeplitz"],"Answer_count":2,"Last_activity":"2016-07-26 09:05:37","Link":"http://stats.stackexchange.com/questions/171342/more-efficient-way-to-create-block-toeplitz-matrix-in-r","Creator_reputation":48}
{"_id":{"$oid":"5837a58da05283111e4d78d2"},"View_count":50,"Display_name":"user3639557","Question_score":2,"Question_content":"I am reading this, and don't understand how this is reached \\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha+n)}=\\frac{(\\alpha+n)\\beta(\\alpha+1,n)}{\\alpha\\Gamma(n)}The following relation mentioned on the Wikipedia,\\beta(\\alpha,n)=\\frac{\\Gamma(\\alpha)\\Gamma(n)}{\\Gamma(\\alpha+n)} \\Rightarrow \\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha+n)}=\\frac{\\beta(\\alpha,n)}{\\Gamma(n)} so given the above two, this must be true, \\frac{(\\alpha+n)\\beta(\\alpha+1,n)}{\\alpha\\Gamma(n)}=\\frac{\\beta(\\alpha,n)}{\\Gamma(n)}which means this also must be true, \\frac{(\\alpha+n)\\beta(\\alpha+1,n)}{\\alpha}=\\beta(\\alpha,n). I can't see why this is true, given the definition of the  function.","Creater_id":56676,"Start_date":"2016-07-26 08:19:05","Question_id":225721,"Tags":["distributions","gamma-distribution","beta-distribution"],"Answer_count":1,"Last_activity":"2016-07-26 08:56:28","Link":"http://stats.stackexchange.com/questions/225721/gamma-and-beta-function-relation","Creator_reputation":310}
{"_id":{"$oid":"5837a58da05283111e4d78df"},"View_count":10,"Display_name":"thatWiseGuy","Question_score":1,"Question_content":"I have many (250) instances of an experiment in which a binary random variable is the output. I selected a sampling of 80 instances and verified the result in every case to be success. I suspect that all 255 instances are success, but I cannot be sure without testing all of them.Question: Based on testing only these 80 instances, how can I calculate a confidence level that all 250 instances are success? Is this even possible?I tried using the confidence interval formula for a Bernoulli random variable, but because all instances sampled were success, I came up with the confidence interval (1,1) for any alpha level, which isn't what I'm looking for.More generally, how do you deal with the situation where all samples are of the same class? The problem isn't that my sample size is too small--it's just that a seemingly large majority of instances (potentially 100%) are successes.","Creater_id":124580,"Start_date":"2016-07-26 08:38:27","Question_id":225723,"Tags":["bernoulli-distribution"],"Answer_count":0,"Last_activity":"2016-07-26 08:38:27","Link":"http://stats.stackexchange.com/questions/225723/find-confidence-level-when-sampled-bernoulli-trials-are-all-successes","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d78e1"},"View_count":97,"Display_name":"EAguirre","Question_score":0,"Question_content":"I have create a matrix with bootstrap estimation of the ATT.First I extract a sample using bsample command with cluster , then I estimate the  propensity score ,and the ATT using the psmatch2 (with kernel option), and finally I save the result in a matrix.In matrix form#delimit ;matrix input mymat=(0.1518 \\0.1406 \\0.1162 \\0.1222 \\0.0841 \\0.1578 \\0.2332 \\0.09 \\0.1362 \\0.1478 \\0.1984 \\0.1706 \\0.2553 \\0.0375 \\0.1081 \\0.1719 \\0.1873 \\0.0904 \\0.1259 \\0.1782 \\0.0761 \\0.1202 \\0.1353 \\0.072 \\0.1219 \\0.0915 \\0.0482 \\0.1197 \\0.0557 \\0.1192 \\0.1138 \\0.1516 \\0.096 \\0.1999 \\0.0502 \\0.1458 \\0.2614 \\0.1882 \\0.1613 \\0.1496 \\0.1719 \\0.1361 \\0.1496 \\0.1111 \\0.2091 \\0.0939 \\0.1053 \\0.1339 \\0.0633 \\0.044 \\0.1965 \\0.1847 \\0.1415 \\0.1367 \\0.2746 \\0.0844 \\0.0699 \\0.0881 \\0.1399 \\0.0481 \\0.1454 \\0.0721 \\0.0571 \\0.0902 \\0.1602 \\0.1352 \\0.0597 \\0.1764 \\0.0704 \\0.1703 \\0.1994 \\0.0313 \\0.2192 \\0.0511 \\0.1771 \\0.1828 \\0.1367 \\0.1496 \\0.1656 \\0.1847 \\0.1081 \\0.2596 \\0.0546 \\0.1651 \\0.1315 \\0.1104 \\0.1102 \\0.2174 \\0.0487 \\0.1384 \\0.0824 \\0.1121 \\0.0275 \\0.2632 \\0.1299 \\0.1135 \\0.194 \\0.2056 \\0.1198 \\0.2419 );Or in dta fileinput ATT   0.1518  0.1406  0.1162  0.1222  0.0841  0.1578  0.2332  0.09  0.1362  0.1478  0.1984  0.1706  0.2553  0.0375  0.1081  0.1719  0.1873  0.0904  0.1259  0.1782  0.0761  0.1202  0.1353  0.072  0.1219  0.0915  0.0482  0.1197  0.0557  0.1192  0.1138  0.1516  0.096  0.1999  0.0502  0.1458  0.2614  0.1882  0.1613  0.1496  0.1719  0.1361  0.1496  0.1111  0.2091  0.0939  0.1053  0.1339  0.0633  0.044  0.1965  0.1847  0.1415  0.1367  0.2746  0.0844  0.0699  0.0881  0.1399  0.0481  0.1454  0.0721  0.0571  0.0902  0.1602  0.1352  0.0597  0.1764  0.0704  0.1703  0.1994  0.0313  0.2192  0.0511  0.1771  0.1828  0.1367  0.1496  0.1656  0.1847  0.1081  0.2596  0.0546  0.1651  0.1315  0.1104  0.1102  0.2174  0.0487  0.1384  0.0824  0.1121  0.0275  0.2632  0.1299  0.1135  0.194  0.2056  0.1198  0.2419  endI can found easily an IC   quietly:sum ATT ,detail  display \"90% CI: [ \" round(`r(p5)',0.001) , round(`r(p95)',0.001) \"]\"  90% CI: [ -.163 ,-.066]My question is how can I find a p-value   Here are some thoughts, using:  http://www.stata.com/manuals13/rbootstrap.pdf  local NBootstrap   100            /* k */  local ATTObserved -0.128   quietly : sum ATT  local aux           =  r(mean)    gen auxvar          = (1/(`NBootstrap'-1)) *(ATT-`aux')*(ATT-`aux')   quietly : sum auxvar  local SDBootstrap=sqrt( r(sum) )t-test   gen ttestvar=  (ATT- `ATTObserved' )/`SDBootstrap'  local ttesobserved (`ATTObserved') /`SDBootstrap'  display \"Bias\" `ATTObserved'-`aux'Bias             -.012436  display \" ttest observed\" `ttesobserved'ttest observed   -4.3199073  count if abs(ttestvar) \u0026gt; abs(`ttesobserved') \u0026amp; ATT!=.0  drop auxvar ttestvar","Creater_id":26956,"Start_date":"2016-07-25 18:38:42","Question_id":225629,"Tags":["p-value","stata","bootstrap","matching"],"Answer_count":1,"Last_activity":"2016-07-26 08:22:20","Link":"http://stats.stackexchange.com/questions/225629/bootstrap-p-value-in-stata","Creator_reputation":72}
{"_id":{"$oid":"5837a58da05283111e4d78ee"},"View_count":60,"Display_name":"RTrain3K","Question_score":0,"Question_content":"I've been looking into missing data imputation via multiple imputation chained equations (MICE) and I've yet to come across a discussion of when to use MICE rather than regression to impute data. Until now, whenever I've needed to impute data, I've been able to create a regression that explains 75-90% of explained variance in variable that I want to impute observations for and predict observations for the missing data. My predicament now is that I have a variable that I need to impute missing data for and the best model I can come up with only explains 35% of the variance in the available observations. My question is how can imputation via MICE perform better than just using regression since MICE is based on \"chained\" regression techniques of the variables within the data set? In other words, isn't the accuracy of MICE based on the predictive power of the variables used in the chained equations? Is it simply better because it does many simulations?","Creater_id":82576,"Start_date":"2016-07-26 08:17:09","Question_id":225720,"Tags":["regression","multivariate-analysis","data-imputation","mice"],"Answer_count":0,"Last_activity":"2016-07-26 08:17:09","Link":"http://stats.stackexchange.com/questions/225720/when-to-use-multiple-imputation-chained-equations-vs-regression-to-impute-data","Creator_reputation":57}
{"_id":{"$oid":"5837a58da05283111e4d78f0"},"View_count":69,"Display_name":"M. Beausoleil","Question_score":2,"Question_content":"I'm trying to add an interaction term to a JAGS model. I found this where it shows some interaction terms. But I don't understand what's the guide lines to create an interaction term, which is different than what's proposed here. I have this code:    for(ind in 1:nind) { ## nind = nrow(dX)      for(yr in 1:nyear) {        logit(phi[ind,yr]) \u0026lt;-          phi.sp[species[ind]] + ## effect of species             phi.year[yr] + ## effect of year               phi.pc1[species[ind]]*pc1[ind] + ## Effect of morphology, but only the PCA scores               phi.pc2[species[ind]]*pc2[ind] +                PC1_2 * pc1[ind] * pc2[ind]of individuals captured)        } ## (yr in 1:year)Is that correct?Also, is it a good idea to add an interaction between PC axes? Since a scaling 2 would preserve a Mahalanobis distance and focus on correlation between axis and vectors, what would an interaction between a PC1 and PC2 tell?","Creater_id":93498,"Start_date":"2016-07-23 14:01:32","Question_id":225299,"Tags":["interaction","model","jags"],"Answer_count":1,"Last_activity":"2016-07-26 08:06:39","Link":"http://stats.stackexchange.com/questions/225299/how-to-add-an-interaction-term-in-jags","Creator_reputation":270}
{"_id":{"$oid":"5837a58da05283111e4d78fd"},"View_count":44,"Display_name":"user90772","Question_score":0,"Question_content":"Is there a way to obtain a smooth estimate of the hazard function at specific x coordinates in R? My problem is related to functional data analysis where I have a number of hazard function estimates (data for different periods) so the functions need to be smooth but the knots need to be placed at the same points (these will be some quantiles).","Creater_id":91239,"Start_date":"2016-07-26 08:05:56","Question_id":225714,"Tags":["r","survival","kernel-smoothing","hazard","functional-data-analysis"],"Answer_count":0,"Last_activity":"2016-07-26 08:05:56","Link":"http://stats.stackexchange.com/questions/225714/smooth-hazard-function-at-given-knots-in-r","Creator_reputation":66}
{"_id":{"$oid":"5837a58da05283111e4d78ff"},"View_count":10,"Display_name":"Shantanu","Question_score":0,"Question_content":"when i am building my model in r \u0026amp; looking the goodness of fit test for model fitting, at .5 threshold level, my p value \u003e.05, which tells me that my model is fitting the data well.As soon as i change my threshold value to .4, based on what is my priority from the model, my hosmer lemeshow test becomes significant (p\u0026lt;.05). I dont know if its supposed to be like that or there is some problem with the model?Anyone Please clarify this doubt","Creater_id":9079,"Start_date":"2016-07-26 07:59:25","Question_id":225711,"Tags":["logistic","multinomial"],"Answer_count":0,"Last_activity":"2016-07-26 07:59:25","Link":"http://stats.stackexchange.com/questions/225711/hosmer-lemeshow-test-becomes-significant-when-cut-off-value-changes","Creator_reputation":47}
{"_id":{"$oid":"5837a58da05283111e4d7901"},"View_count":72,"Display_name":"Munichong","Question_score":0,"Question_content":"I have four vectors of numbers, one is the ground truth (binary, rach number is either 1 or 0), the other three are the corresponding prediction (each number is a probability from 0 to 1) generated by three different models. I use both RMSD and log-loss to evaluate my models. I know RMSD, but I am new to Log-loss. Someone suggests me that log-loss is better for such probabilistic prediction. But I receive very different results:RMSD:Model1: 0.43456628175892914Model2: 0.41783951818827036Model3: 0.4274284133425074Log-loss:Model1: 0.56063854990284279Model2: 1.4259755316961391Model3: 0.68864661902342794Using RMSD, the Model2 seems to be the best, while using Log-loss, it is the worst. Why may this happen? Which one should I trust?Thanks.","Creater_id":35802,"Start_date":"2016-07-26 07:11:29","Question_id":225700,"Tags":["machine-learning","logistic","experiment-design","loss-functions","model-evaluation"],"Answer_count":0,"Last_activity":"2016-07-26 07:41:20","Link":"http://stats.stackexchange.com/questions/225700/rmsd-vs-log-loss","Creator_reputation":246}
{"_id":{"$oid":"5837a58da05283111e4d7903"},"View_count":5471,"Display_name":"Kerry","Question_score":7,"Question_content":"I am confused and just need some confirmation about calculating the relative variable importance value for the co-variates I used in AIC model selection procedures.  I know that there is this one discussion but it doesn't quite confirm explicitly enough what I should do.Burnham and Anderson (2002) describe a simple way to quantify variable importance.  Page 168: Estimates of the relative importance of predictor variables  xj can best be made by summing the AIC weights across all the models  in the set where variable j occurs.However, to use this method, one must have an equal number of models for each variable; otherwise, some variables will be over represented or under represented resulting in biased relative importance values.  Page 169: When assessing the relative importance of variables using sums of the AIC weights, it is important to achieve a balance in the number of models that contain each  variable j.Does this mean if I have a set of models with their model weights from AIC procedure (these are not ranked by weight, just the order I created them):1   INTERCEPT            2   REPRO   TIME         3   REPRO   TIME    R*T  4   REPRO   TIME    WR  5   REPRO   TIME    WR  WR*R 6   REPRO   TIME    WR  WR*T 7   WRTo calculate the relative variable weight I would sum up the weight for each incident that TIME was in the models and I would do so for each of the other variables.  However, this is not completely correct right? Because there is not a balance in the number of models that contain each variable right?  So, to correct for this I would then divide the sum of these weights by the number of models that had that variable. (Kittle et al 2008 \"the scale-dependent impact of wolf predation risk....\" does this). So, for instance if the sum  of the weights for time was .75 I would divide it by 5 because it was in 5 of the models, likewise, WR would be divided by 4.   It seems like a silly question but it really changes the results and interpretation from my analysis.  Because for instance, WR*T is only in 1 model and it comes out to be in one of the top models so it has a high model weight, but Time and Repro are also in this top model but also in 4 other candidate models.  So dividing the weight of T \u0026amp; R by 5 reduces the importance of T or R from (0.999), giving them a RVI of 0.2 and the RVI to WR*T value of 0.7.  Is that right?In addition to this, my next question would be - do you do this over JUST the \"BEST\" (within 2AIC or what ever criteria) models or over all 7 regardless of what surfaced to the top?  I used MuMIn package and use the importance command, but then when you use the get best models, it asks if you want to recalculate the importance which then it recalculates for just the top models.  Which is more appropriate to use?  This doesn't make sense when only 1 model is the best.  I would then assume it should be calculated over all models.","Creater_id":14125,"Start_date":"2012-12-20 03:26:31","Question_id":46289,"Tags":["r","mixed-model","model-selection","aic"],"Answer_count":2,"Last_activity":"2016-07-26 07:35:46","Link":"http://stats.stackexchange.com/questions/46289/relative-variable-importance-with-aic","Creator_reputation":474}
{"_id":{"$oid":"5837a58da05283111e4d790f"},"View_count":243,"Display_name":"guy","Question_score":6,"Question_content":"Suppose one is interested in stochastic processes for the purpose getting a theoretical understanding of MCMC. They already have a decent understanding of probability theory (let's say at the level Billingsley) and want to develop enough tools to potentially prove theorems about the Markov Chains they construct at a research level. What book or sequence of books would be reasonable for getting up to speed on the theory?EDIT: This is not a question about applying MCMC. It is essentially asking for a mathematics textbook. Something less like Robert and Casella and more like Meyn and Tweedie (an answer I probably would have accepted when I asked this question). ","Creater_id":5339,"Start_date":"2012-11-07 16:25:24","Question_id":43096,"Tags":["references","mcmc","markov-process"],"Answer_count":1,"Last_activity":"2016-07-26 07:33:10","Link":"http://stats.stackexchange.com/questions/43096/what-books-to-read-for-mcmc-theory","Creator_reputation":3124}
{"_id":{"$oid":"5837a58da05283111e4d7911"},"View_count":22,"Display_name":"Hassan Saif","Question_score":0,"Question_content":"I've been checking several related questions about p-value correction and multiple hypothesis testing, but I've not figured out yet how and when to apply them in text classification problemsWe have N classifiers that we calculate their accuracy in a binary document classification problem (i.e., Topic-related, Topic-unrelated). We also perform a pair-wise comparison between all the classifiers using the same dataset. The null hypothesis is that performances do not differ. We used T-Test to calculate p-values and evaluate the hypothesis.My questions are:1) Do I need to apply p-value correction on our computed p-values?2) Is it necessary to perform multiple hypothesis testing given that I'm testing against multiple hypothesis related to each pair of classifiers I'm evaluating? If yes, How can this be done? ","Creater_id":27018,"Start_date":"2016-07-26 07:21:35","Question_id":225704,"Tags":["hypothesis-testing","multiple-comparisons"],"Answer_count":0,"Last_activity":"2016-07-26 07:21:35","Link":"http://stats.stackexchange.com/questions/225704/multiple-hypothesis-testing-in-binary-text-classification","Creator_reputation":101}
{"_id":{"$oid":"5837a58da05283111e4d7913"},"View_count":24,"Display_name":"Vaniax","Question_score":1,"Question_content":"I investigated the relation between an angle  and a sensor value . So I have  which can be modeled here as a simple polynom. When I want to use this relationship in a real system I have the problem that I need the value  with a high frequency, but the sensor is rather slow. So I tried to model  and  is the time from a high frequency clock. Once again, a polynomial model is sufficient here.Is it possible to combine these two models in the form . I believe a normal multiple regression is not possible because  and  are highly correlated. Are there alternative methods I can use?Edit:I'm not sure, but is this an example of a mediation situation?","Creater_id":29550,"Start_date":"2016-07-26 06:32:43","Question_id":225693,"Tags":["regression","correlation","multiple-regression","multicollinearity"],"Answer_count":0,"Last_activity":"2016-07-26 06:56:32","Link":"http://stats.stackexchange.com/questions/225693/combine-two-regression-models-when-variables-are-highly-correlated","Creator_reputation":118}
{"_id":{"$oid":"5837a58da05283111e4d7915"},"View_count":38,"Display_name":"German Demidov","Question_score":1,"Question_content":"Sorry for this question, but I am really not sure how to calculate BIC for my situation.My models are mixtures of normals with different number of components. Variances are equal for all components within the model.I have 10 datasets of 1000 points each. The datasets are generated by different models in terms of variances, but all models from 10 datasets have the same number of components and the same means of normal components. This is why I can not just mix all 10 datasets in one.So, I have models . Only one of them is true for all 10 datasets. Using usual BIC dataset by dataset I can choose, i.e., 8 times  and 2 times other models. How can I estimate BIC for 10 datasets simultaneously?I can calculate the number of parameters: for each dataset it is justnumber of estimated means + 1 estimated variance per model + number of estimated proportionsBut what should I do with the number of datapoints  in this formula? Should I sum all BICs dataset by dataset, or put all points/sum of log-likelihood into the BIC formula?I found BIC calculation here, but I want to understand and to be sure if the procedure from Mclust is valid (for my case) because I am solving several problems separately, it is not a multi-dimensional problem in some sense.","Creater_id":100673,"Start_date":"2016-07-26 01:28:33","Question_id":225651,"Tags":["gaussian-mixture","bic"],"Answer_count":0,"Last_activity":"2016-07-26 06:40:28","Link":"http://stats.stackexchange.com/questions/225651/how-to-calculate-bic-for-multidimensional-problem","Creator_reputation":344}
{"_id":{"$oid":"5837a58da05283111e4d7917"},"View_count":30,"Display_name":"lithium","Question_score":2,"Question_content":"I'm working through Convolutional Neural Network paper here on adversarial learning and I'm having trouble with the derivative proof of adversarial logistic regression. The correct answer presented (on page 4) is:E_{x,y~p_{data}}\\zeta(y(\\epsilon||w||_1-w^Tx-b))However, my derivation, based on the loss function from page three gives the sign of the gradient as  rather than . I've been going through my math for the past two hours and if anyone could shed any insight, that would be really helpful. I've written out my logic below. \\begin{align}E_{x,y~p_{data}}\\zeta(-y(w^Tx-b))  \\\\\\zeta(z) \u0026amp;= \\log(1 + \\exp(z))  \\\\[5pt]\\frac{\\partial{E_{xy}}}{\\partial{x}} \u0026amp;= \\partial x(\\frac{1}{1+e^{-y(w^Tx+b)}})  \\\\[5pt]\u0026amp;=\\frac{e^{-y(w^Tx+b)}}{1+e^{-y(w^Tx+b)}}\\partial x (-y(w^Tx+b))  \\\\[5pt]\u0026amp;=\\frac{e^{-y(w^Tx+b)}}{1+e^{-y(w^Tx+b)}}(-yw^T)  \\\\[5pt]\\sign\\bigg(\\frac{\\partial{E_{xy}}}{\\partial{x}}\\bigg)\u0026amp;=-\\sign(yw^T)\\end{align}","Creater_id":124498,"Start_date":"2016-07-25 16:40:08","Question_id":225614,"Tags":["self-study","neural-networks","deep-learning","proof","conv-neural-network"],"Answer_count":0,"Last_activity":"2016-07-26 06:38:16","Link":"http://stats.stackexchange.com/questions/225614/adversarial-learning-gradient-derivation","Creator_reputation":11}
{"_id":{"$oid":"5837a58da05283111e4d7919"},"View_count":293,"Display_name":"Berk U.","Question_score":7,"Question_content":"I understand the advantages of using the Lasso (e.g. scalability, regularization). That being said, I am also aware that the Lasso is an approximate method for feature selection, and that it does not necessarily return the optimal subset of features (i.e., the one that would be identified through -minimization, or a brute-force search). In light of this, I'm wondering:What are the practical disadvantages of using the Lasso for feature selection in binary classification problems?Is there a realistic example where the Lasso returns a subset of features that is completely different from the true optimal set of features? Note: To be clear, I know that there was a related discussion on Lasso vs. stepwise regression. The reason why I've posted a new question instead of posting in the old forum is because:the old question was about regression problems the old question compares Lasso to stepwise regression (also an approximate method). In comparison, I suppose this is trying to compare Lasso (-penalty regularization) to brute force (-penalty regularization), which would be optimal.","Creater_id":3572,"Start_date":"2015-03-05 14:05:27","Question_id":140544,"Tags":["feature-selection","lasso"],"Answer_count":1,"Last_activity":"2016-07-26 06:30:41","Link":"http://stats.stackexchange.com/questions/140544/what-are-the-disadvantages-of-using-lasso-for-feature-selection-in-classificatio","Creator_reputation":932}
{"_id":{"$oid":"5837a58da05283111e4d791b"},"View_count":674,"Display_name":"Paul","Question_score":4,"Question_content":"I'm using the concept of Hedonic regression in order to model the prices for real estates. I'm having some trouble with my approach.What I have and what I domy data consists out of real estates with following charcteristics: price | livingArea | propertyArea | condoFloorNumber | roomCount | elevator | garage | quiet | etc.I run a robust regression without intercept lmRob(price ~ . -1)What I wanta model with which I can predict the price of real estates, but which are not in the used data setalso it would be nice to have some constraints on the coefficientsProblemsvery often I get bad values for the coefficients ex: bathroomCount = -80000. it's not possible that with a additive bathroom , the price of the house will sink with 80.000€also I tried to use the function pcls in order to put some constraints on the coefficients, but this method gave very bad results. In the plot Y = price and X = livingArea. as you can see, the regression line isn't correct.another thought was to transform the regression problem into a maximization or minimization problem, but didn't managed to do italso I tried to use different regression methods lm, lmrob, ltsReg, MARS, but they also give me bad coefficients. (sometimes this bad coefficients make a good price estimation)I think that the big number of dummy variables damages a little bit the regressionIs my approach false?Does someone have some hints, tricks for me? (I'm not a statistician)[UPDATE]This is how the plotted data looks like. LivingArea is the only non-dummy variable.[UPDATE 2]y = bX      meansy = b_0*X_0 + b_1*X_1 + ... + b_k*X_k     which is an equation system like this:y[0] = b_0*X_0[0] + b_1*X_1[0] + ... + b_k*X_k[0]...y[n] = b_0*X_0[n] + b_1*X_1[n] + ... + b_k*X_k[n]Did I got it right? If so, isn't possible to add some inequality constraints equation to it. example:b_0 \u0026gt;= 2000b_2 \u0026lt;= b_0/2[UPDATE 3]I'm running the regression without intercept, because if all the characteristics of a real estate = 0, then of course it'S price = 0. Nobody would pay for an apartment with 0m².but it seems that the regression line where it was used an intercept (blue) looks far more better than the regression line without intercept (green). I can't understand why it is so. and why doesn't the regression line without intercept start at the point (0,0)?","Creater_id":32346,"Start_date":"2015-06-12 01:26:14","Question_id":156619,"Tags":["r","regression","estimation","regression-coefficients"],"Answer_count":2,"Last_activity":"2016-07-26 06:21:54","Link":"http://stats.stackexchange.com/questions/156619/modeling-prices-with-the-hedonic-regression","Creator_reputation":201}
{"_id":{"$oid":"5837a58da05283111e4d791d"},"View_count":14,"Display_name":"LoveMeow","Question_score":0,"Question_content":"My survey has the following components:1. Some descriptive data such as gender, employment, age2. All other data is collected on a equally spaced scale of 1 to 5I want to do a test to see if there is significant difference between 2 clusters that I have obtained from doing a K means clustering using Euclidean distance. I am not sure how to do it since there could be many things that contribute to the difference between the clusters, i.e the gender or the employment, or the age?My questions being, how do I test or find the feature that contributes to have these 2 separate clusters?  How do I find the feature that contributes to this clustering pattern?I am using python scipy kmeans2 method to do the clustering.","Creater_id":53787,"Start_date":"2016-07-26 06:16:19","Question_id":225690,"Tags":["python","survey","k-means"],"Answer_count":0,"Last_activity":"2016-07-26 06:16:19","Link":"http://stats.stackexchange.com/questions/225690/find-features-that-contribute-to-the-cluster-formation-in-survey-data","Creator_reputation":121}
{"_id":{"$oid":"5837a58da05283111e4d791f"},"View_count":25,"Display_name":"Jen","Question_score":4,"Question_content":"I have a data set of mean trait values for each of 18 populations, and want to test whether several ecological variables are related to variation in traits. I'm using the corrected Akaike information criterion to evaluate which ecological model fits best. One of the models uses distance-based Moran eigenvector maps (dbMEMs) based on geographic distance (following Legendre et al 2015 Methods in Ecology \u0026amp; Evolution, and earlier work). In generating the dbMEMs, I recovered 4 significant and positive dbMEMs. My understanding is that these 4 synthetic variables should be used together in subsequent analyses, to evaluate unmeasured spatially-structured ecological variation.A model of the mean trait values that includes all 4 dbMEMs has a strong adjusted-R2 value compared to some other models (e.g., PC1 from a PCA on various ecological variables expected to be important). The other models appear to be a weak fit. But, the AICc value is high for the dbMEMs model because it is penalized for having 4 variables. Because these 4 variables 'travel' together and are used together, it seems reasonable to me to treat them as 1 variable, and to therefore calculate AICc with a k value that is 3 less than the original value. For example, I can calculate AICc with k = 3, rather than k = 6 (accounting for the intercept, error, and 4 dbMEM variables). The AICc is naturally lower then and reflects how the model actually seems to fit the data relative to other models. Is it valid to 'correct' the penalty for extra variables in this way? I can't find a reference for it.","Creater_id":121139,"Start_date":"2016-06-24 03:17:33","Question_id":220443,"Tags":["aic","degrees-of-freedom","information-theory","eigenvalues","ecology"],"Answer_count":1,"Last_activity":"2016-07-26 05:55:27","Link":"http://stats.stackexchange.com/questions/220443/can-i-reduce-the-aicc-penalty-for-multiple-variables-when-some-variables-should","Creator_reputation":38}
{"_id":{"$oid":"5837a58da05283111e4d7921"},"View_count":26,"Display_name":"Michael Lambertus","Question_score":1,"Question_content":"I have a question regarding an output interpretation.The model can be described as followedmodel \u0026lt;- lm(variable ~ p1*p2 , data = df)predictor 1 (p1) is fixed, group;predictor 2 (p2) is continuous, tiredness,and variable is a testscore.Now, I set up 3 contrastsmat \u0026lt;- cbind( \"A\"=c(-1/3, -1/3, -1/3, 1),\"B\"=c(-1/2,-1/2, 1, 0), \"C\"=c(-1, 1,0 , 0))contrasts(df$p1) \u0026lt;- matNow, I am not sure how to interpret this output \u0026gt; summary.lm(model)Call:lm(formula = variable ~ p1 * p2, data = df)Residuals:Min       1Q   Median       3Q      Max -1.73499 -0.31352  0.04792  0.47164  1.21074 Coefficients:Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)   3.84715    0.27487  13.996   \u0026lt;2e-16 ***p1A          -0.83339    0.45499  -1.832   0.0696 .  p1B           0.40285    0.47043   0.856   0.3936    p1C           0.01339    0.38644   0.035   0.9724    p2            0.06174    0.05141   1.201   0.2322    p1A:p2        0.15982    0.08958   1.784   0.0770 .  p1B:p2       -0.07540    0.08557  -0.881   0.3801    p1C:p2        0.01532    0.07084   0.216   0.8292    ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.6717 on 116 degrees of freedomMultiple R-squared:  0.05534,   Adjusted R-squared:  -0.001669 F-statistic: 0.9707 on 7 and 116 DF,  p-value: 0.456Well, it is not significant, that is fine. The result of the main effects and interaction was not significant (not reported here). Groups did not differ in test score, tiredness does not seem to have an influence on the test score and tiredness does not seem to significantly interact with the test score (again, not reported here).But what is the correct interpretation of p1A:p2? For example, is it significant?Thank you very much in advance!","Creater_id":124544,"Start_date":"2016-07-26 05:12:05","Question_id":225685,"Tags":["r","interpretation","ancova","contrasts"],"Answer_count":0,"Last_activity":"2016-07-26 05:35:27","Link":"http://stats.stackexchange.com/questions/225685/interpretation-output-aov-and-contrasts","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d7923"},"View_count":52,"Display_name":"syakey","Question_score":1,"Question_content":"I am using dynamic time warping to extract similarity between sequences. Basically, my sequences are consist of simple number (chain codes) not a signal or any time series.I did the backtracking and finding the optimal warp path of my sequences, and also plot the optimal path of the two sequences (one of it is the sequence template).My question is, besides looking at the distance between sequences (lowest distance means it's a perfect match with the template), what can i discuss or extract from plotting the optimal path as shown in the figure below?","Creater_id":89085,"Start_date":"2015-09-09 07:52:02","Question_id":171750,"Tags":["classification","data-visualization"],"Answer_count":1,"Last_activity":"2016-07-26 04:48:23","Link":"http://stats.stackexchange.com/questions/171750/dynamic-time-warping-data-visualization","Creator_reputation":6}
{"_id":{"$oid":"5837a58da05283111e4d7925"},"View_count":40,"Display_name":"Mocialov Boris","Question_score":1,"Question_content":"I have two confusion matrices:(1)110  -45  -45  -44  -45-46   91  -46  -46  -46-87  -86   98  -87  -87-118 -118 -118  118 -118-79  -79  -79  -79   91with accuracy 36.6 and(2)99   99   98  100   9995   95   96   95   9695   95   95   95   94104  105  106  105  10589   91   91   91   91with accuracy 31.7Confusion matrix (1) is more useful than (2), because it shows the the classifier can be used to classify more accuratelySince the accuracies are very similar, what other measure can I use to show that the classifier that produced matrix (2) is basically useless?","Creater_id":124548,"Start_date":"2016-07-26 04:39:01","Question_id":225682,"Tags":["classification","confusion-matrix"],"Answer_count":0,"Last_activity":"2016-07-26 04:39:01","Link":"http://stats.stackexchange.com/questions/225682/confusion-matrix-measure","Creator_reputation":109}
{"_id":{"$oid":"5837a58da05283111e4d7927"},"View_count":65,"Display_name":"Andrew Moylan","Question_score":3,"Question_content":"Suppose I have some samples for  and I fit two linear least squares models:  and .The two lines of best fit found in this way will generally be different.It seems, however, that the  for these two models will always be the same. I couldn't see why this should be from the definition of . Is it obvious for some reason?","Creater_id":124539,"Start_date":"2016-07-26 03:19:22","Question_id":225673,"Tags":["least-squares","r-squared"],"Answer_count":2,"Last_activity":"2016-07-26 04:26:29","Link":"http://stats.stackexchange.com/questions/225673/why-is-the-r2-for-fitting-yx-the-same-as-for-fitting-xy","Creator_reputation":116}
{"_id":{"$oid":"5837a58da05283111e4d7929"},"View_count":21,"Display_name":"rowdywalrus","Question_score":0,"Question_content":"I'm trying to model auction participation for a single individual. It occurs to me that their participation is binary (yes/no), but also continuous in the 'yes' case. Any general pointers for how to correctly parameterize both the binary and continuous components of their participation?EDIT: Okay, let me try to frame my question a little better. So as I said, I'm trying to predict auction participation of an individual, based on a number of characteristics of the auction and the individual. To that end, I am trying to figure out the correct functional form for the model I need to use. I realize the response is likely to be zero (they choose not to bid), with a relatively high probability. However, if they do bid, they will be bidding a lot of money. So I would need the predictors to map to zero with some probability p or a to a range of numbers with probability 1-p. So, as an example, one configuration of inputs would output 0, while another would output a number between 10,000 and 100,000. Hope this helps clear up my question. As I'm sure it's evident, I'm pretty new to this! Thanks!","Creater_id":123215,"Start_date":"2016-07-25 13:50:39","Question_id":225596,"Tags":["regression","logistic","binary-data","indicator-variables"],"Answer_count":0,"Last_activity":"2016-07-26 04:19:06","Link":"http://stats.stackexchange.com/questions/225596/modeling-an-indicator-variable-with-continuous-properties","Creator_reputation":23}
{"_id":{"$oid":"5837a58da05283111e4d792b"},"View_count":17,"Display_name":"KSoliman","Question_score":0,"Question_content":"I am trying to convert my variance-covariance computation, which is based on daily returns of 700 points of data, into a different scale/time period. I am trying to basically have the computed result be representative of a quarterly number.I would really appreciate any help. Thank you!","Creater_id":124542,"Start_date":"2016-07-26 03:54:31","Question_id":225678,"Tags":["matrix","covariance-matrix"],"Answer_count":0,"Last_activity":"2016-07-26 03:54:31","Link":"http://stats.stackexchange.com/questions/225678/scaling-variance-covariance-matrix-of-daily-returns-to-quarterly","Creator_reputation":1}
{"_id":{"$oid":"5837a58da05283111e4d792d"},"View_count":23,"Display_name":"Darkkey","Question_score":0,"Question_content":"I have data that is split into three classes (A, B and noise). The data amount is around 10000 samples, and A and B is only less than 5-10% of data. What is the best approach to handle this situation and what ML methods are most suitable for it?The misclassification of noise (treating noise as A or B) and treating A as B (or vice versa) is more important to avoid. Treating A or B as noise is not so important error unless we have some correct A and classifications.In this case, I'm also thinking of making multistage system (first filter noise using binary classification) and then classify data into A and B classes. Will this approach be more useful? ","Creater_id":123759,"Start_date":"2016-07-19 02:02:44","Question_id":224475,"Tags":["classification","unbalanced-classes","multi-class"],"Answer_count":1,"Last_activity":"2016-07-26 03:49:20","Link":"http://stats.stackexchange.com/questions/224475/suggestions-on-choosing-correct-method-for-multi-class-classification-of-imbalan","Creator_reputation":3}
{"_id":{"$oid":"5837a58da05283111e4d792f"},"View_count":27,"Display_name":"Guido167","Question_score":0,"Question_content":"So I am using a zero-inflated model to (1) model the presence/absence of an animal over certain habitat characteristics using a binomial distribution (2) model the count data over the same characteristics using a Poisson distribution.In the model I have interactions terms, which seem to cause a problem.The model summary when run without interactions looks like this:cr_f1 = formula(cr ~ depth + habtype2 + quarter + hurseason + year + lightregime)cr_zip1 = zeroinfl(cr_f1, dist = \"poisson\", link = \"logit\", data = allUVCdata)summary(cr_zip1)Call:zeroinfl(formula = cr_f1, data = allUVCdata, dist = \"poisson\", link = \"logit\")Pearson residuals:    Min      1Q  Median      3Q     Max -1.1635 -0.5602 -0.2665  0.1853 13.5072 Count model coefficients (poisson with log link):                   Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)       -1.550410   0.547966  -2.829 0.004664 ** depth              0.009839   0.002015   4.882 1.05e-06 ***habtype2Pinnacles  0.722851   0.186170   3.883 0.000103 ***habtype2Unexposed  0.673974   0.181744   3.708 0.000209 ***quarter2          -0.091091   0.084284  -1.081 0.279803    quarter3          -0.174710   0.105632  -1.654 0.098139 .  quarter4          -0.152594   0.091621  -1.665 0.095815 .  hurseasonY         0.195164   0.069169   2.822 0.004779 ** year2013           0.072892   0.104984   0.694 0.487483    year2014           0.077301   0.102959   0.751 0.452775    year2015          -0.141339   0.112293  -1.259 0.208153    lightregimeLight   0.300129   0.507200   0.592 0.554026    Zero-inflation model coefficients (binomial with logit link):                  Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept)        6.46630    0.92640   6.980 2.95e-12 ***depth             -0.13438    0.01389  -9.672  \u0026lt; 2e-16 ***habtype2Pinnacles -3.24941    0.49217  -6.602 4.05e-11 ***habtype2Unexposed -0.49248    0.41829  -1.177 0.239048    quarter2          -0.57489    0.24885  -2.310 0.020875 *  quarter3          -0.94783    0.34292  -2.764 0.005710 ** quarter4          -1.08198    0.30459  -3.552 0.000382 ***hurseasonY        -0.17579    0.23780  -0.739 0.459754    year2013          -0.25773    0.23098  -1.116 0.264512    year2014          -1.95033    0.25047  -7.787 6.88e-15 ***year2015          -2.22317    0.34970  -6.357 2.05e-10 ***lightregimeLight  -0.28793    0.79682  -0.361 0.717842    ---Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Number of iterations in BFGS optimization: 36 Log-likelihood: -4053 on 24 DfSo now I wanted to include some interactions terms, but then I get the following error.cr_f1 = formula(cr ~ depth + habtype2 + quarter + hurseason + year + lightregime + depth*quarter + depth*hurseason + depth*year + depth*lightregime + habtype2*quarter + habtype2*hurseason + habtype2*lightregime + habtype2*year + hurseason*year + hurseason*lightregime + year*lightregime)cr_zip1 = zeroinfl(cr_f1, dist = \"poisson\", link = \"logit\", data = allUVCdata)Error in solve.default(as.matrix(fit$hessian)) :   system is computationally singular: reciprocal condition number = 8.39796e-38I get the same error using a negative binomial distribution. A search here on the forum and on Google hinted me that the error has something to do with singularities, and given the fact that the error only occurs with the interaction terms in the model, I guess that one (or more) interactions are the cause. However, I cannot find a suitable solution for the problem. All variables except 'depth' are categorical, but I don't know how to test if an interaction causes singularities. What is the best way to deal with this?","Creater_id":101294,"Start_date":"2016-07-26 03:45:04","Question_id":225675,"Tags":["r","multiple-regression","generalized-linear-model","multicollinearity","zero-inflation"],"Answer_count":0,"Last_activity":"2016-07-26 03:45:04","Link":"http://stats.stackexchange.com/questions/225675/zero-inflated-glm-and-singularities","Creator_reputation":30}
{"_id":{"$oid":"5837a58da05283111e4d7931"},"View_count":71,"Display_name":"Kaja","Question_score":1,"Question_content":"I know such a problem is explained many times, but I have still a problem with the concept and interpretation:I would like to estimate export weight for 2016The red point is estimated pointred lines are  prediction intervalblue lines are confidence intervalAs I understand the actual export weight for 2016 is between the red lines with probability 0.95 (95% prediction interval)and the parameter of fitted model: (here  and )\\mathit{Y}=\\beta_0+\\beta_1X_1+\\varepsilonare between both blue lines confidence interval. That means possible green lines are between both blue lines (confidence interval)My Question:If all possible green lines are between confidence interval, then there is not possible to have the estimated point outside the confidence interval. But we have determined, it is possible to have an estimated point between red line or prediction interval.How can I interpret it correctly","Creater_id":35437,"Start_date":"2016-07-26 01:35:27","Question_id":225652,"Tags":["regression","confidence-interval","predictive-models","prediction-interval"],"Answer_count":1,"Last_activity":"2016-07-26 03:05:53","Link":"http://stats.stackexchange.com/questions/225652/prediction-interval-vs-confidence-interval-in-linear-regression-analysis","Creator_reputation":125}
{"_id":{"$oid":"5837a58da05283111e4d7933"},"View_count":292,"Display_name":"mscnvrsy","Question_score":4,"Question_content":"I am using Gibbs sampling in the MCMC estimation of a stochastic volatility model. One of the posterior distributions is an Inverse Gamma distribution.I was struggling with the sampling procedure or to be precise with the link to the Gamma distribution. Is it true that to get , I have to sample  and take ? This is also inline with posts on mathworks.However, I also read it up on wolfram and in my opinion they contradict themselves in the article (https://reference.wolfram.com/language/ref/InverseGammaDistribution.html).From the paragraph Background\u0026amp;Context (which deals with the generalized gamma distribution though):vs. the description from Details (specifically the fourth point):Does this discrepancy arise due to the difference between the inverse gamma and the generalized inverse gamma?","Creater_id":123353,"Start_date":"2016-07-20 06:00:26","Question_id":224714,"Tags":["distributions","sampling","gibbs","inverse-gamma"],"Answer_count":1,"Last_activity":"2016-07-26 02:57:02","Link":"http://stats.stackexchange.com/questions/224714/sampling-from-an-inverse-gamma-distribution","Creator_reputation":100}
{"_id":{"$oid":"5837a58da05283111e4d7935"},"View_count":117,"Display_name":"Davide","Question_score":1,"Question_content":"I have a dataset in the following form: \u0026lt;id\u0026gt; \u0026lt;city\u0026gt; \u0026lt;treated\u0026gt; \u0026lt;time\u0026gt; \u0026lt;after\u0026gt;where id identifies the individuals in my panel, city is the location where the individual live (non-time varying), treated is a dummy indicating those individual that are eventually treated (0: non-treated, 1: treated), time is a year-month variable, and after is a dummy (0: before, 1: after) indicating the period in which the treated unit are under treatment.Withi this data I am running a DD specification:  y = i.treated##i.after + i.timeNow, as is common practice I want to add treatment specific time trends (this should relax the parallel trend assumption), so I run:  y = i.treated##i.after + i.time + c.treated#c.timeTo make the model even more flexible I want to introduce city-treatment specific linear trends:  y = i.treated##i.after + i.time + i.city#c.treated#c.timeMy question is, does it make sense to include city-treatment trends? Results are quite different using treatment specific or city-treatment specific trends.Moreover, can someone explain to me what's the statistical difference between adding trends using: i.city#c.treated#c.timeor:  i.city#i.treated#c.timeMy understanding is that the second approach creates two trends for every city , one for treated and one for untreated units (which I think it is what I want), while the first approach creates a trend for every city-treated group.    If someone can help me understand the statistical difference between the two approaches, or suggest some papers to read, it would be really useful.Thanks a lot!EDIT: sorry but I was wrong, I didn't figure out the statistical difference  between adding i.city#c.treated#c.time or i.city#i.treated#c.time, so if anyone knows what is the correct way to add city-treatment specific linear trends, this would be very helpful -- thanks","Creater_id":124287,"Start_date":"2016-07-24 19:06:08","Question_id":225429,"Tags":["difference-in-difference"],"Answer_count":0,"Last_activity":"2016-07-26 02:55:45","Link":"http://stats.stackexchange.com/questions/225429/treatment-specific-trends-in-a-difference-in-differences-regression","Creator_reputation":20}
{"_id":{"$oid":"5837a58da05283111e4d7937"},"View_count":25,"Display_name":"Pieter","Question_score":0,"Question_content":"I'm building a classifier for text analysis sentiment. I have a large training set for positive, neutral and negative mentions. Should the training data sets be similar in size? Currently my training set for 'neutral' is about 10x larger than the set for 'positive' and 'neutral'. On a side note - when the classifier encounters a new word that doesn't occur in either of the 3 training sets, how do I assign a probability to the word that takes into account the different training set sizes? ","Creater_id":124517,"Start_date":"2016-07-25 22:37:19","Question_id":225642,"Tags":["machine-learning","classification","natural-language","naive-bayes","sentiment-analysis"],"Answer_count":1,"Last_activity":"2016-07-26 02:33:07","Link":"http://stats.stackexchange.com/questions/225642/classification-training-sets-different-sizes","Creator_reputation":101}
{"_id":{"$oid":"5837a58da05283111e4d7939"},"View_count":10,"Display_name":"user249613","Question_score":1,"Question_content":"Are there any ways to simplify this multiple integral?\\hat{f}\\left(\\left.y\\right|\\alpha\\right)=\\int_{-\\infty}^{\\infty}\\cdots\\int_{-\\infty}^{\\infty}\\hat{f}\\left(\\left.y\\right|\\theta_{1}\\right)\\hat{h}_{1}\\left(\\left.\\theta_{1}\\right|\\theta_{2}\\right)\\cdots\\hat{h}_{K}\\left(\\left.\\theta_{K}\\right|\\alpha\\right)d\\theta_{1}\\cdots d\\theta_{K}Here, the density function  depends on parameter  which is unknown and is governed by another density function,  with hyper-parameter  which could again be governed by another density  with hyper-parameter  and so no until we have density  with hyper-parameter  and . The compound density function  is then given by the above expression. Two cases arise, depending on , the number of levels of hyper-parameters we have.1)Case One is finite2)Case Two,  tends to infinityPlease point out any references / relevant material and state any assumptions that can help with the simplification as well.Thanks in Advance,Post on Mathematics Forum: http://math.stackexchange.com/questions/1862990/simplifying-multiple-integral-for-compound-probability-density-function","Creater_id":80273,"Start_date":"2016-07-26 02:28:09","Question_id":225663,"Tags":["distributions","conditional-probability","pdf","integral"],"Answer_count":0,"Last_activity":"2016-07-26 02:28:09","Link":"http://stats.stackexchange.com/questions/225663/simplifying-multiple-integral-for-compound-probability-density-function","Creator_reputation":161}
{"_id":{"$oid":"5837a58da05283111e4d793b"},"View_count":88,"Display_name":"Kartheek Palepu","Question_score":2,"Question_content":"I have the code below where a simple rule based classification data set is formed:# # Data preparationdata = data.frame(A = round(runif(100)), B = round(runif(100)), C = round(runif(100)))# Y - is the classification output columndataA == 1 \u0026amp; dataC == 0), 1, ifelse((dataB == 1 \u0026amp; dataA == 0 \u0026amp; dataC == 0), 1, 0)))# Shuffling the data setdata = data[sample(rownames(data)), ]I have divided the data set into training and testing so that I can validate my results on the test set. Once it is done I have tried building a simple neural-net with the number of neurons in hidden layer is chosen by looping (as mentioned here)i.e. N_h = \\frac{N_s} {(alpha * (N_i + N_o))}where, = number of input neurons. = number of output neurons. = number of samples in training data set. = an arbitrary scaling factor usually 2-10. Code attached here - It was giving poor over fitted results. But, when I have built a simple random forest on the same data set. I am getting the train and test errors as - Please help me in understanding why neural nets are failing in a simple case where random forest is working with  Accuracy.Note: I have used only one hidden layer (assuming one hidden will be enough to solve simple classification problems) and iterated on the number of neurons in the hidden layer.Also, help me in choosing better parameters for the neural nets so that it can classify the data better.","Creater_id":85734,"Start_date":"2016-07-22 04:52:02","Question_id":225100,"Tags":["r","neural-networks"],"Answer_count":1,"Last_activity":"2016-07-26 02:24:48","Link":"http://stats.stackexchange.com/questions/225100/why-neural-network-is-failing-in-a-simple-classification-case","Creator_reputation":135}
{"_id":{"$oid":"5837a58da05283111e4d793d"},"View_count":125,"Display_name":"Rachel Kogan","Question_score":0,"Question_content":"It is my understanding that the k-fold cross-validation estimate of test error usually underestimates actual test error.  I’m confused why this is the case.  I see why the training error is usually lower than the test error - because you are training the model on the very same data that you are estimating the error on! But that isn’t the case for cross-validation - the fold that you measure error on is specifically left out during the training process.Also, Is it correct to say that cross-validation estimate of test error is biased downward?","Creater_id":124470,"Start_date":"2016-07-25 11:34:51","Question_id":225573,"Tags":["cross-validation","bias"],"Answer_count":2,"Last_activity":"2016-07-26 02:17:10","Link":"http://stats.stackexchange.com/questions/225573/why-does-the-cv-estimate-of-test-error-underestimate-actual-test-error","Creator_reputation":1}
{"_id":{"$oid":"5837a58da05283111e4d793f"},"View_count":20,"Display_name":"mahsa.nst","Question_score":1,"Question_content":"I'm trying to do zero trick but i got stuck. I tried the following model:modeldGC.model \u0026lt;- function(){C \u0026lt;- 10000for(i in 1:(2*m)){    zeros[i] ~ dpois(zeros.mean[i])    zeros.mean[i] \u0026lt;- -l[i]    #log-likelihood               gamma[i] \u0026lt;- exp(inprod(X[i,],coef)+u[f[i]]+e[i])              u[f[i]] ~ dnorm(0,tau.u)              e[i]~ dnorm(0,tau.e)    l[i] \u0026lt;- log(pgamma(alphagamma[i], alphaY[i],1)-                        pgamma(alphagamma[i], alphaY[i]+alpha, 1))    #log-link + linear predictor    }priors      for(t in 1:p){        coef[t] ~ dnorm(0,0.001)                   }# end of loop for t       log.u.Sig ~ dnorm(0,0.001)      u.Sig \u0026lt;- exp(log.u.Sig)      tau.u \u0026lt;- 1/pow(u.Sig,2)      log.e.Sig ~ dnorm(0,0.001)      e.Sig \u0026lt;- exp(log.e.Sig)      tau.e \u0026lt;- 1/pow(e.Sig,2)      alpha ~ dgamma(0.001,0.001)}ESTIMATION OF PARAMETERS by JAGS####dat \u0026lt;- list(Y=Y, X=X, m=m, p=p, f=f)    # list of essential information for applying jags modelmod \u0026lt;- jags.fit(dat, c(\"coef\", \"u.Sig\", \"e.Sig\",\"alpha\"), dGC.model, n.iter = 1000)but I got message from jags:Attempt to redefine node u[1]best regards.","Creater_id":124533,"Start_date":"2016-07-26 02:16:27","Question_id":225660,"Tags":["generalized-linear-model"],"Answer_count":0,"Last_activity":"2016-07-26 02:16:27","Link":"http://stats.stackexchange.com/questions/225660/zero-trick-in-jags-for-glmm","Creator_reputation":6}
{"_id":{"$oid":"5837a58ea05283111e4d7941"},"View_count":25,"Display_name":"mimok","Question_score":1,"Question_content":"I am developing a prognostic index using the LASSO technique and wondering how to deal with the highly correlated predictor variables. Should I choose the ones I want to include in the LASSO a priori or is the LASSO programmed to select the best/better of highly correlated predictors variables?thanks.","Creater_id":124532,"Start_date":"2016-07-26 02:02:59","Question_id":225659,"Tags":["correlation","multiple-regression","predictive-models","lasso"],"Answer_count":0,"Last_activity":"2016-07-26 02:02:59","Link":"http://stats.stackexchange.com/questions/225659/dealing-with-correlated-predictors-when-using-lasso","Creator_reputation":6}
{"_id":{"$oid":"5837a58ea05283111e4d7943"},"View_count":47,"Display_name":"Drag0","Question_score":1,"Question_content":"I'm creating ensemble of neural networks for a simple binary classification task. Every neural network is generated and trained a bit differently (number of hidden layers, number of neurons per layer, learning rate etc.).   My question is, should I train these weaker networks and optimize each  of them as much as possible or should I just use them as soon they  reach \"good enough\" classification?My \"good enough\" classification is around 80% since I can get it pretty fast without too much hassle of parameter optimization, but I can also reach 92-93% with some parameter tuning and it takes me 15-30 minutes to do it. What did you discover works better in general?","Creater_id":116838,"Start_date":"2016-07-25 09:24:00","Question_id":225550,"Tags":["machine-learning","neural-networks","ensemble"],"Answer_count":1,"Last_activity":"2016-07-26 02:02:45","Link":"http://stats.stackexchange.com/questions/225550/should-i-optimize-neural-networks-that-are-part-of-ensemble-of-neural-networks","Creator_reputation":155}
{"_id":{"$oid":"5837a58ea05283111e4d7945"},"View_count":780,"Display_name":"tomas","Question_score":2,"Question_content":"Just want to check that I am performing my cross validation procedures right. I'm using a non-linear svm. I do a five fold cross validation (5 splits of test/train on my original training data) and for each fold, run a grid search to find the optimal parameters for that train/test pair (e.g. best parameters where model is fitted on the training data then evaluated on the test data). After five iterations, for each parameter set (2 hyper-parameters), i have 5 fit measures. I just use the average of those 5 #s and find the parameter set with the best average. Does this sound correct? I'm pretty new to this so am not entirely aware of what other methods are there.Additionally, was wondering a few more things:1) Any way to make it faster? Cross validation + grid search is pretty computationally intensive.2) Any other validation methods rather than k-fold (stratified I might add) cv? My data are timeseries so I was considering a moving window type validation, but wasn't sure. Any thoughts welcome.I should add that I'm asking because my training fit (the average over the 5 cv folds) is still much better than my actual test data fit. I'm trying to figure out what might be causing this and the best way to reduce this difference. I realize increasing the # of folds may help though it also raises question 1) as well.Thanks guys and thumbs up on the site. Great info.","Creater_id":6434,"Start_date":"2012-02-09 15:15:27","Question_id":22552,"Tags":["machine-learning","predictive-models","cross-validation","svm"],"Answer_count":2,"Last_activity":"2016-07-26 01:45:08","Link":"http://stats.stackexchange.com/questions/22552/cross-validation-procedure-is-this-right","Creator_reputation":657}
{"_id":{"$oid":"5837a58ea05283111e4d7947"},"View_count":32,"Display_name":"DeltaIV","Question_score":4,"Question_content":"In a comment to this question, user @whuber cited the possibility of using a periodic version of splines to fit periodic data. I would like to know more about this method, in particular the equations defining the splines, and how to implement them in practice (I'm mostly an R user, but I can make do with MATLAB or Python, if the need arises). Also, but this is a \"nice to have\", it would be great to know about possible advantages/disadvantages with respect to  trigonometric polynomials fitting, which is how I usually deal with these kind of data (unless the response is not very smooth, in which case I switch to Gaussian Process with periodic kernel). ","Creater_id":58675,"Start_date":"2016-07-26 01:37:08","Question_id":225653,"Tags":["regression","time-series","seasonality","splines"],"Answer_count":0,"Last_activity":"2016-07-26 01:37:08","Link":"http://stats.stackexchange.com/questions/225653/periodic-splines-to-fit-periodic-data","Creator_reputation":1296}
{"_id":{"$oid":"5837a58ea05283111e4d7949"},"View_count":13,"Display_name":"EEEEEric","Question_score":1,"Question_content":"I want to predict a specific time interval(ex. patient processing time in clnic) with some boolean value like whether this patient has cough or whether he/she has certain disease. I have tried using linear regression but the accuracy, about 0.01, of prediction is poor when using boolean value like 1 or 0 as parameter. Afterwards I add previous time interval as another parameter, the accuracy improved to 0.1 but still quite inaccurate.   I wondered if there are any other linear algorithm which can be used to predict for such question. Or I should used other non-linear one? Or I should see this question as a time serious prediction although it seems that there's no specific relation between the result and the time at given point.  Besides, I also see some posts said that using machine learning such RNN may not get better result while compared with some traditional statistic models.","Creater_id":124527,"Start_date":"2016-07-26 01:18:24","Question_id":225650,"Tags":["machine-learning","time-series","prediction","analysis"],"Answer_count":0,"Last_activity":"2016-07-26 01:18:24","Link":"http://stats.stackexchange.com/questions/225650/time-interval-prediction","Creator_reputation":6}
{"_id":{"$oid":"5837a58ea05283111e4d794b"},"View_count":78,"Display_name":"Daniel Pinto","Question_score":4,"Question_content":"I have read in many textbooks that as the ratio between the number of coefficients to be estimated in a VAR and the number of time periods increases above a certain threshold, estimation becomes unreliable due to a loss of degrees of freedom. However, in no textbook was I able to find information regarding what that threshold is. Could someone provide some insight into this or recommend literature?Suppose I have 4 variables and 3 lags. I then have 4+4*3=16 coefficients. My question is, how many observations should the model have so as to ensure that lacks of degree of freedom is not a problem. Would 10 times the number of coefficients be fine? Or is 5 times enough? That is what I mean by threshold. Clearly, 32 seems not enough from an intuitive standpoint. But I assume there must be some theoretical work, maybe based on simulations, that can give a more accurate (and less intuitive) answer.","Creater_id":99392,"Start_date":"2016-07-24 11:53:37","Question_id":225383,"Tags":["time-series","sample-size","standard-error","var","degrees-of-freedom"],"Answer_count":1,"Last_activity":"2016-07-26 01:05:55","Link":"http://stats.stackexchange.com/questions/225383/required-sample-size-and-degrees-of-freedom-for-a-var","Creator_reputation":44}
{"_id":{"$oid":"5837a58ea05283111e4d794d"},"View_count":30,"Display_name":"Tina","Question_score":0,"Question_content":"I have data from my experiment where two groups participated (monolinguals vs. bilinguals).There was a language switching as well as a task switching part to the experiment where the data tells how fast the participants responded as well as whether or not their answer was correct (correct vs. incorrect).Do you know good statistical methods for analyzing the data? I am trying to see whether bilinguals perform better in task switching than monolinguals.I have looked at methods including: Two-way repeated measures ANOVA, IRT and some others but I don't feel like they apply to my case and I am therefore seeking inspiration.","Creater_id":124524,"Start_date":"2016-07-26 00:33:11","Question_id":225647,"Tags":["mathematical-statistics","dataset","standard-deviation","accuracy"],"Answer_count":1,"Last_activity":"2016-07-26 01:00:20","Link":"http://stats.stackexchange.com/questions/225647/statistical-methods","Creator_reputation":3}
{"_id":{"$oid":"5837a58ea05283111e4d794f"},"View_count":96,"Display_name":"Paulo","Question_score":1,"Question_content":"I am trying to perform a logistic regression with the following code   Y ~ x1+x2+x3,data=data, family=binomial(link=\"logit\"). However on inspection of both the outcome and predictors i noticed that they are characterized by spatial auto-correlation. My question is, how do I account for the spatial auto-correlation, to get better coefficients? ","Creater_id":110605,"Start_date":"2016-07-25 23:48:51","Question_id":225645,"Tags":["r","logistic","gis","spatial-interaction-model"],"Answer_count":0,"Last_activity":"2016-07-26 00:14:43","Link":"http://stats.stackexchange.com/questions/225645/performing-spatial-logistic-regression-in-r","Creator_reputation":6}
{"_id":{"$oid":"5837a58ea05283111e4d7951"},"View_count":12,"Display_name":"Shiyue","Question_score":0,"Question_content":"I'm training a 5-layer convolutional neural network on about 300 images with size . We can say that the number of features before the first layer is . So it seems like I don't have enough data to train on so many features. But in my model I actually use  kernels,  filters on each layer. Intuition tells me that every time my model is learning within the filter, so I don't actually need more data points. Is this true? I'm not clear how different parameters come into play in CNN. So thank you for explanations!","Creater_id":122966,"Start_date":"2016-07-25 23:42:09","Question_id":225644,"Tags":["machine-learning","conv-neural-network"],"Answer_count":0,"Last_activity":"2016-07-25 23:42:09","Link":"http://stats.stackexchange.com/questions/225644/if-number-of-examples-is-smaller-than-number-of-features-but-greater-than-size","Creator_reputation":101}
{"_id":{"$oid":"5837a58ea05283111e4d7953"},"View_count":13044,"Display_name":"pitosalas","Question_score":75,"Question_content":"As the election is a one time event, it is not an experiment that can be repeated. So exactly what does the statement \"Hillary has a 75% chance of winning\" technically mean? I am seeking a statistically correct definition not an intuitive or conceptual one.I am an amateur statistics fan who is trying to respond to this question that came up in a discussion. I am pretty sure there's a good objective response to it but I can't come up with it myself...","Creater_id":124322,"Start_date":"2016-07-24 06:59:11","Question_id":225353,"Tags":["probability","prediction","politics"],"Answer_count":9,"Last_activity":"2016-07-25 22:05:31","Link":"http://stats.stackexchange.com/questions/225353/probability-of-a-single-real-life-future-event-what-does-it-mean-when-they-say","Creator_reputation":478}
{"_id":{"$oid":"5837a58ea05283111e4d7955"},"View_count":11,"Display_name":"llewmills","Question_score":0,"Question_content":"I am following the section in Chapter 4 of Singer and Willett's Applied Longitudinal Data Analysis about deviance-based hypothesis tests, trying to apply it to my own design.I am attempting to test the fixed effect, over time, of three different sets of instructions (variable INSTRUCTION with three levels: TRUTH, MISINFORMATION, NOINFORMATION) on the outcome variable. There are four measurement occasions for each participant. There are also several covariates. The first level of the submodel isI am a little confused how to express the level-two models with three levels of the grouping variable (Singer and Willett only use two-level predictors).This is important for performing deviance-based hypothesis tests statistics. If there was only two levels of the fixed effect INSTRUCTION and the reference level was TRUTH then the level-two submodels would be quite straightforwardI wish to compare the deviance statistic of an the controlled growth model with INSTRUCTION to an unconditional growth model (where there is no influence of instruction) then this involves constraining two parameters,  and  to 0. The deviance statistics of these two models can be compared by calculating the difference in deviance statistics and determining if this difference exceeds a critical value on the chi-squared distribution with 2 degrees of freedom.How many degrees of freedom would I use for the same model comparison except with my three-level INSTRUCTION variable instead of the two-level? Would it be four because in addition to constraining the effect of MISINFORMATION on initial value () and rate of change () I would also be constraining the effect of NOINFORMATION (i.e.  and )?Or would it still be two degrees of freedom because these are just different levels of the INSTRUCTION variable? p.s. apologies for my confusion over subscripts. They reflect my confusion about how to express the model.  ","Creater_id":79732,"Start_date":"2016-07-25 21:49:24","Question_id":225641,"Tags":["chi-squared","panel-data","deviance"],"Answer_count":0,"Last_activity":"2016-07-25 21:49:24","Link":"http://stats.stackexchange.com/questions/225641/determining-degrees-of-freedom-for-deviance-based-hypothesis-tests","Creator_reputation":120}
{"_id":{"$oid":"5837a58ea05283111e4d7957"},"View_count":16,"Display_name":"Myggen--","Question_score":1,"Question_content":"Forecasting seasonal components is an important practical problem in finance, where products that are highly exposed to monthly seasonality in consumer prices are traded. For example, one can trade swaps based on a monthly (unknown) index value of the European Harmonised Index of Consumer Prices (HICP). The state of the art method for seasonally adjusting data seems to be X-13ARIMA-SEATS, which is produced by the US Census Bureau.However, this software seems to have been set up mainly to adjust the data historically for seasonality and other deterministic effects and forecast the entire series into the future. In finance, forecasts of trend values (for consumer price indices, for example) can generally can be observed in the market, but the seasonal component must be forecasted. Any tips or comments on whether one can extract forecasts of the seasonal components within X-13ARIMA-SEATS would be greatly appreciated","Creater_id":49061,"Start_date":"2016-07-25 21:34:31","Question_id":225639,"Tags":["forecasting","arima"],"Answer_count":0,"Last_activity":"2016-07-25 21:34:31","Link":"http://stats.stackexchange.com/questions/225639/forecasting-seasonal-components-in-x-13arima-seats","Creator_reputation":6}
{"_id":{"$oid":"5837a58ea05283111e4d7959"},"View_count":3717,"Display_name":"Garrith Graham","Question_score":33,"Question_content":"To demonstrate the concept of Naïve Bayes Classification, consider the example displayed in the illustration above. As indicated, the objects can be classified as either GREEN or RED. My task is to classify new cases as they arrive, i.e., decide to which class label they belong, based on the currently exiting objects.Since there are twice as many GREEN objects as RED, it is reasonable to believe that a new case (which hasn't been observed yet) is twice as likely to have membership GREEN rather than RED. In the Bayesian analysis, this belief is known as the prior probability. Prior probabilities are based on previous experience, in this case the percentage of GREEN and RED objects, and often used to predict outcomes before they actually happen.Thus, we can write:Since there is a total of 60 objects, 40 of which are GREEN and 20 RED, our prior probabilities for class membership are:Having formulated our prior probability, we are now ready to classify a new object (WHITE circle). Since the objects are well clustered, it is reasonable to assume that the more GREEN (or RED) objects in the vicinity of X, the more likely that the new cases belong to that particular color. To measure this likelihood, we draw a circle around X which encompasses a number (to be chosen a priori) of points irrespective of their class labels. Then we calculate the number of points in the circle belonging to each class label. From this we calculate the likelihood:From the illustration above, it is clear that Likelihood of X given GREEN is smaller than Likelihood of X given RED, since the circle encompasses 1 GREEN object and 3 RED ones. Thus:Although the prior probabilities indicate that X may belong to GREEN (given that there are twice as many GREEN compared to RED) the likelihood indicates otherwise; that the class membership of X is RED (given that there are more RED objects in the vicinity of X than GREEN). In the Bayesian analysis, the final classification is produced by combining both sources of information, i.e., the prior and the likelihood, to form a posterior probability using the so-called Bayes' rule (named after Rev. Thomas Bayes 1702-1761).Finally, we classify X as RED since its class membership achieves the largest posterior probability.This is where the difficulty of my maths understanding comes in. p(Cj | x1,x2,x...,xd) is the posterior probability of class membership, i.e., the probability that X belongs to Cj but why write it like this? Calculating the likelihood?Posterior Probability?I never took math, but my understanding of naive bayes is fine I think just when it comes to these decomposed methods confuses me. Could some one help with visualizing these methods and how to write the math out in an understandable way?","Creater_id":6875,"Start_date":"2012-01-27 10:29:14","Question_id":21822,"Tags":["machine-learning","naive-bayes"],"Answer_count":3,"Last_activity":"2016-07-25 21:08:17","Link":"http://stats.stackexchange.com/questions/21822/understanding-naive-bayes","Creator_reputation":376}
{"_id":{"$oid":"5837a58ea05283111e4d795b"},"View_count":21,"Display_name":"crippledlambda","Question_score":0,"Question_content":"I know the identity that matrix  is a product of  and  (all of which are integer matrices in my case):\\begin{equation}\\mathbf{Y} = \\mathbf{X} \\mathbf{B}\\end{equation}With knowledge of , , , I would like to find a matrix  from which I can obtain  from :\\begin{equation}\\mathbf{Y}\\mathbf{P} = \\mathbf{X}\\end{equation}I compute  in one of two ways: 1) calculate the pseudoinverse  and 2) solve  for .My questions:Under what conditions does the pseudo-inverse fail? Under what conditions does the \"solve\" method fail?By \"fail,\" I mean that  does not provide an accurate reconstruction of .Is it related to the extent of overfitting? I would like to know if I can anticipate failures from knowledge of the matrix properties. There is this post  suggesting the \"solve\" method is always better for numerical reasons. However, my main objective is interpreting the elements of  and not speed. Conceptually, the pseudo-inverse only depends on knowledge of  -- and therefore the elements of  would have fixed interpretations for many such  and  -- so for this reason it is desirable for my problem.I provide two examples in R below.Example 1. Pseudo-inverse fails but solve works is 33 and  is 32. Consequently,  is 32.X \u0026lt;- rbind(c(1, 0, 1),           c(1, 1, 1),           c(1, 2, 1))B \u0026lt;- rbind(c(3, 0),           c(2, 0),           c(2, 1))Y \u0026lt;- X %*% BP.inv \u0026lt;- MASS::ginv(B)P.solve \u0026lt;- limSolve::Solve(Y, X)par(mfrow=c(1,2))matplot(X, Y %*% P.inv, xlim=range(X), ylim=range(X))abline(0, 1)matplot(X, Y %*% P.solve, xlim=range(X), ylim=range(X))abline(0, 1)The following figure shows the reconstruction of  from  by the pseudo-inverse method (left) and \"solve\" method (right).Example 2. Pseudo-inverse and solve both fail is 55 and  is 52. Consequently,  is 52.X \u0026lt;- rbind(c(2, 2, 0, 0, 0),           c(1, 2, 1, 0, 0),           c(2, 1, 0, 1, 0),           c(2, 1, 1, 0, 0),           c(3, 0, 0, 0, 1))B \u0026lt;- rbind(c(3, 0),           c(2, 0),           c(2, 1),           c(1, 1),           c(0, 1))Y \u0026lt;- X %*% BP.inv \u0026lt;- MASS::ginv(B)P.solve \u0026lt;- limSolve::Solve(Y, X)par(mfrow=c(1,2))matplot(X, Y %*% P.inv, xlim=range(X), ylim=range(X))abline(0, 1)matplot(X, Y %*% P.solve, xlim=range(X), ylim=range(X))abline(0, 1)The following figure shows the reconstruction of  from  by the pseudo-inverse method (left) and \"solve\" method (right).","Creater_id":1021,"Start_date":"2016-07-25 19:39:50","Question_id":225630,"Tags":["regression","matrix-inverse"],"Answer_count":0,"Last_activity":"2016-07-25 21:03:30","Link":"http://stats.stackexchange.com/questions/225630/how-to-anticipate-bad-solutions-to-system-of-equations","Creator_reputation":233}
{"_id":{"$oid":"5837a58ea05283111e4d795d"},"View_count":123,"Display_name":"Md. Abid Hasan","Question_score":2,"Question_content":"I am trying to fit a SVM to my data. My dataset contains 3 classes and I am performing 10 fold cross validation (in LibSVM): ./svm-train -g 0.5 -c 10 -e 0.1 -v 10 training_dataThe help thereby states:-c cost : set the parameter C of C-SVC, epsilon-SVR, and nu-SVR (default 1)For me, providing higher cost (C) values gives me higher accuracy. What does C in SVM actually mean? Why and when should I use higher/lower values (or the LibSVM given default value) of C? ","Creater_id":122171,"Start_date":"2016-07-24 15:08:34","Question_id":225409,"Tags":["cross-validation","svm","libsvm","parameterization","parameter-optimization"],"Answer_count":1,"Last_activity":"2016-07-25 20:53:12","Link":"http://stats.stackexchange.com/questions/225409/what-does-the-cost-c-parameter-mean-in-svm","Creator_reputation":13}
{"_id":{"$oid":"5837a58ea05283111e4d795f"},"View_count":145,"Display_name":"Brianna","Question_score":0,"Question_content":"I am trying to perform a multivariate multiple regression in SPSS. I have 5 independent variables and 4 dependent variables. I know it can be done in SPSS using the GLM-multivariate option. My questions are:What are the assumptions for a multivariate multiple regression?How do I interpret the findings (how are these findings different from the findings of a regular multiple regression)?I am using this link as a reference (however, this is MPlus and I need the information for SPSS).Any ideas, code, or references would be greatly appreciated.","Creater_id":95284,"Start_date":"2015-11-16 11:06:52","Question_id":182048,"Tags":["regression","spss","multivariate-analysis"],"Answer_count":1,"Last_activity":"2016-07-25 20:40:00","Link":"http://stats.stackexchange.com/questions/182048/multivariate-multiple-regression-assumptions-how-to-interpret-findings-spss","Creator_reputation":1}
{"_id":{"$oid":"5837a58ea05283111e4d7961"},"View_count":47,"Display_name":"Rania Z","Question_score":0,"Question_content":"I have this sample dataset which is as follow:  head(data)2016-03-19 0.01109 0.014182016-03-20 0.00882 0.014452016-03-21 0.00953 0.009772016-03-22 0.01022 0.009982016-03-23 0.01141 0.011452016-03-24 0.00973 0.00966....As I plot the whole dataset in Excel, the trend is as following:However, in R, when I used Causal Impact, I would get a different view for the plot data, as you can see the first panel of this plot.Can anyone please help me with it? Why is there the difference between these two charts? The test starts from 5/19.2016. I think the impact between control group and test group should be neutral. However, in causal impact package, it's significantly positive.the dataset is in the google link.docs.google.com/spreadsheets/d/1xcdK04ut9EOyiRjkUhsQ3l95T1V_zrH02Qk3JBDQWvM/edit?usp=sharing","Creater_id":124323,"Start_date":"2016-07-24 07:12:08","Question_id":225356,"Tags":["r","dataset","package","causalimpact"],"Answer_count":1,"Last_activity":"2016-07-25 20:33:04","Link":"http://stats.stackexchange.com/questions/225356/the-causal-impact-package-in-r-could-not-trace-the-right-number-in-the-test-grou","Creator_reputation":8}
{"_id":{"$oid":"5837a58ea05283111e4d7963"},"View_count":17,"Display_name":"Sean M.","Question_score":0,"Question_content":"I've given a dataset of N samples like: (x,y)(-100,1), (-90, 30), (-50,60), (-10,90), (0,100), (10, 90), (30, 20),(100,4)Now I would like to determine how well the data fits a perfect gaussian (normal) distribution. For that reason, I want to fit a gaussian curve to the data and in an additional step compare the estimated gaussian with the \"perfect\" gaussian. Any ideas how to achieve this in C++, Boost or Eigen functions? I'm quite stuck here. Is this even a good approach? What I actually want is a measure of how well the data follows a gaussian distribution shape. ","Creater_id":null,"Start_date":"2016-07-21 16:01:22","Question_id":225632,"Tags":["c++"],"Answer_count":0,"Last_activity":"2016-07-25 20:05:01","Link":"http://stats.stackexchange.com/questions/225632/how-to-fit-a-gaussian-curve-to-data-for-a-goodness-of-fit-c","Creator_reputation":null}
{"_id":{"$oid":"5837a58ea05283111e4d7965"},"View_count":55,"Display_name":"jeremy radcliff","Question_score":3,"Question_content":"It seems that most people favor using t_{stat} = \\frac{\\hat{p}-p_0}{\\sqrt{p_0(1-p_0)/n}} instead of t_{stat} = \\frac{\\hat{p}-p_0}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}when doing Hypothesis Testing with sample proportions (at least so were we told in class).This doesn't make sense to me, since we use  for sample means, and especially in a case for example where we would be testing for whether or not a coin is biased.How does it make sense to use  if we're already suspecting that the coin might be biased? Why not use  the way we use  for sample means? ","Creater_id":60672,"Start_date":"2016-07-22 11:51:01","Question_id":225163,"Tags":["hypothesis-testing","proportion"],"Answer_count":3,"Last_activity":"2016-07-25 19:18:05","Link":"http://stats.stackexchange.com/questions/225163/why-do-we-use-p-0-instead-of-hatp-for-sample-proportion-hypothesis-testin","Creator_reputation":375}
{"_id":{"$oid":"5837a58ea05283111e4d7967"},"View_count":8,"Display_name":"Tarek Soukieh","Question_score":0,"Question_content":"We have a call center that dropped 10% in customer satisfaction since last month. We want to know the factors that led to the 10% decrease not the factors that affect satisfaction. How can I model this?I have each case handled in the call center and I can develop a model to tell which of the case variable affect customer satisfaction, but how can I can measure the effects on that aggregate 10% decrease, any ideas?Thank you in advance.","Creater_id":79085,"Start_date":"2016-07-25 18:16:17","Question_id":225628,"Tags":["modeling","operations-research"],"Answer_count":0,"Last_activity":"2016-07-25 18:16:17","Link":"http://stats.stackexchange.com/questions/225628/causes-of-change-in-satisfaction","Creator_reputation":27}
{"_id":{"$oid":"5837a58ea05283111e4d7969"},"View_count":92,"Display_name":"Myurathan Kajendran","Question_score":3,"Question_content":"I want to run time series regressions (Fama-French three factor with my new factor). I have following tables.Table01    Date     Port_01  Port_02 --------- Port_18    01/1965     0.85    0.97               1.86    02/1965     8.96    7.2                0.98    03/1965     8.98    7.9                8.86 Table 02    Date        Market   SMB    HML     WXO    01/1965      0.85    0.97    0.86    0.87    02/1965      8.96    7.2     0.98    0.79    03/1965      8.98     7.9    8.86    0.86Table 01 is my Y variable and Table 02 is my independent variables. I want to regress 18 portfolios on factors on Table 02. I want the intercepts to be stored in a vector (N x 1). and residuals to be stored in a matrix (T x N), check the image. I know how to run regression individually. But It there must be a better way to do. Please help me. Thanks in advance. ","Creater_id":124303,"Start_date":"2016-07-25 03:14:33","Question_id":225487,"Tags":["r","regression","computational-statistics"],"Answer_count":1,"Last_activity":"2016-07-25 18:06:23","Link":"http://stats.stackexchange.com/questions/225487/running-many-multiple-regressions-at-once-in-r","Creator_reputation":18}
{"_id":{"$oid":"5837a58ea05283111e4d796b"},"View_count":61,"Display_name":"Alex","Question_score":3,"Question_content":"I would like some clarification regarding a regression specified using Wilkinson-Rogers notation to produce coefficients for all levels in a categorical variable.Consider the regression specifed by Income ~ Sex, where Sex is a categorical variable with two levels, Male and Female. This regression will produce two coefficients, one for the intercept, and one for either the level Male or Female, as the effect of one level of Sex is automatically incorporated into the intercept.My question is what coefficients should the notation Income ~ Sex - 1 produce? This is used to exclude the intercept term. In R, which uses a 'modified form' of Wilkinson-Rogers this fits coefficients to both levels of Sex. However, in Matlab, one level is still dropped, thus only one coefficient is produced.","Creater_id":22199,"Start_date":"2016-07-25 16:52:53","Question_id":225615,"Tags":["regression","mixed-model","notation"],"Answer_count":1,"Last_activity":"2016-07-25 17:51:09","Link":"http://stats.stackexchange.com/questions/225615/levels-of-a-categorical-variable-in-wilkinson-rogers-notation-without-intercept","Creator_reputation":727}
{"_id":{"$oid":"5837a58ea05283111e4d796d"},"View_count":546,"Display_name":"LWZ","Question_score":1,"Question_content":"I'm trying to compare several sets of experiment data, by comparing means. I read there are several different tests such as Each Pair, Student’s t and All Pairs, Tukey HSD, which give different circles of different radius, an example shown below How are the circles defined? How do I calculate the radius? And is there a rule what test one should use for what kind of data?","Creater_id":20684,"Start_date":"2013-02-09 20:03:22","Question_id":49672,"Tags":["t-test","mean","tukey-hsd","jmp"],"Answer_count":2,"Last_activity":"2016-07-25 17:50:54","Link":"http://stats.stackexchange.com/questions/49672/comparing-many-means-in-jmp","Creator_reputation":106}
{"_id":{"$oid":"5837a58ea05283111e4d796f"},"View_count":103,"Display_name":"a.powell","Question_score":1,"Question_content":"I have consulted this question on the basis of prediction intervals for loess (How to calculate prediction intervals for LOESS?). However, I am unaware of whether it is proper or possible to use prediction intervals for forecasts made from loess? What problems could come from doing so, if any? How would I do so in R?Currently I have attempted to calculate forecasts by using a for loop that predicts the values for a certain number of subsequent days, but I am unaware of how to produce prediction intervals for the same set of forecasts. I would hope to adjust the forecast so that my time-series and prediction look similar to this:","Creater_id":119338,"Start_date":"2016-07-25 17:38:53","Question_id":225620,"Tags":["r","regression","prediction-interval","loess"],"Answer_count":0,"Last_activity":"2016-07-25 17:38:53","Link":"http://stats.stackexchange.com/questions/225620/prediction-interval-for-loess-forecast","Creator_reputation":538}
{"_id":{"$oid":"5837a58ea05283111e4d7971"},"View_count":52,"Display_name":"Machina333","Question_score":0,"Question_content":"I am completely new to the recurrent neural network models. I am using rnnlib to run their examples. What is the meaning of timesteps in the output ? Do timesteps mean that number of output the RNN outputs ?For example-:Output-: A,B,CWill the timesteps here be 3 since it has generated 3 outputs ?Here is a sample output from rnnlib-:training...data sequence:file = arabic_offline.ncindex = 0tag = data/pf416_008.tifinput shape = (365 129 1)timesteps = 47085target label sequence (length 8):maE laB aaE seB laA zaE naM maBoutput label sequence (length 4):naB raA haE raAweight_optimiser weight updates:minmax(wts) = -0.100045 0.100051minmax(derivs) = -7.27329 5.25175minmax(deltas) = -0.000525175 0.000727329output shape = (21 121)errors:ctcError 82.4798deletions 4insertions 0labelError 8seqError 1substitutions 4data sequence:file = arabic_offline.ncindex = 1tag = data/dj21_024.tifinput shape = (642 103 1)timesteps = 66126target label sequence (length 13):teE laM maM jaM laB aaA kaA waE seB seE naB waE taBoutput label sequence (length 2):naB raAweight_optimiser weight updates:minmax(wts) = -0.10034 0.100256minmax(derivs) = -12.2291 5.46554minmax(deltas) = -0.00101921 0.00187751output shape = (36 121)errors:ctcError 140.356deletions 11insertions 0labelError 12seqError 1substitutions 1What is the meaning of timesteps here ?","Creater_id":101559,"Start_date":"2016-03-14 05:38:04","Question_id":201585,"Tags":["rnn"],"Answer_count":0,"Last_activity":"2016-07-25 17:03:50","Link":"http://stats.stackexchange.com/questions/201585/rnn-what-are-timesteps","Creator_reputation":158}
{"_id":{"$oid":"5837a58ea05283111e4d7973"},"View_count":10,"Display_name":"lmcshane","Question_score":0,"Question_content":"Looking for resources (online resource, book, etc) that details how to analyze questionnaire data. I would like the resource to specifically deal with multiple choice questions, differences in sample size (comparing different N sizes between years), regression, and data visualization. I would like it to either be in R or unrelated in software type. ","Creater_id":107127,"Start_date":"2016-07-25 15:22:55","Question_id":225609,"Tags":["references","survey"],"Answer_count":0,"Last_activity":"2016-07-25 16:51:27","Link":"http://stats.stackexchange.com/questions/225609/questionnaire-analysis-resources","Creator_reputation":136}
{"_id":{"$oid":"5837a58ea05283111e4d7975"},"View_count":114,"Display_name":"user6571411","Question_score":3,"Question_content":"How can the magnitude of an intervention be quantified in a segmented time series regression?I am attempting to replicate the methodology of Decline in pneumonia admissions after routine childhood immunisation with pneumococcal conjugate vaccine in the USA: a time-series analysis. There are several other published papers with similar methodology where the methods chapter is not informative. All of these papers cite Wagner et al., however that paper describes a much simpler linear model. I have a 120 month time series of count data that exhibits strong seasonal variation, has a baseline negative trend and a known public health intervention at  that would be expected to decrease the counts (increase the negative trend). I have fitted this with a negative binomial model with AR(2) errors and dummy variables for months, pre-intervention trend, an intervention indicator and post-intervention trend. I did this in R statistics with the function glm.nb from the MASS package. #In R version 3.3.1 with packages dplyr, MASS#generate a comparable time-series ts() base \u0026lt;- rnbinom(n = 120, size = 1400, prob = 0.5)season \u0026lt;- rep(c(600, 400, 150, 0, -50, -80, -300, -600, 50, 100, 200, 300), 10)ts \u0026lt;- ts(base + season, start = c(2000,1), end = c(2009,12), frequency = 12)#generate the independant variablesmonth.f \u0026lt;- factor(rep(1:12, 10))dummy.months \u0026lt;- model.matrix(~month.f +0)require(dplyr); lag1 \u0026lt;- lag(ts); lag2 \u0026lt;- lag(lag(ts))time.interv \u0026lt;- 72pre.interv.trend \u0026lt;- c(1:time.interv, rep(0, 48))interv.indicator \u0026lt;- c(rep(0, time.interv), rep(1, 48))post.interv.trend \u0026lt;- c(rep(0, time.interv), 1:48)df \u0026lt;- cbind.data.frame(ts, dummy.months,lag1,lag2,interv.indicator,pre.interv.trend,post.interv.trend)#the fitted modelrequire(MASS); fit \u0026lt;- glm.nb(ts ~. + 0, data = df)I have attempted several approachesI have tried using the forecast package to forecast the pre-intervention time series and then subtract the observed values from the predicted. However, the 95%CI intervals became so large that there would have been no theoretical way for the observed time series to fall outside them.  I have refit the model without the intervention variables and subtracted fit_intervention from fit_nonintervention. The refitted model exhibits fairly similar fitted values with an overall decreased model fit.  ","Creater_id":123935,"Start_date":"2016-07-21 15:08:13","Question_id":225138,"Tags":["time-series","forecasting","intervention-analysis"],"Answer_count":2,"Last_activity":"2016-07-25 16:31:58","Link":"http://stats.stackexchange.com/questions/225138/method-for-quantifying-intervention-effect-in-time-series","Creator_reputation":28}
{"_id":{"$oid":"5837a58ea05283111e4d7977"},"View_count":13,"Display_name":"ML2016","Question_score":1,"Question_content":"When I try to confirm the type I error rate of the logrank test by simulation, I found something strange...Firstly, I generated 100 observations from exponential distribution with lamda1=1 as group 1 and another 100 observations from the same distribution as group 2. No censoring. Repeating this for 1000 times and using survdiff in R I got 1000 logrank test statistics (O-E)/sqrt(V). About 50 out of the 1000 tests had p value \u0026lt; 0.05. So everything is good and there is no surprise. However, in my 2nd simulation I generated 80 observations from exponential distribution with lamda1=1 and 20 observations from exponential distribution with lamda2=10 as group 1 (a mix of 2 different exponentials), and same for group 2. Again no censoring. The type I error is surprisingly only about 1%, way below the 5% level. I believe the reason is that the variance from the logrank test statistics (i.e. the V in (O-E)/sqrt(V)) is about 49, which overestimates the true variance of (O-E), which seems to be about 25 based on the 1000 simulated values.Do you have any insights why this is happening? The logrank test is non-paramatric so I wasn't expecting such a tweak in the underlying distribution will result in such a big impact on the type I error. What should be the correct variance estimator for O-E in such a situation?Thanks!","Creater_id":124494,"Start_date":"2016-07-25 16:12:13","Question_id":225612,"Tags":["variance","type-i-errors","logrank"],"Answer_count":0,"Last_activity":"2016-07-25 16:22:09","Link":"http://stats.stackexchange.com/questions/225612/a-strange-finding-with-the-logrank-test-statistics","Creator_reputation":6}
{"_id":{"$oid":"5837a58ea05283111e4d7979"},"View_count":54,"Display_name":"jeza","Question_score":3,"Question_content":"I encountered two statements in Tibshirani's article Regression Shrinkage and Selection via the Lasso, p.58 (pdf).  Subset selection is a \"discrete process\" which leads to unstable coefficients.  ridge regression is a \"continuous process\" leading to stable coefficients.My question is, why ridge is called continuous process while subset selection is a discrete process. Also, why ridge leads to stability and the Subset selection to instability. ","Creater_id":120788,"Start_date":"2016-07-24 18:37:30","Question_id":225426,"Tags":["regularization","ridge-regression","shrinkage"],"Answer_count":1,"Last_activity":"2016-07-25 15:57:06","Link":"http://stats.stackexchange.com/questions/225426/continuous-process-vs-discrete-process","Creator_reputation":151}
{"_id":{"$oid":"5837a58ea05283111e4d797b"},"View_count":19,"Display_name":"Douglas Gaskell","Question_score":0,"Question_content":"I am trying to determine if the following is a valid SLA, I was presented with this as a fix for us not being able to differentiate the SLA between multiple groups answering calls:1 QueueTwo Groups handling that queue (Group A \u0026amp; Group B)SLA is met for a call when it is answered within 40 seconds (40s wait time)Queue had 100 calls for the day, with an average SLA of 70%. Group A answered 40 calls, Group B answered 60 calls from the same queue. To determine the SLA for the groups individually, the method that was presented was to look at the wait time only for the calls answered by that group. So the SLA for Group A would be determined by looking at the wait time for the 40 calls they answered, and for Group B for the 60 calls they answered.To me this seems skewed, as you are only looking at what was answered, for all intents and purposes wouldn't this be like picking random calls from those 100 for the day and calculating a service level? Expand this out to 1,000,000 calls and I would assume Group A and Group B should have a similar service level as the Queue average (All other factors being equal)?Is this flawed, if not, why is it accurate? If so, why is it flawed?Note: I am not sure what to tag this, please edit this as needed.","Creater_id":95866,"Start_date":"2016-07-25 11:48:08","Question_id":225575,"Tags":["estimation","validation"],"Answer_count":1,"Last_activity":"2016-07-25 15:23:42","Link":"http://stats.stackexchange.com/questions/225575/is-an-sla-pulled-by-just-the-calls-answered-by-a-certain-group-and-not-for-the-w","Creator_reputation":106}
{"_id":{"$oid":"5837a58ea05283111e4d797d"},"View_count":40,"Display_name":"Denis Kelleher","Question_score":2,"Question_content":"BEGINNING:I want to ask a few soft questions about statistics. I understand that maybe/probably some/all off these questions cannot be answered categorically but I would like to try and get some informed opinions on them anyway. I want to learn some practical statistics and that I could apply to actual problems/data sets (e.g. Forecasting the outcome of a sporting event or election or back-testing/designing some kind of strategy). MIDDLE:I'm almost finished an undergraduate degree in Mathematics and Economics and I just don't feel equipped to actually do anything particularly impressive/useful/reliable with what I've learnt. I have taken all the usual statistics modules in, for example, Classical Inference, Bayesian Inference, Econometrics (basically a course on regression), Generalized Linear Models and could understand most everything I did to a level that was more than sufficient for exams, but I still don't feel I'd know or be able implement what's most important. I'd imagine some of these important questions might be:What methods/models apply where?Why do these methods/models apply here?What are the key assumptions you're making?Why might they be wrong? What are the consequences if they are wrong?And then I also have questions like:Well what about machine learning/neuro-networks and all these other new/exciting areas? Should I instead be exploring those areas rather than learning something else?How much statistics is there?How much is useful? How do we know?What's up and coming at the forefront of statistics?what doesn't look like it will stand the test of time?Maybe my question might be better phrased as:What's the most useful technique(s)/theories/thing(s) you know? Why? When have you used them/what makes them useful?Ok fair enough. I'm convinced. How can I teach myself? Where do I start?What do you foresee being the major up and coming themes in statistics? If it's too difficult to say, how does one begin to gather an educated opinion on the(se) topic(s)?What about the techniques that we already have? How good are they?What are the great accomplishments of statistics to date?How good are we at predicting things and explaining things through numbers?END:I have found the amount of statistics available to learn overwhelming. I don't know where to start or why to start anywhere. I'd like to be pointed in the right direction.Books, articles, websites/tutorials and resources that you think might be pointing me in the right direction would be greatly appreciated, as would any advice more generally on how to frame this problem better or phrase these questions better would Apologies if this question is poorly phrased and/or long and overwritten. I'm very interested in getting to the bottom of some issues raised above.And finally apologies if this question/something similar has been asked elsewhere on this site or belongs elsewhere. I had a look around and couldn't find what I was looking for, which is hardly surprising given I'm not quite sure what I'm asking.Many thanks in advance.","Creater_id":101429,"Start_date":"2016-07-25 14:48:20","Question_id":225605,"Tags":["prediction"],"Answer_count":0,"Last_activity":"2016-07-25 15:18:24","Link":"http://stats.stackexchange.com/questions/225605/soft-question-practical-statistcs-what-to-learn-why-learn-it-and-how","Creator_reputation":11}
{"_id":{"$oid":"5837a58ea05283111e4d797f"},"View_count":88,"Display_name":"emlu","Question_score":4,"Question_content":"  Let  be a random sample from the uniform distribution on , where .   Let  and  be the largest and smallest order statistics.  Show that the statistic  is a jointly complete sufficient statistic for the parameter .  It is no problem to show sufficiency using factorization, but I would love a hint on completeness.  I can show  implies  for the one parameter uniform distribution, but am getting stuck on the two parameter.  I tried playing around with  and using the joint distribution of of  and , but I am not sure if I am going in the correct direction, as the calculus is tripping me up.  Any assistance is appreciated.      ","Creater_id":124474,"Start_date":"2016-07-25 12:10:55","Question_id":225580,"Tags":["self-study","uniform","sufficient-statistics","complete-statistics"],"Answer_count":0,"Last_activity":"2016-07-25 15:00:48","Link":"http://stats.stackexchange.com/questions/225580/jointly-complete-sufficient-statistics-uniforma-b","Creator_reputation":21}
