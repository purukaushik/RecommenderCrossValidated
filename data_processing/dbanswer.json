{"_id":{"$oid":"5837a572a05283111e4d29fc"},"Last_activity":"2016-08-31 07:03:31","Creator_reputation":1035,"Question_score":1,"Answer_content":"The value of the correlation coefficient is of importance to qualify its uncertainty. It can be shown with the Cramer-Rao lower bound.In the particular case of a correlation coefficient, you can find results here: CRLB for Pearson linear correlation, slides 12 and 13.It means that the variance of an unbiased estimator cannot be lower than that.You can notice that the higher the correlation, the more precise the estimation can be.The coefficient you propose SD(x)/Mean(x) has the same idea in it (well done), but is totally ad hoc, and, to the best of my knowledge, not theoretically supported.Also, be careful, what is the mean correlation? The arithmetic average of the estimated correlation values? There are arguments (cf. information geometry) to show that it is not a good definition. For example, in the multivariate case, an arithmetic average of correlation matrices will not even yield a correlation matrix: there are good chance that the resulting matrix is not positive semi-definite.If you want to work further these questions, you can google \"information geometry of the multivariate normal\" to have a glimpse of what you can/should do. Good luck!","Display_name":"mic","Creater_id":67168,"Start_date":"2016-08-31 07:03:31","Question_id":232661}
{"_id":{"$oid":"5837a572a05283111e4d2a13"},"Last_activity":"2016-08-31 06:35:09","Creator_reputation":668,"Question_score":1,"Answer_content":"1) If you want your hypotheses to partition the universe, where do you stop? Imagine you have a dataset drawn from a normal distribution. You might consider the hypotheses:: : : The data weren't Normal after all but followed some other distribution: The data didn't even have to be real valued, we just happened to observe nothing but real values...In practice, if the sample mean of population 2 is lower than that of population 1 we will always be rejecting an alternative hypothesis of  whether the null hypothesis contained an equals or a .It is perfectly possible to perform a likelihood ratio test for an \"\" hypothesis against a \"\" hypothesis.2) Yes, you can swap the signs but you don't swap which hypothesis is the null. The two formulations are equivalent:The reason for this is that the datasets were symmetric in a sense, but the null hypothesis enjoys a special privilege (it is \"innocent until proven guilty\", or at least 95%-so.)","Display_name":"JDL","Creater_id":129051,"Start_date":"2016-08-31 06:09:34","Question_id":232665}
{"_id":{"$oid":"5837a572a05283111e4d2a14"},"Last_activity":"2016-08-31 06:25:37","Creator_reputation":11,"Question_score":1,"Answer_content":"Your assumption is correct and you explained it nicely yourself.If you swap the hypotheses, then you must keep in mind that level of significance  and 1-power will swap places.","Display_name":"Pancake","Creater_id":129616,"Start_date":"2016-08-31 06:13:38","Question_id":232665}
{"_id":{"$oid":"5837a572a05283111e4d2a21"},"Last_activity":"2016-08-31 06:26:14","Creator_reputation":109,"Question_score":0,"Answer_content":"The best one (subjective, in my opinion) will be introduction to statistical reasoning by Gary Smith. This book is worth its weight in gold for elementary statistics and is filled with tons of real world examples and stories.Slightly more academic but nonetheless good starting point will be introduction to probability and statistics by Sheldon Ross.","Display_name":"hssay","Creater_id":26218,"Start_date":"2016-08-31 06:26:14","Question_id":232662}
{"_id":{"$oid":"5837a572a05283111e4d2a34"},"Last_activity":"2016-08-31 05:46:29","Creator_reputation":1838,"Question_score":0,"Answer_content":"I'm not an expert in this area at all, but I'm also interested in the question, so I did some digging.First, It definitely is, possible to calculate the variance of a LOESS model, because such variance estimates are available from some packages, e.g. R's loess (see this post for a usage example).The citation for that loess function states[1] that:  We can specify properties of the variances of the  in one of two ways. The  first is simply that they are a constant, . The second is that  has constant variance , where the a priori weights, , are positive and known.HoweMeaning, I think, that you either have to assume that the error variance is globally constant, or that you know your weights a priori. However, the chapter doesn't actually describe the process for calculating the error variance in either situation, and I haven't yet found another reference that does...ReferencesW. S. Cleveland, E. Grosse and W. M. Shyu (1992) Local regression models. Chapter 8 of Statistical Models in S eds J.M. Chambers and T.J. Hastie, Wadsworth \u0026amp; Brooks/Cole. ","Display_name":"naught101","Creater_id":9007,"Start_date":"2016-08-31 05:46:29","Question_id":189625}
{"_id":{"$oid":"5837a572a05283111e4d2a43"},"Last_activity":"2016-08-31 04:43:39","Creator_reputation":341,"Question_score":5,"Answer_content":"The standard deviation  is invariant under changes in location, and scales directly with the scale of the random variable. Thus, for constants  and : . In your case  and , then .https://en.wikipedia.org/wiki/Standard_deviation","Display_name":"Nick","Creater_id":111637,"Start_date":"2016-08-31 04:05:19","Question_id":232640}
{"_id":{"$oid":"5837a572a05283111e4d2a44"},"Last_activity":"2016-08-31 04:32:08","Creator_reputation":2544,"Question_score":2,"Answer_content":"So you haveVar(x) = 25We know variance is additiveVar(y) = Var(2x-3) = Var(2x) + Var(3) = 4 \\cdot Var(x) + 0 = 4 \\cdot 25 = 100ThenSD(y) = \\sqrt {100} = 10Which makes a lot of sense. You added a constant () to every number in : does it change the spread of the numbers? No, of course not, the elements of  are equally spaced this way, just centered around another number.You also multiplied every number  by : does it change the spread of the numbers? Yes, it does. How much? Exactly  times, which is the result.","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-08-31 04:32:08","Question_id":232640}
{"_id":{"$oid":"5837a572a05283111e4d2a45"},"Last_activity":"2016-08-31 03:59:12","Creator_reputation":3327,"Question_score":2,"Answer_content":"It might help to know that  where  is a random variable and  and  are constants. Standard deviation is square root of variance. ","Display_name":"bdeonovic","Creater_id":17661,"Start_date":"2016-08-31 03:59:12","Question_id":232640}
{"_id":{"$oid":"5837a572a05283111e4d2a46"},"Last_activity":"2016-08-31 03:49:26","Creator_reputation":25,"Question_score":0,"Answer_content":"The standard deviation measures the volatility of a variable. Since 3 is a constant (not a random variable) and constants have no volatility, the standard deviation of y is 10.","Display_name":"user259047","Creater_id":127605,"Start_date":"2016-08-31 03:49:26","Question_id":232640}
{"_id":{"$oid":"5837a572a05283111e4d2a53"},"Last_activity":"2012-02-14 04:44:38","Creator_reputation":1885,"Question_score":3,"Answer_content":"With a lot of variables, at some point you are going to want to figure out what's what, and this will be easier if they are meaningful when put in alphabetical order. You are less likely to group them by whether they are logged or not than whether they are in the same \"family\".  So, I'd rearrange your example as this:gt_100k_income               income_gt_100kis_missing_income            income_missinglg10_income                  income_lg10age_20_lt_25_age             age_ge_20_lt_25zscale_age                   age_zscaleratio_ln_income_ln_age       income_ln_over_age_lnI realize that this is exactly the opposite of what some software does automatically (such as Excel pivot tables or Alteryx Summaries), but Bill Gates isn't right all the time.It's probably more important to be consistent with your method, than what that particular method is.","Display_name":"zbicyclist","Creater_id":3919,"Start_date":"2012-02-13 22:12:23","Question_id":22766}
{"_id":{"$oid":"5837a572a05283111e4d2a54"},"Last_activity":"2012-02-13 22:34:22","Creator_reputation":2061,"Question_score":3,"Answer_content":"You're making me think about a kind of Hungarian notation for feature names. Cool.I like feature names that sort alphabetically on some meaningful domain category. In your example, I'd have all income-related features start with income_.I also like feature name suffixes that make it obvious what kind of values the feature can take. For instance, if the feature is binary, I may have it end in _is, e.g., income_missing_is. If the feature is a frequency count, it's a _freq, and you immediately know that that will never be less than zero.If the feature is automatically generated by some special mechanism, e.g., third-party software or some cross-referenced dataset, that is sometimes useful in the feature name. For instance, income_census2010_bracket.You will want to search and filter feature names, so always use underscore separators and lower case identifiers, never camel case.Verbosity is not generally an issue; 40-60 characters is fine.","Display_name":"Jack Tanner","Creater_id":8207,"Start_date":"2012-02-13 22:34:22","Question_id":22766}
{"_id":{"$oid":"5837a572a05283111e4d2a61"},"Last_activity":"2016-08-31 04:58:18","Creator_reputation":668,"Question_score":1,"Answer_content":"You are almost there; the weighted mean is \\frac{\\sum w_ix_i}{\\sum w_i}. Just divide both sides by  and you have the result you want.","Display_name":"JDL","Creater_id":129051,"Start_date":"2016-08-31 04:58:18","Question_id":232656}
{"_id":{"$oid":"5837a572a05283111e4d2a6e"},"Last_activity":"2016-08-31 04:43:21","Creator_reputation":668,"Question_score":2,"Answer_content":"OLS is based on the Normal distribution, which is continuous. If this model predicts a response of 50mg, then what it is really saying is that it expects the response to come from a Normal distribution with mean 50mg and some standard deviation. For the sake of concreteness I'll say that SD is 5mg.The chance of the response being exactly 50mg is zero under this model. However, what I think you are really asking is \"what is the chance the observed value rounds to 50mg (i.e. it's between 49.5 and 50.5, or between 45 and 55). This can be computed by referring to tables or using inbuilt calculator functions. In my example the probability of being between 49.5 and 50.5 would have been a little under 8%, and of being between 45 and 55 would have been 68%.","Display_name":"JDL","Creater_id":129051,"Start_date":"2016-08-31 04:43:21","Question_id":232654}
{"_id":{"$oid":"5837a572a05283111e4d2a7b"},"Last_activity":"2016-08-31 04:38:24","Creator_reputation":668,"Question_score":0,"Answer_content":"This depends on your working hypotheses.If you believe that the interaction between  and  will be the same for everybody (same additive effect on the log-odds ratio) then the second model (under (a)) is appropriate. If you believe it depends on the individual, then the first form is appropriate.If I understand the question correctly, then you are saying  is constant given ID and . In that case, there's no point having A*B|ID in the model since that expands to (A + B + A:B|ID) and the last term has as many levels as there are data. The \"random effects\" would end up being a perfect fit for the individual means.If  is constant across both sessions then it should not appear in the random effect at all. In effect you would have no way to untangle which part of the random effect was down to  and which down to ID. Thus the appropriate form would be ~1+A+B+(1+A|ID).","Display_name":"JDL","Creater_id":129051,"Start_date":"2016-08-31 04:38:24","Question_id":232648}
{"_id":{"$oid":"5837a572a05283111e4d2a88"},"Last_activity":"2016-08-31 04:24:12","Creator_reputation":25170,"Question_score":3,"Answer_content":"It is a categorical distribution. It is discrete since it can take a countable number of values. Discreteness is not about being integer, you can as well have your distinct values labeled by colors, names, letters, or anything you want. It's expected value is simply E(X) = \\sum_{i=1}^k p_i x_i for  distinct outcomes  occuring with probabilities .","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-31 04:16:38","Question_id":232649}
{"_id":{"$oid":"5837a572a05283111e4d2a95"},"Last_activity":"2016-08-28 09:16:56","Creator_reputation":4307,"Question_score":0,"Answer_content":"If you define  and , then your model is\\mathbf{K}_{t+1}\\vert(\\mathbf{K}_t,a,\\theta,\\mathbf{C})\\sim \\mathrm{N}_{\\boldsymbol{\\mu},\\mathbf{C}}i.e. the distribution of your data at  conditioned on both the data at , and on your parameters, is a bivariate normal with the specified non-zero conditional mean. The equation above is the likelihood.If your covariance matrix were isotropic () this would be simple linear regression (i.e. with  and ). So the only change is your have a general covariance matrix, so three components (e.g. two variances and a correlation coefficient).For MLE you want to determine the parameter values that make the gradient of the log-likelihood zero. Your parameters are , where I have expanded out the diagonal and off-diagonal components of the symmetric covariance matrix. For each parameter you get an equation by setting the gradient to zero, i.e.  for , where  is the negative log likelihood.Note that for the first two parameters  you can get the gradient from the chain rule, i.e. \\frac{\\partial L}{\\partial a}=\\frac{\\partial L}{\\partial \\mu_{\\kappa}}\\frac{\\partial \\mu_{\\kappa}}{\\partial a} and \\frac{\\partial L}{\\partial \\theta}=\\frac{\\partial L}{\\partial \\mu_K}\\frac{\\partial \\mu_K}{\\partial \\theta}where  and .","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-26 21:24:21","Question_id":231983}
{"_id":{"$oid":"5837a572a05283111e4d2aa4"},"Last_activity":"2016-08-31 04:18:09","Creator_reputation":3559,"Question_score":1,"Answer_content":"The term stratified analysis is usually reserved for an analysis where you divide the data-set into strata using one of your variables and then perform the same analysis within each stratum. So from your description that is definitely not what you did. The term combined analysis is not one with which I am familiar but it sounds more like a meta-analysis which is not what you did either.What you did will normally be reported using the name of the model you actually used, so if you used a multinomial logistic regression that is what you would report (for example, I am not saying you have to do that).","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-31 04:18:09","Question_id":232619}
{"_id":{"$oid":"5837a572a05283111e4d2ab5"},"Last_activity":"2016-08-31 04:02:38","Creator_reputation":56,"Question_score":0,"Answer_content":"in this  link the author tells you to change your statistics (correlation values) population distribution into a normal one (by Fisher transform). Then, by Z-test, you can test your null hypothesis, e.g. if the 2 correlations are equal.","Display_name":"Teodor Petrut","Creater_id":129527,"Start_date":"2016-08-31 04:02:38","Question_id":232608}
{"_id":{"$oid":"5837a572a05283111e4d2ab6"},"Last_activity":"2016-08-31 03:16:54","Creator_reputation":20442,"Question_score":1,"Answer_content":"You can test the difference between two observed correlations and see how large the probability is that the difference you observed is simply due to sampling variability, assuming that the \"true\" correlations are equal. This is a classical application of testing for statistical-significance. The result will depend on the number of observations that the two observed correlations come from and similar things.Diedenhofen \u0026amp; Musch (2015, PLoS ONE) give an overview of possible tests and describe the 'cocor' package for R that implements them.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2016-08-31 00:38:54","Question_id":232608}
{"_id":{"$oid":"5837a572a05283111e4d2ac3"},"Last_activity":"2015-09-19 11:04:45","Creator_reputation":1,"Question_score":0,"Answer_content":"This is same as no. Of selection of any thing from out of 'n' things.for each thing we have two options select or deselect,means 2 choice for each thing and here total 'n' things so,there are no. Of selection is 2*2*2••••••••••n times,this is equal to 2^n.","Display_name":"ambuj jha","Creater_id":89867,"Start_date":"2015-09-19 11:04:45","Question_id":27266}
{"_id":{"$oid":"5837a572a05283111e4d2ac4"},"Last_activity":"2012-04-27 11:09:49","Creator_reputation":24502,"Question_score":14,"Answer_content":"See http://en.wikipedia.org/wiki/Combination#Number_of_k-combinations_for_all_k which says  \\sum_{k=0}^{n} \\binom{n}{k} = 2^n You can prove this using the binomial theorem where . Now, since  for any , it follows that  \\sum_{k=1}^{n} \\binom{n}{k} = 2^n - 1 In your case , so the answer is . ","Display_name":"Macro","Creater_id":4856,"Start_date":"2012-04-27 10:59:00","Question_id":27266}
{"_id":{"$oid":"5837a572a05283111e4d2ac5"},"Last_activity":"2012-04-27 10:58:23","Creator_reputation":4892,"Question_score":8,"Answer_content":"Homework?Hint:Remember the binomial theorem:\r\r(x+y)^n = \\sum_{k=0}^{n}\\binom{n}{k}x^ky^{n-k}\r\rNow, if you could just find x and y so that  is constant... ","Display_name":"Erik","Creater_id":10524,"Start_date":"2012-04-27 10:58:23","Question_id":27266}
{"_id":{"$oid":"5837a572a05283111e4d2ad2"},"Last_activity":"2012-06-21 12:21:37","Creator_reputation":1021,"Question_score":1,"Answer_content":"We must count the number of integer points in a 5-D space () with and A trivial simplification: let , so now we have and This corresponds to an equilateral 5-D tetraedron (standard simplex). And the number of points if given by the 5-simplex number (generalization of triangular-tetraedral numbers to five dimensions) :(Notice that the problem was eased because the restrictions eactly coincide with the simplex. If the maximum number were 31 or 29 instead of 30, it would have been a little more difficult)","Display_name":"leonbloy","Creater_id":2546,"Start_date":"2012-06-21 12:21:37","Question_id":30875}
{"_id":{"$oid":"5837a572a05283111e4d2ad3"},"Last_activity":"2012-06-21 12:11:07","Creator_reputation":507,"Question_score":0,"Answer_content":"I made a 5 dimension array of the sums and then counted which ones were below 31. In R:a=b=c=d=e=2:22all combinations of a + bab=outer(a,b,FUN=\"+\")all combinations of a + b + cabc=outer(ab,c,FUN=\"+\")...abcd=outer(abc,d,FUN=\"+\")abcde=outer(abcd,e,FUN=\"+\")So the size of abcde is 21x21x21x21x21 about 4million entrieslength(which(abcde\u0026lt;31))Gives:53130If you want the combinations listed use:mixes=(which(abcde\u0026lt;31,arr.ind=T)+1)/100","Display_name":"Seth","Creater_id":10630,"Start_date":"2012-06-21 11:19:22","Question_id":30875}
{"_id":{"$oid":"5837a572a05283111e4d2ae0"},"Last_activity":"2016-08-31 03:28:54","Creator_reputation":56,"Question_score":1,"Answer_content":"The Brownian noise produced by the Brownian random walk is the integral of the white noise, whose PSD is flat. The proof is given here.","Display_name":"Teodor Petrut","Creater_id":129527,"Start_date":"2016-08-31 03:28:54","Question_id":232622}
{"_id":{"$oid":"5837a572a05283111e4d2af1"},"Last_activity":"2016-08-31 02:52:46","Creator_reputation":145,"Question_score":1,"Answer_content":"One of the biggest problems I can think of is that you become very likely to generate samples that do not exist in the data you generated your model from.An example could be if you made a model that generates mammals based on their features such as whether they have a tail, how many eyes they have and the number of legs. Most mammals have 4 legs, so let's assign the probability 0.9 to that feature and a small subset (humans and monkeys I guess) have 2 legs, so they get a probability of 0.1 and the probability for any other number of legs is 0.0.As far as I know there are no mammals with more or less than 2 eyes, so let's just assign the probability 1.0 to that and 0.0 to everything else.      1 | 2 | 3 | 4 | 5 |...legs 0.0|0.1|0.0|0.9|0.0|0.0 eyes 0.0|1.0|0.0|0.0|0.0|0.0Now, if you are maximizing over the product of probabilities you will find that the probability of generating an animal with 2 eyes and 4 legs is 0.9, 2 eyes and 2 legs is 0.1 whereas generating an animal with 2 eyes and 10 legs is 0.0, so you're staying pretty close to observations you would see in your training set.p(legs=4, eyes=2) = 0.9\\\\p(legs=2, eyes=2) = 0.1\\\\p(legs=5, eyes=2) = 0.0If you instead maximize over the sum of probabilities a mammal with 2 eyes and 4 legs will get a value of 1.9, 2 eyes and 2 legs 1.1 and a mammal with 2 eyes and 3 or even 400 legs gets a value of 1.0. f(x) = \\sum p(x)\\\\f(legs=4, eyes=2) = 1.9\\\\f(legs=2, eyes=2) = 1.1\\\\f(legs=5, eyes=2) = 1.0The problems here are of course that first of all these values can no longer be interpreted as probabilities as they are no longer constrained to [0, 1], but also that samples not observed in the training set are almost as likely as observed but rare samples.Maybe there are cases where generating one probable feature makes outlandish values for other features likely, but nothing immediately springs to mind.","Display_name":"Simon Thordal","Creater_id":56538,"Start_date":"2016-08-31 02:52:46","Question_id":232604}
{"_id":{"$oid":"5837a572a05283111e4d2af2"},"Last_activity":"2016-08-31 02:39:34","Creator_reputation":3023,"Question_score":6,"Answer_content":" is the probability that you observe  for a given parameter value .  In a sample you observe several  simultanuously and (assuming independence) the probability that you observe all the values in your sample simultanuously is .  So maximising this with respect to  means that you look for the value of  that makes your (full) observed sample most probable.   is (assuming that  that the observations do not intersect) would be the probability that you observe at least one of the .  So if you maximise this then you look for the value of  such that at least one of the -values is observed.  As you have observed all the  values, it makes sense to compute the probability that you all observed these simultanuously I think. This is also what @Alex says in his comment on the ''joint probability''. As @Tim argues (+1) there are reasons to log-transform it and as the 'log' is monotic you will find the same . Due to your comment ''the Bayesian approach does not maximize the joint prob'' I make the following remark: if you observe one value  then the Bayes approach would compute the posterior . If I observe a second observation  then I will update my estimate using the ''updated prior'', so you compute  and that prior would certainly be the one that already takes into account the other (independent) observations (), so  would be  and the new posterior becomes . After observing the full sample you find the posterior , so what you say: ''The Bayesian apporach does not maximize the joint prob'' is not fully correct; the Bayesian approach is also using the joint probability of the full sample, i.e. , in the likelihood function.","Display_name":"fcop","Creater_id":83346,"Start_date":"2016-08-31 02:00:59","Question_id":232604}
{"_id":{"$oid":"5837a572a05283111e4d2af3"},"Last_activity":"2016-08-31 02:30:37","Creator_reputation":25170,"Question_score":7,"Answer_content":"As you already noticed, maximum likelihood is used when we are dealing with independent and identically distributed random variables. By the definition of independent random variables  p_{X,Y}(x, y) = p_X(x)\\,p_Y(y) So if we are interested in the joint distribution of independent random variables, then by definition, we need to multiply them. However, you can always take the sum of log-probabilities to obtain log-likelihood given the properties of logarithms, specifically \\log\\ a + \\log\\ b = \\log\\ (ab) In fact, using log-likelihoods is preferable to using \"raw\" likelihoods because they behave better when optimizing and due to underflow issues.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-30 23:55:49","Question_id":232604}
{"_id":{"$oid":"5837a572a05283111e4d2b00"},"Last_activity":"2016-08-31 02:50:46","Creator_reputation":404,"Question_score":14,"Answer_content":"For White standard errors clustered by group with the plm framework trycoeftest(model.plm, vcov=vcovHC(model.plm,type=\"HC0\",cluster=\"group\"))where model.plm is a plm model.See also this linkhttp://www.inside-r.org/packages/cran/plm/docs/vcovHC or the plm package documentationEDIT:For two-way clustering (e.g. group and time) see the following link:http://people.su.se/~ma/clustering.pdfHere is another helpful guide for the plm package specifically that explains different options for clustered standard errors: http://www.princeton.edu/~otorres/Panel101R.pdfClustering and other information, especially for Stata, can be found here:http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm","Display_name":"chandler","Creater_id":12053,"Start_date":"2013-05-28 11:25:53","Question_id":10017}
{"_id":{"$oid":"5837a572a05283111e4d2b01"},"Last_activity":"2016-02-26 10:05:41","Creator_reputation":463,"Question_score":0,"Answer_content":"If you don't have a time index, you don't need one: plm will add a fictitious one by itself, and it won't be used unless you ask for it. So this call should work: \u0026gt; x \u0026lt;- plm(price ~ carat, data = diamonds, index = \"cut\") Error in pdim.default(index[[1]], index[[2]]) :   duplicate couples (time-id) Except that it doesn't, which suggests you've hit a bug in plm. (This bug has now been fixed in SVN. You can install the development version from here.)But since this would be a fictitious time index anyway, we can create it by ourselves: diamonds$ftime \u0026lt;- 1:NROW(diamonds)  ##fake timeNow this works: x \u0026lt;- plm(price ~ carat, data = diamonds, index = c(\"cut\", \"ftime\"))coeftest(x, vcov.=vcovHC)## ## t test of coefficients:## ##       Estimate Std. Error t value  Pr(\u0026gt;|t|)    ## carat  7871.08     138.44  56.856 \u0026lt; 2.2e-16 ***## ---## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Important note: vcovHC.plm() in plm will by default estimate Arellano clustered by group SEs. Which is different from what vcovHC.lm() in sandwich will estimate (e.g. the vcovHC SEs in the original question), that is heteroscedasticity-consistent SEs with no clustering.   A separate approach to this is sticking to lm dummy variable regressions and the multiwayvcov package. library(\"multiwayvcov\")fe.lsdv \u0026lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)coeftest(fe.lsdv, vcov.= function(y) cluster.vcov(y, ~ cut, df_correction = FALSE))## ## t test of coefficients:## ##                      Estimate Std. Error t value  Pr(\u0026gt;|t|)    ## carat                 7871.08     138.44  56.856 \u0026lt; 2.2e-16 ***## factor(cut)Fair      -3875.47     144.83 -26.759 \u0026lt; 2.2e-16 ***## factor(cut)Good      -2755.14     117.56 -23.436 \u0026lt; 2.2e-16 ***## factor(cut)Very Good -2365.33     111.63 -21.188 \u0026lt; 2.2e-16 ***## factor(cut)Premium   -2436.39     123.48 -19.731 \u0026lt; 2.2e-16 ***## factor(cut)Ideal     -2074.55      97.30 -21.321 \u0026lt; 2.2e-16 ***## ---## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1In both cases you will get the Arellano (1987) SEs with clustering by group. The multiwayvcov package is a direct and significant evolution of Arai's original clustering functions. You can also look at the resulting variance-covariance matrix from both approaches, yielding the same variance estimate for carat: vcov.plm \u0026lt;- vcovHC(x)vcov.lsdv \u0026lt;- cluster.vcov(fe.lsdv, ~ cut, df_correction = FALSE)vcov.plm##          carat## carat 19165.28diag(vcov.lsdv)##                carat      factor(cut)Fair      factor(cut)Good factor(cut)Very Good   factor(cut)Premium     factor(cut)Ideal ##            19165.283            20974.522            13820.365            12462.243            15247.584             9467.263 ","Display_name":"landroni","Creater_id":36515,"Start_date":"2016-02-20 07:38:16","Question_id":10017}
{"_id":{"$oid":"5837a572a05283111e4d2b02"},"Last_activity":"2015-04-27 09:12:06","Creator_reputation":453,"Question_score":6,"Answer_content":"After a lot of reading, I found the solution for doing clustering within the lm framework.There's an excellent white paper by Mahmood Arai that provides a tutorial on clustering in the lm framework, which he does with degrees-of-freedom corrections instead of my messy attempts above. He provides his functions for both one- and two-way clustering covariance matrices here.Finally, although the content isn't available free, Angrist and Pischke's Mostly Harmless Econometrics has a section on clustering that was very helpful. Update on 4/27/2015 to add code from blog post.api=read.csv(\"api.csv\") #create the variable api from the corresponding csvattach(api) # attach of data.frame objectsapi1=api[c(1:6,8:310),] # one missing entry in row nr. 7modell.api=lm(API00 ~ GROWTH + EMER + YR_RND, data=api1) # creation of a simple linear model for API00 using the regressors Growth, Emer and Yr_rnd.##creation of the function according to Arai:clx \u0026lt;- function(fm, dfcw, cluster) {    library(sandwich)    library(lmtest)    library(zoo)    M \u0026lt;- length(unique(cluster))    N \u0026lt;- length(cluster)    dfc \u0026lt;- (M/(M-1))*((N-1)/(N-fmDNUM) #creation of results.","Display_name":"Richard Herron","Creater_id":1445,"Start_date":"2011-04-28 08:08:21","Question_id":10017}
{"_id":{"$oid":"5837a572a05283111e4d2b13"},"Last_activity":"2016-08-31 02:07:01","Creator_reputation":6,"Question_score":0,"Answer_content":"I maybe have an answer which I'm not sure about : I think that I should transfer my VECM to a VAR form, and then I could analyse the Eigenvalues of the compagnon matrix of VAR. There should be r unit roots where r is the number of cointegration relationships, and the module of the rest Eigenvalue should be strictly inferior to 1.The definition of a compagnon matrix could be found at page 7 and 8 of this link : http://economia.unipv.it/pagp/pagine_personali/erossi/rossi_VAR_PhD.pdfPlease confirm if my idea is correct. Also, if my question is not clear enough, please tell me.","Display_name":"coldcola","Creater_id":129586,"Start_date":"2016-08-31 02:07:01","Question_id":232413}
{"_id":{"$oid":"5837a572a05283111e4d2b22"},"Last_activity":"2016-08-31 01:55:55","Creator_reputation":23,"Question_score":1,"Answer_content":"Another journal paper seems to explain the terms:  \"Based on whether kurtosis is larger than 3, the non-Gaussian  process is classified as “hardening” (kurtosis\u0026lt;3) and “softening”  (kurtosis\u003e3) cases\"Revisiting moment-based characterization for wind pressures","Display_name":"Kuti","Creater_id":116458,"Start_date":"2016-08-31 01:55:55","Question_id":232530}
{"_id":{"$oid":"5837a572a05283111e4d2b2f"},"Last_activity":"2016-08-31 01:24:08","Creator_reputation":668,"Question_score":0,"Answer_content":"(OLS) regressing a response variable against a vector of ones is equivalent to fitting a constant model consisting of the mean of the response. The residuals are then .If the response has already been centred then this is a no-op.","Display_name":"JDL","Creater_id":129051,"Start_date":"2016-08-31 01:24:08","Question_id":232615}
{"_id":{"$oid":"5837a572a05283111e4d2b3c"},"Last_activity":"2016-08-31 00:39:33","Creator_reputation":121,"Question_score":1,"Answer_content":"Another way to look at the problem.Lets call a 'real result' a 1,2,3,5 or 6.What is the probability of winning on the first roll, if you got a 'real result'? 2/5What is the probability of winning on the second roll, if the second roll is the first time you got a 'real result'? 2/5Same for third, fourth.So, you can break your sample in (infinte) smaller samples, and those samples all give the same probability.","Display_name":"josinalvo","Creater_id":29107,"Start_date":"2016-08-31 00:39:33","Question_id":232106}
{"_id":{"$oid":"5837a572a05283111e4d2b3d"},"Last_activity":"2016-08-29 21:47:11","Creator_reputation":1826,"Question_score":3,"Answer_content":"All of the above answers are correct, but they don't explain why they are correct, and why you can ignore so many details and avoid having to solve a complicated recurrence relation.The reason why the other answers are correct is the Strong Markov property, which for a discrete Markov Chain is equivalent to the regular Markov property. https://en.wikipedia.org/wiki/Markov_property#Strong_Markov_propertyBasically the idea is that the random variable the number of times until the die does not land on 4 for the first time) is a stopping time. https://en.wikipedia.org/wiki/Stopping_time A stopping time is a random variable which doesn't depend on any future information. In order to tell whether the th roll of the die is the first one which has not landed on a 4 (i.e. in order to decide whether ), you only need to know the value of the current roll, and of all previous rolls, but not of any future rolls -- thus  is a stopping time, and the Strong Markov property applies.What does the Strong Markov property say? It says that the number which the die lands on at the th roll, as a random variable, , is independent of the values of ALL previous rolls. So if the die rolls 4 once, twice, ..., 50 million times, ...,  times before finally landing on another value for the th roll, it won't affect the probability of the event that .\\mathbb{P}(X_{\\tau}\u0026gt;4|\\tau=1)=\\mathbb{P}(X_{\\tau}\u0026gt;4|\\tau=2)=\\dots = \\mathbb{P}(X_{\\tau}\u0026gt;4|\\tau=50,000,000)=\\dots Therefore we can assume, without loss of generality, that . This is just the probability that the die lands a value greater than 4 given that it does not land on 4, which we can calculate very easily:\\mathbb{P}(X_1\u0026gt;4|X\\not=4) = \\frac{\\mathbb{P}(X_1 \u0026gt; 4 \\cap X_1 \\not=4)}{\\mathbb{P}(X_1 \\not= 4)} = \\frac{\\mathbb{P}(X_1 \u0026gt; 4)}{\\mathbb{P}(X_1 \\not=4)}=\\frac{\\frac{1}{3}}{\\frac{5}{6}}=\\frac{1}{3}\\cdot\\frac{6}{5}=\\frac{2}{5}  which of course is the correct answer.You can read more about stopping times and the Strong Markov property in Section 8.3 of (the 4th edition of) Durrett's Probability Theory and Examples, p. 365.","Display_name":"William","Creater_id":113090,"Start_date":"2016-08-29 21:47:11","Question_id":232106}
{"_id":{"$oid":"5837a572a05283111e4d2b3e"},"Last_activity":"2016-08-29 16:12:07","Creator_reputation":7412,"Question_score":45,"Answer_content":"Just solve it using algebra:\\begin{aligned}P(W) \u0026amp;= \\tfrac 2 6 + \\tfrac 1 6 \\cdot P(W) \\\\[7pt]\\tfrac 5 6 \\cdot P(W) \u0026amp;= \\tfrac 2 6 \\\\[7pt]P(W) \u0026amp;= \\tfrac 2 5.\\end{aligned}","Display_name":"dsaxton","Creater_id":78861,"Start_date":"2016-08-27 18:48:25","Question_id":232106}
{"_id":{"$oid":"5837a572a05283111e4d2b3f"},"Last_activity":"2016-08-29 08:57:06","Creator_reputation":428,"Question_score":14,"Answer_content":"The answers by dsaxton (http://stats.stackexchange.com/a/232107/90759) and GeoMatt22 (http://stats.stackexchange.com/a/232107/90759) give the best approaches to the problem. Another is to realize that your expressionP(W) = \\frac13+\\frac16\\left(\\frac13+\\frac16(\\cdots)\\right)Is really a geometric progression:\\frac13+\\frac16\\frac13+\\frac1{6^2}\\frac13+\\cdotsIn general we have\\sum_{n=0}^{\\infty}a_0q^n = \\frac{a_0}{1-q}so here we haveP(W) = \\frac{\\frac13}{1-\\frac16} = \\frac13:\\frac56=\\frac{6}{15}=\\frac25.Of course, the way to prove the general formula for the sum of a geometric progression, is by using an algebraic solution similarly to dsaxton.","Display_name":"Meni Rosenfeld","Creater_id":90759,"Start_date":"2016-08-28 03:44:10","Question_id":232106}
{"_id":{"$oid":"5837a572a05283111e4d2b40"},"Last_activity":"2016-08-27 18:14:29","Creator_reputation":4307,"Question_score":79,"Answer_content":"Note: This is an answer to the initial question, rather than the recurrence.If she rolls a 4, then it essentially doesn't count, because the next roll is independent. In other words, after rolling a 4 the situation is the same as when she started. So you can ignore the 4. Then the outcomes that could matter are 1-3 and 5-6. There are 5 distinct outcomes, 2 of which are winning. So the answer is 2/5 = 0.4 = 40%.","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-27 18:14:29","Question_id":232106}
{"_id":{"$oid":"5837a572a05283111e4d2b4f"},"Last_activity":"2016-08-31 00:36:32","Creator_reputation":328,"Question_score":0,"Answer_content":"We have , and the vector  sums to one, . Now the vector  is constructed. The first  terms of  sums to one, , and the last term is just .The proof shows that whan  the distribution of the vector  factorizes as a product of a Dirichlet distribution for the fist  terms, and a Beta distribution for the last term . Thus we can sample from the Dirichlet distribution by fist simulating  from a Beta distribution, and then simulating the remaining  from a Dirichlet. To do this, the same approach can be applied again, first simulating the last term from a Beta etc. which is the stick breaking construction.","Display_name":"Mikkel N. Schmidt","Creater_id":41429,"Start_date":"2016-08-31 00:36:32","Question_id":232435}
{"_id":{"$oid":"5837a572a05283111e4d2b5c"},"Last_activity":"2015-04-08 20:04:36","Creator_reputation":4927,"Question_score":1,"Answer_content":"Despite the term multiple factor analysis (MFA), used to describe the factor analysis (FA) that you've performed, it seems to me like a standard PCA approach (or, FA via PCA, at best), which focuses on principal components. Instead, I suggest you to use exploratory factor analysis (EFA) and then confirmatory factor analysis (CFA), both of which focus on latent variables approach. EFA serves as an alternative dimensionality reduction technique with an added benefits of discovering latent factor structure, which has more explanatory power. Let me know, if you need further help.","Display_name":"Aleksandr Blekh","Creater_id":31372,"Start_date":"2015-04-08 20:04:36","Question_id":145441}
{"_id":{"$oid":"5837a572a05283111e4d2b69"},"Last_activity":"2016-08-30 23:22:13","Creator_reputation":5179,"Question_score":1,"Answer_content":"The remark is not referring to continuous-time--continuous-observation Kalman-Bucy filters, but to discrete-time Kalman filters. The confusion seems to be only due to the OP not knowing about the discrete-time version (which in my experience is most commonly meant when 'Kalman filter' is mentioned). See, for example, the Wikipedia article 'Kalman filter' or [1]. In the discrete-time case, the state-space of the nodes is indeed not discrete but  for the measurements and  for the observations. There are however other Bayesian networks with continuous state-space (for the variables) and Gaussian conditional distributions, too [e.g. 2].The discrete-time linear-Gaussian dynamic-system model can be written as a dynamic Bayesian network as follows.Time-slice  consists of nodes  and  and there is an edge pointing from  to .The intertemporal edges are from  to .The conditional probability distributions are  and  where all quantities except  are known matrices.The Kalman filter is then an algorithm for sequentially updating the distributions of  given observed  in this dynamic Bayesian network. The only probability theory required is computing conditional distributions of (finite-dimensional) multivariate Gaussian distributions. Caveat: There exists also something called 'Continuous-time Bayesian networks'[3] but I'm not aware of any connection between them and the Kalman-Bucy filter's model.References[1]: Simo Särkkä (2013). Bayesian Filtering and Smoothing. Cambridge University Press. Section 4.3. Available on the author's webpage. (Conflict-of-interest disclaimer: the author was my PhD advisor)[2]: F.V. Jensen (2001), Bayesian Networks and Decision Graphs, Springer (p. 69)  (Curiously,this book p. 65 claims that a \"Kalman filter\" is any hidden Markov model with only one variable having intertemporal 'relatives' but this is definitely nonstandard usage)[3]: Nodelman, U., Shelton, C. R., \u0026amp; Koller, D. (2002, August). Continuous time Bayesian networks. In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence (pp. 378-387).","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-30 23:22:13","Question_id":232594}
{"_id":{"$oid":"5837a572a05283111e4d2b76"},"Last_activity":"2016-08-30 23:21:31","Creator_reputation":8283,"Question_score":2,"Answer_content":"(Edit: I initially completely ignored the TossCoin() function for some reason.)You're doing the following:Draw . This will be either 0 (70% of the time) or 1 (30% of the time).Draw  until . This could more succinctly be represented as: if  is 0, set ; if  is 1, set . Even more succinctly, set .Return . Since  is either 0 or 1,  is just . So you're just returning .Thus, your overall program is just adding up 100  draws, i.e. sampling from . This doesn't seem to be what you wanted.Your problem is this:Let  be your first sample from a Bernoulli(.3), and  the second. Then, as you know, if  and  are sampled independently:So, yes, 0 1 and 1 0 are equal probability. If you sampled  and  independently, rejecting the sample when they were both 0 or both 1, then your intention is correct:P(X=0, Y=1 \\mid X \\ne Y) = .5 = P(X=1, Y=0 \\mid X \\ne Y).But TossCoin() doesn't take two independent samples. It takes one sample, and then constrains the other one to be the other value. That's a different thing entirely:\\begin{align}P(\\mathtt{TossCoin}\\text{'s samples} = (0, 1))\u0026amp;= P(X = 0) P(Y = 1 \\mid Y \\ne X, X = 0)= .7 \\times 1\\\\P(\\mathtt{TossCoin}\\text{'s samples} = (1, 0))\u0026amp;= P(X = 1) P(Y = 0 \\mid Y \\ne X, X = 1)= .3 \\times 1\\end{align}","Display_name":"Dougal","Creater_id":9964,"Start_date":"2016-08-30 22:44:35","Question_id":232596}
{"_id":{"$oid":"5837a572a05283111e4d2b77"},"Last_activity":"2016-08-30 22:50:36","Creator_reputation":8337,"Question_score":2,"Answer_content":"No. Your Bernoulli function returns 1 with probability .3, as the comment notes, not .5. And I don't know what you intended TossCoin to do, but what it actually does is return 1 with probability .3, just like Bernoulli itself.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-30 22:50:36","Question_id":232596}
{"_id":{"$oid":"5837a572a05283111e4d2b86"},"Last_activity":"2014-11-19 06:57:33","Creator_reputation":77,"Question_score":2,"Answer_content":"Can't add it as a comment, so here it comes as an answer: As user28 already said, the marginal mean refers to the mean of a factor level, which - in a cross-table - is at the table's margins, hence the name marginal mean. Why this term is not entirely redundant? \"Mean\" could mean just any mean, e.g. the mean of all right handed men in the example of user28. By saying \"mean of factor A\" you should mean the mean of all levels of factor A, but you could mean (or be misunderstood as meaning) the mean of one level of factor A. Using the term \"marginal mean of factor A\" makes it unambiguously clear what you mean.","Display_name":"evoked_potential","Creater_id":46022,"Start_date":"2014-11-19 06:57:33","Question_id":4245}
{"_id":{"$oid":"5837a572a05283111e4d2b87"},"Last_activity":"2010-11-05 10:00:14","Creator_reputation":14458,"Question_score":3,"Answer_content":"I'd assume it means the sample analogue of the marginal expectation , as opposed to the sample analogue of a conditional expectation , where  could be anything.","Display_name":"onestop","Creater_id":449,"Start_date":"2010-11-05 09:42:02","Question_id":4245}
{"_id":{"$oid":"5837a572a05283111e4d2b88"},"Last_activity":"2010-11-05 08:01:44","Creator_reputation":null,"Question_score":6,"Answer_content":"Perhaps, the term originates from how the data is represented in a contingency table. See this example from the wiki.In the above example, we would speak of marginal totals for gender and handedness when referring to the last column and the bottom row respectively. If you see the wiktionary the first definition of marginal is:    of, relating to, or located at a margin or an edge  Since the totals (and means if means are reported) are at the edge of the table they are referred to as marginal totals (and marginal means if the edges have means).","Display_name":"user28","Creater_id":null,"Start_date":"2010-11-05 08:01:44","Question_id":4245}
{"_id":{"$oid":"5837a572a05283111e4d2b99"},"Last_activity":"2016-08-30 19:59:48","Creator_reputation":19,"Question_score":1,"Answer_content":"An error is the difference between the observed value and the true value (very often unobserved, generated by the DGP).A residual is the difference between the observed value and the predicted value (by the model).","Display_name":"Leopold W.","Creater_id":117930,"Start_date":"2016-08-30 19:59:48","Question_id":133389}
{"_id":{"$oid":"5837a572a05283111e4d2b9a"},"Last_activity":"2015-04-28 06:59:07","Creator_reputation":75690,"Question_score":6,"Answer_content":"Errors pertain to the true data generating process (DGP), whereas residuals are what is left over after having estimated your model.  In truth, assumptions like normality, homoscedasticity, and independence apply to the errors of the DGP, not your model's residuals.  (For example, having fit  parameters in your model, only  residuals can be independent.)  However, we only have access to the residuals, so that's what we work with.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2015-01-14 08:40:03","Question_id":133389}
{"_id":{"$oid":"5837a572a05283111e4d2ba9"},"Last_activity":"2016-08-30 19:44:18","Creator_reputation":1632,"Question_score":0,"Answer_content":"Suggestion: Histograms usually only assign the x-axis data to have occurred at the midpoint of the bin and omit x-axis measures of location of greater accuracy. The effect this has on the derivatives of fit can be quite large. Let us take a trivial example. Suppose we take the classical derivation of a Dirac delta but modify it so that we start with a Cauchy distribution at some arbitrary median location with a finite scale (full width half-maximum). Then we take the limit as the scale goes to zero. If we use the classical definition of a histogram and do not change bin sizes we will capture neither the location or the scale. If however, we use a median location within bins of even of fixed width, we will always capture the location, if not the scale when the scale is small relative to the bin width.For fitting values where the data is skewed, using fixed bin midpoints will x-axis shift the entire curve segment in that region, which I believe relates to the question above. STEP 1 Here is an almost solution. I used  in each histogram category, and just displayed these as the mean x-axis value from each bin. Since each histogram bin has a value of 8, the distributions all look uniform, and I had to offset them vertically to show them. The display is not the correct answer, but it is not without information. It correctly tells us that there is an x-axis offset between groups. It also tells us that the actual distribution appears to be slightly U shaped. Why? Note that the distance between mean values is further apart in the centers, and closer at the edges. So, to make this a better representation, we should borrow whole samples and fractional amounts of each bin boundary sample to make all the mean bin values on the x-axis equidistant. Fixing this and displaying it properly would require a bit of programming. But, it may just be a way to make histograms so that they actually display the underlying data in some logical format. The shape will still change if we change the total number of bins covering the range of the data, but the idea is to resolve some of the problems created by binning arbitrarily.STEP 2 So let's start borrowing between bins to try to make the means more evenly spaced. Now, we can see the shape of the histograms beginning to emerge. But the difference between means is not perfect as we only have whole numbers of samples to swap between bins. To remove the restriction of integer values on the y-axis and complete the process of making equidistant x-axis mean values, we have to start sharing fractions of a sample between bins.Step 3 The sharing of values and parts of values.  As one can see, the sharing of parts of a value at a bin boundry can improve the uniformity of distance between mean values. I managed to do this to three decimal places with the data given. However, one cannot, I do not think, make the distance between mean values exactly equal in general, as the coarseness of the data will not permit that. One can, however, do other things like use kernel density estimation. Here we see Annie's data as a bounded kernel density using Gaussian smoothings of 0.1, 0.2, and 0.4. The other subjects will have shifted functions of the same type, provided one does the same thing as I did, namely use the lower and upper bounds of each data set. So, this is no longer a histogram, but a PDF, and it serves the same role as a histogram without some of the warts.","Display_name":"Carl","Creater_id":99274,"Start_date":"2016-08-27 13:10:03","Question_id":51718}
{"_id":{"$oid":"5837a572a05283111e4d2baa"},"Last_activity":"2015-03-23 06:10:42","Creator_reputation":152613,"Question_score":142,"Answer_content":"This problem has long been known*, though perhaps not as widely as it should be -- you rarely see it mentioned in elementary-level discussions (though there are exceptions).* for example, Paul Rubin[1] put it this way: \"it's well known that changing the endpoints in a histogram can significantly alter its appearance\". .I think it's an issue that should be more widely discussed when introducing histograms. I'll give some examples and discussion.Why you should really be wary of relying on a single histogram of a data setTake a look at these four histograms:That's four very different looking histograms.If you paste the following data in (I'm using R here):Annie \u0026lt;- c(3.15,5.46,3.28,4.2,1.98,2.28,3.12,4.1,3.42,3.91,2.06,5.53,5.19,2.39,1.88,3.43,5.51,2.54,3.64,4.33,4.85,5.56,1.89,4.84,5.74,3.22,5.52,1.84,4.31,2.01,4.01,5.31,2.56,5.11,2.58,4.43,4.96,1.9,5.6,1.92)Brian \u0026lt;- c(2.9, 5.21, 3.03, 3.95, 1.73, 2.03, 2.87, 3.85, 3.17, 3.66, 1.81, 5.28, 4.94, 2.14, 1.63, 3.18, 5.26, 2.29, 3.39, 4.08, 4.6, 5.31, 1.64, 4.59, 5.49, 2.97, 5.27, 1.59, 4.06, 1.76, 3.76, 5.06, 2.31, 4.86, 2.33, 4.18, 4.71, 1.65, 5.35, 1.67)Chris \u0026lt;- c(2.65, 4.96, 2.78, 3.7, 1.48, 1.78, 2.62, 3.6, 2.92, 3.41, 1.56, 5.03, 4.69, 1.89, 1.38, 2.93, 5.01, 2.04, 3.14, 3.83, 4.35, 5.06, 1.39, 4.34, 5.24, 2.72, 5.02, 1.34, 3.81, 1.51, 3.51, 4.81, 2.06, 4.61, 2.08, 3.93, 4.46, 1.4, 5.1, 1.42)Zoe \u0026lt;- c(2.4, 4.71, 2.53, 3.45, 1.23, 1.53, 2.37, 3.35, 2.67, 3.16, 1.31, 4.78, 4.44, 1.64, 1.13, 2.68, 4.76, 1.79, 2.89, 3.58, 4.1, 4.81, 1.14, 4.09, 4.99, 2.47, 4.77, 1.09, 3.56, 1.26, 3.26, 4.56, 1.81, 4.36, 1.83, 3.68, 4.21, 1.15, 4.85, 1.17)Then you can generate them yourself:opar\u0026lt;-par()par(mfrow=c(2,2))hist(Annie,breaks=1:6,main=\"Annie\",xlab=\"V1\",col=\"lightblue\")hist(Brian,breaks=1:6,main=\"Brian\",xlab=\"V2\",col=\"lightblue\")hist(Chris,breaks=1:6,main=\"Chris\",xlab=\"V3\",col=\"lightblue\")hist(Zoe,breaks=1:6,main=\"Zoe\",xlab=\"V4\",col=\"lightblue\")par(opar)Now look at this strip chart:x\u0026lt;-c(Annie,Brian,Chris,Zoe)g\u0026lt;-rep(c('A','B','C','Z'),each=40)stripchart(x~g,pch='|')abline(v=(5:23)/4,col=8,lty=3)abline(v=(2:5),col=6,lty=3)(If it's still not obvious, see what happens when you subtract Annie's data from each set: head(matrix(x-Annie,nrow=40)))The data has simply been shifted left each time by 0.25.Yet the impressions we get from the histograms - right skew, uniform, left skew and bimodal - were utterly different. Our impression was entirely governed by the location of the first bin-origin relative to the minimum.So not just 'exponential' vs 'not-really-exponential' but 'right skew' vs 'left skew' or 'bimodal' vs 'uniform' just by moving where your bins start.Edit: If you vary the binwidth, you can get stuff like this happen:That's the same 34 observations in both cases, just different breakpoints, one with binwidth  and the other with binwidth .x \u0026lt;- c(1.03, 1.24, 1.47, 1.52, 1.92, 1.93, 1.94, 1.95, 1.96, 1.97, 1.98,   1.99, 2.72, 2.75, 2.78, 2.81, 2.84, 2.87, 2.9, 2.93, 2.96, 2.99, 3.6,   3.64, 3.66, 3.72, 3.77, 3.88, 3.91, 4.14, 4.54, 4.77, 4.81, 5.62)hist(x,breaks=seq(0.3,6.7,by=0.8),xlim=c(0,6.7),col=\"green3\",freq=FALSE)hist(x,breaks=0:8,col=\"aquamarine\",freq=FALSE)Nifty, eh? Yes, those data were deliberately generated to do that... but the lesson is clear - what you think you see in a histogram may not be a particularly accurate impression of the data.At the very least, you should always do histograms at several different binwidths or binorigins, or preferably both. Or check a kernel density estimate at not-too-wide a bandwidth.One approach that reduces the arbitrariness of histograms is averaged shifted histograms, (that's one on that most recent set of data) but if you go to that effort, I think you might as well use a kernel density estimate. I prefer to do several histograms in any case, and to err toward using more bins than typical defaults tend to give.While histograms may sometimes be misleading, boxplots are even more prone to such problems; with a boxplot you don't even have the ability to say \"use more bins\". See the four very different data sets in this post, all with identical, symmetric boxplots, even though one of the data sets is quite skew.[1]: Rubin, Paul (2014) \"Histogram Abuse!\", Blog post, OR in an OB world, Jan 23 2014 link ... (alternate link)","Display_name":"Glen_b","Creater_id":805,"Start_date":"2013-03-09 00:00:59","Question_id":51718}
{"_id":{"$oid":"5837a572a05283111e4d2bab"},"Last_activity":"2013-04-18 12:11:32","Creator_reputation":33186,"Question_score":25,"Answer_content":"A kernel density or logspline plot may be a better option compared to a histogram.  There are still some options that can be set with these methods, but they are less fickle than histograms.  There are qqplots as well.  A nice tool for seeing if data is close enough to a theoretical distribution is detailed in: Buja, A., Cook, D. Hofmann, H., Lawrence, M. Lee, E.-K., Swayne, D.F and Wickham, H. (2009) Statistical Inference for exploratory data analysis and model diagnostics Phil. Trans. R. Soc. A 2009 367, 4361-4383 doi: 10.1098/rsta.2009.0120The short version of the idea (still read the paper for details) is that you generate data from the null distribution and create several plots one of which is the original/real data and the rest are simulated from the theoretical distribution.  You then present the plots to someone (possibly yourself) that has not seen the original data and see if they can pick out the real data.  If they cannot identify the real data then you don't have evidence against the null.The vis.test function in the TeachingDemos package for R help implement a form of this test.Here is a quick example.  One of the plots below is 25 points generated from a t distribution with 10 degrees of freedom, the other 8 are generated from a normal distribution with the same mean and variance.The vis.test function created this plot and then prompts the user to choose which of the plots they think is different, then repeats the process 2 more times (3 total).  ","Display_name":"Greg Snow","Creater_id":4505,"Start_date":"2013-03-08 11:38:48","Question_id":51718}
{"_id":{"$oid":"5837a572a05283111e4d2bac"},"Last_activity":"2013-04-12 11:42:15","Creator_reputation":331,"Question_score":23,"Answer_content":"Cumulative distribution plots [MATLAB, R] – where you plot the fraction of data values less than or equal to a range of values – are by far the best way to look at distributions of empirical data. Here, for example, are the ECDFs of this data, produced in R:This can be generated with the following R input (with the above data):plot(ecdf(Annie),xlim=c(min(Zoe),max(Annie)),col=\"red\",main=\"ECDFs\")lines(ecdf(Brian),col=\"blue\")lines(ecdf(Chris),col=\"green\")lines(ecdf(Zoe),col=\"orange\")As you can see, it's visually obvious that these four distributions are simply translations of each other. In general, the benefits of ECDFs for visualizing empirical distributions of data are:They simply present the data as it actually occurs with no transformation other than accumulation, so there's no possibility of accidentally deceiving yourself, as there is with histograms and kernel density estimates, because of how you're processing the data.They give a clear visual sense of the distribution of the data since each point is buffered by all the data before and after it. Compare this with non-cumulative density visualizations, where the accuracy of each density is naturally unbuffered, and thus must be estimated either by binning (histograms) or smoothing (KDEs).They work equally well regardless of whether the data follows a nice parametric distribution, some mixture, or a messy non-parametric distribution.The only trick is learning how to read ECDFs properly: shallow sloped areas mean sparse distribution, steep sloped areas mean dense distribution. Once you get the hang of reading them, however, they're a wonderful tool for looking at distributions of empirical data.","Display_name":"StefanKarpinski","Creater_id":24295,"Start_date":"2013-04-12 11:25:35","Question_id":51718}
{"_id":{"$oid":"5837a572a05283111e4d2bb9"},"Last_activity":"2016-08-30 19:29:07","Creator_reputation":4408,"Question_score":4,"Answer_content":"My answer is not limit to K means, but check if we have curse of dimensionality for any distance based methods. K-means is based on a distance measure (for example, Euclidean distance)Before run the algorithm, we can check the distance metric distribution, i.e., all distance metrics for all pairs in of data. If you have  data points, you should have  distance metrics. If the data is too large, we can check a sample of that.If we have the curse of dimensionality problem, what you will see, is that these values are very close to each other. This seems very counter-intuitive, because it means every one is close or far away from every one and distance measure is basically useless.Here is some simulation to show you such counter-intuitive results. If all of the features are uniformly distributed, and if there are have too many dimensions, every distance metrics should be close to , which comes from . Feel free to change the uniform distribution to other distributions. For example, if we change to normal distribution (change runif to rnorm), it will converge to another number with large number dimensions.Here is the simulation for dimension from 1 to 500, the features are uniform distribution from 0 to 1.plot(0, type=\"n\",xlim=c(0,0.5),ylim=c(0,50))abline(v=1/6,lty=2,col=2)grid()n_data=1e3for (p in c(1:5,10,15,20,25,50,100,250,500)){    x=matrix(runif(n_data*p),ncol=p)    all_dist=as.vector(dist(x))^2/p    lines(density(all_dist))}","Display_name":"hxd1011","Creater_id":113777,"Start_date":"2016-08-30 09:55:56","Question_id":232500}
{"_id":{"$oid":"5837a572a05283111e4d2bba"},"Last_activity":"2016-08-30 12:16:04","Creator_reputation":75690,"Question_score":9,"Answer_content":"It helps to think about what The Curse of Dimensionality is.  There are several very good threads on CV that are worth reading.  Here is a place to start:  Explain “Curse of dimensionality” to a child.  I note that you are interested in how this applies to -means clustering.  It is worth being aware that -means is a search strategy to minimize (only) the squared Euclidean distance.  In light of that, it's worth thinking about how Euclidean distance relates to the curse of dimensionality (see: Why is Euclidean distance not a good metric in high dimensions?).  The short answer from these threads is that the volume (size) of the space increases at an incredible rate relative to the number of dimensions.  Even  dimensions (which doesn't seem like it's very 'high-dimensional' to me) can bring on the curse.  If your data were distributed uniformly throughout that space, all objects become approximately equidistant from each other.  However, as @Anony-Mousse notes in his answer to that question, this phenomenon depends on how the data are arrayed within the space; if they are not uniform, you don't necessarily have this problem.  This leads to the question of whether uniformly-distributed high-dimensional data are very common at all (see: Does “curse of dimensionality” really exist in real data?).  I would argue that what matters is not necessarily the number of variables (the literal dimensionality of your data), but the effective dimensionality of your data.  Under the assumption that  dimensions is 'too high' for -means, the simplest strategy would be to count the number of features you have.  But if you wanted to think in terms of the effective dimensionality, you could perform a principle components analysis (PCA) and look at how the eigenvalues drop off.  It is quite common that most of the variation exists in a couple of dimensions (which typically cut across the original dimensions of your dataset).  That would imply you are less likely to have a problem with -means in the sense that your effective dimensionality is actually much smaller.  A more involved approach would be to examine the distribution of pairwise distances in your dataset along the lines @hxd1011 suggests in his answer.  Looking at simple marginal distributions will give you some hint of the possible uniformity.  If you normalize all the variables to lie within the interval , the pairwise distances must lie within the interval .  Distances that are highly concentrated will cause problems; on the other hand, a multi-modal distribution may be hopeful (you can see an example in my answer here: How to use both binary and continuous variables together in clustering?).However, whether -means will 'work' is still a complicated question.  Under the assumption that there are meaningful latent groupings in your data, they don't necessarily exist in all of your dimensions or in constructed dimensions that maximize variation (i.e., the principle components).  The clusters could be in the lower-variation dimensions (see: Examples of PCA where PCs with low variance are “useful”).  That is, you could have clusters with points that are close within and well-separated between on just a few of your dimensions or on lower-variation PCs, but aren't remotely similar on high-variation PCs, which would cause -means to ignore the clusters you're after and pick out faux clusters instead (some examples can be seen here: How to understand the drawbacks of K-means).  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-08-30 10:18:51","Question_id":232500}
{"_id":{"$oid":"5837a573a05283111e4d2bcb"},"Last_activity":"2016-08-30 19:08:50","Creator_reputation":71,"Question_score":0,"Answer_content":"I try to show one aspect of similarity and difference in this example:We have a sample from a population,everybody agrees that Xbar is the best estimate we have for mu.Frequentist point of view: Xbar is a random variable that its probability density function is concentrated around mu. The constant mu is probably close to my xbar in this range called Confidence Interval.Bayesian point of view: Mu is an unknown variable for me, conditional to this data (my sample), if I bet that the population mean is close to Xbar within this range called Credible Interval, I expect to win most of the time. Great statisticians in the forum, please correct my answer if needed.Yours,Amir","Display_name":"Amir","Creater_id":128742,"Start_date":"2016-08-30 19:08:50","Question_id":232565}
{"_id":{"$oid":"5837a573a05283111e4d2bda"},"Last_activity":"2016-08-30 18:17:32","Creator_reputation":447,"Question_score":0,"Answer_content":"This is kind of an open ended, opinion question, but I think it's fair to say that some areas of intense active research include deep learning, convolutional neural networks, and various automated feature extraction principles. There is also much discussion around platforms for machine learning: the relative merits of R/Python/Matlab/Julia. There's no shortage of topics!","Display_name":"HEITZ","Creater_id":86794,"Start_date":"2016-08-30 18:17:32","Question_id":232566}
{"_id":{"$oid":"5837a573a05283111e4d2be7"},"Last_activity":"2016-08-30 18:14:32","Creator_reputation":39261,"Question_score":0,"Answer_content":"Just represent the baseline covatiate in the most flexible way that the sample size allows, perhaps using indicator variables in your example. If it was continuous you could use a spline.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2016-08-30 18:14:32","Question_id":232578}
{"_id":{"$oid":"5837a573a05283111e4d2bf4"},"Last_activity":"2016-08-30 17:59:13","Creator_reputation":763,"Question_score":0,"Answer_content":"@Bryan: I agree that the missing data will create a problem with classical test theory.  Both IRT and Rasch are quite robust to missing data however (the size of the measurement error just increases with missing data).  As long as you peform DIF within a Rasch or IRT framework you should be able to demonstrate DIF across class groups, sex, etc.  A search of the Rasch Measurement Transactions website should help you find suitable references.  ","Display_name":"doug.numbers","Creater_id":24341,"Start_date":"2016-08-30 17:59:13","Question_id":231297}
{"_id":{"$oid":"5837a573a05283111e4d2c0c"},"Last_activity":"2015-08-20 06:26:40","Creator_reputation":241,"Question_score":13,"Answer_content":"No-one has suggested ggplot2 for this??library(MASS)library(ggplot2)n \u0026lt;- 1000x \u0026lt;- mvrnorm(n, mu=c(.5,2.5), Sigma=matrix(c(1,.6,.6,1), ncol=2))df = data.frame(x); colnames(df) = c(\"x\",\"y\")commonTheme = list(labs(color=\"Density\",fill=\"Density\",                        x=\"RNA-seq Expression\",                        y=\"Microarray Expression\"),                   theme_bw(),                   theme(legend.position=c(0,1),                         legend.justification=c(0,1)))ggplot(data=df,aes(x,y)) +   geom_density2d(aes(colour=..level..)) +   scale_colour_gradient(low=\"green\",high=\"red\") +   geom_point() + commonThemeWhich produces the following:However, other stuff can be done too, quite easily, such as the following:ggplot(data=df,aes(x,y)) +   stat_density2d(aes(fill=..level..,alpha=..level..),geom='polygon',colour='black') +   scale_fill_continuous(low=\"green\",high=\"red\") +  geom_smooth(method=lm,linetype=2,colour=\"red\",se=F) +   guides(alpha=\"none\") +  geom_point() + commonThemeWhich produces the following:","Display_name":"ADP","Creater_id":17602,"Start_date":"2015-08-20 06:26:40","Question_id":31726}
{"_id":{"$oid":"5837a573a05283111e4d2c0d"},"Last_activity":"2012-07-05 14:20:44","Creator_reputation":37824,"Question_score":24,"Answer_content":"Here is my take, using base functions only for drawing stuff:library(MASS)  # in case it is not already loaded set.seed(101)n \u0026lt;- 1000X \u0026lt;- mvrnorm(n, mu=c(.5,2.5), Sigma=matrix(c(1,.6,.6,1), ncol=2))## some pretty colorslibrary(RColorBrewer)k \u0026lt;- 11my.cols \u0026lt;- rev(brewer.pal(k, \"RdYlBu\"))## compute 2D kernel density, see MASS book, pp. 130-131z \u0026lt;- kde2d(X[,1], X[,2], n=50)plot(X, xlab=\"X label\", ylab=\"Y label\", pch=19, cex=.4)contour(z, drawlabels=FALSE, nlevels=k, col=my.cols, add=TRUE)abline(h=mean(X[,2]), v=mean(X[,1]), lwd=2)legend(\"topleft\", paste(\"R=\", round(cor(X)[1,2],2)), bty=\"n\")For more fancy rendering, you might want to have a look at ggplot2 and stat_density2d(). Another function I like is smoothScatter():smoothScatter(X, nrpoints=.3*n, colramp=colorRampPalette(my.cols), pch=19, cex=.8)","Display_name":"chl","Creater_id":930,"Start_date":"2012-07-05 14:20:44","Question_id":31726}
{"_id":{"$oid":"5837a573a05283111e4d2c19"},"Last_activity":"2015-08-08 07:33:59","Creator_reputation":75690,"Question_score":7,"Answer_content":"You are right that k-means clustering should not be done with data of mixed types.  Since k-means is essentially a simple search algorithm to find a partition that minimizes the within-cluster squared Euclidean distances between the clustered observations and the cluster centroid, it should only be used with data where squared Euclidean distances would be meaningful.  When your data consist of variables of mixed types, you need to use Gower's distance.  CV user @ttnphns has a great overview of Gower's distance here.  In essence, you compute a distance matrix for your rows for each variable in turn, using a type of distance that is appropriate for that type of variable (e.g., Euclidean for continuous data, etc.); the final distance of row  to  is the (possibly weighted) average of the distances for each variable.  One thing to be aware of is that Gower's distance isn't actually a metric.  Nonetheless, with mixed data, Gower's distance is largely the only game in town.  At this point, you can use any clustering method that can operate over a distance matrix instead of needing the original data matrix.  (Note that k-means needs the latter.)  The most popular choices are partitioning around medoids (PAM, which is essentially the same as k-means, but uses the most central observation rather than the centroid), various hierarchical clustering approaches (e.g., median, single-linkage, and complete-linkage; with hierarchical clustering you will need to decide where to 'cut the tree' to get the final cluster assignments), and DBSCAN which allows much more flexible cluster shapes.  Here is a simple R demo (n.b., there are actually 3 clusters, but the data mostly look like 2 clusters are appropriate):  library(cluster)  # we'll use these packageslibrary(fpc)  # here we're generating 45 data in 3 clusters:set.seed(3296)    # this makes the example exactly reproduciblen      = 15cont   = c(rnorm(n, mean=0, sd=1),           rnorm(n, mean=1, sd=1),           rnorm(n, mean=2, sd=1) )bin    = c(rbinom(n, size=1, prob=.2),           rbinom(n, size=1, prob=.5),           rbinom(n, size=1, prob=.8) )ord    = c(rbinom(n, size=5, prob=.2),           rbinom(n, size=5, prob=.5),           rbinom(n, size=5, prob=.8) )data   = data.frame(cont=cont, bin=bin, ord=factor(ord, ordered=TRUE))  # this returns the distance matrix with Gower's distance:  g.dist = daisy(data, metric=\"gower\", type=list(symm=2))We can start by searching over different numbers of clusters with PAM:    # we can start by searching over different numbers of clusters with PAM:pc = pamk(g.dist, krange=1:5, criterion=\"asw\")pc[2:3]# crit# [1] 0.0000000 0.6227580 0.5593053 0.5011497 0.4294626pc = pcclustering)#    1  2# 1 22  0# 2  0 23table(pccluster)#    1  2# 1 22  0# 2  0 23table(cutree(hc.m, k=2), cutree(hc.c, k=2))#    1  2# 1 14  8# 2  7 16Of course, there is no guarantee that any cluster analysis will recover the true latent clusters in your data.  The absence of the true cluster labels (which would be available in, say, a logistic regression situation) means that an enormous amount of information is unavailable.  Even with very large datasets, the clusters may not be sufficiently well separated to be perfectly recoverable.  In our case, since we know the true cluster membership, we can compare that to the output to see how well it did.  As I noted above, there are actually 3 latent clusters, but the data give the appearance of 2 clusters instead:  pcclustering[16:30]   # these were actually cluster 2 in the data generating process# 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #  2  2  1  1  1  2  1  2  1  2  2  1  2  1  2 pc$clustering[31:45]   # these were actually cluster 3 in the data generating process# 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #  2  1  2  2  2  2  1  2  1  2  2  2  2  2  2 ","Display_name":"gung","Creater_id":7290,"Start_date":"2015-08-04 11:42:24","Question_id":130974}
{"_id":{"$oid":"5837a573a05283111e4d2c1a"},"Last_activity":"2015-05-16 11:18:39","Creator_reputation":146,"Question_score":0,"Answer_content":"Look at this paper by Finch, http://www.jds-online.com/files/JDS-192.pdf.  It describes both why applying continuous methods to binary data may inaccurately cluster the data, and more importantly what are some choices in appropriate distance functions.  It does not answer how to cluster with k-means, but rather how to properly cluster binary data using non-Euclidean metrics and a hierarchical method like Ward.","Display_name":"Chris","Creater_id":21827,"Start_date":"2015-05-16 11:18:39","Question_id":130974}
{"_id":{"$oid":"5837a573a05283111e4d2c27"},"Last_activity":"2016-08-29 13:41:00","Creator_reputation":17344,"Question_score":0,"Answer_content":"I would agree with the definition you've encountered in the WLLN rather than their definition of a \"population mean\" as being a \"sample mean of the entire population\". They differ in some important ways. Suppose for instance that I conduct a survey which samples the first 100 people who come through the mall on a Saturday. My experiment does not generalize to all mall patrons, or even the first 100 to come through on any other day of the week. An important distinction in frequentist statistics is that of an infinitely sized \"population\". In my mall survey, the \"population\" would be defined as an infinite number of independent replications of my experiment in which I \"rewound time\" and did my survey again and again and again. This is counterfactual reasoning. If you sample 100% of a finite population without replacement and obtain a sample mean  you can still replicate that experiment an infinite number of times and sample people again and again and again obtaining sample mean  an infinite number of times. So a proper finite sample CI would be  to .The answer above obviates this.The population characteristic does not have a distribution. By \"values obtained\" do you mean the estimand? In statistics, if a sample is obtained from IID observations with a mean , then the sample average will also have mean , that's simply a result of linear operators.Practically, it would not be an estimate if you sampled everybody. You can safely say that you have found the \"true mean\". RVs are mappings or functions, they have nothing to do with sample/population distinctions. I think what you are saying here is a mistake. A parameter in a model may be the mean, and we may call that  (or  or... there is no conventional notation here), but there are other distributions with different parameters like shape, scale, shift, or rate parameters which we may call  or  or... Obviously if you call a \"scale\" parameter  then no the sample mean does NOT estimate this value. However, if the mean is a well defined quantity, it is some function of the parameters and you will consistently estimate this value with the sample mean. ","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-29 11:26:35","Question_id":232300}
{"_id":{"$oid":"5837a573a05283111e4d2c34"},"Last_activity":"2016-08-30 12:54:56","Creator_reputation":110,"Question_score":0,"Answer_content":"Firstly, the z or t value (depending on what family you run) is the coefficient divided by the standard error. The p value is then derived from the normal or t distributions using this z or t value.The stars don't really add much in my view. You will see underneath the table of coefficients that there is a line which starts 'Signif. codes'. This gives the key. So a coefficient marked *** is one whose p value \u0026lt; 0.001. One whose coefficient is marked ** is p \u0026lt; 0.01. And so on.For example (taken from http://www.ats.ucla.edu/stat/r/dae/logit.htm):mydata \u0026lt;- read.csv(\"http://www.ats.ucla.edu/stat/data/binary.csv\")mydatarank)mylogit \u0026lt;- glm(admit ~ gre + gpa + rank, data = mydata, family = \"binomial\")summary(mylogit)Gives the following output:Call:glm(formula = admit ~ gre + gpa + rank, family = \"binomial\",     data = mydata)Deviance Residuals:     Min       1Q   Median       3Q      Max  -1.6268  -0.8662  -0.6388   1.1490   2.0790  Coefficients:             Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept) -3.989979   1.139951  -3.500 0.000465 ***gre          0.002264   0.001094   2.070 0.038465 *  gpa          0.804038   0.331819   2.423 0.015388 *  rank2       -0.675443   0.316490  -2.134 0.032829 *  rank3       -1.340204   0.345306  -3.881 0.000104 ***rank4       -1.551464   0.417832  -3.713 0.000205 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 499.98  on 399  degrees of freedomResidual deviance: 458.52  on 394  degrees of freedomAIC: 470.52Number of Fisher Scoring iterations: 4You can see that gre has a p value = 0.038. This has one asterisk by it because that is \u0026lt; 0.05. rank4 has a p value = 0.0002 and so has three asterisks because this is \u0026lt; 0.001.I just use the asterisks to quickly scan the table but I never look at them beyond that.","Display_name":"Matthew","Creater_id":35556,"Start_date":"2016-08-30 12:54:56","Question_id":232548}
{"_id":{"$oid":"5837a573a05283111e4d2c41"},"Last_activity":"2016-08-30 12:34:04","Creator_reputation":518,"Question_score":4,"Answer_content":"Generally, if a coefficient is statistically insignificant, then we say the estimate is too imprecise to make any claim about suggestive correlation. However you should do a hypothesis test on the effect of x being positive, not just zero (which is likely what your regression output gave you). This may change your inference. So , with  ","Display_name":"VCG","Creater_id":124896,"Start_date":"2016-08-30 12:34:04","Question_id":232537}
{"_id":{"$oid":"5837a573a05283111e4d2c50"},"Last_activity":"2016-08-30 12:07:50","Creator_reputation":7559,"Question_score":5,"Answer_content":"Your code is somehow bugged, and the sign shouldn't change.The ordinary least squares estimator is: b = (X'X)^{-1} X'y Let  be some invertible linear transformation. Our transformed data is: \\hat{X} =  X AThe OLS estimator on the transformed data is: \\begin{align*} \\hat{b} \u0026amp;= (A'X'XA)^{-1} A'X'y \\\\\u0026amp;= A^{-1} (X'X)^{-1} A'^{-1}A'X'y \\\\\u0026amp;= A^{-1} b \\end{align*}The coefficients  you estimate using the transformed data should be a linear transformation (using ) of the coefficients  you estimate using the original data.If you're simply standardizing , what should  look like? If the last column of  is a column of ones (because a constant is including in the regression), then  would be something like: A = \\left[ \\begin{array}{cccc} \\frac{1}{\\sigma_x} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{\\sigma_y} \u0026amp; 0 \\\\ - \\frac{\\mu_x}{\\sigma_x} \u0026amp; - \\frac{\\mu_y}{\\sigma_y} \u0026amp; 1  \\end{array} \\right] Multiplying  by  is basically equivalent to subtracting the mean for each non-constant column and dividing by the standard deviation. A^{-1} = \\left[ \\begin{array}{cccc} {\\sigma_x} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; {\\sigma_y} \u0026amp; 0 \\\\ \\mu_x \u0026amp; \\mu_y \u0026amp; 1  \\end{array} \\right]  \\hat{b}_x = \\sigma_x b_x \\quad \\quad \\hat{b}_y = \\sigma_y b_y  \\quad \\quad \\hat{b}_1 = b_1 + b_x \\mu_x + b_y \\mu_y And so the sign of estimates for variables shouldn't change (except the sign for the constant may change). If you don't include a constant though in the regression, all bets are off.","Display_name":"Matthew Gunn","Creater_id":97925,"Start_date":"2016-08-30 01:50:47","Question_id":232433}
{"_id":{"$oid":"5837a573a05283111e4d2c5d"},"Last_activity":"2016-08-30 11:51:49","Creator_reputation":250,"Question_score":0,"Answer_content":"It seems like a newer method. But what do you accomplish by doing it? What exactly is your question?My suggestion is to not use biased/stratified sampling (or sample subsets of dependent records), but rather account for the dependence using mixed models/multilevel models or clustered errors.You can find several texts for mixed models and some SO discussions on regression with clustered errors.In your approach, you may loose information while sampling from groups.","Display_name":"Earnest_learner","Creater_id":41289,"Start_date":"2016-08-30 11:51:49","Question_id":232511}
{"_id":{"$oid":"5837a573a05283111e4d2c6e"},"Last_activity":"2016-08-30 10:15:52","Creator_reputation":120,"Question_score":0,"Answer_content":"You can compare slopes and intercepts using dummy variables.  You need one less dummy variable that the number of regressions you are comparing.  If you have two regression lines the dummy variable  has values of  or .  Assuming you have multiple sample  and , some of them will be from one group, lets call it , the others will be from another .  To the set of  -  variables you would add  with value  for observations from  and value  for observations from . You now fit a model where  ~  and .  Multiple regression will give you coefficients  and  for the , ,  and .  The equation just fitted is     For   where  the  equation is    (I)  For   where  the equation is    (II)      The intercept is , the slope is   If the coefficient for  is statistically significant then the slopes and intercepts are calculated as in (II), otherwise there is no differences and the model is (I).The process can be extended to as many groups just adding more dummy variables , etc.    If you have 3 groups as you describe this is the table for the dummy variables values:        The equation to be fitted is    ~   The regression equation will be:                     is the slope;   is the intercept.           is the slope;   is the intercept.  Hope this helps.  Is quite straightforward.","Display_name":"LDBerriz","Creater_id":128189,"Start_date":"2016-08-27 06:06:40","Question_id":232040}
{"_id":{"$oid":"5837a573a05283111e4d2c7b"},"Last_activity":"2016-08-30 09:22:21","Creator_reputation":1,"Question_score":0,"Answer_content":"I found this rather intuitive:  The perplexity of whatever you're evaluating, on the data you're  evaluating it on, sort of tells you \"this thing is right about as  often as an x-sided die would be.\"http://planspace.org/2013/09/23/perplexity-what-it-is-and-what-yours-is/","Display_name":"pandasEverywhere","Creater_id":129507,"Start_date":"2016-08-30 09:22:21","Question_id":10302}
{"_id":{"$oid":"5837a573a05283111e4d2c7c"},"Last_activity":"2011-05-05 00:07:56","Creator_reputation":13273,"Question_score":6,"Answer_content":"You have looked at the Wikipedia article on perplexity. It gives the perplexity of a discrete distribution as 2^{-\\sum_x p(x)\\log_2 p(x)} which could also be written as \\exp\\left({\\sum_x p(x)\\log_e \\frac{1}{p(x)}}\\right)  i.e. as a weighted geometric average of the inverses of the probabilities. For a continuous distribution, the sum would turn into a integral.The article also gives a way of estimating perplexity for a model using  pieces of test data 2^{-\\sum_{i=1}^N \\frac{1}{N} \\log_2 q(x_i)} which could also be written \\exp\\left(\\frac{{\\sum_{i=1}^N \\log_e \\left(\\dfrac{1}{q(x_i)}\\right)}}{N}\\right) \\text{ or } \\sqrt[N]{\\prod_{i=1}^N  \\frac{1}{q(x_i)}}or in a variety of other ways, and this should make it even clearer where \"log-average inverse probability\" comes from. ","Display_name":"Henry","Creater_id":2958,"Start_date":"2011-05-05 00:07:56","Question_id":10302}
{"_id":{"$oid":"5837a573a05283111e4d2c89"},"Last_activity":"2016-08-30 09:21:07","Creator_reputation":1294,"Question_score":0,"Answer_content":"There's a few things that confuse me by your comments and question. Instead of \"I would like to find the posterior density of the parameters to conduct Gibbs sampling\" I assume you mean that you would like to conduct Gibbs sampling in order to sample from  and thereby have draws from . Gibbs amounts to alternatively drawing  and then . Doing this gives you draws from . You can integrate out  if you're only interested in  (throw away parts of the joint samples). You don't want to deal with  at all, really. It's intractable.Both parts are relatively easy, though. Because both of these distributions are closed form. Drawing paths can be accomplished by taking means and covariances from kalman smoother (not filter, another thing that was mistaken in your comments), and using those to draw from a big multivariate normal. Page 391 of the book I linked below mentions the forward backward algorithm in Frühwirth-Schnatter, S. (1994), DATA AUGMENTATION AND DYNAMIC LINEAR MODELS to do this.The other part you need to draw from . I am assuming it's closed-form, although I haven't worked it out completely. And you didn't give the priors exactly anyway.Check out page 390 and 391 of http://www.springer.com/us/book/9781441978646 for more details.","Display_name":"Taylor","Creater_id":8336,"Start_date":"2016-08-30 09:21:07","Question_id":229134}
{"_id":{"$oid":"5837a573a05283111e4d2c98"},"Last_activity":"2016-08-29 11:22:36","Creator_reputation":8337,"Question_score":0,"Answer_content":"I'm not totally sure what \"cross-sectional dependence\" means, but I assume it means dependence between subjects, as opposed to the dependence between timepoints that you naturally expect in a longitudinal setting. In that case, I agree with you: sampling subjects who are grouped into households means that there is dependence between subjects in the same household. (E.g., knowing a father's height tells you something about his son's height.) A good way to deal with this in a mixed-model context is to have a per-household random effect. It might be that accounting for this dependency won't meaningfully change the conclusions of your analysis, and so it can be safely ignored—but there's no real way to tell in advance, so it makes more sense not to ignore it.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-29 11:22:36","Question_id":232340}
{"_id":{"$oid":"5837a573a05283111e4d2ca5"},"Last_activity":"2012-02-13 14:19:37","Creator_reputation":15542,"Question_score":4,"Answer_content":"I found The Workflow of Data Analysis Using Stata to be a good book, particularly (but non only) as a Stata user. I found much with which to disagree, but even that helped clarify why I do things certain ways.","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2012-02-13 14:19:37","Question_id":22697}
{"_id":{"$oid":"5837a573a05283111e4d2ca6"},"Last_activity":"2012-02-13 09:51:55","Creator_reputation":764,"Question_score":2,"Answer_content":"CRISP-DM, coined by SPSS company (now belongs to IBM) is an acronym for the data mining process, which is the same as for \"data analysis\". SAS has a similar process called SEMMA.","Display_name":"Galit Shmueli","Creater_id":1945,"Start_date":"2012-02-13 09:51:55","Question_id":22697}
{"_id":{"$oid":"5837a573a05283111e4d2ca7"},"Last_activity":"2012-02-12 20:26:12","Creator_reputation":3211,"Question_score":17,"Answer_content":"My favorite \"plan\" or \"list\" is Scott Emerson's document Organizing Your Approach to a Data Analysis.Note: the last two pages are under the heading \"General Requirements for Ph.D. Applied Exam\" but the advice given there generalizes to working on any analysis problem.","Display_name":"Mike Wierzbicki","Creater_id":5594,"Start_date":"2012-02-12 20:26:12","Question_id":22697}
{"_id":{"$oid":"5837a573a05283111e4d2cb2"},"Last_activity":"2016-08-30 08:25:30","Creator_reputation":12897,"Question_score":1,"Answer_content":"ARMAConsider  that follows an ARMA() process. Suppose for simplicity it has zero mean. Then y_t = \\varphi_1 y_{t-1} + \\dotsc + \\varphi_p y_{t-p} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dotsc + \\theta_q \\varepsilon_{t-q} where  for some density  with zero mean and variance . This defines the ARMA() process, if I am not mistaken.Conditional on information ,  can be partitioned into a known (predetermined) part  (which is the conditional mean of  given ) and a random part :\\begin{aligned} y_t \u0026amp;= \\mu_t + \\varepsilon_t; \\\\\\mu_t \u0026amp;= \\varphi_1 y_{t-1} + \\dotsc + \\varphi_p y_{t-p} + \\theta_1 \\varepsilon_{t-1} + \\dotsc + \\theta_q \\varepsilon_{t-q} \\ \\ \\text{(known, predetermined)}; \\\\\\varepsilon_t \u0026amp;~\\sim D(0,\\sigma^2) \\ \\ \\text{(random)}. \\\\\\end{aligned}The conditional mean  itself follows a process similar to ARMA() but without the random contemporaneous error term:  \\mu_t = \\varphi_1 \\mu_{t-1} + \\dotsc + \\varphi_p \\mu_{t-p} + (\\varphi_1 + \\theta_1) \\varepsilon_{t-1} + \\dotsc + (\\varphi_m + \\theta_m) \\varepsilon_{t-m}, where ;  for ; and  for . Note that this process has order () rather than () as does .We can write the conditional distribution of  in terms of its past conditional means (rather than past realized values) and model parameters as\\begin{aligned}y_t \u0026amp;\\sim D(\\mu_t,\\sigma_t^2); \\\\\\mu_t \u0026amp;= \\varphi_1 \\mu_{t-1} + \\dotsc + \\varphi_p \\mu_{t-p} + (\\varphi_1 + \\theta_1) \\varepsilon_{t-1} + \\dotsc + (\\varphi_m + \\theta_m) \\varepsilon_{t-m}; \\\\\\sigma_t^2 \u0026amp;= \\sigma^2. \\\\\\end{aligned}GARCHConsider  that follows a GARCH() process. Suppose for simplicity it has zero mean. Then\\begin{aligned}y_t \u0026amp;\\sim D(\\mu_t,\\sigma_t^2); \\\\\\mu_t \u0026amp;= 0; \\\\\\sigma_t^2 \u0026amp;= \\omega + \\alpha_1 \\varepsilon_{t-1}^2 + \\dotsc + \\alpha_s \\varepsilon_{t-s}^2 + \\beta_1 \\sigma_{t-1}^2 + \\dotsc + \\beta_r \\sigma_{t-r}^2. \\\\\\end{aligned}where  is some density.The conditional variance  follows a process similar to ARMA() but without the random contemporaneous error term.ARMA-GARCHConsider  that has unconditional mean zero and follows an ARMA()-GARCH() process. Then\\begin{aligned}y_t \u0026amp;\\sim D(\\mu_t,\\sigma_t^2); \\\\\\mu_t \u0026amp;= \\varphi_1 \\mu_{t-1} + \\dotsc + \\varphi_p \\mu_{t-p} + (\\varphi_1 + \\theta_1) \\varepsilon_{t-1} + \\dotsc + (\\varphi_m + \\theta_m) \\varepsilon_{t-m}; \\\\\\sigma_t^2 \u0026amp;= \\omega + \\alpha_1 \\varepsilon_{t-1}^2 + \\dotsc + \\alpha_s \\varepsilon_{t-s}^2 + \\beta_1 \\sigma_{t-1}^2 + \\dotsc + \\beta_r \\sigma_{t-r}^2; \\\\\\end{aligned}where  is some density, e.g. Normal;  for ; and  for .The conditional mean process due to ARMA has essentially the same shape as the conditional variance process due to GARCH, just the lag orders may differ (allowing for a nonzero unconditional mean of  should not change this result significantly). Importantly, neither has random error terms once conditioned on , thus both are predetermined.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-24 10:38:04","Question_id":41509}
{"_id":{"$oid":"5837a573a05283111e4d2cb3"},"Last_activity":"2016-08-24 10:47:37","Creator_reputation":12897,"Question_score":9,"Answer_content":"Edit: I realized the answer was lacking and have thus provided a more precise answer (see below -- or maybe above). I have edited this one for factual mistakes and am leaving it for the record.Different focus parameters:ARMA is a model for the realizations of a stochastic process with focus on the conditional mean.GARCH is a model for the conditional variance of a process.Stochastic versus deterministic model:ARMA is a stochastic model in the sense that the conditional mean dependent variable -- the realizations of the stochastic process -- is specified as a sum of a deterministic function of lagged dependent variable and lagged model error (the conditional mean) and a stochastic error term. GARCH is a deterministic model in the sense that the dependent variable -- the conditional variance of the process -- is a purely deterministic function of lagged variables.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2015-11-20 06:50:19","Question_id":41509}
{"_id":{"$oid":"5837a573a05283111e4d2cb4"},"Last_activity":"2014-01-04 03:48:00","Creator_reputation":29,"Question_score":2,"Answer_content":"The ARMA and GARCH processes are very similar in their presentation. The dividing line between the two is very thin since we get GARCH when an ARMA process is assumed for the error variance.","Display_name":"user36853","Creater_id":36853,"Start_date":"2014-01-04 03:48:00","Question_id":41509}
{"_id":{"$oid":"5837a573a05283111e4d2cb5"},"Last_activity":"2012-10-30 09:42:16","Creator_reputation":4775,"Question_score":27,"Answer_content":"You are conflating the features of a process with its representation. Consider the (return) process . An ARMA(p,q) model specifies the conditional mean of the process as\\begin{align}\\mathbb{E}(Y_t \\mid \\mathcal{I}_t) \u0026amp;= \\alpha_0 + \\sum_{j=1}^p \\alpha_j Y_{t-j}+ \\sum_{k=1}^q \\beta_k\\epsilon_{t-k}\\\\\\end{align}Here,  is the information set at time , which is the -algebra generated by the lagged values of the outcome process . The GARCH(r,s) model specifies the conditional variance of the process\\begin{alignat}{2}\u0026amp; \\mathbb{V}(Y_t \\mid \\mathcal{I}_t) \u0026amp;{}={}\u0026amp; \\mathbb{V}(\\epsilon_t \\mid \\mathcal{I}_t) \\\\\\equiv \\,\u0026amp; \\sigma^2_t\u0026amp;{}={}\u0026amp; \\delta_0 + \\sum_{l=1}^r \\delta_j \\sigma^2_{t-l} + \\sum_{m=1}^s \\gamma_k Y^2_{t-m}\\end{alignat}Note in particular the first equivalence .Aside: Based on this representation, you can write\\epsilon_t \\equiv \\sigma_t Z_twhere  is a strong white noise process, but this follows from the way the process is defined.The two models (for the conditional mean and the variance) are perfectly compatible with each other, in that the mean of the process can be modeled as ARMA, and the variances as GARCH. This leads to the complete specification of an ARMA(p,q)-GARCH(r,s) model for the  process as in the following representation\\begin{align}Y_t \u0026amp;= \\alpha_0 + \\sum_{j=1}^p \\alpha_j Y_{t-j} + \\sum_{k=1}^q \\beta_k\\epsilon_{t-k} +\\epsilon_t\\\\ \\mathbb{E}(\\epsilon_t\\mid \\mathcal{I}_t) \u0026amp;=0,\\, \\forall t \\\\\\mathbb{V}(\\epsilon_t \\mid \\mathcal{I}_t) \u0026amp;= \\delta_0 + \\sum_{l=1}^r \\delta_l \\sigma^2_{t-l} + \\sum_{m=1}^s \\gamma_m Y^2_{t-m}\\, \\forall t\\end{align}","Display_name":"tchakravarty","Creater_id":8141,"Start_date":"2012-10-30 08:01:24","Question_id":41509}
{"_id":{"$oid":"5837a573a05283111e4d2cc4"},"Last_activity":"2016-08-30 08:18:41","Creator_reputation":416,"Question_score":1,"Answer_content":"glmnet does not take this into account when assigning folds of cross-validation.  If it has a fold with too few samples from one class, it will make the model, but it will throw a warning.  In the example below we train a binomial classifier with 98 examples from class A and 2 from class B, making it impossible for most folds to contain an example of class B.cv.glmnet(matrix(rnorm(200),ncol=2),           c(rep(\"A\", 97),rep(\"B\",3)),           family = \"binomial\", nfolds = 10)The model is built, but it gives the following warning repeated 11 timesIn lognet(x, is.sparse, ix, jx, y, weights, offset, alpha,  ... :  one multinomial or binomial class has fewer than 8  observations;   dangerous groundIf you want to protect from this you can manually assign your cross-validation folds using the foldid parameter. For example, if we have a set up like you described, we could do the following:# make our example datax \u0026lt;- matrix(rnorm(88),ncol=2)y \u0026lt;- c(rep(0, 30), rep(1, 14))nfold \u0026lt;- 5# assign folds evenly using the modulus operatorfold0 \u0026lt;- sample.int(sum(y==0)) %% nfoldfold1 \u0026lt;- sample.int(sum(y==1)) %% nfoldfoldid \u0026lt;- numeric(length(y))foldid[y==0] \u0026lt;- fold0foldid[y==1] \u0026lt;- fold1foldid \u0026lt;- foldid + 1# perform cross-validationcv.glmnet(x, y, foldid = foldid, family = \"binomial\")One other parameter you may want to consider is the weights parameter.  This parameter weights the error metric and can be used to help balance the classifier by increasing the impact of a misclassification on the class with lower representation.  To make the misclassification error balanced for the two classes you would set the weight for each instance equal to 1 - (fraction of total) of its class.# calculate what fraction of the total each class hasfraction \u0026lt;- table(y)/length(y)# assign 1 - that value to a \"weights\" vectorweights \u0026lt;- 1 - fraction[as.character(y)]# make the modelcv.glmnet(x, y, foldid = foldid, family = \"binomial\", weights = weights)","Display_name":"Barker","Creater_id":127913,"Start_date":"2016-08-29 23:13:58","Question_id":232228}
{"_id":{"$oid":"5837a573a05283111e4d2cd1"},"Last_activity":"2016-08-25 04:39:07","Creator_reputation":3327,"Question_score":1,"Answer_content":"LASSO solutions are solutions that minimizeQ(\\beta|X,y) = \\dfrac{1}{2n}||y-X\\beta||^2 + \\lambda\\sum_{j}|\\beta_j|the adaptive lasso simply adds weights to this to try to counteract the known issue of LASSO estimates being biased. Q_a(\\beta|X,y,w) = \\dfrac{1}{2n}||y-X\\beta||^2 + \\lambda\\sum_{j}w_j|\\beta_j|Often you will see , where  are some initial estimates of the  (maybe from just using LASSO, or using least squares, etc). Sometimes adaptive lasso is fit using a \"pathwise approach\" where the weight is allowed to change with w_j(\\lambda) = w(\\tilde{\\beta}_j(\\lambda)). In the  package the weights can be specified with the weights argument. I'm not sure if you can specify the \"pathwise approach\" in . ","Display_name":"bdeonovic","Creater_id":17661,"Start_date":"2016-08-25 03:56:53","Question_id":231643}
{"_id":{"$oid":"5837a573a05283111e4d2cd2"},"Last_activity":"2016-08-25 04:21:39","Creator_reputation":12897,"Question_score":1,"Answer_content":"Brief answers to your questions:Lasso and adaptive lasso are different. (Check Zou (2006) to see how adaptive lasso differs from standard lasso.)Lasso is a special case of elastic net. (See Zou \u0026amp; Hastie (2005).)Adaptive lasso is not a special case of elastic net.Elastic net is not a special case of lasso or adaptive lasso. Function glmnet in \"glmnet\" package in R performs lasso (not adaptive lasso) for alpha=1. Does lasso work under milder conditions than adaptive lasso? I cannot answer this one (should check Zou (2006) for insights).Only the adaptive lasso (but not lasso or elastic net) has the oracle property. (See Zou (2006).)References:Zou, Hui. \"The adaptive lasso and its oracle properties.\" Journal of the American Statistical Association 101.476 (2006): 1418-1429.Zou, Hui, and Trevor Hastie. \"Regularization and variable selection via the elastic net.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67.2 (2005): 301-320.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-25 03:55:27","Question_id":231643}
{"_id":{"$oid":"5837a573a05283111e4d2cdf"},"Last_activity":"2016-08-30 08:15:02","Creator_reputation":11335,"Question_score":1,"Answer_content":"If you want to find \"a possible range of values where this index could be useful\" then you should examine the continuous relation of your index to outcome. Looking for a single cut point will not accomplish that task. See this question for discussion in the general regression context and this question in a broader machine-learning context.For a novel index it's particularly important to understand how its values over its range are related to outcome. Maybe you will find that there is some upper or lower limit beyond which changes in the index don't matter, but you won't learn that unless you look in detail. It's also important to see whether the index adds anything to already established prognostic variables, something you can't do with a single-predictor Cox model.Furthermore, the word \"optimal\" can hide a lot of assumptions. In the classification context, it can make the assumption that false positives have the same costs as false negatives, which isn't always the case. See this answer for discussion of cut points in the context of Cox models.If you nevertheless are compelled to look for a cut point, your proposed method seems to be essentially that used by the cutp function in the R survMisc package. I would recommend that you try your cut point selection on multiple bootstrap samples from your data, as that best mimics repeated sampling from the underlying population. Unless your numeric index is hiding some true dichotomy in an underlying phenomenon, my hunch is that you will find a pretty wide range of \"optimal\" cut points, however defined, among those repeated samples. The bootstrap results at least will show your readers how much reliability to associate with the cut point value that you propose.","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-08-30 08:15:02","Question_id":232485}
{"_id":{"$oid":"5837a573a05283111e4d2cee"},"Last_activity":"2016-08-29 06:24:03","Creator_reputation":25170,"Question_score":1,"Answer_content":"As already stated in the comments, you may need more information then your sample to estimate the probability. The only thing that you can estimate from the sample is probability of  coming from the same distribution as your sample given the empirical distribution of your sample. Basically you would be assuming that distribution of the population is similar to the distribution of your sample (and if you have doubts if the small sample you have is representative, then it seems that you are not willing to make such assumptions).The value  out of range for your data so obviously you cannot use cumulative empirical distribution function to calculate the probabilities, the same with using bootstrap in here.Small sample size does not make estimating parameters of distribution harder, it only makes it less reliable, so there is no reason why you couldn't use parametric distribution.However if you are not interested in fitting parametric distribution, you can use nonparameteric approach such as using kernel density. In this case you also have to estimate (or pick) a parameter for bandwidth and make few other decisions such as picking kernels, or other parameters, but you do not have to make decisions about form of parametric distribution for your data. Below you can see kernel density estimate based on your data.  ","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-29 06:24:03","Question_id":232290}
{"_id":{"$oid":"5837a573a05283111e4d2cfb"},"Last_activity":"2016-08-09 02:12:33","Creator_reputation":755,"Question_score":1,"Answer_content":"The above expectation inside your triple integral can be found via the following trick.When , as you say, is a log-normal process, then  is multinormal with moment generating functionM(u,v,w) = E(e^{u Y(r) + v Y(s) + w Y(t)})=e^{\\mu(u+v+w) + \\frac12(u,v,w)^T\\Sigma (u,v,w)}where  is the covariance matrix of .  Presumably you know the elements of  if you know the autocovariance function of ?  From this you can work out the above expectation since this equalsE(X(r)X(s)X(t)) = E(e^{Y(r)+Y(s)+Y(t)}) = M(1,1,1),by definition of the moment generating function.","Display_name":"Jarle Tufto","Creater_id":77222,"Start_date":"2016-08-09 02:12:33","Question_id":228818}
{"_id":{"$oid":"5837a573a05283111e4d2cfc"},"Last_activity":"2016-08-08 09:42:45","Creator_reputation":7,"Question_score":-1,"Answer_content":"You could appeal to Fubini's theorem to rewrite the third moment as:E[X^3] = E_X\\left[\\int_0^L\\int_0^L\\int_0^L E[X(t)X(u)]X(v) dt dudv  \\right],and apply what you have already calculated for . You may end up with a not-so-nice expression.","Display_name":"NotSure","Creater_id":127017,"Start_date":"2016-08-08 09:42:45","Question_id":228818}
{"_id":{"$oid":"5837a573a05283111e4d2d09"},"Last_activity":"2016-08-30 07:19:14","Creator_reputation":2726,"Question_score":1,"Answer_content":"My guess is that when you are performing the Bayesian updating, you are not using the correct updated prior. Logistic regression has no conjugate prior thus you will not be able to perform Bayesian updating the way that you want to. In addition, the default prior for bayesglm is a Cauchy distribution and you have only updated the location and scale for this distribution (in fact you haven't ever updated the intercept location and scale), but have never updated the degrees of freedom. Finally, the prior in bayesglm is always independent amongst the coefficients, but the updated posterior will not necessarily be independent. (There is also the question of what the scaled argument is doing to the prior, so you will likely want scaled=FALSE.)You may be able to get closer to what you want by taking samples from the posterior and then optimizing the location, scale, and degrees of freedom to find the best fitting t distribution (likely you will do this for each coefficient separately). Then use this as the prior for the next data subset. ","Display_name":"jaradniemi","Creater_id":40440,"Start_date":"2016-08-30 07:19:14","Question_id":232425}
{"_id":{"$oid":"5837a573a05283111e4d2d16"},"Last_activity":"2016-08-30 06:58:08","Creator_reputation":69,"Question_score":0,"Answer_content":"I recently created a model for the amount of health insurance claims made by an individual on the basis of age, gender etc. and used a Gaussian family distribution.The  I got was 30.58% which I believe to be good considering how random the amount a person spends (given the person has no pre-existing condition, since those are not covered) on health insurance is.I recently read a post where an  of 7% on stock market prediction resulted in millions of dollars of additional profit. The stock market is truly volatile and most affected by current events that typically happen randomly. Therefore, an  for a statistical model based on the stock market would be low while still being useful.","Display_name":"Ali Turab Lotia","Creater_id":124010,"Start_date":"2016-08-30 05:27:52","Question_id":232478}
{"_id":{"$oid":"5837a573a05283111e4d2d23"},"Last_activity":"2016-08-30 07:08:05","Creator_reputation":1885,"Question_score":1,"Answer_content":"#1 - Yes. Restricting the data range is one way to accommodate that shift in the market. #2 - First question would be whether you even NEED an overall model. For forecasting purposes, you wouldn't. The next issue would be how you are going to accommodate those market changes (maybe with a regressor added to the ARIMA model?). Hyndman's free online textbook has a chapter on this. Finally, if you go through some automated ARIMA identification scheme with the regression, you might also force the (3,0,2) model to see if it does just about as well; given a choice of equivalently fitting models, choosing the one that does best on recent data is a good choice.","Display_name":"zbicyclist","Creater_id":3919,"Start_date":"2016-08-30 07:08:05","Question_id":232497}
{"_id":{"$oid":"5837a573a05283111e4d2d30"},"Last_activity":"2016-08-30 06:56:04","Creator_reputation":11,"Question_score":1,"Answer_content":", for some , then .If , then you can use the upper bound  to obtain the bound , and expand it if you wish to do so.","Display_name":"Seems","Creater_id":129489,"Start_date":"2016-08-30 06:49:28","Question_id":232390}
{"_id":{"$oid":"5837a573a05283111e4d2d3d"},"Last_activity":"2012-03-26 21:50:03","Creator_reputation":10749,"Question_score":7,"Answer_content":"Total number of possibilities1) Close! You've got 62 choices for the first character, 62 for the 2nd, etc, so you end up with , which is an absurdly huge number.Collision with a \"Target\" String2) As we established above, there are  potential strings. You want to know how many you'd need to guess to have better than 1 in 100,000 odds of guessing the \"target\" string. Essentially, you're asking what \\frac{x}{62^{20}} \\ge \\frac{1}{10^5}To get it spot on, you'd have to round x up (or add one, if they're precisely equal), but as you'll see in a second, it doesn't really matter.Through basic algebra, we can rearrange that as \\begin{aligned}\r10^5x \u0026amp;\\ge 62^{20}\\\\ \r10^5{x} \u0026amp;\\ge (6.2 \\cdot 10)^{20}\\\\\r10^5x \u0026amp;\\ge 6.2^{20} \\cdot 10^{20}\\\\\rx \u0026amp;\\ge 6.2^{20} \\cdot 10^{15}\r\\end{aligned}Doing the  math,  is about , so let's call the whole thing  or, more succinctly, a whole heck of a lot. This is, of course, why long passwords work really well :-) For real passwords, of course, you have to worry about strings of length less than or equal to twenty, which increases the number of possibilities even more. Duplicates in the listNow, let's consider the other scenario. Strings are generated at random and we want to determine how many can be generated before there's a 1:100,000 chance of any two strings matching. The classic version of this problem is called the Birthday Problem (or 'Paradox') and asks what the probability that two of n people have the same birthday. The wikipedia article[1] looks decent and has some tables that you might find useful. Nevertheless, I'll try to give you the flavor for the answer here too.Some things to keep in mind:-The probability of a match and of not having a match must sum to 1, so  and vice versa.-For two independent events  and , the probability of . To get the answer, we're going to start by calculating the probability of not seeing a match for a fixed number of strings . Once we know how to do that, we can set that equation equal to the threshold (1/100,000) and solve for .For convenience, let's call  the number of possible strings (). We're going to 'walk' down the list and calculate the probability that the ^{th} string matches any of the strings \"above\" it in the list. For the first string, we've got  total strings and nothing in the list, so . For the second string, there are still  total possibilities, but one of those has been \"used up\" by the first string, so the probability of a match for this string is  For the third string, there are two ways for it a match and therefore  ways not to, so  and so on. In general, the probability of the th string not matching the others is P_{k}(\\textrm{no match})= \\frac{N-k+1}{N}However, we want the probability of no matches between any of the  strings. Since all of the events are independent (per the question), we can just multiply these probabilities together, like this:P(\\textrm{No Matches}) = \\frac{N}{N} \\cdot \\frac{N-1}{N} \\cdot \\frac{N-2}{N} \\cdots \\frac{N-k+1}{N}That can be simplified a little bit:\\begin{aligned}\rP(\\textrm{No Matches}) \u0026amp;= \\frac{N \\cdot (N-1) \\cdot (N-2) \\cdots (N-k+1)}{N^k} \\\\\rP(\\textrm{No Matches}) \u0026amp;= \\frac{N!}{N^k \\cdot (N-k)!} \\\\\rP(\\textrm{No Matches}) \u0026amp;= \\frac{k! \\cdot \\binom{N}{k}}{N^k} \\\\\r\\end{aligned}\rThe first step just multiplies the fractions together, the second uses the definition of factorial () to replace the products of  with something a little more manageable, and the final step swaps in a binomial coefficient. This gives us an equation for the probability of having no matches at all after generating  strings. In theory, you could set that equal to  and solve for . In practice, it's going to be difficult to an answer since you'll be multiplying/dividing by huge numbers--factorials grow really quickly ( is more than 150 digits long). However, there are approximations, both for computing the factorial and for the whole problem. This paper[2] suggests  k = 0.5 + \\sqrt{0.25 - 2N\\ln(p)} where p is the probability of not seeing a match. His tests max out at , but it's still pretty accurate there. Plugging in your numbers, I get approximately .References[1] http://en.wikipedia.org/wiki/Birthday_problem[2] Mathis, Frank H. (June 1991). \"A Generalized Birthday Problem\". SIAM Review (Society for Industrial and Applied Mathematics) 33 (2): 265–270. JSTOR Link","Display_name":"Matt Krause","Creater_id":7250,"Start_date":"2012-03-24 19:54:27","Question_id":25211}
{"_id":{"$oid":"5837a573a05283111e4d2d4a"},"Last_activity":"2013-11-09 11:21:37","Creator_reputation":1293,"Question_score":2,"Answer_content":"To understand the problem easily let's consider that all the five cards are of the same 'type' (either all spade, or all diamond, or whatever you wish). So the face value is different only by the 5 different symbols from the 13 symbols that a particular 'type' of card has. There are four 'types' of cards- diamonds, hearts, spades and clubs.Now consider how many combinations of cards (by 'types') can there be with these same five symbols! Each of the five cards can be drawn from any of the four 'types' (diamonds, hearts, spades and clubs) of cards that a deck has. So, there can be 4^5 possible combinations of cards by these 'types' (one card can have 4 possible types, 2 cards can have 4^2 possible types,...and so on!). That is why 4^5 is being multiplied.","Display_name":"Blain Waan","Creater_id":12603,"Start_date":"2013-11-09 10:58:38","Question_id":75060}
{"_id":{"$oid":"5837a573a05283111e4d2d57"},"Last_activity":"2016-08-30 06:36:28","Creator_reputation":313,"Question_score":4,"Answer_content":"One does not compare the absolute values of two AICs (which can be like  but also ), but considers their difference:\\Delta_i=AIC_i-AIC_{\\rm min},where  is the AIC of the -th model, and  is the lowest AIC one obtains among the set of models examined (i.e., the prefered model). The rule of thumb, outlined e.g. in Burnham \u0026amp; Anderson 2004, is:if , then there is substantial support for the -th model (or the evidence against it is worth only a bare mention), and the proposition that it is a proper description is highly probable;if , then there is strong support for the -th model;if , then there is considerably less support for the -th model;models with  have essentially no support.Now, regarding the 0.7% mentioned in the question, consider two situations: and  is bigger by 0.7%: . Then  so there is no substantial difference between the models. and  is bigger by 0.7%: . Then  so there is no support for the 2-nd model.Hence, saying that the difference between AICs is 0.7% does not provide any information.The AIC value contains scaling constants coming from the log-likelihood, and so  are free of such constants. Onemight consider  a rescaling transformation that forces the best model to have .The formulation of AIC penalizes the use of an excessive number of parameters, hence discourages overfitting. It prefers models with fewer parameters, as long as the others do not provide a substantially better fit. AIC tries to select a model (among the examined ones) that most adequately describes reality (in the form of the data under examination). This means that in fact the model being a real description of the data is never considered. Note that AIC gives you the information which model describes the data better, it does not give any interpretation. Personally, I would say that if you have a simple model and a complicated one that has a much lower AIC, then the simple model is not good enough. If the more complex model is really much more complicated but the  is not huge (maybe , maybe  - depends on the particular situation) I would stick to the simpler model if it's really easier to work with.Further, you can ascribe a probability to the -th model viap_i=\\exp\\left(\\frac{-\\Delta_i}{2}\\right),which provides a relative (compared to ) probability that the -th models minimizes the AIC. For example,  corresponds to  (quite high), and  corresponds to  (quite low). The first case means that there is 47% probability that the -th model might in fact be a better description than the model that yielded , and in the second case this probability is only 0.05%.Finally, regarding the formula for AIC:AIC=2k-2\\mathcal{L},it is important to note that when two models with similar  are considered, the  depends solely on the numberof parameters due to the  term. Hence, when , the relative improvement is due to actual improvement of the fit, not to increasing the number of parameters only.TL;DRIt's a bad reason; use the difference between the absolute values of the AICs.The percentage says nothing.Not possible to answer this question due to no information on the models, data, and what does different results mean.","Display_name":"corey979","Creater_id":72352,"Start_date":"2016-08-30 06:36:28","Question_id":232465}
{"_id":{"$oid":"5837a573a05283111e4d2d66"},"Last_activity":"2012-01-08 09:12:36","Creator_reputation":5822,"Question_score":3,"Answer_content":"Just to make sure we are on the same page: You a sequence of 1000 samples with 7 features each. There is a sequential pattern in there which is why you process them with an RNN. At each timestep It depends. It might get better if you use different normalizations, hard to tell.To me it just sounds like classification. I am not sure what you mean by ranking exactly.No reason to be skeptical. Normally, training error drops like that--extremly quick for few iterations, very slow afterwards.No, absolutely not. For some tasks, less than 100 iterations (= passes over the training set) suffice.You are the one who has to say whether the error is small enough. :) We can't tell you without knowing what you are using the network for.Hard to tell. You should use early stopping instead. Train the network until the error on some held out validation set rises--that's the moment from which on you only overfit. Use the weights found then to evaluate on a test set. (That makes it three sets: training, validation, test set).Here are some tips that I can give:make sure to clamp your maximal updates to some fixed value. E.g. when you do a learning step, don't apply updates bigger than 0.1 (RPROP can already do this),try Long Short-Term Memory,try Hessian free optimization (Ilya Sutskever has code on his webpage).","Display_name":"bayerj","Creater_id":2860,"Start_date":"2012-01-08 09:12:36","Question_id":20756}
{"_id":{"$oid":"5837a573a05283111e4d2d73"},"Last_activity":"2015-08-28 14:29:57","Creator_reputation":30436,"Question_score":0,"Answer_content":"A not irrelevant digression, to validate also my comment about : We are considering an expression where in the numerator we have the sum of  Bernoulli random variables, i.e. dichotomous discrete random variables taking the values . So we have to consider whether the event \"sum of Bernoullis is zero\" has strictly positive probability.  The probability that all draws will be zero is equal to \\Pr \\left(\\text{\\{All b_i's are zero\\}}\\right) = \\left(1-\\frac {1}{N^{\\gamma}}\\right)^NThen for example, if  we have\\lim_{n\\rightarrow \\infty}\\Pr \\left(\\text{\\{All b_i's are zero\\}}\\right) = \\lim_{n\\rightarrow \\infty}\\left(1-\\frac {1}{N}\\right)^N = 1/e \\approx 0.37and a very high probability indeed. Informally you can check that if  the sum will have limiting probability of being zero equal to unity, while only if  the limiting probability of the sum being zero, will equal zero.Another way to explore this is to remember that the sum of i.i.d Bernoullis is a Binomial random variable,\\sum_{i=1}^{N}b_{i} \\sim \\text{Bin}\\big(\\mu_n = N^{1-\\gamma}, \\sigma^2_n = N^{1-\\gamma}-N^{1-2\\gamma}\\big)If , as a comment indicated that it is the case of interest here,  we avoid the case where the sum in the denominator takes the value zero, at the limit (because for finite  this can still very well be the case).Assuming from here on that , we can definew_i = \\frac {b_i}{\\sum_{i=1}^{N}b_{i}},\\;\\; \\sum_i^N w_i = 1,\\;\\; \\forall N and the expression of interest becomes\\sum_{i=1}^N w_ix_iwhich is a weighted average, and more importantly, a convex combination of the 's, but with the weights being random variables. This is a rather advanced technical issue, that nevertheless has been studied. A bit informally, and exploiting the fact that the weights relate to Bernoullis, note that the values each weight can take are two,w_i \\in \\left\\{0, \\frac 1{\\sum_{i=1}^Nb_i}\\right\\}We wonder whether\\sum_{i=1}^N w_ix_i - \\frac 1N\\sum_{i=1}^N x_i \\xrightarrow{p}0 \\;\\;???Denote  the number of Bernoullis that take the value . This means that .  Then we can write\\sum_{i=1}^N w_ix_i - \\frac 1N\\sum_{i=1}^N x_i = \\sum_{w_i \\neq 0}w_ix_i - \\frac 1N\\sum_{i=1}^N x_i =\\frac {1}{\\sum_{i=1}^N b_i}\\sum_{w_i \\neq 0}x_i - \\frac 1N\\sum_{i=1}^N x_i=\\frac {1}{M}\\sum_{w_i \\neq 0}x_i - \\frac 1N\\sum_{i=1}^N x_iNow  is not a number but the Binomial random variable we described before. Still we have that , and if  the two sums above will have the same probability limit, , and the assertion of the question is verified. Intuitively, if, as  becomes \"very large\",  will tend to take very large values also, the first sum will behave like a proper sample average of the 's.So the condition we require is thatM=\\sum_{i=1}^Nb_i \\rightarrow \\infty By looking at its mean and variance, we see that as long as  it will diverge. So it appears that \\gamma \u0026lt;1 \\implies \\frac{\\sum_{i=1}^{N}b_{i}x_{i}}{\\sum_{i=1}^{N}b_{i}} \\xrightarrow{p} \\muA quick simulation supports this: I generated  samples from a chi-square with  degrees of freedom (so its expected value is ), and the same number of samples from a Bernoulli with  (so  here). I then calculated the expression of interest and I obtained\\frac{\\sum_{i=1}^{N}b_{i}x_{i}}{\\sum_{i=1}^{N}b_{i}} = 3.016","Display_name":"Alecos Papadopoulos","Creater_id":28746,"Start_date":"2015-08-28 14:29:57","Question_id":169157}
{"_id":{"$oid":"5837a573a05283111e4d2d80"},"Last_activity":"2015-12-01 06:15:46","Creator_reputation":992,"Question_score":1,"Answer_content":" means, for any , that\\begin{equation*}\\lim_{n \\to \\infty} \\mathbb{P}\\left( \\left|f_n - L \\right| \u0026lt; \\epsilon \\right) = 1.\\end{equation*}If you have  for every appropriate  then  for any .  Take each of those differences to be nonnegative for simplicity; then for any ,\\begin{equation*}\\left\\{\\left|Z_n - L\\right| \u0026lt; \\epsilon\\right\\} \\subseteq \\left\\{\\left|Y_n - L\\right| \u0026lt; \\epsilon\\right\\} \\subseteq \\left\\{\\left|X_n - L\\right| \u0026lt; \\epsilon\\right\\}.\\end{equation*}You thus have that , and the standard squeeze theorem gives you the result you're looking for.","Display_name":"jtobin","Creater_id":7706,"Start_date":"2015-12-01 05:49:36","Question_id":184370}
{"_id":{"$oid":"5837a573a05283111e4d2d8d"},"Last_activity":"2014-07-26 12:31:11","Creator_reputation":30436,"Question_score":3,"Answer_content":"You are right.  \"If  is a sample of  observations such that  and Var[ such that  as  then \\lim_{n\\rightarrow \\infty}P\\left(\\left|\\frac 1n\\sum_{i=1}^nX_i - \\frac 1n\\sum_{i=1}^nE(X_i)\\right|\u0026lt;\\epsilon\\right) =1I guess you can make the notational mapping.Since by design we assume different moments for each , each comes from a different population. So if by  you mean values of the index , then  is not a population, but a set including three values of the index with each value representing a different population.  If you consider the random variables , it is a pair coming from two different populations -you do not \"unite\" the two populations \"into one\" because, being different with respect to the object of study (convergence of sample moments), how could they form a single population (for the purposes of the specific study)? Have you contemplated how is the abstract concept of \"statistical population\" defined?","Display_name":"Alecos Papadopoulos","Creater_id":28746,"Start_date":"2014-07-26 12:23:08","Question_id":109489}
{"_id":{"$oid":"5837a573a05283111e4d2d9a"},"Last_activity":"2016-03-09 03:07:16","Creator_reputation":78,"Question_score":0,"Answer_content":"Oh, I could finally find the answerIt is in \"Theorem 14.4-1 in Bishop et al\"The question was applied by this theorem for LLN","Display_name":"kurtkim","Creater_id":70877,"Start_date":"2016-03-09 03:07:16","Question_id":200354}
{"_id":{"$oid":"5837a573a05283111e4d2d9b"},"Last_activity":"2016-03-07 07:13:34","Creator_reputation":7412,"Question_score":3,"Answer_content":"The expression is a little odd because there's no reason to mention  when using \"big O\" notation.  In any case,\\begin{align}\\frac{\\sum_{i=1}^{n} X_i}{n} \u0026amp;= \\mu + n^{-1/2} \\frac{\\sum_{i=1}^{n}(X_i - \\mu)}{\\sqrt{n}} \\\\\u0026amp;= \\mu + O_p(n^{-1/2})\\end{align}since  converges in distribution.","Display_name":"dsaxton","Creater_id":78861,"Start_date":"2016-03-07 07:13:34","Question_id":200354}
{"_id":{"$oid":"5837a573a05283111e4d2da8"},"Last_activity":"2014-12-02 17:59:09","Creator_reputation":687,"Question_score":1,"Answer_content":"I found the answer, but posting the hint here if anyone else wants to solve it. For , first prove the probabilistic convergence of , and then using the fact that  find the probabilistic convergence for . For  use a similar trick. ","Display_name":"Daniel","Creater_id":17812,"Start_date":"2014-12-01 14:22:43","Question_id":126139}
{"_id":{"$oid":"5837a573a05283111e4d2db5"},"Last_activity":"2016-06-20 12:49:51","Creator_reputation":147175,"Question_score":3,"Answer_content":"Let's begin by parsing the meaning of the indicators. is a random variable equal to  when  is at the (true) median  or lower.  Thus, by definition, it is a Bernoulli (that is, ) variable.  All we need to know about it is the chance it equals .  Writing  for the distribution of (any of the) , that chance is . is a random variable equal to  when  is at the sample median  or lower.  By definition, at least half of the sample is equal to or less than the sample median (and at least half is equal to or greater than the sample median).We all know the difficulties with sample medians: they are uniquely defined only for samples whose size  is odd and, when  has a finite jump (reflecting one or more values with positive probabilities), there is some chance that several  will be tied at the median value.  A rigorous analysis would address these separate possibilities.  To get some insight into how it might go, consider the simplest case where (a)  has no finite jumps (this includes all continuous distributions) and (b)  is odd.  Because of (a), it is certain there will be no tie at the median and because of (b), exactly  of the  (for  ranging from  through ) are equal to or less than the sample median.  In this situation, then,\\sum_{i=1}^n I\\{X_i \\le \\hat m_n\\}is a sum of  ones and  zeros, therefore equal to a constant .At this point, the rest is easy: the numerator of the fraction, being a sum of  iid Bernoulli variables, has a Binomial distribution minus the constant .  Because  has no jumps, .  Therefore the numerator is a random variable with mean  minus : its mean is .  It variance is that of the Binomial distribution, equal to .  The denominator divides the mean by  and causes the variance to be divided by .  Consequently,Z_n = \\frac{\\sum_{i=1}^{n}\\big(1\\{X_i\\leq m\\} - 1\\{X_i\\leq\\hat m _n\\}\\big)}{\\sqrt{n}}is a random variable with mean  and variance .  In fact, as is well known (it's an easy consequence of the Central Limit Theorem), the subsequence  converges to a Normal distribution of mean zero and variance .  Because this is far from zero, the original sequence cannot possibly converge to zero.It is actually more interesting when  has a jump at its median.  Specifically, this is the case where F(m) = \\Pr(X_i \\le m) \\gt 1/2 \\text{ and } \\Pr(X_i \\ge m) \\gt 1/2.For very large , the Central Limit Theorem implies the chance that  is vanishingly small (this chance rapidly decreases with ).  Consequently, since  obviously is zero when ,  will almost always equal zero.  The limit in probability of the  must be zero in this case.The two situations are readily simulated.  The top row of the figure shows the cases where  jumps at the median while the bottom row shows the continuous case.  The headings give the sample sizes .  From left to right it is evident how the top row progresses toward a zero distribution while the bottom row progresses to a Normal distribution (whose density is shown as a red curve for reference).Here is the R code that produced the figure.n.sim \u0026lt;- 5e4 # Simulation sizek \u0026lt;- 4       # Jump of size k/(2k+1) in the middle of the distributionn.size \u0026lt;- c(33, 111, 333, 1111) # Sample sizespar(mfrow=c(2,length(n.size)))for (n in n.size) {  x \u0026lt;- matrix(floor(runif(n*n.sim, 0, 2*k+1)), nrow=n)  z \u0026lt;- apply(x, 2, function(y) {sum(y \u0026lt;= k) - sum(y \u0026lt;= median(y))})/sqrt(n)  hist(z, main=paste(n), freq=TRUE)}for (n in n.size) {  x \u0026lt;- matrix(rnorm(n*n.sim, k), nrow=n)  z \u0026lt;- apply(x, 2, function(y) {sum(y \u0026lt;= k) - sum(y \u0026lt;= median(y))})/sqrt(n)  h \u0026lt;- hist(z, main=paste(n), freq=TRUE, breaks=25)  dx \u0026lt;- diff(h$breaks[1:2])  curve(dnorm(x, 0, 1/2)*n.sim*dx, add=TRUE, col=\"Red\", lwd=2)}","Display_name":"whuber","Creater_id":919,"Start_date":"2016-06-20 12:49:51","Question_id":219526}
{"_id":{"$oid":"5837a573a05283111e4d2dc2"},"Last_activity":"2016-04-04 04:43:16","Creator_reputation":11915,"Question_score":6,"Answer_content":"The short answer is: An MCMC is a MC, but not all MCs are MCMC. The slightly longer answer: MC methods are a class of methods, of which MCMC is one possibility. Even MCMC does not uniquely define your method as there are different variations of MCMC. You can read more in: Robert, C. P., \u0026amp; Casella, G. (2004). Monte Carlo statistical methods. New York: Springer.","Display_name":"Maarten Buis","Creater_id":23853,"Start_date":"2013-07-09 06:04:22","Question_id":63767}
{"_id":{"$oid":"5837a573a05283111e4d2dc3"},"Last_activity":"2014-05-13 15:14:28","Creator_reputation":390,"Question_score":2,"Answer_content":"In general Monte Carlo (MC) refers to estimating an integral by using random sampling to avoid curse of dimensionality problem. Also, once you have the samples, it's possible to compute the expectations of any random variable with respect to the sampled distribution.A subclass of MC is MCMC you set up a Markov chain whose stationary distribution is the target distribution that you want to sample from. The main thing about many MCMC methods is that due to the fact that you've set up a Markov chain, the samples are positively correlated and thereby increases the variance of your integral/expectation estimates. The better situation is to make your samples independent (or have carefully constructed negative correlation) to reduce the variance. However, many distributions that you want to sample from are incredibly complicated objects and are difficult to sample from directly. Hence the construction and use of MCMC.","Display_name":"queenbee","Creater_id":42728,"Start_date":"2014-05-13 15:14:28","Question_id":63767}
{"_id":{"$oid":"5837a573a05283111e4d2dd0"},"Last_activity":"2014-10-01 09:17:01","Creator_reputation":147175,"Question_score":8,"Answer_content":"The expectation of a random variable  is the Lebesgue integral\\mathbb{E}[X] = \\int_\\Omega X(\\omega)d\\mathbb{P}(\\omega).The Lebesgue integral is constructed in a sequence of steps whereby its domain of application is broadened to encompass an ever wider variety of random variables.  The first steps ultimately define the integral for variables with non-negative values: the complications of integrating functions which might oscillate arbitrarily between negative and positive values are thereby avoided.  To extend the integral to variables with negative values, decompose them into their positive and negative parts:X(\\omega) = X^{+}(\\omega) - X^{-}(\\omega)where  when  and  otherwise; similarly, .  These are readily seen to be random variables, too (that is, they will be measurable).  The integral is defined to be the difference\\int_\\Omega X(\\omega)d\\mathbb{P}(\\omega) = \\int_\\Omega X^{+}(\\omega)d\\mathbb{P}(\\omega) -  \\int_\\Omega X^{-}(\\omega)d\\mathbb{P}(\\omega),each of which involves a non-negative random variable and therefore the meaning of its integral has already been defined.At this point conventions may vary.  The Wikipedia articles I have linked to declare that the integral is defined only when both the positive and negative integrals are finite.  One could, however, allow that the integral is also defined when at most one of the integrals is finite.  We could say that it equals \"\" when the integral of the positive part diverges and equals \"\" when the integral of the negative part diverges.In this extended sense of being defined, consider a random variable  with a half-Cauchy distribution.  Its probability density function (PDF)  is defined and equal to  when  and otherwise equal to .  Thus  , and by definition\\mathbb{E}(X) = \\int_{-\\infty}^{+\\infty} f(x) dx = \\frac{2}{\\pi}\\int_0^\\infty \\frac{x dx}{1+x^2} - \\int_\\mathbb{R} 0 dx.Although the first integral diverges, the second obviously is finite, so we could consider this expectation to be infinite.  This example answers the question, but a full appreciation requires analysis of a distribution that looks infinite but actually cannot be defined at all. The standard example is the Cauchy distribution (also known as the Student t with one degree of freedom).For a Cauchy-distributed variable the PDF is  everywhere.  Splitting the expectation into its positive and negative parts yields\\mathbb{E}(X) = \\frac{1}{\\pi}\\int_0^\\infty \\frac{x dx}{1+x^2} - \\frac{1}{\\pi}\\int_{-\\infty}^0 \\frac{-x dx}{1+x^2}.Now both sides diverge.  Since an expression like \"\" is nonsensical, we have no choice but to declare this expectation undefined.  One way to convince yourself of this is to consider the various ways in which the integral might be calculated: they concern how the limits of  are approached.  Pick any nonnegative real value .  As a mechanism to control the relative rates at which those limits increase, definef(n) = \\sqrt{(1+n^2)\\exp(2\\pi\\alpha)-1}.As  grows large without bound, so does .  Therefore, if this integral indeed had a well-defined value, it would be valid to compute it as\\frac{1}{\\pi}\\int_{-\\infty}^{+\\infty} \\frac{x dx}{1+x^2} =\\,(?) \\lim_{n\\to\\infty}\\frac{1}{\\pi}\\int_{-n}^{f(n)} \\frac{x dx}{1+x^2}because both the limits,  and , are expanding to encompass the entire Real line.This plot of the PDF shows how  is chosen to assure that the upper limit  extends just a little further to the right than the lower limit  extends to the left. The parts between  and  balance, contributing  to the expectation.  The value of  is chosen so that the contribution from the excess--shown in red--is always equal to , no matter what  may be.But a straightforward calculation gives\\frac{1}{\\pi}\\int_{-n}^{f(n)} \\frac{x dx}{1+x^2} = \\frac{1}{2\\pi}\\log(1+x^2)|_{-n}^{f(n)} = \\frac{1}{2\\pi}\\left(\\log(1+f(n)^2) - \\log(1+n^2)\\right)=\\alpha.(Using the integration endpoints  and  shows that  is a possible value of this limit, too.)  Accordingly, since this integral can be made to equal any Real number merely by varying how the limits are taken, it cannot be considered to have a definite value.","Display_name":"whuber","Creater_id":919,"Start_date":"2014-09-30 15:36:22","Question_id":117376}
{"_id":{"$oid":"5837a573a05283111e4d2dd1"},"Last_activity":"2014-09-30 12:52:31","Creator_reputation":3131,"Question_score":0,"Answer_content":"There are some useful and well-known distributions which have undefined mean. I think one of the most used is Cauchy distribution. See Wikipedia article.For example the standard Cauchy distribution has pdf f(x)=\\frac{1}{\\pi(1+x^2)}So, the mean is E(x) = \\int_{-\\infty}^{\\infty}x\\frac{1}{\\pi(1+x^2)}dx = \\frac{log(1+x^2)}{2\\pi}\\bigg|_{-\\infty}^{\\infty}You can see that both ends evaluates to infinity, so the expected value is undefined.","Display_name":"rapaio","Creater_id":16709,"Start_date":"2014-09-30 12:52:31","Question_id":117376}
{"_id":{"$oid":"5837a573a05283111e4d2dde"},"Last_activity":"2015-12-18 22:42:42","Creator_reputation":20197,"Question_score":3,"Answer_content":"Consider independent random variables  where . In your book's terminology,  and . Define  and note that  is what your book calls  whose mean is . Hence,Z_n = \\left(S_n - \\frac{n+1}{2}\\right)\\sim N\\left(0,\\frac{\\sigma^2}{n}\\right).Now, Chebyshev's Inequality says that\\begin{align}P\\{|Z_n| \u0026gt; \\epsilon\\} \u0026amp;= P\\left\\{\\left|S_n - \\frac{n+1}{2}\\right|\u0026gt; \\epsilon\\right\\}\\\\\u0026amp;= P\\left\\{\\left|\\bar{X}_n - \\bar{\\mu_n}\\right| \u0026gt; \\epsilon\\right\\}\\\\\u0026amp;\\leq \\frac{\\sigma^2}{n\\epsilon^2} \\to 0 ~~ \\text{as}~~ n \\to \\infty\\end{align}Thus,  converges to  in probability, but, as you correctlydeduced, it cannot be said that  converges to in probability because that limitdoes not exist; the sequence of numbers  diverges.In whuber's comment, he gives an example of a sequence ofnumbers  for which the limit does not exist and thesequence neither converges nor diverges. We can construct asimilar example by modifying the conditions above.Suppose instead that  so that nowS_n \\sim \\begin{cases}N\\left(\\frac{1}{2},\\frac{\\sigma^2}{n}\\right),\u0026amp; n ~~\\text{even},\\\\N\\left(-\\frac{1}{2}-\\frac 1n,\\frac{\\sigma^2}{n}\\right),\u0026amp; n ~~\\text{odd}, \\end{cases}so that the sequence  is a sequence whose terms arealternately positive (and fixed at ) and negative (andapproaching  as . Thus, the sequencedoes not diverge, nor does it approach a limit. Similarly, thesequence  converges to a sequence of alternate values.","Display_name":"Dilip Sarwate","Creater_id":6633,"Start_date":"2015-12-18 08:07:28","Question_id":187285}
{"_id":{"$oid":"5837a573a05283111e4d2ddf"},"Last_activity":"2015-12-17 12:05:41","Creator_reputation":7412,"Question_score":2,"Answer_content":"Presumably we're to assume independence here, otherwise here is a simple counterexample: let  be Bernoulli and set  for .  I'm not totally sure what you're asking, but assuming independence the fact that  converges to zero in probability is immediate from Chebyshev's inequality.  Just note that for P \\left ( \\frac{\\sum_{i=1}^{n} (X_i - \\mu_i)}{n} \\geq \\epsilon \\right ) \\leq \\frac{\\sum_{i=1}^{n} \\sigma^2_i}{n^2 \\epsilon^2}and the condition tells us that this bound goes to zero as .  Which part of this is causing confusion?","Display_name":"dsaxton","Creater_id":78861,"Start_date":"2015-12-17 09:35:54","Question_id":187285}
{"_id":{"$oid":"5837a573a05283111e4d2dec"},"Last_activity":"2016-08-26 13:01:41","Creator_reputation":869,"Question_score":1,"Answer_content":"There seems to be a fairly deep misunderstanding of what you're trying to do, so while this may solve your immediate problem, I urge you to read one or several tutorials on out of sample prediction. This question is not about xgboost, and it is not about labels -- it is about a basic statistical procedure, that it is imperative to understand before doing analysis.In your example, you are using the same function twice -- in the second case, trying to FIT a NEW model to your testing data (without labels). However, labels or NOT, this is not what testing data is FOR. When you are doing out of sample prediction: First, you fit a model based on your training data, and get parameter estimate  (your fitted model).Y_{train} \\sim f(X_{train}; \\theta)Then, using your fitted model, and your testing data, you PREDICT new labels -- in other words, what SHOULD the labels on your test data be, based on what your model says?\\hat{Y}_{test} \\leftarrow f(X_{test}; \\hat{\\theta}_{train})Last, you compare the actual testing labels with the predicted ones, to get your out of sample error. \\mathbb{E}[(\\hat{Y}_{test} - Y_{train})^2]A very quick search of the xgboost documentation returns the predict function:## S4 method for signature 'xgb.Booster'predict(object, newdata, missing = NULL,outputmargin = FALSE, ntreelimit = NULL, predleaf = FALSE)Meaning, the call you really want is:y.test.hat \u0026lt;- predict(dtrain, test, missing=NA)I hope that sets you on the right track, but I also hope that the next time you spend hours stuck on something, you consider that it may not be a software problem.","Display_name":"Sophologist","Creater_id":52211,"Start_date":"2016-08-26 12:41:13","Question_id":231954}
{"_id":{"$oid":"5837a573a05283111e4d2dfb"},"Last_activity":"2016-08-26 12:58:35","Creator_reputation":869,"Question_score":0,"Answer_content":"Yes -- some important problems, but not intractable ones.The big issue (and this is easier to see if you draw a graph) is that if your theory is right, you won't get good estimates of the parameters, in the first regression. Really, draw it out -- make a node for every variable, and every time an independent variable is put in a regression, draw an arrow between it, and the variable it's predicting. You'll see that there are two paths from v1, and v2, to Y -- for each, one directly, and one through v3. As a result, the model won't be able to cleanly separate where the influence is coming from, if you throw everything in at once.What you want to do instead is run the second regression FIRST -- and then, don't use v3, but rather use the residuals from the first regression, i.e. the parts of v3 that v1 and v2 could NOT 'explain.' I.e.v3 \\sim v1 + v2y \\sim v1 + v2 + r3where  is , i.e. what's left over, after prediction. Then (I believe) you'll be okay.","Display_name":"Sophologist","Creater_id":52211,"Start_date":"2016-08-26 12:58:35","Question_id":231950}
{"_id":{"$oid":"5837a573a05283111e4d2e0a"},"Last_activity":"2016-08-26 12:53:11","Creator_reputation":7672,"Question_score":0,"Answer_content":"To allow flexibility in the algebra, and circumvent the fact that\\small\\begin{bmatrix} \\Sigma_{aa} \u0026amp; \\Sigma_{ab}\\\\\\Sigma_{ba} \u0026amp; \\Sigma_{bb}\\end{bmatrix}^{-1}\\neq\\begin{bmatrix} \\Sigma_{aa}^{-1} \u0026amp; \\Sigma_{ab}^{-1}\\\\\\Sigma_{ba}^{-1} \u0026amp; \\Sigma_{bb}^{-1} \\end{bmatrix}We replace the  by the precision matrix:\\begin{align}\\boldsymbol \\Sigma^{-1} =\\begin{bmatrix} \\Sigma_{aa} \u0026amp; \\Sigma_{ab}\\\\\\Sigma_{ba} \u0026amp; \\Sigma_{bb}\\end{bmatrix}^{-1}=\\begin{bmatrix} \\Lambda_{aa} \u0026amp; \\Lambda_{ab}\\\\\\Lambda_{ba} \u0026amp; \\Lambda_{bb}\\end{bmatrix}\\end{align}Now we can expand the quadratic exponent of the joint pdf of the partitioned multivariate Gaussian :-\\frac{1}{2}({\\bf x} - {\\boldsymbol \\mu})^T\\, \\Sigma^{-1} \\, ({\\bf x} - \\boldsymbol \\mu)\\\\= -\\frac{1}{2} \\begin{bmatrix} {\\bf x}_a - \\boldsymbol\\mu_a \u0026amp; {\\bf x_b} - \\boldsymbol\\mu_b \\end{bmatrix}^T\\begin{bmatrix} \\Lambda_{aa} \u0026amp; \\Lambda_{ab}\\\\\\Lambda_{ba} \u0026amp; \\Lambda_{bb}\\end{bmatrix}\\begin{bmatrix} {\\bf x}_a - \\boldsymbol\\mu_a \\\\ {\\bf x}_b - \\boldsymbol \\mu_b \\end{bmatrix}\\small\\\\ = -\\frac{1}{2}\\left[\\small ({\\bf x}_a-\\boldsymbol\\mu_a)^T\\, \\Lambda_{aa}({\\bf x}_a-\\boldsymbol\\mu_1) + 2 \\,({\\bf x}_a-\\boldsymbol\\mu_a)^T\\, \\Lambda_{ab}({\\bf x}_b-\\boldsymbol\\mu_b)+({\\bf x}_b-\\boldsymbol\\mu_b)^T\\, \\Lambda_{bb}({\\bf x}_b-\\boldsymbol\\mu_b)\\right]\\\\= \\color{blue}{-\\frac{1}{2}\\small ({\\bf x}_a-\\boldsymbol\\mu_a)^T\\, \\Lambda_{aa}({\\bf x}_a-\\boldsymbol\\mu_1)} -  \\,({\\bf x}_a-\\boldsymbol\\mu_a)^T\\, \\Lambda_{ab}({\\bf x}_b-\\boldsymbol\\mu_b)-\\frac{1}{2} ({\\bf x}_b-\\boldsymbol\\mu_b)^T \\Lambda_{bb}({\\bf x}_b-\\boldsymbol\\mu_b)to prove that we end up with quadratic exponents. Hence the joint distribution will be Gaussian. The mean and variance will fully characterize the distribution. The color indicates the only quadratic  form (see below).Finding the mean and variance \"completing the square\" is the step used in here. However, in the book it seems as though the operation was simply to expand the quadratic form, and then convert the resulting expression into an  polynomial form as commented by @them:\\small -\\frac{1}{2}({\\bf x}- \\boldsymbol\\mu)^T \\Sigma^{-1}({\\bf x}-\\boldsymbol\\mu) =-\\frac{1}{2}\\left({\\bf x}^T\\Sigma^{-1}{\\bf x} - \\boldsymbol\\mu^T\\Sigma^{-1}{\\bf x} - X^T \\Sigma^{-1} \\boldsymbol\\mu + \\boldsymbol\\mu^T\\Sigma^{-1}\\boldsymbol\\mu \\right)and noting that  \\small -\\frac{1}{2}({\\bf x}- \\boldsymbol\\mu)^T \\Sigma^{-1}({\\bf x}-\\boldsymbol\\mu) =-\\frac{1}{2}{\\bf x}^T\\Sigma^{-1}{\\bf x} + {\\bf x}^T \\Sigma^{-1} \\boldsymbol\\mu -\\frac{1}{2} \\boldsymbol\\mu^T\\Sigma^{-1}\\boldsymbol\\mu and since  does not depend on  we can just turn it into a constant :\\begin{eqnarray}\\small -\\frac{1}{2}({\\bf x}- \\boldsymbol\\mu)^T \\Sigma^{-1}({\\bf x}-\\boldsymbol\\mu) =\\color{brown}{-\\frac{1}{2}{\\bf x}^T\\Sigma^{-1}{\\bf x}} + {\\bf x}^T \\Sigma^{-1} \\boldsymbol\\mu +C \\qquad{(2.71)}\\\\=-\\frac{1}{2}\\begin{bmatrix} {\\bf x}_a \u0026amp; {\\bf x}_b\\end{bmatrix}^T\\Lambda \\begin{bmatrix} {\\bf x}_a \\\\ {\\bf x}_b\\end{bmatrix} + {\\bf x}^T \\Sigma^{-1} \\boldsymbol\\mu +C\\\\=\\color{blue}{-\\frac{1}{2}}\\begin{bmatrix}\\color{blue}{ {\\bf x}_a} \u0026amp; X_b\\end{bmatrix}^T\\begin{bmatrix} \\color{blue}{\\Lambda_{aa}} \u0026amp; \\Lambda_{ab}\\\\\\Lambda_{ba} \u0026amp; \\Lambda_{bb}\\end{bmatrix}\\begin{bmatrix} \\color{blue}{{\\bf x}_a} \\\\ {\\bf x}_b\\end{bmatrix} + {\\bf x}^T \\Sigma^{-1} \\boldsymbol\\mu + C\\end{eqnarray}When conditioning on  (acting now as a constant), the quadratic form in the exponent , of which  is the variance, will given by the elements colored in blue (compare to the part in red two lines prior). This explains mention of\\color{blue}{-\\dfrac{1}{2}{\\bf x}_a^T\\Lambda_{aa}{\\bf x}_a }in the book and in the OP. In this expression  has been assimilated into ; otherwise we have again the blue-colored expression in the second part of the answer.Hence the variance of  will be:\\Sigma_{a\\vert b} = \\Lambda_{aa}^{-1} At this point the book moves on to the mean.This link provides the pertinent three pages in Pattern Recognition and Machine Learning by Christopher Bishop.And here is a link to very pertinent material on completing the square as a technique to derive the marginal and conditional pdf of a multivariate Gaussian.","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-08-25 06:31:36","Question_id":231293}
{"_id":{"$oid":"5837a573a05283111e4d2e19"},"Last_activity":"2016-08-26 12:46:11","Creator_reputation":12260,"Question_score":3,"Answer_content":"There is no final decision tree. When you predict with a Random Forest, the data is presented to every tree and in your example, each tree reaches a decision on Species, which is one of three possibilities: Versicolor, Virginica, or Setosa. When your 500 trees have each decided, the mode (most common answer) is chosen as the forest's decision.So the mode is the mode of the decisions. There is no mode of the trees. (I doubt that any two trees are the same, and defining a mode of trees that doesn't involve exact duplicates might be difficult.)","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-08-26 12:46:11","Question_id":231955}
{"_id":{"$oid":"5837a573a05283111e4d2e28"},"Last_activity":"2016-08-26 12:39:30","Creator_reputation":111,"Question_score":1,"Answer_content":"All data are measured with finite precision, so all data is technically discrete (whether or not the process from which the data arise is discrete or continuous). However, a lot of data is precise enough to be modeled as continuous.I would treat yours at continuous.","Display_name":"Great38","Creater_id":122545,"Start_date":"2016-08-26 12:39:30","Question_id":231943}
{"_id":{"$oid":"5837a573a05283111e4d2e35"},"Last_activity":"2016-08-26 06:23:41","Creator_reputation":518,"Question_score":2,"Answer_content":"So if your question is \"how can we apply the LLN with finite data\" then I give the answer I gave in the comments: \"So you don't ever have infinitely many. It's that after you have a large finite number, you get `close' to the limit value of \"But, if your question is rather: How do we reconcile the LLN with a finite population, then I lead to to this fantastic post about finite population sampling :Are \u0026quot;random sample\u0026quot; and \u0026quot;iid random variable\u0026quot; synonyms? The other post about  being equivalent to  is problematic as LLN is a result of probability limits and sequences and those necessarily are not finite in index. So n is only ever able to reach N if we sample without replacement, meaning the samples are no longer iid. If we sample with replacement then we never run out of n. ","Display_name":"VCG","Creater_id":124896,"Start_date":"2016-08-25 14:25:51","Question_id":231666}
{"_id":{"$oid":"5837a573a05283111e4d2e42"},"Last_activity":"2016-08-26 11:52:26","Creator_reputation":13,"Question_score":0,"Answer_content":"I think maybe you are confused because you are looking for some kind of firm estimation of the consequences. But here is the rub with missing data. The data is missing so in many cases one cannot say what it's affect on your regression coefficients, SEs etc. would be.Having data that has some points that are pleasantly MCAR will reduce the power of your model as there is a loss in statistical efficiency. But that is all.Having data that has some points that are MAR is a bit annoying. Listwise deletion (also termed complete case analysis) may lead to results that are biased, but as the term suggests, one should be able to make good judgemental use of the observations and avoid such bias. Obviously the problem is to be able to say unequivocally that the data is MAR and for this you need 'separability': where the parameters for the missingness process are distinct from the model explaining the primary dependent. Most methods for dealing with incomplete data rest on the MAR assumption.Having data that has some points which are MNAR is is a complete and very common b*tch. Here, we are trying to say something about the world (by modelling it, in say your LR) but we have observed too little of it to say anything. Going ahead and modelling the complete data will mean you cannot infer anything about the wider population, because your results will be biased - you don't have external validity. You won't know how because the information is, well, missing. You cannot make causal interpretations (internal validity) because you are missing data and the missingness indicators are dependent on that unobserved data.  Without collecting more data or, in some cases, imputing the missing data... you are only able to talk about your incomplete sample. We have to go beyond the data and the model. Sensitivity analysis is a good idea as if you wish to continue to model the data. You could think of doing a logistic regression using the missingness indicators as binary variables as a starting point to understand your patterns of missingness.","Display_name":"MissD","Creater_id":63608,"Start_date":"2016-08-26 11:45:07","Question_id":231939}
{"_id":{"$oid":"5837a573a05283111e4d2e4f"},"Last_activity":"2016-08-26 11:25:36","Creator_reputation":573,"Question_score":2,"Answer_content":"Yes, this is usually the case when creating autoregressive (AR) models, as you can see in this question: Estimate ARMA coefficients through ACF and PACF inspectionHowever, ACF and PACF compute linear correlations, while neural networks are nonlinear, so the applicability of those techniques may not be obvious. This paper shows that they are indeed useful for feature selection on neural networks.","Display_name":"rcpinto","Creater_id":49196,"Start_date":"2016-08-26 11:25:36","Question_id":231611}
{"_id":{"$oid":"5837a573a05283111e4d2e5c"},"Last_activity":"2016-08-26 11:18:06","Creator_reputation":402,"Question_score":1,"Answer_content":"rjags uses inital value. Your theta merely have fallen from 150 to about 5 for only one iteration (from 0 to 1). The main causes are the model and eta's inital value.model.strings \u0026lt;- ...model.spec \u0026lt;- ...jags.inits \u0026lt;- ...jags \u0026lt;- ...jagstheta       # modelstate()[[1]]$theta       # iter=1's theta    # [1] 3.608781 4.668491 7.904416 6.406210js.coda[[1]][,c(\"theta[1]\",\"theta[2]\",\"theta[3]\",\"theta[4]\")]    # theta[1] theta[2] theta[3] theta[4]    # 3.608781 4.668491 7.904416 6.406210    # the same","Display_name":"cuttlefish44","Creater_id":119029,"Start_date":"2016-08-25 23:45:44","Question_id":231787}
{"_id":{"$oid":"5837a573a05283111e4d2e69"},"Last_activity":"2016-08-26 11:10:15","Creator_reputation":109,"Question_score":0,"Answer_content":"You can use The caret Package.  This package uses, among many other 100's of models, the glmnet model.  However, caret has it's own cross validation function and allows you to specify a custom evaluation function.  Within the trainControl function, you should include summaryFunction=your_custom_cv_func where your_custom_cv_func takes the same form as the built-in cv functions.  See this SO answer for details: http://stackoverflow.com/a/16866460/1569064","Display_name":"AGS","Creater_id":13304,"Start_date":"2016-08-26 11:10:15","Question_id":174396}
{"_id":{"$oid":"5837a573a05283111e4d2e76"},"Last_activity":"2016-08-26 10:58:48","Creator_reputation":12298,"Question_score":1,"Answer_content":"To add to the other answer (i.e. I agree that family=quasipoisson is likely to be the right choice).It does (all other things being equal) make sense to use a distribution that naturally applies to count data (Poisson, negative binomial; quasi-Poisson is not a distribution but fits in the same category).  However, the most important step here is to check the diagnostic plots: plot(fitted_model,which=3) will show you a scale-location plot which will give you an idea of whether the model is correctly scaling the variance with the mean (this should be an approximately even cloud of points, with a relatively constant smoothed line going through it).Based on your diagnostic plots above, I'm going to guess that most of your responses are 0 or 1 with only a few observations \u003e1 (this pattern is what gives rise to the odd lines of points in your diagnostic plots).  Under these circumstances, it might be best to collapse to the data to (0,\u003e0) and do a binomial/Bernoulli model (e.g. eb1bird_cnt\u0026gt;0)) and use family=binomial ...It is not a good idea to try to pick the best distribution by looking at the parameter summaries, for two reasons: (1) the parameter estimates and p-values don't actually tell you which model fits best; (2) looking at parameter summaries for lots of models will tempt to you to pick the one with the largest number of stars (which may not actually be the appropriate model).Also, it probably doesn't make too much sense to pick out the months that happen to be significant and say that there's something special about them; rather, test the overall effect of the month variable (e.g. via anova() or car::Anova()), then describe the overall seasonal pattern verbally/graphically.","Display_name":"Ben Bolker","Creater_id":2126,"Start_date":"2016-08-26 09:24:22","Question_id":231883}
{"_id":{"$oid":"5837a573a05283111e4d2e77"},"Last_activity":"2016-08-26 05:31:51","Creator_reputation":668,"Question_score":3,"Answer_content":"The answer is likely to be quasipoisson.This will depend a bit on how much data you have. Is it only slightly more than the number of parameters (12)? Assuming you have at least, say, 24 counts:When you model data with a poisson distribution, you are saying that the variance of that data is equal to its mean. In other words, if you predict a count of 10000, then the variance of that count is 10000 (std.dev 100).In real life, that isn't always true. Some data have more variance than this, and some less. It looks like your data has less (if we predict a count of 10000, then the variance appears to be more like 1371 rather than 10000).Your (non-quasi-)poisson model ignores that fact. It is taking the predictive variance to always be equal to the predictive mean even when the data indicates otherwise. This is why it thinks the parameters are insignificant, because it is highly overstating the predictive variance.If you only have 13-15 rows of data then it might just be that the poisson glm happens to fit very well and the residuals were smaller than expected.If the counts are reasonably large, the Gaussian distribution is a good approximation. If some counts are quite small (say, less than 25) then it works less well. Bear in mind also that if you use a Gaussian LM, the effects are additive (observing in November = +1000 birds against June, for example) rather than multiplicative (observing in November = x2 birds against June)","Display_name":"JDL","Creater_id":129051,"Start_date":"2016-08-26 05:31:51","Question_id":231883}
{"_id":{"$oid":"5837a573a05283111e4d2e84"},"Last_activity":"2016-08-26 09:39:06","Creator_reputation":825,"Question_score":4,"Answer_content":"Using the block-inverse formula, if we write the correlation matrixas M = \\left[\\begin{matrix}A \u0026amp; B\\\\B^t \u0026amp; D\\end{matrix}\\right]then the bottom right block of the inverse correlation matrix will be(D-B^tA^{-1}B)^{-1}Now assume that we break the correlation matrix into blocks of size  and , so that  is a  matrix containing the entry .  In this case, we get \\begin{align*}M^{-1}_{nn}\u0026amp;=\\frac{1}{1-B^tA^{-1}B}\\\\1-\\frac{1}{M^{-1}_{nn}}\u0026amp;=B^tA^{-1}B.\\end{align*}Next, assume WLOG (see note below) that the variables involved all have variance 1 and mean 0, so the correlation matrix is also the covariance matrix.  Then  is the covariance matrix for , and  is the vector of covariances between  and .It follows that the regression coefficients for  given  are and therefore, letting  denote the least-squares fit of  given , we get\\begin{align*}1-\\frac{1}{M^{-1}_{nn}} =B^tA^{-1}B = (A^{-1}B)^tA(A^{-1}B)\u0026amp;= \\beta^tA\\beta\\\\ \u0026amp;= Var(\\hat{X_n})\\\\ \u0026amp;= Cov(\\hat{X_n},X_n).\\end{align*}Since  by assumption, it follows that R=Cor(\\hat{X_n},X_n)=\\frac{Cov(\\hat{X_n},X_n)}{\\sqrt{Var(\\hat{X_n})}}=\\sqrt{1-\\frac{1}{M^{-1}_{nn}}}Note: as @MarkStone points out, WLOG means \"without loss of generality.\"  In this case, the assumption of mean 0 and variance 1 is without loss of generality because we can recenter and scale if necessary, and the rescaling parameters will carry through the calculations and yield the same ultimate result.","Display_name":"mpr","Creater_id":30712,"Start_date":"2016-08-26 09:27:30","Question_id":231905}
{"_id":{"$oid":"5837a573a05283111e4d2e91"},"Last_activity":"2016-08-26 09:59:58","Creator_reputation":11,"Question_score":1,"Answer_content":"Let me provide a response for the first situation that you analysed because the second situation essentially parallels it, except that you have two more items in the second situation and you chose a different model (more about that below).  In providing this response, in some places I have a different interpretation from the extended explanation that has been provided elsewhere in these posts. As I understand it, you had 17 raters (participants), each of whom provided a rating on 5-point scales to seven different items AND you are wanting to see whether there is much agreement between the 17 raters in how they rated those 7 items.  I think that, in order to do this (which is surely a pretty unusual situation; usually there are not as many as 17 raters involved in assessing something), you should have selected ABSOLUTE (not consistent) measures in SPSS, and, if your participants are the only raters of interest in this situation (I assume they are, and that you are not wanting to generalize your results to other participants / raters) you should indeed have chosen Model 3 (i.e., 2-way mixed, NOT Model 2 as you did in your second setup), which is the FIRST model offered in SPSS.  So, in essence, you have made a basic mistake in selecting the kind of ICC that provides a consistency solution SPSS.  (Sorry to give you the bad news.)Next, when you choose an ICC from the output you should choose the ICC from the row titled \"Single measures\" (i.e., .133) because each of your participants made a single rating for each of the 7 items (and I assume you entered 17 scores into the ICC analysis for each item).  If you had averaged all of your 17 participants' ratings on each item BEFORE entering the data into the ICC analysis, it would be appropriate for you to report the ICC that pertains to the Averaged measures (.519).  But, from your description, you didn't average the ratings that were made by your participants.If you had chosen Absolute rather than Consistency for your first analysis, an ICC as low as .133 would indicate that your 17 participants / raters exhibited EXTREMELY little agreement among themselves in terms of how they rated the 7 items.An article in 2016 by Trevethan in the journal Health Services and Outcomes Research Methodology provides the background for this answer as well as a lot of other information concerning the selection and interpretation of ICCs.Finally, the small number of items (7 in the first situation) might create some problems statistically.  I am sorry, but I am not able to provide advice about that. Maybe it's OK in your situation, but it might be advisable to consult a friendly statistician.ReferencesTrevethan, R. (2016). \"Intraclass correlation coefficients: Clearing the air, extending some cautions, and making some requests.\" Health Services and Outcomes Research Methodology. DOI 10.1007/s10742-016-0156-6. (Online publication available until volume, issue, and page numbers have been assigned.)","Display_name":"Robert Trevethan","Creater_id":128797,"Start_date":"2016-08-24 09:02:44","Question_id":64725}
{"_id":{"$oid":"5837a573a05283111e4d2e92"},"Last_activity":"2015-12-21 23:29:41","Creator_reputation":27863,"Question_score":1,"Answer_content":"You might want to read the article by LeBreton and Senter (2007). It's a fairly accessible overview of how to interpret ICC and related indicators of inter-rater agreement.LeBreton, J. M., \u0026amp; Senter, J. L. (2007). Answers to 20 questions about interrater reliability and interrater agreement. Organizational Research Methods.","Display_name":"Jeromy Anglim","Creater_id":183,"Start_date":"2015-12-21 23:29:41","Question_id":64725}
{"_id":{"$oid":"5837a573a05283111e4d2e93"},"Last_activity":"2013-07-25 06:06:11","Creator_reputation":26139,"Question_score":5,"Answer_content":"  I am struggling to find anything online which deals with interpreting  thisThe output you present is from SPSS Reliability Analysis procedure. Here you had some variables (items) which are raters or judges for you, and 17 subjects or objects which were rated. Your focus was to assess inter-rater aggreeement by means of intraclass correlation coefficient.In the 1st example you tested p=7 raters, and in the 2nd you tested p=9.More importantly, your two outputs differ in the respect how the raters are considered. In the 1st example, the raters are a fixed factor, which means they are the population of raters for you: you infer about only these specific raters. In the 2nd example, the raters are a random factor, which means they are a random sample of raters for you, while you want infer about the population of all possible raters which those 9 pretend to represent.The 17 subjects that were rated constitute a random sample of population of subjects. And, since each rater rated all 17 subjects, both models are complete two-way (two-factor) models, one is fixed+random=mixed model, the other is random+random=random model.Also, in both instances you requested to assess the consistency between raters, that is, how well their ratings correlate, - rather than to assess the absolute agreement between them - how much identical their scores are. With measuring consistency, Average measures ICC (see the tables) are identical to Cronbach's alpha. Average measures ICC tells you how reliably the/a group of p raters agree. Single measures ICC tells you how reliable is for you to use just one rater. Because, if you know the agreement is high you might choose to inquire from just one rater for that sort of task.If you tested the same number of the same raters (and the same subjects) under both models you'd see that the estimates in the table are the same under both models. However, as I've said, the interpretation differs in that you can generalize the conclusion about the agreement onto the whole population of raters only with two-way random model. You can see also a footnote saying that the mixed model assumes there is no rater-subject interaction; to put clearer, it means that the raters lack individual partialities to subjects' characteristics not relevant to the rated task (e.g. to hair colour of an examenee).SPSS Reliability Analysis procedure assumes additivity of scores (which logically implies interval or dichotomous but not ordinal level of data) and bivariate normality between items/raters. However, F test is quite robust.","Display_name":"ttnphns","Creater_id":3277,"Start_date":"2013-07-18 10:54:52","Question_id":64725}
{"_id":{"$oid":"5837a573a05283111e4d2e94"},"Last_activity":"2013-07-19 01:28:54","Creator_reputation":134,"Question_score":0,"Answer_content":"I have traced the answer in new Stata 13 documentation onICC.The question remains on whether the F test in this case can be used given the data does not follow the assumptions of normal distribution.","Display_name":"Cesare Camestre","Creater_id":26267,"Start_date":"2013-07-18 23:45:02","Question_id":64725}
{"_id":{"$oid":"5837a573a05283111e4d2ea1"},"Last_activity":"2016-08-26 09:47:09","Creator_reputation":3508,"Question_score":0,"Answer_content":"Note that the Kenward-Roger method bases on REML but introduces a modification on the estimated covariance matrix for small samples. See their original work in Biometrics (1997): \"Small sample inference for fixed effects from REML\". So it would not make sense to implement their method with something else than REML.","Display_name":"Horst Gr\u0026#252;nbusch","Creater_id":28705,"Start_date":"2016-08-26 09:47:09","Question_id":231900}
{"_id":{"$oid":"5837a573a05283111e4d2eb0"},"Last_activity":"2016-01-06 23:09:39","Creator_reputation":1384,"Question_score":1,"Answer_content":"Check out APPENDIX: THE REML ESTIMATION METHOD from within this SAS-related resource from author David Dickey. \"We can always find (n-1) numbers Z with known mean 0 and thesame sum of squares and theoretical variance as the n Y values. This motivates the division of the Z sum of squares by the number of Zs, which is n-1.\"When I was in grad school, REML was made out to be the best thing since sliced bread. From studying the lme4 package, I learned that it doesn't really generalize that well and maybe it isn't that important in the grand scheme of things.","Display_name":"Ben Ogorek","Creater_id":35131,"Start_date":"2016-01-06 23:09:39","Question_id":151654}
{"_id":{"$oid":"5837a573a05283111e4d2eb1"},"Last_activity":"2016-01-06 22:34:21","Creator_reputation":3023,"Question_score":3,"Answer_content":"The bias in the variance stemms from the fact that the mean has been estimated from the data and therefore the 'spread of that data around this estimated mean' (i.e. tha variance) is smaller than the spread of the data around the 'true' mean. See also : Intuitive explanation for dividing by  when calculating standard deviation?The fixed effets determine the model 'for the mean', therefore, if you can find a variance estimate that was derived without estimating the mean from the data (by 'marginalising out the fixed effects (i.e. the mean)') then this underestimation of the spread (i.e. variance) will be mitigated. This is the 'intuitive' understanding why REML estimates eliminate the bias; you find an estimate for the variance without using the 'estimated mean'.   ","Display_name":"fcop","Creater_id":83346,"Start_date":"2015-09-08 05:36:47","Question_id":151654}
{"_id":{"$oid":"5837a573a05283111e4d2ec0"},"Last_activity":"2016-08-26 08:31:19","Creator_reputation":4408,"Question_score":1,"Answer_content":"For accessing a  complexity of a model, number of free parameters is a good start, with it you can calculate AIC or BIC from number of free parameters. And getting number of free parameters in a Multi Layer Perception (MLP) neural network can be found here: Number of parameters in an artificial neural network for AICIn addition, there are some cases, that you have a lot parameters, but they are not \"totally free\" / with regularization. For example, for linear regression, if you have  features but  data points, it is totally OK to fit a model with  coefficients, but regularize the coefficients with a large regularization parameter. You can search Ridge Regression or Lasso Regression for details.In Neural network case, it is also possible people have a very compacted network structure (many layers many neurons) but with some regularization in there. In that case, the method mentioned above will not work.Finally, I would not agree your statement about random forest. As discussed in Breiman's original paper: in creasing number of trees is will not lead a more complex model / have over fitting. Instead, the out of bag (OOB) error will converge, if you have large number of trees. In practice, if computational power is not a concern, building a random forest with large number trees is actually recommended.To your comment:The model complexity is an abstract concept, and can be defined in different ways. AIC and BIC are some definitions and other way of defining it exists. See this Definition of model complexity in XGBoost as an example.In addition, it is fine, if two NN has different structure, but it is still can have same complexity. Here is an example: say, we are doing polynomial regression. You have 2 ways, one is have a higher order model with more regularization, another is lower order without regularization. You can have same \"complexity\" but the structure are different.","Display_name":"hxd1011","Creater_id":113777,"Start_date":"2016-08-26 06:47:07","Question_id":231899}
{"_id":{"$oid":"5837a573a05283111e4d2ecd"},"Last_activity":"2016-08-26 08:26:23","Creator_reputation":16,"Question_score":0,"Answer_content":"Yes, by definition of the Kullback-Leibler divergence.Yes, since  if and only if  almost surely.","Display_name":"Kassio","Creater_id":129101,"Start_date":"2016-08-26 08:26:23","Question_id":231917}
{"_id":{"$oid":"5837a573a05283111e4d2eda"},"Last_activity":"2016-08-24 17:50:07","Creator_reputation":4307,"Question_score":1,"Answer_content":"Savitzky-Golay filters are used to estimate smoothed values for a function and its derivatives, given  pairs. Kernel Density Estimates are used to estimate smoothed values for a probability density function , given a sample of  points.I am not sure if this answers your question, but moving least squares is a generalization of Savitzky-Golay that uses an arbitrary kernel to weight the points. Savitzky-Golay filtering corresponds to the case of a uniform (\"box\") kernel, when the function  is sampled on a uniform  grid.Now in terms of smoothing, that can actually be an issue with higher-order Savitzky-Golay filters (i.e. Runge's phenomenon). In general if some kernel/filter has negative weights, then it can easily result in roughening of the input signal. The simplest example are derivative filters, but even resampling kernels can have these effects. In statistics typically a kernel will be non-negative, so these image processing \"interpolation kernels\" (e.g. Lanczos) are really a different beast.","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-24 17:24:56","Question_id":231529}
{"_id":{"$oid":"5837a573a05283111e4d2ee9"},"Last_activity":"2014-11-27 14:45:30","Creator_reputation":152613,"Question_score":12,"Answer_content":"Some points to start with:i) these distributional conventions are at best approximations. They can be convenient models, but we shouldn't confuse that with the actual distribution of stock prices or returns.ii) stock prices are typically increasing (but in any case, have changing mean; the mean isn't stable). So when we're talking about the distribution of stock prices, we're usually not referring to their marginal distribution, but a conditional distribution. So we often tend to mean something more like  is approximately lognormal with mean changing with  (specifically, conditionally lognormal, conditional on some previous value and elapsed time). The variance, too, can change, in which case both mean and variance condition on some previous value and time. So for example, by \"stock prices are approximately lognormal\" we might mean   or equivalently iii) Note that for small , .For short period returns, such as daily returns, generally,  is quite small, typically on the order of 0.01 - or often less - in absolute value.When that ratio is small, That is, the return is approximately the change in log stock price (try it with real stock prices and see they're almost identical).So if  y_t \\, \\dot{\\sim} \\, \\log N(\\log(y_{t-1})+\\mu_\\text{daily},\\sigma^2_\\text{daily}) which implies\\log(y_t) \\, \\dot{\\sim} \\, N(\\log(y_{t-1})+\\mu_\\text{daily},\\sigma^2_\\text{daily}) then \\frac{y_t-y_{t-1}}{y_{t-1}}\\approx \\log(y_t)-\\log(y_{t-1})\\,\\dot{\\sim}\\,N(\\mu_\\text{daily},\\sigma^2_\\text{daily})","Display_name":"Glen_b","Creater_id":805,"Start_date":"2014-11-27 14:34:19","Question_id":125761}
{"_id":{"$oid":"5837a573a05283111e4d2ef8"},"Last_activity":"2016-08-26 07:36:43","Creator_reputation":63,"Question_score":0,"Answer_content":"I think what you're after is:   model1 \u0026lt;- lmer(rto_Embryos ~ Pollution * Temperature + (1 | Population), data = mydata, REML = F)This syntax reflects the hierarchical nature of your data while enabling you to assess if the effect of temperature varies in presence of pollution. Keeping pollution in both, the random and the fixed portions of the model is incorrect. It seems you were trying to interact the fixed term with the random term but this is not how it works in lme4, check this and this for useful examples on lme4 syntax.One aspect of your model that may cause problems, as Robert Long points out in the comments, is the low number of levels you have on the random effects. According to the mixed-models faq a minimum of 5 or 6 levels is required to have a reliable estimation of the variance. Conceptually, in my opinion, you should use a hierarchical model, but in practice you might run into issues with such low number of levels (i.e. your variance may be estimated to be 0 and/or the distribution of your random effects might be different from 0), if that's the case, including Population as a fixed term could be a viable workaround.Also, for comparison with other models through likelihood ratio tests, it's ok to keep REML = F but you should set it to REML = TRUE for your final model.Hope this helps","Display_name":"donlelek","Creater_id":41834,"Start_date":"2016-08-24 09:38:55","Question_id":230978}
{"_id":{"$oid":"5837a573a05283111e4d2f05"},"Last_activity":"2016-08-26 07:17:09","Creator_reputation":584,"Question_score":1,"Answer_content":"Ok, I managed to get the answer I wanted, which also explains why the estimator is unbiased and consistent. Here it is:The model is:y_{it} = x_{it}\\beta + c_{i} + \\epsilon_{it} From RE we obtain an estimation of . Define the estimation error : \\hat{u}_{it} \\equiv y_{it} - x_{it}\\hat{\\beta} Now, define the linear predictor  as the mean of the estimation error:\u0009\\bar{u}_{it} \\equiv \\frac{\\sum_{t=1}^{T}\\hat{u}_{i}}{T} = \\bar{y_{it}} - \\bar{x}_{i}\\hat{\\beta}  This is, allegedly, the BLUP estimator of . To confirm this, let us evaluate the statistical properties of this predictor. To do this, replace the original model into the above expression. After some rearranging, the outcome is: \\bar{u}_{it} = \\bar{x}_{i}\\beta - \\bar{x}_{i}\\hat{\\beta} + c_{i} + \\frac{\\sum_{t=1}^{T}\\epsilon_{it}}{T} The expectation of this estimator is:\u0009E(\\bar{u}_{it}) = \\bar{x}_{i}\\beta - \\bar{x}_{i}E(\\hat{\\beta}) + E(c_{i}) + \\frac{\\sum_{t=1}^{T}E(\\epsilon_{it})}{T} Assume  is an unbiased estimator of  (requires strict exogeneity, unobserved component orthogonal to regressors, and rank condition). Moreover,  (trivial when constant included in ). In consequence,  is an unbiased estimator of .Regarding consistency, the probability limit of this predictor is: p \\lim\\limits_{T \\rightarrow \\infty} \\bar{u}_{it} =  p \\lim\\limits_{T \\rightarrow \\infty} \\left( \\bar{x}_{i}\\beta\\right) - p \\lim\\limits_{T \\rightarrow \\infty} \\left(\\bar{x}_{i}\\hat{\\beta}\\right) + p \\lim\\limits_{T \\rightarrow \\infty} c_{i}  + p \\lim\\limits_{T \\rightarrow \\infty} \\left( \\frac{\\sum_{t=1}^{T}\\epsilon_{it}}{T}\\right)  Again,  is a consistent estimator of . This is, . Furthermore, , which is zero. Therefore: p \\lim\\limits_{T \\rightarrow \\infty} \\bar{u}_{it} = c_{i} Or, equivalently: \\bar{u}_{it} \\xrightarrow{P} c_{i} This proves that the predictor is indeed BLUP.","Display_name":"luchonacho","Creater_id":100369,"Start_date":"2016-08-26 07:17:09","Question_id":231437}
{"_id":{"$oid":"5837a573a05283111e4d2f06"},"Last_activity":"2016-08-24 11:11:36","Creator_reputation":518,"Question_score":2,"Answer_content":"So the true model has the unobserved individual level time invariant heterogeneity:So we estimate: , where Use pooled ols to get  and let  and More info here: http://www.utdallas.edu/~d.sul/Econo1/lec_note_part3.pdf","Display_name":"VCG","Creater_id":124896,"Start_date":"2016-08-24 11:11:36","Question_id":231437}
{"_id":{"$oid":"5837a573a05283111e4d2f13"},"Last_activity":"2012-02-10 00:40:23","Creator_reputation":649,"Question_score":2,"Answer_content":"In chapter 5 of Data Mining with R, the author shows some ways to choose the most useful predictors. (In the context of bioinformatics, where each sample row has 12,000+ columns!)He first uses some filters based on statistical distribution. For instance, if you have half a dozen predictors all with a similar mean and s.d. then you can get away with just keeping one of them.He then shows how to use a random forest to find which ones are most useful predictors. Here is a self-contained abstract example. You can see I've got 5 good predictors, 5 bad ones. The code shows how to just keep the best 3.set.seed(99)d=data.frame(  y=c(1:20),  x1=log(c(1:20)),  x2=sample(1:100,20),  x3=c(1:20)*c(11:30),  x4=runif(20),  x5=-c(1:20),  x6=rnorm(20),  x7=c(1:20),  x8=rnorm(20,mean=100,sd=20),  x9=jitter(c(1:20)),  x10=jitter(rep(3.14,20))  )library(randomForest)rf=randomForest(y~.,d,importance=T)print(importance(rf))#         %IncMSE IncNodePurity# x1  12.19922383    130.094641# x2  -1.90923082      6.455262# ...i=importance(rf)best3=rownames(i)[order(i[,\"%IncMSE\"],decreasing=T)[1:3]]print(best3)#[1] \"x1\" \"x5\" \"x9\"reduced_dataset=d[,c(best3,'y')]The author's last approach is using a hierarchical clustering algorithm to cluster similar predictors into, say, 30 groups. If you want 30 diverse predictors you then choose one from each of those 30 groups, randomly.Here is some code, using the same sample data as above, to choose 3 of the 10 columns:library(Hmisc)d_without_answer=d[,names(d)!='y']vc=varclus(as.matrix(d_without_answer))print(cutree(vc$hclust,3))# x1  x2  x3  x4  x5  x6  x7  x8  x9 x10 #  1   2   1   3   1   1   1   2   1   3 My sample data does not suit this approach at all, because I have 5 good predictors and 5 that are just noise. If all 10 predictors were slightly correlated with y, and had a good chance of being even better when used together (which is quite possible in the financial domain), then this may be a good approach.","Display_name":"Darren Cook","Creater_id":5503,"Start_date":"2012-02-10 00:40:23","Question_id":22393}
{"_id":{"$oid":"5837a573a05283111e4d2f14"},"Last_activity":"2012-02-08 00:37:06","Creator_reputation":126,"Question_score":1,"Answer_content":"This problem is usually called Subset Selection and there are quite a few different approaches. See Google Scholar for an overview over related articles.","Display_name":"Florian Brucker","Creater_id":9038,"Start_date":"2012-02-08 00:37:06","Question_id":22393}
{"_id":{"$oid":"5837a573a05283111e4d2f15"},"Last_activity":"2012-02-07 21:14:10","Creator_reputation":3481,"Question_score":2,"Answer_content":"You might consider using a method like LASSO that regularizes least squares by selecting a solution that minimizes the one norm of the vector of parameters.  It turns out that this has the effect in practice of minimizing the number of nonzero entries in the parameter vector.    Although LASSO is popular in some statistical circles many other related methods have been considered in the world of compressive sensing.    ","Display_name":"Brian Borchers","Creater_id":8200,"Start_date":"2012-02-07 21:14:10","Question_id":22393}
{"_id":{"$oid":"5837a573a05283111e4d2f16"},"Last_activity":"2012-02-07 10:36:43","Creator_reputation":39261,"Question_score":6,"Answer_content":"Method 1 doesn't work.  Method 2 has hope depending on how you do it.  It's better to enter principal components in descending order of variance explained.  A more interpretable approach is to do variable clustering, then reducing each cluster to a single score (not using Y), then fit a model with the cluster scores.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2012-02-07 10:36:43","Question_id":22393}
{"_id":{"$oid":"5837a573a05283111e4d2f23"},"Last_activity":"2016-08-26 07:05:31","Creator_reputation":584,"Question_score":1,"Answer_content":"I'm afraid your exploration so far of the intuition behind  is incorrect. By definition, the correction mechanism is asymptotic. This means that it always takes infinite time to adjust. As such,  cannot be associated with a time measure. To back up my claims, and without loss of generality, consider a simplified model: \\Delta y_{t} = -b(y_{t-1} - \\bar{y}) + e_{t} Note I have defined  as positive. This is just for simplicity.Assume that there was a shock in  so that , and there was no shock thereafter (you can adapt this to put the shock in period  and then look into the future). This is, .Then, we can re-write the model as: y_{t} - y_{t-1}  = -b(y_{t-1} - \\bar{y}) Rearranging: y_{t} = (1-b)y_{t-1} + b\\bar{y} This is, current level is a weighted average of previous level and long-run level. This should immediate deny any relation between time and . But let's continue.Now, iterate from the above equation using backward substitution. You will get: y_{t} = (1-b)^t y_{0} + b\\bar{y}\\big(1 + (1-b) + (1-b)^2 + \\cdots + (1-b)^{t-1}\\big) The sum inside the parenthesis is a geometric series. Thus, the equation can be written as: y_{t} = (1-b)^t y_{0} + b\\bar{y}\\big(\\frac{1-(1-b)^{t}}{1-(1-b)}\\big) Simplifying the : y_{t} = (1-b)^t y_{0} + \\bar{y}\\big(1-(1-b)^{t}\\big) Rearranging: y_{t} = \\bar{y} + (1-b)^t(y_{0} - \\bar{y}) From here you can see that convergence into the long-run level, or steady state, only happens asymptotically, for . This is: \\lim\\limits_{t \\rightarrow \\infty} y_{t} = \\bar{y} To conclude,  has no time interpretation, as convergence always takes infinite time. The correct interpretation of  is as the speed of adjustment. Although apparently counterintuitive, they are unrelated.","Display_name":"luchonacho","Creater_id":100369,"Start_date":"2016-08-24 05:58:37","Question_id":228683}
{"_id":{"$oid":"5837a573a05283111e4d2f35"},"Last_activity":"2016-08-26 06:22:34","Creator_reputation":1,"Question_score":-1,"Answer_content":"Competitive mediation is familiar for me because we studied the Baron and Kenny (1987) paper. We learned the concept in master level research methods in M.S organizational and industrial psychology in Iran / Isfahan. Competitive mediation is a type of partial mediation that occurs when is removed the mediating variable relationship between X and Y is existed too and significant.  Competitive mediation and complementary mediation are two types of partial mediation. When we cross these signs of a, b, and c together, if result is negative, we deduce this is competitive mediation. But if, when we cross these signs 0f a, b, and c toghether, the result is positive, we deduce this is a complementary mediation.","Display_name":"farzaneh","Creater_id":129059,"Start_date":"2016-08-26 02:24:28","Question_id":92693}
{"_id":{"$oid":"5837a573a05283111e4d2f36"},"Last_activity":"2014-04-05 22:51:53","Creator_reputation":2036,"Question_score":1,"Answer_content":"The positive sign for the a path means that  is positively associated with , and the negative sign for the b path merely means that  is negatively associated with  (as  increases,  tends to decrease). In other words, increases in  predict increases in , which, in turn, predict decreases in . The significant mediation indicates that part of the statistical association between  and  is transmitted indirectly through changes in , regardless of the specific sign for the a and b paths.I'm not familiar with the term \"competitive mediation\". However, it seems to refer to the fact that the direct effect of  on  is positive, whereas the mediation model suggests that increases in  are indirectly associated with decreases in , through increases in . ","Display_name":"Patrick Coulombe","Creater_id":24808,"Start_date":"2014-04-05 22:42:14","Question_id":92693}
{"_id":{"$oid":"5837a573a05283111e4d2f45"},"Last_activity":"2013-12-08 15:35:48","Creator_reputation":9087,"Question_score":3,"Answer_content":"There are two basic criteria used to apply the strong law of large numbers. One of them requires that the variables are IID and that their expectation be finite. Since  are IID, so are , and  is finite. So \\frac{\\sum_{i=1}^nX_i^2}{n}converges almost surely to the expected value of , that is to say 5. This illustrates the power of the strong law of large numbers. The criteria to apply it are very general and very often met in practice.For the last question, rewrite the ratio as \\frac{\\sum_{i=1}^nX_i}{\\sqrt{5n}}\\frac{\\sqrt{5}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^nX_i^2}}.The first term converges in distribution to  and the second term converges almost surely to 1. According to Slutsky's lemma (third item), the whole converges in distribution to .","Display_name":"gui11aume","Creater_id":10849,"Start_date":"2013-12-08 15:35:48","Question_id":79007}
{"_id":{"$oid":"5837a573a05283111e4d2f52"},"Last_activity":"2015-12-16 16:44:11","Creator_reputation":770,"Question_score":1,"Answer_content":"There is something not quite right about the equation at the end of your question. On the left you have , but on the right  still appears as a finite number. Fortunately, your question can be answered in a straightforward way using the Strong Law of Large Numbers. First, though, you have to understand what you mean by \"iid samples\". These samples are actually random variables. Formally, we think of iid samples as being multiple independent copies of , which we denote by , all with the same distribution. Likewise, we also consider independent identically distributed copies  of . Then the claim would be that  almost surely i.e. everywhere except perhaps on a set of probability zero.To prove this, you apply the Strong Law of Large Numbers to the sequence , giving  almost surely. Next apply the Strong Law to , telling you that . Therefore  almost surely. You can apply the same argument to the  random variables as well. Then divide, to get the conclusion you were looking for.Note that you don't need independence between  and  for this conclusion to hold. And you don't need normality, only that the expectation and variance of  and  are finite. ","Display_name":"S. Catterall","Creater_id":86998,"Start_date":"2015-12-16 16:44:11","Question_id":186600}
{"_id":{"$oid":"5837a573a05283111e4d2f5f"},"Last_activity":"2016-04-05 01:17:06","Creator_reputation":13273,"Question_score":4,"Answer_content":"It might be clearer to state the weak law as \\overline{Y}_n\\ \\xrightarrow{P}\\ \\mu \\,\\textrm{ when }\\ n \\to \\infty , \\text{ i.e. } \\forall \\varepsilon \\gt 0: \\lim_{n\\to\\infty}\\Pr\\!\\left(\\,|\\overline{Y}_n-\\mu| \\lt \\varepsilon\\,\\right) = 1and the strong law as \\overline{Y}_n\\ \\xrightarrow{a.s.}\\ \\mu \\,\\textrm{ when }\\ n \\to \\infty , \\text{ i.e. } \\Pr\\!\\left( \\lim_{n\\to\\infty}\\overline{Y}_n = \\mu \\right) = 1You might think of the weak law as saying that the sample average is usually close to the mean when the sample size is big, and the strong law as saying the sample average almost certainly converges to the mean as the sample size grows.  The difference happens when failures of the sample average to be close to the mean are big enough to prevent convergence.As an illustration using R, take Wikipedia's first example, with  being exponentially distributed random variable with parameter  and  so .  Let's consider  cases where the sample size is :set.seed(1)cases \u0026lt;- 100samplesize \u0026lt;- 10000Xmat \u0026lt;- matrix(rexp(samplesize*cases, rate=1), ncol=samplesize)Ymat \u0026lt;- sin(Xmat) * exp(Xmat) / Xmatplot(samplemeans \u0026lt;- rowMeans(Ymat),     main=\"most sample averages close to expectation\")abline(h=pi/2, col=\"red\")but now look at the failure of the running sample average over the same  million observations to get to the mean and stay thereplot(cumsum(Ymat)/(1:(samplesize*cases)),    main=\"running sample average not always converging to expectation\")abline(h=pi/2, col=\"red\") ","Display_name":"Henry","Creater_id":2958,"Start_date":"2016-04-05 01:17:06","Question_id":205496}
{"_id":{"$oid":"5837a573a05283111e4d2f6c"},"Last_activity":"2013-01-30 12:51:20","Creator_reputation":1293,"Question_score":1,"Answer_content":"The mathematical formulations of the \"Strong\" and \"Weak\" Laws of Large Numbers look somewhat similar. Yet, the two Laws are quite different in nature :The Weak Law never considers infinite sequences of realizations of a random variable. It only states that imbalanced sequences are less likely to occur as one considers longer sequences.On the other hand, the Strong Law considers only infinite sequences of realizations of a random variable, and more precisely, the set of these infinite sequences. It states that the set of imbalanced sequences has probability 0 in a sense that generalizes the concept of \"set of measure 0\".It can be shown that the Strong Law implies the Weak Law, which can therefore be regarded as a consequence of the Strong Law.The converse is, however, wrong : it is possible to exhibit sequences of r.v.s following the Weak Law, but not the Strong one. So the terms \"Weak\" and \"Strong\" are indeed justified. For example, Let your sequence be i.i.d. with densityYou can obtain a WLLN but not a SLLN, due to the Borel-Cantelli lemma. ","Display_name":"Blain Waan","Creater_id":12603,"Start_date":"2013-01-30 12:51:20","Question_id":47310}
{"_id":{"$oid":"5837a573a05283111e4d2f79"},"Last_activity":"2012-02-10 05:51:43","Creator_reputation":20197,"Question_score":5,"Answer_content":"The OP says  The central limit theorem states that the mean of i.i.d. variables, as N goes to infinity, becomes normally distributed.I will take this to mean that it is the OP's belief that for i.i.d. randomvariables  with mean  and standard deviation , the cumulative distribution function  ofZ_n = \\frac{1}{n} \\sum_{i=1}^n X_iconverges to the cumulative distribution function of ,a normal randomvariable with mean  and standard deviation .  Or, theOP believes minor re-arrangements of this formula, e.g. the distributionof  converges to the distribution of ,or the distribution of  converges to the distribution of , the standard normal random variable.  Note as an examplethat these statements imply that P\\{|Z_n - \\mu| \u0026gt; \\sigma\\} = 1 - F_{Z_n}(\\mu + \\sigma) + F_{Z_n}((\\mu + \\sigma)^-) \\to 1-\\Phi(1)+\\Phi(-1) \\approx 0.32as .The OP goes on to say  This raises two questions:      Can we deduce from this the law of large numbers? If the law of large numbers says that the mean of a sample of a random variable's values equals the true mean μ as N goes to infinity, then it seems even stronger to say that (as the central limit says) that the value becomes N(μ,σ) where σ is the standard deviation.  The weak law of large numbers says that for i.i.d. random variables with finite mean , given any ,P\\{|Z_n - \\mu| \u0026gt; \\epsilon\\} \\to 0 ~~ \\text{as}~ n \\to \\infty.Note that it is not necessary to assume that the standard deviation isfinite.So, to answer the OP's question,The central limit theorem as stated by the OP does not implythe weak law of large numbers.  As , the OP's version of the central limit theorem says that  whilethe weak law says that From a correct statement of the central limit theorem, one can at best deduce only a restricted form of the weak law of large numbersapplying to random variables with finite mean and standarddeviation.  But the weak law of large numbers also holds for randomvariables such as Pareto random variables with finite means butinfinite standard deviation.I do not understand why saying that the sample mean convergesto a normal random variable with nonzero standard deviation isa stronger statement than saying that the sample mean convergesto the population mean, which is a constant (or a random variablewith zero standard deviation if you like). ","Display_name":"Dilip Sarwate","Creater_id":6633,"Start_date":"2012-02-10 05:51:43","Question_id":22557}
{"_id":{"$oid":"5837a573a05283111e4d2f7a"},"Last_activity":"2012-02-09 20:41:49","Creator_reputation":21558,"Question_score":5,"Answer_content":"For law of large numbers, you need to have all variables to be defined on the same probability space (as the law of large numbers is a statement about probability of an event determined by , for all  simultaneously). For convergence in distribution, you can have different probability spaces, and that simplifies many aspects of the proofs (e.g., increasing nested spaces, very common for various triangular array proofs). But it also means you cannot make any statements concerning the joint distributions of  and , say. So no, convergence in distribution does not imply the law of large numbers, unless you have a common probability space for all variables.","Display_name":"StasK","Creater_id":5739,"Start_date":"2012-02-09 20:41:49","Question_id":22557}
{"_id":{"$oid":"5837a573a05283111e4d2f7b"},"Last_activity":"2012-02-09 19:23:53","Creator_reputation":602,"Question_score":2,"Answer_content":"First, though there are many definitions, one of the standard forms of the central limit theorem says that  converges in distribution to , where  is the sample mean of  iid copies of some random variable . Secondly, suppose we have two independent random variables  and . Then \\sqrt{n}(\\frac{1}{n}\\sum_{j=1}^n(aX_j+Y_j) - E(aX+Y)) \\to \\mathcal N(0, Var(aX+Y))or\\sqrt{n}a(\\bar{X}_n- EX)+\\sqrt{n}(\\bar{Y}_n -EY) \\to \\mathcal N(0, a^2Var(X)+Var(Y)).In other words, a linear combination of random variables wont converge to a linear combination of normals under the CLT, just one normal. This makes sense because a linear combination of random variables is just a different random variable that CLT can be applied to directly. ","Display_name":"Daniel Johnson","Creater_id":8242,"Start_date":"2012-02-09 19:23:53","Question_id":22557}
{"_id":{"$oid":"5837a573a05283111e4d2f88"},"Last_activity":"2011-10-11 08:56:22","Creator_reputation":999,"Question_score":3,"Answer_content":"The answer to your second question is yes: The sample mean is a minimum contrast estimator when your function  is , when x and u are real numbers, or , when x and u are column vectors. This follows from least-squares theory or differential calculus.  A minimum contrast estimator is, under certain technical conditions, both consistent and asymptotically normal.  For the sample mean, this already follows from the LLN and the central limit theorem.  I don't know that minimum contrast estimators are \"optimal\" in any way. What's nice about minimum contrast estimators is that many robust estimators (e.g. the median, Huber estimators, sample quantiles) fall into this family, and we can conclude that they are consistent and asymptotically normal just by applying the general theorem for minimum contrast estimators, so long as we check some technical conditions (though often this is much difficult than it sounds).One notion of optimality that you don't mention in your question is efficiency which, roughly speaking, is about how large a sample you need to get an estimate of a certain quality.  See http://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency for a comparison of the efficiency of mean and median (mean is more efficient, but the median is more robust to outliers). For the third question, without some restriction on the set of functions f over which you are finding the argmin, I don't think the sample mean will be optimal. For any distribution P, you can fix f to be a constant that ignores the 's and minimizes the loss for the particular P.  Sample mean can't beat that. Minimax optimality is a weaker condition than the one you give: instead of asking that  be the best function for any  in a class, you can ask that  have the best worst-case performance.  That is, between the argmin and the expectation, put in a .  Bayesian optimality is another approach: put a prior distribution on , and take the expectation over  as well as the sample from .","Display_name":"DavidR","Creater_id":6640,"Start_date":"2011-10-04 20:57:28","Question_id":16507}
{"_id":{"$oid":"5837a573a05283111e4d2f89"},"Last_activity":"2011-10-04 18:05:26","Creator_reputation":226,"Question_score":1,"Answer_content":"Another way to characterize an estimator:Please read about Consistent Estimator here.Sample Mean is consistent due to LLN.","Display_name":"Rohit Banga","Creater_id":6076,"Start_date":"2011-10-04 18:05:26","Question_id":16507}
{"_id":{"$oid":"5837a573a05283111e4d2f96"},"Last_activity":"2014-01-05 05:43:38","Creator_reputation":9087,"Question_score":7,"Answer_content":"There are two theorems (of Kolmogorov) and both require that the expected value be finite. The first holds when variables are IID, the second, when sampling is independent and the variance of the  satisfies\\sum_{n=1}^\\infty \\frac{V(X_n)}{n^2} \u0026lt; \\inftySay that all  have expected value 0, but their variance is  so that the condition obviously fails. What happens then? You can still compute an estimated mean, but that mean will not tend to 0 as you sample deeper and deeper. It will tend to deviate more and more as you keep sampling.Let's give an example. Say that  is uniform  so that the condition above fails epically.\\sum_{n=1}^\\infty \\frac{V(X_n)}{n^2} = \\sum_{n=1}^\\infty \\frac{n^2 2^{2n+2}}{12}\\frac{1}{n^2} = \\frac{1}{3} \\sum_{n=1}^\\infty 4^n = \\infty.By noting that\\bar{X}_n = \\frac{X_n}{n} + \\frac{n-1}{n}\\bar{X}_{n-1},we see by induction that the computed average  is always within the interval . By using the same formula for , we also see that there is always a chance greater than  that  lies outside . Indeed,  is uniform  and lies outside  with probability . On the other hand,  is in  by induction, and by symmetry it is positive with probability . From these observations it follows immediately that  is greater than  or smaller than ,each with a probability larger than . Since the probability that  is greater than , there cannot be convergence to 0 as  goes to infinity.Now, to specifically answer your question, consider an event . If I understood well, you ask \"in what conditions is the following statement false?\" \\lim_{n \\rightarrow \\infty} \\frac{1}{n}\\sum_{k = 1}^{n} 1_A(X_k) = P(X \\in A), \\; [P]\\;a.s.where  is the indicator function of the event , i.e.  if  and  otherwise and the  are identically distributed (and distributed like ).We see that the condition above will hold, because the variance of an indicator function is bounded above by 1/4, which is the maximum variance of a Bernouilli 0-1 variable. Still, what can go wrong is the second assumption of the strong law of large numbers, namely independent sampling. If the random variables  are not sampled independently then convergence is not ensured.For example, if  =  for all  then the ratio will be either 1 or 0, whatever the value of , so convergence does not occur (unless  has probability 0 or 1 of course). This is a fake and extreme example. I am not aware of practical cases where convergence to the theoretical probability will not occur. Still, the potentiality exists if sampling is not independent.","Display_name":"gui11aume","Creater_id":10849,"Start_date":"2012-06-06 05:43:30","Question_id":29882}
{"_id":{"$oid":"5837a573a05283111e4d2fa3"},"Last_activity":"2013-02-02 11:44:15","Creator_reputation":147175,"Question_score":17,"Answer_content":"This figure shows the distributions of the means of  (blue),  (red), and  (gold) independent and identically distributed (iid) normal distributions (of unit variance and mean ):As  increases, the distribution of the mean becomes more \"focused\" on .  (The sense of \"focusing\" is easily quantified: given any fixed open interval  surrounding , the amount of the distribution within  increases with  and has a limiting value of .)However, when we standardize these distributions, we rescale each of them to have a mean of  and a unit variance: they are all the same then.  This is how we see that although the PDFs of the means themselves are spiking upwards and focusing around , nevertheless every one of these distributions is still has a Normal shape, even though they differ individually.The Central Limit Theorem says that when you start with any distribution--not just a normal distribution--that has a finite variance, and play the same game with means of  iid values as  increases, you see the same thing: the mean distributions focus around the original mean (the Weak Law of Large Numbers), but the standardized mean distributions converge to a standard Normal distribution (the Central Limit Theorem).","Display_name":"whuber","Creater_id":919,"Start_date":"2013-02-02 11:44:15","Question_id":49123}
{"_id":{"$oid":"5837a573a05283111e4d2fb0"},"Last_activity":"2013-08-20 12:15:44","Creator_reputation":91,"Question_score":1,"Answer_content":"I think what you need is a zero-one law. The most famous of these is the Kolmogorov Zero-One Law, which states that any event in the event space we're interested in will either eventually occur with probability 1 or never occur with probability 1. That is to say, there is no grey area of events that may happen.","Display_name":"owensmartin","Creater_id":27050,"Start_date":"2013-08-20 12:15:44","Question_id":67721}
{"_id":{"$oid":"5837a573a05283111e4d2fb1"},"Last_activity":"2013-08-19 08:34:42","Creator_reputation":152613,"Question_score":12,"Answer_content":"You could explain that even as an event specified a priori, the probability that it occurs is not low. Indeed, it's not so hard to calculate the probability of 3 or more rolls of sixes in a row for at least one die out of 200.[Incidentally, there's a nice approximate calculation you can use - if you have  trials there there's a probability of   of a 'success' (for  not too small), the chance of at least one 'success' is about . More generally, for  trials, the probability is about . In your case you're looking at  trials for a probability of  where  and , so , giving a probability of around 60% that you'll see 3 sixes in a row at least once out of the 200 sets of 3 rolls.I don't know that this specific calculation has a particular name, but the general area of rare events with many trials is related to the Poisson distribution. Indeed the Poisson distribution itself is sometimes called 'the law of rare events', and even occasionally 'the law of small numbers' (with 'law' in these cases meaning 'probability distribution').]--However, if you didn't specify that particular event before the rolling and only say afterward 'Hey, wow, what are the chances of that?', then your probability calculation is wrong, because it ignores all the other events about which you'd say 'Hey, wow, what are the chances of that?'. You've only specified the event after you observe it, for which 1/216 doesn't apply, even with only one die. Imagine I have a wheelbarrow full of small, but distinguishable dice (maybe they have little serial numbers) - say I have ten thousand of them. I tip the wheelbarrow full of dice out:die #    result00001      400002      100003      5 .         . .         . .         .09999      610000      6... and I go \"Hey! Wow, what are the chances I'd get '4' on die #1 and '1' on die #2 and ... and '6' on die #999 and '6' on die #10000?\"That probability is  or about . That's an astonishingly rare event! Something amazing must be going on. Let me try again. I shovel them all back in, and tip the wheelbarrow out again. Again I say \"hey, wow, what are the chances??\" and again it turns out I have an event of such astonishing rarity it should only happen once in the lifetime of a universe or something. What's up?Simply, I am doing nothing but trying to calculate the probability of an event specified after the fact as if it had been specified a priori. If you do that, you get crazy answers.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2013-08-19 00:34:12","Question_id":67721}
{"_id":{"$oid":"5837a573a05283111e4d2fb2"},"Last_activity":"2013-08-19 03:24:47","Creator_reputation":276,"Question_score":16,"Answer_content":"Law of truly large numbers:http://en.wikipedia.org/wiki/Law_of_truly_large_numbers\"with a sample size large enough, any outrageous thing is likely to happen.\"","Display_name":"Emilio M Bumachar","Creater_id":29288,"Start_date":"2013-08-19 03:24:47","Question_id":67721}
{"_id":{"$oid":"5837a573a05283111e4d2fb3"},"Last_activity":"2013-08-19 02:32:59","Creator_reputation":443,"Question_score":3,"Answer_content":"I think that your statement \"If you do enough tests, even unlikely things are bound to happen\", would be better expressed as \"If you do enough tests, even unlikely things are likely to happen\".  \"bound to happen\" is a bit too definite for a probability issue and I think the association of unlikely with likely in this context makes the point you are trying to put over.","Display_name":"Robert Jones","Creater_id":19815,"Start_date":"2013-08-19 02:32:59","Question_id":67721}
{"_id":{"$oid":"5837a573a05283111e4d2fc0"},"Last_activity":"2014-03-08 10:24:47","Creator_reputation":21558,"Question_score":36,"Answer_content":"Frankly, I don't think the law of large numbers has a huge role in industry. It is helpful to understand the asymptotic justifications of the common procedures, such as maximum likelihood estimates and tests (including the omniimportant GLMs and logistic regression, in particular), the bootstrap, but these are distributional issues rather than probability of hitting a bad sample issues.Beyond the topics already mentioned (GLM, inference, bootstrap), the most common statistical model is linear regression, so a thorough understanding of the linear model is a must. You may never run ANOVA in your industry life, but if you don't understand it, you should not be called a statistician.There are different kinds of industries. In pharma, you cannot make a living without randomized trials and logistic regression. In survey statistics, you cannot make a living without Horvitz-Thompson estimator and non-response adjustments. In computer science related statistics, you cannot make a living without statistical learning and data mining. In public policy think tanks (and, increasingly, education statistics), you cannot make a living without causality and treatment effect estimators (which, increasingly, involve randomized trials). In marketing research, you need to have a mix of economics background with psychometric measurement theory (and you can learn neither of them in a typical statistics department offerings). Industrial statistics operates with its own peculiar six sigma paradigms which are but remotely connected to mainstream statistics; a stronger bond can be found in design of experiments material. Wall Street material would be financial econometrics, all the way up to stochastic calculus. These are VERY disparate skills, and the term \"industry\" is even more poorly defined than \"academia\". I don't think anybody can claim to know more than two or three of the above at the same time.The top skills, however, that would be universally required in \"industry\" (whatever that may mean for you) would be time management, project management, and communication with less statistically-savvy clients. So if you want to prepare yourself for industry placement, take classes in business school on these topics.UPDATE: The original post was written in February 2012; these days (March 2014), you probably should call yourself \"a data scientist\" rather than \"a statistician\" to find a hot job in industry... and better learn some Hadoop to follow with that self-proclamation.","Display_name":"StasK","Creater_id":5739,"Start_date":"2012-02-14 12:39:35","Question_id":22804}
{"_id":{"$oid":"5837a573a05283111e4d2fc1"},"Last_activity":"2012-02-15 01:26:53","Creator_reputation":27730,"Question_score":12,"Answer_content":"I think a good understanding of the issues relating to the bias-variance tradeoff.  Most statisticians will end up, at some point, analysing a dataset that is small enough for the variance of an estimator or the parameters of the model to be sufficiently high that bias is a secondary consideration.","Display_name":"Dikran Marsupial","Creater_id":887,"Start_date":"2012-02-15 01:26:53","Question_id":22804}
{"_id":{"$oid":"5837a573a05283111e4d2fc2"},"Last_activity":"2012-02-14 23:20:32","Creator_reputation":3646,"Question_score":7,"Answer_content":"A solid understanding of the substantive problem to be addressed is as important as any particular statistical approach.  A good scientist in the industry is more likely than a statistician without such knowledge to come to a reasonable solution to their problem.  A statistician with substantive knowledge can help.","Display_name":"Brett","Creater_id":485,"Start_date":"2012-02-14 23:20:32","Question_id":22804}
{"_id":{"$oid":"5837a573a05283111e4d2fc3"},"Last_activity":"2012-02-14 12:46:30","Creator_reputation":75690,"Question_score":8,"Answer_content":"I wouldn't say this is very similar to something like the law of large numbers or the central limit theorem, but because making inferences about causality is often central, understanding Judea Pearl's work on using structured graphs to model causality is something people should be familiar with.  It provides a way to understand why experimental and observational studies differ with respect to the causal inferences they afford, and offers ways to deal with observational data.  For a good overview, his book is here.","Display_name":"gung","Creater_id":7290,"Start_date":"2012-02-14 12:46:30","Question_id":22804}
{"_id":{"$oid":"5837a573a05283111e4d2fc4"},"Last_activity":"2012-02-14 12:06:16","Creator_reputation":2313,"Question_score":6,"Answer_content":"In my view, statistical inference is most important for a practitioner. Inference has two parts: 1) Estimation \u0026amp; 2) Hypothesis testing. Hypothesis testing is important one. Since in estimation mostly a unique procedure, maximum likelihood estimation, followed and it is available most statistical package(so there is no confusion). Frequent practitioners questions are around significant testing of difference or causation analysis.  Important hypothesis tests can be find in this link .Knowing about Linear models, GLM or in general statistical modelling is  required for causation interpretation. I assume future of data analysis include Bayesian inference.  ","Display_name":"vinux","Creater_id":7788,"Start_date":"2012-02-14 12:06:16","Question_id":22804}
{"_id":{"$oid":"5837a573a05283111e4d2fc5"},"Last_activity":"2012-02-14 11:51:59","Creator_reputation":17344,"Question_score":6,"Answer_content":"The Delta-Method, how to calculate the variance of bizarre statistics and find their asymptotic relative efficiency, to recommend changes of variable and explain efficiency boosts by \"estimating the right thing\". In conjunction with that, Jensen's Inequality for understanding GLMs and strange kinds of bias which arise in transformations like above. And, now that bias and variance are mentioned, the concept of the bias-variance trade-off and MSE as an objective measure of predictive accuracy.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2012-02-14 11:51:59","Question_id":22804}
{"_id":{"$oid":"5837a573a05283111e4d2fc6"},"Last_activity":"2012-02-14 10:57:12","Creator_reputation":24502,"Question_score":11,"Answer_content":"To point out the super obvious one:Central Limit Theoremsince it allows practitioners to approximate -values in many situations where getting exact -values is intractable. Along those same lines, any successful practitioner would be well served to be familiar, in general, with Bootstrapping","Display_name":"Macro","Creater_id":4856,"Start_date":"2012-02-14 10:57:12","Question_id":22804}
{"_id":{"$oid":"5837a573a05283111e4d2fd3"},"Last_activity":"2012-03-14 11:46:52","Creator_reputation":147175,"Question_score":25,"Answer_content":"  Here is the rub: Apple is so big, it’s running up against the law of  large numbers.    Also known as the golden theorem, with a proof attributed to the  17th-century Swiss mathematician Jacob Bernoulli, the law states that  a variable will revert to a mean over a large sample of results. In  the case of the largest companies, it suggests that high earnings  growth and a rapid rise in share price will slow as those companies  grow ever larger.This muddled jumble actually refers to three different phenomena!The (various) Laws of Large Numbers are fundamental in probability theory for characterizing situations where it is reasonable to expect large samples to give increasingly better information about a process or population being sampled.  Indeed, Jacob Bernoulli was the first to recognize the need to state and prove such a theorem, which appeared in his posthumous Ars Conjectandi in 1713 (edited by nephew Nicholas Bernoulli).There is no apparent valid application of such a law to Apple's growth.Regression toward the mean was first recognized by Francis Galton in the 1880's.  It has often been underappreciated among business analysts, however.  For example, at the beginning of 1933 (during the depths of a Great Depression), Horace Secrist published his magnum opus, the Triumph of Mediocrity in Business.  In it, he copiously examined business time series and found, in every case, evidence of regression toward the mean.  But, failing to recognize this as an ineluctable mathematical phenomenon, he maintained that he had uncovered a basic truth of business development!  This fallacy of mistaking a purely mathematical pattern for the result of some underlying force or tendency (now often called the \"regression fallacy\") is reminiscent of the quoted passage.(It is noteworthy that Secrist was a prominent statistician, author of one of the most popular statistics textbooks published at the time.  On JSTOR, you can find a lacerating review of Triumph... by Harold Hotelling published in JASA in late 1933.  In a subsequent exchange of letters with Secrist, Hotelling wrote   My review ... was chiefly devoted to warning readers not to conclude that business firms have a tendency to become mediocre ...  To \"prove\" such a mathematical result by a costly and prolonged numerical study ... is analogous to proving the multiplication table by arranging elephants in rows and columns, and then doing the same for numerous other kinds of animals.  The performance, though perhaps entertaining, and having a certain pedagogical value, is not an important contribution either to zoology or to mathematics.[JASA Vol. 29, No. 186 (June 1934), pp 198 and 199].)The NY Times passage seems to make the same mistake with Apple's business data.If we read on in the article, however, we soon uncover the author's intended meaning:  If Apple’s share price grew even 20 percent a year for the next decade, which is far below its current blistering pace, its \\3 trillion by 2022.This, of course, is a statement about extrapolation of exponential growth.  As such it contains echoes of Malthusian population predictions.  The hazards of extrapolation are not confined to exponential growth, however.  Mark Twain (Samuel Clements) pilloried wanton extrapolators in Life on the Mississippi (1883, chapter 17):  Now, if I wanted to be one of those ponderous scientific people, and 'let on' to prove ... what will occur in the far future by what has occurred in late years, what an opportunity is here! ... Please observe:--     In the space of one hundred and seventy-six years the Lower Mississippi has shortened itself two hundred and forty-two miles. That is an average of a trifle over one mile and a third per year. Therefore, any calm person, who is not blind or idiotic, can see that in the “Old Oolitic Silurian Period,” just a million years ago next November, the Lower Mississippi River was upwards of one million three hundred thousand miles long, and stuck out over the Gulf of Mexico like a fishing-rod. And by the same token any person can see that seven hundred and forty-two years from now the Lower Mississippi will be only a mile and threequarters long, and Cairo and New Orleans will have joined their streets together, and be plodding comfortably along under a single mayor and a mutual board of aldermen. There is something fascinating about science. One gets such wholesale returns of conjecture out of such a trifling investment of fact.”(Emphasis added.)  Twain's satire compares favorably to the article's quotation of business analyst Robert Cihra:  If you extrapolate far enough out into the future, to sustain that growth Apple would have to sell an iPhone to every man, woman, child, animal and rock on the planet.(Unfortunately, it appears Cihra does not heed his own advice: he rates this stock a \"buy.\"  He might be right, not on the merits, but by virtue of the greater fool theory.)If we take the article to mean \"beware of extrapolating previous growth into the future,\" we will get much out of it.  Investors who think this company is a good buy because its PE ratio is low (which includes several of the notable money managers quoted in the article) are no better than the \"ponderous scientific people\" Twain skewered over a century ago.A better acquaintance with Bernoulli, Hotelling, and Twain would have improved the accuracy and readability of this article, but in the end it seems to have gotten the message right.","Display_name":"whuber","Creater_id":919,"Start_date":"2012-03-14 11:46:52","Question_id":24562}
{"_id":{"$oid":"5837a573a05283111e4d2fd4"},"Last_activity":"2012-03-13 04:28:25","Creator_reputation":15542,"Question_score":11,"Answer_content":"There is no reason to think that stock price draws over time for a particular company represent independent, identically distributed random variables.","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2012-03-13 04:28:25","Question_id":24562}
{"_id":{"$oid":"5837a573a05283111e4d2fd5"},"Last_activity":"2012-03-13 00:19:11","Creator_reputation":14344,"Question_score":32,"Answer_content":"Humorously enough, I just wrote a blog post on this very subject:http://confounding.net/2012/03/12/thats-not-how-the-law-of-large-numbers-works/Essentially, the Law of Large Numbers is that as the number of trials of a random process increases, the mean of those trials will approach the actual mean (or expectation, for more complex distributions). So while if you flip a coin once and get heads your probability of heads = 1.0, as you flip more and more coins, you'll head closer and closer to 0.50.The author argues that Apple will have trouble in the future due to something that is not actually at all related to the Law of Large Numbers. Namely, that as Apple grows larger, the same % increase in share price, earnings, etc. get harder to reach in absolute dollar terms. Basically, to stay on course, Apple has to get larger and larger hits.Linking that to the behavior of a random process converging to a mean requires some serious mental gymnastics. As far as I can tell, the assertion is that \"The awesomeness of your products\" is a random process, and while Apple has had a streak of \"Above Average\" awesome, they'll eventually have to converge toward a mean of \"Middling\". But that's being really charitable to the author.Just because 500 billion is a large number doesn't mean the \"Law of Large Numbers\" is what's acting on it.","Display_name":"Fomite","Creater_id":5836,"Start_date":"2012-03-13 00:19:11","Question_id":24562}
{"_id":{"$oid":"5837a573a05283111e4d2fe2"},"Last_activity":"2016-08-26 05:42:36","Creator_reputation":668,"Question_score":0,"Answer_content":"The coefficient of variation is defined as the standard deviation divided by the mean. If the mean is negative, then so will the CoV.Two popular measures would be (root) mean square error and mean absolute error. It's probably easiest to explain with a worked example.Say you have 5 packages and your values of (target date - ship date) are (i.e. 4 days late, 1 day late, 3 days early, on time, 2 days early)Now the mean happens to be zero, so the CoV would have been infinite!The mean square error is the mean of  which is . Some people prefer to track the square root of that, but that's just cosmetic.The mean absolute error is the mean of  which is 2.Both options are reasonable; the MSE punishes you more if you occasionally get things very wrong, the MAE is more forgiving.","Display_name":"JDL","Creater_id":129051,"Start_date":"2016-08-26 05:42:36","Question_id":231879}
{"_id":{"$oid":"5837a573a05283111e4d2fef"},"Last_activity":"2016-08-26 04:46:21","Creator_reputation":5787,"Question_score":0,"Answer_content":". is the unconditional expected amount paid by the reinsurer.   is the expected amount paid by the reinsurer given that is pays anything, which has probability  of occurring.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-26 04:46:21","Question_id":231873}
{"_id":{"$oid":"5837a573a05283111e4d2ffe"},"Last_activity":"2016-08-26 05:36:04","Creator_reputation":851,"Question_score":1,"Answer_content":"The way in which I would go about approaching this would be with one way Chi-square tests. When applied to data from all four word orders this will tell you whether the there are more of any one word order than there are of any others (more specifically, does the proportion of any word order differ from .25). This is, in effect, an omnibus test.What you're interested in is where the differences between word orders lie. In order to do this I would still use a Chi-square test, but on pairs of word orders, e.g. subject-object vs. object-subject, subject-object vs. subject-indirectobject-object. These will tell you whether there are differences in the frequencies of each pair.Of course, you will want to apply some sort of correction to account for the inflated error rate due to the multiple comparisons, e.g. a Bonferroni correction","Display_name":"Ian_Fin","Creater_id":29532,"Start_date":"2016-08-26 05:36:04","Question_id":231746}
{"_id":{"$oid":"5837a573a05283111e4d300d"},"Last_activity":"2016-02-10 08:50:21","Creator_reputation":25170,"Question_score":2,"Answer_content":"I do not have any solution for you, but if I were you I would look for inspiration on how to think about the problem in Item Response Theory. Among IRT models, one of the simplest ones is Rasch model for binary data. It is used for modeling student responses for test items. Assume that your data consists of  students that answered  test items  marked  for correct answer and  for incorrect. The data can be modeled using the following model P(X_{ij} = 1) = \\frac{\\exp(\\theta_i - \\beta_j)}{1-\\exp(\\theta_i - \\beta_j)} where  is students \"ability\" and  is items \"difficulty\".As far as I understand, your data is different, but some insights from IRT can be generalized to your problem. You have  questions that (possibly) differ in how difficult they are. Among  persons that answered the questions you can possibly find such persons that were confident about both easy and hard questions -- unfortunately you don't know if they are confident because they are right or if they are overconfident. Similar with the persons who are unconfident about everything. Since, as you say, the questions are frames in such way that it is unlikely that anyone knows the exact answer, then you assume that people ranking high on confidence would be rather overconfident than right. If is it so, then you should probably assign greater weights to answers by people with average ratings of confidence.As I said, I don't have the ready answer and if you find this idea convincing, then you should think about ways of adapting it to your needs. Rasch model was made for different data and different purpose but the take-away message is to think of your problem in terms of two factors for questions' difficulty and people trait confidence.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-02-10 08:50:21","Question_id":181440}
{"_id":{"$oid":"5837a574a05283111e4d301a"},"Last_activity":"2011-12-08 11:16:21","Creator_reputation":147175,"Question_score":13,"Answer_content":"Categorical solutionTreating the values as categorical loses the crucial information about relative sizes.  A standard method to overcome this is ordered logistic regression.  In effect, this method \"knows\" that  and, using observed relationships with regressors (such as size) fits (somewhat arbitrary) values to each category that respect the ordering.As an illustration, consider 30 (size, abundance category) pairs generated assize = (1/2, 3/2, 5/2, ..., 59/2)e ~ normal(0, 1/6)abundance = 1 + int(10^(4*size + e))with abundance categorized into intervals [0,10], [11,25], ..., [10001,25000].Ordered logistic regression produces a probability distribution for each category; the distribution depends on size.  From such detailed information you can produce estimated values and intervals around them.  Here is a plot of the 10 PDFs estimated from these data (an estimate for category 10 was not possible due to lack of data there):Continuous solutionWhy not select a numeric value to represent each category and view the uncertainty about the true abundance within the category as part of the error term?We can analyze this as a discrete approximation to an idealized re-expression  which converts abundance values  into other values  for which the observational errors are, to a good approximation, symmetrically distributed and of roughly the same expected size regardless of  (a variance-stabilizing transformation).To simplify the analysis, suppose the categories have been chosen (based on theory or experience) to achieve such a transformation.  We may assume then that  re-expresses the category cutpoints  as their indexes .  The proposal amounts to selecting some \"characteristic\" value  within each category  and using  as the numerical value of abundance whenever the abundance is observed to lie between  and .  This would be a proxy for the correctly re-expressed value .Suppose, then, that abundance is observed with error , so that the hypothetical datum is actually  instead of .  The error made in coding this as  is, by definition, the difference , which we can express as a difference of two terms\\text{error} = f(a + \\varepsilon) - f(a) - \\left(f(a + \\varepsilon) - f(\\beta_i)\\right).That first term, , is controlled by  (we can't do anything about ) and would appear if we did not categorize aboundances.  The second term is random--it depends on --and evidently is correlated with .  But we can say something about it: it must lie between  and .  Moreover, if  is doing a good job, the second term might be approximately uniformly distributed.  Both considerations suggest choosing  so that  lies halfway between  and ; that is, .These categories in this question form an approximately geometric progression, indicating that  is a slightly distorted version of a logarithm.  Therefore, we should consider using the geometric means of the interval endpoints to represent the abundance data.Ordinary least squares regression (OLS) with this procedure gives a slope of 7.70 (standard error is 1.00) and intercept of 0.70 (standard error is 0.58), instead of a slope of 8.19 (se of 0.97) and intercept of 0.69 (se of 0.56) when regressing log abundances against size.  Both exhibit regression to the mean, because theoretical slope should be close to .  The categorical method exhibits a bit more regression to the mean (a smaller slope) due to the added discretization error, as expected.This plot shows the uncategorized abundances along with a fit based on the categorized abundances (using geometric means of the category endpoints as recommended) and a fit based on the abundances themselves.  The fits are remarkably close, indicating this method of replacing categories by suitably chosen numerical values works well in the example.Some care usually is needed in choosing an appropriate \"midpoint\"  for the two extreme categories, because often  is not bounded there. (For this example I crudely took the left endpoint of the first category to be  rather than  and the right endpoint of the last category to be .)  One solution is to solve the problem first using data not in either of the extreme categories, then use the fit to estimate appropriate values for those extreme categories, then go back and fit all the data.  The p-values will be slightly too good, but overall the fit should be more accurate and less biased.","Display_name":"whuber","Creater_id":919,"Start_date":"2011-12-08 11:16:21","Question_id":19534}
{"_id":{"$oid":"5837a574a05283111e4d301b"},"Last_activity":"2011-12-08 05:25:20","Creator_reputation":19525,"Question_score":2,"Answer_content":"Consider using the logarithm of the size.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2011-12-08 05:25:20","Question_id":19534}
{"_id":{"$oid":"5837a574a05283111e4d3028"},"Last_activity":"2016-06-17 12:49:01","Creator_reputation":658,"Question_score":1,"Answer_content":"Addressing unobserved heterogeneity in panel models with fixed effects for fractional response variables (or nonlinear models in general) is not trivial due to the incidental parameter problem (for  and  fixed), see for example Lancaster (2000) or this answer here at CrossValidated. If  is small (and fixed), fixed effects are inconsistent (and random effects rely probably strongly on the distributional assumptions). So you cannot just compare a random effects and a fixed effects model via Hausman test. Proposals for panel models for fractional response variables can be found in Papke and Wooldridge (2008) or here. ","Display_name":"Arne Jonas Warnke","Creater_id":9866,"Start_date":"2016-06-17 12:49:01","Question_id":58448}
{"_id":{"$oid":"5837a574a05283111e4d3035"},"Last_activity":"2016-08-26 04:19:40","Creator_reputation":1259,"Question_score":0,"Answer_content":"Apparently this question didn't receive a lot of attention. Also, it seems that there's no package for R which computes simultaneous confidence bands for linear regression (!). Since there are R packages for basically everything, this is a bit surprising for me, but it looks like that's how it is. Thus, I decided to write my own function, based on Working-Hotelling bands (basically, Scheffé method applied to a regression surface). It looks like it's working fine, thus I'm posting it as an answer.simultaneous_CBs \u0026lt;- function(linear_model, newdata, level = 0.95){    # Working-Hotelling 1 – α confidence bands for the model linear_model    # at points newdata with α = 1 - level    # estimate of residual standard error    lm_summary \u0026lt;- summary(linear_model)    # degrees of freedom     p \u0026lt;- lm_summarydf[2]    # F-distribution    Fvalue \u0026lt;- qf(level,p,nmp)    # multiplier    W \u0026lt;- sqrt(p*Fvalue)    # confidence intervals for the mean response at the new points    CI \u0026lt;- predict(linear_model, newdata, se.fit = TRUE, interval = \"confidence\",                   level = level)    # mean value at new points    Y_h \u0026lt;- CIse.fit    UB \u0026lt;- Y_h + W*CI$se.fit    sim_CB \u0026lt;- data.frame(LowerBound = LB, Mean = Y_h, UpperBound = UB)}","Display_name":"DeltaIV","Creater_id":58675,"Start_date":"2016-08-26 04:19:40","Question_id":231632}
{"_id":{"$oid":"5837a574a05283111e4d3042"},"Last_activity":"2016-08-26 01:33:00","Creator_reputation":643,"Question_score":0,"Answer_content":"I think the problem is that you are interpreting the correlation argument wrong. This argument allows you to specify the within-subjects correlation, not the correlation of the random effects. The correlation argument is useful when there is residual serial correlation that the random intercept model did not sufficiently not account for.The semi-variogram of longitudinal data helps with this interpretation, which is described in Analysis of Longitudinal Data and this highly detailed paper. With a sufficiently complex within-subject correlation structure, we can fit a random intercept to account for between-person variance, and then also include a model of the serial correlation (e.g. a Gaussian correlation structure to account for the decay in the within-subject correlation at greater lags). In my experience, one can either model the structure of the semi-variogram using both the random andcorrelation` arguments, or one can use a random intercept and slope model. I think of it as two different ways to model the same phenomenon. The first case models the correlation structure directly, whereas the latter conditions out the correlation structure by fitting subject-specific curves.So in the end, it makes sense to me that the model estimates a near-0 serial correlation for the corCompSymm structure because after including the random intercept and slopes, there is no serial correlation remaining to account for. ","Display_name":"Moose","Creater_id":32377,"Start_date":"2016-08-26 01:33:00","Question_id":231795}
{"_id":{"$oid":"5837a574a05283111e4d3043"},"Last_activity":"2016-08-25 18:52:14","Creator_reputation":12298,"Question_score":0,"Answer_content":"The answer to your second question (fit correlation structure without random effects) is to use nlme::gls() (\"generalized least squares\") - it allows the same set of heteroscedasticity (weights argument) and correlation (correlation argument) as lme.","Display_name":"Ben Bolker","Creater_id":2126,"Start_date":"2016-08-25 18:52:14","Question_id":231795}
{"_id":{"$oid":"5837a574a05283111e4d3050"},"Last_activity":"2016-08-26 03:54:57","Creator_reputation":81,"Question_score":1,"Answer_content":"Here is an idea: treat your 2 clusters as 2 classes. Create a binary model with 1 cluster as reference class. The most significant variables are the variables influencing the partition. You can look at statistics like p-value and chi-square to determine, which one is relatively more significant.","Display_name":"muni","Creater_id":124691,"Start_date":"2016-08-26 03:54:57","Question_id":231875}
{"_id":{"$oid":"5837a574a05283111e4d305d"},"Last_activity":"2016-08-26 03:52:07","Creator_reputation":5445,"Question_score":2,"Answer_content":"It is hardly surprising that it fails to converge. As you have shown, the response variable is highly zero-inflated. Even if it did converge, I would treat the results with great suspicion.You need to use a package which allows for a zero-inflated negative binomial or zero-inflated overdispersed Poisson response, such as glmmADMB, glmmTMB and MCMCglmm.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-25 08:06:21","Question_id":231680}
{"_id":{"$oid":"5837a574a05283111e4d306a"},"Last_activity":"2013-01-25 11:42:14","Creator_reputation":318,"Question_score":4,"Answer_content":"Before using standard bootstrap methods, the data should be tested for bias and skewness beforehand and if they exist, \"tricks\" should be used to transform the data to correct for this. BCa (bias-corrected and accelerated) bootstrap by Elfron (1987) takes care of these tricks for you.So, if you have a distribution that has skew and/or bias, you would be able to calculate the true confidence interval. If you randomly drew samples from this distribution and then estimated the confidence interval using the percentile method and the BCa method, the BCa method will tend to be closer to the true confidence interval than the percentile method. The percentile method may worse by underestimating or overestimating the confidence interval. What really matters (although maybe not in the experimenter's mind) is how close the estimate is to the true confidence interval.As mentioned by @phaneron, you should not simply pick the method that gives you the smaller confidence interval. Doing this would systematically underestimate the confidence interval and make the results invalid.","Display_name":"Jonathan","Creater_id":7521,"Start_date":"2013-01-25 11:42:14","Question_id":43635}
{"_id":{"$oid":"5837a574a05283111e4d3077"},"Last_activity":"2016-08-26 03:38:05","Creator_reputation":12897,"Question_score":1,"Answer_content":"BackgroundYour forecast horizon is not  but rather a whole vector . You only seem to have one observation of forecast errors from the different forecasts for each horizon , for . You cannot compare two forecasts  and  with any confidence because you only have one observation for  and . (Testing equality of means from a sample of size 1 won't work well.)AnswersYou cannot directly use the standard Diebold-Mariano test in this setting.If you went for the standard Diebold-Mariano setting and collected multiple forecast errors for one horizon (e.g. using rolling windows within the original sample), you could use the standard Diebold-Mariano test.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-26 02:31:35","Question_id":231860}
{"_id":{"$oid":"5837a574a05283111e4d3084"},"Last_activity":"2016-08-26 01:17:10","Creator_reputation":3559,"Question_score":0,"Answer_content":"The problem here is that you have many more predictors than you think you have. Each predictor factor with  levels counts as  variables in the model. So region alone counts as 23 and most of the others will be multiple too. When you have so many predictor variables it is unlikely that any level will add much predictive power over and above all the rest. Even with 1200 people you are trying to fit a model for which you do not have sufficient data.The issue you had in your previous post here was one of what is called separation. There you had more levels for your outcome variable and some of them were quite infrequent (I suppose). That meant that some combination of your predictor variables was capable of predicting with certainty which way the person voted. You have now got over that problem by using fewer categories.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-26 01:17:10","Question_id":231777}
{"_id":{"$oid":"5837a574a05283111e4d3091"},"Last_activity":"2016-08-26 03:20:47","Creator_reputation":3023,"Question_score":10,"Answer_content":"Statistical hypothesis testing is in some way similar to  'proof by contradiction' in mathematics, i.e. if you want to prove something then assume the opposite and derive a contradiction, i.e. something that is impossible.  In statistics 'impossible' does not exist, but some events are very 'improbable'.  So in statistics, if you want to 'prove' something (i.e. H1) then you assume the opposite (i.e. H0) and if H0 is true you try to derive something improbable.  'Improbable' is defined by the confidence level that you choose.  If, assuming H0 is true, you can find something very improbable, then H0 can not be true because it leads to a 'statistical contradiction'. Therefore H1 must be true. This implies that in statistical hypothesis testing you can only find evidence for H1.  If one can not reject H0 then the only conclusion you can draw is 'We can not prove H1' or 'we do not find evidence that H0 is false and so we accept H0 (as long as we do not find evidence againgst it)'. But there is more ... it also about power.Obviously, as nothing is impossible, one can draw wrong conclusions;  we might find 'false evidence' for H1 meaning that we conclude that H0 is false while in reality it is true.  This is a type I error and the probability of making a type I error is equal to the signficance level that you have choosen. One may also accept H0 while in reality it is false, this is a type II error and the probability of making one is denoted by . The power of the test is defined as  so 1 minus the probability of making a type II error. This is the same as the probability of not making a type II error. So  is the probability of accepting H0 when H0 is false, therefore  is the probability of rejecting H0 when H0 is false which is the same as the probability of rejecting H0 when H1 is true.  By the above, rejecting H0 is finding evidence for H1, so the power is  is the probability of finding evidence for H1 when H1 is true. If you have a test with very high power (close to 1), then this means that if H1 is true, the test would have found evidence for H1 (almost surely) so if we do not find evidence for H1 (i.e. we do not reject H0) and the test has a very high power, then probably H1 is not true (and thus probably H0 is true).  So what we can say is that if your test has very high power , then not rejecting H0 is ''almost as good as'' finding evidence for H0. ","Display_name":"fcop","Creater_id":83346,"Start_date":"2015-07-30 23:41:03","Question_id":163957}
{"_id":{"$oid":"5837a574a05283111e4d3092"},"Last_activity":"2015-07-30 08:48:03","Creator_reputation":19083,"Question_score":4,"Answer_content":"It depends.For instance, I'm testing my series for the unit-root, maybe with ADF test. Null in this case means the presence of unit root. Failing to reject null suggests that there might be a unit root in the series. The consequence is that I might have to go with modeling the series with random walk like process instead of autorgressive.So, although it doesn't mean that I proved unit root's presence, the test outcome is not inconsequential. It steers me towards different kind of modeling than rejecting the null.Hence, in practice failing to reject often means implicitly accepting it. If you're purist then you'd also have the alternative hypothesis of autoregressive, and accept it when failing to reject null.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2015-07-30 08:48:03","Question_id":163957}
{"_id":{"$oid":"5837a574a05283111e4d3093"},"Last_activity":"2015-07-30 08:31:06","Creator_reputation":90,"Question_score":0,"Answer_content":"If we fail to reject the null hypothesis, it does not mean that the null hypothesis is true.  That's because a hypothesis test does not determine which hypothesis is true, or even which one is very much more likely.  What it does assess is whether the evidence available is statistically significant enough to to reject the null hypothesis.So1. the data doesn't provide statistically significant evidence in the difference of the means, but it doesn't conclude that it actually is the mean we define in H0.2. We don't have strength of evidence against the mean being different, but the same as part 1, we can't make finite conclusions on the mean.Check out this link for more info on P values and Significance testsClick here","Display_name":"Jake","Creater_id":77637,"Start_date":"2015-07-30 08:31:06","Question_id":163957}
{"_id":{"$oid":"5837a574a05283111e4d30a0"},"Last_activity":"2016-08-26 03:20:44","Creator_reputation":26139,"Question_score":2,"Answer_content":"This is a good question, but as it appears from it that you know PCA and CCA a deal, so you are able to answer it yourself. And you do:  [CCA] builds the canonical variates not to blindly [wrt the existence  of X] maximize explained variance [in Y], but already with the final  purpose of maximizing correlation with X in mind.Absolutely true. The correlation of the 1st Y's PC with X set will almost always be weaker than the correlation of the 1st Y's CV with it. This comes apparent from pictures comparing PCA with CCA actions.The PCA+regression you conceive of is two-step, initially \"unsupervised\" (\"blind\", as you said) strategy, while CCA is one-step, \"supervised\" strategy. Both are valid - each in own investigatory settings!1st principal component (PC1) obtained in PCA of set Y is a linear combination of Y variables. 1st canonical variate (CV1) extracted from set Y in CCA of sets Y and X is a linear combination of Y variables, too. But they are different. (Explore the linked pics, also pay attention to the phrase that CCA is closer to - actually a form of - regression than to PCA.)PC1 represents set Y. It is the linear summary and the \"deputy\" from set Y, to face the outer-world relationships later (such as in a subsequent regression of PC1 by variables X).CV1 represents set X within set Y. It is the linear image of X belonging to Y, the \"insider\" in Y. The Y-X relationship is already there: CCA is a multivariate regression.Suppose I've got a children sample's results on a school anxiety questionnaire (such as Phillips test) - Y items, and their results on a social adaptation questionnaire - X items. I want to establish the relationship between the two sets. Items of both inside X and inside Y correlate, but they are quite different and I'm not pleased with the idea to bluntly sum up item scores into a single score in either set, so I'm choosing to stay multivariate.If I do PCA of Y, extracting PC1, and then regress in on X items, what does it mean? It means that I respect the anxiety questionnaire (Y items) as the sovereign (closed) domain of phenomena, which can express oneself. Express by issuing its best weighted sum of items (accounting for the maximal variance) which represents the whole set Y - its general factor/pivot/trend, \"mainstream school anxiety complex\", the PC1. It is not before that representation is formed that I turn to the next question how it might be related to social adaptation, the question I'll check in the regression.If I do CCA of Y vs X, extracting the 1st pair of canonical variates - one from each set - having maximal correlation, what does it mean? It means that I suspect the common factor between (behind) both anxiety and adaptation which makes them correlate with each other. However, I have no reason or ground to extract or model that factor by means of PCA or Factor analysis of the combined set \"X variables + Y variables\" (because, for example, I see anxiety and adaptation as two quite different domains conceptually, or because the two questionnaires have very different scales (units) or differently-shaped distributions that I fear to \"merge\", or the number of items is very different in them). I'll be content with just the canonical correlation between the sets. Or I might not be supposing any \"common factor\" behind the sets, and simply think \"X effects Y\". Since Y is multivariate the effect is multidimensional, and I'm asking for the 1st-order, strongest effect. It is given by the 1st canonical correlation and the prediction variable corresponding to it is the CV1 of set Y. CV1 is fished out of Y, Y is not selbständig producer of it.","Display_name":"ttnphns","Creater_id":3277,"Start_date":"2016-08-25 19:08:03","Question_id":231653}
{"_id":{"$oid":"5837a574a05283111e4d30af"},"Last_activity":"2016-08-26 00:23:23","Creator_reputation":54,"Question_score":0,"Answer_content":"The main consideration is to try and keep comparisons as uniform as possible, i.e. the only thing that should differ is the change in behaviour. That would mean the following:Decide beforehand when measurements will be taken and stick to the schedule rigorously. For example, don’t take some morning measurements before breakfast and some after. Try and stick to exact times – and circumstances – as far as possible.Your 1st month should be the control. This is the baseline to which you would compare data from the other months. If you exercise in the first month but not the second, your measurements in the second month would still be a reflection of the fitness gains from the first month. Instead, the first month should be the “do nothing” month.Take a representative number of measurements of each time of day for each “experiment”. I would suggest taking at least 20 for each month/experiment. So that would be 20 morning measurements, 20 at noon and 20 in the evening (or whenever you decide to take the measurements). This is linked to the following point, namely…Only compare “like with like” measurements to determine the experimental effects (e.g. exercise, spicy food, etc.). So morning measurements in month 1 should be compared with morning measurements from month 2, noon measurements from month 1 with noon measurements from month 2, etc. Otherwise you have confounding variables such as time of day. For example, morning measurements from month 1 to noon measurements from month 2 are not comparable if the noon measurements are taken after a stressful 1.5 hour commute and 3 hours of fighting with co-workers. If you want to compare the effect of the time of day (e.g. the effect of the commute and co-workers), then this should be done within months. So compare morning and noon for month 1, morning and noon for month 2, etc. Alternatively, you could group ALL morning measurements from all months and compare them to ALL noon measurements from all months.","Display_name":"Dirk Snyman","Creater_id":82996,"Start_date":"2016-08-26 00:23:23","Question_id":231836}
{"_id":{"$oid":"5837a574a05283111e4d30c4"},"Last_activity":"2016-08-26 02:12:52","Creator_reputation":668,"Question_score":0,"Answer_content":"This is a difficult problem, but I know of two ways you might proceed:1) Attempt a Bayesian approach. That is, specify a prior for  and  and any other parameters, and determine the posterior distribution of those given the data (you will probably have to use a Monte Carlo technique to do that.)2) Penalised regression. Attach a penalty to the size of  and  to stop them diverging in the fitting process. You might be able to modify LASSO to suit your needs.","Display_name":"JDL","Creater_id":129051,"Start_date":"2016-08-26 02:12:52","Question_id":231775}
{"_id":{"$oid":"5837a574a05283111e4d30d3"},"Last_activity":"2016-08-26 01:27:10","Creator_reputation":388,"Question_score":0,"Answer_content":"Your code can be simplified at many points. I'll go them trough step by step.m1   \u0026lt;- 2.50diff \u0026lt;- 0.1m2   \u0026lt;- seq(m1 - diff, m1 + diff, by=diff/15)If you plug in m1 and diff you get m2 \u0026lt;- seq(2.4, 2.6, by = 0.1/15). I generaly prefer using length instead of by as long as you don't net this exact stepwidth (and your question doesn't gives the Impression that this is the case). So I would write m2 \u0026lt;- seq(2.4, 2.6, length = 31).nsamples \u0026lt;- 250 # is fine.d \u0026lt;- data.frame(\"m1\"=rep(m1, length(m2)), \"m2\"=m2, \"pv\"=rep(0, length(m1))) # if you are only interesered in the plot shown, you don't need a data.frame.About the loop:for(i in 1:nrow(d)) {  dpv[i] + t.test(   rnorm(n=nsamples, mean=dm2[i], sd=0.1)       )pv[i] \u0026lt;- dpv[i] with 0, so you calculate dpv[i] \u0026lt;- t.test(...). Instead of building the difference from two independent normal variable draws, you can draw from the distribution of the differences: rnorm(n = nsamples, mean = dm2[i], sd = sqrt(0.1^2 + 0.1^2)). If we recapitulate what dm2 is we see that dm2[i] is 2.5 - seq(2.4, 2.6, length = 31) which is nothing else than seq(-0.1, 0.1, length = 31).So what I would do is the following:nsamples \u0026lt;- 250diff \u0026lt;- 0.1new \u0026lt;- seq(-diff , diff, length = 31)p \u0026lt;- array(dim = length(new))for(i in 1:length(new)) {  p[i] \u0026lt;- t.test(rnorm(n=nsamples, mean = new[i], sd = sqrt(2)/10))p.value)","Display_name":"Qaswed","Creater_id":112892,"Start_date":"2016-08-23 03:17:25","Question_id":230155}
{"_id":{"$oid":"5837a574a05283111e4d30e0"},"Last_activity":"2016-08-26 01:27:10","Creator_reputation":764,"Question_score":0,"Answer_content":"With approx n=50,000 in each sample, there's no point in a statistical test since it will most likely be statistically significant regardless of the meaning of the difference. Instead, I'd plot the two distributions (histograms and boxplots) and carefully consider what is the practical meaning of a difference of 0.3 between the two genders. Most importantly, the question is: what will this result be used for? Is it really the averages that you need to compare?","Display_name":"Galit Shmueli","Creater_id":1945,"Start_date":"2016-08-26 01:27:10","Question_id":231820}
{"_id":{"$oid":"5837a574a05283111e4d30e1"},"Last_activity":"2016-08-25 21:59:51","Creator_reputation":1352,"Question_score":1,"Answer_content":"This looks like a Wilcox rank sum test (Mann–Whitney U test) to me.It's a non-parametric testThe null hypothesis is that the two samples come from the same populationAssuming your observations in male and female are independent Your data is ordinal, for example 3.9 is better than 3.6Please note with 50,000 sample size, you test will most likely be significant. You may want to take a smaller sample for the test.","Display_name":"Student T","Creater_id":34623,"Start_date":"2016-08-25 20:01:32","Question_id":231820}
{"_id":{"$oid":"5837a574a05283111e4d30e2"},"Last_activity":"2016-08-25 21:25:36","Creator_reputation":536,"Question_score":0,"Answer_content":"I would suggest using the Kolmogorov-Smirnov Test to determine if these two datasets differ significantly. It is non-parametric, so this suits your dataset.An advantage of the K-S test is that you can plot the cumulative fraction of the two groups and view these graphically.","Display_name":"Sandeep S. Sandhu","Creater_id":55831,"Start_date":"2016-08-25 21:25:36","Question_id":231820}
{"_id":{"$oid":"5837a574a05283111e4d30f1"},"Last_activity":"2016-08-26 01:25:33","Creator_reputation":25170,"Question_score":0,"Answer_content":"\"Not applicable\" is not a missing data but a valid information! Imagine that in survey you ask about ages of respondent's children:  Is your son (1) much younger, (2) younger, (3) same age, (4) older, (5) much older then your daughter,  or either is the question (6) not applicable because you do not have  either son, daughter, or both?If you coded \"not applicable\" as , you'd conclude that people who do not have sons, or do not have daughters have much much younger sons than daughters. If you code it as , then you'd conclude that they have much older sons. If you code it as some middle value, you'd conclude that childless person's children are in the same age...If you need to calculate average, then exclude those participants who marked \"non applicable\" from this calculation (you can report count or percentage of such cases) and treat \"not applicable\" as another dummy variable ignoring the fact that it was a part of the same question.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-26 01:25:33","Question_id":231817}
{"_id":{"$oid":"5837a574a05283111e4d30f2"},"Last_activity":"2016-08-25 21:11:01","Creator_reputation":1352,"Question_score":2,"Answer_content":"While @Kodiologist is correct, your data doesn't look like missing data to me. Not Applicable means the respondent is nether agreeing nor disagreeing the question. It's NOT a missing value because it gives you something about the respondent. In this case, you should divide by 5.EDIT: It doesn't make sense to use 0 for Not Applicable because it's not a missing value. What about 3?","Display_name":"Student T","Creater_id":34623,"Start_date":"2016-08-25 21:06:53","Question_id":231817}
{"_id":{"$oid":"5837a574a05283111e4d30f3"},"Last_activity":"2016-08-25 20:58:00","Creator_reputation":8337,"Question_score":2,"Answer_content":"Dealing with missing data is a major topic unto itself, with many different available techniques and theoretical approaches. The two options you've mentioned are by no means the only options. However, of the two, the second is definitely better. It's equivalent to simply removing the missing observation before calculating the mean. The first option treats a missing observation as if the subject very strongly disagreed, which doesn't make much sense, in general.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-25 20:58:00","Question_id":231817}
{"_id":{"$oid":"5837a574a05283111e4d3102"},"Last_activity":"2016-08-25 20:04:27","Creator_reputation":1352,"Question_score":0,"Answer_content":"What's your model? Note that predict is not a ROCR function, ROCR starts from prediction. It's your job to specify the model correctly. Of course, what the model generates have a significant impact on you AUC statistics, because you're comparing your prediction against testDatavar? It's supposed to be the classified labels. Are they related to your binary or probability prediction (I don't know what model you're using)?Note that you don't choose your AUC statistics, you choose your model that maximize AUC statistics as much as possible.","Display_name":"Student T","Creater_id":34623,"Start_date":"2016-08-25 19:53:24","Question_id":231818}
{"_id":{"$oid":"5837a574a05283111e4d3111"},"Last_activity":"2016-08-26 00:41:26","Creator_reputation":1969,"Question_score":0,"Answer_content":"(1) almost all classification algorithms assume an equal cost of misclassification. Will thus be dependent of the ratio of 1/0 in the dependent variable. Will thus often not be useful (without adjustment) when the ratio of outcomes is large.(2) this is known known issue. use cost sensitive models (C50) or oversample minority case /oversample minority case to impose a different (your preferred) cost ratio onto the model, rather than using default 1:1 ratioUpdate:(1) the Kunh book: Applied Predictive Modeling - has a chapter on this(2) for random forests, the Andy Liaw paper is often referenced: Using Random Forest to Learn Imbalanced Data","Display_name":"charles","Creater_id":34658,"Start_date":"2016-08-25 03:07:29","Question_id":230687}
{"_id":{"$oid":"5837a574a05283111e4d311e"},"Last_activity":"2016-08-26 00:03:46","Creator_reputation":54,"Question_score":-1,"Answer_content":"Spearman’s rho and Kendall’s tau can both be used for non-parametric data, as they are both measures of rank correlation. One key difference: Spearman’s rho can be used for both continuous AND discrete variables, while Kendall’s tau should only be used for discrete variables. Spearman’s rho takes into account how far the ranks are from each other, while Kendall’s tau only considers whether they are equal or not.In your case, your data are discrete and either is thus appropriate. However, if you were to measure the heights as continuous variables (e.g. with decimal points) then Spearman’s rho would be the preferred measure.I would not be concerned about the zeroes. As you say, they reflect a real consideration for your study.Interpretation of Spearman’s rho and Kendall’s tau are similar. They can range from -1 to +1, with -1 reflecting a perfect negative correlation (as one variable increases, the other decreases), 0 reflecting no relationship between the variables and +1 reflecting a perfect positive correlation (as one variable increases, the other will also increase). In both instances, your values are close to 1 and it is thus safe to say that the data are strongly positively correlated, i.e. as the available height under branches increases, the height of the grass can be expected to increase. This is easily interpreted from the p values. In both cases, p \u0026lt; 0.05 and the relationship is thus significant. Also in both cases, the rho/tau values are positive, hence a significant, positive correlation is evident. This is usually the case, i.e. rho and tau generally lead to similar inferences.In this case, I don’t think it would be necessary to use tau-b for the calculation. You are unlikely to find manifest differences that would influence your interpretation.There is no need for a post-hoc test to determine the nature of the correlation. As stated above, the rho or tau value provides you with this information. Closer to -1 indicates strongly negative correlation, closer to 0 indicates weak/no correlation, closer to +1 indicates strongly positive correlation.","Display_name":"Dirk Snyman","Creater_id":82996,"Start_date":"2016-08-26 00:03:46","Question_id":231829}
{"_id":{"$oid":"5837a574a05283111e4d311f"},"Last_activity":"2016-08-25 23:53:56","Creator_reputation":1352,"Question_score":0,"Answer_content":"Pearson's correlation doesn't assume normality, so you should use it. You really don't need Kendall's tau in your example.In your analysis, you should start off with a simple plot. Like this:grass  \u0026lt;- c(0,0,0,8,0,0,0,2,6,0,0,1,0,0,0,8,0,7,15)height \u0026lt;- c(0,0,0,16,0,0,0,2,6,0,0,1,0,0,0,15,0,7,15)plot(grass, height)This is clearly linear and monotonic positive, and so it was pointless for you to do a normality test.Both Pearson and Spearman give similar results:cor(grass, height) # 0.9300721cor(grass, height, method='spearman') # 0.9947245In this example, it's not really not that important to do a correlation test because you know the p-value will be very small and your result will be significant. But let's do it anyway:cor(grass, height) # 8.227e-09cor.test(grass, height, method='spearman', exact=FALSE) # 2.2e-16In the first line, we did a test for Pearson's correlation. Your very small p-value give you confidence that your correlation is not zero (not very surprising).In the second line, we have a test for Spearman's correlation. Note that your zeros made the default exact test impossible, so you'll need to set exact to FALSE to approximate your test statistic with t approximation. Again, this is very significant. ","Display_name":"Student T","Creater_id":34623,"Start_date":"2016-08-25 23:53:56","Question_id":231829}
{"_id":{"$oid":"5837a574a05283111e4d312c"},"Last_activity":"2016-08-25 23:55:13","Creator_reputation":1,"Question_score":0,"Answer_content":"Hint: Page 4 of the manual of the package indicates that mu represents the mean and sigma represents the standard deviation (square root of the variance):https://cran.r-project.org/web/packages/mixdist/mixdist.pdfSince the Gamma distribution has closed form mean and standard deviation, you can easily transformed into scale and shape:https://en.wikipedia.org/wiki/Gamma_distribution","Display_name":"shar","Creater_id":129038,"Start_date":"2016-08-25 23:55:13","Question_id":231814}
{"_id":{"$oid":"5837a574a05283111e4d3139"},"Last_activity":"2016-08-24 03:47:28","Creator_reputation":410,"Question_score":1,"Answer_content":"I think I found my answer.What I described is a form of soft max normalization as described in this Wikipedia article.Softmax_NormalizationThe inclusion of 1.69897 softens effect of the tails to a greater extent.","Display_name":"Chris","Creater_id":70282,"Start_date":"2016-08-24 03:47:28","Question_id":231390}
{"_id":{"$oid":"5837a574a05283111e4d3146"},"Last_activity":"2016-08-25 20:42:44","Creator_reputation":1,"Question_score":0,"Answer_content":"I would like to put my 2 cents here. Accuracy (A) is a more generic measure for any classifier. But, Precision (P) or Recall (R) are more domain-specific. E.g. for the spam-No spam classifier, you need to choose b/w False Negative and False Positive i.e. whether a non-spam spammed is critical for ur business or a spam not spammed. If u choose the former go for maximizing the P and in 2nd case R would be more important. Why F-score, is a bit different question. Actually, you can over-inflate your P or R depending on the business need, as discussed, so, a more conservative measure, Harmonic Mean (it is closest to the lower value among AM, GM and HM) is more suitable in order to test a given model. ","Display_name":"Binit Bhagat","Creater_id":129029,"Start_date":"2016-08-25 20:42:44","Question_id":231786}
{"_id":{"$oid":"5837a574a05283111e4d3153"},"Last_activity":"2016-08-25 19:21:47","Creator_reputation":152613,"Question_score":1,"Answer_content":"If all you know is the mean and standard deviation, you can at best try to bound them but the bounds are quite wide.The max or min can be anywhere between  and  standard deviations from the mean.However, for even , the range (max-min) cannot be less than  (the version of standard deviation that has an  denominator rather than ), which is , and can be as much as  (for odd  the bounds are slightly different but I won't labor the point).So for example if the sample size,  was 25, and the mean was 100 and the sd was 15, then the maximum might be anywhere between  and .If  was instead 100, then those bounds move to 101.5 and 235.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-25 19:09:26","Question_id":231793}
{"_id":{"$oid":"5837a574a05283111e4d3154"},"Last_activity":"2016-08-25 15:34:43","Creator_reputation":547,"Question_score":1,"Answer_content":"No. Two different samples with different min and max values can have the same mean and standard deviation.","Display_name":"John Yetter","Creater_id":31135,"Start_date":"2016-08-25 15:34:43","Question_id":231793}
{"_id":{"$oid":"5837a574a05283111e4d3163"},"Last_activity":"2016-08-25 18:09:04","Creator_reputation":341,"Question_score":1,"Answer_content":"We can plot an outlier box plot with fences to graphically display our data. Then we can define quantities for identifying extreme values in the tails of the distribution. For instance, if Q1 is the first quartile, Q3 is the third quartile and the interquartile range is IQ = Q3-Q1 thenlower inner fence: Q1 - 1.5*IQupper inner fence: Q3 + 1.5*IQlower outer fence: Q1 - 3*IQupper outer fence: Q3 + 3*IQFinally, we can apply an outlier detection criteria. A point beyond an inner fence on either side is considered a mild outlier. A point beyond an outer fence is considered an extreme outlier.   See example of an outlier box plot. ","Display_name":"Nick","Creater_id":111637,"Start_date":"2016-08-25 18:09:04","Question_id":231801}
{"_id":{"$oid":"5837a574a05283111e4d3170"},"Last_activity":"2016-08-25 18:06:13","Creator_reputation":16,"Question_score":1,"Answer_content":"If using EM to estimate the GMM parameters, BIC works quite well in choosing the number of clusters (in my experience). However EM is susceptible to 'singularities' in the likelihood function, that is, when a component's variance becomes very small.A better approach is to use a Bayesian treatment based on variational methods - Variational GMM. See Christopher Bishop's Pattern Recognition and Machine Learning book (chapter 10). This resolves both issues - avoiding singularities and choosing the number of clusters / components. In my experience, it works quite well.","Display_name":"devendraj","Creater_id":129016,"Start_date":"2016-08-25 18:06:13","Question_id":228330}
{"_id":{"$oid":"5837a574a05283111e4d3171"},"Last_activity":"2016-08-04 15:04:09","Creator_reputation":21,"Question_score":2,"Answer_content":"There are a number of methods to select the number of components in a mixture model. There is a recent arxiv paper that presents a new Bayesian methodology that seems to be efficient compared to others as well as a literature review on the subject:On choosing mixture components via non-local priors","Display_name":"Ronnie Coleman","Creater_id":126674,"Start_date":"2016-08-04 15:04:09","Question_id":228330}
{"_id":{"$oid":"5837a574a05283111e4d317e"},"Last_activity":"2016-08-25 09:47:20","Creator_reputation":56,"Question_score":4,"Answer_content":"Extreme example:  a standard normal distribution, and  a Cauchy distribution (just remove a constant from  to make it more similar to your context). Then,\\frac{f(x)^2}{p(x)} = \\dfrac{\\pi(1+x^2)}{(2\\pi)}\\exp(-x^2),and\\frac{f(x)^4}{p(x)^3} = \\dfrac{\\pi^3(1+x^2)^3}{(2\\pi)^2}\\exp(-2x^2),which are clearly integrable since they are related to the moments of normal distributions, which always exist.","Display_name":"Eso","Creater_id":128965,"Start_date":"2016-08-25 09:30:29","Question_id":231715}
{"_id":{"$oid":"5837a574a05283111e4d318b"},"Last_activity":"2016-08-25 17:34:42","Creator_reputation":7559,"Question_score":0,"Answer_content":"To evaluate your forecast accuracy, what you want to do is cross validation. What you don't want to do is data dredging, data fishing.","Display_name":"Matthew Gunn","Creater_id":97925,"Start_date":"2016-08-25 17:34:42","Question_id":231802}
{"_id":{"$oid":"5837a574a05283111e4d3199"},"Last_activity":"2016-08-25 17:08:11","Creator_reputation":1,"Question_score":-2,"Answer_content":"Something might not be dirctly related.  If you have two series  and  that , and if you suspect there are relationship between  and .  You could make a plot between  and  to examine their relationship.","Display_name":"Zhu Jinxuan","Creater_id":90818,"Start_date":"2016-08-25 17:08:11","Question_id":231024}
{"_id":{"$oid":"5837a574a05283111e4d319a"},"Last_activity":"2016-08-23 13:16:06","Creator_reputation":1259,"Question_score":6,"Answer_content":"The model you are referring to, simple linear regression, a.k.a. \"the line of best fit\" (I am confusing model and estimation method here), is admittedly very simple (as the name says). Why studying it? I can see a lot of reasons. In the following I assume that the concept of random variable has been at least informally introduced, because you mentioned it in your question. pedagogical: of course, for you it's obvious that real-valued random variables with finite second order moments form an Hilbert space. Maybe it was already obvious when you first studied probability theory. But statistics is not only teached to math students: there is a wider public, from physics to economics, to computer science, to social science, etc. These students may encounter statistics early in their course of study. They may or may not have been expoused to linear algebra, and even in the first case, they may not have seen it from the more abstract point of view of a math course. For these students, the very concept of approximating a random variable by another random variable is not so immediate. Even the basic property of the simple linear model, i.e., the fact that the error and the predictor are orthogonal random variables, is sometimes surprising to them. The fact that you can define an \"angle\" between random variables (\"nasty\" objects! measurable functions from a probability space to a measurable space) may be obvious to you, but not necessarily to a freshman. Thus, if the study of vector spaces starts with the good ol' Euclidean plane, doesn't it make sense to start the study of statistical models with the simplest one? procedural: with simple linear regression you can introduce the concept of parameter estimation, and thus the method of least squares, standard errors, etc. in its simplest case. If you think this is trivial, keep in mind that a lot of professionals, who use statistics in their job/research but are not statisticians, are deeply confused about the frequentist confidence interval! Anyway, once the easiest case has been covered, you can go to multiple linear regression. Once this is mastered, then all the linear models are available for estimation. In other words, if I can fit the model  (by OLS, or LARS in case regularization is needed, etc.), then I can fit all models of the kind .  This is a really powerful class of models, which, as noted by @DaeyoungLim, can approximate all functions in the Hilbert space, if you have an infinite set of basis functions, and if they generate a vector subspace which is dense in the Hilbert space. practical: there are numerous successful applications of simple linear regression. Okun's law in economics, Hooke's law, Ohm's law and Charles's law in physics, the relationship between blood systolic pressure and age in medicine (I have no idea if it has a name!) are all examples of simple linear regression, with varying degrees of accuracy.","Display_name":"DeltaIV","Creater_id":58675,"Start_date":"2016-08-23 06:50:00","Question_id":231024}
{"_id":{"$oid":"5837a574a05283111e4d319b"},"Last_activity":"2016-08-23 13:11:42","Creator_reputation":121,"Question_score":3,"Answer_content":"Linear Regression's popularity is due in part to it's interpretability - that is, non-technical people can understand the parameter coefficients with just a little bit of explanation.  This adds a great deal of value in business situations, where end users of the output or predictions may not have a deep understanding of math/statistics.Yes, there are assumptions and limitations with this technique (as with all approaches), and it may not provide the best fit in many cases. But Linear Regression is very robust, and can often perform quite well even when assumptions are violated.For these reasons, it is definitely worth studying.","Display_name":"B.Frost","Creater_id":108070,"Start_date":"2016-08-23 13:11:42","Question_id":231024}
{"_id":{"$oid":"5837a574a05283111e4d319c"},"Last_activity":"2016-08-23 12:07:58","Creator_reputation":882,"Question_score":4,"Answer_content":"A further reason is the lovely way regression gives a unified treatment of techniques like ANOVA. To me, the usual 'elementary' treatment of ANOVA seems quite obscure, yet a regression-based treatment is crystal clear. I suspect this has much to do with the way regression models make explicit some assumptions that in 'elementary' treatments are tacit and unexamined. Furthermore, the conceptual clarity offered by such a unifying perspective is accompanied by similar practical benefits when time comes to implement methods in statistical software.This principle applies not only to ANOVA, but to extensions like restricted cubic splines--which notably address your second question. ","Display_name":"David C. Norris","Creater_id":41404,"Start_date":"2016-08-23 12:07:58","Question_id":231024}
{"_id":{"$oid":"5837a574a05283111e4d319d"},"Last_activity":"2016-08-21 22:50:46","Creator_reputation":441,"Question_score":9,"Answer_content":"I agree not all relations are linear in itself but quite a lot of relations can be linearly approximated. We have seen many such cases in mathematics such as the Taylor series or Fourier series etc. The key point here is, geomatt22 said in the comment, you can in general transform the nonlinear data and apply some kind of transformation with basis functions and linearize the relationship. The reason universities only address 'multiple linear regression models' (including simple regression models) is because they are the building block to models of a more advanced level which are also linear.Mathematically speaking, as long as you can prove that a certain linear approximation is dense in a Hilbert space, then you will be able to use the approximation to represent a function in the space.","Display_name":"Daeyoung Lim","Creater_id":98085,"Start_date":"2016-08-21 22:50:46","Question_id":231024}
{"_id":{"$oid":"5837a574a05283111e4d31ac"},"Last_activity":"2016-08-25 15:35:04","Creator_reputation":1008,"Question_score":41,"Answer_content":"The other answers are truly marvelous - they give real life examples.I want to explain why this can happen despite our intuition to the contrary.See this geometrically!Correlation is the cosine of the angle between the vectors. Essentially, you are asking whether it is possible that  makes an acute angle with  (positive correlation) makes an acute angle with  (positive correlation) makes an obtuse angle with  (negative correlation)Yes, of course:In this example:Your Intuition is Right!However, your surprise is not misplaced.The angle between vectors is a distance metric on the unit sphere, so it satisfies the triangle inequality: \\measuredangle AB \\le \\measuredangle AC + \\measuredangle BCthus, since , \\arccos corr(A,B) \\le \\arccos corr(A,C) + \\arccos corr(B,C) therefore corr(A,B) \\ge corr(A,C)\\times corr(B,C) - \\sqrt{(1-corr^2(A,C))(1-corr^2(B,C))} So, if , then if , then if , then ","Display_name":"sds","Creater_id":13538,"Start_date":"2016-08-09 20:20:11","Question_id":229052}
{"_id":{"$oid":"5837a574a05283111e4d31ad"},"Last_activity":"2016-08-21 01:32:17","Creator_reputation":1,"Question_score":0,"Answer_content":"C = mB + n (A-proj_B(A))then\\left\u0026lt;C,A\\right\u0026gt; = m\\left\u0026lt;B,A\\right\u0026gt; + n\\left\u0026lt;A,A\\right\u0026gt; -n \\left\u0026lt;B,A\\right\u0026gt; Then covariance between C and A could be negative  in two conditions:","Display_name":"Zhu Jinxuan","Creater_id":90818,"Start_date":"2016-08-21 01:32:17","Question_id":229052}
{"_id":{"$oid":"5837a574a05283111e4d31ae"},"Last_activity":"2016-08-09 17:45:25","Creator_reputation":381,"Question_score":28,"Answer_content":"I've heard this car analogy which applies well to the question:Driving uphill (A) is positively related to the driver stepping on the gas (B)Driving uphill (A) has a negative effect on vehicle speed (C)Stepping on the gas (B) has a positive effect on vehicle speed (C)The key here is the driver's intention to maintain a constant speed (C), therefore the positive correlation between A and B naturally follows from that intention. You can construct endless examples of A, B, C with this relationship thus.The analogy comes from an interpretation of Milton Friedman's Thermostat and comes from an interesting analysis of monetary policy and econometrics, but that's irrelevant to the question.","Display_name":"congusbongus","Creater_id":127186,"Start_date":"2016-08-09 17:25:01","Question_id":229052}
{"_id":{"$oid":"5837a574a05283111e4d31af"},"Last_activity":"2016-08-09 13:23:08","Creator_reputation":5445,"Question_score":7,"Answer_content":"Yes, this is trivial to demonstrate with a simulation:Simulate 2 variables, A and B that are positively correlated:\u0026gt; require(MASS)\u0026gt; set.seed(1)\u0026gt; Sigma \u0026lt;- matrix(c(10,3,3,2),2,2)\u0026gt; dt \u0026lt;- data.frame(mvrnorm(n = 1000, rep(0, 2), Sigma))\u0026gt; names(dt) \u0026lt;- c(\"A\",\"B\")\u0026gt; cor(dt)          A         BA 1.0000000 0.6707593B 0.6707593 1.0000000Create variable C:\u0026gt; dtA - dt\\operatorname{cor}(A,B) \u0026gt; 0\\operatorname{cor}(A,C) \u0026gt; 0\\operatorname{cor}(B,C) \u0026lt; 0$\u0026gt; set.seed(1)\u0026gt; Sigma \u0026lt;- matrix(c(1,0.5,0.5,0.5,1,-0.5,0.5,-0.5,1),3,3)\u0026gt; dt \u0026lt;- data.frame(mvrnorm(n = 1000, rep(0,3), Sigma, empirical=TRUE))\u0026gt; names(dt) \u0026lt;- c(\"A\",\"B\",\"C\")\u0026gt; cor(dt)    A    B    CA 1.0  0.5  0.5B 0.5  1.0 -0.5C 0.5 -0.5  1.0","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-09 12:34:07","Question_id":229052}
{"_id":{"$oid":"5837a574a05283111e4d31b0"},"Last_activity":"2016-08-09 12:32:35","Creator_reputation":7559,"Question_score":32,"Answer_content":"Yes, two co-occuring conditions can have opposite effects.For example:Making outrageous statements (A) is positively related to being entertaining (B).Making outrageous statements (A) has a negative effect on winning elections (C).Being entertaining (B) has a positive effect on winning elections (C).","Display_name":"Matthew Gunn","Creater_id":97925,"Start_date":"2016-08-09 12:24:11","Question_id":229052}
{"_id":{"$oid":"5837a574a05283111e4d31bd"},"Last_activity":"2016-08-25 15:18:45","Creator_reputation":1,"Question_score":0,"Answer_content":"I feel like this question has been largely addressed on the web. I will limit myself to post three videos on which the intuition behind the convergence of random variables is presented. Of course, someone braver, more energetic, and with more imagination than me may be willing to spend some time on writing yet another explanation:Convergence in Random Variables (part 1/3)Convergence in Random Variables (part 2/3)Convergence of random variables (part 3/3)","Display_name":"Combo","Creater_id":129004,"Start_date":"2016-08-25 14:24:30","Question_id":231773}
{"_id":{"$oid":"5837a574a05283111e4d31cc"},"Last_activity":"2016-08-25 15:06:43","Creator_reputation":6,"Question_score":0,"Answer_content":"All calculations was correct. I forgot that Fisher info formula is  only in regular models. So to get the right answer we must center , and then, as @eric_kernfeld told, eliminate expectation.The correct result is ","Display_name":"Rievturge","Creater_id":116498,"Start_date":"2016-08-25 15:06:43","Question_id":231329}
{"_id":{"$oid":"5837a574a05283111e4d31db"},"Last_activity":"2016-08-25 14:42:15","Creator_reputation":1,"Question_score":0,"Answer_content":"The result of the error I was getting was due to dplyr being active.  Once I detached this library, the code provided in plm matches that of xtreg.","Display_name":"Ralph M","Creater_id":128998,"Start_date":"2016-08-25 14:42:15","Question_id":231774}
{"_id":{"$oid":"5837a574a05283111e4d31e8"},"Last_activity":"2016-08-25 14:21:03","Creator_reputation":755,"Question_score":3,"Answer_content":"Looking at the sample ACF of your differenced series, this seems to cut off at lags .  This suggests that the differenced series is MA(1) so overall you possibly have an IMA(1,1) model,(1-B)Y_t = (1 - \\theta_1 B)w_t.When you fit an incorrect ARI(1,1) model to the data, the residuals will be given by \\hat w_t = (1-B)(1-\\hat\\phi_1 B)Y_t, where  is the estimate of the AR parameter in the incorrect ARI(1,1) model.   Applying  to both sides of the first equation yields\\hat w_t = (1-\\hat\\phi_1 B)(1-\\theta_1 B)w_t.Provided that the true model is IMA(1,1), this tells you that the residuals  from the fitted incorrect ARI(1,1) model should behave like a MA(2) process (rather than white noise if the correct model was fitted).  Judged by your plot of the ACF of the residuals, however, this does not seem to be the case for your data (this ACF should cut off at lags ) which suggests that something else must be going on here or that there is some error in how you have computed the ACF or the residuals.","Display_name":"Jarle Tufto","Creater_id":77222,"Start_date":"2016-08-25 11:37:54","Question_id":231345}
{"_id":{"$oid":"5837a574a05283111e4d31f5"},"Last_activity":"2016-08-25 14:13:22","Creator_reputation":25170,"Question_score":0,"Answer_content":"Check the Statistics 110: Probability course by Joe Blitzstein (Harvard University). Materials and lectures are freely available online. Blitzstein provides many examples of common probability distribution and \"stories\" behind them, that make it easier to memorize what is the general idea behind them. You can check also the paper  Lawrence, M. and McQueston, J.T. (2008). Univariate Distribution Relationships. American Statistician, 62(1): 45–53. or this diagram by John D. Cook and Compendium of Conjugate Priors by  Daniel Fink (1997).","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-25 14:13:22","Question_id":231722}
{"_id":{"$oid":"5837a574a05283111e4d31f6"},"Last_activity":"2016-08-25 09:59:42","Creator_reputation":111,"Question_score":2,"Answer_content":"A lot of these exist. Here are a couple:This one is a flowchart for choosing a distribution.This is a summary of common distributions.Wikipedia often has applications for various distributions on their pages.","Display_name":"Great38","Creater_id":122545,"Start_date":"2016-08-25 09:59:42","Question_id":231722}
{"_id":{"$oid":"5837a574a05283111e4d3203"},"Last_activity":"2016-08-25 14:12:46","Creator_reputation":25669,"Question_score":7,"Answer_content":"Since(y_1,\\ldots,y_r)\\sim\\frac{n!\\theta^{-r}}{(n-r)!}e^{-\\frac{1}{\\theta}[\\sum_{i=1}^{r}y_i+(n-r)y_r]}\\mathbb{I}_{y_\\le y_2\\le \\ldots \\le y_r}you have the joint pdf of . From there, you can deduce the pdf of s_r=\\sum_{i=1}^{r}y_i+(n-r)y_r\\,.Indeed, because the Jacobian of the transform is constant,\\begin{align*}f_s(y_1,\\ldots,y_{r-1},s_r) \u0026amp;\\propto f_Y\\left(y_1,\\ldots,\\left\\{s_r-\\sum_{i=1}^{r-1}y_i\\right\\}\\Big/(n-r+1)\\right) \\\\\u0026amp;\\propto \\theta^{-r} \\exp\\{-s_r/\\theta\\}\\mathbb{I}_{y_\\le y_2\\le \\ldots \\le\\left\\{s_r-\\sum_{i=1}^{r-1}y_i\\right\\}/(n-r+1)}\\end{align*}implies by integration in  thatf_s(s_r)\\propto\\theta^{-r} \\exp\\{-s_r/\\theta\\}s_r^{r-1}Indeed,\\begin{align*}f_s(s_r)\u0026amp;=\\int\\cdots\\int f_s(y_1,\\ldots,y_{r-1},s_r)\\text{d}y_1\\cdots\\text{d}y_{r-1}\\\\\u0026amp;= \\theta^{-r} \\exp\\{-s_r/\\theta\\}\\int\\cdots\\int \\mathbb{I}_{y_\\le y_2\\le \\ldots \\le\\left\\{s_r-\\sum_{i=1}^{r-1}y_i\\right\\}/(n-r+1)}\\text{d}y_1\\cdots\\text{d}y_{r-1}\\end{align*}leads to constraint  by  and byy_{r-1}\\le \\left\\{s_r-\\sum_{i=1}^{r-1}y_i\\right\\}/(n-r+1)=\\left\\{s_r-\\sum_{i=1}^{r-2}y_i\\right\\}/(n-r+1)-\\frac{y_{r-1}}{n-r+1}which simplifies intoy_{r-1}\\le \\left\\{s_r-\\sum_{i=1}^{r-2}y_i\\right\\}/(n-r+2)If one starts integrating in , the most inner integral is\\begin{align*}\\int_{y_{r-2}}^{\\{s_r-\\sum_{i=1}^{r-2}y_i\\}/(n-r+2)}\\text{d}y_{r-1}\u0026amp;=\\left\\{s_r-\\sum_{i=1}^{r-2}y_i\\right\\}/(n-r+2)-y_{r-2}\\\\\u0026amp;=\\left\\{s_r-\\sum_{i=1}^{r-3}y_i\\right\\}/(n-r+2)-\\frac{(n-r+1)y_{r-2}}{n-r+2}\\end{align*}and from there one can proceed by recursion.Hences_r\\sim\\mathcal{G}a(r,1/\\theta)Here is an R simulation to show the fit:obtained as followsn=10r=5    sim=matrix(rexp(n*1e4),1e4,n)sim=t(apply(sim,1,sort))res=apply(sim[,1:r],1,sum)+(n-r)*sim[,5]hist(res,prob=TRUE)curve(dgamma(x,sh=(n-r),sc=1),add=TRUE)","Display_name":"Xi\u0026#39;an","Creater_id":7224,"Start_date":"2015-08-01 10:39:46","Question_id":161145}
{"_id":{"$oid":"5837a574a05283111e4d3210"},"Last_activity":"2011-02-06 07:48:56","Creator_reputation":17725,"Question_score":4,"Answer_content":"Below, I briefly sketch out one \"back of the envelope\" calculation, which is in no way optimal, but does at least show that the probability converges to one exponentially fast. Before getting into the (gory) details, we give the answer.\r\\Pr(\\text{at least one actor chooses a unique outcome}) \\geq 1 - (e/4)^{n/2} .\rAs mentioned before, comparing this result to the above for small  shows that the result is not super sharp. But, it does provide a guarantee for all  and shows that as  grows, the probability goes to 1 at least exponentially fast. (The actual rate is probability more like ).The idea is to turn the problem into an occupancy or coupon-collecting problem. If you want a more precise answer, you can refer to the (extensive) literature on these subjects, but to prove a faster rate takes messier analysis. Let  be the proportion of actors that choose a unique outcome (when we have  total actors), which we use below. (Technically, the analysis below is for even , but that's a minor point. The same result holds for odd ; just replace  by  and  by  in the relevant places.)Restating the problem in terms of balls and bins. We have  balls and  bins. The \"balls\" are the \"actors\" and the \"bins\" are the possible outcomes. We want to know the probability that at least one bin has only a single ball in it. The probability that any particular bin has only one ball is . So, the expected proportion of balls in a bin by themselves (i.e., actors that choose a unique outcome) is\r\\mathbb{E}(U_n) = \\left(1 - \\frac{1}{2n}\\right)^n \\to e^{-1/2} \\approx 60.65\\%\rThis gives us pretty good hope that the probability of at least one ball being in a bin by itself is (very) high.To get a bound on this probability, note that if each of the occupied bins has at least two balls in it, then at most  bins can be occupied. So\r\\Pr(\\text{no bin has exactly 1 ball}) \\leq \\Pr(\\text{no more than n/2 bins are occupied})\rNow, if no more than  bins are occupied, then there must be some subset of  bins that are all empty. There are  such subsets, and so by the union bound, we get\r\\Pr(\\text{some subset of 3n/2 bins is empty}) \\leq {2n \\choose n/2} \\left(1 - \\frac{3n/2}{2n}\\right)^{n} = {2n \\choose n/2} 4^{-n}\rTo finish up, we use the elementary inequality\r{2n \\choose n/2} \\leq \\left(\\frac{2n e}{n/2}\\right)^{n/2} = (4e)^{n/2} .\rPutting it all together, we get\r\\Pr(\\text{no bin has exactly 1 ball}) \\leq (4 e)^{n/2} 4^{-n} = (e/4)^{n/2} .\rHence,\r\\Pr(\\text{at least one actor chooses a unique outcome}) \\geq 1 - (e/4)^{n/2} .\r","Display_name":"cardinal","Creater_id":2970,"Start_date":"2011-02-01 19:44:08","Question_id":6762}
{"_id":{"$oid":"5837a574a05283111e4d3211"},"Last_activity":"2011-02-01 07:56:57","Creator_reputation":13273,"Question_score":5,"Answer_content":"This is not a simple calculation in general, though the probability will be almost 1 if  is any substantial size.   To take your example of five people and 10 bars, there are , i.e 100,000 possible equally-likely distributions.  All of these will have at least one person alone, except the 10 cases where all five are in a single bar or the 900 cases where three are in a particular bar and two in another.  That leaves 99090 distributions which satisfy your condition, more than 99%.   Note that 99090 is not divisible by .For  to , the fractions seem to be      and , where the denominator in each case is . I would expect the probabilities then to keep rising as  increases beyond 6.    ","Display_name":"Henry","Creater_id":2958,"Start_date":"2011-01-31 17:44:41","Question_id":6762}
{"_id":{"$oid":"5837a574a05283111e4d321e"},"Last_activity":"2016-08-25 13:44:21","Creator_reputation":7559,"Question_score":2,"Answer_content":"Percent increase sounds fine to me. If you shrink the denominator of a fraction by , that's:equivalent to multiplying the denominator by which is equivalent to multiplying the overall number by .which is an increase of 25%More algebra:Going from  to  is \\begin{align*}\\frac{37.5-30}{30} \u0026amp;= \\frac{\\frac{300}{8} - \\frac{300}{10}}{\\frac{300}{10}}\\\\\u0026amp;=  \\left( \\frac{\\frac{10}{8} - 1}{1} \\right) \\\\\u0026amp;= .25\\end{align*}In variables:\\begin{align*}\\frac{\\frac{x}{\\alpha y} - \\frac{x}{y}}{\\frac{x}{y}}\u0026amp;= \\frac{1}{\\alpha} - 1  \\\\\\end{align*}","Display_name":"Matthew Gunn","Creater_id":97925,"Start_date":"2016-08-25 13:44:21","Question_id":231756}
{"_id":{"$oid":"5837a574a05283111e4d322b"},"Last_activity":"2010-08-05 18:40:54","Creator_reputation":156,"Question_score":6,"Answer_content":"If you wish to trade processing speed for memory (which I think you do), I would suggest the following algorithm:Set up a loop from 1 to N Choose K, indexed by iEach i can be considered an index to a combinadic, decode as suchUse the combination to perform your test statistic, store the result, discard the combinationRepeatThis will give you all N Choose K possible combinations without having to create them explicitly. I have code to do this in R if you'd like it (you can email me at mark dot m period fredrickson at-symbol gmail dot com).","Display_name":"Mark M. Fredrickson","Creater_id":729,"Start_date":"2010-08-05 18:40:54","Question_id":1286}
{"_id":{"$oid":"5837a574a05283111e4d322c"},"Last_activity":"2010-08-05 03:17:47","Creator_reputation":17873,"Question_score":1,"Answer_content":"Generating combinations is pretty easy, see for instance this; write this code in R and then process each combination at a time it appears.","Display_name":"mbq","Creater_id":88,"Start_date":"2010-08-05 03:17:47","Question_id":1286}
{"_id":{"$oid":"5837a574a05283111e4d3239"},"Last_activity":"2016-01-18 07:49:44","Creator_reputation":8577,"Question_score":1,"Answer_content":"If the data are more or less normal in each group, you can do a two-sample t-test to compare the scores. If you want to compare all four scores at once, you can do a multivariate t-test. The two-sample t-test really only requires that the sample averages of the two groups be normal. This will happen if the data are normal, but it is also a fair assumption when the data are sort of normalesque --i.e., one mode, more or less symmetric. One of the most important theorems in Statistics is the Central Limit Theorem, which states that in most situations, the sample average tends towards normality as the sample gets large. So even if your big sample is not normal, the average of 675 items will be pretty close, and your t-test will work. In fact, if the original data are symmetric and you don't have wild outliers, the average of a sample of 25 is pretty close to normal as well. convergence can be rapid.Now, a word about statistical tests. Another big theorem states that when the null hypothesis is false, your test will reject the null when the sample gets large. So when you have a big sample, like 675, even a small departure from normality with be picked up by Kolmogorov-Smirnov. A similar departure may not be detected by a sample of size 25. That's why you think your small sample is normal and your large sample is not.It's also why a lot of people don't test for normality before carrying out a subsequent test. A better plan is to do your tests, and then look at the residuals and see if they look normal. Plot a histogram, or do a quantile plot. Whatever software you are using will have options for doing this, or you should switch software. Rather than check for normality using a test, the better approach is to graph the data, examine outliers (if they exist), and possibly remove them. Then do the comparison. Some people would advise a graphical approach to comparing your groups: do boxplots for the 25 and the 675. I like the idea of a formal test in this case because the sample sizes differ so much. The average of the 25 could differ a lot from the average of the 675 due simply to random fluctuation. That sort of distinction can be hard to eyeball on a boxplot, so best to do the test.","Display_name":"Placidia","Creater_id":14188,"Start_date":"2016-01-18 07:49:44","Question_id":191202}
{"_id":{"$oid":"5837a574a05283111e4d3246"},"Last_activity":"2016-08-25 13:19:17","Creator_reputation":292,"Question_score":1,"Answer_content":"If you are going to use edge detection, you will have to use distance transform to do the kind of classification you are thinking of. Once that is done you need to create a distance matrix between the test image(s) (ones without the label) and the training image(s) (ones with the label).But may I suggest using HoG transform instead, or at least a Sobel filter instead of an edge detector. The Sobel filter at least is simple to implement in Matlab and I'm sure someone has implemented the HoG filter. The reason is simple: the edge detectors give you binary features and this in my opinion makes it harder to compare since it is not scale and position invariant.Once the feature vector is done, choose a classifier (SVMs, CART, NN etc.) to classify into the classes.","Display_name":"Sachin_ruk","Creater_id":29537,"Start_date":"2015-06-28 22:16:44","Question_id":158696}
{"_id":{"$oid":"5837a574a05283111e4d3247"},"Last_activity":"2015-06-28 22:07:52","Creator_reputation":153,"Question_score":0,"Answer_content":"Remember that when doing computer vision and image processing you should assure that all images are taken in the same conditions. Preliminary preparation of data (exposure, resizing, lighting, filtering etc.) dramatically reduces problems that might occur later.Yes, choosing image pixels as an input features seems to be a reasonable choice. Remember that for even a relatively small image the number of features grows very fast (i.e. 256 x 256 px image resutls in 65 536 features). Therefore some dimension reduction technique should be applied (i.e PCA). You might use Python scikit-learn library that provides all necessary tools.I'm not sure about the performance of your approach - if your dataset does not have a representative amount of images of each class it would probably fail. You could consider experimenting with other features obtained from Gray Level Co-ocurance Matrix (enables many useful metrics, but your image should be represented in gray scale) or  Zernike Moments to describe objects/shapes in an image (more info here).Regards.","Display_name":"Khozzy","Creater_id":55763,"Start_date":"2015-06-26 00:20:29","Question_id":158696}
{"_id":{"$oid":"5837a574a05283111e4d3248"},"Last_activity":"2015-06-26 00:07:08","Creator_reputation":5937,"Question_score":4,"Answer_content":"Your approach goes in the line of the popular histogram of gradients approach. See here and the corresponding Wikipedia entry. Now unless you have some already labelled data, training such a system is quite laborious. I possible, I would start by using some available implementation to experiment with, like the one offered by scikit-image.There are some other features, like Linear Binary Pattern, but there are not as powerful as HOG. See in the module corresponding of scikit-image for a list of features and their implementations.As for CNN, you should not need to extract any features. The system learns the features automatically. That is one of the nice properties of deep architectures. A huge number of papers show that these systems learn some edge oriented filters features (in the same line as the idea you are considering).Note that these features do not consider color. That may be an interesting feature for you to consider. Or extract the features for each of the color channels.Hope this helps.","Display_name":"jpmuc","Creater_id":17908,"Start_date":"2015-06-26 00:07:08","Question_id":158696}
{"_id":{"$oid":"5837a574a05283111e4d3255"},"Last_activity":"2016-08-25 13:17:36","Creator_reputation":336,"Question_score":0,"Answer_content":"Your intuition is good. The term you defined is called the time-delayed mutual information,  and is often used as a measure of temporal dependence.Under what circumstances do we expect time-delayed mutual information to fail, while transfer entropy succeeds? Suppose  and  are binary, set . Suppose  and  is identical to . Now if you look at the time delayed mutual information, you will see that you get one bit. On the other hand, the transfer entropy will be zero bits. So one measure calls this strong temporal dependence and the other sees no temporal dependence. What is the intuitive difference here and why does it arise? You can use  to perfectly predict , and that's why the time-delayed mutual information is 1 bit. However, if you look at , you can also perfectly predict .  Because you can already perfectly predict  from its own past, knowing about  doesn't add anything, and that's meaning behind the transfer entropy formula. It tries to prevent things like two temporal processes that repeat with the same frequency from looking correlated. Two clocks on different continents can have high mutual information, but no transfer entropy.","Display_name":"Greg Ver Steeg","Creater_id":88727,"Start_date":"2016-08-25 13:17:36","Question_id":231702}
{"_id":{"$oid":"5837a574a05283111e4d3268"},"Last_activity":"2016-08-23 11:39:32","Creator_reputation":101,"Question_score":0,"Answer_content":"I know I'm late to the party, but in case anybody sees this and is looking for a response here are some thoughts. Context can be very important in the application of RL algorithms (so forgive me if this is of no use to you). A very interesting method for determining feature vectors (particularly in problems with unique state-space geometries) is using proto-value functions (PVF). The essential idea is that on any state space, there exist basis functions which can be used to approximate any value function you might place on that state-space. This set of functions is based on the intrinsic geometry of the state space and is 'coordinate free' in a sense.You can read about PVF in the works by Mahadevan and Maggioni. In particular, their journal paper:http://www.jmlr.org/papers/volume8/mahadevan07a/mahadevan07a.pdfAdditionally, you may want to look at both basis function refinement and selection methods. Some references to get you started can be found on pg 98-99 of Busoniu et al:https://books.google.com/books?id=UGUqcl8_T9QC\u0026amp;pg=PA95\u0026amp;lpg=PA95\u0026amp;dq=finding+value+function+approximators+automatically\u0026amp;source=bl\u0026amp;ots=Xka5TPU4WB\u0026amp;sig=uz-UwxhiM59uvMbLQSBTsxk8YyA\u0026amp;hl=en\u0026amp;sa=X\u0026amp;ved=0ahUKEwjk2MKil9jOAhUMXB4KHX2FDx4Q6AEIHjAA#v=onepage\u0026amp;q=finding%20value%20function%20approximators%20automatically\u0026amp;f=false","Display_name":"David","Creater_id":128699,"Start_date":"2016-08-23 11:39:32","Question_id":127252}
{"_id":{"$oid":"5837a574a05283111e4d3277"},"Last_activity":"2016-08-25 13:00:43","Creator_reputation":12897,"Question_score":1,"Answer_content":"From a purely technical perspective, the common remedy for unequal time series lengths used in applications is to match time indices of the different series in a sensible way (e.g. match  with , not with  for , unless the subject-matter logic suggests otherwise) and thento cut the values that stick out in the beginning or the end of the series. This way you are throwing out data, but often it is a minor loss (in your case, you would only lose 1 observation). For example, this is typically done in AR and VAR models where lags of the original variables are shorter series than the original series. (Note that in AR and VAR models the time indices across the original and the lagged series are matched with time lags intentionally and the data matrix looks like , unlike what I mention in point 1.)But there still remains a question how to interpret PCA for variables transformed using different orders of diferencing...","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-25 13:00:43","Question_id":231313}
{"_id":{"$oid":"5837a574a05283111e4d3278"},"Last_activity":"2016-08-24 06:45:45","Creator_reputation":4307,"Question_score":2,"Answer_content":"Since you are doing dimensionality reduction already (via PCA), then a simple thing to do would be to use the shorter length as your base, and split any longer series into multiple (redundant) series.Say you start with two time series,  and , where . However  has a secular trend, so you want to difference it, and use  instead. A simple option is to increase the dimensionality of your \"data\" from 2 to 3, adding a \"look ahead\" version of . So now your data would be , , and , where now .Here the idea would be that if the two  series have the \"same information\", then the PCA will tell you this.Without knowing more about your particular data and application, I cannot say whether or not this technique would be appropriate (e.g. in terms of causality or your system dynamics).","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-24 06:45:45","Question_id":231313}
{"_id":{"$oid":"5837a574a05283111e4d3287"},"Last_activity":"2016-08-25 12:46:27","Creator_reputation":744,"Question_score":2,"Answer_content":"A general way to do this is to formulate a null model: a statistical model for what generates your points in the absence of anomalies. You can then estimate the parameters of your model and evaluate the joint density of each row; low densities are more anomalous. I favor this approach because it reveals the assumptions implied by your choice of anomalosity (anomalousness?).For example, if you assume your columns  and  are independently normal, then you can estimate the mean and variance of each column separately with e.g.  and  normalize by those values to get  and The negative log density will increase with , so points with the largest values are most anomalous.Another option is to use an anomaly detection method with strong empirical performance -- FRaC by Noto et al comes to mind. This method uses a similar framework, but it focuses on the relationships among columns, modeling Pr(X|Y) and Pr(Y|X) instead of Pr(X,Y).","Display_name":"eric_kernfeld","Creater_id":86176,"Start_date":"2016-08-25 12:46:27","Question_id":231737}
{"_id":{"$oid":"5837a574a05283111e4d328c"},"Last_activity":"2016-08-25 12:36:11","Creator_reputation":5787,"Question_score":2,"Answer_content":"First question: . So .  Second question: for all . Therefore, by dominated convergence (because the left-hand side is dominated by 1, which is integrable with respect to ), we may take the integral of both sides with respect to , obtaining , which evaluates to , thereby proving the assertion that  converges in distribution to .","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-25 10:21:36","Question_id":231581}
{"_id":{"$oid":"5837a574a05283111e4d3299"},"Last_activity":"2016-08-25 12:34:31","Creator_reputation":7915,"Question_score":0,"Answer_content":"  Is my evaluation on this small dataset good enough to tell whether my model is performing well?It's difficult to tell when using a fraction of the available dataset. Also, it depends on the complexity of the task, among other things.  Does the above chart shows that my model is getting overfitting easily?Yes since overfitting starts before epoch 5.  Do you have any recommendation for quickly evaluating a new proposed model   and avoid overfitting?Add some patience to avoid computing more epochs that needed, and some regularization technique to avoid overfitting too early","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-25 12:27:45","Question_id":231416}
{"_id":{"$oid":"5837a574a05283111e4d32a6"},"Last_activity":"2016-08-25 12:28:59","Creator_reputation":8337,"Question_score":2,"Answer_content":"A -value can't actually reach 0, in most cases, so this just means that you obtained  and your software rounded it to 0.000.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-25 12:28:59","Question_id":231748}
{"_id":{"$oid":"5837a574a05283111e4d32b3"},"Last_activity":"2016-08-25 12:26:23","Creator_reputation":151,"Question_score":0,"Answer_content":"Given that xtrue[i]'s are constrained, Stan requires that these constraints are included in the variable declaration. To my knowledge, these constraints must be scalar quantities.Below, I worked around this requirement by considering auxillary parameters, xraw[i], which have a truncated normal distribution.m \u0026lt;- \"data {  int\u0026lt;lower = 1\u0026gt; N;  real x[N];}parameters {  real\u0026lt;lower=-1, upper=0\u0026gt; xraw[N]; }transformed parameters {  real xtrue[N];  for(i in 1:N)  xtrue[i] = xraw[i] + x[i];}model{  for(i in 1:N){    target += normal_lpdf(xraw[i]| 0, 1);    target += -log_diff_exp(normal_lcdf(0| 0, 1), normal_lcdf(-1| 0, 1));  }}\"library(rstan)rstan_options(auto_write = FALSE)options(mc.cores = parallel::detectCores())nobs=10xtrue=runif(nobs,0,5)xobs=ceiling(xtrue+rnorm(nobs,0,1))dat=list(N=length(xobs),x=xobs)init_fun \u0026lt;- function() {list(xtrue=xobs-.5) }mod \u0026lt;- stan_model(model_code = m)s \u0026lt;- sampling(mod, data = dat, iter = 2000, chains = 1, thin = 3, init = init_fun)fit=stan(model_code=m, data = dat,iter = 2000, chains = 1,thin=3,init=init_fun)parms=extract(s,c('xtrue'))xtrue \u0026lt;- colMeans(parms[['xtrue']])head(xobs)[1] 4 2 5 6 4 2   head(xtrue)[1] 3.533775 1.507112 4.561159 5.545677 3.538002 1.520043par(mfrow = c(2,5))for(i in 1:10) {  hist(samples$xtrue[,i], prob=T, main = paste(c(\"xtrue[\",i,\"]\"), collapse=\"\"), xlab=NULL)  curve(dnorm(x, xobs[i], 1)/(.5 - pnorm(-1)), add=T, lty=2)}The posterior draws appear to follow the correct distributions:","Display_name":"Eric Mittman","Creater_id":87365,"Start_date":"2016-08-25 12:26:23","Question_id":202563}
{"_id":{"$oid":"5837a574a05283111e4d32c0"},"Last_activity":"2016-08-25 12:20:46","Creator_reputation":2852,"Question_score":0,"Answer_content":"I have solved this problem using a blockwise coordinate descent-type algorithm. Given the low interest in this question I leave the details out but if anyone is interested in this or a similar problem and run into this question and answer I'd be happy to expand.Repeat until convergence:Initialize , ,  and Update  by solving the first order condition Update  by solving the first order condition Rescale ,  and  to satisfy the constraints without changing the likelihood value. E.g., , where  denotes Hadamard product and  denotes Kronecker product.Let  and for  update  by solving the first order condition .Some notes: every update is available in closed form, and every update is convex but the full problem is not convex in general, i.e. there is no guarantee that the point of convergence is a unique global maximum of the likelihood function.","Display_name":"Student001","Creater_id":37483,"Start_date":"2016-08-25 12:20:46","Question_id":228264}
{"_id":{"$oid":"5837a574a05283111e4d32cd"},"Last_activity":"2016-08-25 12:20:36","Creator_reputation":14005,"Question_score":2,"Answer_content":"Here's another approach that uses a common trick with characteristic functions to avoid having to work out the sums / integrals. I'll set  without loss of generality, it simplifies notation and can be put back in in obvious ways in what follows.  This means all the \"\"s below are not related to the  in the problem statement, until the very end where I include it again.First, note that the definition of  involves the sum of  corresponding to .  This can be thought of as summing \"observed\" , where an  is \"observed\" with a probability  that is the same across all ..  The number of \"observed\" , label it , is therefore distributed Poisson, which is the same as Poisson.The proof of this is straightforward.  The number of observed , label it , conditional upon  is clearly distributed Binomial.  Now, let's look at the characteristic function (ch.f.) of the Binomial distribution:We will want to integrate out  w.r.t. the Poisson distribution to get the ch.f. of .  The simple way to do this is to note that:Writing out the integration (summation) gives us:Looking at this, we can see this will have the same form as the ch.f. of a Poisson distribution (), just with  substituted in wherever  appears in the ch.f.  Making this substitution gives us:which quickly reduces to:which can be rearranged to:which is the ch.f. of a Poisson variate with mean .  Substituting  for  and  for  gives us the result.On to step 2.  Now we have the ch.f. of the number of elements in the sum . Let's define  as the ch.f. of ,  as the ch.f. of the sum of   and  as the ch.f. of a single .  Since the elements are i.i.d., we know that, conditional upon , We can apply exactly the same approach as above to integrate out :where we know that  is a Poisson distribution.  This will be the ch.f. of a Poisson distribution with  substituted for :Adding the  from the original problem statement gives the answer:","Display_name":"jbowman","Creater_id":7555,"Start_date":"2016-08-25 12:20:36","Question_id":231194}
{"_id":{"$oid":"5837a574a05283111e4d32ce"},"Last_activity":"2016-08-25 11:46:32","Creator_reputation":380,"Question_score":3,"Answer_content":"I was missing the knowledge of the exponential series:e^x=1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+\\frac{x^4}{4!}+\\cdotsI also made a mistake when I separated the uniform in the expectation. Fixing these problems:\\begin{align}\\mathbb{E}(e^{iuY_1})\u0026amp;=\\sum_nP(N=n)\\mathbb{E}(e^{iuY_1}\\mid N=n)\\\\\u0026amp;=\\sum_nP(N=n)\\prod_{j=1}^n\\mathbb{E}(e^{iu\\mathbb{1}_{\\{U_j\\leq 1\\}}X_j})\\quad\\text{(by independence)}\\\\\u0026amp;=\\sum_n P(N=n)\\left(\\mathbb{E}(e^{iu\\mathbb{1}_{\\{U_1\\leq 1\\}}X_1})\\right)^n\\quad\\text{(by i.i.d.)}\\\\\u0026amp;=\\sum_{n=0}^\\infty\\frac{(\\lambda T)^n e^{-(\\lambda T)}}{n!}\\left(\\mathbb{E}(e^{iu\\mathbb{1}_{\\{U_1\\leq 1\\}}X_1})\\right)^n\\quad\\text{(by Poisson)}\\\\\u0026amp;=e^{-(\\lambda T)}\\cdot e^{(\\lambda T)\\mathbb{E}(e^{iu\\mathbb{1}_{\\{U_1\\leq 1\\}}X_1})}\\quad\\text{(by the exponential series)}\\end{align}We can calculate the expectation by conditioning on the uniform:\\mathbb{E}(e^{iu\\mathbb{1}_{\\{U_1\\leq 1\\}}X_1})=\\frac{T-1}{T}+\\frac{1}{T}\\int e^{iux}f(x)dxSubstituting and doing some algebra we get the answer:\\begin{align}\\mathbb{E}(e^{iuY_1})\u0026amp;=e^{\\lambda \\int (e^{iux}-1)f(x)dx}\\end{align}","Display_name":"Guilherme Salom\u0026#233;","Creater_id":25824,"Start_date":"2016-08-25 11:46:32","Question_id":231194}
{"_id":{"$oid":"5837a574a05283111e4d32db"},"Last_activity":"2015-01-26 02:00:55","Creator_reputation":1274,"Question_score":6,"Answer_content":"Let's recall some formulas about the Gaussian process regression. Suppose that we have a sample . For this sample loglikelihood has the form:L = -\\frac12 \\left( \\log |K| + \\mathbf{y}^T K^{-1} \\mathbf{y}\\right),where  is the sample covariance matrix. There  is a covariance function with parameters we tune using the loglikelihood maximization. The prediction (posterior mean) for a new point  has the form:\\hat{y}(\\mathbf{x}) = \\mathbf{k} K^{-1} \\mathbf{y}, there  is a vector of covariances between new point and sample points.Now note that Gaussian processes regression can model exact linear models. Suppose that covariance function has the form . In this case prediction has the form:\\hat{y}(\\mathbf{x}) = \\mathbf{x}^T X^T (X X^T)^{-1} \\mathbf{y} = \\mathbf{x}^T (X^T X)^{-1} X^T \\mathbf{y}.The identity is true in case  is nonsingular which is not the case, but this is not a problem in case we use covariance matrix regularization. So, the rightest hand side is the exact formula for linear regression, and we can do linear regression with Gaussian processes using proper covariance function.Now let's consider a Gaussian processes regression with another covariance function (for example, squared exponential covariance function of the form , there  is a matrix of hyperparameters we tune). Obviously, in this case posterior mean is not a linear function (see image).. So, the benefit is that we can model nonlinear functions using a proper covariance function (we can select a state-of-the-art one, in most cases squared exponential covariance function is a rather good choice). The source of nonlinearity is not the trend component you mentioned, but the covariance function.","Display_name":"Alexey","Creater_id":11984,"Start_date":"2013-01-03 15:28:57","Question_id":46738}
{"_id":{"$oid":"5837a574a05283111e4d32e8"},"Last_activity":"2016-08-25 11:26:36","Creator_reputation":536,"Question_score":0,"Answer_content":"Your sampling objective would be to have more samples from the minority class (which is 2% of population), to match the majority class to train a better classifier.For this, you should sample with replacement form the minority class, which means you could end up having many copies of the same minority class sample in the training set.For the majority class, you can sample without replacement since you have many records available to use.For highly imbalanced classes, under-sampling the majority class and over-sampling the minority class works best. But its advisable to try out different sample sizes when training your classifier and find out what works best for your dataset.Edit: You could first split the training set into majority class and minority class, sample from them separately and then re-combine them to get the final training set. If you're not comfortable doing this by writing your own code in R, you could use the downSample and upSample functions of the caret package.","Display_name":"Sandeep S. Sandhu","Creater_id":55831,"Start_date":"2016-08-24 01:55:43","Question_id":231422}
{"_id":{"$oid":"5837a574a05283111e4d32f7"},"Last_activity":"2016-08-25 10:21:02","Creator_reputation":3559,"Question_score":0,"Answer_content":"There are several ways of combining -values and some of them have this property and some do not. This is partly because the problem is not well specified. There has been an extensive simulation study of many of the most-well known methods. The bottom line is that if you want the property of cancellation you can have it but you do not have to.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-25 10:21:02","Question_id":31070}
{"_id":{"$oid":"5837a574a05283111e4d32f8"},"Last_activity":"2012-06-25 16:10:57","Creator_reputation":25887,"Question_score":2,"Answer_content":"The Fisher combination test is intended to combine information from separate tests done on independent data sets in order to obtain power when the individual tests may not have sufficient power.  The idea is that if the  null hypotheses are all correct the -value will be uniformly distributed on  indpendently of each other. This means that  will be  with  degrees of freedom.  Rejecting this combined null hpyothesis leads to the conclusion that at least one of the null hypotheses is false.  That is what you are doing when you apply this procedure.","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-06-25 16:08:16","Question_id":31070}
{"_id":{"$oid":"5837a574a05283111e4d3307"},"Last_activity":"2016-08-25 10:14:29","Creator_reputation":1,"Question_score":0,"Answer_content":"In particular, if , then\\{\\omega\\in \\Omega: X(\\omega) +Y(\\omega) \\leq L\\}\\subset \\{\\omega\\in \\Omega: X(\\omega) +Y(\\omega) + Z(\\omega) \\leq L\\}.Consequently . There is a more complex condition under which the inequallity is still true, but you need to analyze conditions on  such that  (law of total probability).","Display_name":"RIPO","Creater_id":128972,"Start_date":"2016-08-25 10:09:20","Question_id":231728}
{"_id":{"$oid":"5837a574a05283111e4d3314"},"Last_activity":"2016-08-25 09:56:53","Creator_reputation":41,"Question_score":0,"Answer_content":"The best solution I found so far is to use the bin values as my outcome variable in a Linear Mixed Model with the name (1 to n) of the bins as explanatory variable and ID as fixed factor. And then check for differences between the bins with post-hoc tests.If anyone has a better approach I am still open to suggestion.","Display_name":"have fun","Creater_id":70361,"Start_date":"2016-08-25 09:56:53","Question_id":231088}
{"_id":{"$oid":"5837a574a05283111e4d3323"},"Last_activity":"2016-08-25 09:45:28","Creator_reputation":1117,"Question_score":2,"Answer_content":"A triple difference-in-difference is the correct specification for this problem. I'll present a conceptual explanation and then a mathematical one.Conceptually, the standard (double) difference-in-difference can also be thought of as estimating heterogeneous treatment effect. In this perspective, time is the \"treatment\", and we want to estimate how time affects the outcome differentially across two groups. (Of course, time itself doesn't cause anything. It's just a stand-in for the real treatment that happens between the two time periods).Thus, we can extend the standard D-in-D into triple D-in-D if we want to add another layer of heterogeneous treatment effect (i.e. the heterogeneity across big firms vs small firms in your cases).Mathematically, the specification would be as follows:\\begin{equation}Y = \\alpha + \\beta_1 T + \\beta_2 G + \\beta_3 B + \\gamma_1 TG + \\gamma_2 GB + \\gamma_3 TB + \\delta_1 TGB\\end{equation}with\\begin{align}T \u0026amp;= \\text{treatment time} \\\\G \u0026amp;= \\text{treatment group} \\\\B \u0026amp;= \\text{big firms}\\end{align}The DD estimate for treatment effect in small firms is  (exactly the same as the standard DD)The DD estimate for treatment effect in big firms is Thus the treatment effect for big and small firms differs by , which is also the coefficient of the triple interaction term, or the DDD estimate.","Display_name":"Heisenberg","Creater_id":20148,"Start_date":"2016-08-25 09:45:28","Question_id":183302}
{"_id":{"$oid":"5837a574a05283111e4d3324"},"Last_activity":"2016-08-25 08:21:25","Creator_reputation":11,"Question_score":1,"Answer_content":"I think this (exploring the heterogeneous treatment effects of DD for different groups) could be easily confused with the DDD method. However, they share the same specification, I'd just run the following:where   is what you want.","Display_name":"dryang","Creater_id":128837,"Start_date":"2016-08-24 13:47:58","Question_id":183302}
{"_id":{"$oid":"5837a574a05283111e4d3333"},"Last_activity":"2016-08-25 09:37:45","Creator_reputation":17344,"Question_score":1,"Answer_content":"Hint: the likelihood ratio test is not an exact test. The LRT is based upon the asymptotic distribution of the likelihood ratio statistic, i.e. . With  and  arising from likelihoods in MLE parameters for  and  dimensional supports respectively.An exact test is based upon the actual distribution of the sufficient statistic. What is the sufficient stat for an exponential distribution (how do you show that?) and what distribution does it take (under the null)?","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-25 09:37:45","Question_id":231708}
{"_id":{"$oid":"5837a574a05283111e4d3340"},"Last_activity":"2016-02-16 05:54:21","Creator_reputation":106,"Question_score":3,"Answer_content":"Although coldmoon's answer is perfectly valid, I would like to propose a slightly different approach:General gradient expressionFirst, let's simply restrict our model to be of the form  with  a positive parametric function and  a partition function (=normalizing coefficient).Main approach (where the likelihood comes from)The objective is to learn the underlying data distribution. In practice, we minimize the distance between the model and data distributions according to the Kullback-Leibler divergence. Let  be the space in which samples lie (eg:  for binary images),  a set of N trainingsamples: D_{KL}(P_{data}, P_{model}) = \\int_{x \\in \\chi} P_{data}(x) log\\left(\\frac{P_{data}(x)}{P_{model}(x)}\\right) dx Since the underlying distribution ruling the dataset is unknown, is only known through the samples and cannot be summedover . However, this expression is also the expectation of, so we approximateit by an empirical expectation over :\\begin{align} D_{KL} (P_{data}, P_{model}) \u0026amp;= \\frac{1}{N} \\sum_{x \\in {\\bf X}} \\log\\left(\\frac{P_{data}(x)}{P_{model}(x)}\\right) \\\\ \u0026amp;= \\sum_{x \\in {\\bf X}} \\frac{1}{N} \\log (\\frac{1}{N}) - \\sum_{x \\in {\\bf X}} \\frac{1}{N} \\log(P_{model} (x)) \\quad (\\text{iid samples} \\rightarrow P_{data}(x) = \\frac{1}{N}) \\\\ \u0026amp;= -\\log(N) - \\frac{1}{N} \\sum_{x \\in {\\bf X}} \\log\\left(\\frac{f(x, \\Theta)}{Z(f,\\Theta)}\\right) \\\\ \u0026amp;= -\\log(N) + \\log(Z(f, \\Theta)) - \\frac{1}{N} \\sum_{x \\in {\\bf X}} \\log(f(x; \\Theta)) \\end{align} Gradient for the fitting processWe want to minimize:D_{KL} (P_{data}, P_{model}) = -\\log(N) + \\log(Z(f, \\Theta)) - \\frac{1}{N} \\sum_{x \\in {\\bf X}} \\log(f(x; \\Theta))The derivative of the sum with respect to the model parameters identifies as an empirical expectation over the training dataset:    \\frac{1}{N} \\sum_{x \\in {\\bf X}} \\frac{\\partial \\log(f(x; \\Theta))}{\\partial \\Theta}    = \\left\u0026lt; \\frac{\\partial \\log(f(x; \\Theta))}{\\partial \\Theta} \\right\u0026gt;_{x \\in {\\bf X} }Rewriting the first term's derivative brings another expectation term:\\begin{align}     \\frac{\\partial \\log(Z(f; \\Theta))}{\\partial \\Theta}       \u0026amp; = \\frac{1}{Z(f, \\Theta)}          \\frac{\\partial Z(f, \\Theta)}{\\partial \\Theta}  \\\\     \u0026amp; = \\frac{1}{Z(f, \\Theta)}          \\frac{\\partial }{\\partial \\Theta} \\int_{x \\in \\chi} f(x,\\Theta) dx  \\\\     \u0026amp; = \\int_{x \\in \\chi} \\frac{1}{Z(f, \\Theta)}                            \\frac{\\partial f(x,\\Theta)}{\\partial \\Theta} dx  \\\\\\end{align}Since , it comes that:    \\frac{\\partial \\log(Z(X; \\Theta))}{\\partial \\Theta}     = \\int_{x \\in \\chi} p(x; \\Theta)                         \\frac{\\partial \\log(f(x;\\Theta))}{\\partial \\Theta} dxwhich is a formal expectation on the underlying model distribution usuallywritten as:    \\left\u0026lt;         \\frac{\\partial \\log(f(x; \\Theta))}{\\partial \\Theta}     \\right\u0026gt;_{x \\sim p(x;\\Theta)}Hence the whole gradient expression:    \\Delta\\Theta = \\left\u0026lt;             \\frac{\\partial \\log(f(x; \\Theta))}{\\partial \\Theta}          \\right\u0026gt;_{x \\sim p(x;\\Theta)}         - \\left\u0026lt;             \\frac{\\partial \\log(f(x; \\Theta))}{\\partial \\Theta}         \\right\u0026gt;_{x \\in {\\bf X} }Application to RBM with binary unitsSince the objective is to fit the distribution of the visible units from the RBM, the model distribution  is given by:p({\\bf v}) = \\sum_{{\\bf \\tilde{h}} \\in \\chi_h} p({\\bf v},{\\bf \\tilde{h}})= \\frac{\\sum_{{\\bf \\tilde{h}} \\in \\chi_h} f({\\bf v},{\\bf \\tilde{h}},W)}{Z(f, \\Theta)}By definition, . Using the general expression of the gradient above, we now need tocompute:\\begin{align}    \\frac{\\partial \\log \\left(                                \\sum_{{\\bf \\tilde{h}} \\in \\chi_h}                                     f({\\bf v},{\\bf \\tilde{h}},W)                        \\right)}         {\\partial w_{ij}}    \u0026amp;= \\frac{1}{ \\sum_{{\\bf \\tilde{h}} \\in \\chi_h} f({\\bf v},{\\bf \\tilde{h}},W) }        \\sum_{{\\bf \\tilde{h}} \\in \\chi_h}             \\frac{\\partial f({\\bf v},{\\bf \\tilde{h}}, W)}            {\\partial w_{ij}}\\end{align}for the sake of readability, I now abbreviate the sum indexes:\\begin{align}    \u0026amp;= \\frac{1}{ Z \\times p({\\bf v}) }        \\sum_{{\\bf \\tilde{h}}}            \\frac{\\partial}{\\partial w_{ij}}            exp(\\sum_{(i,j) \\in V} v_i w_{ij} \\tilde{h}_j)\\\\    \u0026amp;= \\frac{1}{ Z \\times p({\\bf v}) }        \\sum_{{\\bf \\tilde{h}}}  f({\\bf v},{\\bf \\tilde{h}},W) v_i \\tilde{h}_j      = \\frac{v_i}{p({\\bf v})}     \\sum_{{\\bf \\tilde{h}}}  p({\\bf v}, {\\bf \\tilde{h}}) \\tilde{h}_j\\\\    \u0026amp;= \\frac{v_i}{p({\\bf v})}     \\sum_{{\\bf \\tilde{h}}} \\tilde{h}_j p({\\bf v}, \\tilde{h}_j) p({\\bf \\tilde{h}}^{-j} | {\\bf v}, \\tilde{h}_j)\\end{align}Using a proper decomposition of the sum, it is possible to factor and cancel the states of hidden units other than , which I designate by :\\begin{align}    \u0026amp;= \\frac{v_i}{p({\\bf v})}     \\sum_{\\tilde{h_j}} \\tilde{h}_jp({\\bf v}, h_j)    \\sum_{{\\bf \\tilde{h}}^{-j}}  p({\\bf \\tilde{h}}^{-j} | {\\bf v}, \\tilde{h}_j) \\\\    \u0026amp;= \\frac{v_i}{p({\\bf v})}     \\sum_{\\tilde{h_j}} \\tilde{h}_jp(\\tilde{h}_j| {\\bf v}) p({\\bf v})     = \\sum_{\\tilde{h_j}} v_i \\tilde{h}_j p(\\tilde{h}_j| {\\bf v}) \\\\    \u0026amp;= v_i E_{{\\bf v}}[h_j]\\end{align}Consequently, the model expectation is given by (hang on :-) ):\\begin{align}\\left\u0026lt; v_i E_{{\\bf v}}\\left[h_j\\right]) \\right\u0026gt;_{{\\bf v} \\sim p({\\bf v})}\u0026amp;= \\sum_{{\\bf v}} p({\\bf v}) \\sum_{\\tilde{h_j}} v_i \\tilde{h}_j p(\\tilde{h}_j| {\\bf v}) \\\\\u0026amp;= \\sum_{{\\bf v}} \\sum_{\\tilde{h_j}} v_i \\tilde{h}_j p(\\tilde{h}_j, {\\bf v}) \\\\\u0026amp;= \\sum_{v_i} \\sum_{\\tilde{h_j}} v_i \\tilde{h}_j \\sum_{{\\bf v}^{-i}}p(\\tilde{h}_j, v_i, {\\bf v^{-i}}) \\\\\u0026amp;= \\left\u0026lt; v_i h_j \\right\u0026gt;_{v_i, h_j \\sim p({\\bf v}, {\\bf h})}\\end{align}And finally, the whole gradient comes as:\\Delta W_{ij} = \\left\u0026lt; v_i h_j \\right\u0026gt;_{v_i, h_j \\sim p({\\bf v}, {\\bf h})}- \\left\u0026lt; v_i E_{{\\bf v}}\\left[h_j\\right] \\right\u0026gt;_{{\\bf v} \\in X}\\qquad \\forall (i,j) \\in V","Display_name":"pixelou","Creater_id":100624,"Start_date":"2016-02-16 04:17:01","Question_id":113395}
{"_id":{"$oid":"5837a574a05283111e4d3341"},"Last_activity":"2015-04-23 20:53:35","Creator_reputation":31,"Question_score":3,"Answer_content":"S represents all of samples you collect. Suppose that we consider only one sample and the red \"v\" represents the sample:","Display_name":"coldmoon","Creater_id":59742,"Start_date":"2015-04-23 20:53:35","Question_id":113395}
{"_id":{"$oid":"5837a574a05283111e4d3342"},"Last_activity":"2015-01-19 14:49:07","Creator_reputation":13,"Question_score":1,"Answer_content":"I have the same problem. Refer to this book page 567 and problem 11.8Simon Haykin. 1998. Neural Networks: A Comprehensive Foundation (2nd ed.). Prentice Hall PTR, Upper Saddle River, NJ, USA.","Display_name":"iBM","Creater_id":48991,"Start_date":"2015-01-19 14:49:07","Question_id":113395}
{"_id":{"$oid":"5837a574a05283111e4d3351"},"Last_activity":"2016-08-25 09:12:32","Creator_reputation":147175,"Question_score":5,"Answer_content":"Let the columns of  be , the corresponding entries of  be , the columns of  be , and the error columns be .Notice thate_i = Y_i - a_i X_i.Each parameter  is involved in only one of these expressions.  Therefore, the sum of squares of the , equal to , can be minimized by separately and independently finding  that minimize the squared norms .  That's a set of  (univariate) regression-through-the-origin problems.  With no constraints on the , the solutions would be\\hat a_i = \\frac{Y_i^\\prime X_i}{X_i^\\prime X_i}.If any of the  lies outside the constraining interval , the convexity of the objective function shows you only need to examine its values on the boundary . A simple approach is an exhaustive search of both points: that is, compare the values of  and , choosing  when the former is smaller and  otherwise.Here are two examples with  rows and  columns.  They were generated by creating the  and  matrices randomly and adding random errors to them to obtain .  Provided the entries in  are all in the range , the estimated values should be close to the original ones (depending on how large the random errors are).  The left plot in each example is a dotplot of the estimate  and the original parameter value , enabling visual comparison of the estimates to the parameters.  The right plot in each example is a scatterplot of the residuals of this fit (the ) against the original errors.  When the constraints are not applied (as in the bottom row), this scatterplot should be tightly focused on the line of equality.  When constraints are applied (the top row), there will be more scatter (contributed by the corresponding columns).The R code to produce this figure will let you experiment with arbitrary values of  and . The estimate of  consists of four lines exactly paralleling the analysis: computation of the regression coefficient, of the two values at the boundary, and the comparisons needed to select the best one.  It is fast and parallelizable--apart from the plotting step, it will run in seconds even when  is in the millions ().m \u0026lt;- 100n \u0026lt;- 5par(mfrow=c(2,2))for (i in c(23, 19)) {  #  # Generate data.  #  set.seed(i)  x \u0026lt;- matrix(rnorm(m*n), m)  alpha \u0026lt;- rnorm(n, 1/2, 1/2)  eps \u0026lt;- matrix(rnorm(m*n, 0, 1/4), m)  y \u0026lt;- t(t(x) * alpha) + eps  #  # Compute A.  #  a \u0026lt;- colSums(x*y) / colSums(x*x)  a.0 \u0026lt;- colSums(y*y)  a.1 \u0026lt;- colSums((y-x)*(y-x))  a \u0026lt;- ifelse(0 \u0026lt;= a \u0026amp; a \u0026lt;= 1, a, ifelse(a.0 \u0026lt;= a.1, 0, 1))  #  # Plot results.  #  e \u0026lt;- y - x %*% diag(a)  u \u0026lt;- rbind(Parameter=alpha, Estimate=a)  dotchart(u, col=ifelse(abs(u-1/2)\u0026gt;1/2, \"Red\", \"Blue\"), cex=0.6, pch=20,            xlab=\"Parameter value\")  plot(as.vector(eps), as.vector(e), asp=1, col=\"#00000040\",        xlab=\"Error\", ylab=\"Residual\")}","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-25 06:23:51","Question_id":231657}
{"_id":{"$oid":"5837a574a05283111e4d3352"},"Last_activity":"2016-08-25 06:51:07","Creator_reputation":5787,"Question_score":3,"Answer_content":"Here is a more automated way of solving all at once than that provided by @whuber. But I agree with all his insights, and his approach could be used instead. This  way of doing things essentially stacks several independent (i.e., separable) problems into one, and automatically handles the bound constraints.I would solve the following problem:Minimize the Frobenius norm of , subject to the constraint that A is diagonal, with entries in the range of  to .The Frobenius norm squared is the sum of the squared entries of the matrix .  We can as well minimize the Frobenius norm, due to square root being strictly monotonically increasing, thereby producing the same optimal solution A.Here is what it would look like under CVX under MATLAB. This will find the globally optimal solution to the problem I specified. You could do something similar under another computing environment.Let's make up some sample problem data:AA = diag([0.2 0.9 0.8])% Randomly generate some 5 by 3 X as matrix of independent N(0,1)X = rand(5,3);% Generate a sample Y, by adding matrix of Normal error with standard deviation 0.1Y = X * AA + 0.1 * randn(5,3);% Now here's the code to solve itcvx_beginvariable A(3,3) diagonalminimize(norm(Y-X*A,'fro'))0 \u0026lt;= diag(A) \u0026lt;= 1cvx_endHere's the solution:0.2637         0         0     0    0.8482         0     0         0    0.7914Now use AA = diag([0.2 0.9 1.1]), so the \"true\" solution would violate the bounds. I will reuse the same error draws as previously, and generate the new Y corresponding to this new AA. The solution this time is 0.2637         0         0     0    0.8482         0     0         0    1.0000","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-25 06:51:07","Question_id":231657}
{"_id":{"$oid":"5837a574a05283111e4d335f"},"Last_activity":"2016-08-25 09:10:09","Creator_reputation":12897,"Question_score":3,"Answer_content":"If you assume the underlying time series  is not autocorrelated (which is a reasonable assumption for daily financial returns), then\\text{Var}( x_{t+1} + \\dotsc + x_{t+K} ) = \\text{Var}(x_{t+1}) + \\dotsc + \\text{Var}(x_{t+K})and you can substitute forecasts for the theoretical quantities:\\widehat{\\text{Var}}( x_{t+1} + \\dotsc + x_{t+K} ) = \\widehat{\\text{Var}}(x_{t+1}) + \\dotsc + \\widehat{\\text{Var}}(x_{t+K}).You have your daily foreasts  with  around 22 for working days or 30 for calender days; this allows you to obtain the monthly forecast .Meanwhile, in presence of autocorrelation you would have\\text{Var}( x_{t+1} + \\dotsc + x_{t+K} ) = \\text{Var}(x_{t+1}) + \\dotsc + \\text{Var}(x_{t+K}) + \\sum_{i=1}^K \\sum_{j=1}^K \\text{Cov}(x_{t+i},x_{t+j})and a corresponding expression for forecasts in places of theoretical quantities.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-25 09:10:09","Question_id":231704}
{"_id":{"$oid":"5837a574a05283111e4d336c"},"Last_activity":"2016-08-25 08:41:24","Creator_reputation":755,"Question_score":1,"Answer_content":"The bivariate Poisson does not accommodate negative correlation between  and .  A model for this could be constructed by applying the Poisson quantile function to each component of a Gaussian copula.  The resulting bivariate probability mass function is easily computed in R with following code where the vector lambda contains the parameters of the two marginal Poisson distributions and rho is the correlation of the standard binormal distribution.library(mvtnorm)dbipoisgausscopula \u0026lt;- function(x, lambda, rho) {   pmvnorm(lower=qnorm(ppois(x-1,lambda)),      upper=qnorm(ppois(x,lambda)),      mean=c(0,0),      sigma=matrix(c(1,rho,rho,1),2,2)   )}","Display_name":"Jarle Tufto","Creater_id":77222,"Start_date":"2016-08-25 08:41:24","Question_id":229881}
{"_id":{"$oid":"5837a574a05283111e4d336d"},"Last_activity":"2016-08-22 15:42:19","Creator_reputation":25170,"Question_score":1,"Answer_content":"You can use bivariate Poisson distribution with probability mass functionf(x,y) = \\exp\\{-(\\lambda_1+\\lambda_2+\\lambda_3)\\}\\frac{\\lambda_1^x}{x!}\\frac{\\lambda_2^y}{y!}\\sum^{\\min(x,y)}_{k=0}{x \\choose k}{y \\choose k}k!\\left(\\frac{\\lambda_3}{\\lambda_1\\lambda_2}\\right)^kwhere  and  and , so you can treat  as a measure of dependence between the two marginal Poisson distributions. The pmf and random generation for this distribution is implemented in extraDistr package if you are using R.In fact, this distribution was described in terms of analyzing sports data by Karlis and Ntzoufras (2003), so you can check their paper for further details. Those authors in their earlier paper discussed also the univariate Poisson model, where they concluded that independence assumption provides fair approximation since the difference between scores of both teams does not depend on the correlation parameter of bivariate Poisson (Karlis and Ntzoufras, 2000).Kawamura (1984) described estimating parameters for bivariate Poisson distribution by direct search using maximum likelihood. As about regression models, you can use EM algorithm for maximum likelihood estimation, as Karlis and Ntzoufras (2003), or Bayesian model estimated using MCMC. The EM algorithm for bivariate Poisson regression is implemented in bivpois package (Karlis and Ntzoufras, 2005) that is unfortunately out of CRAN at this moment.Karlis, D., \u0026amp; Ntzoufras, I. (2003). Analysis of sports data by using bivariate Poisson models. Journal of the Royal Statistical Society: Series D (The Statistician), 52(3), 381-393.Karlis, D. and Ntzoufras, I. (2000) On modelling soccer data.Student, 3, 229-244.Kawamura, K. (1984). Direct calculation of maximum likelihood estimator for the bivariate Poisson distribution. Kodai mathematical journal, 7(2), 211-221.Karlis, D., and Ntzoufras, I. (2005). Bivariate Poisson and diagonal inflated bivariate Poisson regression models in R. Journal of Statistical Software, 14(10), 1-36.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-22 07:11:59","Question_id":229881}
{"_id":{"$oid":"5837a574a05283111e4d337e"},"Last_activity":"2016-02-18 01:49:05","Creator_reputation":25669,"Question_score":2,"Answer_content":"This is a very classical combinatorics problem beautifully treated by William Feller (1970, Theory of Probability) and since this sounds like an homework or assignment, I can only point you to the Wikipedia page on the topic to help you solve the question by yourself.","Display_name":"Xi\u0026#39;an","Creater_id":7224,"Start_date":"2016-02-18 01:49:05","Question_id":197181}
{"_id":{"$oid":"5837a574a05283111e4d338b"},"Last_activity":"2016-08-25 08:18:53","Creator_reputation":3559,"Question_score":0,"Answer_content":"There is a version of kappa for the case of multiple raters which is usually attributed to Conger. A quick search with your favourite statistical package should find how to compute it. This is likely to be more productive than trying to average values of pair-wise kappa and then compute a variance for the average.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-25 08:18:53","Question_id":201025}
{"_id":{"$oid":"5837a574a05283111e4d3398"},"Last_activity":"2016-08-25 08:06:17","Creator_reputation":8337,"Question_score":1,"Answer_content":"This seems to be equivalent to the question of how to convert a number to a binary numeral. So just use your favorite method for that (perhaps exploiting the fact that the number is probably already internally represented in binary) and interpret each bit as an element of the vector.For more than two classes, increase the radix appropriately (e.g., for 4 classes, convert your numbers to base-4 numerals).For the factorial case, where you want to sample from the  arrangements of  objects, you might be best served with a Fisher-Yates shuffle.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-24 22:55:11","Question_id":231607}
{"_id":{"$oid":"5837a574a05283111e4d33a7"},"Last_activity":"2016-08-25 07:55:26","Creator_reputation":90,"Question_score":1,"Answer_content":"Are you sure you even need machine-learning process for that kind of task? I suggest that a classic text analyzer catching all the URL containing \"github\" or \"gitlab\" could be just fine for what you want to do. Maybe setting up a list of strings that if present in the paper indicate you that it's a tool.Your dataset is way too small to train a classification model. I'm even surprised your reaching 75% of accuracy. I don't know if you have the possibility to get more labelled data but 1000-2000 entries would be a good start to effectively train your model.","Display_name":"LoulouChameau","Creater_id":127872,"Start_date":"2016-08-25 06:35:31","Question_id":231561}
{"_id":{"$oid":"5837a574a05283111e4d33b6"},"Last_activity":"2016-08-25 07:35:07","Creator_reputation":41,"Question_score":4,"Answer_content":"In FE models of the type y_{it} = \\alpha_i + \\beta X_{it} + u_{it} is the incidental parameter, because theoretically speaking, it is of a secondary importance. Usually,  is the important parameter, statistically speaking. But in essence,  is important because it provides useful information on the individual intercept.Most of the panels are short, i.e., T is relatively small.In order to illustrate the incidental parameter problem I will disregard  for simplicity. So the model is now:y_{it} = \\alpha_i + u_{it} \\quad \\quad u_{it}\\sim iiN(0,\\sigma^2)So by using deviations from means method we have  - and that's how we can get .Lets have a look on the estimate for :\\hat{\\sigma}^2 = \\frac{1}{NT}\\sum_i\\sum_t (y_{it}-\\bar{y}_i)^2 = \\sigma^2\\frac{\\chi_{N(T-1)}^2}{NT} = \\sigma^2\\frac{N(T-1)}{NT} = \\sigma^2\\frac{T-1}{T}You can see that if T is \"large\" then the term  disappears, BUT, if T is small (which is the case in most of the panels) then the estimate of  will be inconsistent. This makes the FE estimator to be inconsistent.The reason  is usually consistent because usually N is indeed sufficiently large and therefore has the desired asymptotic requirements.Note that in spatial panels for example, the situation is opposite - T is usually considered large enough, but N is fixed. So the asymptotics comes from T. Therefore in spatial panels you need a large T!Hope it helps somehow.","Display_name":"Corel","Creater_id":128939,"Start_date":"2016-08-25 07:35:07","Question_id":185998}
{"_id":{"$oid":"5837a574a05283111e4d33c2"},"Last_activity":"2016-08-25 07:16:27","Creator_reputation":56,"Question_score":4,"Answer_content":"There is no unique joint distribution. In fact, there are infinite possibilities to construct the joint distribution. For instance, there exist infinitely many copula functions that can be used to construct a joint distributions with such marginals.","Display_name":"Ford","Creater_id":128944,"Start_date":"2016-08-25 07:16:27","Question_id":231688}
{"_id":{"$oid":"5837a574a05283111e4d33cf"},"Last_activity":"2012-04-26 23:17:48","Creator_reputation":27863,"Question_score":7,"Answer_content":"The difference between significant and non-significant is not necessarily significant. In general, you shouldn't look at the p-value to decide which test to use (for article length treatment see Gelman and Stern (2006)).If you are concerned with developing a predictive model, then regression would be suitable and correlation would not be. If you are concerned with summarising bivariate association, then some form of correlation coefficient would be suitable.The p-value of a linear regression with a single predictor will be the same as the p-value for Pearson's correlation. Given that you have only one predictor, then this applies to you. Thus, your question could be rephrased in terms of whether to use Pearson or Spearman's correlation coefficient. This has been discussed elsewhere on this site such as here.For typical Likert scales there is a limit on the degree to which extreme outliers can occur. As such Pearson and Spearman often give similar results. That said, a little movement in p-values is not surprising, and if this just happens to cross the magical .05 line then it may appear more substantial than it really is.","Display_name":"Jeromy Anglim","Creater_id":183,"Start_date":"2012-04-26 23:17:48","Question_id":27205}
{"_id":{"$oid":"5837a574a05283111e4d33dc"},"Last_activity":"2016-08-25 07:15:59","Creator_reputation":518,"Question_score":0,"Answer_content":"So we know that  where  by conditional expectation function decomposition. Basicaly if we have some outcome , we can split it into the expectation and an error term that conditional on X has zero mean. Now suppose we think there is a linear relationship between Y and X:Then from the above we have:  and we say that the conditional expectation function (the LHS) is a linear predictor in this case. This is a model that predicts where the expected value is. But we never actually observe that so we create estimates and get sample averages using data.","Display_name":"VCG","Creater_id":124896,"Start_date":"2016-08-25 07:15:59","Question_id":231686}
{"_id":{"$oid":"5837a574a05283111e4d33e9"},"Last_activity":"2016-08-25 06:51:31","Creator_reputation":3559,"Question_score":2,"Answer_content":"If this is primarily a linguistic question 'What do I call it?' then I think you use the highest term. So if when you plot it the appearance is almost straight but with a slight curve it is still quadratic.Some of the other issues about inclusion of terms of various orders have been dealt with extensively on this site, for instance here","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-25 06:51:31","Question_id":231649}
{"_id":{"$oid":"5837a574a05283111e4d33ea"},"Last_activity":"2016-08-25 04:34:02","Creator_reputation":6,"Question_score":0,"Answer_content":"From my personal experience, I would choose one or another depending on two things. First, depending on the application, the error between the regression line and the data that can be accepted may be different. If choosing just linear regression meets your error requirements, why not keeping it simple. It is usually a tradeoff between the precision of fit and the robustness (the ability to applicate it to other sets of same kind of data).Second, sometimes we have some a priori knowledge on the data. For example, we know that the relationship between y and x is expected to be linear, or that it might be strongly non linear, etc. This may help also to choose between the regression models.","Display_name":"bmr","Creater_id":124132,"Start_date":"2016-08-25 04:34:02","Question_id":231649}
{"_id":{"$oid":"5837a574a05283111e4d33f7"},"Last_activity":"2012-07-05 12:33:01","Creator_reputation":1,"Question_score":10,"Answer_content":"These are called Electronic Lab Notebooks (ELN).Here are some of the open source options I've looked at:The Sage Notebook.The new IPython Notebook, which can now be run as a webapp on EC2 and Azure.Leo, which can be used with IPython and in many other ways.Various wiki, blogging, and CMS solutions.","Display_name":"Ed Hagen","Creater_id":6773,"Start_date":"2011-10-12 04:05:53","Question_id":16889}
{"_id":{"$oid":"5837a574a05283111e4d33f8"},"Last_activity":"2011-10-12 11:08:12","Creator_reputation":2304,"Question_score":10,"Answer_content":"My favorite: Evernote. You can tag entries (e.g., 'analysis', 'idea', etc.), you can paste pictures and graphics, and you can share notebooks with collaborators. And: it's basically free (well, freemium). But the free edition is absolutely sufficient for me.","Display_name":"Felix S","Creater_id":6082,"Start_date":"2011-10-12 00:29:37","Question_id":16889}
{"_id":{"$oid":"5837a574a05283111e4d33f9"},"Last_activity":"2011-10-12 10:52:36","Creator_reputation":932,"Question_score":0,"Answer_content":"How about a Boogie Board? You can write your notes on a slate and record them as 's the same idea as the LiveScribe pen but you can save them as PDF files...It isn't out yet, but will be in less than a month. ","Display_name":"Berk U.","Creater_id":3572,"Start_date":"2011-10-12 10:52:36","Question_id":16889}
{"_id":{"$oid":"5837a574a05283111e4d33fa"},"Last_activity":"2011-10-12 07:36:06","Creator_reputation":106,"Question_score":2,"Answer_content":"Personally I have found the Livescribe 'smartpen' a God send.it merges the trusty 'old-world charm' of a traditional pen and paper notebook but inlcudes the ability to record sound (which it synchronises with your pen strokes) ready for later revision. NB- there is a downside and that is you have to buy special paper that works with the pen.....swings and round-a-bouts reallyThe audio/pen stokes an be uploaded onto the web and then attached to many of the other programs already higlihted above.Students I teach (biomechanics) absolutely love this and find later studying of difficult concepts much easier than before (pre livescribe)","Display_name":"Andrew V","Creater_id":431,"Start_date":"2011-10-12 07:02:39","Question_id":16889}
{"_id":{"$oid":"5837a574a05283111e4d33fb"},"Last_activity":"2011-10-12 07:29:43","Creator_reputation":8259,"Question_score":1,"Answer_content":"Check out this article, Beautifying Data in the Real World, from Nature Precedings for some ideas.","Display_name":"Charlie","Creater_id":401,"Start_date":"2011-10-12 07:29:43","Question_id":16889}
{"_id":{"$oid":"5837a574a05283111e4d33fc"},"Last_activity":"2011-10-12 05:52:12","Creator_reputation":12260,"Question_score":1,"Answer_content":"You might want to check out the latest Zotero beta, which is now standalone and doesn't require Firefox.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2011-10-12 05:52:12","Question_id":16889}
{"_id":{"$oid":"5837a574a05283111e4d33fd"},"Last_activity":"2011-10-12 00:39:15","Creator_reputation":24961,"Question_score":2,"Answer_content":"I found Xmind useful. You can attach anything, and tree structure is really useful for organizing. I especially like the feature where you can drill down into the node (topic). There are more similar software products which exploit the same concept.","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2011-10-12 00:39:15","Question_id":16889}
{"_id":{"$oid":"5837a574a05283111e4d33fe"},"Last_activity":"2011-10-11 23:48:01","Creator_reputation":1035,"Question_score":2,"Answer_content":"I've never used it personally, but Microsoft has a piece of software in the Office suite called OneNote that accomplishes a similar goal to your e-lab notebook specifications. Refer to their website for more information. They also offer a free trial bundled with MS Office here.","Display_name":"Christopher Aden","Creater_id":1118,"Start_date":"2011-10-11 23:48:01","Question_id":16889}
{"_id":{"$oid":"5837a574a05283111e4d340d"},"Last_activity":"2016-08-25 06:06:45","Creator_reputation":38,"Question_score":0,"Answer_content":"The method is very easy: I'll rescale the likelihood which is fine because it doesn't have to integrate to 1.","Display_name":"Paula","Creater_id":126827,"Start_date":"2016-08-25 06:06:45","Question_id":231438}
{"_id":{"$oid":"5837a574a05283111e4d341a"},"Last_activity":"2010-12-05 06:19:33","Creator_reputation":37824,"Question_score":15,"Answer_content":"The threshold is chosen such that it ensures convergence of the hypergeometric distribution ( is its SD), instead of a binomial distribution (for sampling with replacement), to a normal distribution (this is the Central Limit Theorem, see e.g., The Normal Curve, the Central Limit Theorem, and Markov's and Chebychev's Inequalities for Random Variables). In other words, when  (i.e.,  is not 'too large' compared to ), the FPC can safely be ignored; it is easy to see how the correction factor evolves with varying  for a fixed : with , we have  when  while  when . When , the FPC approaches 1 and we are close to the situation of sampling with replacement (i.e., like with an infinite population).To understand this results, a good starting point is to read some online tutorials on sampling theory where sampling is done without replacement (simple random sampling). This online tutorial on Nonparametric statistics has an illustration on computing the expectation and variance for a total. You will notice that some authors use  instead of  in the denominator of the FPC; in fact, it depends on whether you work with the sample or population statistic: for the variance, it will be  instead of  if you are interested in  rather than .As for online references, I can suggest youEstimation and statistical inferenceA new look at inference for the Hypergeometric DistributionFinite Population Sampling with Application to the Hypergeometric DistributionSimple random sampling","Display_name":"chl","Creater_id":930,"Start_date":"2010-12-05 05:32:46","Question_id":5158}
{"_id":{"$oid":"5837a574a05283111e4d3427"},"Last_activity":"2016-08-25 05:49:36","Creator_reputation":158,"Question_score":2,"Answer_content":"For any regression model, we try to minimize a certain loss function say  (e.g. sum squared residuals for OLS). Note that we can see the likelihood function as a 'negative loss function', since we try to maximize this.In case of a penalized regression (LASSO, Ridge, etc.) we try to minimize L(\\beta|x) + \\lambda P(\\beta), where  is a penalizing function (e.g.  for the LASSO). The  determines the size of trade off between the loss and penalizing function.This is equivalent to maximizing (multiply by ) -L(\\beta|x) - \\lambda P(\\beta) = Likelihood - \\lambda P(\\beta)Hence the subtraction of the penalty term. EDIT: As Mr Validation pointed out in his answer, addition would favor bigger values for . To see this, change the formula to: Likelihood + \\lambda P(\\beta),with the LASSO penalizing function. If we set each  very large or even , these values will produce a better maximization, however these values for  are not realistic.","Display_name":"Marcel10","Creater_id":128538,"Start_date":"2016-08-25 05:35:56","Question_id":231664}
{"_id":{"$oid":"5837a574a05283111e4d3428"},"Last_activity":"2016-08-25 05:18:19","Creator_reputation":31,"Question_score":2,"Answer_content":"Because otherwise it would favor bigger and bigger coefficients and the estimates would blow up. You rather ask, why do we add or substract anything ;)","Display_name":"Mr Validation","Creater_id":128910,"Start_date":"2016-08-25 05:18:19","Question_id":231664}
{"_id":{"$oid":"5837a574a05283111e4d3437"},"Last_activity":"2016-08-25 05:45:30","Creator_reputation":744,"Question_score":0,"Answer_content":"\"Background model\" is used here as a synonym for the statistical term \"null model\". These terms mean a framework using probability theory to show what type of data we would expect to see if the effect/signal we are interested in were absent. This is a way to get a handle on false positives: we should use techniques that only seldom claim to make discoveries under the null (background) model.","Display_name":"eric_kernfeld","Creater_id":86176,"Start_date":"2016-08-25 05:45:30","Question_id":231248}
{"_id":{"$oid":"5837a574a05283111e4d3444"},"Last_activity":"2016-08-25 05:15:34","Creator_reputation":18701,"Question_score":11,"Answer_content":"A one-inflated Poisson model for a count  is\\begin{align}\\Pr(Y_i = 1) \u0026amp;= \\pi_i +(1-\\pi_i)\\cdot\\mu_i\\mathrm{e}^{-\\mu_i}\\\\\\Pr(Y_i = y_i) \u0026amp;= (1-\\pi_i)\\cdot\\frac{\\mu_i^{y_i}\\mathrm{e}^{-\\mu_i}}{y_i!} \\qquad \\text{when } y_i\\neq 1\\end{align}where the Poisson mean  \u0026amp; Bernoulli probability  are related to the predictors through appropriate link functions. You can define a similar model to inflate probabilities for any values you choose.Still, zero has a special (\u0026amp; once controversial) place among the counting numbers\u0026mdash;in a sense representing the absence of anything to count. And it's the \"nothing\" vs \"something\" distinction, rather than the \"one\" vs \"any other count\" distinction that tends to be relevant across a wide range of phenomena we like to model: there's one process that gives a nought, one, two, ... count \u0026amp; another that gives no count at all.","Display_name":"Scortchi","Creater_id":17230,"Start_date":"2014-03-21 04:18:24","Question_id":90817}
{"_id":{"$oid":"5837a574a05283111e4d3445"},"Last_activity":"2016-08-24 19:13:51","Creator_reputation":86,"Question_score":2,"Answer_content":"The R package VGAM has function vglm which can be used to fit all sorts of Poisson-esque models. You can use it to specify a one-inflated model, so something like vglm(Y~X,family=oipospoisson(),data=data). See here for more details. ","Display_name":"Justin","Creater_id":127487,"Start_date":"2016-08-24 17:40:52","Question_id":90817}
{"_id":{"$oid":"5837a574a05283111e4d3453"},"Last_activity":"2016-08-25 04:23:04","Creator_reputation":111,"Question_score":1,"Answer_content":"If y is the binary category (such as \"yes or no\" and \" or \"), you should domodel.add(Dense(2, activation='sigmoid')) # because your output/y has two columnsrather than model.add(Dense(1,activation='sigmoid')) # 1 neuron onlyOr you can just use only 1 neuron and keep only 1 column from your y.","Display_name":"rilut","Creater_id":89041,"Start_date":"2016-08-25 03:48:02","Question_id":231569}
{"_id":{"$oid":"5837a575a05283111e4d345e"},"Last_activity":"2016-08-25 03:41:25","Creator_reputation":25170,"Question_score":1,"Answer_content":"  1) we see that the plot of the empirical cdf and the plot of the cdf  are not exactly the same. They have similar shape but do not coincide  at all points along the abscissa. How can we then say that the  population height is distributed as the specified normal distribution?This is because ecdf computes empirical cdf, i.e. cumulative distribution function of your sample, not the true cdf of the population (that you correctly given on your plot of theoretical cdf).  2) When we calculate the population mean and standard deviation, we do  not get the true values 150 and 30. Why are then the population mean  and variance defined as they are? Is it only in an infinite population  the expression for the population mean, variance etc. give the true  values?Because you calculate sample mean and sample standard deviation. As ecdf, they are estimates of the true parameters.  3) According to the weak law of large numbers the sample mean  converges to the true mean as the number of observations approaches  infinity. How can then the expression for the population mean give the  true value of the mean?It does not. It gives you estimate of the true mean that approaches your true mean as your sample size approaches infinity.  4) Can then a population consist of a finite amount of individuals or  are finite sets of individuals just samples?Certainly yes. You can have population of all patients that visited your local dentist dr. Henry last year. This population is obviously finite. You can draw sample of such population and calculate it's mean and standard deviation that will be estimates of the true mean and true standard deviation. Obviously, if you calculated mean on the whole population it would differ from the one estimated on sample taken from it.The problem with your example is that you are confusing sample with population. In your example you have drawn sample of size 100 from the infinite population. For finite population example you should rather do something like below:pop \u0026lt;- rnorm(1000, 150, 30) # sample your population from hyper-populationmean(pop) # this is mean of your populationsd(pop)   # this is sd of your populationsam \u0026lt;- sample(pop, 100) # take sample of size 100 from your populationmean(sam) # this is mean of your samplesd(sam)   # this is sd of your sampleCheck also:What is the difference between a population and a sample?","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-25 03:34:50","Question_id":231625}
{"_id":{"$oid":"5837a575a05283111e4d346b"},"Last_activity":"2016-08-25 03:29:34","Creator_reputation":71,"Question_score":3,"Answer_content":"Confidence interval is an estimate of an interval in which mean of  observations will fall when x=xiIn its formula1/n + \\frac{(x-\\bar x)^2}{\\sum (x_i - \\bar x)^2}Tends to 0.Prediction interval is an estimate of an interval in which future individual observations will fall when x=xiIn its formula1 + 1/n + \\frac{(x-\\bar x)^2}{\\sum (x_i - \\bar x)^2}tends to 1That means that the confidence interval for the mean of the outcomes at xi gets smaller as sample size grows. (as Central limit Theorem would suggest) which means that by increase of the sample size our estimate for the average (mean) outcome for xi gets better. \\lim_{n-\u0026gt;infinity}{CI = \\hat y}But the dispersion of the distribution of y|xi \"the probability of an individual outcome\" at xi, Doesn't change very much because central limit theorem is related to central tendencies not to individual behavior or outcomes. Therefore the prediction interval doesn't change very much. \\lim_{n-\u0026gt;infinity}{PI = \\hat y \\pm t_{\\alpha/2, n-2} \\sqrt{MSE}}Individual behavior remains uncertain no matter how much you increase your sample size ;)","Display_name":"Amir","Creater_id":128742,"Start_date":"2016-08-25 03:29:34","Question_id":231626}
{"_id":{"$oid":"5837a575a05283111e4d3478"},"Last_activity":"2016-08-25 03:27:00","Creator_reputation":140,"Question_score":0,"Answer_content":"Such a small sample of 4 cannot give any good estimate of mean or standard deviation of the population, hence the Coefficient of variation will have a massive error. To remedy, I suggest to merge low-observation group with a similar one to increase the sample size.","Display_name":"Jan Sila","Creater_id":124222,"Start_date":"2016-08-25 03:27:00","Question_id":231532}
{"_id":{"$oid":"5837a575a05283111e4d3485"},"Last_activity":"2016-08-25 03:17:53","Creator_reputation":31,"Question_score":1,"Answer_content":"I think it's pretty plain evident that cross validation is statistically better for estimating parameters or checking stability. That's because it measures prediction error ideally over all data with equal weights. But that brings you to statistics of course. Because traditional prediction errors and model fit statistics do the job as well, if you have the right model. So in principle, if you believe in your model, you don't need to do prediction at all, you just fit your model by using all data and calculate prediction errors from statistical theory. That's what most scientists and scholars do, I believe.But if you believe that you don't have the right model, and you are just using a wrong model to mimic the data, then your question becomes relevant. As said, I would use cross validation to calibrate the parameters, but to demonstrate predictive ability, I would use a separate validation data set. Why? Because it's more convincing, more understanadable, and finally, because you very likely report the parameters calibrated from the whole data, not from any of you training subsets.Moreover, I would not just randomly pick the validation data, but I would choose it somehow conceptually different from the training data: For example, past - future, Americans - Europeans. I think that's the ultimate test of model validity. Traditional cross validation (and randomly picked validation data) just measure internal validity (terminology my own). Systematically picked validation data measure external validity which is generalizability. And science is about generalizing patterns.And needless to say, the prediction gains should be statistically significant. Otherwise, it's just optimistic reporting of noise.Of course this is a very rigorous take on model validation, and I believe most modelers actually fail one or two: cross validation, external validation or statistical significance. Because it's all too easy to overfit a model in small data and then go like 'whoah, what good predictions'. But I've also seen examples to the contrary: In cancer research, it was every day practice to first fit a random forest by cross validation, and then to demonstrate predictive ability on different patients. Because in that way they could make the doctors and biologists to believe in the result.This is my take on the issue as a practicing statistician some 5+ years into the working life.","Display_name":"Mr Validation","Creater_id":128910,"Start_date":"2016-08-25 03:17:53","Question_id":231171}
{"_id":{"$oid":"5837a575a05283111e4d3486"},"Last_activity":"2016-08-25 02:26:42","Creator_reputation":15549,"Question_score":1,"Answer_content":"Predictive accuracy always needs to be calculated on unseen data - whether that data is unseen via cross validation splits or via a separate data set. So often the most important point is to avoid leaks between training and test data. This may be easier to achieve with hold out (e.g. by obtaining test cases only after model training is finished) than for resampling.But careful: very often \"hold out\" or \"independent test\" are used that are in fact a single random split of the available data set. That procedure is of course prone to the same data leaks that cross validation is. Yes, for simple data, cross validation makes more efficient use of your data. And in small sample size situations, that can be the crucial advantage of resampling. But when you have to deal with multiple confounders and need to split independently for all those confounders, that advantage vanishes very fast because you end up excluding large parts of your data from both test and training set for each surrogate model. Related: Is hold-out validation a better approximation of \u0026quot;getting new data\u0026quot; than k-fold CV?Hold out vs. validation experimentUPDATE: described scenario of 100k (I assume cases) x unknown no of variates.That is certainly not a small sample size situation. In this situation, a random hold out set of 10 % = 10000 cases should have no practically relevant difference to cross validation results. The more so, as a random subset is prone to the same data leaks that cross validation is prone to as well: confounders that lead to clustering in the data. If you have such confounders, your effective sample size may be orders of magnitude below the 100k rows, and any kind of splitting that doesn't take care of those confounders will mean a data leak between training and test and lead to overoptimistic bias in the error estimates.The more efficient use of cases in cross validation is mostly relevant with small data sets where stability of the model is an issue and must be checked (which is easily done by cross validation), and uncertainty of the test result due to small numbers of test cases is largehere cross validation is better as a full run will test each case.For theory, I recommend reading up the relevant parts of The Elements of Statistical Learning.These papers have empirical results on bias and variance of different validation schemes (though they deal explicitly with small sample size situations): Kohavi, R.: A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Mellish, C. S. (ed.) Artificial Intelligence Proceedings 14 International Joint Conference, 20 -- 25. August 1995, Montréal, Québec, Canada, Morgan Kaufmann, USA, 1137 - 1145 (1995). Beleites, C.; Baumgartner, R.; Bowman, C.; Somorjai, R.; Steiner, G.; Salzer, R. \u0026amp; Sowa, M. G.: Variance reduction in estimating classification error using sparse datasets, Chemom Intell Lab Syst, 79, 91 - 100 (2005).  ","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2016-08-23 02:49:48","Question_id":231171}
{"_id":{"$oid":"5837a575a05283111e4d3497"},"Last_activity":"2016-08-25 02:49:03","Creator_reputation":158,"Question_score":0,"Answer_content":"The pdf (probability density function) of a normally distributed univariate variable  given  (mean) and  (standard deviation) is given by: .Let us focus on the part in the exponent:  and extend this to the multivariate case with matrix notation.  will become  (note that  and  are now column vectors, in your case with element  and . will become  (with elements  in your case). If we combine these elements, we get your expression for .So to answer your questions: is the term in the exponent of the pdf of the (multivariate) normal distribution. Note that a normal distribution is a Gaussian distribution.How the equation  is derived: Write this expression as vectors/matrices with the elements as I have described above and apply basic matrix algebra (substraction and matrix multiplication is all you need).","Display_name":"Marcel10","Creater_id":128538,"Start_date":"2016-08-25 02:49:03","Question_id":231629}
{"_id":{"$oid":"5837a575a05283111e4d34a4"},"Last_activity":"2016-08-25 02:47:17","Creator_reputation":104,"Question_score":1,"Answer_content":"Suppose  responses are observed in stage 1, and let  be the number of responses observed in stage 2. Then  will be rejected if . However,  (i.e. the number of patients in stage 2), which means that the inequality  must be satisfied.Letting , then the above implies , and hence  for .Taking your example with parameters  gives , and so the value of  will be the same for all values of , as you observed. ","Display_name":"David Robertson","Creater_id":128762,"Start_date":"2016-08-25 02:47:17","Question_id":104201}
{"_id":{"$oid":"5837a575a05283111e4d34b1"},"Last_activity":"2016-08-25 02:33:42","Creator_reputation":25170,"Question_score":0,"Answer_content":"I had similar experience as yours with real-life data. Boruta does not give you any guarantees, you should treat it's output rather as a \"suggestion\", then definite answer.This was even discussed by Kursa and Rudnicki (2010) in their paper about Boruta:  One should note that the Boruta is a heuristic procedure designed to  find all relevant attributes, including weakly relevant attributes.  Following Nilsson et al. (2007), we say that attribute is weakly  important when one can find a subset of attributes among which this  attribute is not redundant. The heuristic used in Boruta implies that  the attributes which are significantly correlated with the decision  variables are relevant, and the significance here means that  correlation is higher than that of the randomly generated attributes.You could try also other methods, e.g. entropy-based (check FSelectorRcpp project).Feature selection algorithms are far from perfect. Marcin Kosiński compared performance of three different methods and got three different solutions from each.(source: r-addict.com)Kursa, M.B., \u0026amp; Rudnicki, W.R. (2010). Feature selection with the Boruta package. Journal of Statistical Software, 36(11), 1-12.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-25 02:11:46","Question_id":231623}
{"_id":{"$oid":"5837a575a05283111e4d34be"},"Last_activity":"2016-08-25 02:24:36","Creator_reputation":388,"Question_score":0,"Answer_content":"If you try the followingtmp \u0026lt;- runif(1000) # generate random valestmp[sample(1:1000, size = 100, replace = FALSE)] \u0026lt;- NA # generate some NAsX \u0026lt;- matrix(tmp, ncol = 10) # make tmp a 100 x 10 matrix score \u0026lt;- apply(X, 1, function(x) sum(!is.na(x))) # get the score (number of complete covariates)y \u0026lt;- sample(c(0, 1), size = 100, replace = TRUE) # generate binary ymod \u0026lt;- glm(y ~ X + score, family = binomial(link = \"logit\")) # estimate a logistic regressionround(coef(mod), digits = 2) # gives#(Intercept)     X1     X2    X3     X4     X5     X6     X7     X8     X9     X10  score #       2.74  -1.92  -1.36  0.24   0.45  -1.22   0.75  -3.84  -0.10   0.03    0.62     NAIf you think about it is clear that the coefficient of score has to be NA: in the regression only complete observations are used; and this means that only observations with a score of 10. So score is a constant varible in the regression model without any explanation. This changes of course if you can drop (non relevant) variables (eg):glm(y ~ X[, c(1, 4:6, 9)] + score, family = binomial(link = \"logit\"))","Display_name":"Qaswed","Creater_id":112892,"Start_date":"2016-08-25 02:24:36","Question_id":230260}
{"_id":{"$oid":"5837a575a05283111e4d34cb"},"Last_activity":"2016-08-25 01:56:11","Creator_reputation":648,"Question_score":1,"Answer_content":"We have two publications in this domain, but I am not entirely convinced in the approaches. But you should have a look.In the publication below, we explored a circular layout to visualize the overlap of clusters. You could click segments to see them colored in a scatterplot (but of course scatterplots do not scale to high-dimensional data).This allows to see how clusters related to each other.The circular layout is beneficial, because the complexity of the visualization is higher on the outer circle, where we have more space. A \"linear\" display is less readable. The size of the segments is based on pairs like many of the evaluation indexes (e.g. ARI). In above image, the red and yellow clusters are one cluster in the outer area, which causes additional pairs (the bottom segment, which does not exist in the inner circle - because these are two pairs there, they do not \"pair\"). Understanding how the \"pairs\" work is pretty non-intuitive, sorry.This approach can use more than two clusterings, but usually at three clusterings it already becomes rather incomprehensible.  E. Achtert, S. Goldhofer, H.-P. Kriegel, E. Schubert, A. Zimek  Evaluation of Clusterings – Metrics and Visual Support  In Proceedings of the 28th International Conference on Data Engineering (ICDE), Washington, DC: 1285–1288, 2012. The code in ELKI is expected to work, but I have not verified this in a looong time.The second reference I can suggest is this:  E. Schubert, A. Koos, T. Emrich, A. Züfle, K. A. Schmid, A. Zimek  A Framework for Clustering Uncertain Data  Proceedings of the VLDB Endowment, 8(12): 1976–1979, 2015. While this is titled \"Clustering Uncertain Data\", it is very closely connected to alternative clustering. Because uncertain data can be clustered by looking at alternatives obtained when taking \"certain\" samples from the uncertain data, and then running a traditional clustering algorithm.The tau value shows how similar the clusterings in each cluster are. The confidence probability is an estimate how many clustering results will look similar to the representative. The numbers may not add up to 1 - in this screenshot, we expect some 7% of clusterings to be dissimilar from the examples that we show here.With uncertain data, you need to look at many samples. But then you have 50-100 (or even many more) clusterings. Thus, we cluster clusterings based on their similarity, to get some representative clusterings the user can then explore. Again, this functionality is available in ELKI, but you need to set some 30-40 parameters to reproduce these results (there is no \"standard format\" for uncertain data, many of these parameters are to specify the uncertainty that is associated with the data points).Details on representative clusterings are here:  A. Züfle, T. Emrich, K. A. Schmid, N. Mamoulis, A. Zimek, M. Renz  Representative Clustering of Uncertain Data  In Proceedings of the 20th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), New York, NY: 243–252, 2014. ","Display_name":"Erich Schubert","Creater_id":18215,"Start_date":"2016-08-25 01:56:11","Question_id":230775}
{"_id":{"$oid":"5837a575a05283111e4d34d9"},"Last_activity":"2016-08-25 00:54:03","Creator_reputation":1,"Question_score":0,"Answer_content":"Ignore zero and carry out the computation.entropyFunc =(data) -\u0026gt;    H = 0    for i of  data         #since the log function isnt defined at 0         if data[i] ? 0            H += ( (-1) * data[i] * Math.log2(data[i]) )    return H","Display_name":"ch1ru16","Creater_id":128865,"Start_date":"2016-08-24 20:47:39","Question_id":57069}
{"_id":{"$oid":"5837a575a05283111e4d34da"},"Last_activity":"2016-05-27 10:24:55","Creator_reputation":56,"Question_score":2,"Answer_content":"Ignore the zero probabilities, and carry on summation using the same equation.for each object{         double e=0.0;        for (int i=0;i\u0026lt;n;i++){            if (p[i]!=0)                e = e+ p[i]*Log(p[i],2);        }        e=-1 * e;    print e;}//the entropy for rows 1,2 and 3 is 1.52, 1.85, and 0.00 respectively ","Display_name":"Ahmad Hassanat","Creater_id":71312,"Start_date":"2016-05-27 07:31:11","Question_id":57069}
{"_id":{"$oid":"5837a575a05283111e4d34e8"},"Last_activity":"2016-08-25 00:22:39","Creator_reputation":1230,"Question_score":2,"Answer_content":"Bag of word, whether used with unigrams or bigrams, breaks the structure of the text. Features whose sole existence is informative regarding the concept are valuable in this context.If you use bigrams, then the feature \"White house\" is a good indication to politics. When you use unigrams then both \"white\" and \"house\" are less informative.Note that you lose the context when using bag of word.With respect to \"white\" and \"house\" you will get a similar representation for:The white house announced a new welfare policy.The white car parked near the house.On the other hand, the more token you aggregate, the higher the number on ngrams you will have. The number of occurrences of each ngram will be lower so your computation will become more sensitive to noise.Luckily, you don't have to choose either unigrams or bigrams. A straight forward way is to use both. That will lead to many features, which is a problem on its own.A better approaches is to use only the ngrams that are not accidental. You can identify ngrams whose occurrences is significantly higher than the expected if they were indented, lead to more mutual information or whatever measure that fits you needs. That will remove most bigrams, keep bigrams like \"white house\" and usually a good source for insight in manual observation.","Display_name":"Dan Levin","Creater_id":81056,"Start_date":"2016-08-25 00:22:39","Question_id":231427}
{"_id":{"$oid":"5837a575a05283111e4d34f5"},"Last_activity":"2016-06-20 11:13:38","Creator_reputation":8883,"Question_score":1,"Answer_content":"I think that what you describe is a standard application of multivariate functional data clustering. In the context of multivariate functional data each data unit is treated as the relation of a -dimensional stochastic (often Gaussian) process .Jacques \u0026amp; Preda (the authors of the nice survey paper you attach) have (somewhat) recently published a paper on \"Model-based clustering for multivariate functional data (2014)\" which extends their earlier work on \"Clustering multivariate functional data (2012)\". Approximately at the same time Chiou et al. also on \"Multivariate functional principal component analysis: A normalization approach (2014)\". Note that the two approach are quite different; Chiou's approach has a particular (very flexible) parametric association between the curve-samples while Jacques \u0026amp; Preda is much more data-driven.Both of these works are based on multivariate functional principal component analysis (MvFPCA). Earlier applications where alluded in Ramsay \u0026amp; Silverman's \"Functional Data Analysis (2005)\" book when they looked at hip and knee angle curves at the same time but they did not follow up. A underappreciated work is actually Yang's et al. work on \"Functional singular component analysis (2011)\", they seems to have somewhat stumbled upon MvFPCA but then decided to focus on the analysis of the singular values (in fairness they deal only with two-processes' data). Effectively Berrendero et al. with their work on \"Principal components for multivariate functional data (2011)\" gave the first full treatment of the subject.  MvFPCA by itself is still an active domain of research; getting the good basis in higher dimensions is a tricky part, many non-parametric approaches like spline and kernel smoothers get really variable. Happ \u0026amp; Greven have  2015 arXiv paper on the matter of \"Multivariate Functional Principal Component Analysis for Data Observed on Different (Dimensional) Domains\"; in that work for example for 3-dimensional data they look into DCT as the basis-of-choice (so something fully parametric) and leave non-parametric basis estimation to the side.Having said all that, if you look at the J\u0026amp;P (2014) you will see that given you project the data to a lower dimensional manifold, multivariate clustering techniques are rather competitive. As a first, easy pass I would suggest you do just that: use an MvFPCA approach of your choice, get the projection scores and try a standard well-understood multivariate clustering approach. I have found mixture-models (like the ones in the CRAN package Rmixmod) to work reasonably well but I am sure other techniques can give fruitful results too.Software-wise in R, the CRAN package Funclustering appears to have some MvFPCA out of the box; H\u0026amp;G (2015) have a package named MFPCA on github, Yang's et al. Singular Decomposition is available in the CRAN  packagefdapace.","Display_name":"usεr11852","Creater_id":11852,"Start_date":"2016-06-20 11:13:38","Question_id":212037}
{"_id":{"$oid":"5837a575a05283111e4d3502"},"Last_activity":"2016-08-24 23:53:24","Creator_reputation":1315,"Question_score":1,"Answer_content":"As a first observation, your z-scores are not going to give you what you want. A large z-score tells you that the new arrival count is anomalously large, not that the arrival curve is accelerating.Secondly, I would strongly advise you start with a simpler approach. There are a few possibilities for this 'simpler approach', but here's the one I'd recommend: Smooth your arrival curve using EWMA with some small half-life, then take the diff. This smoothed-diff is a one-sided way to approximate the derivative at some timescale.  Smooth-diff again to get the second derivative. Rank the second derivative values cross-sectionally.Simple approaches like these are great because they take a few minutes to implement and they get you 70% of the performance of a more complex model.  You might not end up using it in production, but a) it gives you something to fall back on if all else fails and b) it allows you to build out the rest of your data-processing pipeline.","Display_name":"Andy Jones","Creater_id":62206,"Start_date":"2016-08-24 23:53:24","Question_id":231544}
{"_id":{"$oid":"5837a575a05283111e4d350f"},"Last_activity":"2016-08-24 23:19:54","Creator_reputation":8337,"Question_score":6,"Answer_content":"The term \"null hypothesis\" is usually used in a frequentist setting, where characteristics of the population, such as its mean, are regarded as fixed, not random. There, it makes no sense to talk about the probability of the null hypothesis.In a Bayesian setting, these characteristics are regarded as random and we can talk about things like the probability of a population mean equalling 0. However, a typical Bayesian would give a prior probability of 0 to many common frequentist null hypotheses, such as the hypothesis that the mean of a normal distribution exactly equals a prespecified value.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-24 23:19:54","Question_id":231580}
{"_id":{"$oid":"5837a575a05283111e4d351c"},"Last_activity":"2016-08-24 22:58:20","Creator_reputation":3023,"Question_score":48,"Answer_content":"Probably the kid likes to eat cookies, so let us assume that you have a whole truck with cookies having a different colour, a different shape, a different taste, a different price ...If the kid has to choose but only take into account one characteristic e.g. the taste, then it has four possibilities: sweet, salt, sour, bitter, so the kid only has to try four cookies to find what (s)he likes most.  If the kid likes combinations of taste and colour, and there are 4 (I am rather optimistic here :-) ) different colours, then he already has to choose among 4x4 different types;If he wants, in addition, to take into account the shape of the cookies and there are 5 different shapes then he will have to try 4x4x5=80 cookiesWe could go on, but after eating all these cookies he might already have belly-ache ... before he can make his best choice :-)As you can see (@Almo) most (all?) things become more complicated as the number of dimensions increases, this holds for adults, for computers and also for kids.","Display_name":"fcop","Creater_id":83346,"Start_date":"2015-08-28 04:42:34","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d351d"},"Last_activity":"2016-06-17 03:55:09","Creator_reputation":161,"Question_score":2,"Answer_content":"I have come across the following link that provides a very intuitive (and detailed) explanation of curse of dimensionality:http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/  In this article, we will discuss the so called ‘Curse of Dimensionality’, and explain why it is important when designing a classifier. In the following sections I will provide an intuitive explanation of this concept, illustrated by a clear example of overfitting due to the curse of dimensionality.In a few words this article derives (intuitively) that adding more features (i.e. increasing the dimensionality of our feature space) requires to collect more data. In fact the amount of data we need to collect (to avoid overfitting) grows exponentially as we add more dimensions.It also has nice illustrations like the following one:","Display_name":"kostas","Creater_id":116849,"Start_date":"2016-06-17 03:05:41","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d351e"},"Last_activity":"2016-01-03 23:03:20","Creator_reputation":323,"Question_score":1,"Answer_content":"Fcop offered a great analogy with cookies but have covered only the sampling density aspect of the curse of dimensionality. We can extend this analogy to the sampling volume or the distance by distributing same number of Fcop's cookies in, say, ten boxes in one line, 10x10 boxes flat on the table and 10x10x10 in a stack. Then you can show that to eat the same share of cookies the child will have to open ever more boxes. It is really about the expectations but let's take a \"worst case scenario\" approach to illustrate. If there are 8 cookies and we want to eat a half i.e. 4, from 10 boxes in a worst case  we only need to open 6 boxes. That's 60% - just about a half too. From 10x10 (again in a worst case) - 96(%). And from 10x10x10 - 996(99,6%). That's almost all of them!May be the storage room analogy and distance walked between rooms would do better than boxes here.","Display_name":"Diego","Creater_id":98469,"Start_date":"2016-01-03 23:03:20","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d351f"},"Last_activity":"2015-12-31 12:23:05","Creator_reputation":13461,"Question_score":2,"Answer_content":"Me: \"I am thinking of a small brown animal beginning with 'S'. What is it?\"Her: \"Squirrel!\"Me: \"OK, a harder one.  I am thinking of a small brown animal.  What is it?\"Her: \"Still a squirrel?\"Me: \"No\"Her: \"Rat, mouse, vole?Me: \"Nope\"Her: \"Umm... give me a clue\"Me: \"Nope, but I'll do some thing better: I'll let you answer to a CrossValidated  question\"Her: [groans]Me: \"The question is: What is the curse of dimensionality?  And you already know the answer\"Her: \"I do?\"Me: \"You do. Why was it harder to guess the first animal than the second?\"Her: \"Because there are more small brown animals than small brown animals beginning with 'S'?\"Me: \"Right. And that's the curse of dimensionality.  Let's play again.\"Her: \"OK\"Me: \"I'm thinking of something. What is it?\"Her: \"No fair. This game is way to hard.\"Me: \"True. That's why they call it a curse. You just can't do well without knowing the things I tend to think about.\"","Display_name":"conjugateprior","Creater_id":1739,"Start_date":"2015-12-31 12:23:05","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d3520"},"Last_activity":"2015-10-06 16:13:14","Creator_reputation":1340,"Question_score":34,"Answer_content":"The analogy I like to use for the curse of dimensionality is a bit more on the geometric side, but I hope it's still sufficiently useful for your kid.It's easy to hunt a dog and maybe catch it if it were running around on the plain (two dimensions). It's much harder to hunt birds, which now have an extra dimension they can move in. If we pretend that ghosts are higher-dimensional beings (akin to the Sphere interacting with A. Square in Flatland), those are even more difficult to catch. :)","Display_name":"J. M.","Creater_id":830,"Start_date":"2015-08-28 08:47:56","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d3521"},"Last_activity":"2015-08-30 17:54:27","Creator_reputation":19083,"Question_score":4,"Answer_content":"Think of a circle enclosed in unit square.Think of a sphere enclosed in the unit cube.Think of an n-dimensional hyper sphere enclosed in the n-dimensional unit hyper cube.The volume of the hyper cube is 1, of course, when measured in  units. However, the volume of a hyper sphere shrinks with n growing.If there was something interesting inside the hyper sphere it's harder and harder to see it in higher dimensions. In -dimensional case the hyper sphere disappears! That's the curse.UPDATE:It seems that some folks didn't get the connection to statistics. You can see the relationship if you imagine picking a random point inside a hyper cube. In two dimensional case the probability that this point is inside the circle (hyper sphere) is , in three dimensional case it's  etc. In the -dimensional case the probability is zero.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2015-08-28 22:48:41","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d3522"},"Last_activity":"2015-08-30 10:14:54","Creator_reputation":4186,"Question_score":0,"Answer_content":"There is a classic, textbook, math problem that shows this.Would you rather earn (option 1) 100 pennies a day, every day for a month, or (option 2) a penny doubled every day for a month?  You can ask your child this question.If you choose option 1,on day 1 you get 100 pennieson day 2 you get 100 pennieson day 3 you get 100 pennies...on day 30 you get 100 pennieson the  day you get 100 pennies.the total number of pennies is found by multiplying the number of days by the number of pennies per day: \\sum_{i=1}^{30}100 = 30 \\cdot 100 = 3000 If you choose option 2:on day 1 you get 1 pennyon day 2 you get 2 pennieson day 3 you get 4 pennieson day 4 you get 8 pennieson day 5 you get 16 pennies...on day 30 you get 1,073,741,824 pennieson the  day you get  pennies.the total number of pennies is observing that the sum of all prior days is one less than the number of pennies received on the current day: \\sum_{i=1}^{30}2^n= \\left(2^{31} \\right)-1  = 2147483648 - 1 = 2147483647 Anyone with greed will choose the bigger number.  Simple greed is easy to find, and requires little thought.  Unspeaking animals are easily capable of greed - insects are notoriously good at it.  Humans are capable of much more.If you start out with one penny instead of a hundred the greed is easier, but if you change the power for a polynomial it is more complex.  Complex can also mean much more valuable.About \"the curse\"The \"most important\" physics-related mathematical operation is matrix inversion.  It drives solutions of systems of partial differential equations, the most common of which are Maxwell's equations (electromagnetics), Navier Stokes equations(fluids), Poisson's equation (diffusive transfer), and variations on Hookes Law (deformable solids).  Each of these equations has college courses built around them.Raw matrix inversion as taught in Linear Algebra, aka Gauss-Jordan method, requires order of  operations to complete.  Here \"n\" is not the number of dimensions, but the number of discretized chunks.  It abstracts to number of dimensions easily.  If it takes 10 chunks to adequately represent the geometry of a 2d object, it takes at least 10^2 to adequately represent a 3d analog, and 10^2^2 to represent a 4d analog.  If you are thinking in terms of geometry you might say \"there aren't 4 dimensions\" but in terms of physical quantities like temperature, concentration, or velocity in a particular direction each require their own \"column\" and count as a dimension.  Taking these equations from 2d to 3d can increase the \"n\" by several powers.The curse exists because if it is overcome there is a pot of golden value at the end of the rainbow.  It isn't easy - great minds have engaged the problem vigorously.  link:https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations ","Display_name":"EngrStudent","Creater_id":22452,"Start_date":"2015-08-30 06:31:19","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d3523"},"Last_activity":"2015-08-30 00:37:30","Creator_reputation":718,"Question_score":3,"Answer_content":"The curse of dimensionality is somewhat fuzzy in definition as it describes different but related things in different disciplines. The following illustrates machine learning’s curse of dimensionality:Suppose a girl has ten toys, of which she likes only those in italics:a brown teddy beara blue cara red traina yellow excavatora green booka grey plush walrusa black wagona pink balla white bookan orange dollNow, her father wants to give her a new toy as a present for her birthday and wants to ensure that she likes it. He thinks very hard about what the toys she likes have in common and finally arrives at a solution. He gives his daughter an all-coloured jigsaw puzzle. When she does not like, he responds: “Why don’t you like it? It does contain the letter w.”The father has fallen victim to the curse of dimensionality (and in-sample optimisation). By considering letters, he was moving in a 26-dimensional space and thus it was very likely that he would find some criterion separating the toys liked by the daughter. This did not need to be a single-letter criterion as in the example, but could have also been something like  contains at least one of a, n and p but none of u, f and s.To adequately tell whether letters are a good criterion for determining which toys his daughter likes, the father would have to know his daughter’s preferences on a gargantuan amount of toys¹ – or just use his brain and only consider parameters that are actually conceivable to affect the daughter’s opinion.¹ order of magnitude: , if all letters were equally likely and he would not take into account multiple occurrences of letters.","Display_name":"Wrzlprmft","Creater_id":36423,"Start_date":"2015-08-30 00:37:30","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d3524"},"Last_activity":"2015-08-28 22:53:52","Creator_reputation":7702,"Question_score":0,"Answer_content":"My 6 yo is more on the verse of the primary cause research, like in \"but where did all this gas in the universe come from?\"... well, I’ll imagine your child understand \"higher dimensions\", which seems very unlikely to me.Let’s ask the following question: pick random points (uniformly) in a -cube , one by one. How long does it take to get a point in the lower corner ? The answer, young lad, is that the probability for a random point to be in this lower corner is , which means that the expected number of points to draw before hitting the left corner is  (by the properties of the geometric distribution). And as you know it from the wheat and chessboard problem, this quickly becomes awfully huge.Now go pick up your room, daddy’s got to work.PS about clustering... think about your points scattered in this high dimension box. It’s so big that there are  sub-boxes with edges of length . It will take some time before picking two points in the same sub-box. Well that can a problem even when the point are not drawn uniformly at random, but in some clusters. If the clusters are not chosen arbitrarily small, it can take very long before picking two points in the same sub-box. You understand that this hinders clustering...","Display_name":"Elvis","Creater_id":8076,"Start_date":"2015-08-28 22:42:39","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d3525"},"Last_activity":"2015-08-28 07:24:03","Creator_reputation":9929,"Question_score":4,"Answer_content":"Suppose you want to ship some goods. You want to waste as little space as possible when packaging the goods (i.e., leave as little empty space as possible), because shipping costs are related to volume of the envelope/box. The containers at your disposal (envelopes, boxes) have right angles, so no sacks etc.First problem: ship a pen (a \"line\") - you can build a box around it with no space lost.Second problem: ship a CD (a \"sphere\"). You need to put it into a square envelope. Depending how old the child is, she may be able to calculate how much of the envelope will remain empty (and still know that there are CDs and not just downloads ;-)).Third problem: ship a football (soccer, and it has to be inflated!). You will need to put it into a box, and some space will remain empty. That empty space will be a higher fraction of the total volume than in the CD example.At that point my intuition using this analogy stops, because I cannot imagine a 4th dimension.EDIT: The analogy is most useful (if at all) for nonparametric estimation, which uses observations \"local\" to the point of interest to estimate, say, a density or a regression function at that point. The curse of dimensionality is that in higher dimensions, one either needs a much larger neighborhood for a given number of observations (which makes the notion of locality questionable) or a large amount of data.","Display_name":"Christoph Hanck","Creater_id":67799,"Start_date":"2015-08-28 02:39:36","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d3526"},"Last_activity":"2015-08-28 07:08:47","Creator_reputation":151,"Question_score":15,"Answer_content":"Ok, so let's analyze the example of the child clustering its toys.Imagine the child has only 3 toys:a blue soccer balla blue freesbea green cube (ok maybe it's not the most fun toy you can imagine)Let's do the following initial hypothesis regarding how a toy can be made:Possible colors are: red, green, bluePossible shapes are: circle, square, triangleNow we can have have (num_colors * num_shapes) = 3 * 3 = 9 possible clusters. The boy would cluster the toys as follows: CLUSTER A) contains the blue ball and the blue freesbe, because thay have the same color and shapeCLUSTER B) contains the super-funny green cubeUsing only these 2 dimensions (color, shape) we have 2 non-empty clusters: so in this first case 7/9 ~ 77% of our space is empty. Now let's increase the number of dimensions the child has to consider. We do also the following hypothesis regarding how a toy can be made:Size of the toy can vary between few centimeters to 1 meter, in step of ten centimeters: 0-10cm, 11-20cm, ..., 91cm-1mWeight of the toy can vary in a similar manner up to 1 kilogram, with steps of 100grams: 0-100g, 101-200g, ..., 901g-1kg.If we want to cluster our toys NOW, we have (num_colors * num_shapes * num_sizes * num_weights) = 3 * 3 * 10 * 10= 900 possible clusters.The boy would cluster the toys as follows: CLUSTER A) contains the blue soccer ball because is blue and heavyCLUSTER B) contains the blue freesbe because is blue and lightCLUSTER C) contains the super-funny green cubeUsing the current 4 dimensions (shape, color, size, weigth) only 3 clusters are non empty: so in this case 897/900 ~ 99.7% of the space is empty. This is an example of what you find on Wikipedia (https://en.wikipedia.org/wiki/Curse_of_dimensionality):...when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. Edit: I'm not sure i could really explain to a child why distance sometimes goes wrong in high-dimensional spaces, but let's try to proceed with our example of the child and his toys.Consider only the 2 first features {color, shape} everyone agrees that the blue ball is more similar to the blue freesbe than to the green cube.Now let's add other 98 features {say: size, weight, day_of_production_of_the_toy, material, softness, day_in_which_the_toy_was_bought_by_daddy, price etc}: well, to me would be increasingly more difficult to judge which toy is similar to which.  So:  A large number of features can be irrelevant in a certain comparison of similarity, leading to a corruption of the signal-to-noise ratio.In high dimensions, all examples \"look-alike\".  If you listen to me, a good lecture is \"A Few Useful Things to Know about Machine Learning\" (http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf), paragraph 6 in particular presents this kind of reasoning.  Hope this helps!","Display_name":"ndrplz","Creater_id":87027,"Start_date":"2015-08-28 05:23:18","Question_id":169156}
{"_id":{"$oid":"5837a575a05283111e4d3535"},"Last_activity":"2016-08-24 22:21:59","Creator_reputation":12897,"Question_score":1,"Answer_content":"  Also, is it OK to directly use least square method to find  and  first? I know they should still be unbiased but no longer the MLE.In autoregressive models the OLS parameter estimates are biased (but still consistent).  I was wondering if I could fit the AR(1) model directly with error terms follow a t distribution?You may use maximum likelihood estimation. Formulate the model likelihood for the scaled Student- case and find the parameter values that maximize it using optimization. (This works fine in theory, but I think I have read somewhere that the estimates of the degrees of freedom can be quite imprecise in practice. I cannot recall the source, though.)","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-24 22:21:59","Question_id":231582}
{"_id":{"$oid":"5837a575a05283111e4d3542"},"Last_activity":"2016-08-24 22:15:21","Creator_reputation":181,"Question_score":3,"Answer_content":"No, there are special cases where these two overlap but in general they are different. It is confusing because they are related and can both be used for nonparametric regression. Also the word \"kernel\" is ambiguous here as there is a distinction between kernel machines and kernel density estimate type nonparametric regression. An example of overlap is that relevance vector machines (RVMs) which can be seen as type of Bayesian kernelised GLM with sparsity inducing priors, can also be formulated as a Gaussian process. This is described in the Rasmussen \u0026amp; Williams book mentioned in the comment.Gaussian processes are, strictly speaking, a type of distribution where every finite sample has a joint Gaussian distribution. Nothing says that this distribution needs to be used for regression. Gaussian processes can be used for unsupervised learning, such as Gaussian process latent variable models.  Gaussian processes can also be used for optimisation. Kernelised GLMs don't really make sense in either of these contexts.There are a couple of other differences:GPs require the kernel to be positive semi-definite, kernelised GLMs do not.fitting kernelised GLMs requires parameter estimation, fitting GPs do not. ","Display_name":"NaN","Creater_id":17760,"Start_date":"2016-08-24 21:33:22","Question_id":231585}
{"_id":{"$oid":"5837a575a05283111e4d354f"},"Last_activity":"2016-08-24 21:50:57","Creator_reputation":1523,"Question_score":0,"Answer_content":"Have a look at the mnps function in the twang package, described in this primer from the RAND Corporation. The package optimizes the tuning parameters of gradient-boosted tree models (highly flexible, non-linear, regularized) to user-specified sample-balance criteria. Then you can estimate the sample weights based on the optimized model.","Display_name":"Brash Equilibrium","Creater_id":7616,"Start_date":"2016-08-24 21:50:57","Question_id":89807}
{"_id":{"$oid":"5837a575a05283111e4d3550"},"Last_activity":"2014-03-12 15:06:08","Creator_reputation":39261,"Question_score":3,"Answer_content":"It is not hard to do simultaneous covariate adjustment for multiple propensity scores.  I recommend always using the logit propensity scale, and expanding those into restricted cubic splines.  An example paper is Mark et al (1994) Circulation 89:2015-2025 where we analyzed three treatments.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2014-03-12 15:06:08","Question_id":89807}
{"_id":{"$oid":"5837a575a05283111e4d355d"},"Last_activity":"2016-08-24 21:48:41","Creator_reputation":152613,"Question_score":8,"Answer_content":"  For t-tests, according to most texts there's an assumption that the population data is normally distributed. I don't see why that is. Doesn't a t-test only require that the sampling distribution of sample means is normally distributed, and not the population?The t-statistic consists of a ratio of two quantities, both random variables. It doesn't just consist of a numerator.For the t-statistic to have the t-distribution, you need not just that the sample mean have a normal distribution. You also need:that the  in the denominator be such that * that the numerator and denominator be independent.*(the value of  depends on which  test -- in the one-sample  we have )For those three things to be actually true, you need that the original data are normally distributed.   If it is the case that t-test only ultimately requires normality in the sampling distribution, the population can look like any distribution, right? Let's take iid as given for a moment. For the CLT to hold the population has to fit the conditions... -- the population has to have a distribution to which the CLT applies. So no, since there are population distributions for which the CLT doesn't apply.  So long as there is a reasonable sample size. Is that not what the central limit theorem states?No, the CLT actually says not one word about \"reasonable sample size\".It actually says nothing at all about what happens at any finite sample size.I'm thinking of a specific distribution right now. It's one to which the CLT certainly does apply. But at , the distribution of the sample mean is plainly non-normal. Yet I doubt that any sample in the history of humanity has ever had that many values in it. So - outside of tautology - what does 'reasonable ' mean?So you have twin problems:A. The effect that people usually attribute to the CLT -- the increasingly close approach to normality of the distributions of sample means at small/moderate sample sizes -- isn't actually stated in the CLT**. B. \"Something not so far from normal in the numerator\" isn't enough to get the statistic having a t-distribution**(Something like the Berry-Esseen theorem gets you more like what people are seeing when they look at the effect of increasing sample size on distribution of sample means.)The CLT and Slutsky's theorem together give you (as long as all their assumptions hold) that as , the distribution of the t-statistic approaches standard normal. It doesn't say whether any given finite  might be enough for some purpose.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2015-05-13 22:27:31","Question_id":141314}
{"_id":{"$oid":"5837a575a05283111e4d356a"},"Last_activity":"2016-08-24 21:44:51","Creator_reputation":152613,"Question_score":6,"Answer_content":"As far as I know, there isn't really a pivotal quantity for *, though it's possible to construct approximately pivotal quantities if  isn't small. (I include  there just in case you have multiple observations from the same Poisson. In many cases you'll just have the one count. From here on I'll just refer to  and  as if we were using a single .)* However, that's not to say nothing can be done. You can derive an interval for  from the relationship between the Poisson and the chi-square (see the end of this section). I can't say that it counts as pivotal, though.For example,  is approximately normal with nearly constant variance (this is related to the Anscombe transform for the Poisson), so could be used to construct an approximate pivotal quantity (by subtracting its mean for example);  - Freeman-Tukey - is another, similar choice from which you could obtain an approximately pivotal quantity. (Indeed, the confidence interval here relies on just such an approach)To my recollection, some papers have given other quantities - if I remember where I've seen these, I'll add references.With large values of , the simpler  might be used as an approximately pivotal quantity.But with small  (more generally small ), there's really not much that can be done - I don't think you get a function of the parameter and the data whose distribution doesn't depend on the parameter. If your  is down around 0.5 or 1 or 2, say, there's not much you can really do about it... the large spikes at 0, 1 and 2 change substantially in relative probability with  and no transformation is going to alter that.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2014-11-05 15:58:47","Question_id":122836}
{"_id":{"$oid":"5837a575a05283111e4d3577"},"Last_activity":"2016-06-08 12:35:49","Creator_reputation":466,"Question_score":2,"Answer_content":"I am going to take an approach similar to others, but I will focus on deriving the general case and then moving to the particular. Consider instead that you have  people of which  are friends and you pick an arbitrary number  people. You might then ask what is the probability that  of those  people are from the group of friends?The number of possible states can be found by looking at  from which you choose . This should be straightforward, but in case you need the answer:   We can then calculate the number of states where this is fulfilled. First out of the  friends we pick . Out of the non-friends we pick .  How many states is this?  Overall then what is the probability?  We can check that our expressions work by summing over  to ensure that the probability sums to 1.   I will leave it to you to plug in the numbers.","Display_name":"Kitter Catter","Creater_id":105368,"Start_date":"2016-06-08 12:35:49","Question_id":147857}
{"_id":{"$oid":"5837a575a05283111e4d3578"},"Last_activity":"2016-06-08 11:51:09","Creator_reputation":13481,"Question_score":1,"Answer_content":"Encode the problem: you have an urn with 35 white balls. You remove 7 balls, paint them blue, and put them back in the urn. You mix all the balls in the urn. If you draw 3 balls at random from the urn without replacement, what is the probability of getting exactly 2 blue balls? () The hypergeometric distribution is your friend here. The second question is similar: what is the probability of getting 3 white balls? ()","Display_name":"Zen","Creater_id":9394,"Start_date":"2016-06-08 11:29:18","Question_id":147857}
{"_id":{"$oid":"5837a575a05283111e4d3579"},"Last_activity":"2016-06-08 11:04:50","Creator_reputation":289,"Question_score":1,"Answer_content":"Here is the logic of the probabilities. The  representation is hidden because I believe that is what you are going for. Try to work it out1) What the probability that exactly two of the group of friends is chosen?2 of the friends chose means P(Friend~draw1) = \\frac{7}{35} and then P(Friend~draw2) = \\frac{6}{34} and finally P(Not~ Friend~draw3) = \\frac{28}{33} This give a probability of P(2~friends~on~3~draws) = \\frac{7}{35}\\times\\frac{6}{34}\\times\\frac{28}{33} However you can draw {friend,friend,Not friend} or {friend,Not friend,friend} or {Not friend,friend,friend} there are  ways of picking 2 friends out of 3. In general we can write the probability of picking from a group as  and  where  and  are the number of remaining people in each group respectively. ~~P(friend,friend,Not friend) = \\frac{7}{35}\\times\\frac{6}{34}\\times\\frac{28}{33} ~~P(friend,Not friend,friend) = \\frac{7}{35}\\times\\frac{28}{34}\\times\\frac{6}{33}  +~P(Not friend,friend,friend) = \\frac{28}{35}\\times\\frac{7}{34}\\times\\frac{6}{33}Since multiplication is both associative and commutative these 3 probabilities are equivalent. Therefore we can write ~~P(2 ~friends ~ on ~3 ~draws) = \\frac{7}{35}\\times\\frac{6}{34}\\times\\frac{28}{33}\\times 3   And this can be expressed in chooses as  \\frac{{7 \\choose 2}{28\\choose1}}{35 \\choose 3}2) Probability that exactly none of the seven friends are chosen?For the first draw you have P(Not~ Friend~draw1) = \\frac{28}{35} on the second draw you now have 27 non-friends and 34 remaining students P(Not~ Friend~draw2) = \\frac{27}{34} and on the third draw you have again one less friend and one less student P(Not~ Friend~draw3) = \\frac{26}{33} So the probability of drawing 3 non-friend is P(3~ non~ friends~ out ~of~ 3) = \\frac{28}{35}\\times\\frac{27}{34}\\times\\frac{26}{33}    This is the same as doing \\frac{28 \\choose 3}{35 \\choose 3}","Display_name":"user2864849","Creater_id":97803,"Start_date":"2016-06-08 10:35:29","Question_id":147857}
{"_id":{"$oid":"5837a575a05283111e4d357a"},"Last_activity":"2016-05-07 00:10:41","Creator_reputation":11,"Question_score":0,"Answer_content":"7 friends to shose from.What the probability that exactly two of the group of friends is chosen?Ans.(7C2) 1 person left to choose other than from those friends. Total number of students are 35. Two are already selected so from 35 32 remains. now 1 student more is needed from these 32. (1C32). Now it would be : (7C2)*(1C32).","Display_name":"Shaka lery","Creater_id":114834,"Start_date":"2016-05-07 00:10:41","Question_id":147857}
{"_id":{"$oid":"5837a575a05283111e4d3587"},"Last_activity":"2016-07-19 14:48:29","Creator_reputation":152613,"Question_score":48,"Answer_content":"Since you ask for insights, I'm going to take a fairly intuitive approach rather than a more mathematical tack:Following the concepts in my answer here, we can formulate a ridge regression as a weighted regression with dummy data by adding  (in your formulation) observations, where ,  and  for . If you write out the new RSS for this expanded data set, you'll see the additional observations each add a term of the form , so the new RSS is the original  -- and minimizing the RSS on this new, expanded data set is the same as minimizing the ridge regression criterion.So what can we see here? As  increases, the additional -rows  each have one component that increases, and so the influence of these points also increases. They pull the fitted hyperplane toward themselves. Then as  and the corresponding components of the 's go off to infinity, all the involved coefficients \"flatten out\" to .I'll give an intuitive sense of why we're talking about ridges first (which also suggests why it's needed), then tackle a little history. The first is adapted from my answer here:If there's multicollinearity, you get a \"ridge\" in the likelihood function (likelihood is a function of the 's). This in turn yields a long \"valley\" in the RSS (since RSS=). Ridge regression \"fixes\" the ridge - it adds a penalty that turns the ridge into a nice peak in likelihood space, equivalently a nice depression in the criterion we're minimizing:[Clearer image]The actual story behind the name is a little more complicated. In 1959 A.E. Hoerl [1] introduced ridge analysis for response surface methodology, and it very soon [2] became adapted to dealing with multicollinearity in regression ('ridge regression'). See for example, the discussion by R.W. Hoerl in [3], where it  describes Hoerl's (A.E. not R.W.) use of contour plots of the response surface* in the identification of where to head to find local optima (where one 'heads up the ridge'). In ill-conditioned problems, the issue of a very long ridge arises, and insights and methodology from ridge analysis are adapted to the related issue with the likelihood/RSS in regression, producing ridge regression.* examples of response surface contour plots (in the case of quadratic response) can be seen here (Fig 3.9-3.12).That is, \"ridge\" actually refers to the characteristics of the function we were attempting to optimize, rather than to adding a \"ridge\" (+ve diagonal) to the  matrix (so while ridge regression does add to the diagonal, that's not why we call it 'ridge' regression).For some additional information on the need for ridge regression, see the first link under list item 2. above.References:[1]: Hoerl, A.E. (1959). Optimum solution of many variables equations. Chemical Engineering Progress, 55 (11) 69-78.[2]: Hoerl, A.E. (1962). Applications of ridge analysis to regression problems. Chemical Engineering Progress, 58 (3) 54-59.[3] Hoerl, R.W. (1985). Ridge Analysis 25 Years Later.American Statistician, 39 (3), 186-192 ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2015-05-07 17:18:15","Question_id":151304}
{"_id":{"$oid":"5837a575a05283111e4d3588"},"Last_activity":"2015-05-08 05:08:43","Creator_reputation":3298,"Question_score":27,"Answer_content":"If  then our penalty term will be infinite for any  other than , so that's the one we'll get. There is no other vector that will give us a finite value of the objective function.(Update: Please see Glen_b's answer. This is not the correct historical reason!)This comes from ridge regression's solution in matrix notation. The solution turns out to be\\hat \\beta = (X^TX + \\lambda I)^{-1} X^TY.The  term adds a \"ridge\" to the main diagonal and guarantees that the resulting matrix is invertible. This means that, unlike OLS, we'll always get a solution.Ridge regression is useful when the predictors are correlated. In this case OLS can give wild results with huge coefficients, but if they are penalized we can get much more reasonable results. In general a big advantage to ridge regression is that the solution always exists, as mentioned above. This applies even to the case where , for which OLS cannot provide a (unique) solution.Ridge regression also is the result when a normal prior is put on the  vector. Here's the Bayesian take on ridge regression:Suppose our prior for  is . Then because  [by assumption] we have that\\pi(\\beta | y) \\propto \\pi(\\beta) f(y|\\beta)\\propto \\frac{1}{(\\sigma^2/\\lambda)^{p/2}} \\exp \\left( -{\\lambda \\over 2\\sigma^2} \\beta^T\\beta \\right) \\times \\frac{1}{(\\sigma^2)^{n/2}} \\exp \\left( \\frac{-1}{2\\sigma^2} ||y - X\\beta||^2 \\right)\\propto \\exp \\left( -{\\lambda \\over 2\\sigma^2} \\beta^T\\beta - \\frac{1}{2\\sigma^2} ||y - X\\beta||^2 \\right).Let's find the posterior mode (we could look at posterior mean or other things too but for this let's look at the mode, i.e. the most probable value).This means we want\\max_{\\beta \\in \\mathbb R^p} \\ \\exp \\left( -{\\lambda \\over 2\\sigma^2} \\beta^T\\beta - \\frac{1}{2\\sigma^2} ||y - X\\beta||^2 \\right)which is equivalent to\\max_{\\beta \\in \\mathbb R^p} \\ -{\\lambda \\over 2\\sigma^2} \\beta^T\\beta - \\frac{1}{2\\sigma^2} ||y - X\\beta||^2  because  is strictly monotone and this in turn is equivalent to\\min_{\\beta \\in \\mathbb R^p} ||y - X\\beta||^2 + \\lambda \\beta^T\\betawhich ought to look pretty familiar. Thus we see that if we put a normal prior with mean 0 and variance  on our  vector, the value of  which maximizes the posterior is the ridge estimator. Note that this treats  more as a frequentist parameter because there's no prior on it but it isn't known, so this isn't fully Bayesian.Edit: you asked about the case where .We know that a hyperplane in  is defined by exactly  points. If we are running a linear regression and  then we exactly interpolate our data and get . This is a solution, but it is a terrible one: our performance on future data will most likely be abysmal. Now suppose : there is no longer a unique hyperplane defined by these points. We can fit a multitude of hyperplanes, each with 0 residual sum of squares.A very simple example: suppose . Then we'll just get a line between these two points. Now suppose  but . Picture a plane with these two points in it. We can rotate this plane without changing the fact that these two points are in it, so there are uncountably many models all with a perfect value of our objective function, so even beyond the issue of overfitting it is not clear which one to pick.As a final comment (per @gung's suggestion), the LASSO (using an  penalty) is commonly used for high dimensional problems because it automatically performs variable selection (sets some ). Delightfully enough, it turns out that the LASSO is equivalent to finding the posterior mode when using a double exponential (aka Laplace) prior on the  vector. The LASSO also has some limitations, such as saturating at  predictors and not necessarily handling groups of correlated predictors in an ideal fashion, so the elastic net (convex combination of  and  penalties) may be brought to bear.","Display_name":"Chaconne","Creater_id":30005,"Start_date":"2015-05-07 12:01:45","Question_id":151304}
{"_id":{"$oid":"5837a575a05283111e4d3595"},"Last_activity":"2016-08-24 21:28:53","Creator_reputation":1523,"Question_score":1,"Answer_content":"First, your question is limited to propensity score matching, but there are other matching procedures that do not use propensity scores, such as exact matching, genetic matching, and coarsened exact matching. See Ho et al. 2007 for a review. These alternative methods are non-parametric and do not make linear assumptions. In fact, unless your propensity-score model is very good, there are theoretical arguments that suggest such methods are preferable.Second, propensity scores are most generally the predictions from a model that estimates probabilities conditional on covariates. Nothing prevents you from using non-linear models, such as random forest, boosted regression trees, neural networks, etc. In fact, there is a package for R called twang developed by the RAND Corporation that uses the gbm package to optimize the tuning parameters of boosted regression trees with respect to user-specified sample-balance criteria, then output the predicted propensity scores for your sample using the balance-optimized model. You can also specify whether you are after an average treatment effect (ATE) on the population or on the treated (ATT). I wrote a silly little LinkedIn post that gives some brief pointers on how to use twang-generated propensity scores as the distance metric for nearest-neighbor matching as implemented in the MatchIt package.","Display_name":"Brash Equilibrium","Creater_id":7616,"Start_date":"2016-08-24 21:28:53","Question_id":183184}
{"_id":{"$oid":"5837a576a05283111e4d3619"},"Last_activity":"2016-08-24 14:01:14","Creator_reputation":520,"Question_score":5,"Answer_content":"You may use ar.ols to estimate this non-stationary series.From the docs,   ar.ols fits the general AR model to a possibly non-stationary and/or  multivariate system of series x.Example with your data:ar.ols(x,demean=FALSE)Call:ar.ols(x = x, demean = FALSE)Coefficients:1  2  1  1 ","Display_name":"A. Webb","Creater_id":56912,"Start_date":"2016-08-24 14:01:14","Question_id":231594}
{"_id":{"$oid":"5837a576a05283111e4d361a"},"Last_activity":"2016-08-24 13:54:16","Creator_reputation":320,"Question_score":6,"Answer_content":"As I said in my comment, you are going toward a wrong direction.ar is assuming x as a stationary process AR(p). The default estimation method \"yule-walker\" is a moment estimator. Please see Yule-Walker equations of autoregressive process for more.ar selects order p by minimizing AIC. For you example Fibonacci sequence x, it has selected p = 1. The resulting coefficient, by Yule-Walker equations, matches the sample ACF at lag 1:z \u0026lt;- acf(x, lag.max = 1)#    0     1#1.000 0.553Since model assumption is wrong, you definitely can not get c(1, 1) as the answer. A crude way to get you to the right estimation is using least squares linear regression:N \u0026lt;- length(x)y \u0026lt;- x[3:N]x1 \u0026lt;- x[2:(N-1)]    # lag-1 x2 \u0026lt;- x[1:(N-2)]    # lag-2lm(y ~ x1 + x2 - 1)  ## drop intercept (as you know it for sure)#Coefficients:#x1  x2  # 1   1  ","Display_name":"Zheyuan Li","Creater_id":117783,"Start_date":"2016-08-24 13:54:16","Question_id":231594}
{"_id":{"$oid":"5837a576a05283111e4d3627"},"Last_activity":"2016-08-24 20:52:20","Creator_reputation":7682,"Question_score":8,"Answer_content":"  Why they represent covariance with 4 separated matrices?  I emphasize this each notion as matrix. what happen if each notion become a matrix In this case the vectors  and  are really block vectors. In the case of an -dimensional  vector we could expand it as follows:\\boldsymbol Y= \\begin{bmatrix} \\color{blue}{Y_1} \\\\ \\color{red}{Y_2} \\end{bmatrix}=\\begin{bmatrix}\\color{blue}{Y_{11}\\\\Y_{12}\\\\\\vdots\\\\ Y_{1h}}\\\\\\color{red}{Y_{21}\\\\Y_{22}\\\\\\vdots\\\\ Y_{2k}}\\end{bmatrix}\\tag{}showing the partition of the  coordinates into two groups of size  and , respectively, such that . A parallel illustration would immediately follow for the  vector of population means.The block matrix of covariances would hence follow as:\\begin{bmatrix}\\Sigma_{\\color{blue}{11}} \u0026amp; \\Sigma_{\\color{blue}{1}\\color{red}{2}}\\\\\\Sigma_{\\color{red}{2}\\color{blue}{1}} \u0026amp; \\Sigma_{\\color{red}{22}}\\end{bmatrix} \\tag {}where\\small\\Sigma_{\\color{blue}{11}}=\\begin{bmatrix}\\sigma^2({\\color{blue}{Y_{11}}}) \u0026amp; \\text{cov}(\\color{blue}{Y_{11},Y_{12}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{blue}{Y_{11},Y_{1h}}) \\\\\\text{cov}(\\color{blue}{Y_{12},Y_{11}}) \u0026amp; \\sigma^2({\\color{blue}{Y_{12}}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{blue}{Y_{12},Y_{1h}}) \\\\\\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\\\text{cov}(\\color{blue}{Y_{1h},Y_{11}})  \u0026amp;  \\text{cov}(\\color{blue}{Y_{1h},Y_{12}}) \u0026amp;\\dots\u0026amp; \\sigma^2({\\color{blue}{Y_{1h}}})\\end{bmatrix} \\tag{}with \\small \\Sigma_{\\color{blue}{1}\\color{red}{2}}=\\begin{bmatrix}\\text{cov}({\\color{blue}{Y_{11}}},\\color{red}{Y_{21}}) \u0026amp; \\text{cov}(\\color{blue}{Y_{11}},\\color{red}{Y_{22}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{blue}{Y_{11}},\\color{red}{Y_{2k}}) \\\\\\text{cov}({\\color{blue}{Y_{12}}},\\color{red}{Y_{21}}) \u0026amp; \\text{cov}(\\color{blue}{Y_{12}},\\color{red}{Y_{22}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{blue}{Y_{12}},\\color{red}{Y_{2k}}) \\\\\\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\\\text{cov}({\\color{blue}{Y_{1h}}},\\color{red}{Y_{21}}) \u0026amp; \\text{cov}(\\color{blue}{Y_{1h}},\\color{red}{Y_{22}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{blue}{Y_{1h}},\\color{red}{Y_{2k}})\\end{bmatrix}\\tag{}its transpose...\\small \\Sigma_{\\color{red}{2}\\color{blue}{1}}=\\begin{bmatrix}\\text{cov}({\\color{red}{Y_{21}}},\\color{blue}{Y_{11}}) \u0026amp; \\text{cov}(\\color{red}{Y_{21}},\\color{blue}{Y_{12}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{red}{Y_{21}},\\color{blue}{Y_{1h}}) \\\\\\text{cov}({\\color{red}{Y_{22}}},\\color{blue}{Y_{11}}) \u0026amp; \\text{cov}(\\color{red}{Y_{22}},\\color{blue}{Y_{12}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{red}{Y_{22}},\\color{blue}{Y_{1h}}) \\\\\\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\\\text{cov}({\\color{red}{Y_{2k}}},\\color{blue}{Y_{11}}) \u0026amp; \\text{cov}(\\color{red}{Y_{2k}},\\color{blue}{Y_{12}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{red}{Y_{2k}},\\color{blue}{Y_{1h}})\\end{bmatrix}\\tag{}and\\small \\Sigma_{\\color{red}{22}}=\\begin{bmatrix}\\sigma^2({\\color{red}{Y_{21}}}) \u0026amp; \\text{cov}(\\color{red}{Y_{21},Y_{22}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{red}{Y_{21},Y_{2k}}) \\\\\\text{cov}(\\color{red}{Y_{22},Y_{21}}) \u0026amp; \\sigma^2({\\color{red}{Y_{22}}}) \u0026amp; \\dots \u0026amp; \\text{cov}(\\color{red}{Y_{22},Y_{2k}}) \\\\\\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\\\text{cov}(\\color{red}{Y_{2k},Y_{21}})  \u0026amp;  \\text{cov}(\\color{red}{Y_{2k},Y_{22}}) \u0026amp;\\dots\u0026amp; \\sigma^2({\\color{red}{Y_{2k}}})\\end{bmatrix} \\tag{}These partitions come into play in proving that the marginal distributions of a multivariate Gaussian are also Gaussian, as well as in the actual derivation of marginal and conditional pdf's.","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-08-23 19:44:44","Question_id":231385}
{"_id":{"$oid":"5837a576a05283111e4d3633"},"Last_activity":"2016-08-24 19:34:46","Creator_reputation":98,"Question_score":0,"Answer_content":"Some questions I would ask if I could explore the data:Are the missing values somehow correlated to a feature? Does it make sense instead to create a new binary feature tracking whether a given feature is missing?How many observations are missing feature values? Do I have enough data to simply discard all observations that are not complete? If I regularize a model with the \"missing data\" binary feature I mentioned earlier, does regularization push this feature out? (If so, you can probably drop the observations without issue).Try clustering the various features with missing values according to output label. Is there clear separation between the classes? If so, you may want to impute using a class-level mean instead of a mean across all observations.Your intuition is right that one often imputes missing data by using class-level means instead of sample-level means, but that might not be necessary here.","Display_name":"achompas","Creater_id":25660,"Start_date":"2016-08-24 19:34:46","Question_id":231586}
{"_id":{"$oid":"5837a576a05283111e4d3646"},"Last_activity":"2016-08-24 18:45:41","Creator_reputation":4864,"Question_score":2,"Answer_content":"Here's the first, informal definition of Markov state given in that section (emphasis mine):  For example, a checkers position--the current configuration of all the pieces on the board--would serve as a Markov state because it summarizes everything important about the complete sequence of positions that led to it. Much of the information about the sequence is lost, but all that really matters for the future of the game is retained. I won't duplicate the full formal definition, but it concludes thus:  In other words, a state signal has the Markov property, and is a Markov state, if and only if (3.5) is equal to (3.4) for all  and histories .Note that (3.4) and (3.5) are the conditional probabilities of the next state-reward pair conditioned on the entire history (3.4) and just the current state (3.5).Here's another useful point:  It also follows that Markov states provide the best possible basis for choosing actions. That is, the best policy for choosing actions as a function of a Markov state is just as good as the best policy for choosing actions as a function of complete histories.If forced to wager, I'd suggest that the intent of the first question was to get the reader to reason through this definition and argue one way or the other. Example 3.5, the pole balancing problem, provides a useful skeleton:   In the pole-balancing task introduced earlier, a state signal would be Markov if it specified exactly, or made it possible to reconstruct exactly, the position and velocity of the cart along the track, the angle between the cart and the pole, and the rate at which this angle is changing (the angular velocity). In an idealized cart-pole system, this information would be sufficient to exactly predict the future behavior of the cart and pole, given the actions taken by the controller. All to say that I think you're on the right track with an answer of \"No, here's why.\" In particular, a single image would give no indication of prior movement.","Display_name":"Sean Easter","Creater_id":28462,"Start_date":"2016-08-24 18:45:41","Question_id":231515}
{"_id":{"$oid":"5837a576a05283111e4d3647"},"Last_activity":"2016-08-24 11:51:35","Creator_reputation":240,"Question_score":-1,"Answer_content":"I had the same doubt, this is how I reasoned about it. By \"Markov state of the environment\" I believe the question means does the agent have any notion of how the states in the environment transition. Based on this definition the first time the agent receives an image it has no notion of transitions hence it does not have a Markov state of the environment.Please feel free to correct this post if its wrong.","Display_name":"Adi","Creater_id":22545,"Start_date":"2016-08-24 11:51:35","Question_id":231515}
{"_id":{"$oid":"5837a576a05283111e4d3654"},"Last_activity":"2016-08-24 18:31:26","Creator_reputation":152693,"Question_score":1,"Answer_content":"If  and  are bivariate normal their sum is normal (this can be shown in several ways).In particular for our present purpose, if they're independent and  respectively then  and hence their mean is as well (since if  is normal so is ).Wikipedia gives three proofs for the independence case here: Sum of normally distributed random variables. It then gives a proof for bivariate normal variables.Despite the potential implication of the title of the Wikipedia article I linked, it only discusses jointly normal variables. More generally it's not true of sums of normal random variates - if you don't have joint normality it's usually not the case the the sum is normal.While I've focused on two variables, this discussion applies in turn to  independent variables.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-24 18:14:15","Question_id":231562}
{"_id":{"$oid":"5837a576a05283111e4d3655"},"Last_activity":"2016-08-24 16:23:48","Creator_reputation":447,"Question_score":0,"Answer_content":"This statement is not well formed. \"A\" sample mean isn't normal or not normal, it just is -- it can't be 'distributed' because a sample mean is just one number. I can't be sure without more context but I think the author is referring to the central limit theorem which states that the sampling distribution of the mean tends towards normality as sample size increases. And, this is true for [nearly] any parent distribution. For instance, if you sample values from an exponential distribution, calculate the mean, then repeat this process, the distribution of those mean values will be normal. As to the why this must be the case, there are mathematical explanations and conceptual ones. Many, actually. One conceptual way to think about it is that a single sample mean 'tries' to do the same thing every time - summarize the parent. Sometimes, you'll sample some extreme values, causing that single mean to vary a bit. But after a series of sample means, the 'errors' you make are equally likely to be above and below the true parent mean, and most likely to be close to the real value. Thus, the distribution is normal. ","Display_name":"HEITZ","Creater_id":86794,"Start_date":"2016-08-24 16:23:48","Question_id":231562}
{"_id":{"$oid":"5837a576a05283111e4d3662"},"Last_activity":"2016-08-24 18:26:13","Creator_reputation":1,"Question_score":-1,"Answer_content":"It would seem that the distribution of the response conforms to a Poisson distribution. Though I would say that the model itself should be modeled using a log link I am unsure if the lmer function allows for generalization of the residuals. However, I believe the correct extraction of residuals for count data should be 'Pearson'.One other thing to consider is that your abline() function is set through zero with a slope of one, in other words the reference line will never actually follow the true theoretical quantiles. Use the function qqline()in order to rectify that.","Display_name":"Stan Mastrantonis","Creater_id":128856,"Start_date":"2016-08-24 16:23:52","Question_id":125258}
{"_id":{"$oid":"5837a576a05283111e4d3671"},"Last_activity":"2014-04-07 04:24:08","Creator_reputation":2648,"Question_score":4,"Answer_content":"The p-value calculated by print.summary.lm (use getAnywhere(print.summary.lm) to study the code)  is rounded for floating point precision using format.pval.2.2e-16 is the value of .Machine$double.eps, which is   the smallest positive floating-point number x such that 1 + x != 1So, the rounding is not arbitrary, but for numerical reasons.","Display_name":"Roland","Creater_id":11849,"Start_date":"2014-04-07 04:12:16","Question_id":92824}
{"_id":{"$oid":"5837a576a05283111e4d367e"},"Last_activity":"2016-08-24 16:57:43","Creator_reputation":471,"Question_score":3,"Answer_content":"sklearn's RF oob_score_ (note the trailing underscore) seriously isn't very intelligible compared to R's, after reading the sklearn doc and source code.My advice on how to improve your model is as follows:sklearn's RF used to use the terrible default of max_features=1 (as in \"try every feature on every node\"). Then it's no longer doing random column(/feature)-selection like a random-forest. Change this to e.g.max_features=0.33 (like R's mtry) and rerun. Tell us the new scores. \"Most of the features have shown negligible importance\". Then you need to do Feature Selection, as per the doc - for classification. See the doc and other articles here on CrossValidated.SE. Do the FS on a different (say 20-30%) holdout set than the rest of the training, using e.g. sklearn.cross_validation.train_test_split() (yes the name is a bit misleading). Now tell us the scores you get after FS?You said \"after removing bad data (about third of the data), the labels were more or less 2% for 0 and 49% for each of -1/+1\" ; then you have a severe class imbalance. Also: \"confusion matrix shows model only succeeds for class 0, and fails in about 50% of the cases between +1 and -1\". This is a symptom of the class imbalance. Either you use stratified sampling, or train a classifier with examples for +1 and -1 class. You can either do a OAA (One-Against-All) or OAO (One-Against-One) classifier. Try three OAA classifiers, one for each class. Finally tell us those scores?","Display_name":"smci","Creater_id":7291,"Start_date":"2015-08-24 23:21:47","Question_id":95818}
{"_id":{"$oid":"5837a576a05283111e4d367f"},"Last_activity":"2016-04-19 02:30:47","Creator_reputation":141,"Question_score":0,"Answer_content":"a different take on the question: to start with, you have to associate a loss with every misclassification you do. This price-paid/loss/penalty for misclassification would(probably) be different for False Positive(FP) vs False Negatives(FN). Some classifications, say cancer detection, would rather have more FPs than FNs. Some other, say spam filter, would rather allow certain spams(FN) than block mails(FP) from your friend. Building on this logic you can have use F1-score or Accuracy, whatever suits your purpose.( for eg. I could be happy if my spam filter has no FPs and a score of .1 as I have 10% less spams to worry about. On the other hand someone else could be unhappy with even .9 (90% spams filtered). What would be good score then?)","Display_name":"Anurag Priyadarshi","Creater_id":32404,"Start_date":"2016-04-19 02:30:47","Question_id":95818}
{"_id":{"$oid":"5837a576a05283111e4d3680"},"Last_activity":"2016-03-19 07:06:21","Creator_reputation":993,"Question_score":0,"Answer_content":"Q: What is a good oob score for random forests with sklearn, three-class classification?A: Depends. In my view, if learning and testing samples are drawn from the same distribution, then -in my view- OOB is equal to approximately 3-fold cross-validation. So if we repeat the same question but with \"3-fold cross-validation\", the answer would be the same, which is \"generally, the highest the accuracy the merrier, unless you fear to overfit your learning set because someone told you that the true testing samples are of a different distribution\".Can you give me your dataset? I can have little fun with it and tell you what I manage to do with it for free.","Display_name":"caveman","Creater_id":100507,"Start_date":"2016-03-19 07:06:21","Question_id":95818}
{"_id":{"$oid":"5837a576a05283111e4d368d"},"Last_activity":"2016-08-24 16:56:17","Creator_reputation":266,"Question_score":0,"Answer_content":"No. The entropy of  is maximized when  is a uniformly distributed in Example for  P(X_1 = 0, X_2 = 0 ) = 1/3 \\\\P(X_1 = 1, X_2 = 0 ) = 1/6 \\\\P(X_1 = 0, X_2 = 1 ) = 1/6 \\\\P(X_1 = 1, X_2 = 1 ) = 1/3Tha above joint distribution in ,  has the highest entropy for  . Also, by symmetry,  ,  are indentically distributed in the above distribution.","Display_name":"Vivek Bagaria","Creater_id":99046,"Start_date":"2016-08-24 16:56:17","Question_id":228400}
{"_id":{"$oid":"5837a576a05283111e4d3699"},"Last_activity":"2016-08-24 16:44:58","Creator_reputation":56,"Question_score":0,"Answer_content":"Perhaps your data has duplicate instances, or at least very similar instances, so when taking some instances for testing, you leaving other similar instances in the training data, this situation is as if your are testing and training on the same data.You may use Weka to remove the duplicate instances and do the experiment again and see if you still get the same results.","Display_name":"Ahmad Hassanat","Creater_id":71312,"Start_date":"2016-08-24 16:44:58","Question_id":144115}
{"_id":{"$oid":"5837a576a05283111e4d369a"},"Last_activity":"2015-03-31 13:20:22","Creator_reputation":97,"Question_score":1,"Answer_content":"It would appear there's something wrong with my current methodology.  Haven't figured out what, exactly, but implementing k-fold immediately demonstrated this to me, as a single random input for a value results no meaningful learning occurring in the network.Back to the drawing board....","Display_name":"Joel Graff","Creater_id":62362,"Start_date":"2015-03-31 13:20:22","Question_id":144115}
{"_id":{"$oid":"5837a576a05283111e4d36a7"},"Last_activity":"2016-08-24 16:44:11","Creator_reputation":3670,"Question_score":1,"Answer_content":"The variance of a variable  with negative binomial distribution with expectation  and dispersion parameter  is . Thus, the negative binomial distribution is always overdispersed with  and only reaches equidispersion for  (i.e., the Poisson distribution).There are various ways how underdispersion can occur, e.g., through zero-inflation or zero-truncation, which both have corresponding regression models. There are also more flexible extensions of the Poisson that can accomodate underdispersion, e.g., Conwell-Maxwell-Poisson among others.If you do not need a full likelihood model, a quasi-Poisson model with estimated dispersion parameter (which can be ) may be useful as well.","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2016-08-24 16:44:11","Question_id":230730}
{"_id":{"$oid":"5837a576a05283111e4d36b4"},"Last_activity":"2016-08-24 16:08:49","Creator_reputation":29925,"Question_score":20,"Answer_content":"There are many different ways to produce a PCA biplot and so there is no unique answer to your question. Here is a short overview.We assume that the data  matrix  has  data points in rows and is centered (i.e. column means are all zero). For now, we do not assume that it was standardized, i.e. we consider PCA on covariance matrix (not on correlation matrix). PCA amounts to a singular value decomposition \\mathbf X=\\mathbf{USV}^\\top, you can see my answer here for details: Relationship between SVD and PCA. How to use SVD to perform PCA?In a PCA biplot, two first principal components are plotted as a scatter plot, i.e. first column of  is plotted against its second column. But normalization can be different; e.g. one can use:Columns of : these are principal components scaled to unit sum of squares;Columns of : these are standardized principal components (unit variance);Columns of : these are \"raw\" principal components (projections on principal directions).Further, original variables are plotted as arrows; i.e.  coordinates of an -th arrow endpoint are given by the -th value in the first and second column of . But again, one can choose different normalizations, e.g.:Columns of : I don't know what an interpretation here could be;Columns of : these are loadings;Columns of : these are principal axes (aka principal directions, aka eigenvectors).Here is how all of that looks like for Fisher Iris dataset:Combining any subplot from above with any subplot from below would make up  possible normalizations. But according to the original definition of a biplot introduced in Gabriel, 1971, The biplot graphic display of matrices with application to principal component analysis (this paper has 2k citations, by the way), matrices used for biplot should, when multiplied together, approximate  (that's the whole point). So a \"proper biplot\" can use e.g.  and . Therefore only three of the  are \"proper biplots\": namely a combination of any subplot from above with the one directly below.Using loadings, i.e. , for arrows has a large benefit in that they have useful interpretations (see also here about loadings). Length of the loading arrows approximates the standard deviation of original variables (squared length approximates variance), scalar products between any two arrows approximate the covariance between them, and cosines of the angles between arrows approximate correlations between original variables. To make a \"proper biplot\", one should choose , i.e. standardized PCs, for data points. Gabriel (1971) calls this \"PCA biplot\" and writes that   This [particular choice] is likely to provide a most useful graphical aid in interpreting multivariate matrices of observations, provided, of course, that these can be adequately approximated at rank two. On the other hand, one can prefer to plot raw PCs  together with loadings. This is an \"improper biplot\", but was e.g. done by @vqv on the most elegant biplot I have ever seen: Visualizing a million, PCA edition -- it shows PCA of the wine dataset. Note that @vqv had to scale all the arrows and the correlation circle by some (arbitrary?) factor  to adjust the scale for the  scatter plot...The figure you posted (default outcome of R biplot function) is a \"proper biplot\" with  and . Unfortunately, the biplot function makes a weird choice of scaling all arrows down by a factor of  and displaying the text labels where the arrow endpoints should have been.PCA on correlation matrixIf we further assume that the data matrix  has been standardized so that column standard deviations are all equal to , then we are performing PCA on the correlation matrix. Here is how the same figure looks like:Here the loadings are even more attractive, because (in addition to the above mentioned properties), they give exactly (and not approximately) correlation coefficients between original variables and PCs. Correlations are all smaller than  and loadings arrows have to be inside a \"correlation circle\" of radius , which is sometimes drawn on a biplot as well (I plotted it on the corresponding subplot above). Note that the biplot by @vqv (linked above) was done for a PCA on correlation matrix, and also sports a correlation circle.Further reading:PCA and Correspondence analysis in their relation to Biplot - detailed treatment by @ttnphns.What is the proper association measure of a variable with a PCA component (on a biplot / loading plot)? - geometric explanation by @ttnphns of what the loading arrows on a biplot mean.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2015-03-12 17:00:18","Question_id":141085}
{"_id":{"$oid":"5837a576a05283111e4d36c3"},"Last_activity":"2016-08-24 15:58:15","Creator_reputation":29925,"Question_score":8,"Answer_content":"  Let's say I choose some linear combination of these variables -- e.g. , could I work out how much variance in the data this describes?This question can be understood in two different ways, leading to two different answers.A linear combination corresponds to a vector, which in your example is . This vector, in turn, defines an axis in the 6D space of the original variables. What you are asking is, how much variance does projection on this axis \"describe\"? The answer is given via the notion of \"reconstruction\" of original data from this projection, and measuring the reconstruction error (see Wikipedia on Fraction of variance unexplained). Turns out, this reconstruction can be reasonably done in two different ways, yielding two different answers. Approach #1Let  be the centered dataset ( rows correspond to samples,  columns correspond to variables), let  be its covariance matrix, and let  be a unit vector from . The total variance of the dataset is the sum of all  variances, i.e. the trace of the covariance matrix: . The question is: what proportion of  does  describe? The two answers given by @todddeluca and @probabilityislogic are both equivalent to the following: compute projection , compute its variance and divide by : R^2_\\mathrm{first} = \\frac{\\mathrm{Var}(\\X \\w)}{T} = \\frac{\\w^\\top \\S \\w}{\\mathrm{tr}(\\S)}.This might not be immediately obvious, because e.g. @probabilityislogic suggests to consider the reconstruction  and then to compute \\frac{\\|\\X\\|^2 - \\|\\X-\\X \\w \\w^\\top\\|^2}{\\|\\X\\|^2}, but with a little algebra this can be shown to be an equivalent expression.Approach #2Okay. Now consider a following example:  is a  dataset with covariance matrix \\S = \\left(\\begin{array}{c}1\u0026amp;0.99\\\\0.99\u0026amp;1\\end{array}\\right) and  is simply an  vector:The total variance is . The variance of the projection onto  (shown in red dots) is equal to . So according to the above logic, the explained variance is equal to . And in some sense it is: red dots (\"reconstruction\") are far away from the corresponding blue dots, so a lot of the variance is \"lost\".On the other hand, the two variables have  correlation and so are almost identical; saying that one of them describes only  of the total variance is weird, because each of them contains \"almost all the information\" about the second one. We can formalize it as follows: given projection , find a best possible reconstruction  with  not necessarily the same as , and then compute the reconstruction error and plug it into the expression for the proportion of explained variance: R^2_\\mathrm{second}=\\frac{\\|\\X\\|^2 - \\|\\X-\\X \\w \\v^\\top\\|^2}{\\|\\X\\|^2}, where  is chosen such that  is minimal (i.e.  is maximal). This is exactly equivalent to computing  of multivariate regression predicting original dataset  from the -dimensional projection .It is a matter of straightforward algebra to use regression solution for  to find that the whole expression simplifies to R^2_\\mathrm{second}=\\frac{\\|\\S \\w\\|^2}{\\w^\\top \\S \\w \\cdot \\mathrm{tr}(\\S)}. In the example above this is equal to , which seems reasonable.Note that if (and only if)  is one of the eigenvectors of  , i.e. one of the principal axes, with eigenvalue  (so that  ), then both approaches to compute  coincide and reduce to the familiar PCA expression R^2_\\mathrm{PCA} = R^2_\\mathrm{first} = R^2_\\mathrm{second} = \\lambda/\\mathrm{tr}(\\S) = \\lambda/\\sum \\lambda_i.PS. See my answer here for an application of the derived formula to the special case of  being one of the basis vectors: Variance of the data explained by a single variable.Appendix. Derivation of the formula for Finding  minimizing the reconstruction  is a regression problem (with  as univariate predictor and  as multivariate response). Its solution is given by \\v^\\top = \\left((\\X \\w)^\\top (\\X \\w)\\right)^{-1}(\\X \\w)^\\top \\X = (\\w^\\top \\S \\w)^{-1} \\w^\\top \\S.Next, the  formula can be simplified as R^2=\\frac{\\|\\X\\|^2 - \\|\\X-\\X \\w \\v^\\top\\|^2}{\\|\\X\\|^2} = \\frac{\\|\\X \\w \\v^\\top\\|^2}{\\|\\X\\|^2} due to the Pythagoras theorem, because the hat matrix in regression is an orthogonal projection (but it is also easy to show directly).Plugging now the equation for , we obtain for the numerator: \\|\\X \\w \\v^\\top\\|^2 = \\mathrm{tr}\\left(\\X \\w \\v^\\top (\\X \\w \\v^\\top)^\\top\\right) = \\mathrm{tr}(\\X\\w\\w^\\top\\S\\S\\w\\w^\\top\\X^\\top)/(\\w^\\top\\S\\w)^2=\\mathrm{tr}(\\w^\\top\\S\\S\\w)/(\\w^\\top\\S\\w) = \\|\\S\\w\\|^2 / (\\w^\\top\\S\\w).The denominator is equal to  resulting in the formula given above.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2015-01-27 17:54:21","Question_id":8630}
{"_id":{"$oid":"5837a576a05283111e4d36c4"},"Last_activity":"2015-01-28 02:23:33","Creator_reputation":41,"Question_score":4,"Answer_content":"Let the total variance, , in a data set of vectors be the sum of squared errors (SSE) between the vectors in the data set and the mean vector of the data set, T = \\sum_{i} (x_i-\\bar{x}) \\cdot (x_i-\\bar{x}) where  is the mean vector of the data set,  is the ith vector in the data set, and  is the dot product of two vectors.  Said another way, the total variance is the SSE between each  and its predicted value, , when we set .  Now let the predictor of , , be the projection of vector  onto a unit vector . f_c(x_i) = (c \\cdot x_i)c  Then the  for a given  is SSE_c = \\sum_i (x_i - f_c(x_i)) \\cdot (x_i - f_c(x_i))I think that if you choose  to minimize , then  is the first principal component.If instead you choose  to be the normalized version of the vector , then  is the variance in the data described by using  as a predictor.","Display_name":"todddeluca","Creater_id":3864,"Start_date":"2011-03-23 08:34:35","Question_id":8630}
{"_id":{"$oid":"5837a576a05283111e4d36c5"},"Last_activity":"2015-01-28 02:22:26","Creator_reputation":15777,"Question_score":9,"Answer_content":"If we start with the premise that all variables have been centred (standard practice in PCA), then the total variance in the data is just the sum of squares:T=\\sum_{i}(A_{i}^{2}+B_{i}^{2}+C_{i}^{2}+D_{i}^{2}+E_{i}^{2}+F_{i}^{2})This is equal to the trace of the covariance matrix of the variables, which equals the sum of the eigenvalues of the covariance matrix.  This is the same quantity that PCA speaks of in terms of \"explaining the data\" - i.e. you want your PCs to explain the greatest proportion of the diagonal elements of the covariance matrix.  Now if we make this an objective function for a set of predicted values like so:S=\\sum_{i}\\left(\\left[A_{i}-\\hat{A}_{i}\\right]^{2}+\\dots+\\left[F_{i}-\\hat{F}_{i}\\right]^{2}\\right)Then the first principal component minimises  among all rank 1 fitted values .  So it would seem like the appropriate quantity you are after is P=1-\\frac{S}{T}To use your example , we need to turn this equation into rank 1 predictions.  First you need to normalise the weights to have sum of squares 1.  So we replace  (sum of squares ) with .  Next we \"score\" each observation according to the normalised weights:Z_{i}=\\frac{1}{\\sqrt{30}}A_{i}+\\frac{2}{\\sqrt{30}}B_{i}+\\frac{5}{\\sqrt{30}}C_{i}Then we multiply the scores by the weight vector to get our rank 1 prediction.\\begin{pmatrix}\\hat{A}_{i} \\\\\\hat{B}_{i} \\\\\\hat{C}_{i} \\\\\\hat{D}_{i} \\\\\\hat{E}_{i} \\\\\\hat{F}_{i}\\end{pmatrix}=Z_{i}\\times\\begin{pmatrix}\\frac{1}{\\sqrt{30}} \\\\\\frac{2}{\\sqrt{30}} \\\\\\frac{5}{\\sqrt{30}} \\\\0 \\\\0 \\\\0\\end{pmatrix}Then we plug these estimates into  calculate .  You can also put this into matrix norm notation, which may suggest a different generalisation.  If we set  as the  matrix of observed values of the variables ( in your case), and  as a corresponding matrix of predictions.  We can define the proportion of variance explained  as:\\frac{||O||_{2}^{2}-||O-E||_{2}^{2}}{||O||_{2}^{2}}Where  is the Frobenius matrix norm.  So you could \"generalise\" this to be some other kind of matrix norm, and you will get a difference measure of \"variation explained\", although it won't be \"variance\" per se unless it is sum of squares.","Display_name":"probabilityislogic","Creater_id":2392,"Start_date":"2011-08-14 08:21:29","Question_id":8630}
{"_id":{"$oid":"5837a576a05283111e4d36d6"},"Last_activity":"2016-08-24 15:28:25","Creator_reputation":21,"Question_score":2,"Answer_content":"This is a hierarchical or linear mixed model. Be careful about using improper noninformative priors in this context since they often (not always) lead to improper posteriors. See for instance:  Hobert, James P., and George Casella. \"The effect of improper priors on Gibbs sampling in hierarchical linear mixed models.\" Journal of the American Statistical Association 91.436 (1996): 1461-1473.Unfortunately, there is no easy way to sample from the corresponding posterior distribution. Also, you need the conditionals, rather than the posteriors, in order to implement a Gibbs sampler.Some useful are packages for sampling from the posterior associated to this kind of models are spBayes (adaptive Metropolis within Gibbs) and MCMCglmm (block Gibbs sampler).","Display_name":"Omnia","Creater_id":128851,"Start_date":"2016-08-24 15:28:25","Question_id":231545}
{"_id":{"$oid":"5837a576a05283111e4d36e3"},"Last_activity":"2016-08-24 15:19:53","Creator_reputation":1,"Question_score":0,"Answer_content":"As implemented, the learning curve generation code is trying to jointly optimize your parameters and hyperparameters for each size.  In that sense, the learning curve is meant to show the \"best\" it can do at each possible size rather than to hold any particular parameters constant.If you want to hold the parameters constant, you want to manually generate your hyperparameter permutations from one of the grid search iterators and then generate the learning curves inside of a loop over all those hyperparameter permutations.It's a great question though and I agree that it's something that's not discussed enough.  ","Display_name":"Jim K.","Creater_id":71463,"Start_date":"2016-08-24 15:19:53","Question_id":223854}
{"_id":{"$oid":"5837a576a05283111e4d36f0"},"Last_activity":"2016-08-24 15:09:09","Creator_reputation":147374,"Question_score":2,"Answer_content":"It's an unusual question (with no evident applications), but it has been clearly stated, so I will answer it as written.The mean is the expectation.  Expectations for a distribution  can be found by integrating against the survival function .Consider any random variable  whose distribution is supported on the interval  with no jumps at the endpoints (like all Beta distributions).  Then according to the foregoing result,\\mu_{k+1}=\\mathbb{E}(X^{k+1}) = \\int_0^1 (1-F(x))\\left((k+1) x^{k}\\right)dx = 1 - (k+1)\\int_0^1 x^kF(x)dx.Easy algebra gives raw moments of the unnormalized CDF in terms of moments of :\\int_0^1 x^k F(x)dx = \\frac{1}{k+1}\\left(1 - \\mu_{k+1})\\right).\\tag{1}The question asks to find the first normalized moment , which is found by plugging  and  into :m = \\frac{\\int_0^1 xF(x)dx}{\\int_0^1 F(x)dx} = \\frac{(1-\\mu_2)/2}{1-\\mu_1}.\\tag{2}Usually we can look up the central moments.  In terms of them,  is the variance plus the square of the mean .  For the Beta Distribution,\\mu_1 = \\frac{\\alpha}{\\alpha+\\beta}and\\mu_2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} + \\mu_1^2 = \\frac{\\alpha(1+\\alpha)}{(\\alpha+\\beta)(1+\\alpha+\\beta)}.Plug those expressions into  (and simplify if you wish):m = \\frac{1 + 2\\alpha+\\beta}{2(1+\\alpha+\\beta)}= 1 - \\frac{1+\\beta}{2(1+\\alpha+\\beta)} = \\frac{1}{2} + \\frac{\\alpha}{2(1+\\alpha+\\beta)}.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-24 14:54:45","Question_id":231534}
{"_id":{"$oid":"5837a576a05283111e4d36fd"},"Last_activity":"2016-08-24 14:52:51","Creator_reputation":5445,"Question_score":2,"Answer_content":"Is there any particular reason why you use lme and not lmer ? The latter is more recent and better in many situations.You are correct in your interpretation of the intercept. To avoid confusion I will refer to this as the global intercept, since your model also has random intercepts. To be slightly more formal about it, the global intercept is the mean value of Delta when the numerical covariates are zero and the categorical covariates are at their reference level(s), in your case when Treatment is 1, for the \"average\" group (ID). However, each group has it's own intercept, which will be an offset from the global intercept.The estimate for Treatment2 is the difference in the mean of Delta when treatment is 2, compared to when it is 1, for observations within the same group and holding the other covariates constant. Likewise for Treatment3. If you want to compare Treatment2 with Treatment3 then you could recode your data with either of those as the reference level instead, using the relevel function.This is a linear model, so, since the estimate for Date is negative this means there is a linear association between Delta and Date, where the estimate for Date is the slope. Note that in the presence of an interaction, the main effects of the variables do not have the usual interpretation. For numerical variables, the main effect is the association of that variable with the outcome when the other variable is zero. With numerical variables this often makes no sense (when zero is not a plausible and/or when zero falls far outside the observed range), so centering the data makes interpretation more sensible. In your case, the estimate for the interaction is several orders of magnitude smaller than the main effects and not significant at the 0.05 level, so you could consider removing it.Your statement \"Delta is significantly affected by Light and Date\" I would alter to \"There is a statistically significant linear association between Delta and both Light and Date\" and quote the confidence intervals for each estimate. Try to avoid causal language such as \"Y is affected by X\". If you have a research hypothesis that such a causal effect exists then you can say that your model supports this hypothesis. If you think there might be nonlinear associations, then you could investigate this by including higher order terms, such as Date^2 and Date^3 in addition to the linear term.  Plotting the data before modelling it is good practice and often informs possibly non-linearities.You should also check the model assumptions, particularly that the residuals and random intercepts are plausibly normally distributed, homoscedastic, and without autocorrelation.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-24 13:58:13","Question_id":231504}
{"_id":{"$oid":"5837a576a05283111e4d370a"},"Last_activity":"2016-08-24 14:52:05","Creator_reputation":324,"Question_score":2,"Answer_content":"A test for difference between proportions is as follows:Then the test statistic is:Z = \\frac{(\\hat{p}_2 - \\hat{p}_1) - C}{\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}} The following R code ought to work:SE = sqrt((p1*(1-p1)/n1 + p2*(1-p2)/n2))z = ((p1 - p2) - C)/SE1 - pnorm(abs(z))Note: For this test to be appropriate, each sample much be independent and random, and must have at least 10 successes and 10 failures.  It's also problematic when  or  is close to zero.  Thanks to @whuber for pointing out these requirements.","Display_name":"David C","Creater_id":96030,"Start_date":"2016-08-23 14:15:13","Question_id":231357}
{"_id":{"$oid":"5837a576a05283111e4d370b"},"Last_activity":"2016-08-23 13:46:56","Creator_reputation":33186,"Question_score":3,"Answer_content":"The standard formula for testing equality of 2 proportions (using the normal approximation) uses a pooled estimate of the proportion that is appropriate when the null of equal proportions is true.  In your case the proportions are not equal, so the pooled proportion is not appropriate.One option is that you can code the formula that does not pool the proportion, then compute the p-value, etc. from the normal approximation.Another option is to just use prop.test but ignore the p-value part and look to see if the confidence interval includes the C value that you are interested in. If C is not in the interval then that is equivalent to rejecting the null and if C is in the interval then that is equivalent of a p-value greater than alpha (not enough evidence to reject).  You don't get an exact p-value, but you get the same decision.","Display_name":"Greg Snow","Creater_id":4505,"Start_date":"2016-08-23 13:46:56","Question_id":231357}
{"_id":{"$oid":"5837a576a05283111e4d371a"},"Last_activity":"2016-08-24 14:40:38","Creator_reputation":518,"Question_score":0,"Answer_content":"So we suppose that , say =3Now take your original model then add and subtract 3, so that it doesn't change:  We do this `trick' because we want to manipulate the original equation without changing it. Now , since , we know Take our constant  and transform it into We can do this because the constant just absorbs anything in the regression equation.Do the same thing to u and we getEstimating this model is identical to estimating the old one, we've just shifted that 3 around between the error and constant term. The point of the exercise is that we can re-specify it so that is has zero mean, which we want to be able to estimate the constant. ","Display_name":"VCG","Creater_id":124896,"Start_date":"2016-08-24 14:40:38","Question_id":231549}
{"_id":{"$oid":"5837a576a05283111e4d3729"},"Last_activity":"2016-08-24 14:19:14","Creator_reputation":147374,"Question_score":2,"Answer_content":"Focus on the form of the integrand that matches what you can compute.  Specifically, the expectation can be written\\eqalign{\\mathbb{E}(X^k) \u0026amp;= A\\int_0^\\infty x^k(1+x)e^{-\\theta x}\\left(B + (C+Dx)e^{-\\theta x}\\right)\\mathrm{d}x \\\\\u0026amp;=A\\int_0^\\infty\\left((Bx^k + Bx^{k+1})e^{-\\theta x} + (Cx^k + (C+D)x^{k+1} + Dx^{k+2})e^{-2\\theta x}\\right)\\mathrm{d}x}with , , , and .This splits into a sum of five gamma integrals, so we can read off the answer directly: it isA\\left(Bk!\\theta^{-k-1} + B(k+1)!\\theta^{-k-2} + Ck!(2\\theta)^{-k-1} + (C+D)(k+1)!(2\\theta)^{-k-2} + D(k+2)!(2\\theta)^{-k-3}\\right).Factor out  to obtain\\frac{Ak!}{\\theta^{k+2}}\\left(B(\\theta+k+1) + C2^{-k-1}\\theta + (C+D)(k+1)2^{-k-2} + D(k+2)(k+1)2^{-k-3}\\theta^{-1}\\right).The term involving  equals\\frac{k!}{\\theta^k(1+\\theta)}(1-\\lambda)(\\theta+k+1),recognizable in the first part of the answer.  The other terms equal\\frac{k!\\lambda}{\\theta^k(1+\\theta)^2 2^{k+2}}\\left(k^2 + (4\\theta+5)k + 4(1+\\theta)^2\\right).That's not remotely like what the author obtained.  Although I, too, may have made some algebraic mistake, it is obvious that some multiple of  must be involved, because the original integral includes a multiple of , which will introduce  upon integration: that  cannot be canceled off by any of the other terms.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-24 14:19:14","Question_id":231163}
{"_id":{"$oid":"5837a576a05283111e4d3736"},"Last_activity":"2016-08-24 13:58:21","Creator_reputation":15542,"Question_score":0,"Answer_content":"Here's a toy example with negative correlation in the errors in the two equations.Let the outcome  represent immigrant earnings, where  is years of schooling. The participation equation  is the decision to emigrate. We only observe  if . There's an unobserved non-cognitive trait called restlessness that makes emigration more likely, we will represent by . But restlessness can also lower wages, so it is part of the error in the earnings equation, but it enters it negatively, so  and  are negatively correlated.Let's suppose that , so education makes people more likely to leave their home. Folks with low  will only emigrate if they have a high , but people with high  will emigrate with almost any value of . This means that the estimate of the effect of schooling on immigrant income will be too steep since the low education immigrants in our earnings data will be overwhelmingly more restless.  ","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-08-24 12:38:00","Question_id":231374}
{"_id":{"$oid":"5837a576a05283111e4d3743"},"Last_activity":"2016-08-15 06:43:17","Creator_reputation":1058,"Question_score":0,"Answer_content":"You can do this using the tools from generalizability theory. You can think of these as extensions of the ICC that allow for multiple sources of variation (e.g., source of measurement and repeated measures).Brennan, R. L. (2001). Generalizability Theory. New York: Springer-Verlag. Cronbach, L.J., Nageswari, R., \u0026amp; Gleser, G.C. (1963). Theory of generalizability: A liberation of reliability theory. The British Journal of Statistical Psychology, 16, 137-163.Shavelson, R.J., \u0026amp; Webb, N.M. (1991). Generalizability Theory: A Primer. Thousand Oaks, CA: Sage.","Display_name":"Jeffrey Girard","Creater_id":111380,"Start_date":"2016-08-15 06:43:17","Question_id":229443}
{"_id":{"$oid":"5837a576a05283111e4d3750"},"Last_activity":"2016-08-24 13:47:54","Creator_reputation":147374,"Question_score":2,"Answer_content":"You got it right: since the standard deviations are  hours, the variances are  (squared hours).  The sum of the variances is  squared hours and its square root is  hours.  Apparently from this the authors would construct a range by subtracting and adding this to the mean, giving  to  hours.In general, when  independent and equal estimates are summed to equal  and each has  error, then the corresponding means obviously equal  and their variances all equal(\\alpha 1000/n)^2.When  of these are summed and you take the square root you get\\alpha 1000/\\sqrt{n}.Relative to the total of  this equals\\alpha/\\sqrt{n}.In words, this says the relative error in the sum is only  times as large the relative errors in the components.The authors probably proceeded by wondering how many components with  error would have to be involved in order to cut the absolute error down from  to .  Because that's a factor of five reduction, the number of components would have to be  (because ).  That's where the numbers in the example came from.Incidentally, the quoted analysis is ambiguous.  If the parts were not all exactly  of the whole, the answer would be different.  For instance, if one part contributed  (with a  error of  hours) and the other  parts contributed  each, all of them with  error, then it should be clear that only the first part really makes any difference and that the overall error in the sum of the  parts is actually nearly  hours, not  hours.This of course is just common sense rendered mathematically.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-24 13:47:54","Question_id":58155}
{"_id":{"$oid":"5837a576a05283111e4d3751"},"Last_activity":"2013-06-04 14:36:57","Creator_reputation":2476,"Question_score":0,"Answer_content":"Perhaps the problem is trying to point out that with the same relative error (50%) there is a big difference in whether the estimate is made as a whole or in parts. You've already seen the whole example. For the in parts case, you have each estimate is on average 40 hours, with variance of 20*20 = 400. For 25 parts, this would be a total variance of 25(400) = 10000, which gives a standard deviation of 100 hours. Thus the range from 900 to 1100 hours, which is plus or minus one standard deviation. In my opinion this is a poor example presented poorly (the text, not your question), but at least we can understand what they are doing. ","Display_name":"soakley","Creater_id":24073,"Start_date":"2013-06-04 14:36:57","Question_id":58155}
{"_id":{"$oid":"5837a576a05283111e4d3752"},"Last_activity":"2013-05-05 05:37:08","Creator_reputation":1351,"Question_score":0,"Answer_content":"Well, as I see this is just given to you. You suppose it is known from somewhere: \"If the estimate is independently made in 25 parts, each with 50% error\" - it starts from here and just used as a condition in the following sentences.","Display_name":"sashkello","Creater_id":14741,"Start_date":"2013-05-05 05:37:08","Question_id":58155}
{"_id":{"$oid":"5837a576a05283111e4d375f"},"Last_activity":"2013-06-21 04:55:06","Creator_reputation":57702,"Question_score":1,"Answer_content":"Two points:1) Are you aware of new work on mediation by, e.g., MacKinnon? e.g Annual Review Pschology, this web page and this book?Eseentially, MacKinnon treats mediation as existing on a continuum rather than being either present or absent (which is what Baron and Kenny's old article does).2) You can include the covariates in the regression equations and proceed with either approach. There is no need to split the data for the categorical variable. I can't help with SPSS, but it should be straightforward. If you still have SPSS problems with this, however, the better site is StackOverflow. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2013-06-21 04:55:06","Question_id":62247}
{"_id":{"$oid":"5837a576a05283111e4d376c"},"Last_activity":"2016-08-24 13:04:31","Creator_reputation":3354,"Question_score":1,"Answer_content":"It is always hard to assess a priori the performance of a pre-treatment on the data. Even something as simple as normalizing the data does not have an obvious influence on the performance on the later trained classifiers (see per example this post : Normalizing data worsens the performance of CNN?). However the following links may help you implement your idea :Text Classification With Word2Vec the author assesses the performance of various classifiers on text documents, with a word2vec embedding. It happens that the best performance is attained with the \"classical\" linear support vector classifier and a TF-IDF encoding (the approach is really helpful in terms of code, especially if you work with python and sk-learn)Regarding SVMs, there are kernels designed for text. I once had nice results with Information diffusion kernels and TF-IDF encoding. Or you have kernels that works directly on strings : Text Classification using String Kernels, but their implementations are scarcer...","Display_name":"RUser4512","Creater_id":73794,"Start_date":"2016-08-24 12:52:40","Question_id":177955}
{"_id":{"$oid":"5837a576a05283111e4d376d"},"Last_activity":"2015-10-21 13:13:59","Creator_reputation":4111,"Question_score":0,"Answer_content":"The best place to start is with a linear kernel, since this is a) the simplest and b) often works well with text data. You could then try nonlinear kernels such as the popular RBF kernel. ","Display_name":"tdc","Creater_id":7365,"Start_date":"2015-10-21 13:13:59","Question_id":177955}
{"_id":{"$oid":"5837a576a05283111e4d377a"},"Last_activity":"2016-08-24 12:56:44","Creator_reputation":21,"Question_score":2,"Answer_content":"There seems to be a misunderstanding in your question. In SMC, you start with a sample  and you transform it into a sample  which is approximately from the target distribution. So, there is no burn-in, nor correlation, nor chain involved.See:http://www-irma.u-strasbg.fr/~guillou/meeting/cappe.pdfhttp://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/johansen/talks/20090309.pdf","Display_name":"Coach","Creater_id":128830,"Start_date":"2016-08-24 12:56:44","Question_id":231447}
{"_id":{"$oid":"5837a576a05283111e4d3787"},"Last_activity":"2016-08-24 12:56:39","Creator_reputation":1,"Question_score":0,"Answer_content":"Lance, C. E., Butts, M. M., \u0026amp; Michels, L. C. (2006). The sources of four commonly reported cutoff criteria what did they really say?. Organizational research methods, 9(2), 202-220.\"Comparing this section to citations to it, we note several things. First, we suspect that most authors who cite Nunnally’s .70 reliability criterion would not agree that they are trying to save time and energy in an early stage of research by using measures that have only modest reliabilities. Rather, we suspect that most researchers would claim to be conducting basic (orperhaps applied) research, for which purpose Nunnally clearly recommended a reliability standard of .80. Carmines and Zeller (1979) made a similar recommendation: “As a general rule, we believe that reliabilities should not be below.80 for widely used scales” (p. 51). Thus, our second point is that .80, and not .70 as has been attributed, appears to be Nunnally’s recommendedreliability standard for the majority of purposes cited in organizational research.\"","Display_name":"Rob Hartman","Creater_id":128829,"Start_date":"2016-08-24 12:56:39","Question_id":70274}
{"_id":{"$oid":"5837a576a05283111e4d3788"},"Last_activity":"2013-09-17 12:04:04","Creator_reputation":6694,"Question_score":12,"Answer_content":"The following two papers discuss cut-off values for reliability indices:Lance, C.E., Butts, M.M., \u0026amp; Michels, L.C. (2006). The sources of four commonly reported cutoff criteria: What did they really say? Organizational Research Methods, 9 (2), 202-220. Henson, R.K. (2001). Understanding internal consistency reliability estimates: A conceptual primer on coefficient alpha. Measurement and Evaluation in Counseling and Development, 34 (3), 177-189. Strictly speaking neither of them supports the specific scale you describe – the first one in particular is rather critical of the whole idea of a conventional cut-off values – but they do point to many key publications on this topic so digging up those references might bring you to the original sources.Kline (in the 1993 edition of the Handbook cited by Gavin in his answer) traces his cut-off value to Guilford and Nunnally. IIRC, Nunnally never provided much justification for his recommendation and actually changed it from one edition to the next of his Psychometric Theory but his writings have been very influential so he might very well be most responsible of the popularity of the notion that .7 is acceptable and .9 excellent.Incidentally, Cronbach's  is often misinterpreted and has been thoroughly criticized. Even the very idea of aiming for higher internal consistency has been called into question (most notably by Cattell, cf. “bloated specifics”). All that to say that looking for the original source of this or that convention might be of some historical interest but none of this is terribly useful to inform psychological measurement.","Display_name":"Gala","Creater_id":6029,"Start_date":"2013-09-17 11:55:45","Question_id":70274}
{"_id":{"$oid":"5837a576a05283111e4d3789"},"Last_activity":"2013-09-17 09:41:38","Creator_reputation":17409,"Question_score":1,"Answer_content":"Wikipedia cites the sources asGeorge, D., \u0026amp; Mallery, P. (2003). SPSS for Windows step by step: A simple guide and reference. 11.0 update (4th ed.). Boston: Allyn \u0026amp; Bacon.Kline, P. (1999). The handbook of psychological testing (2nd ed.). London: RoutledgeI would follow up those references to see if they cite additional, primary sources. However, as a rule of thumb, these value descriptions may not have a primary source.","Display_name":"Gavin Simpson","Creater_id":1390,"Start_date":"2013-09-17 09:41:38","Question_id":70274}
{"_id":{"$oid":"5837a576a05283111e4d3796"},"Last_activity":"2016-08-24 12:48:10","Creator_reputation":372,"Question_score":0,"Answer_content":"A Matérn covariance matrix with  is almost converging to a Squared Exponential kernel.So I think that a Radial Basis Function (RBF) based approach is perfect in this scenario. It is fast, it works for the kind of black-box function that you have, and you can get measures of uncertainty.You can alternatively use inducing point approximations for GPs, have a look at FITC in the literature, but you have the same problem of where to select the inducing points.","Display_name":"ancillary","Creater_id":72242,"Start_date":"2016-08-24 12:48:10","Question_id":228452}
{"_id":{"$oid":"5837a576a05283111e4d37a3"},"Last_activity":"2016-08-24 12:33:52","Creator_reputation":21578,"Question_score":7,"Answer_content":"The term ``calibration'', as applied to survey weights, appears to have been coined by Deville and Sarndal (1992). They put an umbrella on a bunch of different procedures that used the known population totals:\\sum_{i \\in \\mathcal{U}} Y_i = T_iwhere  is a vector of characteristics known for every unit in the population . For general human populations, these would be census data on demographic characteristics, like age, gender, race/ethnicity, education, geography (regions, states, provinces), and may be income. For establishment populations, these variables typically have to do with establishment size and income. For list samples -- whatever it is that you have attached to your sample.Deville and Sarndal (1992) discussed how to go from design weights (inverse probabilities of selection),  where  is the sample drawn from , to calibrated weights  such that\\sum_{i \\in \\mathcal{S}} w_i y_i = T_ii.e., the sample agrees with the population on these variables. They did this by optimizing a distance function F(w_i,d_i) \\ge 0 \\to \\min, \\quad F(r_i,r_i) = 0, \\mbox{ subject to } \\sum_{i \\in \\mathcal{S}} w_i y_i = T_iTypically, as is often the case in statistics, bringing in additional information improves variances asymptotically, although may throw things off and introduce weird small sample biases. Deville and Sarndal (1992) quantified these asymptotic efficiency gains, which was their central contribution to the literature.In regards of using auxiliary data, survey statistics stands as a pretty unique branch. Bayesian folks utilize auxiliary data in their priors. The i.i.d. frequentists / likelihoodists usually don't have much of a way to incorporate auxiliary information, it seems, as all information must be contained in the likelihood. There is, however, a branch of empirical likelihood estimation where auxiliary information is being used to generate and/or aggregate estimating equations; in fact the empirical likelihood objective functions is one of the objective function cases considered by Deville and Sarndal (1992). (Econometricians should sniff, quite properly, and point out that they have known the ways to calibrate statistical models via generalized method of moments for more than 30 years by now, since Hansen (1982). A quadratic loss is another naturally interesting case in Deville and Sarndal (1992); while it is the easiest to compute, it can give rise to negative weights that are usually considered weird.)Another use of the term ``calibration'' in statistics that I heard of is the reverse regression, in which you have inaccurate measurements of the variable of interest and want to recover the value of the predictor (the running example I was given by my marathon runner of a stats professor was measuring the distance of the course by biking it and counting the revolutions of the bike wheels, vs. more accurate GPS measurements -- that was in the late 1990s era before smarthphones and handheld GPS devices.) You calibrate your bike on an established 1km course, and then try to bike around to get 42 that much.There may be yet other uses. I am not sure dumping them all in one entry is particularly wise though. You indicated factor analysis as one potential user of that term, but I am not particularly well aware of how it is used there.","Display_name":"StasK","Creater_id":5739,"Start_date":"2015-07-27 15:45:50","Question_id":163414}
{"_id":{"$oid":"5837a576a05283111e4d37a4"},"Last_activity":"2016-06-30 12:09:12","Creator_reputation":3636,"Question_score":1,"Answer_content":"There are other uses of the term \"calibration.\" For instance in this CV thread, Frank Harrell discusses it in the context of determining model fit:  The key thing to check first is the model's calibration, either using  the bootstrap to correct for overfitting or using a huge independent  sample not used for model development or fitting.Understanding how good a prediction is, in logistic regressionTypically, in predictive modeling, calibration of a model refers to evaluating the goodness-of-fit (or model accuracy) on the training data, whereas predictive error is evaluated on the test data.","Display_name":"DJohnson","Creater_id":82102,"Start_date":"2016-06-30 12:09:12","Question_id":163414}
{"_id":{"$oid":"5837a576a05283111e4d37a5"},"Last_activity":"2016-06-30 11:59:03","Creator_reputation":1,"Question_score":0,"Answer_content":"Calibration means that you set desired parameters with known good values which would result in expected outcomes. Calibrated values are well established values. The confidence of its values, which as depicted in the calibration curve σ, shows the confidence in the estimated values as deviation from the mean μ. Here, 68% of values are within one standard deviation σ away from the mean; while 95% of the values lie within two sigmas.","Display_name":"BitsInForce","Creater_id":121834,"Start_date":"2016-06-30 11:59:03","Question_id":163414}
{"_id":{"$oid":"5837a576a05283111e4d37a6"},"Last_activity":"2015-07-27 14:52:24","Creator_reputation":417,"Question_score":3,"Answer_content":"say you run a survey and get 1,000 responses.  maybe you conducted your survey via cell phone and older people don't have cell phones at the same rate as younger people.  so 5% of your survey respondents (N=50) were senior citizens but maybe according to the united states census bureau, 15% of americans are actually senior citizens.  and let's say respondent age is going to matter for your survey analysis, for the stuff you ultimately publish.   in order for your 1,000 responses to properly generalize to a more realistic population, you need to give your senior citizens weights of 3x (the weighted N needs to be 150) and scale everyone else's weights down a bit (the nonelderly N=950 should have a weighted N of 850).  calibration, raking, post-stratification are techniques to bring your survey data set closer to some official total.  this is a simple case, but most survey weights are re-computed based on more than just one factor (in this case, age)","Display_name":"Anthony Damico","Creater_id":16939,"Start_date":"2015-07-27 14:52:24","Question_id":163414}
{"_id":{"$oid":"5837a576a05283111e4d37b3"},"Last_activity":"2016-08-24 12:31:15","Creator_reputation":885,"Question_score":2,"Answer_content":"Your approach seems valid for me. However,  it does not consider conditional or interdependent relationship of variables within every group. Your are building a regression model for every individual feature rather than considering a specific combination. In simple terms, a variable that is not significant by itself can be significant when considered with another feature.Generally, there are many sound methods for feature selection including filter and wrapper models. You may use some ranking method such as mRMR to rank your variables. Then, take the top k ranked features as well as the ones you deem relevant to include. Regarding the wrapper model, you may optimize the performance of your regression models using any search method such as GA. For preserving the selection of the admired featuers, you can add constraints to the optimization process. This way will lead to selection of variables that optimize the performance given some other featuers fixed as intrepretable ones.To interpret the association or to measure \"relevance\" of any selected variable, various metrics can be applied based on your definition of relevance (there is no single agreed upon definition of relevance). Once again, you may use any correlation or mutual information based metric e.g. mRMR, JMI and ICAP.","Display_name":"soufanom","Creater_id":14888,"Start_date":"2013-02-28 03:46:44","Question_id":50960}
{"_id":{"$oid":"5837a576a05283111e4d37c0"},"Last_activity":"2016-08-24 12:28:42","Creator_reputation":372,"Question_score":0,"Answer_content":"Have you tried Gaussian Processes?A random function  is said to be a Gaussian Process iff(x) \\sim GP(m(x), k(x, x')), \\quad x, x' \\in \\mathbb R^p.The mean function  is the location parameter and the kernel function represents the covariance between two outputs as function of the inputs:k(x, x') = \\mathrm{cov}[f(x), f(x')].You can encode your prior information about the dependence of  on the previous value in the kernel function.This means that you have to assign a higher correlation value when the points are neighbouring.The Markovian kernel is the Exponential kernel (or Ornstein-Uhlenbeck).The GP will learn itself the nonlinear dependence in the output space.If you are not familiar with the model, you need to have a look at Kevin Murphy, Machine Learning A Probabilistic Perspective, MIT - chapter 15.","Display_name":"ancillary","Creater_id":72242,"Start_date":"2016-08-24 12:28:42","Question_id":230936}
{"_id":{"$oid":"5837a576a05283111e4d37cd"},"Last_activity":"2016-08-24 12:20:05","Creator_reputation":2186,"Question_score":1,"Answer_content":"I'm assuming all training/CV/test performance is bad and thereby that the problem is not overfitting. In a nutshell, you then could try the following to meaningfully reduce your features:Use feature correlation to reduce correlated features,Feature selection techniques such as feature filters and feature wrappers,Feature reduction with using techniques like PCA, orModels that internally \"weight\" features themselves.Things you should consider: As @KeithHughitt mentioned, the problem might be that the relation you seek is simply not present in your data. In such a case it might be impossible for models to perform and generalize well. The \"one perfect\" solution for those cases does not exist, but, as you already mentioned, deriving features (same information but differently processed) and/or adding information (new information, e.g. with recording more features) might help. Another explanation for bad predictive performance with big data/many features might be: the feature-target relation is too complex to be represented accordingly by your model (e.g. trying to model circular data with a linear model). In such cases, another option besides adding preproceesing/feature derivation would be to employ more complex models. But those usually come at the cost of increased calculation power, such as with deep learning.","Display_name":"geekoverdose","Creater_id":112731,"Start_date":"2016-08-24 12:20:05","Question_id":231415}
{"_id":{"$oid":"5837a576a05283111e4d37df"},"Last_activity":"2016-08-24 11:54:13","Creator_reputation":240,"Question_score":1,"Answer_content":"Checking your gradient against Finite differences method aka numerical differentiation is a good way to find errors.See this: https://en.wikipedia.org/wiki/Numerical_differentiation","Display_name":"Adi","Creater_id":22545,"Start_date":"2016-08-24 11:54:13","Question_id":231410}
{"_id":{"$oid":"5837a576a05283111e4d37e0"},"Last_activity":"2016-08-24 11:20:16","Creator_reputation":1113,"Question_score":2,"Answer_content":"When I was learning SGD, this[1] left me with a strong impression and I quote Leon Bottou as possible/alternative answer/solution to your question,\"During the last twenty years, I have often been approached for advice in setting the learning rates  of some rebellious stochastic gradient descent program. My advice is to forget about the learning rates and check that the gradients are computed correctly. This reply is biased because people who compute the gradients correctly quickly find that setting small enough learning rates is easy. Those who ask usually have incorrect gradients. Carefully checking each line of the gradient computation code is the wrong way to check the gradients. Use finite differences....\" (page 8, [1])[1] Bottou, L. (2012). Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade (pp. 421-436). Springer Berlin Heidelberg.","Display_name":"horaceT","Creater_id":98646,"Start_date":"2016-08-24 11:20:16","Question_id":231410}
{"_id":{"$oid":"5837a576a05283111e4d37e1"},"Last_activity":"2016-08-24 08:18:48","Creator_reputation":5787,"Question_score":3,"Answer_content":"There is something \"wrong\" with the algorithm, gradient descent (its very name is an example of false advertising). That doesn't mean there is a mistake in your implementation.Gradient descent, i.e., steepest descent without use of line search or trust regions, need not converge, and can actually diverge, even on a strictly convex function, even in one dimension.For example, apply gradient descent to the function . Its gradient is .  Apply gradient descent with a learning rate of ; this is the classic steepest descent, implemented without the protection afforded by line search or trust regions.  Start at . The gradient is , so the next point is . Its gradient is , so the next point is . its gradient is , so the next point is .   Hmm, this seems to be diverging.  On a nice strictly convex function. Gradient descent has actually achieved monotonic increase in the objective function.So yes, there is no guarantee gradient descent will descend on every iteration.  And even with a smaller learning rate, it may not necessarily be small enough to ensure descent.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-24 07:39:20","Question_id":231410}
{"_id":{"$oid":"5837a576a05283111e4d37f0"},"Last_activity":"2016-08-24 10:59:05","Creator_reputation":12907,"Question_score":0,"Answer_content":"  order P = 2. So it has to cut off at lag 2 in PACF plot. but in PACF lag 2 didn't cut off. For a purely autoregressive process AR(), it is ACF (not PACF) that cuts off after  lags. (For a reference check any time series textbook. You can also simulate some AR() processes to see the phenomenon empirically.)  Is it ok to force the model based on those assumptions?If you find significant parameters and white-noise, normal residuals there is little more to ask. There does not seem to be a fault with the model based on the current information. However, the nice results might also be due to chance (plausible when fitting a large number of candidate models and selecting the best-fitting one). Then you would need to sample more data and examine the model performance there (estimate on the original sample, evaluate on the new sample). This way you could see whether the fit is not just spuriously good.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-24 10:59:05","Question_id":231328}
{"_id":{"$oid":"5837a576a05283111e4d37ff"},"Last_activity":"2016-08-24 10:23:59","Creator_reputation":104,"Question_score":1,"Answer_content":"On Uniformly Minimum Variance Unbiased Estimation when no Complete Sufficient Statistics Exist by L. Bondesson gives some examples of UMVUEs which are not complete sufficient statistics, including the following one:Let  be independent observations of a random variable , where  and  are unknown, and  is gamma distributed with known shape parameter  and known scale parameter . Then  is the UMVUE of . However, when  then there is no complete sufficient statistic for .","Display_name":"David Robertson","Creater_id":128762,"Start_date":"2016-08-24 10:23:59","Question_id":167373}
{"_id":{"$oid":"5837a576a05283111e4d3800"},"Last_activity":"2015-08-21 14:56:02","Creator_reputation":111,"Question_score":1,"Answer_content":"Let us show that there can be a UMVUE which is not a sufficient statistic. First of all, if the estimator  takes (say) value  on all samples, then clearly  is a UMVUE of , which latter can be considered a (constant) function of . On the other hand, this estimator  is clearly not sufficient in general. It is a bit harder to find a UMVUE  of the \"entire\" unknown parameter  (rather than a UMVUE of a function of it) such that  is not sufficient for . E.g., suppose the \"data\" are given just by one normal r.v. , where  is unknown. Clearly,  is sufficient and complete for . Let  if  and  if , and let; as usual, we denote by  and , respectively, the cdf and pdf of .So, the estimator  is unbiased for  and is a function of the complete sufficient statistic . Hence,  is a UMVUE of . On the other hand, the function  is continuous and strictly increasing on , from  to . So, the correspondence  is a bijection. That is, we can re-parametirize the problem, from  to , in a one-to-one manner. Thus,  is a UMVUE of , not just for the \"old\" parameter , but for the \"new\" parameter  as well. However,  is not sufficient for  and therefore not sufficient for . Indeed, \\begin{multline*}\\mathsf{P}_\\tau(X\u0026lt;-1|Y=0)=\\mathsf{P}_\\tau(X\u0026lt;-1|X\u0026lt;0)=\\frac{\\mathsf{P}_\\tau(X\u0026lt;-1)}{\\mathsf{P}_\\tau(X\u0026lt;0)} \\\\=\\frac{\\Phi(-\\tau-1)}{\\Phi(-\\tau)}\\sim\\frac{\\varphi(-\\tau-1)/(\\tau+1)}{\\varphi(-\\tau)/\\tau}\\sim\\frac{\\varphi(-\\tau-1)}{\\varphi(-\\tau)}=e^{-\\tau-1/2} \u0009\\end{multline*}as ; here we used the known asymptotic equivalence  as , which follows by the l'Hospital rule. So,  depends on  and hence on , which shows that  is not sufficient for  (whereas  is a UMVUE for ). ","Display_name":"Iosif Pinelis","Creater_id":86566,"Start_date":"2015-08-21 14:56:02","Question_id":167373}
{"_id":{"$oid":"5837a576a05283111e4d380d"},"Last_activity":"2012-09-11 06:40:54","Creator_reputation":19535,"Question_score":0,"Answer_content":"If you use the term \"clustering\" in the sense of \"near duplicate detection\", the online updating of IR duplicate indexes is an obvious candidate.Think of Google image search that wants to merge duplicate images as they are spidered and coming into the index (instead of bulk-rebuilding the index, as everybody used to do).If you use a broader term of clustering, such as \"related but not identical objects\" it is a bit harder to find something. Try to think of actual data sources that produce a stream of images that is not continuous video...","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2012-09-11 06:40:54","Question_id":36082}
{"_id":{"$oid":"5837a576a05283111e4d3819"},"Last_activity":"2014-09-15 12:03:24","Creator_reputation":1605,"Question_score":68,"Answer_content":"We can find different Resampling methods, or loosely called \"simulation\" methods, that depend upon resampling or shuffling of the samples. There might be differences in opinions with respect to proper terminology, but the following discussion tries to generalize and simplify what is available in the appropriate literature:Resampling methods are used in (1) estimating precision / accuracy of sample statistics through using subset of data (e.g. Jackknifing) or drawing randomly with replacement from a set of data points (e.g. bootstrapping) (2)  Exchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests) (3) Validating models by using random subsets (bootstrapping, cross validation) (see wikipedia: resampling methods)BOOTSTRAPING \"Bootstrapping is a statistical method for estimating the sampling distribution of an estimator by sampling with replacement from the original sample\". The method assigns measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates. The basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by resampling the sample data and performing inference on (resample → sample). As the population is unknown, the true error in a sample statistic against its population value is unknowable. In bootstrap-resamples, the 'population' is in fact the sample, and this is known; hence the quality of inference from resample data → 'true' sample is measurable.\" see wikipedia Yvar \u0026lt;- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)#To generate a single bootstrap samplesample(Yvar, replace = TRUE)  #generate 1000 bootstrap samplesboot \u0026lt;-list()for (i in 1:1000)    boot[[i]] \u0026lt;- sample(Yvar,replace=TRUE)In univariate problems, it is usually acceptable to resample the individual observations with replacement (\"case resampling\"). Here we resample the data with replacement, and the size of the resample must be equal to the size of the original data set.In regression problems,   case resampling refers to the simple scheme of resampling individual cases - often rows of a data set in regression problems, the explanatory variables are often fixed, or at least observed with more control than the response variable. Also, the range of the explanatory variables defines the information available from them. Therefore, to resample cases means that each bootstrap sample will lose some information (see Wikipedia). So it will be logical to sample rows of the data rather just Yvar. Yvar \u0026lt;- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)Xvar \u0026lt;- c(rep(\"A\", 5),  rep(\"B\", 5),    rep(\"C\", 5))mydf \u0026lt;- data.frame (Yvar, Xvar)    boot.samples \u0026lt;- list()for(i in 1:10) {   b.samples.cases \u0026lt;- sample(length(Xvar), length(Xvar), replace=TRUE)    b.mydf \u0026lt;- mydf[b.samples.cases,]    boot.samples[[i]] \u0026lt;- b.mydf}str(boot.samples) boot.samples[1]You can see some cases are repeated as we are sampling with replacement.\"Parametric bootstrap -  a parametric model is fitted to the data, often by maximum likelihood, and samples of random numbers are drawn from this fitted model. Usually the sample drawn has the same sample size as the original data. Then the quantity, or estimate, of interest is calculated from these data. This sampling process is repeated many times as for other bootstrap methods. The use of a parametric model at the sampling stage of the bootstrap methodology leads to procedures which are different from those obtained by applying basic statistical theory to inference for the same model.\"(see Wikipedia). The following is parametric bootstrap with normal distribution assumption with mean and standard deviation parameters. Yvar \u0026lt;- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)# parameters for Yvar mean.y \u0026lt;- mean(Yvar)sd.y \u0026lt;- sd(Yvar)#To generate a single bootstrap sample with assumed normal distribution (mean, sd)rnorm(length(Yvar), mean.y, sd.y) #generate 1000 bootstrap samplesboot \u0026lt;-list()for (i in 1:1000)    boot[[i]] \u0026lt;- rnorm(length(Yvar), mean.y, sd.y)There are other variants of bootstrap, please consult the wikipedia page or any good statical book on resampling. JACKNIFE\"The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size N, the jackknife estimate is found by aggregating the estimates of each N − 1 estimate in the sample.\" see: wikipedia The following shows how to jackknife the Yvar. jackdf \u0026lt;- list()jack \u0026lt;- numeric(length(Yvar)-1)for (i in 1:length (Yvar)){for (j in 1:length(Yvar)){     if(j \u0026lt; i){             jack[j] \u0026lt;- Yvar[j]}  else if(j \u0026gt; i) {              jack[j-1] \u0026lt;- Yvar[j]}}jackdf[[i]] \u0026lt;- jack}jackdf\"the regular bootstrap and the jackknife, estimate the variability of a statistic from the variability of that statistic between subsamples, rather than from parametric assumptions. For the more general jackknife, the delete-m observations jackknife, the bootstrap can be seen as a random approximation of it. Both yield similar numerical results, which is why each can be seen as approximation to the other.\" See this question on Bootstrap vs Jacknife. RANDOMIZATION TESTS\"In parametric tests we randomly sample from one or more populations. We make certain assumptions about those populations, most commonly that they are normally distributed with equal variances. We establish a null hypothesis that is framed in terms of parameters, often of the form m1  -m2 = 0 . We use our sample statistics as estimates of the corresponding population parameters, and calculate a test statistic (such as a t test). For example: in  Student's t - test for differences in means when variances are unknown, but are considered to be equal.  The hypothesis of interest is that  H0: m1 = m2. One of alternative hypothesis would be stated as : HA: m1 \u0026lt; m2.Given two samples drawn from populations 1 and 2, assuming that these are normally distributed populations with equal variances, and that the samples were drawn independently and at random from each population, then a statistic whose distribution is known can be elaborated to test H0.One way to avoid these distributional assumptions has been the approach now called non - parametric, rank - order, rank - like, and distribution - free statistics. These distribution - free statistics are usually criticized for being less \"efficient\" than the analogous test based on assuming the populations to be normally distributed. Another alternative approach is randomization approach - \"process of randomly assigning ranks to observations independent of one's knowledge of which sample an observation is a member.  A randomization test makes use of such a procedure, but does so by operating on the observations rather than the joint ranking of the observations.  For this reason, the distribution of an analogous statistic (the sum of the observations in one sample) cannot be easily tabulated, although it is theoretically possible to enumerate such a distribution\" (see)Randomization tests differ from parametric tests in almost every respect. (1) There is no requirement that we have random samples from one or more populations—in fact we usually have not sampled randomly. (2) We rarely think in terms of the populations from which the data came, and there is no need to assume anything about normality or homoscedasticity (3) Our null hypothesis has nothing to do with parameters, but is phrased rather vaguely, as, for example, the hypothesis that the treatment has no effect on the how participants perform.(4)  Because we are not concerned with populations, we are not concerned with estimating (or even testing) characteristics of those populations (5) We do calculate some sort of test statistic, however we do not compare that statistic to tabled distributions. Instead, we compare it to the results we obtain when we repeatedly randomize the data across the groups, and calculate the corresponding statistic for each randomization. (6) Even more than parametric tests, randomization tests emphasize the importance of random assignment of participants to treatments.\" see. The type of randomization test that is very popular is permutation test. If our sample size is 12 and 5, the total permutation possible is C(12,5) = 792. If our sample sizes been 10 and 15 then over 3.2 million arrangements would have been possible. This is computing challenge: What then?  Sample.  When the universe of possible arrangements is too large to enumerate why not sample arrangements from this universe independently and at random?  The distribution of the test statistic over this series of samples can then be tabulated, its' mean and variance computed, and the error rate associated with an hypothesis test estimated.PERMUTATION TESTAccording to wikipedia \"A permutation test (also called a randomization test, re-randomization test, or an exact test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points. Permutation tests exist for any test statistic, regardless of whether or not its distribution is known. Thus one is always free to choose the statistic which best discriminates between hypothesis and alternative and which minimizes losses.\"The difference between permutation and bootstrap is that bootstraps sample with replacement, and permutations sample without replacement. In either case, the time order of the observations is lost and hence volatility clustering is lost — thus assuring that the samples are under the null hypothesis of no volatility clustering. The permutations always have all of the same observations, so they are more like the original data than bootstrap samples. The expectation is that the permutation test should be more sensitive than a bootstrap test. The permutations destroy volatility clustering but do not add any other variability.See the question on permutation vs bootstrapping - \"The permutation test is best for testing hypotheses and bootstrapping is best for estimating confidence intervals\". So to perform permutation in this case we can just change replace = FALSE in the above bootstrap example.Yvar \u0026lt;- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)     #generate 1000 bootstrap samples       permutes \u0026lt;-list()    for (i in 1:1000)        permutes[[i]] \u0026lt;- sample(Yvar,replace=FALSE)In case of more than one variable, just picking of the rows and reshuffling the order will not make any difference as the data will remain same. So we reshuffle the y variable. Something what you have done, but I do not think we do not need double reshuffling of both x and y variables (as you have done).  Yvar \u0026lt;- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)Xvar \u0026lt;- c(rep(\"A\", 5),  rep(\"B\", 5),    rep(\"C\", 5))mydf \u0026lt;- data.frame (Yvar, Xvar) permt.samples \u0026lt;- list()    for(i in 1:10) {       t.yvar \u0026lt;- Yvar[ sample(length(Yvar), length(Yvar), replace=FALSE) ]       b.df \u0026lt;- data.frame (Xvar, t.yvar)        permt.samples[[i]] \u0026lt;- b.df     }    str(permt.samples)    permt.samples[1]MONTE CARLO METHODS \"Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results; typically one runs simulations many times over in order to obtain the distribution of an unknown probabilistic entity. The name comes from the resemblance of the technique to the act of playing and recording results in a real gambling casino. \" see Wikipedia\"In applied statistics, Monte Carlo methods are generally used for two purposes:(1) To compare competing statistics for small samples under realistic data conditions. Although Type I error and power properties of statistics can be calculated for data drawn from classical theoretical distributions (e.g., normal curve, Cauchy distribution) for asymptotic conditions (i. e, infinite sample size and infinitesimally small treatment effect), real data often do not have such distributions.(2) To provide implementations of hypothesis tests that are more efficient than exact tests such as permutation tests (which are often impossible to compute) while being more accurate than critical values for asymptotic distributions.Monte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate randomization test is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice – or more frequently—for the efficiency of not having to track which permutations have already been selected).\"Both MC and Permutation test are sometime collectively called randomization tests. The difference is in MC we sample the permutation samples, rather using all possible combinations see.CROSS VALIDATION The idea beyond cross validation is that models should be tested with data that were not used to fit the model. Cross validation is perhaps most often used in the context of prediction. \"Cross-validation is a statistical method for validating a predictive model. Subsets of the data are held out for use as validating sets; a model is fit to the remaining data (a training set) and used to predict for the validation set. Averaging the quality of the predictions across the validation sets yields an overall measure of prediction accuracy.One form of cross-validation leaves out a single observation at a time; this is similar to the jackknife. Another, K-fold cross-validation, splits the data into K subsets; each is held out in turn as the validation set.\" see Wikipedia . Cross validation is usually done with quantitative data. You can convert your qualitative (factor data) to quantitative someway to fit a linear model and test this model. The following is simple hold-out strategy where 50% of data is used for model prediction while rest is used for testing. Lets assume Xvar is also quantitative variable.     Yvar \u0026lt;- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)    Xvar \u0026lt;- c(rep(1, 5),  rep(2, 5),    rep(3, 5))    mydf \u0026lt;- data.frame (Yvar, Xvar)    training.id \u0026lt;- sample(1:nrow(mydf), round(nrow(mydf)/2,0), replace = FALSE)    test.id \u0026lt;- setdiff(1:nrow(mydf), training.id)   # training dataset     mydf.train \u0026lt;- mydf[training.id]    #testing dataset     mydf.test \u0026lt;- mydf[test.id]Unlike bootstrap and permutation tests the cross-validation dataset for training and testing is different. The following figure shows a summary of resampling in different methods.Hope this helps a bit.","Display_name":"rdorlearn","Creater_id":19762,"Start_date":"2014-06-25 09:47:22","Question_id":104040}
{"_id":{"$oid":"5837a576a05283111e4d381a"},"Last_activity":"2014-06-25 03:01:47","Creator_reputation":151,"Question_score":4,"Answer_content":"Here's my contribution.DataYvar \u0026lt;- c(8,9,10,13,12,          14,18,12,8,9,          1,3,2,3,4)Xvar \u0026lt;- rep(LETTERS[1:3], each=5)mydf \u0026lt;- data.frame(Yvar, Xvar)Monte CarloI see Monte Carlo as a method to obtain a distribution of an (outcome) random variable, which is the result of a nontrivial function of other (input) random variables.  I don't immediately see an overlap with the current ANOVA analysis, probably other forum members can give their input here.BootstrappingThe purpose is to have an idea of the uncertainty of a statistic calculated from an observed sample.  For example: we can calculate that the sample mean of Yvar is 8.4, but how certain are we of the population mean for Yvar?  The trick is to do as if the sample is the population, and sample many times from that fake population.n \u0026lt;- 1000bootstrap_means \u0026lt;- numeric(length=n)for(i in 1:n){   bootstrap_sample \u0026lt;- sample(x=Yvar, size=length(Yvar), replace=TRUE)   bootstrap_means[i] \u0026lt;- mean(bootstrap_sample)}hist(bootstrap_means)We just took samples and didn't assume any parametric distribution.  This is the nonparametric bootstrap.  If you would feel comfortable with assuming for example that Xvar is normally distributed, you can also sample from a normal distribution (rnorm(...)) using the estimated mean and standard deviation, this would be the parametric bootstrap.Other users might perhaps give applications of the bootstrap with respect to the effect sizes of the Xvar levels?JackknifingThe jackknife seems to be a bit outdated.  Just for completeness, you could compare it more or less to the bootstrap, but the strategy is here to see what happens if we leave out one observation (and repeat this for each observation).Cross-validationIn cross-validation, you split your (usually large) dataset in a training set and a validation set, to see how well your estimated model is able to predict the values in the validation set.  I personally haven't seen yet an application of cross-validation to ANOVA, so I prefer to leave this part to others.Randomization/permutation testsBe warned, terminology is not agreed upon.  See Difference between Randomization test and Permutation test.The null hypothesis would be that there is no difference between the populations of groups A, B and C, so it shouldn't matter if we randomly exchange the labels of the 15 values of Xvar.  If the originally observed F value (or another statistic) doesn't agree with those obtained after randomly exchanging labels, then it probably did matter, and the null hypothesis can be rejected.observed_F_value \u0026lt;- anova(lm(Yvar ~ Xvar))\"F value\"[1]}hist(permutation_F_values, xlim=range(c(observed_F_value, permutation_F_values)))abline(v=observed_F_value, lwd=3, col=\"red\")cat(\"P value: \", sum(permutation_F_values \u0026gt;= observed_F_value), \"/\", n, \"\\n\", sep=\"\")Be careful with the way you reassign the labels in the case of complex designs though.  Also note that in the case of unequal variances, the null hypothesis of exchangeability is not true in the first place, so this permutation test wouldn't be correct.Here we did not explicitly go through all possible permutations of the labels, this is a Monte Carlo estimate of the P-value.  With small datasets you can go through all possible permutations, but the R-code above is a bit easier to understand.","Display_name":"lgbi","Creater_id":5750,"Start_date":"2014-06-25 03:01:47","Question_id":104040}
{"_id":{"$oid":"5837a576a05283111e4d3827"},"Last_activity":"2016-08-24 03:01:42","Creator_reputation":584,"Question_score":1,"Answer_content":"Permutation tests do resampling from the observed units in the dataset, or in the subsample selected (if using if statements). As such:they could perfectly accommodate frequency weights [fweight]. First, the command should internally expand the sample (process which is unequivocal, as there is just one expanded sample). Then, it should do permutation from that expanded sample. THerefore, it is a limitation of the command and not of the theory that frequency weights are not allowed. Actually, a trivial workaround in this setting is to expand the sample yourself (using expand fweight_varname) and then run the permutation test. If you do this, you can also use preserve...restore to avoid creating a huge dataset. In any case, this confirms that the permute test could easily implement frequency weights.in principle, they could also accommodate sampling weights [pweight]. The difference is more theoretical than anything. Sampling weights often come from a probabilistic analysis (e.g. a regression), and as such, they are random variables, with a distribution. In consequence, it is quite a strong assumption that those weights do in fact represent with certainty a given population, which is the case under permutation tests. However, this could again be easily implemented, as informally you can transform pweight into fweight (which is hardly a good advice anyway). I imagine this is why permute does not accept sampling weights.Note however that the fact that force is available might be an indication that Stata programmers know these options are possible, but do not want to assert a method that they might think is not always robust, leaving to the researcher the task of proving that robustness. Interestingly, the same is true for bootstrap. It \"does not\" accept weights, although the force option is there. ","Display_name":"luchonacho","Creater_id":100369,"Start_date":"2016-08-24 03:01:42","Question_id":230720}
{"_id":{"$oid":"5837a576a05283111e4d3834"},"Last_activity":"2015-11-11 09:52:04","Creator_reputation":111,"Question_score":0,"Answer_content":"Just adding another approach, ezPerm of ez package:\u0026gt; # preparing the data\u0026gt; DV \u0026lt;- c(x1, y1)\u0026gt; IV \u0026lt;- factor(rep(c(\"A\", \"B\"), c(length(x1), length(y1))))\u0026gt; id \u0026lt;- factor(rep(1:length(x1), 2))\u0026gt; df \u0026lt;- data.frame(id=id,DV=DV,IV=IV)\u0026gt;\u0026gt; library(ez)\u0026gt; ezPerm( data = df, dv = DV, wid = id, within = IV, perms = 1000)|=========================|100%              Completed after 17 s   Effect     p p\u0026lt;.051     IV 0.016     *This seems to be consistent to the oneway_test of the coin package:\u0026gt; library(coin)\u0026gt; pvalue(oneway_test(DV ~ IV | id,  distribution=approximate(B=999999)))[1] 0.0160800299 percent confidence interval: 0.01575782 0.01640682However, notice that this is not the same example provided by @caracal. In his example, he includes alternative=\"greater\", therefore the difference in p-values ~0.008 vs ~0.016.The aovp package suggested in one of the answers produce suspiciously lower p-values, and runs suspiciously fast even when I try high values for the Iter, Ca and maxIter arguments:library(lmPerm)summary(aovp(DV ~ IV + Error(id/IV), data=df,  maxIter = 1000000000))summary(aovp(DV ~ IV + Error(id/IV), data=df,  Iter = 1000000000))summary(aovp(DV ~ IV + Error(id/IV), data=df,  Ca = 0.00000000001))That said, the arguments seems to be slightly reducing the variations of p-values from ~.03 and ~.1 (I got a bigger range thant the reported by @Henrik), to 0.03 and 0.07. ","Display_name":"toto_tico","Creater_id":37500,"Start_date":"2015-11-10 18:37:42","Question_id":6127}
{"_id":{"$oid":"5837a576a05283111e4d3835"},"Last_activity":"2012-08-25 19:16:08","Creator_reputation":51,"Question_score":5,"Answer_content":"My comments are not about implementation of the permutation test but about the more general issues raised by these data and the discussion of it, in particular the post by G. Jay Kerns.The two distributions actually look quite similar to me EXCEPT for the group of 0s in Y1, which are much different from the other observations in that sample (next smallest is about 50 on the 0-100 scale) as well as all those in X1. I would first investigate whether there was anything different about those observations.Second, assuming those 0s do belong in the analysis, saying the permutation test isn't valid because the distributions appear to differ begs the question. If the null were true (distributions are identical), could you (with reasonable probability) get distributions looking as different as these two? Answering that's the whole point of the test, isn't it? Maybe in this case some will consider the answer obvious without running the test, but with these smallish, peculiar distributions, I don't think I would. ","Display_name":"Andrew Taylor","Creater_id":13598,"Start_date":"2012-08-25 19:16:08","Question_id":6127}
{"_id":{"$oid":"5837a576a05283111e4d3836"},"Last_activity":"2012-05-24 09:44:39","Creator_reputation":7692,"Question_score":3,"Answer_content":"As this question did pop up again, I may add another answer inspired by a recent blog post via R-Bloggers from Robert Kabacoff, the author of Quick-R and R in Action using the lmPerm package.However, this methods produces sharply contrasting (and very unstable) results to the one produced by the coin package in the answer of @caracakl (the p-value of the within-subjects analysis is 0.008). The analysis takes the data preparation from @caracal's answer as well:x1 \u0026lt;- c(99, 99.5, 65, 100, 99, 99.5, 99, 99.5, 99.5, 57, 100, 99.5,         99.5, 99, 99, 99.5, 89.5, 99.5, 100, 99.5)y1 \u0026lt;- c(99, 99.5, 99.5, 0, 50, 100, 99.5, 99.5, 0, 99.5, 99.5, 90,         80, 0, 99, 0, 74.5, 0, 100, 49.5)DV \u0026lt;- c(x1, y1)IV \u0026lt;- factor(rep(c(\"A\", \"B\"), c(length(x1), length(y1))))id \u0026lt;- factor(rep(1:length(x1), 2)) library(lmPerm)summary(aovp( DV ~ IV + Error(id)))produces:\u0026gt; summary(aovp( DV ~ IV + Error(id)))[1] \"Settings:  unique SS \"Error: idComponent 1 :          Df R Sum Sq R Mean SqResiduals 19    15946       839Error: WithinComponent 1 :          Df R Sum Sq R Mean Sq Iter Pr(Prob)  IV         1     7924      7924 1004    0.091 .Residuals 19    21124      1112                ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 If you run this multiple times, the p-values jumps around between ~.05 and ~.1.Although it is an answer to the question let me allow to pose a question at the end (I can move this to a new question if desired):Any ideas of why this analysis is so unstable and does produce a so diverging p-values to the coin analysis? Did I do something wrong?","Display_name":"Henrik","Creater_id":442,"Start_date":"2012-05-24 09:44:39","Question_id":6127}
{"_id":{"$oid":"5837a576a05283111e4d3837"},"Last_activity":"2011-01-10 11:49:31","Creator_reputation":8181,"Question_score":33,"Answer_content":"It shouldn't matter that much since the test statistic will always be the difference in means (or something equivalent). Small differences can come from the implementation of Monte-Carlo methods. Trying the three packages with your data with a one-sided test for two independent variables:\u0026gt; DV \u0026lt;- c(x1, y1)\u0026gt; IV \u0026lt;- factor(rep(c(\"A\", \"B\"), c(length(x1), length(y1))))\u0026gt; library(coin)                    # for oneway_test(), pvalue()\u0026gt; pvalue(oneway_test(DV ~ IV, alternative=\"greater\",+                    distribution=approximate(B=9999))[1] 0.00330033\u0026gt; library(perm)                    # for permTS()\u0026gt; permTS(DV ~ IV, alternative=\"greater\", method=\"exact.mc\",+        control=permControl(nmc=10^4-1))p.value[1] 0.003171822To check the exact p-value with a manual calculation of all permutations, I'll restrict the data to the first 9 values.\u0026gt; x1 \u0026lt;- x1[1:9]\u0026gt; y1 \u0026lt;- y1[1:9]\u0026gt; DV \u0026lt;- c(x1, y1)\u0026gt; IV \u0026lt;- factor(rep(c(\"A\", \"B\"), c(length(x1), length(y1))))\u0026gt; pvalue(oneway_test(DV ~ IV, alternative=\"greater\", distribution=\"exact\"))[1] 0.0945907\u0026gt; permTS(DV ~ IV, alternative=\"greater\", exact=TRUE)p.value[1] 0.1029412# manual exact permutation test\u0026gt; idx  \u0026lt;- seq(along=DV)                 # indices to permute\u0026gt; idxA \u0026lt;- combn(idx, length(x1))        # all possibilities for different groups# function to calculate difference in group means given index vector for group A\u0026gt; getDiffM \u0026lt;- function(x) { mean(DV[x]) - mean(DV[!(idx %in% x)]) }\u0026gt; resDM    \u0026lt;- apply(idxA, 2, getDiffM)  # difference in means for all permutations\u0026gt; diffM    \u0026lt;- mean(x1) - mean(y1)       # empirical differencen in group means# p-value: proportion of group means at least as extreme as observed one\u0026gt; (pVal \u0026lt;- sum(resDM \u0026gt;= diffM) / length(resDM))[1] 0.0945907coin and exactRankTests are both from the same author, but coin seems to be more general and extensive - also in terms of documentation. exactRankTests is not actively developed anymore. I'd therefore choose coin (also because of informative functions like support()), unless you don't like to deal with S4 objects.EDIT: for two dependent variables, the syntax is\u0026gt; id \u0026lt;- factor(rep(1:length(x1), 2))    # factor for participant\u0026gt; pvalue(oneway_test(DV ~ IV | id, alternative=\"greater\",+                    distribution=approximate(B=9999)))[1] 0.00810081","Display_name":"caracal","Creater_id":1909,"Start_date":"2011-01-10 06:22:10","Question_id":6127}
{"_id":{"$oid":"5837a576a05283111e4d3838"},"Last_activity":"2011-01-10 10:06:35","Creator_reputation":null,"Question_score":27,"Answer_content":"A few comments are, I believe, in order.1) I would encourage you to try multiple visual displays of your data, because they can capture things that are lost by (graphs like) histograms, and I also strongly recommend that you plot on side-by-side axes.  In this case, I do not believe the histograms do a very good job of communicating the salient features of your data.  For example, take a look at side-by-side boxplots:boxplot(x1, y1, names = c(\"x1\", \"y1\"))Or even side-by-side stripcharts:stripchart(c(x1,y1) ~ rep(1:2, each = 20), method = \"jitter\", group.names = c(\"x1\",\"y1\"), xlab = \"\")Look at the centers, spreads, and shapes of these!  About three-quarters of the  data fall well above the median of the  data.  The spread of  is tiny, while the spread of  is huge. Both  and  are highly left-skewed, but in different ways.  For example,  has five (!) repeated values of zero.2) You didn't explain in much detail where your data come from, nor how they were measured, but this information is very important when it comes time to select a statistical procedure.  Are your two samples above independent?   Are there any reasons to believe that the marginal distributions of the two samples should be the same (except for a difference in location, for example)?  What were the considerations prior to the study that led you to look for evidence of a difference between the two groups? 3) The t-test is not appropriate for these data because the marginal distributions are markedly non-normal, with extreme values in both samples.  If you like, you could appeal to the CLT (due to your moderately-sized sample) to use a -test (which would be similar to a z-test for large samples), but given the skewness (in both variables) of your data I would not judge such an appeal very convincing.  Sure, you can use it anyway to calculate a -value, but what does that do for you?  If the assumptions aren't satisfied then a -value is just a statistic; it doesn't tell what you (presumably) want to know: whether there is evidence that the two samples come from different distributions.4) A permutation test would also be inappropriate for these data.  The single and often-overlooked assumption for permutation tests is that the two samples are exchangeable under the null hypothesis.  That would mean that they have identical marginal distributions (under the null).  But you are in trouble, because the graphs suggest that the distributions differ both in location and scale (and shape, too).  So, you can't (validly) test for a difference in location because the scales are different, and you can't (validly) test for a difference in scale because the locations are different. Oops.  Again, you can do the test anyway and get a -value, but so what?  What have you really accomplished?5) In my opinion, these data are a perfect (?) example that a well chosen picture is worth 1000 hypothesis tests.  We don't need statistics to tell the difference between a pencil and a barn.  The appropriate statement in my view for these data would be \"These data exhibit marked differences with respect to location, scale, and shape.\"  You could follow up with (robust) descriptive statistics for each of those to quantify the differences, and explain what the differences mean in the context of your original study.6) Your reviewer is probably (and sadly) going to insist on some sort of -value as a precondition to publication.  Sigh!  If it were me, given the differences with respect to everything I would probably use a nonparametric Kolmogorov-Smirnov test to spit out a -value that demonstrates that the distributions are different, and then proceed with descriptive statistics as above.  You would need to add some noise to the two samples to get rid of ties.  (And of course, this all assumes that your samples are independent which you didn't state explicitly.)This answer is a lot longer than I originally intended it to be.  Sorry about that.","Display_name":"user1108","Creater_id":null,"Start_date":"2011-01-10 10:06:35","Question_id":6127}
{"_id":{"$oid":"5837a576a05283111e4d3839"},"Last_activity":"2011-01-10 06:49:44","Creator_reputation":6669,"Question_score":1,"Answer_content":"Are these scores proportions? If so, you certainly shouldn't be using a gaussian parametric test, and while you could go ahead with a non-parametric approach like a permutation test or bootstrap of the means, I'd suggest that you'll get more statistical power by employing a suitable non-gaussian parametric approach. Specifically, any time you can compute a proportion measure within a unit of interest (ex. participant in an experiment), you can and probably should use a mixed effects model that specifies observations with binomially distributed error. See Dixon 2004.","Display_name":"Mike Lawrence","Creater_id":364,"Start_date":"2011-01-10 06:49:44","Question_id":6127}
{"_id":{"$oid":"5837a576a05283111e4d3846"},"Last_activity":"2014-08-23 01:18:18","Creator_reputation":584,"Question_score":5,"Answer_content":"Since estimated p-values are used in order to decide whether to reject the null hypothesis, it is important to consider how the choice of estimator affects the probability of a false rejection. The cited paper by Smyth \u0026amp; Phipson's points out that the unbiased estimator () fails to control the type-I error rate correctly. In contrast, () is a valid (but conservative) p-value estimator - it doesn't lead to excess rejection of the null. (B is the number of random permutations in which a statistic greater or equal than the observed one is obtained and M is the total number of random permutations sampled).Smyth \u0026amp; Phipson also demonstrate that the invalidity of () becomes critical in multiple comparisons settings, where very small p-value estimates are derived and then corrected by multiplication with a factor. An estimation of a zero p-value under the null is especially disastrous in these settings, since it stays zero regardless of the corrections applied.","Display_name":"Tal","Creater_id":20587,"Start_date":"2014-08-19 06:16:42","Question_id":109207}
{"_id":{"$oid":"5837a576a05283111e4d3847"},"Last_activity":"2014-08-18 11:00:51","Creator_reputation":147374,"Question_score":11,"Answer_content":"DiscussionA permutation test generates all relevant permutations of a dataset, computes a designated test statistic for each such permutation, and assesses the actual test statistic in the context of the resulting permutation distribution of the statistics. A common way to assess it is to report the proportion of statistics which are (in some sense) \"as or more extreme\" than actual statistic.  This is often called a \"p-value.\"Because the actual dataset is one of those permutations, its statistic will necessarily be among those found within permutation distribution.  Therefore, the p-value can never be zero.Unless the dataset is very small (less than about 20-30 total numbers, typically) or the test statistic has a particularly nice mathematical form, is not practicable to generate all the permutations.  (An example where all permutations are generated appears at Permutation Test in R.)  Therefore computer implementations of permutation tests typically sample from the permutation distribution.  They do so by generating some independent random permutations and hope that the results are a representative sample of all the permutations.Therefore, any numbers (such as a \"p-value\") derived from such a sample are only estimators of the properties of the permutation distribution.  It is quite possible--and often happens when effects are large--that the estimated p-value is zero.  There is nothing wrong with that, but it immediately raises the heretofore neglected issue of how much could the estimated p-value differ from the correct one?  Because the sampling distribution of a proportion (such as an estimated p-value) is Binomial, this uncertainty can be addressed with a Binomial confidence interval.ArchitectureA well-constructed implementation will follow the discussion closely in all respects.  It would begin with a routine to compute the test statistic, as this one to compare the means of two groups:diff.means \u0026lt;- function(control, treatment) mean(treatment) - mean(control)Write another routine to generate a random permutation of the dataset and apply the test statistic.  The interface to this one allows the caller to supply the test statistic as an argument.  It will compare the first m elements of an array (presumed to be a reference group) to the remaining elements (the \"treatment\" group).f \u0026lt;- function(..., sample, m, statistic) {  s \u0026lt;- sample(sample)  statistic(s[1:m], s[-(1:m)])}The permutation test is carried out first by finding the statistic for the actual data (assumed here to be stored in two arrays control and treatment) and then finding statistics for many independent random permutations thereof:z \u0026lt;- stat(control, treatment) # Test statistic for the observed datasim\u0026lt;- sapply(1:1e4, f, sample=c(control,treatment), m=length(control), statistic=diff.means)Now compute the binomial estimate of the p-value and a confidence interval for it.  One method uses the built-in binconf procedure in the HMisc package:require(Hmisc)                                    # Exports `binconf`k \u0026lt;- sum(abs(sim) \u0026gt;= abs(z))                      # Two-tailed testzapsmall(binconf(k, length(sim), method='exact')) # 95% CI by defaultIt's not a bad idea to compare the result to another test, even if that is known not to be quite applicable: at least you might get an order of magnitude sense of where the result ought to lie.  In this example (of comparing means), a Student t-test usually gives a good result anyway:t.test(treatment, control)This architecture is illustrated in a more complex situation, with working R code, at Test Whether Variables Follow the Same Distribution.ExampleAs a test, I generated  normally distributed \"control\" values from a distribution with mean  and  normally distributed \"treatment\" values from a distribution with mean .set.seed(17)control \u0026lt;- rnorm(10)treatment \u0026lt;- rnorm(20, 1.5)After using the preceding code to run a permutation test I plotted the sample of the permutation distribution along with a vertical red line to mark the actual statistic:h \u0026lt;- hist(c(z, sim), plot=FALSE)hist(sim, breaks=h00.000370.000370.000370.050.010.001kNk/N(k+1)/(N+1)N1010^2=1000.0000051.611.7$ parts per million: a little smaller than the Student t-test reported.  Although the data were generated with normal random number generators, which would justify using the Student t-test, the permutation test results differ from the Student t-test results because the distributions within each group of observations are not perfectly normal.","Display_name":"whuber","Creater_id":919,"Start_date":"2014-08-18 10:55:25","Question_id":109207}
{"_id":{"$oid":"5837a576a05283111e4d3854"},"Last_activity":"2016-08-24 10:10:37","Creator_reputation":869,"Question_score":1,"Answer_content":"I can think of two approaches. First, the simple one --1) Fit your K separate models, privately2) For each one, simulate a data set of that model's original size (or just share the parameter estimates, and have somebody else do the simulation)3) Run a big regression on the pooled, simulated, data.4) POSSIBLY weight the sub-datasets by the original goodness of fit, but I have no principled way of doing that.As far as privacy goes, this is the same as sharing the estimated parameters, intervals, and N, which I assume is okay, and gives you a simple way of combining results, with outcomes that are easy to interpret and manipulate. I don't have a citation for this unfortunately, but nothing about it strikes me as unnaturally dangerous (given your options) -- your big worry will be goodness of fit in the component models though, so I would communicate thoroughly about that, to the extent you can to feel confident.The second approach I know less about, but I have seen talks on models being fit in distributed systems, and thought perhaps a single model could be run on all your separated data without anyone seeing all of it at once. How, I have no clue, but I would look in that direction as well. A recent Stanford stats PhD was working on something like that for the lasso, but that's really all I know.","Display_name":"Sophologist","Creater_id":52211,"Start_date":"2016-08-24 10:05:18","Question_id":231474}
{"_id":{"$oid":"5837a576a05283111e4d3861"},"Last_activity":"2016-08-24 09:29:26","Creator_reputation":5445,"Question_score":2,"Answer_content":"You have participants nested within doctors, so the random structure of the model should reflect this by estimating random intercepts for doctor and participants within doctors, provided that you have sufficient numbers of both (10 is a typical rule of thumb). I would start with the model:glmer(outcome ~ intervention*time + (1|Doctor/ID), data=mydata, family=binomial(link=logit)where intervention*time is shorthand for  intervention + time + intervention:time. I don't know what you mean by \"I want time to be within ID\". From your description, time is simply a pre/post indicator variable. You could allow for the effect of time to differ among participants (and/or doctors) by adding a random coefficient for time:glmer(outcome ~ intervention*time + (time|Doctor/ID), data=mydata, family=binomial(link=logit)In this formulation, the model will estimate time random slopes for both doctors and participants. If you wanted time random slopes for only participants you would use:glmer(outcome ~ intervention + time + intervention:time + (1|Doctor) + (time|Doctor:ID), data=mydata, family=binomial(link=logit)where this uses the fact that (1|Doctor/ID) is just shorthand notation for (1|Doctor) + (1|Doctor:ID","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-24 09:11:47","Question_id":231491}
{"_id":{"$oid":"5837a576a05283111e4d3870"},"Last_activity":"2016-04-22 16:59:37","Creator_reputation":5787,"Question_score":3,"Answer_content":"No one has provided an exact answer, so I'll discuss approximations. If you're content with an approximate answer, you could consider I. Use simulation to estimate the fraction of all possible length j passwords composed of the 68 allowed characters (26 letters, 10 numbers, 32 special characters), i.e. , which are in compliance with the rules. Do separately for j = 14, 13, etc. Multiply the fraction in compliance by the number of passwords of that length, and sum over lengths of 6 to 14.  There might also be some clever ways to incorporate some of the rules restrictions into the simulated candidates, then estimate the fraction of those which are in compliance with the remaining rules. orII. An upper bound approximation (code strands are in MATLAB, but they should be understandable by all):There are 68 allowed characters, so if no passwords were disallowed, there would be sum(68.^(6:14)) = 4.587e25 possible passwords.For a length j password, there is(no numbers) + (no letters) - (no numbers or letters) = probability of not having at least one number and one letter.  The probability of not having at least one number is dominant within this.That comes out as follows:   j    P(not having at least one letter and one number in password of length j)   6             0.429706323418485   7             0.357603415290630   8             0.298900283824793   9             0.250880451281631  10             0.211340972742333  11             0.178563920283941  12             0.151226117928902  13             0.128306817799266  14             0.109011434066698So, decrementing the total number of passwords of lengths 6 to 14 only by those disallowed due to not having at least one number and one letter, results in a total of sum(68.^(6:14).*(1-((58/68).^(6:14)+(42/68).^(6:14)-(32/68).^(6:14))))= 4.086e+25 passwords.The probability of not having 9 or more numbers given at least one exceeds 0.99996.  I believe that disallowance due to all other reasons would only contribute a \"very small\" decrement, although I have not quantified that.  Therefore, the number of allowed passwords will be \"a little less\" than 4.086e+25, if I haven't made a mistake. I leave any further refinement to others.  But this is probably \"good enough for government use\".","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-04-22 16:59:37","Question_id":208645}
{"_id":{"$oid":"5837a576a05283111e4d387c"},"Last_activity":"2016-05-26 12:17:23","Creator_reputation":126,"Question_score":1,"Answer_content":"Consider two cases:1. There are exactly three women on the committee:c(5,3)*c(6,1)=60There are exactly four women on the committee:c(5,4)=5Hence:Solution = 60+5 = 65","Display_name":"F. Knorr","Creater_id":117045,"Start_date":"2016-05-26 12:17:23","Question_id":214844}
{"_id":{"$oid":"5837a576a05283111e4d387d"},"Last_activity":"2016-05-26 11:40:33","Creator_reputation":103,"Question_score":0,"Answer_content":"I did this:Okay. I said:How many ways committee can be formed without specification i.e No 3 women restriction e.t.c:c(11, 4) = 330After that I say:Number of combinations that 5 women that can be taken 3 at a time = c(5, 3) = 10Then I say, number of combinations that 5 women can be taken 4 at a time= c(5, 4) = 5Final answer = 330 - 10 - 5 = 315Do people agree with this?","Display_name":"rert588","Creater_id":117037,"Start_date":"2016-05-26 11:40:33","Question_id":214844}
{"_id":{"$oid":"5837a576a05283111e4d388e"},"Last_activity":"2016-08-24 08:50:41","Creator_reputation":36,"Question_score":0,"Answer_content":"I transformed the data taking the log(10) of N/kg (below). It does improve the presentation, although the data are still not normally distributed, so I will do a Kruskal-Wallis analysis to compare between years.","Display_name":"James Abbott","Creater_id":127578,"Start_date":"2016-08-24 08:50:41","Question_id":229658}
{"_id":{"$oid":"5837a576a05283111e4d388f"},"Last_activity":"2016-08-13 07:54:59","Creator_reputation":57702,"Question_score":-1,"Answer_content":"Depending on what you are interested in, it might be better to plot several quantiles as lines over time, possibly on a log scale, possibly on the raw scale. Since you are interested in food security, I am guessing you would be especially interested in the behavior at the highest quantiles.Another point is that you should make the gap between 2002/3 and 2008 5 times as large as the other gaps, or maybe just delete the earliest time point. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-13 07:54:59","Question_id":229658}
{"_id":{"$oid":"5837a576a05283111e4d389c"},"Last_activity":"2016-08-24 08:46:52","Creator_reputation":4307,"Question_score":0,"Answer_content":"The degrees of freedom (dof) is the number of data points minus the number of equality constraints. Typically the latter is equal to the number of parameters estimated. For the sample mean, the sum of deviations from the mean must be zero, so that removes one dof. For a difference of two means, you can think of your second example as . For regression, if you estimate  parameters by maximum likelihood (or some other continuous optimization), then you have  equality constraints: The gradient of the likelihood function with respect to each parameter must be zero. So there are  degrees of freedom. (For example the sample mean is the least-squares solution of .)","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-24 08:46:52","Question_id":231483}
{"_id":{"$oid":"5837a576a05283111e4d38a9"},"Last_activity":"2016-08-18 12:45:37","Creator_reputation":405,"Question_score":1,"Answer_content":"Many people (outside the specialist experts) who think they are frequentist are in fact Bayesian. This makes the debate a bit pointless. I think that Bayesianism won, but that there are still many Bayesians who think they are frequentist. There are some people who think that they don't use priors and hence they think they are frequentist. This is dangerous logic. This is not so much about priors (uniform priors or non-uniform), the real difference is more subtle.(I'm not formally in the statistics department; my background is maths and computer science. I'm writing because of difficulties I've had trying to discuss this 'debate' with other non-statisticians, and even with some early-career statisticians.)The MLE is actually a Bayesian method. Some people will say \"I'm a frequentist because I use the MLE to estimate my parameters\". I have seen this in peer-reviewed literature. This is nonsense and is based on this (unsaid, but implied) myth that a frequentist is somebody who uses a uniform prior instead of a non-uniform prior).Consider drawing a single number from a normal distribution with known mean, , and unknown variance. Call this variance .Now consider the likelihood function. This function has two parameters,  and  and it returns the probability, given , of .You can imagine plotting this in a heatmap, with  on the x-axis and  on the y-axis, and using the colour (or z-axis). Here is the plot, with contour lines and colours.First, a few observations. If you fix on a single value of , then you can take the corresponding horizontal slice through the heatmap. This slice will give you the pdf for that value of . Obviously, the area under the curve in that slice will be 1. On the other hand, if you fix on a single value of , and then look at the corresponding vertical slice, then there is no such guarantee about the area under the curve.This distinction between the horizontal and vertical slices is crucial, and I found this analogy helped me to understand the frequentist approach to bias. A Bayesian is somebody who says  For this value of x, which values of  give a 'high enough'  value of ?.Alternatively, a Bayesian might include a prior, , but they are still talking about  for this value of x, which values of  give a high enough value of ?So a Bayesian fixes x and looks at the corresponding vertical slice in that contour plot (or in the variant plot incorporating the prior). In this slice, the area under the curve need not be 1 (as I said earlier). A Bayesian 95% credible interval (CI) is the interval which contains 95% of the available area. For example, if the area is 2, then the area under the Bayesian CI must be 1.9.On the other hand, a frequentist will ignore x and first consider fixing , and will ask:  For this , which values of x will appear most often?In this example, with , one answer to this frequentist question is: \"For a given , 95% of the  will appear between  and .\"So a frequentist is more concerned with the horizontal lines corresponding to fixed values of .This is not the only way to construct the frequentist CI, it's not even a good (narrow) one, but bear with me for a moment.The best way to interpret the word 'interval' is not as an interval on a 1-d line, but to think of it as an area on the above 2-d plane. An 'interval' is a subset of the 2-d plane, not of any 1-d line. If somebody proposes such an 'interval', we then have to test is the 'interval' is valid at a 95% confidence/credible level.A frequentist will check the validity of this 'interval' by considering each horizontal slice in turn and looking at the area under the curve. As I said before, the area under this curve will always be one. The crucial requirement is that the area within the 'interval' be at least 0.95.A Bayesian will check validity by instead looking at the vertical slices. Again, the area under the curve will be compared to the subarea that's under the interval. If the latter is at least 95% of the former, then the 'interval' is a valid 95% Bayesian credible interval.Now that we know how to test whether a particular interval is 'valid', the question is how do we choose the best option among the valid options. This can be a black art, but generally you want the narrowest interval. Both approaches tend to agree here - the vertical slices are considered and the goal is to make the interval as narrow as possible within each vertical slice.I have not attempted to define the narrowest possible frequentist confidence interval in the above example. See the comments by @cardinal below for examples of narrower intervals. My goal is not to find the best intervals, but to emphasize the difference between the horizontal and vertical slices in determining validity. An interval that satisfies the conditions of a 95% frequentist confidence interval will usually not satisfy the conditions of a 95% Bayesian credible interval, and vice versa.Both approaches desire narrow intervals, i.e. when considering one vertical slice we want to make the (1-d) interval in that slice to be as narrow as possible. The difference is in how the 95% is enforced - a frequentist will only look at proposed intervals where 95% of each horizontal slice's area is under the interval, whereas a Bayesian will insist that each vertical slice be such that 95% of its area is under the interval.Many non-statisticians don't understand this and they focus only on the vertical slices; this makes them Bayesians even if they think otherwise.","Display_name":"Aaron McDaid","Creater_id":7817,"Start_date":"2012-01-05 09:56:42","Question_id":20558}
{"_id":{"$oid":"5837a576a05283111e4d38aa"},"Last_activity":"2012-11-02 15:57:26","Creator_reputation":1,"Question_score":5,"Answer_content":"As you'll see, there's quite a lot of frequentist-Bayesian debate going on.  In fact, I think it's hotter than ever, and less dogmatic.  You might be interested in my blog:http://errorstatistics.com","Display_name":"Mayo","Creater_id":8967,"Start_date":"2012-02-04 10:49:55","Question_id":20558}
{"_id":{"$oid":"5837a576a05283111e4d38ab"},"Last_activity":"2012-01-05 17:28:27","Creator_reputation":405,"Question_score":10,"Answer_content":"I don't think the Frequentists and Bayesians give different answers to the same questions. I think they are prepared to answer different questions. Therefore, I don't think it makes sense to talk much about one side winning, or even to talk about compromise.Consider all the questions we might want to ask. Many are just impossible questions (\"What is the true value of ?\"). It's more useful to consider the subset of these questions that can be answered given various assumptions. The larger subset is the questions that can be answered where you do allow yourself to use priors. Call this set BF. There is a subset of BF, which is the set of questions that do not depend on any prior. Call this second subset F. F is a subset of BF. Define B = BF \\ B.However, we cannot choose which questions to answer. In order to make useful inferences about the world, we sometimes have to answer questions that are in B and that means using a prior.Ideally, given an estimator you would do a thorough analysis. You might use a prior, but it also would be cool if you could prove nice things about your estimator which do not depend on any prior. That doesn't mean you can ditch the prior, maybe the really interesting questions require a prior.Everybody agrees on how to answer the questions in F. The worry is whether the really 'interesting' questions are in F or in B?An example: a patient walks into the doctor and is either healthy(H) or sick(S). There is a test that we run, which will return positive(+) or negative(-). The test never gives false negatives - i.e . But it will sometimes give false positives - We have a piece of card and the testing machine will write + or - on one side of the card. Imagine, if you will, that we have an oracle who somehow knows the truth, and this oracle writes the true state, H or S, on the other side of the card before putting the card into an envelope.As the statistically-trained doctor, what can we say about the card in the envolope before we open the card? The following statements can be made (these are in F above):If S on one side of the card, then the other side will be +. If H, then the other side will be + with 5% probability, - with 95% probability. (summarizing the last two points) The probability that the two sides match is at least 95%. We don't know what  or  is. We can't really answer that without some sort of prior for . But we can make statements about the sum of those two probabilities.This is as far as we can go so far. Before opening the envelope, we can make very positive statements about the accuracy of the test. There is (at least) 95% probability that the test result matches the truth.But what happens when we actually open the card? Given that the test result is positive (or negative), what can we say about whether they are healthy or sick?If the test is positive (+), there is nothing we can say. Maybe they are healthy, and maybe not. Depending on the current prevalence of the disease () it might be the case that most patients who test positive are healthy, or it might be the case that most are sick. We can't put any bounds on this, without first allowing ourselves to put some bounds on .In this simple example, it's clear that everybody with a negative test result is healthy. There are no false negatives, and hence every statistician will happily send that patient home. Therefore, it makes no sense to pay for the advice of a statistician unless the test result has been positive.The three bullet points above are correct, and quite simple. But they're also useless! The really interesting question, in this admittedly contrived model, is: \\mathcal{P}(S|+) and this cannot be answered without  (i.e a prior, or at least some bounds on the prior)I don't deny this is perhaps an oversimplified model, but it does demonstrate that if we want to make useful statements about the health of those patients, we must start off we some prior belief about their health.","Display_name":"Aaron McDaid","Creater_id":7817,"Start_date":"2012-01-05 14:14:03","Question_id":20558}
{"_id":{"$oid":"5837a576a05283111e4d38ac"},"Last_activity":"2012-01-04 09:28:19","Creator_reputation":14025,"Question_score":52,"Answer_content":"I actually mildly disagree with the premise.  Everyone is a Bayesian, if they really do have a probability distribution handed to them as a prior.  The trouble comes about when they don't, and I think there's still a pretty good-sized divide on that topic.Having said that, though, I do agree that more and more people are less inclined to fight holy wars and just get on with doing what seems appropriate in any given situation.  I would say that, as the profession advanced, both sides realized there were merits in the other side's approaches.  Bayesians realized that evaluating how well Bayesian procedures would do if used over and over again (e.g., does this 95% credible interval (CI) actually contain the true parameter about 95% of the time?) required a frequentist outlook.  Without this, there's no calibration of that \"95%\" to any real-world number.  Robustness?  Model building through iterative fitting etc.?  Ideas that came up in the frequentist world, and were adapted by Bayesians starting in the late 1980s or so.  Frequentists realized that regularization was good, and use it quite commonly these days - and Bayesian priors can be easily interpreted as regularization.  Nonparametric modeling via cubic splines with a penalty function?  Your penalty is my prior!  Now we can all get along.The other major influence, I believe, is the staggering improvement in availability of high-quality software that will let you do analysis quickly.  This comes in two parts - algorithms, e.g., Gibbs sampling and Metropolis-Hastings, and the software itself, R, SAS, ...  I might be more of a pure Bayesian if I had to write all my code in C (I simply wouldn't have the time to try anything else), but as it is, I'll use gam in the mgcv package in R any time my model looks like I can fit it into that framework without too much squeezing, and I'm a better statistician for it.  Familiarity with your opponent's methods, and realizing how much effort it can save / better quality it can provide to use them in some situations, even though they may not fit 100% into your default framework for thinking about an issue, is a big antidote to dogmatism. ","Display_name":"jbowman","Creater_id":7555,"Start_date":"2012-01-03 14:14:16","Question_id":20558}
{"_id":{"$oid":"5837a576a05283111e4d38ad"},"Last_activity":"2012-01-04 08:35:51","Creator_reputation":21578,"Question_score":34,"Answer_content":"This is a difficult question to answer. The number of people who truly do both is still very limited. Hard core Bayesians despise the users of mainstream statistics for their use of -values, a nonsensical, internally inconsistent statistic for Bayesians; and the mainstream statisticians just do not know Bayesian methods well enough to comment on them. In the light of this, you will see a lot of criticism of the null hypothesis significance testing in Bayesian literature (ranging as far as nearly pure biology or pure psychology journals), with little to no response from mainstreamers.There are conflicting manifestation as to \"who won the debate\" in statistics profession. On one hand, the the composition of an average statistics department is that in most places, you will find 10-15 mainstreamers vs. 1-2 Bayesians, although some departments are purely Bayesian, with no mainstreamers at all, except probably for consulting positions (Harvard, Duke, Carnegie Mellon, British Columbia, Montreal in North America; I am less familiar with European scene). On the other hand, you will see that in journals like JASA or JRSS, probably 25-30% of papers are Bayesian. In a way, the Bayesian renaissance may be something like the burst of ANOVA papers in the 1950s: back then, people thought that pretty much any statistics problem can be framed as an ANOVA problem; right now, people think that pretty much anything can be solved with the right MCMC.My feeling is that applied areas don't bother figuring out the philosophical details, and just go with whatever is easier to work with. Bayesian methodology is just too damn complicated: on top of statistics, you also need to learn the art of computation (setting up the sampler, blocking, convergence diagnostics, blah-blah-blah) and be prepared to defend your priors (should you use objective priors, or should you use informative priors if the field has pretty much settled on the speed of light being 3e8 m/s, or even whether the choice of the prior affects whether your posterior will be proper or not). So in most medical or psychology or economics applications, you will see mainstream approaches in the papers written by substantive researchers, although you can also see occasional glimpse of a Bayesian paper -- to have been written by more sophisticated methodologists, or in collaboration with a Bayesian statistician, just because that was the person available at a local department to do this collaborative work.One area where, I think, Bayesian framework is still coming short is model diagnostics -- and that is an important area for practitioners. In Bayesian world, to diagnose a model, you need to build a more complicated one and choose whichever has a better fit by Bayesian factor or BIC. So if you don't like the normality assumption for your linear regression, you can build a regression with Student errors, and let the data generate an estimate of the degrees of freedom, or you can become all fancy and have a Dirichlet process for your error terms and do some M-H jumps between different models. The mainstream approach would be to build a Q-Q plot of studentized residuals and remove outliers, and this is, again, so much simpler.I edited a chapter in a book on this -- see http://onlinelibrary.wiley.com/doi/10.1002/9780470583333.ch5/summary. It is a very archetypal paper, in that gave about 80 references on this debate, all supporting the Bayesian point of view. (I asked the author to extend it in a revised version, which says a lot about it :) ). Jim Berger from Duke, one of the leading Bayesian theorists, gave a number of lectures, and wrote a number of very thoughtful articles on the topic. ","Display_name":"StasK","Creater_id":5739,"Start_date":"2012-01-04 08:35:51","Question_id":20558}
{"_id":{"$oid":"5837a576a05283111e4d38ae"},"Last_activity":"2012-01-04 05:31:46","Creator_reputation":27740,"Question_score":13,"Answer_content":"There is a good reason for still having both, which is that a good craftsman will want to select the best tool for the task at hand, and both Bayesian and frequentist methods have applications where they are the best tool for the job.However, often the wrong tool for the job is used because frequentist statistics are more amenable to a \"statistics cookbook\" approach which makes them easier to apply in science and engineering than their Bayesian counterparts, even though the Bayesian methods provide a more direct answer to the question posed (which is generally what we can infer from the particular sample of data we actually have).  I am not greatly in favour of this as the \"cookbook\" approach leads to using statistics without a solid understanding of what you are actually doing, which is why things like the p-value fallacy crop up again and again.However, as time progresses, the software tools for the Bayesian approach will improve and they will be used more frequently as jbowman rightly says.I am a Bayesian by inclination (it seems to make a lot more sense to me than the frequentist approach), however I end up using frequentist statistics in my papers, partly because I will have trouble with the reviewers if I use Bayesian statistics as they will be \"non-standard\".Finally (somewhat tongue in cheek ;o), to quote Max Plank \"A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.\"","Display_name":"Dikran Marsupial","Creater_id":887,"Start_date":"2012-01-04 05:31:46","Question_id":20558}
{"_id":{"$oid":"5837a576a05283111e4d38bb"},"Last_activity":"2013-12-02 10:30:43","Creator_reputation":2088,"Question_score":2,"Answer_content":"Take the uniform distribution on . Now, , but also  and  as well even though both of the latter two are strict subsets of . (Here the notation  means all elements belonging to set  and not belonging to .)To answer the almost surely part of your question:  if P(A)=0 this means that event A is almost never possible and if  P(A)=1 will almost surely occur.I think you have got it the wrong way round: \"almost surely\" is defined to be .To fully understand the intricacies and subtleties of the definitions, some familiarity with measure theory is helpful. See, e.g. the Cantor set, which has measure [\"length\"] zero but contains a uncountably infinite number of points, compared to any interval on the real line  which has again uncountably infinite number of points but nonzero measure .Given that you are working in Applied statistics, these complicated sets are probably irrelevant to you, so I would not worry about it ( its just something authors like to put in!).","Display_name":"seanv507","Creater_id":27556,"Start_date":"2013-12-01 17:15:50","Question_id":78245}
{"_id":{"$oid":"5837a576a05283111e4d38bc"},"Last_activity":"2013-12-01 18:23:12","Creator_reputation":213,"Question_score":2,"Answer_content":"Following the arguments from @TrynnaDoStat:Let  be a standard normal random distribution, and therefore  has support on whole .  but Then using the principle that{1}, but ","Display_name":"wonghang","Creater_id":16140,"Start_date":"2013-12-01 18:23:12","Question_id":78245}
{"_id":{"$oid":"5837a576a05283111e4d38bd"},"Last_activity":"2013-12-01 17:10:55","Creator_reputation":4284,"Question_score":4,"Answer_content":"Let X be a standard normal random variable with . Here,  but . To show that  does not imply that , consider the following. You flip a coin an infinite number of times. The event of getting all heads  is in the sample space because it is physically possible that tails never appears. Now, let . However, .","Display_name":"TrynnaDoStat","Creater_id":23801,"Start_date":"2013-12-01 16:37:47","Question_id":78245}
{"_id":{"$oid":"5837a576a05283111e4d38cc"},"Last_activity":"2014-12-03 11:07:09","Creator_reputation":18711,"Question_score":10,"Answer_content":"After the development of Probability Theory it was necessary to show that looser concepts answering to the name of \"probability\" measured up to the rigorously defined concept they had inspired. \"Subjective\" Bayesian probabilities were considered by Ramsey and de Finetti, who independently showed that a quantification of degree of belief subject to the constraints of comparability \u0026amp; coherence (your beliefs are coherent if no-one can make a Dutch book against you) has to be a probability.Differences between axiomatizations are largely a matter of taste concerning what should be what defined \u0026amp; what derived. But countable additivity is one of Kolmogorov's that isn't derivable from Cox's or Finetti's, \u0026amp; has been controversial. Some Bayesians (e.g. de Finetti \u0026amp; Savage) stop at finite additivity \u0026amp; so don't accept all of Kolmogorov's axioms. They can put uniform probability distributions over infinite intervals without impropriety. Others follow Villegas in also assuming monotone continuity, \u0026amp; get countable additivity from that.Ramsey (1926), \"Truth and probability\", in Ramsey (1931), The Foundations of Mathematics and other Logical Essaysde Finetti (1931), \"Sul significato soggettivo della probabilit\u0026agrave;\", Fundamenta Mathematic\u0026aelig;, 17, pp 298\u0026thinsp;\u0026ndash;\u0026thinsp;329Villegas (1964), \"On qualitative probability -algebras\", Ann. Math. Statist., 35, 4.","Display_name":"Scortchi","Creater_id":17230,"Start_date":"2014-12-01 06:29:19","Question_id":126056}
{"_id":{"$oid":"5837a576a05283111e4d38cd"},"Last_activity":"2014-12-01 11:31:54","Creator_reputation":500,"Question_score":20,"Answer_content":"In my opinion, Cox-Jaynes interpretation of probability provides a rigorous foundation for Bayesian probability:Cox, Richard T. \"Probability, frequency and reasonable expectation.\" American journal of physics 14.1 (1946): 1-13.Jaynes, Edwin T. Probability theory: the logic of science. Cambridge university press, 2003.Beck, James L. \"Bayesian system identification based on probability logic.\" Structural Control and Health Monitoring 17.7 (2010): 825-847.The axioms of probability logic derived by Cox are:(P1):   (by convention)(P2):  (negation function)(P3):  (conjunction function)Axioms P1-P3 imply the following (Beck, James L. \"Bayesian system identification based on probability logic.\" Structural Control and Health Monitoring 17.7 (2010): 825-847):(P4): a) ; b) ; c) (P5): a) , b) , where  means that  is contained in , and  means that  is equivalent to .(P6): (P7): Assuming that proposition  states that one and only one of propositions  is true, then:a) Marginalization Theorem: b) Total Probability Theorem: c) Bayes' Theorem: For : They imply Kolmogorov's statement of logic, which can be viewed as a special case.In my interpretation of a Bayesian viewpoint, everything is always (implicitly) conditioned on our believes and on our knowledge.The following comparison is taken from Beck (2010): Bayesian system identification based on probability logicThe Bayesian point of viewProbability is a measure of plausibility of a statement based on specified information.Probability distributions represent states of plausible knowledge about systems and phenomena, not inherent properties of them.Probability of a model is a measure of its plausibility relative to other models in a set.Pragmatically quantifies uncertainty due to missing information without any claim that this is due to nature's inherent randomness.The Frequentist point of viewProbability is the relative frequency of occurrence of an inherently random event in the long run.Probability distributions are inherent properties of random phenomena.Limited scope, e.g. no meaning for the probability of a model.Inherent randomness is assumed, but cannot be proven.How to derive Kolmogorov's axioms from the axioms aboveIn the following, section 2.2 of [Beck, James L. \"Bayesian system identification based on probability logic.\" Structural Control and Health Monitoring 17.7 (2010): 825-847.] is summarized:In the following we use: probability measure  on subset  of a finite set :[K1]: [K2]: [K3]:  if  and  are disjoint.In order to derive (K1-K3) from the axioms of probability theory, [Beck, 2010] introduced propositon  that states  and specifies the probability model for . [Beck, 2010] furthermore introduces .P1 implies K1 with  and K2 follows from ; P4(a), and  states that .K3 can be derived from P6:  and  are disjoint means that  and  are mutually exclusive. Therefore, K3: ","Display_name":"Summit","Creater_id":61648,"Start_date":"2014-12-01 04:19:21","Question_id":126056}
{"_id":{"$oid":"5837a576a05283111e4d38da"},"Last_activity":"2016-05-25 06:13:43","Creator_reputation":1,"Question_score":0,"Answer_content":"From a \"real world\" point of view, I find one major difference between a frequentist and a classical or Bayesian \"solution\" that applies to at least three major scenarios. The difference in selecting a methodology depends on whether you need a solution that is impacted by the population probability, or one that is impacted by the individual probability. Examples below:If there is a known 5% probability that males over 40 will die in a given year and require life insurance payments, an insurance company can use the 5% POPULATION percentage to estimate its costs, but to say that each individual male over 40 only has a 5% chance of dying ... is meaningless... Because 5% have a 100% probability of dying - which is a frequentist approach. At the individual level the event either occurs (100% probability) or it does not (0% probability)However, based on this limited information, it is not possible to predict the individuals who have a 100% probability of dying, and the 5% \"averaged\" population probability is useless at the individual level.The above argument applies equally as well to fires in buildings which is why sprinklers are required in all buildings in a population.Both of the above arguments apply equally as well to information systems breeches, damage, or \"hacks\". The population percentages are useless so all systems must be safeguarded.","Display_name":"James J Finn","Creater_id":116881,"Start_date":"2016-05-25 06:13:43","Question_id":31867}
{"_id":{"$oid":"5837a576a05283111e4d38db"},"Last_activity":"2015-06-19 12:17:17","Creator_reputation":1849,"Question_score":19,"Answer_content":"In the frequentist approach, it is asserted that the only sense in which probabilities have meaning is as the limiting value of the number of successes in a sequence of trials, i.e. asp = \\lim_{n\\to\\infty} \\frac{k}{n}where  is the number of successes and  is the number of trials. In particular, it doesn't make any sense to associate a probability distribution with a parameter.For example, consider samples  from the Bernoulli distribution with parameter  (i.e. they have value 1 with probability  and 0 with probability ). We can define the sample success rate to be\\hat{p} = \\frac{X_1+\\cdots +X_n}{n}and talk about the distribution of  conditional on the value of , but it doesn't make sense to invert the question and start talking about the probability distribution of  conditional on the observed value of . In particular, this means that when we compute a confidence interval, we interpret the ends of the confidence interval as random variables, and we talk about \"the probability that the interval includes the true parameter\", rather than \"the probability that the parameter is inside the confidence interval\".In the Bayesian approach, we interpret probability distributions as quantifying our uncertainty about the world. In particular, this means that we can now meaningfully talk about probability distributions of parameters, since even though the parameter is fixed, our knowledge of its true value may be limited. In the example above, we can invert the probability distribution  using Bayes' law, to give\\overbrace{f(p\\mid \\hat{p})}^\\text{posterior} = \\underbrace{\\frac{f(\\hat{p}\\mid p)}{f(\\hat{p})}}_\\text{likelihood ratio} \\overbrace{f(p)}^\\text{prior}The snag is that we have to introduce the prior distribution into our analysis - this reflects our belief about the value of  before seeing the actual values of the . The role of the prior is often criticised in the frequentist approach, as it is argued that it introduces subjectivity into the otherwise austere and object world of probability.In the Bayesian approach one no longer talks of confidence intervals, but instead of credible intervals, which have a more natural interpretation - given a 95% credible interval, we can assign a 95% probability that the parameter is inside the interval.","Display_name":"Chris Taylor","Creater_id":2425,"Start_date":"2012-07-05 08:45:46","Question_id":31867}
{"_id":{"$oid":"5837a576a05283111e4d38dc"},"Last_activity":"2012-07-05 10:52:36","Creator_reputation":1437,"Question_score":11,"Answer_content":"The Bayesian interpretation of probability is a degree-of-belief interpretation.A Bayesian may say that the probability that there was life on Mars a billion years ago is .A frequentist will refuse to assign a probability to that proposition.  It is not something that could be said to be true in half of all cases, so one cannot assign probability .","Display_name":"Michael Hardy","Creater_id":5176,"Start_date":"2012-07-05 10:52:36","Question_id":31867}
{"_id":{"$oid":"5837a576a05283111e4d38dd"},"Last_activity":"2012-07-05 09:27:40","Creator_reputation":489,"Question_score":15,"Answer_content":"You're right about your interpretation of Frequentist probability: randomness in this setup is merely due to incomplete sampling. From the Bayesian viewpoint probabilities are \"subjective\", in that they reflect an agent's uncertainty about the world. It's not quite right to say that the parameters of the distributions \"change\". Since we don't have complete information about the parameters, our uncertainty about them changes as we gather more information. Both interpretations are useful in applications, and which is more useful depends on the situation. You might check out Andrew Gelman's blog for ideas about Bayesian applications. In many situations what Bayesians call \"priors\" Frequentists call \"regularization\", and so (from my perspective) the excitement can leave the room rather quickly. In fact, according to the Bernstein-von Mises theorem, Bayesian and Frequentist inference are actually asymptotically equivalent under rather weak assumptions (though notably the theorem fails for infinite-dimensional distributions). You can find a slew of references about this here.Since you asked for interpretations: I think the Frequentist viewpoint makes great sense when modeling scientific experiments as it was designed to do. For some applications in machine learning or for modeling inductive reasoning (or learning), Bayesian probability makes more sense to me. There are many situations in which modeling an event with a fixed, \"true\" probability seems implausible. For a toy example going back to Laplace, consider the probability that the sun rises tomorrow. From the Frequentist perspective, we have to posit something like infinitely-many universes to define the probability. As Bayesians, there is only one universe (or at least, there needn't be many). Our uncertainty about the sun rising is squelched by our very, very strong prior belief that it will rise again tomorrow. ","Display_name":"yep","Creater_id":8485,"Start_date":"2012-07-05 09:27:40","Question_id":31867}
{"_id":{"$oid":"5837a576a05283111e4d38de"},"Last_activity":"2012-07-05 09:13:46","Creator_reputation":25897,"Question_score":6,"Answer_content":"Chris gives a nice simplistic explanation that properly differentiates the two approaches to probability.  But frequentist theory of probability is more than just looking at the long range proportion of successes.  We also consider data sampled at random from a distribution and estimate parameters of the distribution such as the mean and variance by taking certain types of averages of the data (e.g. for the mean it is the arithmetic average of the observations. Frequentist theory associates a probability with the estimate that is called the sampling distribution.  In frequency theory we are able to show for parameters like the mean that are taken by averaging from the samples that the estimate will converge to the true parameter.  The sampling distribution is used to describe how close the estimate is to the parameter for any fixed sample size n.  Close is defined by a measure of accuracy (e.g. mean square error).At Chris points out for any parameter such as the mean the Bayesian attaches a prior probability distribution on it.  Then given the data Bayes' rule is used to compute a posterior distribution for the parameter.  For the Bayesian all inference about the parameter is based on this posterior distribution.Frequentists construct confidence intervals which are intervals of plausible values for the parameter.  Their construction is based on the frequentist probability that if the process used to generate the interval were repeated many times for independent samples the proportion of intervals that would actually include the true value of the parameter would be at least some prespecified confidence level (e.g. 95%).Bayesians use the a posteriori distribution for the parameter to construct credible regions.  These are simply regions in the parameter space over which the posterior distibution is integrated to get a prespecified probability (e.g. 0.95).  Credible regions are interpreted by Bayesians as regions that have a high (e.g. the prespecified 0.95) probability of including the true value of the parameter.","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-07-05 09:13:46","Question_id":31867}
{"_id":{"$oid":"5837a576a05283111e4d38eb"},"Last_activity":"2016-08-24 08:22:05","Creator_reputation":584,"Question_score":0,"Answer_content":"If you find that there is no cointegration, your model is likely to be misspecified. This is probably because you are omitting very important variables, which are correlated with other regressors, and that are essential in the long run cointegration relationship. Remember that what cointegration looks for is the set of variables that jointly define a long-run, stable relationship. My suggestion is to look for more variables, until you can find a cointegration relationship. The main reference for doing this is, nothing else than the theory. If it happens that you cannot find a cointegration relationship, that is also interesting (publication bias anyone?). You might look to argue why that is the case.Then, you could follow Richard's suggestion and run the model in differences, but the information you can get is, in my experience, very limited, and you still can have misspecification problems that tests cannot highlight. That is the beauty behind a structural model and the battery of associated tests allowed by, for example, error correction models.  Finally, if you need to defend your results against serious people (not because of them but because you want a reliable theory), in my experience models in differences are insufficient, specially in modern macroeconomics, where everything is centered about cointegration.","Display_name":"luchonacho","Creater_id":100369,"Start_date":"2016-08-24 05:01:37","Question_id":228974}
{"_id":{"$oid":"5837a576a05283111e4d38f8"},"Last_activity":"2016-08-24 07:59:48","Creator_reputation":12907,"Question_score":0,"Answer_content":"Off topic (and I do not know the answer).If the appropriate lags for  and  are different, , forcing a common lag  will be suboptimal.If , there will be unnecessarily many parameters in the model resulting in increased estimation variance and loss of power in the Granger causality test.If , there will be omitted variable bias resulting in inconsistent parameter estimates and messing up both the true significance and the power of the Granger causality test.If , there will be a combination of both effects.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-24 07:59:48","Question_id":228201}
{"_id":{"$oid":"5837a576a05283111e4d3905"},"Last_activity":"2016-08-24 07:44:58","Creator_reputation":11,"Question_score":1,"Answer_content":"To a certain extent, and more informally, your proposal is related to the Approximate Bayesian Computation (ABC) approach in which you simulate from your model and check whether or not it is similar to your original data. In your case, you are comparing the two samples using the KS test, and the performance of this approach depends on the tunning parameters (significance level, sample size, and etcetera). The following reference discusses different approaches to estimate the parameters in hidden Markov models, which are related to ABC:  Dean, Thomas A., et al. \"Parameter estimation for hidden Markov models with intractable likelihoods.\" Scandinavian Journal of Statistics 41.4 (2014): 970-987.","Display_name":"Alf","Creater_id":128791,"Start_date":"2016-08-24 07:39:57","Question_id":231476}
{"_id":{"$oid":"5837a576a05283111e4d3912"},"Last_activity":"2016-08-24 07:27:16","Creator_reputation":31,"Question_score":3,"Answer_content":"I agree that this terminology is confusing.  Bias has one meaning in both of these contexts: distance from ideal or target values, but the interpretation depends on which space we are talking about.  I'll explain what I mean with regard to the two quotes in your question.  By using mean-wise imputation, we are adding bias to our estimate.This refers to bias in the data space.  Mean-wise imputation influences the position of your estimates relative to the target values.  The bias-variance trade-off is an important subject when picking models.This refers to bias and variance in the parameter space of models.  That is, if you trained a stochastic model 1000 times, you could observe bias or variance of the parameter values.  A high bias model has consistent parameters, but they differ from an 'optimal' solution.  A high variance model will get different values for the parameters each time it's trained.","Display_name":"Alan Schoen","Creater_id":128786,"Start_date":"2016-08-24 07:11:35","Question_id":202289}
{"_id":{"$oid":"5837a576a05283111e4d3913"},"Last_activity":"2016-03-18 06:21:27","Creator_reputation":7427,"Question_score":4,"Answer_content":"The term \"bias\" has a specific definition in the statistical literature (the difference between the expected value of an estimator and the thing being estimated), but that isn't to say it loses its original, more general meaning.  Which one is intended will depend on context, and oftentimes you will have a mixture of the two.I would say the first usage is in general the less precise kind, as data imputation is a method that's used in applied problems where one need not assume that any true value of the parameter even exists.  Here it's basically synonymous with \"shrunk towards zero.\"As far as the second usage is concerned the term bias-variance trade-off does originally derive from the more formal definition of bias, but nonetheless I would still say this refers more to the general \"inflexibility\" of a model fitting procedure, and not necessarily the question of whether or not an estimated regression function is correct on average.","Display_name":"dsaxton","Creater_id":78861,"Start_date":"2016-03-17 20:54:07","Question_id":202289}
{"_id":{"$oid":"5837a576a05283111e4d3924"},"Last_activity":"2016-08-24 06:28:16","Creator_reputation":104,"Question_score":3,"Answer_content":"1) The definition of  is given in page 140 of the linked paper: it is the sequentially added explained variance when adding regressors with indices in  to a model which has regressors with indices in .Basically, it is how much more of the variance is explained if you add extra regressors , where .More explicitly, suppose you have the usual linear regression model Y = \\beta_0 + X_1 \\beta_1 + \\cdots X_p \\beta_p + \\epsilon Then  is defined as  \\text{svar}(M|S) = \\text{var}(Y|X_j, j \\in S) - \\text{var}(Y|X_j , j \\in M \\cup S)So for your example, , which is the added explained variance if you add the regressor  alone.2) To decompose the variance following Lindeman, Merenda and Gold, you can use the R package relaimpo. ","Display_name":"David Robertson","Creater_id":128762,"Start_date":"2016-08-24 06:28:16","Question_id":231442}
{"_id":{"$oid":"5837a576a05283111e4d393b"},"Last_activity":"2016-08-24 03:52:21","Creator_reputation":410,"Question_score":0,"Answer_content":"To add to the other answers suggesting pnorm...For a potentially optimal method for selecting parameters I suggest this approximation for pnorm.1.0/(1.0+exp(-1.69897*(x-mean(x))/sd(x)))This is essentially Softmax Normalization.Reference Pnorm in a pinch","Display_name":"Chris","Creater_id":70282,"Start_date":"2016-08-24 03:04:49","Question_id":1112}
{"_id":{"$oid":"5837a576a05283111e4d393c"},"Last_activity":"2011-03-15 17:27:24","Creator_reputation":438,"Question_score":0,"Answer_content":"A very simple option is dividing each number in your data by the largest number in your data.  If you have many small numbers and a few very large ones, this might not convey the information well.  But it's relatively easy; if you think meaningful information is lost when you graph the data like this, you could try one of the more sophisticated techniques that others have suggested.","Display_name":"DanB","Creater_id":3700,"Start_date":"2011-03-15 17:27:24","Question_id":1112}
{"_id":{"$oid":"5837a576a05283111e4d393d"},"Last_activity":"2011-03-15 03:49:20","Creator_reputation":14458,"Question_score":23,"Answer_content":"As often, my first question was going to be \"why do you want to do this\", then I saw you've already answered this in the comments to the question: \"I am measuring content across many different dimensions and I want to be able to make comparisons in terms of how relevant a given piece of content is. Additionally, I want to display values across these dimensions that is explicable and easily understood.\"There is no reason to normalize the data so that the max is 1 and the min is zero in order to achieve this, and my opinion is that this would be a bad idea in general. The max or min values could very easily be outliers that are unrepresentative of the population distribution. @osknows parting remark about using -scores is a much better idea. -scores (aka standard scores) normalize each variable using its standard deviation rather than its range. The standard deviation is less influenced by outliers. In order to use -scores, it's preferable that each variable has a roughly normal distribution, or at least has a roughly symmetric distribution (i.e. isn't severely skew) but if necessary you can apply some appropriate data transformation first in order to achieve this; which transformation to use could be determined by finding the best fitting Box–Cox transformation.","Display_name":"onestop","Creater_id":449,"Start_date":"2011-03-15 03:04:23","Question_id":1112}
{"_id":{"$oid":"5837a576a05283111e4d393e"},"Last_activity":"2011-03-14 14:20:55","Creator_reputation":261,"Question_score":1,"Answer_content":"My earlier post has a method to rank between 0 and 1. Advice on classifier input correlationHowever, the ranking I have used, Tmin/Tmax uses the sample min/max but you may find the population min/max more appropriate. Also look up z scores","Display_name":"osknows","Creater_id":3484,"Start_date":"2011-03-14 14:20:55","Question_id":1112}
{"_id":{"$oid":"5837a576a05283111e4d393f"},"Last_activity":"2010-08-02 17:57:51","Creator_reputation":7692,"Question_score":24,"Answer_content":"A very common trick to do so (e.g., in connectionist modeling) is to use the hyperbolic tangent tanh as the 'squashing function\".It automatically fits all numbers into the interval between -1 and 1. Which in your case restricts the range from 0 to 1.In r and matlab you get it via tanh(). Another squashing function is the logistic function (thanks to Simon for the name), provided by , which restricts the range from 0 to 1 (with 0 mapped to .5). So you would have to multiply the result by 2 and subtract 1 to fit your data into the interval between 0 and 1.Here is some simple R code which plots both functions (tanh in red, logistic in blue) so you can see how both squash:x \u0026lt;- seq(0,20,0.001)plot(x,tanh(x),pch=\".\", col=\"red\", ylab=\"y\")points(x,(1 / (1 + exp(-x)))*2-1, pch=\".\",col=\"blue\")","Display_name":"Henrik","Creater_id":442,"Start_date":"2010-08-02 07:56:35","Question_id":1112}
{"_id":{"$oid":"5837a576a05283111e4d3940"},"Last_activity":"2010-08-02 10:33:15","Creator_reputation":101,"Question_score":0,"Answer_content":"There are two ways to implement this that I use commonly. I am always working with realtime data, so this assumes continuous input. Here's some pseudo-code:Using a trainable minmax:define function peak:// keeps the highest value it has receiveddefine function trough:// keeps the lowest value it has receiveddefine function calibrate:// toggles whether peak() and trough() are receiving values or notdefine function scale:// maps input range [trough.value() to peak.value()] to [0.0 to 1.0]This function requires that you either perform an initial training phase (by using calibrate()) or that you re-train either at certain intervals or according to certain conditions. For instance, imagine a function like this:define function outBounds (val, thresh):if val \u003e (thresh*peak.value()) || val \u0026lt; (trough.value() / thresh):calibrate()// peak and trough are normally not receiving values, but if outBounds() receives a value that is more than 1.5 times the current peak or less than the current trough divided by 1.5, then calibrate() is called which allows the function to recalibrate automatically.Using an historical minmax:var arrayLength = 1000var histArray[arrayLength]define historyArray(f):histArray.pushFront(f) //adds f to the beginning of the arraydefine max(array):// finds maximum element in histArray[]return maxdefine min(array):// finds minimum element in histArray[]return mindefine function scale:// maps input range [min(histArray) to max(histArray)] to [0.0 to 1.0]main()historyArray(histArray)scale(min(histArray), max(histArray), histArray[0])// histArray[0] is the current element","Display_name":"msutherl","Creater_id":162,"Start_date":"2010-08-02 09:57:33","Question_id":1112}
{"_id":{"$oid":"5837a576a05283111e4d3941"},"Last_activity":"2010-08-02 09:49:48","Creator_reputation":null,"Question_score":3,"Answer_content":"In addition to the good suggestions by Henrik and Simon Byrne, you could use f(x) = x/(x+1).  By way of comparison, the logistic function will exaggerate differences as x grows larger.  That is, the difference between f(x) and f(x+1) will be larger with the logistic function than with f(x) = x/(x+1).  You may or may not want that effect.","Display_name":"stoplan","Creater_id":null,"Start_date":"2010-08-02 09:49:48","Question_id":1112}
{"_id":{"$oid":"5837a576a05283111e4d3942"},"Last_activity":"2010-08-02 08:20:11","Creator_reputation":2392,"Question_score":8,"Answer_content":"Any sigmoid function will work:The top half of the logistic function (multiply by 2, subtract 1)The error functiontanh, as suggested by Henrik.","Display_name":"Simon Byrne","Creater_id":495,"Start_date":"2010-08-02 08:20:11","Question_id":1112}
{"_id":{"$oid":"5837a576a05283111e4d394f"},"Last_activity":"2016-08-24 03:31:24","Creator_reputation":152693,"Question_score":2,"Answer_content":"What's ?What's  for some constant ?Choose a value of  such that ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-24 03:31:24","Question_id":231443}
{"_id":{"$oid":"5837a576a05283111e4d395c"},"Last_activity":"2016-08-23 04:16:24","Creator_reputation":21,"Question_score":1,"Answer_content":"I think the answer can be found here: http://stackoverflow.com/questions/21082396/multinomial-logistic-multilevel-models-in-rand more specifically, follow the calculation here: http://rpubs.com/bbolker/11703The idea is that instead of fitting a single model, a series of binomial contrasts are used (i.e. dependent variable...model1: category 0 vs category 1; Model 2: category 0 vs category 2; and Model 3: category 1 vs category 2.)","Display_name":"ReadBeard","Creater_id":86393,"Start_date":"2016-08-23 04:16:24","Question_id":231236}
{"_id":{"$oid":"5837a576a05283111e4d3968"},"Last_activity":"2016-08-24 03:20:02","Creator_reputation":38,"Question_score":0,"Answer_content":"I don't know if you still need help but just in case: SMC is basically SIS with resampling. It's quite a new area so people use different terms for the same thing. I believe that in case of this paper, in SMC they look only at the marginal distribution (so they integrate out the past) while in SIS they consider a space of increasing dimensions.Here is a bit more about that: Difference between Sequential Importance Resampling and Sequential Monte Carlo.","Display_name":"Paula","Creater_id":126827,"Start_date":"2016-08-24 03:20:02","Question_id":194967}
{"_id":{"$oid":"5837a576a05283111e4d3975"},"Last_activity":"2016-08-23 23:17:09","Creator_reputation":341,"Question_score":1,"Answer_content":"This is the exponential form: ","Display_name":"Nick","Creater_id":111637,"Start_date":"2016-08-23 23:17:09","Question_id":231407}
{"_id":{"$oid":"5837a576a05283111e4d3984"},"Last_activity":"2016-08-24 02:14:31","Creator_reputation":25265,"Question_score":0,"Answer_content":"As I understand you your data comes from the mixture of two distributions with density f(x_i) = \\pi f_1(x_i) + (1 - \\pi) f_2(x_i) where  is proportion of first class and  are the two components. You have estimated probabilistic labels for the components and want to calculate the variance of your components.First, recall that weighted variance is defined as \\sigma^2 = \\sum_{i=1}^n w_i (x_i - \\bar x)^2 where  are non-negative weights such that . In your case weights for -th component is w_{ik} = \\frac{ \\gamma_{ik} }{\\sum_{n=1}^N \\gamma_{nk}}  where \\gamma_{ik} = \\frac{\\pi_k f_k(x_i)}{\\sum_{j=1}^K \\pi_j f_j(x_n)}  As you noticed,  can be thought as posterior probability that -th observation belongs to -th component.If you wonder if this approach is valid, then you can read about EM algorithm (check also code example). For example, EM algorithm for estimating parameters of mixture of normal distributions would calculate variances of normal distributions in the M step exactly like described above.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-23 06:49:46","Question_id":231277}
{"_id":{"$oid":"5837a576a05283111e4d3995"},"Last_activity":"2016-08-24 01:53:32","Creator_reputation":181,"Question_score":3,"Answer_content":"In the documentation for the mgcv package, there is a page describing the spline-based smoothers available. Moreover Wood (the package author) offers the following advice:  Broadly speaking the default penalized thin plate regression splines  tend to give the best MSE performance, but they are slower to set up  than the other bases.Since in your case you have less than 200 data points, so I don't think you will run into computational issues with the default method. In section 4.1 of Wood's book \"Generalized Additive Models: an introduction with R\", he has a summary of the major smoothing bases (Thin plate regression splines, Duchon splines, Cubic regression splines, P-splines) available in mgcv along with a discussion of their merits and other practical considerations. I have found the book quite helpful in developing my understanding of GAMs. ","Display_name":"NaN","Creater_id":17760,"Start_date":"2016-08-24 00:10:57","Question_id":231399}
{"_id":{"$oid":"5837a577a05283111e4d3a68"},"Last_activity":"2016-08-22 21:58:43","Creator_reputation":5189,"Question_score":1,"Answer_content":"The assumption here that  and  are uncorrelated without mean independence holding is impossible when  takes only two values. Intuitively, correlation measures the linear relationship between the values, so for mean independence to not hold in the presence of zero correlation, the mean  should be a nonlinear function of . But with only two possible values for , there is no room for nonlinearity.ProofLet us assume  and denote the two possible values of  by  and . Using the two assumptions and decomposing over , we get \\begin{equation}\\begin{cases}\\mathbb{P}(D_i=d_1)\\,\\mathbb{E}(\\epsilon_i \\mid D_i = d_1) + \\mathbb{P}(D_i=d_2)\\,\\mathbb{E}(\\epsilon_i \\mid D_i = d_2) = 0 \\\\ \\mathbb{P}(D_i=d_1)\\,\\mathbb{E}(\\epsilon_i \\mid D_i = d_1)\\,d_1 + \\mathbb{P}(D_i=d_2)\\,\\mathbb{E}(\\epsilon_i \\mid D_i = d_2)\\,d_2 = 0\\end{cases} \\end{equation}By solving this system of equations for  and , we see that either  orThe first case would mean  has only one possible value (and mean independence would trivially hold). Assuming both probabilities *, the second case then implies , that is, mean independence. Thus, mean independence follows from the assumptions.*If one of the probabilities is , the corresponding  can technically obtain any value, but then the model would correspond to  having only one possible values.","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-22 21:58:43","Question_id":230200}
{"_id":{"$oid":"5837a577a05283111e4d3a74"},"Last_activity":"2016-04-16 08:12:52","Creator_reputation":20442,"Question_score":1,"Answer_content":"You could look at the Mean Absolute Scaled Error (MASE - see its tag wiki for more information). Basically, it scales the Mean Absolute Error (MAE) by the MAE that the naive random walk would have achieved in-sample.To deal with your outliers, you could either use the median instead of the mean, or use a trimmed or winsorized mean.However, I'd rather try to understand where an actual of 2e6 with a forecast of 1 came from. This sounds like it could have major implications on whatever process relies on your forecast, whether or not you include it in your accuracy calculation.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2016-04-16 08:12:52","Question_id":86215}
{"_id":{"$oid":"5837a577a05283111e4d3a83"},"Last_activity":"2016-08-22 20:05:49","Creator_reputation":2762,"Question_score":3,"Answer_content":"In auto differentiation systems basically when you define an operator (op), you also define together how its derivatives are computed (of course most of the common ops are already provided). So after you write a function by stacking a series of ops, the program can figure out by itself how should the corresponding derivatives be computed (usually by keeping some computation graphs and using the chain rule).The benefit is obvious as it saves us from working out the math, writing the code, verifying the derivatives numerically...Here's how to define an op in Theano and Tensorflow.","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-08-22 04:10:22","Question_id":230973}
{"_id":{"$oid":"5837a577a05283111e4d3a90"},"Last_activity":"2016-02-06 16:54:26","Creator_reputation":3256,"Question_score":1,"Answer_content":"I guess you have tunneled in on tuning too many non-useful hyper parameters, because an easy to use grid-search functionality allowed you to do so.Notice all your explained variances only differ on the fourth digit. You have found, what appears to be a negligible better model settings. But even that you cannot be sure off because:the RF model is non-deterministic and performance will vary slightlya CV only estimates future model performance with a limited precisionnfold CV is not perfect reproducible and should be repeated to increase precisionGrid tuning should be performed with nested CV, but that is not your problem here I think.Only \"grid-tune\" max_features. It has only 6 possoble values. You can run each 5 times and plot it. Check if some setting is repetitively better, probably you find anything from 2-4 perform fine. Max_depth is by default unlimited and that is optimal as long data is not very noisy. You set it to 25, which in practice is unlimited because already =32000 and you \"only\" have 26000 samples. Changing these other hyper parameter will only give you shorter training times(useful) and/or more robust models. Thumb-rule: as explained variance is way above 50%, you do not need to make your model more robust by limiting depth of trees (max_depth, min_samples_split) to  e.g. 3. Max_depth 15 is quite deep, and probably plenty deep enough, just as 2000 are trees enough. So raising and lowering number of trees and depth within the quite fine range does not change anything, and it will be really hard and non-rewarding to find the true best setting.So you have performed a grid search and learned that RF will have the same performance in the parameter space you have tested.If you obtain a testset from a different source you should expect a drop in performance. Your CV only estimate the model performance, if the future test set was drawn from the exactly same population.With 1400 tests, the sample error alone could swing the measured performance +/- 0.03, I guess.If your swapped e.g. to boosting algorithms grid-tuning of multiple parameters would be a more rewarding tool. To improve your model maybe you can refine your features. Look to variable importance, to see what features work well. Could you maybe derive new features with an even higher variable importance? Since your explained variance is quite high(low noise), you may benefit from swapping to xgboost. You may also spend time wondering if this chase of a better model performance of some target by some metric (explained variance) is useful specifically for your purpose. Maybe you don't need the model being that accurate when predicting large values, so you log transpose your target e.g. Maybe you only want to rank your predictions so explained variance could be replace with Spearman rank coefficient.happy modelling:)","Display_name":"Soren Havelund Welling","Creater_id":49132,"Start_date":"2016-02-06 16:28:11","Question_id":194226}
{"_id":{"$oid":"5837a577a05283111e4d3a9d"},"Last_activity":"2016-08-22 19:05:34","Creator_reputation":2762,"Question_score":0,"Answer_content":"They both get zero or very small gradients so they can barely get trained.The difference is, the activation values of “dead” ReLU neurons are almost always zero, whereas the activation values of saturated sigmoid neurons are close to 0 or 1.","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-08-17 22:54:30","Question_id":230399}
{"_id":{"$oid":"5837a577a05283111e4d3a9e"},"Last_activity":"2016-08-18 01:37:41","Creator_reputation":176,"Question_score":0,"Answer_content":"Both of them will have very small gradients, hence both act as a showstopper to learning.Difference is that the likelihood of dead Relu neurons is much less as compared to saturated sigmoidsThe gradient of a sigmoid is:S′(a)=S(a)(1−S(a))When we start learning useful features in the later layers, the activations S(a) are high and thus the gradient or the learning of the previous layers starts reducing.Not matter how careful you are with network parameters you will encounter saturated sigmoids and vanishing gradients in your network.For RelU however the gradients are a constant no matter how good the activation/features are in your last layers, thus the previous layers continue to learn. Unless you really mess up the learning rate, or the weight decay terms, or your bias which forces the RelU's to operate in negative regime for all inputs, it is hard to get a lot of \"dead\" neurons as compared to saturated sigmoids.","Display_name":"Amitoz Dandiana","Creater_id":103889,"Start_date":"2016-08-18 01:37:41","Question_id":230399}
{"_id":{"$oid":"5837a577a05283111e4d3aad"},"Last_activity":"2014-01-05 12:52:05","Creator_reputation":2314,"Question_score":12,"Answer_content":"Rob Hyndman is doing some active research on forecasting with nueral nets.  He recently added the nnetar() function to the forecast package that utilizes the nnet package you reference to fit to time series data.http://cran.r-project.org/web/packages/forecast/index.htmlThe example from the help docs:fit \u0026lt;- nnetar(lynx)fcast \u0026lt;- forecast(fit)plot(fcast)Rob gives more context in this specific section of his online text: Forecasting: principles and practice.(And a big thanks to Rob obviously.)","Display_name":"Shea Parkes","Creater_id":8120,"Start_date":"2013-01-03 11:15:43","Question_id":46887}
{"_id":{"$oid":"5837a577a05283111e4d3aba"},"Last_activity":"2014-06-27 06:49:34","Creator_reputation":1092,"Question_score":4,"Answer_content":"To answer your specific question, you can try the nnls package which does least squares regression with non-negative constraints on the coefficients. You can use it to get the signs you want by changing the signs of the appropriate predictors.By the way, here is a very simple way to create a dataset to demonstrate how it is possible to have positive correlations and negative regression coefficients.\u0026gt; n \u0026lt;- rnorm(200)\u0026gt; x \u0026lt;- rnorm(200)\u0026gt; d \u0026lt;- data.frame(x1 = x+n, x2= 2*x+n, y=x)\u0026gt; cor(d)      x1        x2         y x1 1.0000000 0.9474537 0.7260542 x2 0.9474537 1.0000000 0.9078732 y  0.7260542 0.9078732 1.0000000\u0026gt; plot(d)\u0026gt; lm(y~x1+x2-1, d)Call:lm(formula = y ~ x1 + x2 - 1, data = d)Coefficients:x1  x2  -1   1  ","Display_name":"Innuo","Creater_id":7690,"Start_date":"2014-06-26 10:03:44","Question_id":104890}
{"_id":{"$oid":"5837a577a05283111e4d3abb"},"Last_activity":"2014-06-27 01:18:13","Creator_reputation":152503,"Question_score":9,"Answer_content":"beware the distinction between the marginal correlation and the partial correlation (correlation conditional on other variables). They may legitimately be of different sign.That is  may in fact be negative while the regression coefficient in a multiple regression is positive. There is not necessarily any contradiction in those two things. See also Simpson's paradox, which is somewhat related (especially the diagram). In general you cannot infer that a regression coefficient must be of one sign merely based on an argument about the marginal correlation.Yes, it's certainly possible to constrain regression coefficients to be  or *. There are several ways to do so; some of these can be done readily enough in R, such as via nnls. See also the answers to this question which mention a number of R packages and other possible approaches. However I caution you against hastily ignoring the points in 1. just because many of those are easily implemented.* (you can use programs that do non-negative to do non-positive by negating the corresponding variable)","Display_name":"Glen_b","Creater_id":805,"Start_date":"2014-06-26 18:32:22","Question_id":104890}
{"_id":{"$oid":"5837a577a05283111e4d3abc"},"Last_activity":"2014-06-26 09:55:48","Creator_reputation":57702,"Question_score":9,"Answer_content":"There may well be such a way but I would say that it is not advisable in your circumstances.If you have a result that is impossible either:1) There is a problem with your data2) There is a problem with your definition of \"impossible\"or3) You are using the wrong methodFirst, check the data. Second, check the code. (Or ask others to check it). If both are fine then perhaps something unexpected is happening. Fortunately for you, you have a simple \"impossibility\" - you say two variables cannot be positively correlated. So, make a scatter plot and add a smoother and see. A single outlier might cause this; or it might be a nonlinear relationship. Or something else.But, if you are lucky, you've found something new. As my favorite professor used to say \"If you're not surprised, you haven't learned anything\". ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2014-06-26 09:55:48","Question_id":104890}
{"_id":{"$oid":"5837a577a05283111e4d3ac9"},"Last_activity":"2016-08-22 17:04:25","Creator_reputation":12722,"Question_score":8,"Answer_content":"There are a couple excellent answers already, but I'd like to add one more perspective (hinted at in @Antoni's answer).An idempotent matrix satisfies the matrix equation X^2 = X or X^2 - X = 0 Which we can factor X(X - I) = 0 This means there are relatively few possibilities for the minimal polynomial of The Minimal Polynomial is :In this case  immediately, we have the zero matrix.  Not very interesting.The Minimal Polynomial is :In this case , so .  This is the case you are thinking of.The Minimal Polynomial is :This is the interesting \"in between\" case.Because the minimal polynomial splits into linear factors, the matrix  is diagonalizable, with only  and 's on the resulting diagonal.Another way to look at this:  has a full linearly independent set of eigenvectors, say e_1, e_2, \\ldots, e_{k_1}, f_1, f_2, \\ldots, f_{k_2} With .  Some of the eigenvectors have an associated eigenvalue of zero X e_1 = 0, X e_2 = 0, \\ldots, X e_{k_1} = 0and the rest have an eigenvalue of one X f_1 = f_1, X f_2 = f_2, \\ldots, X f_{k_2} = f_{k_2}Now a geometric picture emerges, and it justifies the name projection matrix.  If we look at the subspace spanned by the 's, then the image of any vector is in this subspace X (a_1 e_1 + \\cdots + a_{k_1} e_{k_1} + b_1 f_1 + \\cdots + b_{k_2} f_{k_2}) = b_1 f_1 + \\cdots + b_{k_2} f_{k_2} and, by the same calculation, the mapping restricted to this subspace is indeed the identity.  You see, the map is really a projection into the subspace spanned by the 's.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-08-22 16:52:10","Question_id":231135}
{"_id":{"$oid":"5837a577a05283111e4d3aca"},"Last_activity":"2016-08-22 16:26:53","Creator_reputation":7682,"Question_score":10,"Answer_content":"Just to illustrate. If you are making reference to the unregularized OLS projection matrix, here is a \"real life\" example showing that there has to be a misconception in your OP. We are regressing miles-per-gallon over vehicle weight of the mtcars dataset, and choosing (to be able to copy and paste), the first 5 rows of data only:dat = mtcars[5,]    dat                   mpg cyl disp  hp drat    wt  qsec vs am gear carbMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2Now let's generate manually the projection matrix. First the model matrix and y:fit = lm(mpg ~ wt, data = dat)X = model.matrix(fit)y = dat[,1]Now the projection matrix:Pr = X %*% ((solve(t(X) %*% X)) %*% t(X))Idempotent?all.equal(Pr,Pr%*%Pr)[1] TRUECheck!Symmetric?all.equal(Pr, t(Pr))[1] TRUECheck!Square? dim(Pr)[1] 5 5Check!Equal to the identity matrix? Well, no... Here is Pr:                   Mazda RX4 Mazda RX4 Wag  Datsun 710 Hornet 4 Drive Hornet SportaboutMazda RX4         0.29313831     0.2064585  0.39511457     0.09088541        0.01440322Mazda RX4 Wag     0.20645850     0.2004479  0.21352984     0.19243366        0.18713015Datsun 710        0.39511457     0.2135298  0.60874366    -0.02858313       -0.18880494Hornet 4 Drive    0.09088541     0.1924337 -0.02858313     0.32783133        0.41743273Hornet Sportabout 0.01440322     0.1871301 -0.18880494     0.41743273        0.56983885Following up on the reply with additional clarification in the OP (as well as now the change in the title),  the argument was ; ; hence . But the orthogonal projection matrix is not invertible, as nicely explained here. So the equations would only hold in the case of the identity matrix. In fact, it's easy to picture it:  will project vectors orthogonal to the column space of  (think error terms) into zero - hence, a projection matrix will have a null space; and hence they are not invertible.For an identity matrix with column vectors in , each vector will be projected not onto a subspace of , but onto  itself. It can be considered a special form of projection matrix, and the only one invertible.","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-08-22 11:35:29","Question_id":231135}
{"_id":{"$oid":"5837a577a05283111e4d3acb"},"Last_activity":"2016-08-22 11:22:13","Creator_reputation":5787,"Question_score":4,"Answer_content":"An idempotent matrix https://en.wikipedia.org/wiki/Idempotent_matrix (see also https://en.wikipedia.org/wiki/Projection_(linear_algebra) ) is a matrix which equals its square.  So  being idempotent means that .  The identity matrix is idempotent, but is not the only such matrix.Projection matrices need not be symmetric, as the the 2 by 2 matrix whose rows are both , which is idempotent, demonstrates. This provides a counterexample to your claim.  By (pre-)multiplying both dies of , you have lost solutions.  Edit: In response to the comment by @Antoni Parellada , the OLS projection matrix  is idempotent and symmetric, but in general it is not equal to the identity matrix, although it is possible that it could equal the identity matrix in some special (unusual) case.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-22 11:05:35","Question_id":231135}
{"_id":{"$oid":"5837a577a05283111e4d3ad8"},"Last_activity":"2016-08-22 16:31:12","Creator_reputation":1,"Question_score":0,"Answer_content":"It is not correct to say that \"if B happened\", the X=15 happened too.\" Instead, if B \"happened\", that is the new treatment is better/works 60% of the time, then what happened previously does not tell us more about the distribution of future outcomes X. Another way of saying that is: conditional on B being true, future events Y do not depend on X.","Display_name":"user128604","Creater_id":128604,"Start_date":"2016-08-22 16:31:12","Question_id":231185}
{"_id":{"$oid":"5837a577a05283111e4d3ae9"},"Last_activity":"2016-08-22 15:34:11","Creator_reputation":5787,"Question_score":1,"Answer_content":"I don't know whether this winds up being a good thing to do, but you can express the distribution of   conditional on , which is a 1D Normal, using the standard Schur complement approach shown in https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions .Let  be the means of .Denote  as the diagonal matrix with entries  and , i.e., the covariance matrix of . Its inverse is obtained by inverting the diagonal elements. conditional on  is Normal and hasmean = \\mu_3 + [k_{13},k_{23}] K_{12}^{-1} [x_1 - \\mu1,x_2 - \\mu_2]^{T}and variance = k_{33} - [k_{13},k_{23}] K_{12}^{-1} [k_{13},k_{23}]^T","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-22 14:28:27","Question_id":231141}
{"_id":{"$oid":"5837a577a05283111e4d3af6"},"Last_activity":"2016-08-22 15:15:42","Creator_reputation":25275,"Question_score":2,"Answer_content":"You have  distinct outcomes, where each can be sampled with some probability. This means that we are talking about categorical distribution that is parametrized by vector of probabilities . In your case you want to sample the \"correct\" answer with probability  and rest of the answers uniformly each with probability . The only question you need to ask yourself is how much likely should the \"correct\" answer be selected. If it's  then you sample all the answers uniformly, if it's greater than you make it more likely. In R you can simulate values from categorical distribution using sample.int(k, n, p, replace = TRUE) where k is the number of categories, n is sample size and p is vector of probabilities, or rcat function from extraDistr package.However if I were you I'd consider if this is a valid model for simulation. I guess it is rarely the case that either answers are given uniformly, or that single answer is more common while other are uniformly distributed. There are available solutions for model-based simulations, e.g. based on Item Response Theory models that are more realistic than your idea (check e.g. psych package for R)","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-22 15:15:42","Question_id":231172}
{"_id":{"$oid":"5837a577a05283111e4d3b03"},"Last_activity":"2016-08-22 15:07:53","Creator_reputation":738,"Question_score":0,"Answer_content":"Given the context of your data, you have two factors:Did the person receive a letter or a text? Did the personal follow through with the recall? (Outcome: 1/0)It seems you want to compare whether one factor was more effective than the other as well. Given that context, I would suggest the z-test. Here is a referrence:https://onlinecourses.science.psu.edu/stat414/node/268The Z-statistic is fairly straightforward to compute, and you can test whether group 1 has a statistically significant higher proportion of success than group 2.Also, if you can resolve the other issues with your data, I would. That would ensure the integrity of your statistical results.Hope this helps.","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-22 15:07:53","Question_id":231173}
{"_id":{"$oid":"5837a577a05283111e4d3b12"},"Last_activity":"2016-08-22 14:58:33","Creator_reputation":121,"Question_score":0,"Answer_content":"Yes, it's correct. To check , I'll directly calculate the F.I. for  from an exponential distribution with rate parameter ; i.e.  for .Then, since our data is i.i.d., we get that the Fisher information .Here, \\begin{align*}i_y(\\theta) \u0026amp;= - E \\left[ \\frac{\\partial^2}{\\partial \\lambda^2} \\ell(\\lambda)  \\right] = -E \\left[ - \\frac{1}{\\lambda^2} \\right] = \\frac{1}{\\lambda^2}\\end{align*}and multiplying by  agrees with your answer.","Display_name":"user288742","Creater_id":98029,"Start_date":"2016-08-22 14:58:33","Question_id":231112}
{"_id":{"$oid":"5837a577a05283111e4d3b1f"},"Last_activity":"2016-08-22 14:54:47","Creator_reputation":1,"Question_score":0,"Answer_content":"I think you could do pretty well simply by adding one for every won pairing and subtracting one for every loss. Over time low priority things will be heavily negative. High priority things highly positive. Keep asking people to compare pairs they haven't seen before that rank close together. ","Display_name":"David Bramwell","Creater_id":128597,"Start_date":"2016-08-22 14:54:47","Question_id":231093}
{"_id":{"$oid":"5837a577a05283111e4d3b29"},"Last_activity":"2016-08-22 14:46:54","Creator_reputation":118,"Question_score":0,"Answer_content":"It turns out that the count of the events gives the more reliable results. The reason seems to be the method I have calculated the mean inter arrival times for each weekday / hour combination wich are merely the means of the inter arrival times that happen to lie in the given combination. By doing this I shortened the interval by a random time.As an example:If you count 4 events in an hour with inter arrival times of 18 minutes (e1 to e2), 6 minutes (e2 to e3) and 12 minutes (e3 to e4), the sum of the inter arrival times is 36 minutes and the mean is 12 minutes. However, the total duration of the interval is 60 minutes, not 36 minutes, thus resulting in an error when one tries to use the measured inter arrival times.In addition, the method I used also used the inter arrival time to the next event in the following our in the current hour, leading to additional error.","Display_name":"user2035177","Creater_id":37477,"Start_date":"2016-08-22 14:46:54","Question_id":229077}
{"_id":{"$oid":"5837a577a05283111e4d3b38"},"Last_activity":"2016-08-22 14:23:07","Creator_reputation":166,"Question_score":0,"Answer_content":"I had this question for myself and found three (really only two) assumptions that are important and could be violated in practice.  I decided to write what I found here since this question helped to think about it.Representative of Future DataI personally don't like when people bring this up because NO algorithm does well when your training data is different than the test data. This is the whole point of the \"no free lunch\" theorem that in order to do better than random guessing you must have additional information or anything can happen.No Information LeakageOkay assuming we are even in a universe where historical data is helpful then we have to make sure there is no information leakage.  By that I mean information from the validation set leaks to the training set. When we partition the data into K-folds and designate one fold to be the validation set then we have \"lost\" information.  K-fold cross validation uses that \"lost\" information to estimate test accuracy.However, lets consider an extreme case.  What happens if the model you used could completely guess every single point in the validation set with just the training set?  Then instead of K-fold cross validation you are instead running your model k times getting exactly the same result.  Obviously that would be no help whatsoever on estimating test accuracy.In practice you couldn't tell so easy because there could just be some information leakage.  A practical example is looking at the sales of different years of the same make and model of car.  Car models don't change much from year to year and so sales will be really similar from year to year. If you have information from 1990-2016 and then take out 2000 as your validation partition then you haven't lost much information in your model.Bias vs Variance trade offK-fold cross validation make any model more complicated.  That is because we believe the model is very biased by the training data.  We add variance into the model (which parameters to be picked) to decrease bias.  This is a known trade-off in statistics that has no clear cut optimal solution.  I think this is basically a corollary of information leakage assumption though.DRTL (Summary)K-fold cross validation assumes that the good historical data gives a very biased model and isn't too redundant. Otherwise we are now in the domain of the free lunch theorem where even \"anti-cross validation\" is a good way to go.Some references:This one claims information leakage is only assumptionGood paper with leave-one-out not working on Iris dataInteresting paper with samples not being representative","Display_name":"AWashburn","Creater_id":112626,"Start_date":"2016-08-22 14:23:07","Question_id":206753}
{"_id":{"$oid":"5837a577a05283111e4d3b45"},"Last_activity":"2016-08-22 14:07:17","Creator_reputation":4423,"Question_score":17,"Answer_content":"Yes, Regularization can be used in all linear methods, no matter it is a regression and classification problem. I would like to show you that there are not too much difference between regression and classification: the only difference is the loss function.Specifically, there are three major components of linear method, Loss Function, Regularization, Algorithms. Where loss function plus regularization is the objective function in the problem in optimization form and the algorithm is the way to solve it.In loss function setting, we can have different loss in both regression and classification cases. For example, Least squares and least absolute deviation are loss for regression. And their math representation are  and .On the other hand, logistic loss and hinge loss are for classification. Their math representation are  and . (Here the definition of  is a little bit unusal, please see the comment section.)In regularization setting, you mentioned about the L1 and L2 regularization, there are also other forms, which will not be listed here.Therefore, in a high level a linear method is\\underset{w}{\\text{minimize}}~~~ \\sum_{x,y} L(w^{\\top} x,y)+\\lambda h(w)If you replace the Loss function from regression setting to logistic loss, you get the logistic regression with regularization.For example, in ridge regression, the optimization problem is\\underset{w}{\\text{minimize}}~~~ \\sum_{x,y} (w^{\\top} x-y)^2+\\lambda w^\\top wIf you replace the loss function with logistic loss, the problem becomes\\underset{w}{\\text{minimize}}~~~ \\sum_{x,y} \\log(1+\\exp{(-w^{\\top}x \\cdot y)})+\\lambda w^\\top wHere you have the logistic regression with L2 regularization.This is how it looks like in a toy synthesized binary data set. The left figure is the data with the linear model (decision boundary). The right figure is the objective function contour (x and y axis represents the values for 2 parameters.). The data set was generated from two Gaussian, and we fit the logistic regression model without intercept, so there are only two parameters we can visualize in the right sub-figure.The blue lines are the logistic regression without regularization and the black lines are logistic regression with L2 regularization. The blue and black points in right figure are optimal parameters for objective function.In this experiment, we set a large , so you can see two coefficients are close to . In addition, from the contour, we can observe the regularization term is dominated and the whole function is like a quadratic bowl.Here is another example with L1 regularization.Note that, the purpose of this experiment is trying to show how the regularization works in logistic regression, but not argue regularized model is better.Here are some animations about L1 and L2 regularization and how it affects the logistic loss objective. In each frame, the title suggests the regularization type and , the plot is objective function (logistic loss + regularization) contour.Some notation comments.  and  are column vectors, is a scalar. So the linear model . If we want to include the intercept term, we can append  as a column to the data.In regression setting,  is a real number and in classification setting . Note it is a little bit strange for the definition of  in classification setting. Since most people use  to represent a predicted value of . In our case,  is a real number, but not in . We use this definition of  because we can simplify the notation on logistic loss and hinge loss.Also note that, in some other notation system, , the form of the logistic loss function would be different.","Display_name":"hxd1011","Creater_id":113777,"Start_date":"2016-08-08 06:33:10","Question_id":228763}
{"_id":{"$oid":"5837a577a05283111e4d3b46"},"Last_activity":"2016-08-11 12:51:18","Creator_reputation":21578,"Question_score":7,"Answer_content":"A shrinkage/regularization method that was originally proposed for logistic regression based on considerations of higher order asymptotic was Firth logistic regression... some while before all of these talks about lasso and what not started, although after ridge regression risen and subsided in popularity through 1970s. It amounted to adding a penalty term to the likelihood,l^*(\\beta) = l(\\beta) + \\frac12 \\ln |i(\\beta)|where  is the information matrix normalized per observation. Firth demonstrated that this correction has a Bayesian interpretation in that it corresponds to Jeffreys prior shrinking towards zero. The excitement it generated was due to it helping fixing the problem of perfect separation: say a dataset  would nominally produce infinite ML estimates, and glm in R is still susceptible to the problem, I believe.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-11 12:51:18","Question_id":228763}
{"_id":{"$oid":"5837a577a05283111e4d3b47"},"Last_activity":"2016-08-08 06:28:03","Creator_reputation":133,"Question_score":4,"Answer_content":"Yes, it is applicable to logistic regression. In R, using glmnet, you simply specify the appropriate family which is \"binomial\" for logistic regression. There are a couple of others (poison, multinomial, etc) that you can specify depending on your data and the problem you are addressing.","Display_name":"godspeed","Creater_id":103245,"Start_date":"2016-08-08 06:28:03","Question_id":228763}
{"_id":{"$oid":"5837a577a05283111e4d3b54"},"Last_activity":"2016-08-22 13:18:48","Creator_reputation":21,"Question_score":3,"Answer_content":"To add to the excellent suggestions above, I would say if you are interested in getting a firm grasp on more basic concepts in probability and statistics, \"From Algorithms to Z-Scores: Probabilistic Computing in Statistics\" is an excellent primer on using computers to understand some of the most important beginner/intermediate concepts in probability theory and stochastic processes.  I'll also second either \"An Introduction to Statistical Learning\" or \"Elements of Statistical Learning\" (ESL) as an introduction to machine learning (ML).  I think ESL in particular is amazing, but it does take a much more mathematics-heavy look at the ML concepts, so if you only consider yourself \"okay\" at stats, you might want to give it a read once you've gotten more experience with ML.If you're interested in Machine Learning for the sake of being employed or solving problems, getting hands-on experience is key.  Take some introduction to data science/machine learning courses.  Andrew Ng does an amazing introduction to machine learning in his course at Coursera here.  I would also suggest you download some datasets and start playing around with them.  If you haven't already, download R and RStudio (in my opinion, more friendly to beginners than Python or Matlab), and sign up at kaggle and do some of their beginner problems.  They have great walkthroughs that can get you using ML with basically no idea what's actually happening, but it gives you an idea about the kind of steps you'd need to take to actually implement an ML solution.I'd personally encourage a combination of starting off using ML tools without really knowing what they do (using Kaggle datasets or similar); and learning fundamental concepts like cross-validation, overfitting, using confusion matrices, different measures of how good a model is, etc.  To me, it's much more important to know how to use the algorithms, and knowing how to identify when things are working/aren't working, than it is to understand how the algorithms work.","Display_name":"Blue_vision","Creater_id":95440,"Start_date":"2016-08-22 13:18:48","Question_id":231092}
{"_id":{"$oid":"5837a577a05283111e4d3b55"},"Last_activity":"2016-08-22 10:31:27","Creator_reputation":19081,"Question_score":5,"Answer_content":"Since you're interested in machine learning, I'd skip probability and mesaure, and jump right into the ML. Andrew Ng's course is a great place to start. You can literally finish it in two weeks.Play with what you've learned for a few weeks, then go back to the roots and study some probabilities. If you're an engineer, then I'm puzzled with how you managed to skip in in college. It used to be the required course in engineering. Anyhow, you can catch up by taking MIT OCW course here.I don't think you need measure theory. Nobody needs measure theory. Those who do, they won't come here to ask, because their advisor will tell them which course to take. If you don't have an advisor then you definitely don't need it. Tautology, but true.The thing with a measure theory's that you can't learn it by \"easy reading\". You have to do the exercises and problems, basically, do it hard way. That's virtually impossible outside of the class room, in my opinion. The best option here is to take a class at the local college, if they offer such. Sometimes, PhD level probabilities course will do the measure and probabilities in one class, which is probably the best deal. I would not recommend taking a pure measure theory class in Math department, unless you really want to torture yourself, though in the end you'd be greatly satisfied.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-08-22 10:13:12","Question_id":231092}
{"_id":{"$oid":"5837a577a05283111e4d3b56"},"Last_activity":"2016-08-22 09:18:34","Creator_reputation":2088,"Question_score":6,"Answer_content":"you don't need measure theory. Measure theory is used by mathematicians to justify other mathematical procedures eg taking limits of integrals approximations.  Most engineers would not have studied measure theory, they would just use the results. The math knowledge required for ML is roughly characterised by being able to integrate a multivariate Gaussian- If you are confident about that then you probably have the multivariable calculus,linear algebra and probability theory background necessary.I would recommend Think Stats by Allen Downey - which aims to teach probability/statistics to programmers. The idea is to leverage programming expertise to do simulations and therefore understand probability theory/statistical methods.  allen downey blog (he has written others )Think stats (free) pdf)","Display_name":"seanv507","Creater_id":27556,"Start_date":"2016-08-22 07:37:45","Question_id":231092}
{"_id":{"$oid":"5837a577a05283111e4d3b57"},"Last_activity":"2016-08-22 09:13:11","Creator_reputation":825,"Question_score":13,"Answer_content":"I think there exists two very good and popular references for you (I started with these ones as well having a background of master in actuarial science):An Introduction to Statistical Learning (with application in R) by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. It is freely available on the site, pretty comprehensive and easy to understand with pratical examples. You can start learning many things even without a very strong statistical background, this reference is good for various profils and includes adequate number of popular algorithms together with its implementation in R without going deep into the mathematical details.The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, Jerome Friedman. Comparing to the first one, this book goes deeper into the mathematical aspects if you want to explore further on the particular algorithms that you find useful for you. (is is free as well)And of course Cross Validated is one of the best sources where you can learn many things, for me: best pratices, statistical misunderstanding and misuse, and many more. After several years of learning at schools / universities as well as seft-learning, I found that my knownledge is too limited when I first went to Cross Validated. I continue to go here every day since the first visit and learn so much. ","Display_name":"Metariat","Creater_id":78313,"Start_date":"2016-08-22 07:15:54","Question_id":231092}
{"_id":{"$oid":"5837a577a05283111e4d3b58"},"Last_activity":"2016-08-22 08:48:26","Creator_reputation":50,"Question_score":2,"Answer_content":"For machine learning, I think Machine Learning: The Art and Science of Algorithms that Make Sense of Data by Peter Flach can be a good resource to start with. It gives a general introduction to machine learning with intuitive examples, and is suitable for beginners. I like this book particularly because of the last chapter, which deals with machine learning experiments. While learning about machine learning, getting to know different models is not enough, and one should be able to compare different machine learning algorithms. I think this book has made it easier to understand how to compare those algorithms.Lecture slides can be found here.","Display_name":"user1219801","Creater_id":111882,"Start_date":"2016-08-22 07:53:51","Question_id":231092}
{"_id":{"$oid":"5837a577a05283111e4d3b59"},"Last_activity":"2016-08-22 08:18:17","Creator_reputation":51,"Question_score":6,"Answer_content":"Here are a couple of free online courses that I've heard are highly recommended:http://projects.iq.harvard.edu/stat110/home  (Depending on your current comfort with probability theory.  Dr. Blitzstein's course became very popular at Harvard even for those who weren't into stats/probability.  I've watched a few of the lectures for my own review and found them very helpful. )https://www.coursera.org/learn/machine-learning (This is the current version of one of Stanford's first massive online courses by Andrew Ng, who ended up co-founding Coursera.  I've been meaning to take this course, but haven't had the time.)","Display_name":"jab","Creater_id":127749,"Start_date":"2016-08-22 08:08:15","Question_id":231092}
{"_id":{"$oid":"5837a577a05283111e4d3b66"},"Last_activity":"2016-08-22 13:27:03","Creator_reputation":2544,"Question_score":1,"Answer_content":"This is the well known large  small  problem, often named as , common in biological, biomedical and imaging problems, basically any field where either data is scarce/expensive to come by or simply carries too much information.There are several techniques often used in this scenario, such as regularization and attribute bagging. Implicitly, you already tested both, respectively on SVMs and Random Forests. You can try to improve upon that, though. Try other regularization penalties, such as elastic net, try using it fused with attribute bagging.Another possibility is that your data simply doesn't explain your outcomes.","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-08-22 13:27:03","Question_id":231145}
{"_id":{"$oid":"5837a577a05283111e4d3b77"},"Last_activity":"2016-08-22 13:02:24","Creator_reputation":26,"Question_score":0,"Answer_content":"Bias is the difference between the value of the (population) parameter and the expected value of the estimate of that parameter. As @matthew-drury points out, unless one knows the population, we cannot calculate the bias. Unless your data is from a complete census of the population or from simulation (when the data is simulated, one sets the parameter for the simulation), the parameters will not be known. Expected value of the estimator itself will require some understanding of the sampling distribution of the estimator and the associated parameters.Having said that, you can estimate the bias possibly via a bootstrap approach. See for example: When is the bootstrap estimate of bias valid?","Display_name":"Just_to_Answer","Creater_id":128491,"Start_date":"2016-08-22 13:02:24","Question_id":231117}
{"_id":{"$oid":"5837a577a05283111e4d3b88"},"Last_activity":"2016-08-22 12:24:52","Creator_reputation":3622,"Question_score":4,"Answer_content":"The basic problem in your data is that it has outliers, not treating them are at least understanding what/where the outliers are present might lead to poor predictions. There is a package in R called tsoutliers which implements Chen and Liu that can help you diagnose outliers in the data. Commercial packages such as Autobox which uses Tsay's outlier detection also has excellent outlier detection capabilities. I'll expand my answer in the next few days bear with me.I used tso function in tsoutliers package to detect outliersdatats \u0026lt;- ts(data,start=c(2003,11),frequency=12)plot.ts(datats)c  \u0026lt;- tso(datats, types = c(\"AO\", \"LS\",\"SLS\"))plot(c)Below are the outputs:Outliers:  type ind    time  coefhat tstat1  SLS  18 2005:04 16923128 6.7652   AO 101 2012:03 36590158 4.7633  SLS 112 2013:02 21989974 4.0964  SLS 113 2013:03 25225304 4.6995   AO 115 2013:05 24259786 3.158In looking at your data, there is an additive outlier at observation 2012:03 and seasonal level shift around 2013:02. You can practically ignore seasonal level shift at 2005:04.tsoutliers provides nice graphical output that shows some instability in the last few years 2012/2013/2014, that is seasonal variation has changed. If you do not account for it,then you are bound to produce poor forecasts.","Display_name":"forecaster","Creater_id":29137,"Start_date":"2015-02-17 17:06:38","Question_id":138108}
{"_id":{"$oid":"5837a577a05283111e4d3b89"},"Last_activity":"2015-02-18 07:12:19","Creator_reputation":14029,"Question_score":1,"Answer_content":"An analysis of this data using AUTOBOX http://www.autobox.com/cms/ (a piece of software that I have helped develop) led to an ARIMA model that evidenced significant instability with respect to it's model parameters as @forecaster had correctly suggested. I followed the paradigm of developing an ARIMA model first as compared to the alternative of detecting Interventions first as that is what the OP did. The software used the CHOW test to discover that the first 45 observations produced a statistically significant difference in parameters versus the last 85. This is NOW visually obvious once the analysis generated that hypothesis is presented. The human eye is no match for good analysis ! The model for the most recent 85 values was automatically developed using the iterative (not list-based) method of Box and Jenkins  (1,0,0)(0,1,0)12 with an outlier at period 101. The reason that the op's model being problematic with outliers and such was a fundamental alteration in the model coefficients. The plot of actual/fit/forecats is here  with forecast plot here  showing the 5 retained values. An analysis of the forecast errors is here  . Note that this is just 1 snapshot and may not be totally representative but it looks good to me . Finally the plot of the residuals is here  with an acf of  suggesting that the model for the last 85 observations might be adequate.","Display_name":"IrishStat","Creater_id":3382,"Start_date":"2015-02-17 19:11:48","Question_id":138108}
{"_id":{"$oid":"5837a577a05283111e4d3b98"},"Last_activity":"2016-08-22 12:12:25","Creator_reputation":33186,"Question_score":1,"Answer_content":"There are a few options.If analyzing data using R (other systems may have the same functionality) you can include an offset in a formula for setting a know relationship as part of the formula.For linear regression, minimizing the squared residuals is just a quadratic programming problem, so adding a non-negative constraint is straight forward if you have a quadratic programming tool.You can fit the unconstrained model, then if any of the slopes are negative you set them to 0 (leave that term out of the model) then refit.  Inference from this method can be a bit iffy, you should do something like bootstrapping the whole process to get the inference rather than trusting the final model.You can use a non-linear regression model tool and use exp(beta)*x to force a positive slope.You can use a Bayesian model and choose a prior for your slopes that restrict the possible values, e.g. use a gamma or half-normal prior on slopes that you want to restrict to be non-negative.The Bayesian approach is probably the most general and lets you include more prior information than just something needs to be non-negative.  But also remember that signs on coefficients in multiple regression cases can be non-intuitive and exploring why a sign is different from what you expect may gain you more insight than just forcing it to match preconceptions.","Display_name":"Greg Snow","Creater_id":4505,"Start_date":"2016-08-22 12:12:25","Question_id":231142}
{"_id":{"$oid":"5837a577a05283111e4d3ba5"},"Last_activity":"2016-08-22 11:02:51","Creator_reputation":180,"Question_score":0,"Answer_content":"Your question is hypothetical.  In the real finite world this sounds like the \"classification of multispectral image data\" and is common in remote sensing.  Do an internet search and you will find plenty of literature, blogs, videos, books, and software.","Display_name":"Brian O\u0026#39;Donnell","Creater_id":108167,"Start_date":"2016-08-22 11:02:51","Question_id":231131}
{"_id":{"$oid":"5837a577a05283111e4d3bb1"},"Last_activity":"2016-08-22 10:56:22","Creator_reputation":1,"Question_score":0,"Answer_content":"I would try giving the NN access to a basic physics simulation that it can use to interpolate between data points. There is a formalization of this technique called Kalman filter.","Display_name":"littlebenlittle","Creater_id":128564,"Start_date":"2016-08-22 10:56:22","Question_id":229143}
{"_id":{"$oid":"5837a577a05283111e4d3bbd"},"Last_activity":"2016-08-22 10:41:49","Creator_reputation":266,"Question_score":0,"Answer_content":"This happens because the initial values of the chains draw non-sensical values for the model. However, as it continues to run the chains move towards better values and the error message stops coming up. So, if its endlessly printing you have a problem. However, if it prints ~ 20-40 times and then stops, you're probably okay. JAGS will keep running and you will get a result. ","Display_name":"colin","Creater_id":30451,"Start_date":"2016-08-22 10:41:49","Question_id":230106}
{"_id":{"$oid":"5837a577a05283111e4d3bcc"},"Last_activity":"2016-08-22 10:35:27","Creator_reputation":4423,"Question_score":2,"Answer_content":"I agree with Mark L. Stone's comment: too many examples, linear algebra is the foundation of statistics and machine learning, because your data is organized in columns and rows, and most widely used operations are multiplication and addition. I will use linear regression as an example.Suppose you are doing multiple linear regression, where you want to build a model for data  (the th data in your data set, and assume your data as  features.)f(x^{(i)})=\\beta_0+\\beta_1x_1^{(i)}+\\beta_2x_2^{(i)}+\\beta_3x_3^{(i)}+\\cdots+\\beta_mx_m^{(i)}And you want to minimize the squared error \\sum_i (y^{(i)}-f(x^{(i)}))^2Such problem can be written in a very concise notation\\min \\|X\\beta-y\\|^2Where  is data matrix, every row is a data instance, and every column is a \"feature\".  is a vector and  is the response vector.  is  norm of a vector. Here is where the matrix algebra comes in: take the derivative respect to  of , you get . You can set it to  and solve it, which is the \"normal equation\" for linear regression, where\\beta=(X^TX)^{-1}X^TyNote that, using matrix inverse is not a good solution from numerical analysis point of view. In real world, such as in R, QR decomposition is used. And that is more matrix algebra. Check this post, you will see matrix operations are everywhereWhat algorithm is used in linear regression?","Display_name":"hxd1011","Creater_id":113777,"Start_date":"2016-08-22 09:45:26","Question_id":231113}
{"_id":{"$oid":"5837a577a05283111e4d3bdb"},"Last_activity":"2013-12-14 11:00:12","Creator_reputation":51,"Question_score":5,"Answer_content":"Interesting discussion. To label stepwise regression as statistical sin is a bit of a religious statement - as long as one knows what they are doing and that the objectives of the exercise is clear, it is definitely a fine approach with its own set of assumptions and, is certainly biased, and does not guarantee optimality, etc. Yet, the same can be said of of lot of other things we do. I have not seen CCA mentioned, which addresses the more fundamental problem of the correlation structure in covariate space, does guarantee optimality, has been around for quite a bit, and it has somewhat of a learning curve. It is implemented on a variety of platforms including R.","Display_name":"gillesc","Creater_id":36196,"Start_date":"2013-12-14 11:00:12","Question_id":13686}
{"_id":{"$oid":"5837a577a05283111e4d3bdc"},"Last_activity":"2012-08-11 11:57:44","Creator_reputation":883,"Question_score":51,"Answer_content":"There are several alternatives to Stepwise Regression. The most used I have seen are:Expert opinion to decide which variables to include in the model.Partial Least Squares Regression. You essentially get latent variables and do a regression with them. You could also do PCA yourself and then use the principal variables.Least Absolute Shrinkage and Selection Operator (LASSO). Both PLS Regression and LASSO are implemented in R packages like PLS: http://cran.r-project.org/web/packages/pls/ and LARS: http://cran.r-project.org/web/packages/lars/index.htmlIf you only want to explore the relationship between your dependent variable and the independent variables (e.g. you do not need statistical significance tests), I would also recommend Machine Learning methods like Random Forests or Classification/Regression Trees. Random Forests can also approximate complex non-linear relationships between your dependent and independent variables, which might not have been revealed by linear techniques (like Linear Regression). A good starting point to Machine Learning might be the Machine Learning task view on CRAN: Machine Learning Task View: http://cran.r-project.org/web/views/MachineLearning.html","Display_name":"Johannes","Creater_id":5544,"Start_date":"2011-07-31 22:20:07","Question_id":13686}
{"_id":{"$oid":"5837a577a05283111e4d3bdd"},"Last_activity":"2012-05-01 10:24:43","Creator_reputation":156,"Question_score":14,"Answer_content":"Model averaging is one way to go (an information-theoretic approach). The R package glmulti can perform linear models for every combination of predictor variables, and perform model averaging for these results. See http://sites.google.com/site/mcgillbgsa/workshops/glmulti Don't forget to investigate collinearity between predictor variables first though. Variance Inflation Factors (available in R package \"car\") are useful here.","Display_name":"OliP","Creater_id":10768,"Start_date":"2012-04-23 06:15:08","Question_id":13686}
{"_id":{"$oid":"5837a577a05283111e4d3bde"},"Last_activity":"2011-08-01 05:32:00","Creator_reputation":13859,"Question_score":16,"Answer_content":"Another option you might consider for variable selection and regularization is the elastic net. It's implemented in R via the glmnet package.","Display_name":"Zach","Creater_id":2817,"Start_date":"2011-08-01 05:32:00","Question_id":13686}
{"_id":{"$oid":"5837a577a05283111e4d3bdf"},"Last_activity":"2011-08-01 05:31:43","Creator_reputation":57702,"Question_score":8,"Answer_content":"@johannes gave an excellent answer. If you are a SAS user, then LASSO is available through PROC GLMSELECT and partial least squares through PROC PLS.David Cassell and I made a presentation about LASSO (and Least Angle Regression) at a couple of SAS user groups. It's available here","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2011-08-01 05:31:43","Question_id":13686}
{"_id":{"$oid":"5837a577a05283111e4d3bec"},"Last_activity":"2012-06-05 14:43:21","Creator_reputation":9087,"Question_score":7,"Answer_content":"Thanks for providing the data so that I could perform some diagnostics.Actually, this is an epic bug of predict.lme. Your factors have more levels in your initial data (for example you have more than 4 countries) than in your new data. A line of code specifically causes the unused levels to be discarded so you end up with matrices of different dimensions, whence the non-conformable argumentsI removed that line and put the code here.In R you can dolibrary(nlme)source(\"http://lab.thegrandlocus.com/static/code/predict.lme_patched.txt\")This registers a new function predict.lme that will be invoked instead of the one from the package nlme and you can run your code. At least it worked for me.Warning: The posted code and the method are neither a replacement nor a real bug fix of the package. The patched function has not been tested beyond its ability to run the bit of code of the OP.","Display_name":"gui11aume","Creater_id":10849,"Start_date":"2012-06-01 07:23:32","Question_id":29513}
{"_id":{"$oid":"5837a577a05283111e4d3bf9"},"Last_activity":"2016-08-22 09:50:07","Creator_reputation":151,"Question_score":1,"Answer_content":"As a partial answer to your question,  you can build a residual model (also known as a variance model) tomodel the residuals for the original model as a function of the predicted response (for example) as follows (in R notation):residual.model \u0026lt;-     lm(abs(residuals(original.model)) ~ predict(original.model), ...)You can then examine the residual model to get a deeper understandingof the data.  From the residual model you can estimate predictionintervals, for example.The residuals are usually much more noisy than the original dataused to build the model, and so there will be more uncertainty about theresidual model than the original model.The above residual model uses lm and thus assumes (at most) a linear relationship betweenthe absolute residuals and the predicted response, which is often a good enoughapproximation to the (unknown) underlying reality---or maybe I should say, a linear residual model is often about themost complicated residual model you would want to use given the noise in theresiduals.My earthR package builds variance models essentially using the above idea(but the ideas are fairly universal and apply not just to earth/MARSmodels).   Some of the background theory can be found in the package vignette Variance models in earth.When reading the vignette, mentally substitute the name of your modelinstead of \"earth\" e.g. substitute \"lm\" for \"earth\".Additional references can be found in the above vignette.  Especially helpful is Carroll and Ruppert Transformation and Weighting in Regression.","Display_name":"Stephen Milborrow","Creater_id":54184,"Start_date":"2016-08-22 09:35:15","Question_id":127001}
{"_id":{"$oid":"5837a577a05283111e4d3bfa"},"Last_activity":"2014-12-07 11:46:57","Creator_reputation":1349,"Question_score":1,"Answer_content":"This has been referred to as residual index, although not consistently. I guess the type of analyses you subject it to would depend on your question of interest (as most result in some level of 'deeper understanding'), and so would pros and cons. Garcia-Berthou discusses cons of one example of such application as \"an ad hoc sequential procedure with no statistical justification\" here http://onlinelibrary.wiley.com/doi/10.1046/j.1365-2656.2001.00524.x/full In other words, if you suspect other factors are affecting the response, why not start with a model that would account for these multiple factors and their interactions. Yet, in other cases it is possible to justify, and there are valid examples of its use in spatial analyses.","Display_name":"katya","Creater_id":57390,"Start_date":"2014-12-07 11:46:57","Question_id":127001}
{"_id":{"$oid":"5837a577a05283111e4d3c07"},"Last_activity":"2016-08-22 09:38:05","Creator_reputation":101,"Question_score":0,"Answer_content":"If you have a large number of samples and a large number of networks to train, training a generic network using the full training set as a base can work well. The generic network is trained as usual. To teach it to specialize, use a very low learning rate (0.0001) and low number of iterations (\u0026lt;10000) to prevent overtraining or a loss of accuracy. Supervise the training very carefully using a testing set that is not part of the training data to ensure that accurate generalizations aren't being \"unlearned\".","Display_name":"Josiah","Creater_id":127374,"Start_date":"2016-08-22 09:38:05","Question_id":229387}
{"_id":{"$oid":"5837a577a05283111e4d3c15"},"Last_activity":"2016-08-21 16:24:51","Creator_reputation":1737,"Question_score":10,"Answer_content":"I would like to correct an erroneous assumption in the original post, a mistake which is relatively common. The OP says:  From what I have read and from answers to other questions I have asked here, maximum likelihood estimation corresponds mathematically (I don't care if it corresponds philosophically, I only care whether it corresponds mathematically) to maximum a priori estimation using a uniform prior (for those who object to this, see the note at the bottom of this question).And the note at the bottom of the post says:  Two objects are equivalent in a mathematical sense if they have the same properties, irrespective of how they are constructed. [...]My objection is that, philosophy aside, maximum likelihood estimation (MLE) and maximum-a-posteriori (MAP) estimation do not have the same mathematical properties. Crucially, MLE and MAP transform differently under (nonlinear) reparametrization of the space. This happens because MLE has a \"flat prior\" in every parametrization, whereas MAP doesn't (the prior transforms as a probability density, so there is a Jacobian term).The definition of a mathematical object includes how the object behaves under operators such as transformation of variables (e.g., see the definition a tensor).In conclusion, MLE and MAP are not the same thing, neither philosophically nor mathematically; this is not an opinion.","Display_name":"lacerbi","Creater_id":80479,"Start_date":"2016-08-21 15:20:48","Question_id":230921}
{"_id":{"$oid":"5837a577a05283111e4d3c16"},"Last_activity":"2016-08-21 11:11:59","Creator_reputation":18711,"Question_score":5,"Answer_content":"  Many people who self-describe as \"Bayesians\", however, seem to reject  using maximum likelihood estimation under any circumstances, even  though it is a special case of (mathematically) Bayesian methods,  because it is a \"frequentist method\".Such people would be rejecting MLE as a general method for making point estimates. In particular cases where they had reason to use a uniform prior \u0026amp; wanted to make a maximum a posteriori estimate they wouldn't be at all bothered by the coincidence of their calculations with MLE.  Apparently Bayesians also use a restricted/limited number of  distributions compared to frequentists, even though those  distributions would also be mathematically correct from a Bayesian  viewpoint.Perhaps sometimes, to make their calculations easier, but not from any point of principle.  I have the impression that there are at least two different  definitions of the term Bayesian commonly in use. The first I would  call \"mathematically Bayesian\" which encompasses all methods of  statistics, since it includes parameters which are constant RVs and  those which are not constant RVs. Then there is \"culturally Bayesian\"  which rejects some \"mathematically Bayesian\" methods because those  methods are \"frequentist\" (i.e. out of personal animosity to the  parameter sometimes being modeled as a constant or frequency).There are certainly distinctions to be made between different approaches to Bayesian inference, but not this one. If there's a sense in which Bayesianism is more general, it's in the willingness to apply the concept of probability to epistemic uncertainty about parameter values \u0026amp; not just the aleatory uncertainty of the data-generating process which is all that frequentism concerns itself with. Frequentist inference is not a special case of Bayesian inference \u0026amp; none of the answers or comments at Is there any mathematical basis for the Bayesian vs frequentist debate? are implying that it is. If in a Bayesian approach you were to consider the parameter a constant random variable, you'd obtain the same posterior whatever the data are\u0026mdash;\u0026amp; to say it's constant but you don't know what value it takes wouldn't be to say anything worth saying.  The frequentist approach takes an entirely different tack \u0026amp; doesn't involve the calculation of posterior distributions at all.","Display_name":"Scortchi","Creater_id":17230,"Start_date":"2016-08-21 10:53:19","Question_id":230921}
{"_id":{"$oid":"5837a577a05283111e4d3c17"},"Last_activity":"2016-08-20 21:40:40","Creator_reputation":4307,"Question_score":6,"Answer_content":"Personally I am a \"pragmatist\" rather than a \"frequentist\" or a \"Bayesian\", so I cannot claim to speak for any camp.That said, I think the distinction you are alluding to is probably not so much MLE vs. MAP, but between point estimates vs. estimating posterior PDFs. As a scientist working in a field with sparse data and large uncertainties, I can sympathize with not wanting to put too much confidence on \"best guess\" results which may be misleading, resulting in overconfidence.A related practical distinction is between parametric vs. non-parametric methods. So for example I think that both Kalman filtering and Particle filtering would be accepted as Recursive Bayesian Estimation. But the Gaussian assumption of Kalman filtering (a parametric method) can give very misleading results if the posterior is not unimodal. To me these kinds of engineering examples highlight where differences are neither philosophical nor mathematical, but manifest in terms of practical results (i.e. will your autonomous vehicle crash?). For the Bayesian enthusiasts I am familiar with, this \"see what works\" engineering-style attitude seems to be predominant ... not sure if this is true more broadly.","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-20 20:33:16","Question_id":230921}
{"_id":{"$oid":"5837a577a05283111e4d3c24"},"Last_activity":"2016-08-22 09:17:49","Creator_reputation":7965,"Question_score":2,"Answer_content":"  What would be a good validation frequency? Should I check my model on the validation data at the end of each epoch? (My batch size is 1)There is no gold rule, computing the validation error after each epoch is quite common. Since your validation set much smaller than your training set, it will not slow down the training much.  Is it the case that the first few epochs might yield worse result before it starts converging to better value? yes  In that case, should we train our network for several epochs before checking for early stopping?You could, but then the issue is how many epochs should you skip. So in practice, most of the time people do not skip any epoch.  How to handle the case when the validation loss might go up and down? In that case, early stopping might prevent my model from learning further, right?People typically define a patience, i.e. the number of epochs to wait before early stop if no progress on the validation set. The patience is often set somewhere between 10 and 100 (10 or 20 is more common), but it really depends on your dataset and network. Example with patience = 10:","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-22 09:17:49","Question_id":231061}
{"_id":{"$oid":"5837a577a05283111e4d3c31"},"Last_activity":"2016-08-21 21:57:20","Creator_reputation":11,"Question_score":1,"Answer_content":"There are multiple ways to handle missing data depending on how much effort you want to put into it. I stumbled into R package (DmWR) that presents several options including interpolation, knn and others. ","Display_name":"moka","Creater_id":128497,"Start_date":"2016-08-21 21:57:20","Question_id":231029}
{"_id":{"$oid":"5837a577a05283111e4d3c3e"},"Last_activity":"2016-08-22 09:16:16","Creator_reputation":130,"Question_score":1,"Answer_content":"The method you describe would be a coarse way to evaluate balance, but a finer way is the following:For each covariate, compute the correlation between the covariate and the treatment variable after conditioning. If it is 0, then the variable will no longer confound the estimate of the treatment effect. Calculating standardized mean differences in the context of binary treatments essentially examines the same thing. Fong, Hazlett, and Imai (2015) consider continuous treatments and compute the absolute Pearson correlations between covariates and treatment to establish balance.It would also be a good idea to evaluate the correlation between treatment and the squared and other polynomial and interaction terms of the covariates. You want all these to be as close to 0 as possible. In general, you want treatment to be independent form the covariates, so you can use whatever methods are appropriate to determine this (e.g., visually examining scatterplots, etc.).The method you describe from Guo \u0026amp; Fraser is effective in theory, and of course would be approximately equivalent in large samples with many subclasses. It would actually be superior, because you aren't limited to polynomial correlations (for the same reason subclassification on the PS is superior to covariate adjustment with the PS: you don't have to assume the functional form of the relationship). The problem is that you are coarsening your treatment into 5 categories, which it is not: it's a continuous variable, so independence should be met over the whole distribution, not just within subclasses.Also, although they recommend it, avoid using hypothesis tests of any kind for balance assessment. Balance can become conflated with power when using them.If you're using R for propensity score analysis, consider the cobalt package for assessing balance. In the next release, balance assessment for continuous treatment will be implemented.","Display_name":"Noah","Creater_id":116195,"Start_date":"2016-08-20 14:06:46","Question_id":230525}
{"_id":{"$oid":"5837a578a05283111e4d3c4b"},"Last_activity":"2016-08-22 09:13:40","Creator_reputation":17374,"Question_score":3,"Answer_content":"I would prefer to report the mean difference and 95% confidence interval. \"We estimated a mean difference of X.X (95% CI X.X - X.X) which was not statistically significant at the 0.05 level.\". This is conventional reporting technique for findings. A verbal description of exactly what a confidence interval and/or p-value are is not what you asked for. Nonetheless an interpretation of a confidence interval is \"an interval in which findings would be expected to fall if the present study was replicated independently an infinite number of times.\" Similarly a -value is interpreted as \"a frequency with which infinite, independent replications of the present study would produce findings as inconsistent or more inconsistent with the null hypothesis than the present study if the null hypothesis were true.\"Your interpretation is wrong because it is not the means that are statistically significant, but their difference. So you can say, \"The mean difference was not statistically significant at the 0.05 level\".","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-17 13:12:46","Question_id":230363}
{"_id":{"$oid":"5837a578a05283111e4d3c60"},"Last_activity":"2016-08-20 22:36:39","Creator_reputation":2443,"Question_score":0,"Answer_content":"One possibility in addition to those already mentioned is to use item-reponse-theory. You could say that the number  positives out of the total  in city  for item  has a  distribution, where  corresponds to your observed proportions. In the simplest form of this approach you would then say that \\pi_{ij} = \\frac{e^{\\theta_j-b_j}}{1+e^{\\theta_j-b_j}}so that the  (a more complicated model might have ).If you then fit this model,  is what you are interested in as an index of how the city is doing.","Display_name":"Bj\u0026#246;rn","Creater_id":86652,"Start_date":"2016-08-20 22:36:39","Question_id":230905}
{"_id":{"$oid":"5837a578a05283111e4d3c61"},"Last_activity":"2016-08-20 17:32:49","Creator_reputation":4307,"Question_score":0,"Answer_content":"There are really two parts to this question: 1) How to standardize variables with different non-Gaussian distributions to make their values comparable, and 2) How to combine these standardized values into a single  index?For the second part, something like PCA would be recommended, to prevent \"double counting\" where the measures are correlated. Peter's answer addresses this issue.In terms of the first part, a common approach is to essentially use quantiles/percentiles. These can be converted to equivalent \"z-scores\", in which case this technique is sometimes called the normal score transform. In the simplest version of this, for each variable you estimate quantiles from the Empirical distribution function. (More broadly, you could use any parametric or non-parametric approach to estimate the CDF.) The variables will then be in \"comparable units\", each approximately uniformly distributed.In the second step, you transform these to approximately normal by putting through the inverse normal CDF function to get the z-scores corresponding to those quantiles. This second step is optional, but can be useful if you want to \"treat the variables as Gaussian\" for subsequent analyses (e.g. PCA or least-squares more generally is most well suited for Gaussian variables).","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-20 17:32:49","Question_id":230905}
{"_id":{"$oid":"5837a578a05283111e4d3c62"},"Last_activity":"2016-08-20 14:54:59","Creator_reputation":57702,"Question_score":0,"Answer_content":"Statistically, you could do factor analysis to combine the percentages into a measure of a single latent variable. This will almost certainly result in one factor explaining a lot of the variance.  The score on that factor will have mean 0 and sd 1, but may or may not be normally distributed.Alternatively, you could try to figure out, substantively, which of the variables will have what influence on other measures. This might involve a lot of literature review, trying to find out how the 3 variables are related to some other variable that is of interest (such as SES or income or whatever).  ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-20 14:54:59","Question_id":230905}
{"_id":{"$oid":"5837a578a05283111e4d3c6f"},"Last_activity":"2016-08-22 08:11:02","Creator_reputation":50,"Question_score":0,"Answer_content":"I think it would be better to follow proceedings of some machine learning-related conferences. Such conferences usually have application tracks, where you can find practical applications of machine learning algorithms. ","Display_name":"user1219801","Creater_id":111882,"Start_date":"2016-08-22 08:11:02","Question_id":217938}
{"_id":{"$oid":"5837a578a05283111e4d3c70"},"Last_activity":"2016-08-22 07:58:33","Creator_reputation":434,"Question_score":4,"Answer_content":"I do not have knowledge in ML. After a little web searching, I found a reddit thread that lists the following books - all of which are legally downloadable for free. You can research the titles of your interest for details. Also comment if you find any of the books helpful (and why).Machine LearningElements of Statistical Learning Hastie, Tibshirani, FriedmanMachine Learning and Bayesian Reasoning David BarberGaussian Processes for Machine Learning Rasmussen and WilliamsInformation Theory, Inference, and Learning Algorithms David MacKay Introduction to Machine Learning Smola and VishwanathanA Probabilistic Theory of Pattern Recognition Devroye, Gyorfi, LugosiIntroduction to Information Retrieval Manning, Rhagavan, ShutzeForecasting: principles and practice Hyndman, Athanasopoulos (Online Book) Probability / StatsIntroduction to statistical thought LavineBasic Probability Theory Robert AshIntroduction to probability Grinstead and SnellPrinciple of Uncertainty KadaneAll of Statistics Larry WassermanLinear Algebra / OptimizationLinear Algebra, Theory, and Applications KuttlerLinear Algebra Done Wrong TreilApplied Numerical Computing VandenbergheApplied Numerical Linear Algebra James DemmelConvex Optimization Boyd and VandenbergheGenetic AlgorithmA Field Guide to Genetic Programming Poli, Langdon, McPheeEvolved To Win SipperEssentials of Metaheuristics Luke","Display_name":"Moazzem Hossen","Creater_id":78807,"Start_date":"2016-06-08 09:49:15","Question_id":217938}
{"_id":{"$oid":"5837a578a05283111e4d3c71"},"Last_activity":"2016-06-08 11:49:40","Creator_reputation":101,"Question_score":1,"Answer_content":"One of the books that I would recommend is Introduction to Statistical Learning and it is free to download. This book is easy to follow with exercises in R. Another good one is Applied Predictive Modeling","Display_name":"vijar","Creater_id":83634,"Start_date":"2016-06-08 11:49:40","Question_id":217938}
{"_id":{"$oid":"5837a578a05283111e4d3c84"},"Last_activity":"2016-08-22 07:12:19","Creator_reputation":4423,"Question_score":1,"Answer_content":"I think it is a notation problem: what does  represent? Note  is not a widely used notation.Are you trying to use  to represent  outcomes from two random events? or Are you tying to use  to represent a product of two random variables.If  represents  outcomes, then the distribution can be described with joint distribution. But most people will use a different notation, where  are used. For example, in a coin flip, people will use  to represent  head flip.On the other hand, if you use  to represent a product of two random variables, the distribution of the new random variable is .","Display_name":"hxd1011","Creater_id":113777,"Start_date":"2016-08-22 07:06:58","Question_id":231094}
{"_id":{"$oid":"5837a578a05283111e4d3c99"},"Last_activity":"2016-06-08 05:40:54","Creator_reputation":403,"Question_score":2,"Answer_content":"I am trying to answer my own question after doing few initial experiments. I tried the SMOTE technique to generate new synthetic samples. And the results are encouraging. It generates synthetic data which has almost similar characteristics of the sample data. The code is  from http://comments.gmane.org/gmane.comp.python.scikit-learn/5278 by Karsten Jeschkies which is as belowimport numpy as npfrom random import randrange, choicefrom sklearn.neighbors import NearestNeighborsdef SMOTE(T, N, k):\"\"\"Returns (N/100) * n_minority_samples synthetic minority samples.Parameters----------T : array-like, shape = [n_minority_samples, n_features]    Holds the minority samplesN : percetange of new synthetic samples:     n_synthetic_samples = N/100 * n_minority_samples. Can be \u0026lt; 100.k : int. Number of nearest neighbours. Returns-------S : array, shape = [(N/100) * n_minority_samples, n_features]\"\"\"    n_minority_samples, n_features = T.shapeif N \u0026lt; 100:    #create synthetic samples only for a subset of T.    #TODO: select random minortiy samples    N = 100    passif (N % 100) != 0:    raise ValueError(\"N must be \u0026lt; 100 or multiple of 100\")N = N/100n_synthetic_samples = N * n_minority_samplesS = np.zeros(shape=(n_synthetic_samples, n_features))#Learn nearest neighboursneigh = NearestNeighbors(n_neighbors = k)neigh.fit(T)#Calculate synthetic samplesfor i in xrange(n_minority_samples):    nn = neigh.kneighbors(T[i], return_distance=False)    for n in xrange(N):        nn_index = choice(nn[0])        #NOTE: nn includes T[i], we don't want to select it         while nn_index == i:            nn_index = choice(nn[0])        dif = T[nn_index] - T[i]        gap = np.random.random()        S[n + i * N, :] = T[i,:] + gap * dif[:]return SThe got the following results with a small dataset of 4999 samples having 2 features.Sample or the small data description          After        Beforecount   4999.000000   4999.000000mean     350.577866    391.757958std      566.065273    693.179718min        0.000000      0.00000025%       52.975000     93.99150050%      183.388000    226.02700075%      414.599000    453.261167max    10980.004000  27028.158333Histogram is as follows￼Scatter plot to see the joint distribution is as follows:After using SMOTE technique to generate twice the number of samples, I get the following          After        Beforecount   9998.000000   9998.000000mean     350.042946    389.020419std      556.334086    652.886148min        0.000000      0.00000025%       53.074959     94.88529550%      184.067407    226.80291275%      414.955448    454.008691max    10685.308012  26688.626042Histogram is as followsScatter plot to see the joint distribution is as follows:","Display_name":"prashanth","Creater_id":86202,"Start_date":"2016-06-07 06:13:08","Question_id":215938}
{"_id":{"$oid":"5837a578a05283111e4d3c9a"},"Last_activity":"2016-06-08 04:08:56","Creator_reputation":3812,"Question_score":2,"Answer_content":"You could also look at MUNGE. It generates synthetic datasets from a nonparametric estimate of the joint distribution. The idea is similar to SMOTE (perturb original data points using information about their nearest neighbors), but the implementation is different, as well as its original purpose. Whereas SMOTE was proposed for balancing imbalanced classes, MUNGE was proposed as part of a 'model compression' strategy. The goal is to replace a large, accurate model with a smaller, efficient model that's trained to mimic its behavior. There are many details you can ignore if you're just interested in the sampling procedure. The paper compares MUNGE to some simpler schemes for generating synthetic data.Basic idea:    Generate a synthetic point as a copy of original data point   Let  be be the nearest neighbor  For each attribute :      If  is discrete: With probability , replace the synthetic point's attribute  with .  If  is continuous: With probability , replace the synthetic point's attribute  with a value drawn from a normal distribution with mean  and standard deviation    and  are parameters    The paper:  Bucila et al. (2006). Model compression.Regarding the stats/plots you showed, it would be good to check some measure of the joint distribution too, since it's possible to destroy the joint distribution while preserving the marginals.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-06-08 04:08:56","Question_id":215938}
{"_id":{"$oid":"5837a578a05283111e4d3ca7"},"Last_activity":"2016-04-24 00:21:12","Creator_reputation":7427,"Question_score":4,"Answer_content":"Identifiability basically refers to whether or not consistent estimators exist for the parameters of the model.  Put another way, if we are told the distribution of the data, can we recover the model parameters?  If not then our model is unidentifiable.Perhaps the simplest example of an unidentifiable model is the overparameterized ANOVA model.  This model takes the formY_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}where  and  are arbitrary constants and  normal.  If we are given the information that  normal for some sets of constants  and , and it is important to note that this is all we can ever hope to learn from the data, then there is no unique way to translate this back into constants ,  and .  This is because we can always take  and  to arrive at the same mean parameter  for different values of the model parameters.  Even if we had infinite data we could never hope to recover these values.  For this reason we impose the constraint  which guarantees a one to one mapping between model and distribution parameters.","Display_name":"dsaxton","Creater_id":78861,"Start_date":"2016-04-24 00:21:12","Question_id":209007}
{"_id":{"$oid":"5837a578a05283111e4d3cb6"},"Last_activity":"2016-08-22 06:01:22","Creator_reputation":735,"Question_score":0,"Answer_content":"The covariance is the same as correlation when the variances are 1.  Thus we can generate a sigma matrix as follows with the pairwise correlations equal to 1:sigma=toeplitz(c(1,rep(0.85,29)))Note i'm using 29 as you asked for p=30. Then we can simulate any length of data from this sigma matrix using rmvnorm:set.seed(1) # to be reproducibledata=rmvnorm(n=200,sigma=sigma)The default mean is 0 for each dimension so I haven't included it in the above.  You can then check the correlation of individual pairs as follows:cor(data[,1],data[,5]);Or you can get the correlation matrix:cor(data);You can see that most of them are around 0.85 ish.  You can make it more readable doing:round(cor(data),2);","Display_name":"adunaic","Creater_id":13409,"Start_date":"2016-08-22 05:49:57","Question_id":231078}
{"_id":{"$oid":"5837a578a05283111e4d3cc3"},"Last_activity":"2015-02-28 09:10:32","Creator_reputation":14029,"Question_score":4,"Answer_content":"My response using AUTOBOX is quite similar to @forecaster but with a much simpler model. Box and Einstein and others have reflected on keeping solutions simple but not too simple. The model that was automatically developed was  . The actual and cleansed plot is very similar  . A plot of the residuals (which should always be shown ) is here  along with the mandatory acf of the residuals  . The statistics of the residuals are always useful in making comparisons between \"dueling models\"  . The Actual/Fit/Forecast graph is here ","Display_name":"IrishStat","Creater_id":3382,"Start_date":"2015-02-28 09:01:28","Question_id":139660}
{"_id":{"$oid":"5837a578a05283111e4d3cc4"},"Last_activity":"2015-02-28 05:29:06","Creator_reputation":1033,"Question_score":0,"Answer_content":"It would seem that your problem would be greatly simplified if you detrended your data.  It appears to decline linearly.  Once you detrend the data, you could apply a wide variety of tests for non-stationarity.","Display_name":"MrMeritology","Creater_id":53819,"Start_date":"2015-02-28 05:29:06","Question_id":139660}
{"_id":{"$oid":"5837a578a05283111e4d3cc5"},"Last_activity":"2015-02-27 19:39:43","Creator_reputation":3622,"Question_score":10,"Answer_content":"You could use time series outlier detection to detect changes in time series.  Tsay's or Chen and Liu's procedures are popular time series outlier detection methods . See my earlier question on this site.R's tsoutlier package uses Chen and Liu's method for detection outliers. SAS/SPSS/Autobox can also do this. See below for the R code to detect changes in time series.library(\"tsoutliers\")dat.ts\u0026lt;- ts(dat.change,frequency=1)data.ts.outliers \u0026lt;- tso(dat.ts)data.ts.outliersplot(data.ts.outliers)tso function in tsoultlier package identifies following outliers. You can read documentation to find out the type of outliers.Outliers:  type ind time coefhat   tstat1   TC  42   42 -2.9462 -10.0682   AO  43   43  1.0733   4.3223   AO  45   45 -1.2113  -4.8494   TC  47   47  1.0143   3.3875   AO  51   51  0.9002   3.4336   AO  52   52 -1.3455  -5.1657   AO  56   56  0.9074   3.7108   LS  62   62  1.1284   3.7179   AO  67   67 -1.3503  -5.502the package also provides nice plots. see below. The plot shows where the outliers are and also what would have happened if there were no outliers.I have also used R package called strucchange to detect level shifts. As an example on your data library(\"strucchange\")breakpoints(dat.ts~1)The program correctly identifies breakpoints or structural changes.Optimal 4-segment partition: Call:breakpoints.formula(formula = dat.ts ~ 1)Breakpoints at observation number:17 41 87 Corresponding to breakdates:17 41 87 Hope this helps","Display_name":"forecaster","Creater_id":29137,"Start_date":"2015-02-27 19:39:43","Question_id":139660}
{"_id":{"$oid":"5837a578a05283111e4d3cc6"},"Last_activity":"2015-02-27 18:58:13","Creator_reputation":4927,"Question_score":3,"Answer_content":"I would approach this problem from the following perspectives. These are just some ideas off the top of my head - please take them with a grain of salt. Nevertheless, I hope that this will be useful.Time series clustering. For example, by using popular dynamic time warping (DTW) or alternative approaches. Please see my related answers: on DTW for classification/clustering and on DTW or alternatives for uneven time series. The idea is to cluster time series into categories \"normal\" and \"abnormal\" (or similar).Entropy measures. See my relevant answer on time series entropy measures. The idea is to determine entropy of a \"normal\" time series and then compare it with other time series (this idea has an assumption of an entropy deviation in case of deviation from \"normality\").Anomaly detection. See my relevant answer on anomaly detection (includes R resources). The idea is to directly detect anomalies via various methods (please see references). Early Warning Signals (EWS) Toolbox and R package earlywarnings seem especially promising.","Display_name":"Aleksandr Blekh","Creater_id":31372,"Start_date":"2015-02-27 18:58:13","Question_id":139660}
{"_id":{"$oid":"5837a578a05283111e4d3cd3"},"Last_activity":"2016-08-22 05:46:35","Creator_reputation":14029,"Question_score":5,"Answer_content":"I totally agree with the sentiment of the previous commentators. I would like to add that all ARIMA model can also be represented as a pure AR model. These weights are referred to as the Pi weights as compared to the pure MA form (Psi weights) . In this way you can view (interpret) an ARIMA model as an optimized weighted average of the past values. In other words rather than assume a pre-specified length and values for a weighted average , an ARIMA model delivers both the length () of the weights and the actual weights (). Y(t) =c_1 Y(t−1) + c_2 Y(t-2) + c_3 Y(t-3)+ ... + c_n Y(t-n) + a(t)In this way an ARIMA model can be explained as the answer to the questionHow many historical values should I use to compute a weighted sum of the past?Precisely what are those values?","Display_name":"IrishStat","Creater_id":3382,"Start_date":"2013-07-01 06:45:36","Question_id":40905}
{"_id":{"$oid":"5837a578a05283111e4d3cd4"},"Last_activity":"2013-07-01 16:16:54","Creator_reputation":2523,"Question_score":17,"Answer_content":"I think that you need to remember that ARIMA models are atheoretic models, so the usual approach to interpreting estimated regression coefficients does not really carry over to ARIMA modelling.In order to interpret (or understand) estimated ARIMA models, one would do well to be cognizant of the different features displayed by a number of common ARIMA models.We can explore some of these features by investigating the types of forecasts produced by different ARIMA models. This is the main approach that I've taken below, but a good alternative would be to look at the impulse response functions or dynamic time paths associated with different ARIMA models (or stochastic difference equations). I'll talk about these at the end.AR(1) ModelsLet's consider an AR(1) model for a moment. In this model, we can say that the lower the value of  then the quicker is the rate of convergence (to the mean). We can try to understand this aspect of AR(1) models by investigating the nature of the forecasts for a small set of simulated AR(1) models with different values for .The set of four AR(1) models that we'll discuss can be written in algebraic notation as:\\begin{equation}Y_{t} = C + 0.95 Y_{t-1} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (1)\\\\Y_{t} = C + 0.8 Y_{t-1} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (2)\\\\Y_{t} = C + 0.5 Y_{t-1} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (3)\\\\Y_{t} = C + 0.4 Y_{t-1} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (4)\\end{equation}where  is a constant and the rest of the notation follows from the OP. As can be seen, each model differs only with respect to the value of . In the graph below, I have plotted out-of-sample forecasts for these four AR(1) models. It can be seen that the forecasts for the AR(1) model with  converges at a slower rate with respect to the other models. The forecasts for the AR(1) model with  converges at a quicker rate than the others. Note: when the red line is horizontal, it has reached the mean of the simulated series.MA(1) ModelsNow let's consider four MA(1) models with different values for . The four models we'll discuss can be written as:\\begin{equation}Y_{t} = C + 0.95 \\nu_{t-1} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (5)\\\\Y_{t} = C + 0.8 \\nu_{t-1} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (6)\\\\Y_{t} = C + 0.5 \\nu_{t-1} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (7)\\\\Y_{t} = C + 0.4 \\nu_{t-1} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (8)\\end{equation}In the graph below, I have plotted out-of-sample forecasts for these four different MA(1) models. As the graph shows, the behaviour of the forecasts in all four cases are markedly similar; quick (linear) convergence to the mean. Notice that there is less variety in the dynamics of these forecasts compared to those of the AR(1) models.Note: when the red line is horizontal, it has reached the mean of the simulated series.AR(2) ModelsThings get a lot more interesting when we start to consider more complex ARIMA models. Take for example AR(2) models. These are just a small step up from the AR(1) model, right? Well, one might like to think that, but the dynamics of AR(2) models are quite rich in variety as we'll see in a moment.Let's explore four different AR(2) models:\\begin{equation}Y_{t} = C + 1.7 Y_{t-1} -0.8 Y_{t-2} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (9)\\\\Y_{t} = C + 0.9 Y_{t-1} -0.2 Y_{t-2} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (10)\\\\Y_{t} = C + 0.5 Y_{t-1} -0.2 Y_{t-2} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (11)\\\\Y_{t} = C + 0.1 Y_{t-1} -0.7 Y_{t-2} + \\nu_{t} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (12)\\end{equation}The out-of-sample forecasts associated with each of these models is shown in the graph below. It is quite clear that they each differ significantly and they are also quite a varied bunch in comparison to the forecasts that we've seen above - except for model 2's forecasts (top right plot) which behave similar to those for an AR(1) model.Note: when the red line is horizontal, it has reached the mean of the simulated series.The key point here is that not all AR(2) models have the same dynamics! For example, if the condition, \\begin{equation}\\alpha_{1}^{2}+4\\alpha_{2} \u0026lt; 0,\\end{equation}is satisfied then the AR(2) model displays pseudo periodic behaviour and as a result its forecasts will appear as stochastic cycles. On the other hand, if this condition is not satisfied, stochastic cycles will not be present in the forecasts; instead, the forecasts will be more similar to those for an AR(1) model.It's worth noting that the above condition comes from the general solution to the homogeneous form of the linear, autonomous, second-order difference equation (with complex roots). If this if foreign to you, I recommend both Chapter 1 of Hamilton (1994) and Chapter 20 of Hoy et al. (2001). Testing the above condition for the four AR(2) models results in the following:\\begin{equation}(1.7)^{2} + 4 (-0.8) = -0.31 \u0026lt; 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (13)\\\\(0.9)^{2} + 4 (-0.2) = 0.01 \u0026gt; 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (14)\\\\(0.5)^{2} + 4 (-0.2) = -0.55 \u0026lt; 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (15)\\\\(0.1)^{2} + 4 (-0.7) = -2.54 \u0026lt; 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (16)\\end{equation}As expected by the appearance of the plotted forecasts, the condition is satisfied for each of the four models except for model 2. Recall from the graph, model 2's forecasts behave (\"normally\") similar to an AR(1) model's forecasts. The forecasts associated with the other models contain cycles.Application - Modelling InflationNow that we have some background under our feet, let's try to interpret an AR(2) model in an application. Consider the following model for the inflation rate ():\\begin{equation}\\pi_{t} = C + \\alpha_{1} \\pi_{t-1} + \\alpha_{2} \\pi_{t-2} + \\nu_{t}.\\end{equation}A natural expression to associate with such a model would be something like: \"inflation today depends on the level of inflation yesterday and on the level of inflation on the day before yesterday\". Now, I wouldn't argue against such an interpretation, but I'd suggest that some caution be drawn and that we ought to dig a bit deeper to devise a proper interpretation. In this case we could ask, in which way is inflation related to previous levels of inflation? Are there cycles? If so, how many cycles are there? Can we say something about the peak and trough? How quickly do the forecasts converge to the mean? And so on.These are the sorts of questions we can ask when trying to interpret an AR(2) model and as you can see, it's not as straightforward as taking an estimated coefficient and saying \"a 1 unit increase in this variable is associated with a so-many unit increase in the dependent variable\" - making sure to attach the ceteris paribus condition to that statement, of course.Bear in mind that in our discussion so far, we have only explored a selection of AR(1), MA(1), and AR(2) models. We haven't even looked at the dynamics of mixed ARMA models and ARIMA models involving higher lags.To show how difficult it would be to interpret models that fall into that category, imagine another inflation model - an ARMA(3,1) with  constrained to zero:\\begin{equation}\\pi_{t} = C + \\alpha_{1} \\pi_{t-1} + \\alpha_{3} \\pi_{t-3} + \\theta_{1}\\nu_{t-1} + \\nu_{t}.\\end{equation}Say what you'd like, but here it's better to try to understand the dynamics of the system itself. As before, we can look and see what sort of forecasts the model produces, but the alternative approach that I mentioned at the beginning of this answer was to look at the impulse response function or time path associated with the system. This brings me to next part of my answer where we'll discuss impulse response functions.Impulse Response FunctionsThose who are familiar with vector autoregressions (VARs) will be aware that one usually tries to understand the estimated VAR model by interpreting the impulse response functions; rather than trying to interpret the estimated coefficients which are often too difficult to interpret anyway.The same approach can be taken when trying to understand ARIMA models. That is, rather than try to make sense of (complicated) statements like \"today's inflation depends on yesterday's inflation and on inflation from two months ago, but not on last week's inflation!\", we instead plot the impulse response function and try to make sense of that.Application - Four Macro VariablesFor this example (based on Leamer(2010)), let's consider four ARIMA models based on four macroeconomic variables; GDP growth, inflation, the unemployment rate, and the short-term interest rate. The four models have been estimated and can be written as:\\begin{eqnarray}Y_{t} \u0026amp;=\u0026amp; 3.20 + 0.22 Y_{t-1} + 0.15 Y_{t-2} + \\nu_{t}\\\\\\pi_{t} \u0026amp;=\u0026amp; 4.10 + 0.46 \\pi_{t-1} + 0.31\\pi_{t-2} + 0.16\\pi_{t-3} + 0.01\\pi_{t-4} + \\nu_{t}\\\\u_{t} \u0026amp;=\u0026amp; 6.2+ 1.58 u_{t-1} - 0.64 u_{t-2} + \\nu_{t}\\\\r_{t} \u0026amp;=\u0026amp; 6.0 + 1.18 r_{t-1} - 0.23 r_{t-2} + \\nu_{t}\\end{eqnarray}where  denotes GDP growth at time ,  denotes inflation,  denotes the unemployment rate, and  denotes the short-term interest rate (3-month treasury).The equations show that GDP growth, the unemployment rate, and the short-term interest rate are modeled as AR(2) processes while inflation is modeled as an AR(4) process. Rather than try to interpret the coefficients in each equation, let's plot the impulse response functions (IRFs) and interpret them instead. The graph below shows the impulse response functions associated with each of these models.Don't take this as a masterclass in interpreting IRFs - think of it more like a basic introduction - but anyway, to help us interpret the IRFs we'll need to accustom ourselves with two concepts; momentum and persistence. These two concepts are defined in Leamer (2010) as follows:  Momentum: Momentum is the tendency to continue moving in the same  direction. The momentum effect can offset the force of regression  (convergence) toward the mean and can allow a variable to move away  from its historical mean, for some time, but not indefinitely.    Persistence: A persistence variable will hang around where it is and  converge slowly only to the historical mean.Equipped with this knowledge, we now ask the question: suppose a variable is at its historical mean and it receives a temporary one unit shock in a single period, how will the variable respond in future periods? This is akin to asking those questions we asked before, such as, do the forecasts contains cycles?, how quickly do the forecasts converge to the mean?, etc.At last, we can now attempt to interpret the IRFs. Following a one unit shock, the unemployment rate and short-term interest rate (3-month treasury) are carried further from their historical mean. This is the momentum effect. The IRFs also show that the unemployment rate overshoots to a greater extent than does the short-term interest rate. We also see that all of the variables return to their historical means (none of them \"blow up\"), although they each do this at different rates. For example, GDP growth returns to its historical mean after about 6 periods following a shock, the unemployment rate returns to its historical mean after about 18 periods, but inflation and short-term interest take longer than 20 periods to return to their historical means. In this sense, GDP growth is the least persistent of the four variables while inflation can be said to be highly persistent.I think it's a fair conclusion to say that we've managed (at least partially) to make sense of what the four ARIMA models are telling us about each of the four macro variables. ConclusionRather than try to interpret the estimated coefficients in ARIMA models (difficult for many models), try instead to understand the dynamics of the system. We can attempt this by exploring the forecasts produced by our model and by plotting the impulse response function.[I'm happy enough to share my R code if anyone wants it.]References Hamilton, J. D. (1994). Time series analysis (Vol. 2). Princeton: Princeton university press.Leamer, E. (2010). Macroeconomic Patterns and Stories - A Guide for MBAs, Springer.Stengos, T., M. Hoy, J. Livernois, C. McKenna and R. Rees (2001). Mathematics for Economics, 2nd edition, MIT Press: Cambridge, MA.","Display_name":"Graeme Walsh","Creater_id":24617,"Start_date":"2013-07-01 14:42:31","Question_id":40905}
{"_id":{"$oid":"5837a578a05283111e4d3cd5"},"Last_activity":"2013-07-01 03:44:42","Creator_reputation":24971,"Question_score":7,"Answer_content":"Note that due to Wold's decomposition theorem you can rewrite any stationary ARMA model as a  model, i.e. :\\Delta Y_t=\\sum_{j=0}^{\\infty} \\psi_j\\nu_{t-j}In this form there are no lagged variables, so any interpretation involving notion of a lagged variable is not very convincing. However looking at the   and the  models separately:Y_t=\\nu_t+\\theta_{1}\\nu_{t-1}Y_t=\\rho Y_{t-1}+\\nu_{t}=\\nu_t+\\rho \\nu_{t-1}+ \\rho^2 \\nu_{t-1}+...you can say that error terms in ARMA models explain \"short-term\" influence of the past, and lagged terms explain \"long-term\" influence. Having said that I do not think that this helps a lot and usually nobody bothers with the precise interpretation of ARMA coefficients. The goal usually is to get an adequate model and use it for forecasting. ","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2013-07-01 03:44:42","Question_id":40905}
{"_id":{"$oid":"5837a578a05283111e4d3ce5"},"Last_activity":"2016-08-22 04:11:42","Creator_reputation":121,"Question_score":2,"Answer_content":"“Trough” is commonly used, although perhaps somewhat colloquial.","Display_name":"Zaz","Creater_id":108007,"Start_date":"2016-08-22 04:11:42","Question_id":163486}
{"_id":{"$oid":"5837a578a05283111e4d3ce6"},"Last_activity":"2015-08-24 01:15:34","Creator_reputation":723,"Question_score":4,"Answer_content":"In signal processing, a commonly used term is negative peak. In mathematics, you use the terms vertex of parabola, highest or lowest point, maximum or minimum. ","Display_name":"Digio","Creater_id":83065,"Start_date":"2015-07-27 22:34:25","Question_id":163486}
{"_id":{"$oid":"5837a578a05283111e4d3ce7"},"Last_activity":"2015-07-28 00:56:58","Creator_reputation":152503,"Question_score":3,"Answer_content":"The peaks are called \"local modes\" (specifically, the x-value is called the mode, the height would be the density at the mode).The highest one is sometimes called \"the mode\". The bottoms of troughs are often called \"antimodes\"","Display_name":"Glen_b","Creater_id":805,"Start_date":"2015-07-28 00:56:58","Question_id":163486}
{"_id":{"$oid":"5837a578a05283111e4d3cf8"},"Last_activity":"2016-08-22 03:41:26","Creator_reputation":11870,"Question_score":4,"Answer_content":"A nice feature of difference-in-differences (DiD) is actually that you don't need panel data for it. Given that the treatment happens at some sort of level of aggregation (in your case cities), you only need to sample random individuals from the cities before and after the treatment. This allows you to estimatey_{ist} = A_g + B_t + \\beta D_{st} + c X_{ist} + \\epsilon_{ist}and get the causal effect of the treatment as the expected post-pre outcome difference for the treated minus the expected post-pre outcome difference for the control.There is a case in which people use individual fixed effects instead of a treatment indicator and this is when we don't have a well-defined level of aggregation at which the treatment occurs. In that case you would estimatey_{it} = \\alpha_i + B_t + \\beta D_{it} + cX_{it}+\\epsilon_{it}where  is an indicator for the post-treatment period for individuals who received the treatment (for example, a job market program which happens all over the place). For more information on this see these lecture notes by Steve Pischke.In your setting, adding individual fixed effects should not change anything with respect to the point estimates. The treatment indicator  will just be absorbed by the individual fixed effects. However, these fixed effects might soak up some of the residual variance and therefore potentially reduce the standard error of your DiD coefficient.Here is a code example which shows that this is the case. I use Stata but you can replicate this in the statistical package of your choice. The \"individuals\" here are actually countries but they are still grouped according to some treatment indicator.* load the data set (requires an internet connection)use \"http://dss.princeton.edu/training/Panel101.dta\"* generate the time and treatment group indicators and their interactiongen time = (year\u0026gt;=1994) \u0026amp; !missing(year)gen treated = (country\u0026gt;4) \u0026amp; !missing(country)gen did = time*treated* do the standard DiD regressionreg y_bin time treated did------------------------------------------------------------------------------       y_bin |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------        time |       .375   .1212795     3.09   0.003     .1328576    .6171424     treated |   .4166667   .1434998     2.90   0.005       .13016    .7031734         did |  -.4027778   .1852575    -2.17   0.033    -.7726563   -.0328992       _cons |         .5   .0939427     5.32   0.000     .3124373    .6875627------------------------------------------------------------------------------ * now repeat the same regression but also including country fixed effects areg y_bin did time treated, a(country)------------------------------------------------------------------------------       y_bin |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------        time |       .375    .120084     3.12   0.003     .1348773    .6151227     treated |          0  (omitted)         did |  -.4027778   .1834313    -2.20   0.032    -.7695713   -.0359843       _cons |   .6785714    .070314     9.65   0.000       .53797    .8191729-------------+----------------------------------------------------------------So you see that the DiD coefficient remains the same when the individual fixed effects are included (areg is one of the available fixed effects estimation commands in Stata). The standard errors are slightly tighter and our original treatment indicator was absorbed by the individual fixed effects and therefore dropped in the regression.In response to the commentI mentioned the Pischke example to show when people use individual fixed effects rather than a treatment group indicator. Your setting has a well defined group structure so the way you have written your model that's perfectly fine. Standard errors should be clustered at the city level, i.e. the level of aggregation at which the treatment occurs (I haven't done this in the example code but in DiD settings the standard errors need to be corrected as demonstrated by the Bertrand et al paper).Regarding the movers, they don't have much of a role to play here. The treatment indicator  is equal to 1 for people who live in a treated city  in the post-treatment period . To compute the DiD coefficient, we actually just need to compute four conditional expectations, namelyc = \\left[ E(y_{ist}|s=1,t=1) - E(y_{ist}|s=1,t=0)\\right] - \\left[ E(y_{ist}|s=0,t=1) - E(y_{ist}|s=0,t=0)\\right]So if you have 4 post-treatment periods for an individual who lives in a treated city for the first two, and then moves to a control city for the remaining two periods, the first two of those observations will be used in the computation of  and the last two in . To make it clear why identification comes from the group differences over time and not from the movers you can visualize this with a simple graph. Suppose the change in the outcome is truly only because of the treatment and that it has a contemporaneous effect. If we have an individual who lives in a treated city after the treatment starts but then moves to a control city, their outcome should go back to what it was before they were treated. This is shown in the stylized graph below.You might still want to think about movers for other reasons though. For instance, if the treatment has a lasting effect (i.e. it still affects the outcome even though the individual has moved) ","Display_name":"Andy","Creater_id":26338,"Start_date":"2016-08-19 14:35:13","Question_id":229996}
{"_id":{"$oid":"5837a578a05283111e4d3d07"},"Last_activity":"2016-03-22 16:20:43","Creator_reputation":14354,"Question_score":3,"Answer_content":"It will depend very much on your question, and your field (or even subfield). As an example, one day in a graduate class, an instructor asked if we thought a relative risk of 1.25 (or something like that) was a \"big effect\".The environmental epidemiologists all raised their hands. Lawsuits would be filed, and EPA regulations passed on things with a RR of 1.25The cancer epidemiologists mostly raised their hands. The \"low-hanging fruit\" for cancer causes has already been found for the most part, so something with that much of an increase in risk is probably a big deal.The infectious disease epidemiologists on the other hand, mostly responded with a shrug - we're often used to dealing with much larger effect sizes.So it depends. You should choose an effect size based on some criteria - is there an initial study that suggests something? A meta-analysis?If there isn't, you may be well served by calculating power over a larger range of variables. For example, this is a figure from a power-calculation I performed:Here, I wasn't sure of either the effect measure, or the ratio of exposed to unexposed individuals in the data, so I varied those to make sure that, under a number of different circumstances that my sample size was sufficient.","Display_name":"Fomite","Creater_id":5836,"Start_date":"2016-03-22 16:20:43","Question_id":202849}
{"_id":{"$oid":"5837a578a05283111e4d3d08"},"Last_activity":"2016-03-22 16:04:34","Creator_reputation":152503,"Question_score":2,"Answer_content":"The effect size will come from subject matter considerations (prior studies, theory, knowledge of similar variables and so on). One of the considerations is what would be a smallest \"substantive\" effect, one big enough to be considered important. You are trying to identify an answer to the question \"what is the effect size that you want to have an 80% chance* of picking up?\".* (or whatever other power you put in to the calculation; there's nothing special about 80% even if it's widely used in psychology). To answer that question, you'd at a minimum need to figure out what sort of effect size people in that subject matter area might care about. That will depend on many things. If you choose a large effect size and base your sample size calculation on that, any smaller effect sizes will be less likely to be identified, so you risk having too small a sample size to spot it. So you also need to have an idea (from subject matter knowledge, as above) how large an effect of this kind might tend to be (others with similar subject matter knowledge will presumably also consider it a likely effect size or can be convinced of it from an argument that relies on that shared understanding of the subject). Guessing much to small will result in unnecessarily huge sample sizes, and guessing much too large risks wasting the study. Note that Cohen (1992) \"A power primer\" said:  My intent was that medium ES represent an effect likely to be visible to the naked eye of a careful observer. (It has since been noted in effect-size surveys that it approximates the average size of observed effects in various fields. ) I set small ES to be noticeably smaller than medium but not so small as to be trivial,... so Cohen's \"small\" effect size would relate to the first thing I mentioned (an effect size \"people will care about\"), while (by his parenthetical comment) his \"medium\" one is closer to typical effect sizes in that subject area (the second criterion I discussed); this will typically be larger than effect sizes people care about (otherwise they're in a subject area where they won't find many of the actual effects that exist worth caring about).","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-03-22 16:04:34","Question_id":202849}
{"_id":{"$oid":"5837a578a05283111e4d3d09"},"Last_activity":"2016-03-22 03:33:01","Creator_reputation":730,"Question_score":2,"Answer_content":"It depends whether you aim for superiority, equivalence, or non-inferiority.In most typical scenarios we aim for superiority.Then the effect size would be the mininal effect that would be clinically relevant or meaningful, and you need subject competence on this.Imagine you want to compare a new and likely more potent drug to lower blood pressure to the one which is standard of care. Let us assume that a difference less than 0.3 mm Hg (eg 0.001 mm Hg) would be irrelevant for patients and physicians (even if statistically significant!). Then you'll aim for an effect size of at least 0.3 mm Hg (say, 0.55 mm Hg). Remember that you also need to factor in the variability of the phenomenon.Take note that in a superiority scenario alpha will be 2-tailed.In a hypothetical case where you compare two blood pressure lowering drugs in a randomized trial with 1:1 allocation, you assume an effect size with mean difference of 0.55 mm Hg, a standard deviation in each group of 0.9 mm, and aim for 0.8 power and 0.05 2-tailed alpha within a superiority framework. According to the computations below (samplesize package in R), you will need 88 patients (44 per group).n.ttest(power = 0.8, alpha = 0.05, mean.diff = 0.55, sd1 = 0.90, k = 1,design = \"unpaired\", fraction = \"balanced\", variance = \"equal\")`Sample size group 1`[1] 44$`Sample size group 2`[1] 44","Display_name":"Giuseppe Biondi-Zoccai","Creater_id":107799,"Start_date":"2016-03-22 00:06:13","Question_id":202849}
{"_id":{"$oid":"5837a578a05283111e4d3d16"},"Last_activity":"2016-08-22 02:35:37","Creator_reputation":33,"Question_score":0,"Answer_content":"By using the PLS Regression you can follow this steps:Apply the algorithm by using a validation method (i recommend the full cross validation);Select the number of latent variables that the validation gives to you;Re-run the pls algorithm with the number of latent variables in step 2 and re-apply the cross validation.With this 3 steps you can obtain an  but the  now is the result of the combination of the old variables, the Score Matrix.Basically you can only compare the original  with the new , because of the nature of the latent variables that you select.","Display_name":"Enzo D\u0026#39;Innocenzo","Creater_id":124731,"Start_date":"2016-08-22 02:35:37","Question_id":231057}
{"_id":{"$oid":"5837a578a05283111e4d3d25"},"Last_activity":"2016-08-22 02:13:11","Creator_reputation":152503,"Question_score":1,"Answer_content":"In this answer I assume you really mean \"set\" rather than \"multiset\" (as we might see more typically in statistics).One measure of set-similarity is Jaccard similarity   (also called the Jaccard index). That is, the number in both sets divided by the number in either set.Correspondingly, the Jaccard dissimilarity between two sets is . We could generalize the Jaccard similarity to more than two sets readily enough. If  is a collection of sets , then   and then perhaps define a measure of dissimilarity (taking on some sense of \"variability\") as its complement, . (However numerous other similarity measures exist, as whuber points out; it depends what you want to measure)You mention using entropy (by which I assume you mean something like cross-entropy).To work with cross entropy you'd need to assign some sort of probabilities to the elements.If the sets were finite, and one were to define the probabilities uniformly that might work, but cross entropy is also not symmetric; you'd presumably want a symmetric measure (you could perhaps add the two cross entropies ). then you'd need to generalize to more than two sets, possibly by summing all the pairwise s. However I don't think this would be especially satisfactory as it stands since it's not 0 when the sets are the same.Related to it but better still might be the symmetrized Kullback-Liebler divergence. Again you would need to generalize to multiple sets.Hopefully these give you some ideas. You should probably look around some of the other similarity and dissimilarity indices that already exist.  ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-20 20:16:31","Question_id":230761}
{"_id":{"$oid":"5837a578a05283111e4d3d36"},"Last_activity":"2016-08-22 00:46:43","Creator_reputation":25838,"Question_score":1,"Answer_content":"There is confusion there on the notion(s) of uniform distribution(s):if the sampling distribution is uniform and depends on a parameter, it means that the support of the uniform depends on, hence that the normalising constant of the density dependson . Therefore the likelihood is not constant on ,which means that the posterior is not uniform on  under a uniform prior. Obviously, if , picking the prior  will produce a constant posterior.If one applies a transform  to a uniform variate,  is only uniform for a constant Jacobian , which means the transform is linear. This does not work with your polynomial transform. And there is no normalising constant involved.","Display_name":"Xi\u0026#39;an","Creater_id":7224,"Start_date":"2016-08-22 00:29:04","Question_id":231047}
{"_id":{"$oid":"5837a578a05283111e4d3d43"},"Last_activity":"2016-08-22 00:38:38","Creator_reputation":628,"Question_score":0,"Answer_content":"If you're interested in use (more than in development), you should give a try to rankade, our ranking system. Rankade is free and easy to use, and it features rankings, stats, and more.If you want to split the contest in two events (out/not and 0/1+ runs), you can use partial rankings feature. Rankade builds a subset for every game in the group, so just name your events using two different names, and you'll automatically get rankings for out/not, 0/1+ runs, and both.After main ranking data, some analysis about differences from three rankings should give you information on your secondary task:  ...players are not simply considered to be weak or strong but are also categorized by how they approach the game. Considering batsmen for example: [...]","Display_name":"Tomaso Neri","Creater_id":93694,"Start_date":"2016-08-21 14:16:03","Question_id":230518}
{"_id":{"$oid":"5837a578a05283111e4d3d50"},"Last_activity":"2016-08-22 00:13:23","Creator_reputation":1,"Question_score":0,"Answer_content":"In principle it is a common practice to use your dependent/outcome variable to impute independent ones. Imputation should keep the data structure (in terms of correlations between variables). Therefore, using the dependent variable to impute others should not be a problem. You might want to later exclude observations with missing values in your dependent variable from analysis.For more information you can check the article from Paul T. Von Hippel (2007) \"REGRESSION WITH MISSING YS: AN IMPROVED STRATEGY FOR ANALYZING MULTIPLY IMPUTED DATA.\" http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9531.2007.00180.x/abstract","Display_name":"Josep","Creater_id":120818,"Start_date":"2016-08-22 00:13:23","Question_id":81244}
{"_id":{"$oid":"5837a578a05283111e4d3d5f"},"Last_activity":"2016-08-21 17:08:54","Creator_reputation":152503,"Question_score":5,"Answer_content":"A classic example would be a trimmed mean.For concreteness consider a 25% trimmed mean, where we average the middle half of the data.That's an L-estimator, but not an M-estimator. It can in a sense be approximated* by a Huber-type M-estimator but they're not the same.* (perhaps 'analogy' would be a better term than 'approximation' -- they might not always be very close together -- if the distribution is quite skew for example. In symmetric cases they're very alike.)While many L-estimators can also be M-estimators, there are a great many that aren't.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-21 17:03:24","Question_id":231011}
{"_id":{"$oid":"5837a578a05283111e4d3d6c"},"Last_activity":"2016-08-21 18:12:43","Creator_reputation":152503,"Question_score":3,"Answer_content":"You're correct that the mean and variance functions are of the same form.This suggests that in very large samples, as long as you don't have observations really close to 1 or 0 they should tend to give quite similar answers because in that situation observations will have similar relative weights.But in smaller samples where some of the continuous proportions approach the bounds, the differences can grow larger because the relative weights given by the two approaches will differ; if the points that get different weights are also relatively influential (more extreme in x-space), the differences may in some cases become substantial.In beta-regression you'd be estimating via ML, and in the case of a quasibinomial model - at least one estimated in R, note this comment in the help:  The quasibinomial and quasipoisson families differ from the binomial and poisson families only in that the dispersion parameter is not fixed at one, so they can model over-dispersion. For the binomial case see McCullagh and Nelder (1989, pp. 124–8). Although they show that there is (under some restrictions) a model with variance proportional to mean as in the quasi-binomial model, note that glm does not compute maximum-likelihood estimates in that model. The behaviour of S is closer to the quasi- variants. I think in betareg you can get  values, and you can as well for GLMs, so at the two fitted models you can compare an approximation of each observation's relative influence (/\"weight\") on its own fitted value (since the other components of the ratio of influences should cancel, or nearly so). This should give a quick sense of which observations are looked at most differently by the two approaches. [One might do it more exactly by actually tweaking the observations one by one and seeing the change in fit per unit change in value]Note that the betareg vignette gives some discussion of the connection between these models at the end of section 2.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-21 18:12:43","Question_id":231017}
{"_id":{"$oid":"5837a578a05283111e4d3d79"},"Last_activity":"2016-08-21 22:39:44","Creator_reputation":441,"Question_score":0,"Answer_content":"If  follow independent and identically distributed , then each  follows a chi-squared distribution with one degree of freedom which leads to the result that  follows . In your case,  follows independent and identically distributed . To apply the above, we need to transform them into standard normal variables and we all know how: standardize. That's where we get to . Square it so that it follows a chi-squared distribution:\\dfrac{\\left(X_{i}-m\\right)^{2}}{2}\\sim \\chi^{2}\\left(1\\right), \\qquad \\sum_{i=1}^{n}\\dfrac{\\left(X_{i}-m\\right)^{2}}{2}\\sim \\chi^{2}\\left(n\\right)If we could do without mathematical rigour, then let's just proceed by simply stating that all the deviations  are independent. So we can obtain the same result except that we lose one degree of freedom because we've estimated the mean  with  in advance.\\dfrac{1}{2}\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}\\sim \\chi^{2}\\left(n-1\\right)","Display_name":"Daeyoung Lim","Creater_id":98085,"Start_date":"2016-08-21 22:39:44","Question_id":230966}
{"_id":{"$oid":"5837a578a05283111e4d3d86"},"Last_activity":"2016-08-21 22:14:37","Creator_reputation":6,"Question_score":0,"Answer_content":"I came across Kruskal-Wallis Rank Sum Test in R.This page explains the usage.   Try this demo example in R console:  ## Hollander \u0026amp; Wolfe (1973), 116.## Mucociliary efficiency from the rate of removal of dust in normal##  subjects, subjects with obstructive airway disease, and subjects##  with asbestosis.x \u0026lt;- c(2.9, 3.0, 2.5, 2.6, 3.2) # normal subjectsy \u0026lt;- c(3.8, 2.7, 4.0, 2.4)      # with obstructive airway diseasez \u0026lt;- c(2.8, 3.4, 3.7, 2.2, 2.0) # with asbestosiskruskal.test(list(x, y, z))## Equivalently,x \u0026lt;- c(x, y, z)g \u0026lt;- factor(rep(1:3, c(5, 4, 5)),            labels = c(\"Normal subjects\",                       \"Subjects with obstructive airway disease\",                       \"Subjects with asbestosis\"))kruskal.test(x, g)## Formula interface.require(graphics)boxplot(Ozone ~ Month, data = airquality)kruskal.test(Ozone ~ Month, data = airquality)","Display_name":"Nari2","Creater_id":30084,"Start_date":"2016-08-21 22:14:37","Question_id":221002}
{"_id":{"$oid":"5837a578a05283111e4d3d93"},"Last_activity":"2016-08-21 22:12:55","Creator_reputation":11,"Question_score":0,"Answer_content":"Datamining could be broken down into two categories. If you are interested in measuring the effect of a data set/variables on a specific variable then this would be considered supervised learning. For deep and exploratory learning with no objective you are undergoing unsupervised learning.  Graphing and statistical analysis of the data (understanding distributions and gaining intuition) are the first steps. ","Display_name":"moka","Creater_id":128497,"Start_date":"2016-08-21 22:12:55","Question_id":12822}
{"_id":{"$oid":"5837a578a05283111e4d3d94"},"Last_activity":"2016-08-21 08:48:10","Creator_reputation":57702,"Question_score":6,"Answer_content":"There's a whole field of exploratory data analysis (EDA), and an excellent book on this subject called Exploratory Data Analysis, by John W. Tukey.I like that you are using graphs - there are many other graphs that can be useful, depending on your data - how many variables? What nature are the variables (Categorical? Numeric? Continuous? Counts? Ordinal?) One graph that is often useful for data with multiple variables is a scatterplot matrix.You can look for various types of outliers, which are often interesting points.But I don't think this whole process can be made really methodical and scientific - exploration is what comes BEFORE the methodical and scientific approaches can be brought in. Here, I think the key aspect is playfulness.","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2011-07-09 05:20:41","Question_id":12822}
{"_id":{"$oid":"5837a578a05283111e4d3d95"},"Last_activity":"2011-07-09 03:42:38","Creator_reputation":14029,"Question_score":1,"Answer_content":"If you have chronological data i.e.time series data then there are \"knowns\" and waiting to be discovered are the \"unknowns\" . For example if you have a sequence of data points for 10 periods such as 1,9,1,9,1,5,1,9,1,9 then based upon this sample one can reasonably expect 1,9,1,9,... to arise in the future. What data analysis reveals is that there is an \"unusual\" reading at period 6 even though it is well within +-3 sigma limits suggesting that the DGF did not hold. Unmasking the Inlier/Outlier allows us to reveal things about the data. We also note that the Mean Value is not the Expected Value. This idea easily extends to detecting Mean Shifts and/or Local Time Trends that may have been unknown before the data was analyzed ( Hypothesis Generation ). Now it is quite possible that the next 10 readings are also 1,9,1,9,1,5,1,9,1,9 suggesting that the \"5\" is not necessarily untoward. If we observe an error process from a suitable model that exhibits provable non-constant variance we might be revealing one of the following states of nature: 1) the parameters might have changed at a particular point in time ; 2. There may be a need for Weighted Analysis (GLS) ; 3. There may be a need to transform the data via a power transform; 4. There may be a need to actually model the variance of the errors. If you have daily data good analysis might reveal that there is a window of response (lead,contemporaneous and lag structure) around each Holiday reflecting consistent/predictable behavior. You might also be able to reveal that certain days of the month have a significant effect or that  Fridays before a Monday holiday have exceptional activity. ","Display_name":"IrishStat","Creater_id":3382,"Start_date":"2011-07-09 03:42:38","Question_id":12822}
{"_id":{"$oid":"5837a578a05283111e4d3da2"},"Last_activity":"2016-08-21 22:05:28","Creator_reputation":11,"Question_score":0,"Answer_content":"If you can use R, DmWR package allows for comparing multiple models through multiple indicators (mse, rmse and others) using function predictionevalaute","Display_name":"moka","Creater_id":128497,"Start_date":"2016-08-21 22:05:28","Question_id":231010}
{"_id":{"$oid":"5837a578a05283111e4d3daf"},"Last_activity":"2016-08-21 21:52:14","Creator_reputation":11,"Question_score":0,"Answer_content":"In general and dismissing collinearity , linear models become more representative as variables are introduced even though R^2 is relatively low in business application. Literature exists explaining this rational. During data exploratory practice one may find two less significant variables resulting in higher significance when clustered (unsupervised data mining ).","Display_name":"moka","Creater_id":128497,"Start_date":"2016-08-21 21:52:14","Question_id":231033}
{"_id":{"$oid":"5837a578a05283111e4d3dc0"},"Last_activity":"2016-08-21 19:16:22","Creator_reputation":26,"Question_score":0,"Answer_content":"I think what you are looking for is \"partial correlation\". See https://en.wikipedia.org/wiki/Partial_correlation - the introductory statement of that page (as of today: Aug 21, 2016) is   In probability theory and statistics, partial correlation measures the degree of association between two random variables, with the effect of a set of controlling random variables removed.","Display_name":"Just_to_Answer","Creater_id":128491,"Start_date":"2016-08-21 19:16:22","Question_id":231006}
{"_id":{"$oid":"5837a578a05283111e4d3dcd"},"Last_activity":"2016-08-21 18:50:37","Creator_reputation":26,"Question_score":1,"Answer_content":"From r documentation (try ?pnorm) we see that:  pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)     ...    pnorm gives the distribution function    ....    lower.tail    logical; if TRUE (default), probabilities are P[X ≤ x] otherwise, P[X \u003e x].Putting it together   pnorm(abs(res2 \\times P(Z \u0026gt; |\\text{stdres}|)p$-value computation if the standardized residuals are assumed to have a standard normal distribution and if the test is two-tailed.","Display_name":"Just_to_Answer","Creater_id":128491,"Start_date":"2016-08-21 18:50:37","Question_id":230994}
{"_id":{"$oid":"5837a578a05283111e4d3dd9"},"Last_activity":"2016-08-21 18:50:20","Creator_reputation":4307,"Question_score":2,"Answer_content":"I believe the quote refers to this algorithm, where the relevant line reads:Here the authors are using the angle-brackets to denote an inner product, which is essentially the standard vector dot product from Physics 101.The second term is the orthogonal projection of  onto . By subtracting this from , the result  is made orthogonal to .","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-21 18:50:20","Question_id":230866}
{"_id":{"$oid":"5837a578a05283111e4d3de8"},"Last_activity":"2016-06-23 00:02:37","Creator_reputation":188,"Question_score":0,"Answer_content":"The gradient magnitude varies during the learning process and for the specific problem at hand. A shallow network that is just beginning with only a few layers and many parameters would not use a small gradient, because a higher sensitivity to small gradients would lead to overfitting of the data. Once a network becomes deeper with many layers then the gradients can be much smaller to distinguish between the increasing number of layers. So the magnitude of the gradient needs to be adjusted based on the problem, and your guess (Stochastic Gradient Descent) is merely a strategy for controlling the gradient while the network is learning. You can quickly search for the control and effects of gradients if you are more interested.","Display_name":"Travis","Creater_id":120288,"Start_date":"2016-06-23 00:02:37","Question_id":220245}
{"_id":{"$oid":"5837a578a05283111e4d3dfb"},"Last_activity":"2016-08-21 16:14:54","Creator_reputation":13,"Question_score":1,"Answer_content":"Or... you could use my own package, 'powerplus' (function 'Matpow'), that allows you to raise any diagonalizable matrix to any power (even complex). For non-diagonalizable matrices, you have the same capabilities as package expm (incidentally, I use it in Matpow's code). To the best of my knowledge, it currently is the most comprehensive R package that exists to deal with matrix exponentiation. Version 3.0 extends capabilities to (some) non-diagonalizable matrices too.","Display_name":"Albert Dorador","Creater_id":123485,"Start_date":"2016-07-16 00:01:54","Question_id":4320}
{"_id":{"$oid":"5837a578a05283111e4d3dfc"},"Last_activity":"2015-12-18 18:32:57","Creator_reputation":404,"Question_score":0,"Answer_content":"You can also use the matrixcalc package, which also provides other matrix operations:\u0026gt; require(matrixcalc)\u0026gt; mat \u0026lt;- matrix(1:9, nrow=3)\u0026gt; matrix.power(mat,2)     [,1] [,2] [,3][1,]   30   66  102[2,]   36   81  126[3,]   42   96  150\u0026gt; mat%*%mat     [,1] [,2] [,3][1,]   30   66  102[2,]   36   81  126[3,]   42   96  150","Display_name":"chandler","Creater_id":12053,"Start_date":"2015-12-18 18:32:57","Question_id":4320}
{"_id":{"$oid":"5837a578a05283111e4d3dfd"},"Last_activity":"2010-11-08 22:55:40","Creator_reputation":887,"Question_score":1,"Answer_content":"There is the following code you can write:  library(Biodem)  png(filename=\"images/mtx.exp_%03d.png\" ,width=480, height=480)    Name: mtx.exp    Title: Calculates the n-th power of a matrix    Aliases: mtx.exp    Keywords: array manip methods    ** Examples    test\u0026lt;-matrix(c(1:16), 4,4)  pow.test\u0026lt;-mtx.exp(test,10)  pow.testfor more details:http://rgm2.lab.nig.ac.jp/RGM2/R_man-2.9.0/library/Biodem/man/mtx.exp.html","Display_name":"mariana soffer","Creater_id":1808,"Start_date":"2010-11-08 22:55:40","Question_id":4320}
{"_id":{"$oid":"5837a578a05283111e4d3dfe"},"Last_activity":"2010-11-08 10:03:27","Creator_reputation":8181,"Question_score":9,"Answer_content":"Package expm provides the matrix %^% number operator notation for its function matpow():\u0026gt; library(expm)    \u0026gt; mat \u0026lt;- matrix(1:9, nrow=3)\u0026gt; mat %^% 2     [,1] [,2] [,3][1,]   30   66  102[2,]   36   81  126[3,]   42   96  150# check\u0026gt; mat %*% mat     [,1] [,2] [,3][1,]   30   66  102[2,]   36   81  126[3,]   42   96  150There's also sqrtm() for taking roots and expm() for matrix exponential.","Display_name":"caracal","Creater_id":1909,"Start_date":"2010-11-08 10:03:27","Question_id":4320}
{"_id":{"$oid":"5837a578a05283111e4d3e0b"},"Last_activity":"2016-08-17 15:12:42","Creator_reputation":152503,"Question_score":0,"Answer_content":"If you believe your response is conditionally Poisson, the obvious model would be a Poisson GLM. ANOVA is unsuitable for count data for a number of reasons; e.g. (i) counts will have variance related to mean (i.e. will be heteroskedastic), and(ii) small counts are distinctly non-normalYou should not do one way ANOVA (or any similar analysis, including Wilcoxon-Mann-Whitney or Poisson regression on only one IV) when you know there's more than one treatment that has been applied.Whether you do a main-effects model or a model with interaction in your Poisson GLM is up to you; that would relate to your subject-area knowledge and needs. (If you don't know which you want, I'd suggest including interaction.)It's difficult to say more in the absence of any discussion of what hypotheses you want to test.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-17 15:12:42","Question_id":230265}
{"_id":{"$oid":"5837a578a05283111e4d3e18"},"Last_activity":"2016-08-21 15:49:51","Creator_reputation":738,"Question_score":3,"Answer_content":"So, instead of trying to explain every detail about the correct use of logistic regression, I'll provide some references that do a good job at explaining assumptions.Assumptions -- In order to build a good statistical model, you'll have to understand the underlying model assumptions, and ensure your data is appropriate. Given that your outcome variable is binary (survived: 0/1), logistic regression is appropriate. However, it seems you have repeated measures per customer, so each observation is not  (independent and identically distributed); this is assumption #3. You're probably fine if you address the potential assumption violation in a report. Otherwise, you'd have to use a different model or perform additional testing. See the following for details on assumptions: https://statistics.laerd.com/spss-tutorials/binomial-logistic-regression-using-spss-statistics.phpInterpretation:When you see your logistic regression output, your coefficients will be reported in the form of for the  covariates you include in your model. If you are specifically interested in probably of outcome given your data, and do not want to deal with log-odds ratios or odds ratios, you'll have to do some transformation of coefficients.Please see the following for further details: http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdfFinally, here is a worked out example in R:http://www.ats.ucla.edu/stat/r/dae/logit.htmOther examples are available in other software if you do not use R.Autocorrelation  My concern is that the yes/no survived in period X is dependent on  the survival of the customer in period X-1.With that said, you want a logistic regression model with AR-1 covariance residual structure. If you are simply interested in population average effects (i.e. fixed effects), I would perform a Generalized Estimating Equation; these models are relatively straightforward and not too computationally intensive. Otherwise, if you're concerned with individual effects (i.e. random effects) you'll need to use a Logistic Mixed-Effects Model. These models carry more assumptions and computational convergence issues. Please see the next reference: http://www.ats.ucla.edu/stat/mult_pkg/glmm.htmHere is a worked out example with logistic linear mixed-effects:http://www.ats.ucla.edu/stat/r/dae/melogit.htmHope this helps. ","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-21 15:49:51","Question_id":230996}
{"_id":{"$oid":"5837a578a05283111e4d3e26"},"Last_activity":"2016-08-21 15:12:19","Creator_reputation":12722,"Question_score":0,"Answer_content":"With a max_depth=7 and n_estimators=300 you have given the model  opportunities to split on your predictor.  This is well beyond the number of pixels of vertical resolution available to show a stair step shape in any graph (my macbook pro is 2880-by-1800 for the entire monitor).Crank down those values and you'll see the stairstep.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-08-21 15:12:19","Question_id":231005}
{"_id":{"$oid":"5837a578a05283111e4d3e33"},"Last_activity":"2016-08-21 14:57:07","Creator_reputation":21,"Question_score":1,"Answer_content":"Weights initialization depend on the activation function being used. Xavier and Bengio(2010) derived a method for initializing weights based on theassumption that the activations are linear. Their method resulted in the formula:\\begin{align}W \\sim U \\left[ -\\frac{\\sqrt 6}{\\sqrt {n_{i} + n_{i+1}}}, \\frac{\\sqrt 6}{\\sqrt {n_{i} + n_{i+1}}} \\right] \\end{align}For weights initialized using uniform distribution where  represents  and   represents .He, Kaiming, et al.(2015) used a derivation method that considered use of ReLUs as the activation function and obtain a weight initialization formula:\\begin{align}W_l \\sim \\mathcal N \\left({\\Large 0}, \\sqrt{\\frac{2}{n_l}} \\right).\\end{align}For weights initialized using Gaussian distribution whose standard deviation (std) is Read a more comprehensive series of articles covering Mathematics behind weights initialization here.","Display_name":"Jefkine Kafuna","Creater_id":128459,"Start_date":"2016-08-21 10:42:08","Question_id":204114}
{"_id":{"$oid":"5837a578a05283111e4d3e34"},"Last_activity":"2016-06-01 19:33:17","Creator_reputation":7965,"Question_score":1,"Answer_content":"As far as I know the two formulas you gave are pretty much the standard initialization. I had done a literature review a while ago, I copied it below if interested.[1] addresses the question:First, weights shouldn't be set to zeros in order to break the symmetry when backprogragating:  Biases can generally be initialized to zero but weights need to be initialized carefully to break the symmetry between hidden units of the same layer. Because different output units receive different gradient signals, this symmetry breaking issue does not concern the output weights (into the output units), which can therefore also be set to zero.Some initialization strategies:[2] and [3] recommend scaling by the inverse of the square root of the fan-inGlorot and Bengio (2010) and the Deep Learning Tutorials use a combination of the fan-in and fan-out:for sigmoid units: sample a Uniform(-r, r) with  (fan-in is the number of inputs of the unit).for hyperbolic tangent units: sample a Uniform(-r, r) with  (fan-in is the number of inputs of the unit).in the case of RBMs, a zero-mean Gaussian with a small standard deviation around 0.1 or 0.01 works well (Hinton, 2010) to initialize the weights.Orthogonal random matrix initialization, i.e. W = np.random.randn(ndim, ndim); u, s, v = np.linalg.svd(W) then use u as your initialization matrix.Also, unsupervised pre-training may help in some situations:  An important choice is whether one should use  unsupervised pre-training (and which unsupervised  feature learning algorithm to use) in order  to initialize parameters. In most settings  we have found unsupervised pre-training to help  and very rarely to hurt, but of course that  implies additional training time and additional  hyper-parameters.Some ANN libraries also have some interesting lists, e.g. Lasagne:Constant([val]) Initialize weights with constant value.Normal([std, mean]) Sample initial weights from the Gaussian distribution.Uniform([range, std, mean]) Sample initial weights from the uniform distribution.Glorot(initializer[, gain, c01b])   Glorot weight initialization.GlorotNormal([gain, c01b])  Glorot with weights sampled from the Normal distribution.GlorotUniform([gain, c01b]) Glorot with weights sampled from the Uniform distribution.He(initializer[, gain, c01b])   He weight initialization.HeNormal([gain, c01b])  He initializer with weights sampled from the Normal distribution.HeUniform([gain, c01b]) He initializer with weights sampled from the Uniform distribution.Orthogonal([gain])  Intialize weights as Orthogonal matrix.Sparse([sparsity, std]) Initialize weights as sparse matrix.[1] Bengio, Yoshua. \"Practical recommendations for gradient-based training of deep architectures.\" Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 437-478.[2] LeCun, Y., Bottou, L., Orr, G. B., and Muller, K. (1998a). Efficient backprop. In Neural Networks, Tricks of the Trade.[3] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" International conference on artificial intelligence and statistics. 2010.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-06-01 19:33:17","Question_id":204114}
{"_id":{"$oid":"5837a578a05283111e4d3e35"},"Last_activity":"2016-05-15 17:53:04","Creator_reputation":163,"Question_score":2,"Answer_content":"The paper 'all you need is a good init' is a good relatively recent article about inits in deep learning. What I liked about it is that:it has a short and effective literature survey on init methods, references included.It achieves very good results without too many bells and whistles on cifar10.","Display_name":"rhadar","Creater_id":102850,"Start_date":"2016-05-15 17:53:04","Question_id":204114}
{"_id":{"$oid":"5837a578a05283111e4d3e36"},"Last_activity":"2016-03-28 07:55:13","Creator_reputation":55,"Question_score":2,"Answer_content":"Recently, Batch Normalization was introduced for this sole purpose. Please find the paper here","Display_name":"user52705","Creater_id":52705,"Start_date":"2016-03-28 07:55:13","Question_id":204114}
{"_id":{"$oid":"5837a578a05283111e4d3e45"},"Last_activity":"2016-08-21 14:20:09","Creator_reputation":8283,"Question_score":2,"Answer_content":"  If I took the whole dataset and divided the total number of balls that got a batsman out by the total number of balls bowled I can see that I would have the average probability of a bowler getting a batsman out - it will be around 0.03 (hopefully I haven't gone wrong already?)Unfortunately, this is maybe already not exactly what you're looking for.Suppose we have a single bowler, and two batsmen: Don Bradman and me. (I know very very little about cricket, so if I'm doing something way off here, let me know.) The games go something like:Don goes to bat, and is out on the 99th bowl.I go to bat, and am immediately out.Don goes to bat, and is out on the 99th bowl.I go to bat, and am immediately out.In this case, there are four outs out of 200 bowls, so the marginal probability of a bowler getting a batsman out is estimated as 4/200 = 2%. But really, the Don's probability of being out is more like 1%, whereas mine is 100%. So if you choose a batsman and a bowler at random, the probability that this bowler gets this batsman out this time is more like (50% chance you picked Don) * (1% chance he gets out) + (50% chance you picked me) * (100% chance I get out) = 50.05%. But if you choose a pitch at random, then it's a 2% chance that it gets out. So you need to think carefully about which of those sampling models you're thinking of.Anyway, your proposal is not crazy. More symbolically, let  be the bowler and  the batsman; let  be the probability that  gets  out. Then you're saying:f(b, m)= \\frac{\\E_{m'}[ f(b, m') ] \\E_{b'}[ f(b', m) ]}{\\E_{b', m'}[ f(b', m') ]}.This does have the desired property that:\\E_{b,m}[f(b, m)] = \\frac{\\E_{b,m'}[ f(b, m') ] \\E_{b',m}[ f(b', m) ]}{\\E_{b',m'}[ f(b', m') ]} = \\E_{b,m}[ f(b, m) ];it's similarly consistent if you take means over only  or .Note that in this case we can assign\\begin{gather}C := \\E_{b, m}[f(b, m)] \\\\g(b) := \\E_{m}[f(b, m)] / \\sqrt{C} \\\\h(m) := \\E_{b}[f(b, m)] / \\sqrt{C} \\\\\\text{so that } f(b, m) = g(b) \\, h(m).\\end{gather}Your assumption is that you can observe  and  reasonably well from the data. As long as (a) you have enough games [which you do] and (b) the players all play each other with reasonably similar frequencies, then this is fine.To elaborate on (b) a bit: imagine that you have data from a bunch of professional games, and a bunch of games of me playing with my friends. If there's no overlap, maybe I look really good compared to my friends, so maybe you think I'm much better than the worst professional player. This is obviously false, but you don't have any data to refute that. If you have a little overlap though, where I played against a professional player one time and got destroyed, then the data does support ranking me and my friends as all way worse than the pros, but your method wouldn't account for it. Technically, the problem here is that you're assuming you have a good sample for e.g. , but your  distribution is biased.Of course your data won't look this bad, but depending on the league structure or whatever, it might have some elements of that problem.You can try working around it with a different approach. The proposed model for  is actually an instance of low-rank matrix factorization models common in collaborative filtering, as in the Netflix problem. There, you choose the function  and  to be of dimension , and represent . You can interpret  as complexifying your model from a single \"quality\" score to having scores along multiple dimensions: perhaps certain bowlers do better against certain types of batsmen. (This has been done e.g. for NBA games.)The reason they're called matrix factorization is because if you make a matrix  with as many rows as bowlers and as many columns as batsmen, you can write this as\\underbrace{\\begin{bmatrix}f(b_1, m_1) \u0026amp; f(b_1, m_2) \u0026amp; \\dots \u0026amp; f(b_1, m_M) \\\\f(b_2, m_1) \u0026amp; f(b_2, m_2) \u0026amp; \\dots \u0026amp; f(b_2, m_M) \\\\\\vdots      \u0026amp; \\vdots      \u0026amp; \\ddots\u0026amp; \\vdots \\\\f(b_N, m_1) \u0026amp; f(b_N, m_2) \u0026amp; \\dots \u0026amp; f(b_N, m_M)\\end{bmatrix}}_{F}=\\underbrace{\\begin{bmatrix}g(b_1) \\\\\\vdots \\\\g(b_N)\\end{bmatrix}}_{G}\\underbrace{\\begin{bmatrix}h(m_1) \\\\\\vdots \\\\h(m_M)\\end{bmatrix}^T}_{H^T}where you've factored an  matrix  into an  one  and an  one .Of course, you don't get to observe  directly. The usual model is that you get to observe noisy entries of  at random; in your case, you get to observe a draw from a binomial distribution with a random number of trials for each entry of .You could construct a probability model like, say:\\begin{gather}G_{ik} \\sim \\mathcal{N}(0, \\sigma_G^2) \\\\H_{jk} \\sim \\mathcal{N}(0, \\sigma_H^2) \\\\F_{ij} = G_i^T H_j \\\\R_{ij} \\sim \\mathcal{Binomial}(n_{ij}, F_{ij})\\end{gather}where the  and  are observed, and you'd probably put some hyperpriors over / and do inference e.g. in Stan.This isn't a perfect model: for one, it ignores that  is correlated to the scores (as I mentioned in the first section), and more importantly, it doesn't constrain  to be in  (you'd probably use a logistic sigmoid or similar to achieve that). A related article, with more complex priors for  and  (but that doesn't use the binomial likelihood) is: Salakhutdinov and Mnih, Bayesian probabilistic matrix factorization using Markov chain Monte Carlo, ICML 2008. (doi / author's pdf)","Display_name":"Dougal","Creater_id":9964,"Start_date":"2016-08-17 12:01:28","Question_id":230337}
{"_id":{"$oid":"5837a578a05283111e4d3e46"},"Last_activity":"2016-08-17 11:56:22","Creator_reputation":143,"Question_score":0,"Answer_content":"You can't infer the correct probability that B will be out given that A is the bowler if A and B never met on the field just based on their averages with other players.","Display_name":"oW_","Creater_id":127417,"Start_date":"2016-08-17 11:56:22","Question_id":230337}
{"_id":{"$oid":"5837a578a05283111e4d3e53"},"Last_activity":"2016-08-21 14:05:04","Creator_reputation":1058,"Question_score":2,"Answer_content":"You could use an analysis of variance (ANOVA) to do null hypothesis significance testing (NHST) or to calculate an intraclass correlation coefficient (ICC). Both approaches are based on the same ANOVA, but they differ in several important ways. I will argue that these differences make the ICC a preferable option, but you may disagree. The important thing is to understand the consequences of whichever choice you make. In short, the NHST approach will only tell you if the raters' measurements are significantly different; it won't tell you how different they are or how the variance associated with raters compares to the variance associated with other sources of variance.I will refer to \"objects of measurement\" and \"raters.\" Objects are the things being measured (e.g., emails, films, or X-Ray images) and raters are the things providing these measurements (e.g., spam filtering algorithms, film critics, or radiologists). The measurements themselves may be nominal categories (e.g., spam or non-spam), ordinal categories (e.g., 1, 2, 3, 4, or 5 stars), or numbers on an interval/ratio scale (e.g., tumor diameter in mm).If you use a one-way model, the ANOVA will yield two mean squares: one for object of measurement and one for residual sources of variance. Because there is no mean square for rater, any variance that is related to differences between raters will be collapsed into the residual term along with variance due to measurement error and other unaccounted for variance. Thus, an NHST based on a one-way model is somewhat confounded in the application you seem to be describing.If you use a two-way model, the ANOVA will yield three mean squares: one for object of measurement, one for rater, and one for residual sources of variance. These mean squares can be used in NHST or to calculate an ICC. This type of model is typically more useful for analyzing inter-rater reliability.To concretize things, let's look at some example data of six objects measured by four raters on a 1 to 10 interval scale, presented in the following object-by-rater matrix:\\begin{array}. \u0026amp; R1 \u0026amp; R2 \u0026amp; R3 \u0026amp; R4 \\\\O1 \u0026amp; 9 \u0026amp; 2 \u0026amp; 5 \u0026amp; 8 \\\\O2 \u0026amp; 6 \u0026amp; 1 \u0026amp; 3 \u0026amp; 2 \\\\O3 \u0026amp; 8 \u0026amp; 4 \u0026amp; 6 \u0026amp; 8 \\\\O4 \u0026amp; 7 \u0026amp; 1 \u0026amp; 2 \u0026amp; 6 \\\\O5 \u0026amp; 10 \u0026amp; 5 \u0026amp; 6 \u0026amp; 9 \\\\O6 \u0026amp; 6 \u0026amp; 2 \u0026amp; 4 \u0026amp; 7 \\\\\\end{array}Using a two-way model, you get the following mean squares:To use these for NHST, you would calculate an  statistic for the rater source of variance (as below), which we can compare to the critical . Our observed  exceeds the critical value, so we can say that the raters differ to a \"statistically significant\" degree. But that's about all...F_{Rater}=\\frac{MSR}{MSE}=\\frac{32.49}{1.02}=31.87 To calculate an ICC, you would use the following formulas to calculate the degree of consistency (denoted ) or absolute agreement (denoted ) between each rater and each other rater (denoted ) or between the mean of all raters and each rater (denoted ). Absolute agreement requires all raters to have the same mean (e.g., assign the same exact number to an object), whereas consistency allows each rater to have their own mean as long as the raters covary together (e.g., see an object as relatively high or low).So basically, the ICC gives you a quantification of reliability instead of a binary \"yes\" or \"no\" and, by virtue of its various formulations, allows you to control various aspects of what counts as reliability and what doesn't. By computing all four formulations, you can also get a sense of the raters' measurements. In this example, consistency is quite reasonable but agreement is a lot lower. So it would appear that the raters have differing means but seem to covary together pretty well. Visual inspection of the example data matrix can also give this sense, although checking a larger matrix would be more difficult. Additionally, the average score ICCs are a lot higher than the single score ICCs (which is always somewhat true) so a researcher could be a lot more comfortable using the average of all 4 raters than the measurements from any single rater.ReferencesMcGraw, K. O., \u0026amp; Wong, S. P. (1996). Forming inferences about some intraclass correlation coefficients. Psychological Methods, 1(1), 30–46.","Display_name":"Jeffrey Girard","Creater_id":111380,"Start_date":"2016-08-21 14:05:04","Question_id":230770}
{"_id":{"$oid":"5837a578a05283111e4d3e64"},"Last_activity":"2016-08-21 11:17:42","Creator_reputation":9048,"Question_score":1,"Answer_content":"For the various levels of New.moon, these are not odds ratios, but odds. So the odds of an \"incident\" is  during the 'new.moonthe rest' phase when TMIN (minimun temperature) is at 0. You could also back-translate this into the chances of an incident (i.e., ).If you want an odds ratio, you have to compare two odds against each other. So, for example, the odds are  for the 'the rest' phase and  for the 'pre' phase. So, the odds ratio is , or in other words, the odds are  times higher during 'the rest' phase compared to the 'pre' phase.For TMIN, the value is an odds ratio, comparing the odds of an incident for a one-unit increase in minimum temperature (so the odds ratio of  versus , where  is the minimum temperature value).","Display_name":"Wolfgang","Creater_id":1934,"Start_date":"2016-08-21 11:17:42","Question_id":230988}
{"_id":{"$oid":"5837a578a05283111e4d3e73"},"Last_activity":"2016-08-20 12:48:07","Creator_reputation":152503,"Question_score":1,"Answer_content":"pnuts' comment is correct. If the true rate of critical errors is actually zero, you won't see a single one. (Even then, you can't prove the rate is zero, you can at best give an upper bound -- in a confidence interval sense -- on the rate.)Conversely if you observe any critical failures in the three month period, you know for sure that the rate of critical failures cannot be zero.Seven is more than 0. The rate of critical failures is not 0.It's not clear to me why the other failures are relevant, but maybe you expressed the hypothesis of interest differently than you intended.You might be able to do something similar to an equivalence test (but in this case only a single one-sided test would be needed). This would require specifying a proportion of total failures or a rate per unit time that is \"practically\" close enough to zero -- some sort of acceptable level of critical failures that you can demonstrate you're below (in effect, because a confidence interval for the parameter of interest would be contained inside the \"acceptable region\"). Your clients may have very different views from you on what rate of errors is acceptable, though.An alternative would be to forget hypothesis tests or trying to give a bound on what's \"acceptable\" and just quote a one-sided interval for the parameter of interest. I wouldn't use t-statistics for that.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-20 12:48:07","Question_id":230891}
{"_id":{"$oid":"5837a578a05283111e4d3e84"},"Last_activity":"2016-08-21 03:43:32","Creator_reputation":5445,"Question_score":3,"Answer_content":"I don't see why Rating is nested within ID. This means that every every unique Rating belongs to one and only one ID which does not appear to be the case. If Rating is to be treated as random, then these should be crossed random effects. See the answer here for the difference between crossed and nested random effects and how to specify them.  That said, however, Rating should not be specified as a random intercept here because 1) there are only 4 levels, which is insufficient and 2) it is a likert scale item so the assumption of normality of random effects is hardly likely to hold.So a better model would bemodel = glmer(Correct ~ (1|ID) + Memory * State * Rating, data, family=binomial)Lastly, note that there a several ways that you can treat Rating as an independent variable, see here, here and here for more details.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-21 03:43:32","Question_id":230911}
{"_id":{"$oid":"5837a578a05283111e4d3e93"},"Last_activity":"2016-08-21 08:54:55","Creator_reputation":153,"Question_score":1,"Answer_content":"0 stands in R for removing the intercept, as well as -1. Why almost always you want to include intercept is well answered here: When is it ok to remove the intercept in a linear regression model?I can imagine one reason you'd be interested in removing intercept, that's when you scale your variables, please see: Interpreting the Intercept in a Regression Model","Display_name":"Piotr Falkowski","Creater_id":96761,"Start_date":"2016-08-21 08:54:55","Question_id":230919}
{"_id":{"$oid":"5837a578a05283111e4d3ea2"},"Last_activity":"2016-08-21 07:59:27","Creator_reputation":57702,"Question_score":-1,"Answer_content":"Within the chi-square type of framework you could use an exact test. However, since you think disease depends on treatment, I suggest logistic regression. Not only does this let you estimate the effect of each treatment, it lets you adjust for covariates that might be related to the disease. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-21 07:59:27","Question_id":230972}
{"_id":{"$oid":"5837a578a05283111e4d3eaf"},"Last_activity":"2016-08-21 06:59:05","Creator_reputation":6,"Question_score":0,"Answer_content":"After looking into the confusionmat code, I realized the problem. Matlab orders the \"positive\" variable as the 2nd dimension, i.e. second row and second column. In most specificity / sensitivity docs I had read the positive variable is ordered as the first dimension. As a result, the following plot matches:tpr = nan(length(T),1); fpr = nan(length(T),1);for ind_F = 1:1:length(T)  t_true = scores \u0026gt;= T(ind_F);  group = resp; grouphat = t_true;  [t_cm,t_c] = confusionmat(group,grouphat);  % ROC : TPR / FPR  tpr(ind_F) = t_cm(2,2)/sum(t_cm(2,:));  fpr(ind_F) = t_cm(1,2)/sum(t_cm(1,:));endfigure; plot(fpr,tpr); xlabel('fpr'); ylabel('tpr');Hopefully this might be helpful for someone else.","Display_name":"FoxRyerson","Creater_id":128379,"Start_date":"2016-08-21 06:59:05","Question_id":230865}
{"_id":{"$oid":"5837a578a05283111e4d3ebb"},"Last_activity":"2016-08-21 06:50:44","Creator_reputation":43,"Question_score":0,"Answer_content":"It depends on the dataset and application, do you have any more information on them? (Sorry I would have commented but I can't)I had a similar application for a TF-IDF matrix, with ~10,000 rows and 76 columns. I had to get a relevance score. For me, metric MDS gave the best results. I used Python with this function. Since your dataset is not that large, you could try all methods. If that takes too much time then use a subset of the data. Using a subset of data you could also manually check the MDS result to see if it's giving results that are reasonable. Using PCA to verify on the whole dataset is also an option though it's not that reliable.","Display_name":"Leela Prabhu","Creater_id":105753,"Start_date":"2016-08-21 06:50:44","Question_id":230963}
{"_id":{"$oid":"5837a578a05283111e4d3ec8"},"Last_activity":"2016-08-21 06:49:27","Creator_reputation":75835,"Question_score":4,"Answer_content":"All of the basic regression-type models that people use are for the mean.  With OLS regression, where the response is assumed conditionally normal, your predicted values, , are the conditional means (cf., here).  So in a GLiM context more broadly, where the response is distributed as something else like a Bernoulli, we also want to predict the mean.  A more general question, setting aside generalized linear models, is why we might want to predict means at all.  First, the mean is the expected value.  Moreover, for distributions in the exponential family (which means they are applicable for the GLiM) the mean is one of the parameters of the distribution.  If you know that the distribution is something or other, and you know the mean, you basically know everything there is know, or at least much of it.  (Some distributions have additional parameters that you would still want to estimate, for example, for the normal you would want to know the variance as well.)  You don't have to want to know the mean, however.  You might want to know the value of some quantile, for example the 37th percentile.  You can model that with quantile regression.  Ordinal logistic regression and the Cox proportional hazards model don't assume distributional forms and aren't estimating conditional means in a direct sense.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-08-21 06:42:50","Question_id":230967}
{"_id":{"$oid":"5837a578a05283111e4d3ed5"},"Last_activity":"2016-05-16 09:42:51","Creator_reputation":620,"Question_score":1,"Answer_content":"The examples provided in ?survdiff are pretty clear. Using some example data included in survival, thissurvdiff(Surv(futime, fustat) ~ rx,data=ovarian)Is testing for a difference in survival between individuals with rx = 1 and rx = 2. For your data, this will compare survival for males versus femalessurvdiff(Surv(time, Status) ~ sex, data=myeloma)And this will compare survival for \u0026lt;= 65 versus \u003e65.survdiff(Surv(time, Status) ~ age, data=myeloma)","Display_name":"Slow loris","Creater_id":114327,"Start_date":"2016-05-16 09:42:51","Question_id":114304}
{"_id":{"$oid":"5837a578a05283111e4d3ed6"},"Last_activity":"2014-09-04 04:07:46","Creator_reputation":531,"Question_score":0,"Answer_content":"It doesn't look right. If you want to limit the analysis to just males or females, the \"sex==1\" or \"sex==2\" is a separate input, the \"subset\" clause. The new commands would beMales\u0026lt;-survdiff(surv(time,Status)~Patients, data = myeloma, sex==1)Females\u0026lt;-survdiff(surv(time,Status)~Patients, data = myeloma, sex==2)You need to specify something as the dependent variable in the equation, and the only remaing variable is Patients.If you actually want to measure the effects of both sex and age together on survival, you need to be doing a stratified log rank test. I've used the function SurvTest(in documentation)/surv_test from the coin package.As far as I could tell, it only takes one stratifying variable, but I came up with a workaround by appending several variables into a new variable and using that as the stratifying variable.There are a couple of other packages that can do the same thing, but I can't think of them right now.","Display_name":"JenSCDC","Creater_id":46522,"Start_date":"2014-09-04 04:07:46","Question_id":114304}
{"_id":{"$oid":"5837a578a05283111e4d3ee3"},"Last_activity":"2016-08-21 03:44:02","Creator_reputation":12907,"Question_score":2,"Answer_content":"When modelling stock prices, it is quite common to transform the original prices  to logarithmic returns  and then employ a GARCH model. Logarithmic returns reflect price changes relative to price levels. If the price was fluctuating at around the same level, logarithmic returns would behave similarly to simple returns (just be scaled by roughly a constant); then not taking logarithms could be justified. But since the price is roughly a random walk, it does not stay around any fixed level for too long, and hence there is a difference between logarithmic and simple returns. Simple returns have an amplitude that changes with the price level, while logarithmic returns have a roughly constant amplitude. Therefore, modelling logarithmic returns is more convenient.Given a GARCH model for (logarithmic) returns, you can predict the volatility iteratively:\\begin{aligned} \\hat\\sigma_{t+1}^2 \u0026amp;= \\hat\\omega + \\hat\\alpha r_{t}^2 + \\hat\\beta \\sigma_{t}^2 \\\\\\hat r_{t+1}^2 \u0026amp;= \\hat\\sigma_{t+1}^2 \\\\ \\hat\\sigma_{t+2}^2 \u0026amp;= \\hat\\omega + \\hat\\alpha r_{t+1}^2 + \\hat\\beta \\hat\\sigma_{t+1}^2 \\\\\\hat r_{t+2}^2 \u0026amp;= \\hat\\sigma_{t+2}^2 \\\\ \u0026amp;\\dotsc\\end{aligned}In R, use function predict on a fitted object from garchFit, in your case, garchA; see p. 30-31 of the \"fGarch\" manual for details.In the above I assumed you are not using an ARMA model for the conditional mean of returns. Predictability in the level of returns via ARMA would be a little naive to expect and would be a sign of market inefficiency. Without ARMA, the point forecast of  for .However, if you decide to use ARMA, replace  and  with  in the GARCH formulas above, where  is a residual from the ARMA model. Also, the point forecast is generally nonzero and is given by the ARMA model.Also, if you use ARMA, estimate both ARMA and GARCH simultaneously (rather than first estimating ARMA and then fitting GARCH on its residuals). This will yield consistent and efficient parameter estimates. This can be done with the same garchFit function as you are using now.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-21 03:26:31","Question_id":230955}
{"_id":{"$oid":"5837a578a05283111e4d3ef2"},"Last_activity":"2016-01-11 10:33:28","Creator_reputation":75835,"Question_score":29,"Answer_content":"+1 to @user11852, and @jem77bfp, these are good answers.  Let me approach this from a different perspective, not because I think it's necessarily better in practice, but because I think it's instructive.  Here are a few relevant facts that we know already:   is the slope of the regression line when both  and  are standardized, i.e., ,   is the proportion of the variance in  attributable to the variance in ,  (also, from the rules for variances):  the variance of a random variable multiplied by a constant is the constant squared times the original variance:\\text{Var}[aX]=a^2\\text{Var}[X]variances add, i.e., the variance of the sum of two random variables (assuming they are independent) is the sum of the two variances:\\text{Var}[X+\\varepsilon]=\\text{Var}[X]+\\text{Var}[\\varepsilon]Now, we can combine these four facts to create two standard normal variables whose populations will have a given correlation,  (more properly, ), although the samples you generate will have sample correlations that vary.  The idea is to create a pseudorandom variable, , that is standard normal, , and then find a coefficient, , and an error variance, , such that , where .  (Note that  must be  for this to work, and that, moreover, .)  Thus, you start with the  that you want; that's your coefficient, .  Then you figure out the error variance that you will need, it's .  (If your software requires you to use the standard deviation, take the square root of that value.)  Finally, for each pseudorandom variate, , that you have generated, generate a pseudorandom error variate, , with the appropriate error variance , and compute the correlated pseudorandom variate, , by multiplying and adding.  If you wanted to do this in R, the following code might work for you:  correlatedValue = function(x, r){  r2 = r**2  ve = 1-r2  SD = sqrt(ve)  e  = rnorm(length(x), mean=0, sd=SD)  y  = r*x + e  return(y)}set.seed(5)x = rnorm(10000)y = correlatedValue(x=x, r=.5)cor(x,y)[1] 0.4945964(Edit: I forgot to mention:) As I've described it, this procedure gives you two standard normal correlated variables.  If you don't want standard normals, but want the variables to have some specific means (not 0) and SDs (not 1), you can transform them without affecting the correlation.  Thus, you would subtract the observed mean to ensure that the mean is exactly , multiply the variable by the SD you want and then add the mean you want. If you want the observed mean to fluctuate normally around the desired mean, you would add the initial difference back.  Essentially, this is a z-score transformation in reverse.  Because this is a linear transformation, the transformed variable will have the same correlation with the other variable as before.  Again, this, in it's simplest form, only lets you generate a pair of correlated variables (this could be scaled up, but gets ugly fast), and is certainly not the most convenient way to get the job done.  In R, you would want to use ?mvrnorm in the MASS package, both because it's easier and because you can generate many variables with a given population correlation matrix.  Nonetheless, I think it's worthwhile to have walked through this process to see how some basic principles play out in a simple way.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2012-10-07 15:11:13","Question_id":38856}
{"_id":{"$oid":"5837a578a05283111e4d3ef3"},"Last_activity":"2012-10-07 17:45:18","Creator_reputation":384,"Question_score":11,"Answer_content":"In general this not a simple thing to do, but I believe there are packages for multivariate normal variable generation (at least in R, see mvrnorm in the MASS package), where you just input covariance matrix and mean vector. There is also one more \"constructive\" approach. Lets say we want to model a random vector  and we have its distribution function . The first step is to get the marginal distribution function i.e. integrate  by all F_{X_1}(x_1)= \\int_{-\\infty}^{\\infty} F(x_1,x_2)dx_2. Then we find  - the inverse function of  and plug in random variable  which is uniformly distributed on interval . In this step we generate first coordinate .Now, since we have got one coordinate, we need to plug it in our initial distribution function , and then get a conditional distribution function with condition .F(x_2 | X_1=\\hat{x}_1)= \\frac{F(\\hat{x}_1,x_2)}{f_{X_1}(\\hat{x}_1)},  where  is a probablity density function of marginal  distribution, i.e. .Then again you generate uniformly on  distributed variable  and plug it in the inverse of . Therefore you obtain . This method can be generalized to vectors with more dimensions, but the downside of it, is that you have to calculate, analytically or numerically so many functions. Idea can be found in this article as well http://www.econ-pol.unisi.it/dmq/pdf/DMQ_WP_34.pdf. If you don't understand the meaning of plugging in uniform variable into inverse probability distribution function try to make a sketch of univariate case and then remember what the geometric interpretation of inverse function is.","Display_name":"jem77bfp","Creater_id":14729,"Start_date":"2012-10-07 13:41:11","Question_id":38856}
{"_id":{"$oid":"5837a578a05283111e4d3ef4"},"Last_activity":"2012-10-07 13:29:03","Creator_reputation":8893,"Question_score":31,"Answer_content":"To answer your question on \"a good, ideally quick way to generate correlated random numbers\":Given a desired variance-covariance matrix  that is by definition positive definite, the Cholesky decomposition of it is: =;  being lower triangular matrix. If you now use this matrix  to project an uncorrelated random variable vector , the resulting projection  will be that of correlated random variables.You can find an concise explanation why this happens here.","Display_name":"usεr11852","Creater_id":11852,"Start_date":"2012-10-07 13:29:03","Question_id":38856}
{"_id":{"$oid":"5837a578a05283111e4d3f05"},"Last_activity":"2016-08-21 01:00:25","Creator_reputation":2443,"Question_score":0,"Answer_content":"If these are red blood cell counts for a fixed volume of blood, then the regression residuals for this type of data is often very well approximated by a normal distribution. Often a log transformation is very helpful when cell counts can be quite low with various suggestions available for 0 counts - e.g. using 0.5 cells or considering it a censored observation with a concentration of \u0026lt;1 cell/volume. Thus, things like ANOVA/ANCOVA/t-test etc. may be appropriate for this type of data (non-parametric methods like rank tests are of course also reasonable and the loss of efficiency is often relatively negligible - although group sizes are pretty small, so parametric approaches are likely worth it), if group allocation was by randomization - otherwise some additional adjustments (e.g. propensity scores) may be needed to account for the observational nature of the data, depending on what one tries to do.","Display_name":"Bj\u0026#246;rn","Creater_id":86652,"Start_date":"2016-08-21 01:00:25","Question_id":230939}
{"_id":{"$oid":"5837a578a05283111e4d3f12"},"Last_activity":"2016-08-20 23:54:20","Creator_reputation":5189,"Question_score":4,"Answer_content":" is our information in the sense that for all , we know whether . Let us use the Tickets in a box metaphor, extended to handle -algebras so that the ticket mentions for all  whether the outcome represented by the ticket belongs to . Now, say that someone else picks the ticket and we don't see it. For any  we may ask whether the ticket says that the outcome is in  and the person holding the ticket tells us. However, if we ask about some , we hear \"Sorry, you don't know that\".Larger -algebra is more informationThis also explains why moving to   means gaining new information -- now we still get answers to -questions about any and additionally to some new questions -- those where .Random variablesSo, the tickets also contains the values of random variables. If the random variable  is -measurable, we get answers to all our questions about its value, such as is  equal to , since by -measurability of , . Or, to handle the delicacies of the uncountable case, we may also ask Is  in the set ? (Since for any particular value we think about, the probability of hearing \"yes\" may be  and that would be boring). So, in this sense we have all information about the realization of the random variable, if our information is  and the RV is -measurable.Caveat: the definition of measurability of random variables restricts the sets  we may ask about. Is  is answered if  is a measurable set in the value space of the random variable (usually Borel -algebra is assumed with  without mentioning). So, in the uncountable (nondiscrete ) case, don't ask whether  is in the Vitali set or the oracle holding the ticket shall be mad.ReferenceI did not cite any reference in the answer but I consulted J. Jacod and P.E. Protter.  Probability essentials (2nd edition), Springer, 2004 about the definition of measurability of random variables. (And have learned these things from the same book previously, if I recall correctly).","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-20 23:28:44","Question_id":230934}
{"_id":{"$oid":"5837a578a05283111e4d3f1f"},"Last_activity":"2016-08-20 23:53:44","Creator_reputation":25838,"Question_score":4,"Answer_content":"If you decompose the joint distribution of  asp(x_1)p(x_2|x_1)\\cdots p(x_n|x_1,\\ldots,x_{n-1})then each term in the product is a Bernoulli distribution, which probability depends on the earlier realisations of the 's.Hence, if I draw  as a Bernoulli variate with probability two independent Bernoulli variates  and  with probabilities  and four independent Bernoulli variates , ... and  with probabilities , ... and ... independent Bernoulli variates  with probabilities I have a large enough collection of independent Bernoulli variates to reconstitute the 's.","Display_name":"Xi\u0026#39;an","Creater_id":7224,"Start_date":"2016-08-20 23:53:44","Question_id":230930}
{"_id":{"$oid":"5837a578a05283111e4d3f2c"},"Last_activity":"2016-08-20 22:23:14","Creator_reputation":5189,"Question_score":0,"Answer_content":"With these desiderata (mention the name of the distribution (Blah), give an explicit form of the pdf, do not waste space with ), I would use the density function notation style discussed in this question Meaning of syntax  (multivariate normal distribution), so \\begin{equation}p(x ; \\alpha, \\beta ) = \\mathrm{Blah}(x;\\alpha,\\beta) = \\beta\\,I\\,\\alpha\\,h\\times\\ldots\\end{equation}where the first equation tells the brand name of the distribution and the second specifies the explicit functional form. However, no guarantees that your readers understand this notation. It would be better to simply check what kind of notations are used in your field. ","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-20 22:23:14","Question_id":230923}
{"_id":{"$oid":"5837a578a05283111e4d3f3b"},"Last_activity":"2016-08-20 21:00:45","Creator_reputation":152503,"Question_score":0,"Answer_content":"  I am interested in differences in both medians and dispersionThe Kruskal-Wallis test doesn't test for differences in medians without one  additional assumption or another.  this site states that the test assumes that the groups have the same shape and variabilityNo it doesn't say that (and a good thing too, or it would be wrong if it did).It says something slightly different:   *In order to know how to interpret the results ... * This is not the same as saying that the test assumes it. Here's the actual situation: If you assume that the only possible differences are location shifts then a difference in location by the measure the Kruskal-Wallis picks up is equally a difference in any measure of location (as long as it exists) -- it's just as much a difference in medians or of means, for example. That's a nice situation to be in, but it's not a requirement.And a good thing, too, otherwise you could pretty much never use it on data like Likert-scale items. Consider that you have one group distributed across all options. If the only possible alternatives are shifts and the only possible values taken by a Likert-type item are categorical, everything would have to shift up or down by a category. If you already take up the width of the scale, that's impossible. There are differences that don't just correspond to location shifts that are perfectly easy to interpret. For example, imagine a variable like \"time to complete a task\". Changes in that might readily be multiplicative rather than additive (a task might take 10% more time or 5% less time, as conditions change for example - this is a stretching or compression). If present, such changes of scale rather than of location would be detected by a Kruskal-Wallis and are no harder to interpret than location shifts.One problem you can have with a Kruskal-Wallis is it's possible for there to be pairwise differences that lead to a cycle: A tends to be larger than B (specifically , B larger than C and C larger than A. Now that situation is tricky to interpret. (However, if you're interested in any differences, this additional difficulty shouldn't bother you in the least, because it still indicates a difference in distributions.)By assuming that the only differences are location shifts, you can certainly avoid this problem ... if the differences are all shifts of the whole distribution up or down, there can be no cycle. But that's much too restrictive a restriction if that's what they're trying to avoid.  mean that Kruskal-Wallis isn't capturing differences in dispersion? It can detect differences in dispersion that change the probability that a value from one of the variables exceeds a value from another -- such as the scale-shifts I mentioned before, for example. But it doesn't detect changes in dispersion that leaves that probability at .  Only medians?As I said above it's not actually a test of medians, but it does detect location shifts. Note that the scale shift I mentioned would also lead to a difference in medians (as well as a difference in dispersion).--Note that you say this:  medians and dispersion - basically any difference in the distributions. No, changes to location and dispersion don't account for all possible changes - its trivial to find differences that are not changes to location and spread for typical measures of each.If you really mean you want to test for any differences, even those that might not be location and dispersion, there are k-sample versions of the Komogorov-Smirnov test and the Cramer-von Mises test that would be suitable. (Or you could do a chi-square, for example, but the others will be more powerful against the sort of differences it sounds like you might like to look for)On the other hand, if you mean that differences other than location and dispersion are of interest, there are nonparametric tests that are sensitive to both shifts at once, such as Rosenbaum's test in the two-sample case (which can pick up when there's an increase in location accompanied by increase in spread -- but the Kruskal-Wallis should manage that particular alternative pretty well in any case).--If neither the Kruskal Wallis not the k-sample Kolmogorov-Smirnov appeal, I'd suggest considering a different strategy, which is based on partitioning the chi-square I mentioned before, into orthogonal components (linear, quadratic, cubic and quartic). The first two components would be the ones to test if you're interested mainly in location and scale differences. ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-20 21:00:45","Question_id":228711}
{"_id":{"$oid":"5837a578a05283111e4d3f48"},"Last_activity":"2016-08-20 21:00:17","Creator_reputation":8337,"Question_score":0,"Answer_content":"Larry Wasserman's All of Statistics is a nice book for getting a whirlwind tour of mathematical statistics. It was the first book on mathematical statistics I used myself. It includes the classics like hypothesis testing and maximum likelihood estimation, but it also has plenty of coverage of more recently developed but equally important topics like bootstrapping. Wasserman always has one foot in statistics and the other foot in machine learning, which I think all contemporary data analysts should do; if you're only familiar with one field of the two, you're going to be missing a lot. Also, the book has a lot of good exercises.If you have a background in real analysis and you want the raw, uncut stuff, by which I mean a measure-theoretic treatment of probability and statistics, try Mark J. Schervish's Theory of Statistics. Schervish is half of DeGroot and Schervish, whose less technical book Probability and Statistics is maybe the most popular book on mathematical statistics today. Theory of Statistics is a helpfully talky book for a topic usually reserved for graduate students who are supposed to do all the work themselves. To be quite honest, I found this book very hard (although not as hard as Jun Shao's Mathematical Statistics) and eventually came to feel the immense effort required to master it wasn't a good use of my time as an applied data analyst. But I still learned a lot and came away with a good understanding of what measure theory is and how it can be used to clean up hairy theoretical difficulties that arise in the more naive traditional approach to probability theory. I also came to better appreciate the similarities and differences of exchangeability and independence.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-20 21:00:17","Question_id":230904}
{"_id":{"$oid":"5837a578a05283111e4d3f57"},"Last_activity":"2016-08-20 19:48:43","Creator_reputation":3622,"Question_score":0,"Answer_content":"You might want to read Guerrero, V.M. (1993) Time-series analysis supported by power transformations. Journal of Forecasting, 12, 37–48.The above paper is the only paper that I know of have developed automatic technique to determine  box cox transformation parameter lambda and importantly this procedure is model independent. This is done by minimizing coeficient of variation of time series. This is implemented in the forecast package in R software. See example below:library('forecast')lambda \u0026lt;- BoxCox.lambda(AirPassengers,lower=0)There are better ways to test stationarities. Box cox transformation is used for stabilizing variance not to check stationarity. You might want to use specific procedures such as Ljung-Box test, augmented Dickey-Fuller test, ACF, PACF and others to check stationarity. See this website for a nice summary of methods and how to apply in .","Display_name":"forecaster","Creater_id":29137,"Start_date":"2016-08-20 14:19:01","Question_id":230883}
{"_id":{"$oid":"5837a578a05283111e4d3f6c"},"Last_activity":"2016-08-20 15:43:36","Creator_reputation":3680,"Question_score":1,"Answer_content":"M5P sets the split point mid-way between the two neighboring numeric values. So age \u0026lt;= 55.5 means that observations with age up to 55 assigned to one node and starting from 56 to the other node. For categorical variables, dummy variables like region=1,3,4 are constructed that can either be 0 or 1. Hence, M5P sets the split point at 0.5. One could argue that region != 1,3,4 might have been more intelligible than region=1,3,4 \u0026lt;= 0.5. See also this discussion on the Weka mailing list: http://weka.8497.n7.nabble.com/Categorical-Variables-in-M5Rules-or-M5P-td17589.html.The first number is simply the number of observations (aka instances) in that node. The second number gives the percentage of the RMSE in that node in relation to the RMSE of an intercept-only model on the entire learning data. (The latter RMSE is simply the standard deviation of the response/target variable on the full data.) See also this discussion on the Weka mailing list: http://weka.8497.n7.nabble.com/in-Regression-Tree-leafs-td36442.html#a36449.The models are not nested because they pertain to different subsamples of the data set. Hence, the ratio of RMSEs can be larger than 100%.","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2016-08-20 15:43:36","Question_id":228724}
{"_id":{"$oid":"5837a578a05283111e4d3f78"},"Last_activity":"2016-08-20 15:02:23","Creator_reputation":620,"Question_score":1,"Answer_content":"One way to think about the Causal Markov Condition (CMC) is giving a rule for \"screening off\": once you know the values of 's parents, all other variables in  become irrelevant for predicting , except for 's descendants. I find examples make the CMC easiest to understand. I did a quick google image search for \"mechanism of cardiovascular disease\" so I can give you a medical example. Take this graph (let's call it ):Say you have a probability distribution  over the variables in . If the CMC holds in  (relative to ), then you can infer that:If I know the patient's amount of Oxidative stress and inflammation, then learning the patient's degree of Plaque progression won't give me any extra information about the patient's Platelets.If I know the patient's amount of Atheroma, then learning the amount of Oxidative stress and inflammation won't help me predict Plaque progression.However, the CMC allows the following possibilities:If I know the patient's amount of Atheroma, then learning the patient's degree of Plaque rupture might still tell me something more about Plaque progression. (I'm learning from a descendant variable.)If I don't know the patient's amount of Oxidative stress and inflammation, then learning the patient's degree of Plaque progression might well give me some extra information that helps me predict the patient's Platelets. (I'm not conditioning on the parents.)","Display_name":"Lizzie Silver","Creater_id":57345,"Start_date":"2016-08-20 14:56:17","Question_id":230897}
{"_id":{"$oid":"5837a578a05283111e4d3f85"},"Last_activity":"2016-08-20 14:53:08","Creator_reputation":4307,"Question_score":3,"Answer_content":"You are correct in your understanding. However, I can try to give some justification of why this term is used the way it is.The \"population\"  is an abstract concept, represented by a hypothetical distribution  which in principle assigns some probability to each numeric value  that the random variable  might take on. This population distribution is essentially never known. But we can try to learn about this distribution from measurements  which constitute a \"sample\" of the population.Of course any statistic we compute from a sample, , (e.g. ) is itself a random variable. The important point is that the \"population\" distribution of  is now a function of the population distribution  but also depends on the sampling scheme. For example  will change if we have a different sample size , or if we have a biased sampling scheme (e.g. due to detection tolerance).If we just knew that  is some random variable, then we might refer to its distribution as a \"population\" distribution (or just \"distribution\", more casually). But if we know that  is actually some statistic computed from a sample of , then we would refer to its distribution as a \"sampling\" distribution.Hopefully this was not more confusing!","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-20 14:53:08","Question_id":230799}
{"_id":{"$oid":"5837a578a05283111e4d3f94"},"Last_activity":"2015-03-18 12:36:34","Creator_reputation":559,"Question_score":0,"Answer_content":"The performanceEstimation package might be helpful. I have not used it yet, but I read that it deals with regression, classification, time series problems. The models can be ranked according to several metrics. Here is the CRAN link as well as a companion paper.","Display_name":"tiagotvv","Creater_id":69634,"Start_date":"2015-03-18 12:36:34","Question_id":142318}
{"_id":{"$oid":"5837a578a05283111e4d3fa3"},"Last_activity":"2016-08-20 13:27:13","Creator_reputation":5189,"Question_score":4,"Answer_content":" is Poisson!Your marginalization, or at least the end result, is correct. The form you have obtained for the distribution is the probability mass function of a Poisson distribution -- just write   as  and behold. That is,\\begin{equation}y \\mid p, \\lambda \\sim \\mathrm{Poisson}(\\lambda\\,p).\\end{equation}You have essentially rediscovered the fact that a Poisson process thinned randomly (so that every point is selected with probability  independent of others) is Poisson. This is a well-known result, a quick Google turned up [1] but I suppose this is in many textbooks. Your situation is analogous to this, since the Poisson distributed random variable  can be interpreted as the number of arrivals in a Poisson process with intensity  in a unit interval. Conditional on  the thinning operation selects each of the  points independently with probability , which is a binomial trial. So, the marginalization could have been 'derived' without any algebraic manipulations by knowing the thinning-of-a-Poisson-process result and realizing how it applies here.Note about Stan implementationNote that this also means you do not have to write your own function in Stan since this is simplyy ~ poisson(lambda .* p).Negative-binomial caseThis extends to the NegBin-case (mentioned in comments), too, since a negative binomial can be represented as a mixture of Poissons where the parameter has a Gamma distribution. Conditional on the gamma-distributed parameter,  is Poisson, too. And if  is gamma-distributed,  is too (fixed ), so when marginalizing over the gamma-distributed parameter,  is negative-binomial. The  negative-binomial has multiple parametrizations -- depending on the parametrization one has to work out how multiplying the Poisson rates by  translates into the parameters of the negative-binomial. Left as an exercise to the reader.Reference[1] http://www.math.uah.edu/stat/poisson/Splitting.html -- Random (formerly Virtual Laboratories in Probability and Statistics), Section 13.5","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-20 13:27:13","Question_id":230789}
{"_id":{"$oid":"5837a578a05283111e4d3fb0"},"Last_activity":"2016-08-20 13:13:56","Creator_reputation":12308,"Question_score":3,"Answer_content":"  Why are the variance and Std.Dev of the random effects zero?Because the marginal variance among sites in your data is less than would be expected from a binomial variable; the variance can't be negative, so it's estimated as zero. The GLMM FAQ discusses this.   How do I check for overdispersion in this model?This looks like a binary (not just binomial) regression, i.e. your responses are 0/1 (if you had \"m out of N\" responses where N\u0026gt;1, you either need a two-column response variable of (successes,failures), or you need to specify the weights argument). Therefore, overdispersion is not identifiable (e.g., see here or here). tl;dr, you don't need to worry about it.  If you did have N\u0026gt;1, the previously linked GLMM FAQ gives some guidance ... the overdisp_fun from there can be used; depending on your philosophy of model-building you can use a hypothesis test (e.g. ?) or a rule of thumb (e.g. overdispersion factor \u003e 1.1) to decide whether you should worry about it.  What should do if there is overdispersion?See previous answer (i.e., don't worry about if you have Bernoulli responses; otherwise, see the FAQ or elsewhere for strategies).  What do I do if the data are unbalanced? (asked in comments)Unless you have data that are severely unbalanced, or unless the data are structured that you have complete separation (all zeros or all ones for some combinations of predictor variables), GLMMs will handle unbalanced data fine; there's no need for manual adjustment.","Display_name":"Ben Bolker","Creater_id":2126,"Start_date":"2016-08-19 09:01:00","Question_id":230721}
{"_id":{"$oid":"5837a578a05283111e4d3fbd"},"Last_activity":"2016-08-17 18:04:35","Creator_reputation":197,"Question_score":1,"Answer_content":"This question needs more clarifications. If you mean that you have a random variable that can take the values from 0 to 4, then it's clearly a discrete variable.","Display_name":"Toney Shields","Creater_id":109437,"Start_date":"2016-08-17 18:04:35","Question_id":230391}
{"_id":{"$oid":"5837a578a05283111e4d3fcc"},"Last_activity":"2014-04-01 03:52:31","Creator_reputation":118,"Question_score":1,"Answer_content":"If you open the source file of LogisticRegression, you will see the loss function being defined:loss = -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])which minimizes the negative log-likelihood (but he uses the mean instead of the sum). In order to change to L2 loss function, you would need to write something like:loss = T.sum(param ** 2)in which param would be the error in your regression.","Display_name":"Sam Felix","Creater_id":36306,"Start_date":"2014-04-01 03:52:31","Question_id":83107}
{"_id":{"$oid":"5837a578a05283111e4d3fcd"},"Last_activity":"2014-02-18 07:08:30","Creator_reputation":101,"Question_score":0,"Answer_content":"For regression, the cost function is the euclidean distance between the output and the ground truth. If there is a L2 distance or euclidean distance in the package, you probably want to use that.","Display_name":"Min Lin","Creater_id":24875,"Start_date":"2014-02-18 07:08:30","Question_id":83107}
{"_id":{"$oid":"5837a578a05283111e4d3fce"},"Last_activity":"2014-01-28 05:48:22","Creator_reputation":101,"Question_score":1,"Answer_content":"I think you should check Dr. LeCun's work, which discusses several loss functions. It uses RBF's units in the output, so I'm not sure it's exactly what you need. As for me it's quite hard to implement it with Theano, so I use softmax and it works good for my tasks. ","Display_name":"user2575760","Creater_id":35873,"Start_date":"2014-01-28 05:48:22","Question_id":83107}
{"_id":{"$oid":"5837a578a05283111e4d3fdf"},"Last_activity":"2015-06-11 01:36:42","Creator_reputation":862,"Question_score":1,"Answer_content":"Convolution with a kernel is done on all input maps and their summation is taken. In the input layer it is obvious since there is only one feature (input map). However, after first convolution, the later comvolutions are summuation of kernel operation on all feature maps. Hence, instead of 48 output feature map at C2, there should be 8 maps. this link explains the network and its back-prop in a clear way.Use  as activation(transform) function. After successful implementation, you can use the others too. You should use the same function for both 'feedforward' and 'back-prop'.I haven't read the paper, but breaking symmetry is about selecting weight from a random distribution. If the weights are the same on feature maps, back propagated error will be the same. As a result network learns the same filters which is not desirable.Rule of connections are already defined as mathematical expressions. The number of kernels, number of layers, kernelsize etc. should be defined symbolically and they sould be assigned in main section of the code.You should add bias before applying activation function. A single bias, most commonly used, is added to feature map. Summing a scalar with a matrix is simply adding the scalar at each indexes of the matrix. If you didn't write a code for NN before, It would be better to start with it.","Display_name":"yasin.yazici","Creater_id":29313,"Start_date":"2015-06-11 01:26:06","Question_id":156445}
{"_id":{"$oid":"5837a578a05283111e4d3fec"},"Last_activity":"2014-11-30 21:21:53","Creator_reputation":681,"Question_score":1,"Answer_content":"When you are trying to recognize objects, edges, etc., the algorithms generally look for deviation from nearby or summed/averaged elements.  The deviation could be negative or positive, but in most cases it is a \"recognition\" regardless of the polarity.  The algorithms impart polarity due to the mathematics, but the interpretation doesn't care if it is a deviation up or down.  However, some filters and algorithms do carry information in the polarity.  If you combine one where polarity matters with one where polarity does not, then you want to be sure you are adding a \"recognition\" properly, so the filter is augmented appropriately and not inappropriately pushed more negative or more positive.","Display_name":"wwwslinger","Creater_id":30202,"Start_date":"2014-11-30 21:21:53","Question_id":126097}
{"_id":{"$oid":"5837a578a05283111e4d3ff9"},"Last_activity":"2015-03-06 13:48:01","Creator_reputation":1398,"Question_score":4,"Answer_content":"In case of CNN filters are applied to small patches of an image at each possible location (which also makes them translation invariant).Autoencoder's hidden layers get whole image (output of the previous layer) as their input, which doesn't look like a good idea for images: usually only spatially local features correlate, whereas more distant ones are less correlated. Also, these hidden neurons are not translation invariant.Thus, CNNs are like usual ANNs with a special kind of regularization, which zeros out most of weights to make use of locality.  ","Display_name":"Artem Sobolev","Creater_id":62549,"Start_date":"2015-03-06 13:48:01","Question_id":140698}
{"_id":{"$oid":"5837a578a05283111e4d4006"},"Last_activity":"2015-12-06 00:05:03","Creator_reputation":11,"Question_score":1,"Answer_content":"I have also been searching for fully explained model of Stacked Convolutional Autoencoders.I came across three different architectures. I am still studying them and I thought these might help others who are also starting to explore CAEs. Any further references to papers or implementations would greatly help.  The one mentioned by you using pooling - unpooling.The layers of (convolve)__x_times -\u003e (deconvolve)__x_times,and get the same size as input.(convolve -\u003e pool)__x_times -\u003e (strided deconvolution)__y_timesthe padding and strides are selected such that the final image size is same as original image.Reference","Display_name":"Ankitp94","Creater_id":76915,"Start_date":"2015-12-06 00:05:03","Question_id":137537}
{"_id":{"$oid":"5837a578a05283111e4d4007"},"Last_activity":"2015-04-22 21:45:56","Creator_reputation":95,"Question_score":7,"Answer_content":"I am currently exploring stacked-convolutional autoencoders.I will try and answer some of your questions to the best of my knowledge. Mind you, I might be wrong so take it with a grain of salt.Yes, you have to \"reverse\" pool and then convolve with a set of filters to recover your output image. A standard neural network (considering MNIST data as input, i.e. 28x28 input dimensions) would be:    28x28(input) -- convolve with 5 filters, each filter 5x5 --\u0026gt;  5 @ 28 x 28 maps -- maxPooling --\u0026gt; 5 @ 14 x 14 (Hidden layer) -- reverse-maxPool --\u0026gt; 5 @ 28 x 28 -- convolve with 5 filters, each filter 5x5 --\u0026gt; 28x28 (output)My understanding is that conventionally that is what one should do, i.e. train each layer separately. After that you stack the layers and train the entire network once more using the pre-trained weights. However, Yohsua Bengio has some research (the reference escapes my memory) showcasing that one could construct a fully-stacked network and train from scratch.My understanding is that \"noise layer\" is there to introduce robustness/variability in the input so that the training does not overfit. As long as you are still \"training\" pre-training or fine-tuning, I think the  reconstruction part (i.e. reversePooling, de-convolution etc) is necesary. Otherwise how should one perform error-back-propagation to tune weights?I have tried browsing through numerous papers, but the architecture is never explained in full. If you find any please do let me know.  ","Display_name":"user2979010","Creater_id":74318,"Start_date":"2015-04-22 21:45:56","Question_id":137537}
{"_id":{"$oid":"5837a578a05283111e4d4014"},"Last_activity":"2016-08-20 13:05:54","Creator_reputation":12907,"Question_score":0,"Answer_content":"It depends on what you want to learn from the test result.If you wonder whether one forecast (say, ) is statistically more accurate than another (say, ), the Diebold-Mariano (DM) test will tell you that. At this point there is no talk of the models that generated the forecasts.The DM test (as any other statistical test) targets making inference on the population rather than the current sample. If (1) the DM test tells you with 95% confidence that  beats  and (2) the forecast generating processes and the data generating process all remain unchanged in the future, then you would expect that  will beat  also in the future.How can you benefit from this result? If you have the forecast generating process available, you could choose to use the one for  rather than for . However, Diebold (2015) does not encourage that:  The Diebold-Mariano (DM) test was intended for comparing forecasts; it has  been, and remains, useful in that regard. The DM test was not intended for comparing models. Much of the large ensuing literature, however, uses DM-type tests for comparing models, in pseudo-out-of-sample environments. In that case, simpler yet more compelling full-sample model comparison procedures exist; they have been, and should continue to be, widely used.If you wonder which of the alternative models is more likely to have generated the data, using the DM test will be problematic in case of nested models, as explained in Clark \u0026amp; McCracken (2001). (Once again, Diebold did not intend the test to be used for comparing models -- see the quote above.)How bad does the DM fail in this sense? There are simulation results reported in the tables of Clark \u0026amp; McCracken (2001), go check them.References:Clark, Todd E., and Michael W. McCracken. \"Tests of equal forecast accuracy and encompassing for nested models.\" Journal of Econometrics 105.1 (2001): 85-110.Free version here.Diebold, Francis X. \"Comparing predictive accuracy, twenty years later: A personal perspective on the use and abuse of Diebold–Mariano tests.\" Journal of Business \u0026amp; Economic Statistics 33.1 (2015): 1-9.Free version here.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-20 01:34:02","Question_id":230566}
{"_id":{"$oid":"5837a578a05283111e4d4021"},"Last_activity":"2016-08-20 13:04:40","Creator_reputation":16,"Question_score":1,"Answer_content":"When I run Pini’s code, Lweights. So the weights and their interpretation appear to be the same. Predictions are also practically the same:max(abs(NLfitted()-Lmresiduals*L$weights^0.5)) \u0026lt; 2e-14.In stats:::nlsModel.plinear, line 69 reads: dev \u0026lt;- sum(resid^2). This makes sense if resid are weighted. But stats:::logLik.nls, line 12 contains the termlog(sum(w*res^2)). The same term appears in the exact same way in stats:::logLik.lm. As a result, the code in Pini’s example, using w^2 in the case of nls, does reproduce the values of logLik, for both lm and nls. In addition, the difference between the logLik values of lm and nls does depend on the data. For example with set.seed(1) I get L_Log_Lik1-NL_Log_Lik1 = 5801.164; while with set.seed(2) I get L_Log_Lik1-NL_Log_Lik1 = 5800.496.This is a confusing situation, and quite possibly we are missing something. Clarifications will be very highly appreciated.","Display_name":"Hillel Bar-Gera","Creater_id":85498,"Start_date":"2016-08-19 06:41:30","Question_id":227180}
{"_id":{"$oid":"5837a579a05283111e4d402e"},"Last_activity":"2016-08-20 12:52:22","Creator_reputation":197,"Question_score":1,"Answer_content":"This is really a lazy question, you can get your answer easily by doing a simple search on google. Anyway, the Exponetial distribution is related to a lot of other distributions: https://en.wikipedia.org/wiki/Exponential_distribution#Related_distributions","Display_name":"Toney Shields","Creater_id":109437,"Start_date":"2016-08-20 12:52:22","Question_id":230895}
{"_id":{"$oid":"5837a579a05283111e4d403b"},"Last_activity":"2016-08-20 12:51:25","Creator_reputation":19535,"Question_score":1,"Answer_content":"Always use the adjusted rand index. There is no reason to use the non-adjusted version.Assuming you have a data set of 100 objects. 90 are type A. 10 are type B in the first clustering. For the second clustering, pick 90 random objects, and label them A, and the remaining 10 B. A typical confusion matrix will look like this:81 1919 1and have a Rand index of somewhere around 0.95 - this looks pretty good. But the labels were given randomly, it must not be good! The adjusted rand index of this solution should be close to 0.Thus:A high Rand index may be due to label distribution. A value of 0.95 can still be random!Adjusted rand values near 0 do indicate random results; values less than 0 even worse-than-guessing.Always prefer adjusted Rand to regular Rand index!In the example of your question, the clusterings are as similar as random labels.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-08-20 12:51:25","Question_id":230838}
{"_id":{"$oid":"5837a579a05283111e4d4048"},"Last_activity":"2016-08-20 12:45:14","Creator_reputation":166,"Question_score":2,"Answer_content":"I asked the same question on Quora Here's the answer    Allocation of funding for different departments of an organization  Picking best performing athletes out of a group of students given limited time and an arbitrary selection threshold  Maximizing website earnings while simultaneously testing new features (in lieu of A/B testing)  You can use them anytime you need to optimize results when you don't have enough data to create a rigorous statistical model.  ","Display_name":"Andy K","Creater_id":27900,"Start_date":"2016-08-20 11:41:57","Question_id":230523}
{"_id":{"$oid":"5837a579a05283111e4d4049"},"Last_activity":"2016-08-18 08:59:02","Creator_reputation":67,"Question_score":8,"Answer_content":"When you play the original Pokemon games (Red or Blue and Yellow) and you get to Celadon city, the Team rocket slot machines have different odds.  Multi-Arm Bandit right there if you want to optimize getting that Porygon really fast.  In all seriousness, people talk about the problem with choosing tuning variables in machine learning.  Especially if you have a lot of of variables, exploration vs exploitation gets talked about.  See like Spearmint or even the new paper in this topic that uses a super simple algorithm to choose tuning parameters (and way outperforms other tuning variable techniques)   ","Display_name":"www3","Creater_id":93111,"Start_date":"2016-08-18 08:59:02","Question_id":230523}
{"_id":{"$oid":"5837a579a05283111e4d404a"},"Last_activity":"2016-08-18 08:35:02","Creator_reputation":25275,"Question_score":6,"Answer_content":"They are used in A/B testing of online advertising, where different ads are displayed to different users and based on the outcomes decisions are made about what ads to show in the future. This is described in nice paper by Google researcher Steven L. Scott.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-18 08:32:19","Question_id":230523}
{"_id":{"$oid":"5837a579a05283111e4d404b"},"Last_activity":"2016-08-18 08:29:24","Creator_reputation":75835,"Question_score":6,"Answer_content":"They can be used in a biomedical treatment / research design setting.  For example, I believe q-learning algorithms are used in Sequential, Multiple Assignment, Randomized Trial (SMART trials).  Loosely, the idea is that the treatment regime adapts optimally to the progress the patient is making.  It is clear how this might be best for an individual patient, but it can also be more efficient in randomized clinical trials.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-08-18 08:29:24","Question_id":230523}
{"_id":{"$oid":"5837a579a05283111e4d4058"},"Last_activity":"2016-08-20 12:32:11","Creator_reputation":3327,"Question_score":1,"Answer_content":"First note that \"and\" refers to intersection \"\" and \"or\" refers to union \"\".Intersection distributions over union (similar to how multiplication distributes over addition). i.e. A \\cap (B \\cup C) = (A\\cap B) \\cup (A \\cap C)So in your scenario we have \\begin{align*}(A\\cap B) \\cap (A \\cup B) \u0026amp;=  ((A\\cap B) \\cap A ) \\cup ((A\\cap B) \\cap B)\\\\\u0026amp;= (A\\cap B) \\cup (A\\cap B)\\\\\u0026amp;= (A\\cap B)\\end{align*}Note you have a typo in your original statement:  is actually .It should be  (you had the number right). ","Display_name":"bdeonovic","Creater_id":17661,"Start_date":"2016-08-20 12:32:11","Question_id":230884}
{"_id":{"$oid":"5837a579a05283111e4d4067"},"Last_activity":"2016-08-20 11:17:32","Creator_reputation":1024,"Question_score":2,"Answer_content":" stands for expectation. It required some knowledge about the pdf of your random variable. Without the pdf, you can compute an empirical estimate with a sum on your data. This can be related to the idea of ergodic processes: can you deduce the statistical properties from a sample of the process?You might want to be careful with your notations. Very often,  denotes an estimate (eg based on sums) of the quantity . You can check for instance: Derive Variance of regression coefficient in simple linear regression.","Display_name":"Laurent Duval","Creater_id":83945,"Start_date":"2016-08-20 10:33:59","Question_id":230879}
{"_id":{"$oid":"5837a579a05283111e4d4068"},"Last_activity":"2016-08-20 11:12:07","Creator_reputation":76,"Question_score":2,"Answer_content":"I assume you're talking about the variance of the regression coefficient estimates (because the variance of the \"true\" coefficient is zero). What you've written is not the variance of . The correct formula is  E \\Big( (\\hat \\beta - E(\\hat \\beta))^2 \\Big) what you wrote is closer to the mean squared error of :  E \\Big( (\\beta - \\hat \\beta)^2 ) \\Big)  but not quite. ","Display_name":"not_bonferroni","Creater_id":117710,"Start_date":"2016-08-20 11:03:47","Question_id":230879}
{"_id":{"$oid":"5837a579a05283111e4d4075"},"Last_activity":"2016-08-20 11:07:58","Creator_reputation":152503,"Question_score":9,"Answer_content":"You can test equality of the mean parameters against the alternative that the mean parameters are unequal with a likelihood ratio test. (However, if the mean parameters do differ and the distribution is exponential, this is a scale shift, not a location shift.)I believe that (but haven't checked) the LR test comes out to be equivalent to the following:Let's say we parameterize the th observation in the first exponential as having pdf  and the th observation in the second sample as having pdf  (over the obvious domains for the observations and parameters).(To be clear, we're working in the mean-form not the rate-form here; this won't affect the outcome of the calculations.)Since the distribution of  is a special case of the gamma, , the distribution of the sum of 's,  is distributed ; similarly that for the sum of the s,  is . Because of the relationship between gamma distributions and chi-squared distributions, it turns out that  is distributed . The ratio of two chi-squares on their degrees of freedom is F. Hence the ratio, .Under the null hypothesis of equality of means, then, , and under the two sided alternative, the values might tend to be either smaller or larger than a value from the null distribution, so you need a two-tailed test.[To check that this was in fact the same as the LR test in small samples one would need to show the LR statistic was monotonic in .]Simulation to check that we didn't make some simple mistake in the algebra:Here I simulated 1000 samples of size 30 for  and 20 for  from an exponential distribution with the same mean, and computed the above ratio-of-means statistic. Below is a histogram of the resulting distribution as well as a curve showing the  distribution we computed under the null:Example, with discussion of computation of two-tailed p-values:To illustrate the calculation, here's two small samples from exponential distributions. The X-sample has 14 observations from a population with mean 10, the Y-sample has 17 observations from a population with mean 15:x: 12.173  3.148 33.873  0.160  3.054 11.579 13.491  7.048 48.836    16.478  3.323  3.520  7.113  5.358y:  7.635  1.508 29.987 13.636  8.709 13.132 12.141  5.280 23.447    18.687 13.055 47.747  0.334  7.745 26.287 34.390  9.596The sample means are 12.082 and 16.077 respectively. The ratio of means is 0.7515The area to the left is straightforward, since it's in the lower tail (calc in R): \u0026gt; pf(r,28,34)  [1] 0.2210767We need the probability for the other tail. If the distribution wassymmetric in the inverse, it would be straightforward to do this.A common convention with the ratio of variances F-test (which is similarly two tailed) is simply to double the one-tailed p-value (effectively what is going on as here; that's also what seems to be done in R, for example); in this case it gives a p-value of 0.44.However, if you do it with a formal rejection rule, by putting an area of  in each tail, you'd get critical values as described here. The p-value is then the largest  that would lead to rejection, which is equivalent toadding the one tailed p-value above to the one-tailed p-value in the other tail for the degrees of freedom interchanged. In the above example that gives a p-value of 0.43.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2013-11-15 16:23:25","Question_id":76689}
{"_id":{"$oid":"5837a579a05283111e4d4076"},"Last_activity":"2016-03-29 07:42:20","Creator_reputation":18711,"Question_score":1,"Answer_content":"As an addendum to @Glen_b's answer, the likelihood ratio isn_x\\log \\frac{n_x}{\\sum x_i} +n_y \\log \\frac{n_y}{\\sum y_j} -(n_x+n_y)\\log\\frac{n_x+n_y}{\\sum x_i +\\sum y_j}which you can re\u0026auml;rrange ton_x\\log\\left(\\frac{n_x}{n_y} + \\frac{1}{r}\\right) + n_y\\log\\left(\\frac{n_y}{n_x}+r\\right) + n_x\\log\\frac{n_y}{n_x+n_y} + n_y\\log \\frac{n_x}{n_x+n_y}where . There's a single minimum at , so the F-test is indeed the likelihood ratio test against one-sided alternatives to the null hypothesis of identical distributions.To perform the likelihood-ratio test proper for a two-sided alternative you can still use the F-distribution; you simply need to find the other value of the ratio of sample means  for which the likelihood ratio is equal to that of the observed ratio , \u0026amp; then . For this example , \u0026amp; , giving an overall p-value of , (rather close to that obtained by the chi-square approximation to the distribution of twice the log likelihood ratio, ).But doubling the one-tailed p-value is perhaps the most common way to obtain a two-tailed p-value: it's equivalent to finding the value of the ratio of sample means  for which the tail probability  is equal to , \u0026amp; then finding . Explained like that, it might seem to be putting the cart before the horse in letting tail probabilities define the extremeness of a test statistic, but it can be justified as being in effect two one-tailed tests (each the LRT) with a multiple comparisons correction\u0026mdash;\u0026amp; people are usually interested in claiming either that  or that  rather than that either  or . It's also less fuss, \u0026amp; even for fairly small sample sizes, gives much the same answer as the two-tailed LRT proper.R code follows:x \u0026lt;- c(12.173, 3.148, 33.873, 0.160, 3.054, 11.579, 13.491, 7.048, 48.836,       16.478, 3.323, 3.520, 7.113, 5.358)y \u0026lt;- c(7.635, 1.508, 29.987, 13.636, 8.709, 13.132, 12.141, 5.280, 23.447,        18.687, 13.055, 47.747, 0.334,7.745, 26.287, 34.390, 9.596)# observed ratio of sample meansr.obs \u0026lt;- mean(x)/mean(y)# sample sizesn.x \u0026lt;- length(x)n.y \u0026lt;- length(y)# define log likelihood ratio functioncalc.llr \u0026lt;- function(r,n.x,n.y){  n.x * log(n.x/n.y + 1/r) + n.y*log(n.y/n.x + r) + n.x*log(n.y/(n.x+n.y)) + n.y*log(n.x/(n.x+n.y))}# observed log likelihood ratiocalc.llr(r.obs,n.x, n.y) -\u0026gt; llr.obs# p-value in lower tailpf(r.obs,2*n.x,2*n.y) -\u0026gt; p.lo# find the other ratio of sample means giving an LLR equal to that observeduniroot(function(x) calc.llr(x,n.x,n.y)-llr.obs, lower=1.2, upper=1.4, tol=1e-6)$root -\u0026gt; r.hi#p.value in upper tailp.hi \u0026lt;- 1-pf(r.hi,2*n.x,2*n.y)# overall p.valuep.value \u0026lt;- p.lo + p.hi#approximate p.value1-pchisq(2*llr.obs, 1)","Display_name":"Scortchi","Creater_id":17230,"Start_date":"2016-03-25 11:16:41","Question_id":76689}
{"_id":{"$oid":"5837a579a05283111e4d4083"},"Last_activity":"2016-07-14 04:22:53","Creator_reputation":20442,"Question_score":1,"Answer_content":"You can quantify the difference between probability distributions using the Earth Mover's distance (EMD).As for testing... assuming that you want to do classical null hypothesis significance testing, you could of course use the EMD as your test statistic. To get its distribution under the null hypothesis, you could simulate your experiments actually measuring the same thing, run your two separate MCMCs, and record the resulting EMD. Do this many times, and you have your EMD null distribution. Finally, compare your \"true\" EMD to this one.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2016-07-14 04:22:53","Question_id":223750}
{"_id":{"$oid":"5837a579a05283111e4d4092"},"Last_activity":"2016-08-20 10:33:38","Creator_reputation":4307,"Question_score":1,"Answer_content":"For a bivariate normal PDF, you can visualize the shape in terms of the covariance ellipse. If  and  are uncorrelated and have equal variances, this will be a circle (centered around the 2D mean of the PDF). As you increase the correlation the ellipse will become more anisotropic, in the limit converging to a line segment with orientation +45 degrees and \"width\" 0. (The principle axes of the ellipse correspond to principle components.)The resulting infinitely high linear ridge could be considered a PDF or not, depending on the definitions used.For example, as noted on Wikipedia, the conditional PDF of  given that  will be a normal distribution,where  is the correlation coefficient. So in the perfectly correlated case where  goes to 1, the normal distribution has a zero variance. Is this a \"PDF\"? The limit of a Gaussian as variance goes to zero is in fact one standard way of defining the Dirac delta function. As to whether or not this is a \"valid PDF\", opinions vary.Of course in practical terms, the marginal PDFs for  and  will each just be univariate normal.","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-20 10:24:24","Question_id":230860}
{"_id":{"$oid":"5837a579a05283111e4d409f"},"Last_activity":"2016-05-11 10:11:10","Creator_reputation":7682,"Question_score":1,"Answer_content":"From the beginning Glen_b's comment was a concern, and I suspect there is a misunderstanding on my part, or something possibly missing in the OP. Here is the thing, if I we have to take your question literally, then it is extremely straightforward. You are saying that order doesn't matter, and that you want the number of combinations (not the probability), such that, choosing  times with replacement from  there are  digits that repeat themselves  times each, and the others are different.Well, all you have to do is choose  among the  choices in , which are the numbers that will repeat themselves  times each, for a total of  digits. Since you will be left with  digits to choose from, but you are indicating that the other digits appear only once, and we know that the chosen four appear only  times each, all these remaining  will be exhausted. Hence, they offer no additional choice, and your number of possible combinations is .Or very closely approximated with a simulation:set.seed(0)library(data.table)picks = 0:9 # Digits to choose from.combinations = function(n){  matrix = matrix(rep(0, 18 * n), nrow = n) # Starting an empty matrix. # we run a loop sampling 18 digits in every iteration:    for(i in 1:n){    # n is the number of experiments we will carry out.    lotto = sample(picks, 18, replace = T) # We pick 18 with replacement...    matrix[i,] = lotto # and fill the matrix one row at a time.  }  # Now we want to just keep the rows that have 8 duplicates:  # 4 digits have 2 duplicates each; hence, 4 x 2 = 8:  duplicates=apply(apply(matrix, 1, function(x) duplicated(x)), 2, function(x) sum(x))  matrix = matrix[duplicates==8,]  # We place the rows of the matrix in increasing order:  matrix = unique(t(apply(matrix, 1, function(x) sort(x))))  # And we keep those unique rows that contain 4 groupings of 3 repeat digits:  matrix = unique(matrix[apply(matrix, 1, function(x) sum(tabulate(rleid(x))==3))==4,])  # In a sufficiently large simulation the number of rows   # of the resulting matrix will be the answer to the question:  nrow(matrix)}combinations(1e6)which yields .And just for clarity, this is what two rows of the matrix in the function look like:Although the order may seem to come into play, it is just a coding \"trick\" to get the combinations being asked in the OP, which I interpreted as \"order does not matter.\" In other words, we could just as well now shuffle the order of the elements in each row.","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-05-10 20:54:07","Question_id":211899}
{"_id":{"$oid":"5837a579a05283111e4d40ac"},"Last_activity":"2016-05-17 10:13:04","Creator_reputation":466,"Question_score":0,"Answer_content":"I believe I have an answer. Let's start by considering the number of possible states. There should be  , one state is one choice of element for each set.To get  we would look at how to get there: {2+2+1},{2+1+1+1},{1+1+1+1+1}How many states are there for these? +  +  The probability is thus This method seems laborious, so if someone has something better I would appreciate it.","Display_name":"Kitter Catter","Creater_id":105368,"Start_date":"2016-05-17 10:13:04","Question_id":210140}
{"_id":{"$oid":"5837a579a05283111e4d40bb"},"Last_activity":"2016-08-20 09:50:45","Creator_reputation":21578,"Question_score":1,"Answer_content":"Define probability. I mean it. Before we progress any further, we need to settle on terms.An intuitive definition of probability is a measure of uncertainty. We are uncertain whether the next coin toss will come up heads or tails. That is uncertainty in the data . We are also uncertain whether the coin is fair or not. That is uncertainty about the model ... or you can call uncertainty about the state of the world.To arrive at the conditional distribution , you need to have the joint distribution  -- i.e., the knowledge of the whole population of coins in circulation, how many of them are forged, and how forged coins behave (which may depend on the way the coins are spun and caught in the air).In the particular example of coins, this is at least conceptually possible -- the government figures are available on the coins that are supposed to be fair (28109 per year), or at least those with stable characteristics. As far as forged coins go, the scale of production of less than a million is probably not worth talking about, so  may be a probability that the coin you got from a cashier's register is unfair. Then you need to come up with a model of how the unfair coin works... and obtain the joint distribution, and condition on the data.In the practical world problems with say medical conditions and the way they work, you may not be able to come up with none of these components of the joint distribution, and can't condition.Bayesian modeling provides a way to simplify the models and come up with these joints . But the devil is in the details. If you say that the fair coin is the one with , and then go ahead and specify a traditional Beta prior, and get the Beta conjugate posterior, then... surprise, surprise!  for either of these continuous distributions, no matter if your prior is  or . So you'd have to incorporate a point mass at , give it a prior mass (, say), and see if your data moves the posterior away from that point mass. This is a more complicated calculation that involve Metropolis-Hastings sampling rather than the more traditional Gibbs sampling.Besides the difficulties in talking about what exactly the right models are, Bayesian methods have limited ways of dealing with model misspecification. If you don't like Gaussian errors, or you don't believe in independence of coin tosses (your hand gets tired after the first 10,000 or so tosses, so you don't toss it as high as the first 1,000 or so times, whch may affect the probabilities), all that you can do in Bayesian world is to build a more complicated model -- stick breaking priors for normal mixtures, splines in probabilities over time, whatever. But there is no direct analogue to Huber sandwich standard errors that explicitly acknowledge that the model may be misspecified, and are prepared to account for that.Going back to my first paragraph -- again, define probability. The formal definition is the trio .  is the space of possible outcomes (combinations of models and data).  is the -algebra of what can be measured on that space.  is the probability measure / density attached to subsets ,  -- which have to be measureable for the mathematics of probability to work. In finite dimensions, most reasonable sets are measurable -- see Borel sets, I am not going to bore you with details. With the more interesting infinite spaces (those of curves and trajectories, for instance), things get hairy very quickly. If you have a random process  on a unit interval in time, then the set  is not measurable, despite its apparent simplicity. (Sets like  are measurable for finite , and in fact generate the required -algebra. But that is not enough, apparently.) So probabilities in large dimensions may get tricky even at the level of definitions, let alone computations.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-20 09:50:45","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40bc"},"Last_activity":"2016-08-11 14:08:17","Creator_reputation":19081,"Question_score":1,"Answer_content":"  But if we want to estimate the probability of the model, why don't we calculate the probability of the model given the experiment?Because we don't know how. There's infinite number of model possible, and their probability space is not defined.Here's a practical example. Let's say I want to forecast US GDP. I get the time series, and fit a model. What is the probability that this model is true?So, let's actually fit a random walk model into GDP series:\\Delta\\ln  y_t=\\mu+e_twhere  is the growth rate and  is a random error. My code below does just that, and it also produces the forecast (red) and compares it historical data (blue). However, who said that GDP is a random walk process? What is it was a trend process? So, let's fit the trend: \\ln y_t = c t+ e_twhere  is the slope of the time trend. The forecast using a trend model is shown on the same chart (yellow).Now, how would you calculate the probability that my random walk model is true? Within MLE we could calculate the likelihood of the drift  given the data set, but that's not the probability. Second, and more importantly, how would you calculate the probability that the model is random walk with this drift knowing that it could also be a trend model? It could be any other number of models that produce this kind of dynamic.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-08-11 13:24:48","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40bd"},"Last_activity":"2016-08-11 13:34:07","Creator_reputation":7702,"Question_score":3,"Answer_content":"I will only add a few remarks; I agree with you that the overuse of -values is harmful.Some people in applied stats misinterpret -values, notably understanding them as the probability that the null hypotheses is true; cf these papers: P Values are not Error Probabilities and Why We Don’t Really Know What \"Statistical Significance\" Means: A Major Educational Failure.An other common misconception is that -values reflect the size of effect detected, or their potential for classification, when they reflect both the size of sample and the size of effects. This leads some people to write papers to explain why variables that have been shown \"strongly associated\" to a character (ie with very small p values) are poor classifiers, like this one...To conclude, my opinion is that -values are so widely used because of publications standards. In applied areas (biostats...) their size are sometimes the sole concern of some reviewers.","Display_name":"Elvis","Creater_id":8076,"Start_date":"2011-12-30 16:01:42","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40be"},"Last_activity":"2012-03-14 03:34:51","Creator_reputation":27740,"Question_score":28,"Answer_content":"Computing the probability that the hypothesis is correct doesn't fit well within the frequentist definition of a probability (a long run frequency), which was adopted to avoid the supposed subjectivity of the Bayesian definition of a probability.  The truth of a particular hypothesis is not a random variable, it is either true or it isn't and has no long run frequency.  It is indeed more natural to be interested in the probability of the truth of the hypothesis, which is IMHO why p-values are often misinterpreted as the probability that the null hypothesis is true.  Part of the difficulty is that from Bayes rule, we know that to compute the posterior probability that a hypothesis is true, you need to start with a prior probability that the hypothesis is true.A Bayesian would compute the probability that the hypothesis is true, given the data (and his/her prior belief).  Essentially in deciding between frequentist and Bayesian approaches is a choice whether the supposed subjectivity of the Bayesian approach is more abhorrent than the fact that the frequentist approach generally does not give a direct answer to the question you actually want to ask - but there is room for both.In the case of asking whether a coin is fair, i.e. the probability of a head is equal to the probability of a tail, we also have an example of a hypothesis that we know in the real world is almost certainly false right from the outset.  The two sides of the coin are non-symmetric, so we should expect a slight asymmetry in the probabilities of heads and tails, so if the coin \"passes\" the test, it just means we don't have enough observations to be able to conclude what we already know to be true - that the coin is very slightly biased!","Display_name":"Dikran Marsupial","Creater_id":887,"Start_date":"2010-12-17 04:06:10","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40bf"},"Last_activity":"2011-12-30 10:10:38","Creator_reputation":15777,"Question_score":16,"Answer_content":"Nothing like answering a really old question, but here goes....p-values are almost valid hypothesis tests.  This is a slightly adapted exerpt taken from Jaynes's 2003 probability theory book (Repetitive experiments: probability and frequency).  Suppose we have a null hypothesis  that we wish to test.  We have data  and prior information .  Suppose that there is some unspecified hypothesis  that we will test  against.  The posterior odds ratio for  against  is then given by:\\frac{P(H_A|DI)}{P(H_0|DI)}=\\frac{P(H_A|I)}{P(H_0|I)}\\times\\frac{P(D|H_AI)}{P(D|H_0I)}Now the first term on the right hand side is independent of the data, so the data can only influence the result via the second term.  Now, we can always invent an alternative hypothesis  such that  - a \"perfect fit\" hypothesis.  Thus we can use  as a measure of how well the data could support any alternative hypothesis over the null.  There is no alternative hypothesis that the data could support over  by greater than .  We can also restrict the class of alternatives, and the change is that the  is replaced by the maximised likelihood (including normalising constants) within that class.  If  starts to become too small, then we begin to doubt the null, because the number of alternatives between  and  grows (including some with non-negligible prior probabilities).  But this is so very nearly what is done with p-values, but with one exception: we don't calculate the probability for  for some statistic  and some \"bad\" region of the statistic.  We calculate the probability for  - the information we actually have, rather than some subset of it, .Another reason people use p-values is that they often amount to a \"proper\" hypothesis test, but may be easier to calculate.  We can show this with the very simple example of testing the normal mean with known variance.  We have data  with an assumed model  (part of the prior information ).  We want to test .  Then we have, after a little calculation:P(D|H_0I)=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp\\left(-\\frac{N\\left[s^2+(\\overline{x}-\\mu_0)^2\\right]}{2\\sigma^2}\\right)Where  and .  This shows that the maximum value of  will be achieved when .  The maximised value is:P(D|H_AI)=(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp\\left(-\\frac{Ns^2}{2\\sigma^2}\\right)So we take the ratio of these two, and we get:\\frac{P(D|H_AI)}{P(D|H_0I)}=\\frac{(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp\\left(-\\frac{Ns^2}{2\\sigma^2}\\right)}{(2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp\\left(-\\frac{Ns^2+N(\\overline{x}-\\mu_0)^2}{2\\sigma^2}\\right)}=\\exp\\left(\\frac{z^2}{2}\\right)Where  is the \"Z-statistic\".  Large values of  cast doubt on the null hypothesis, relative to the hypothesis about the normal mean which is most strongly supported by the data.  We can also see that  is the only part of the data that is needed, and thus is a sufficient statistic for the test.The p-value approach to this problem is almost the same, but in reverse.  We start with the sufficient statistic , and we caluclate its sampling distribution, which is easily shown to be  - where I have used a capital letter to distinguish the random variable  from the observed value .  Now we need to find a region which casts doubt on the null hypothesis: this is easily seen to be those regions where  is large.  So we can calculate the probability that  as a measure of how far away the observed data is from the null hypothesis.  As before, this is a simple calculation, and we get:\\text{p-value}=P(|\\overline{X}-\\mu_0|\\geq |\\overline{x}-\\mu_0||H_0)=1-P\\left[-\\sqrt{N}\\frac{|\\overline{x}-\\mu_0|}{\\sigma}\\leq\\sqrt{N}\\frac{\\overline{X}-\\mu_0}{\\sigma}\\leq \\sqrt{N}\\frac{|\\overline{x}-\\mu_0|}{\\sigma}|H_0\\right]=1-P(-|z|\\leq Z\\leq |z||H_0)=2\\left[1-\\Phi(|z|)\\right]Now, we can see that the p-value is a monotonic decreasing function of , which means we essentially get the same answer as the \"proper\" hypothesis test.  Rejecting when the p-value is below a certain threshold is the same thing as rejecting when the posterior odds is above a certain threshold.  However, note that in doing the proper test, we had to define the class of alternatives, and we had to maximise a probability over that class.  For the p-value, we have to find a statistic, and calculate its sampling distribution, and evaluate this at the observed value.  In some sense choosing a statistic is equivalent to defining the alternative hypothesis that you are considering.Although they are both easy things to do in this example, they are not always so easy in more complicated cases.  In some cases it may be easier to choose the right statistic to use and calculate its sampling distribution.  In others it may be easier to define the class of alternatives, and maximise over that class.This simple example account for a large amount of p-value based testing, simply because so many hypothesis tests are of the \"approximate normal\" variety.  It provides an approximate answer to your coin problem also (by using the normal approximation to the binomial).  It also shows that p-values in this case will not lead you astray, at least in terms of testing a single hypothesis.  In this case, we can say that a p-value is a measure of evidence against the null hypothesis.However, the p-values have a less interpretable scale than the bayes factor - the link between p-value and the \"amount\" of evidence against the null is complex.  p-values get too small too quickly - which makes them difficult to use properly.  They tend overstate the support against the null provided by the data.  If we interpret p-values as probabilities against the null -  in odds form is , when the actual evidence is , and  in odds form is  when the actual evidence is .  Or to put it another way, using a p-value as a probability that the null is false here, is equivalent to setting the prior odds.  So for p-value of  the implied prior odds against the null are  and for p-value of  the implied prior odds against the null are .","Display_name":"probabilityislogic","Creater_id":2392,"Start_date":"2011-12-30 10:10:38","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40c0"},"Last_activity":"2011-10-12 22:46:10","Creator_reputation":14354,"Question_score":6,"Answer_content":"A side note to the other excellent answers: on occasion there are times we don't. For example, up until very recently, they were outright banned at the journal Epidemiology - now they are merely \"strongly discouraged\" and the editorial board devoted a tremendous amount of space to a discussion of them here: http://journals.lww.com/epidem/pages/collectiondetails.aspx?TopicalCollectionId=4","Display_name":"Fomite","Creater_id":5836,"Start_date":"2011-10-12 22:46:10","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40c1"},"Last_activity":"2010-12-17 23:11:01","Creator_reputation":3656,"Question_score":10,"Answer_content":"Your question is a great example of frequentist reasoning and is, actually quite natural.  I've used this example in my classes to demonstrate the nature of hypothesis tests.  I ask for a volunteer to predict the results of a coin flip.  No matter what the result, I record a \"correct\" guess.  We do this repeatedly until the class becomes suspicious.Now, they have a null model in their head.  They assume the coin is fair.  Given that assumption of 50% correct when is everything is fair, every successive correct guess arouses more suspicion that the fair coin model is incorrect.  A few correct guesses and they accept the role of chance.  After 5 or 10 correct guesses, the class always begins to suspect that the chance of a fair coin is low.  Thus it is with the nature of hypothesis testing under the frequentist model.It is a clear and intuitive representation of the frequentist take on hypothesis testing.  It is the probability of the observed data given that the null is true.  It is actually quite natural as demonstrated by this easy experiment.  We take it for granted that the model is 50-50 but as evidence mounts, I reject that model and suspect that there is something else at play.So, if the probability of what I observe is low given the model I assume (the p-value) then I have some confidence in rejecting my assumed model.  Thus, a p-value is a useful measure of evidence against my assumed model taking into account the role of chance.A disclaimer: I took this exercise from a long forgotten article in, what I recall, was one of the ASA journals.","Display_name":"Brett","Creater_id":485,"Start_date":"2010-12-17 22:56:43","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40c2"},"Last_activity":"2010-12-17 13:55:13","Creator_reputation":121,"Question_score":10,"Answer_content":"As a former academic who moved into practice, I'll take a shot. People use p-values because they are useful. You can't see it in textbooky examples of coin flips. Sure they're not really solid foundationally, but maybe that is not as necessary as we like to think when we're thinking academically.In the world of data,  we're surrounded by a literally infinite number of possible things to look into next. With p-value computations all you need as an idea of what is uninteresting and a numerical heuristic for what sort of data might be interesting (well, plus a probability model for uninteresting). Then individually or collectively we can scan things pretty simple, rejecting the bulk of the uninteresting. The p-value allows us to say \"If I don't put much priority on thinking about  this otherwise, this data gives me no reason to change\".I agree p-values can be misinterpreted and overinterpreted, but they're still an important part of statistics. ","Display_name":"internet","Creater_id":2134,"Start_date":"2010-12-17 13:55:13","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40c3"},"Last_activity":"2010-12-17 12:10:47","Creator_reputation":13461,"Question_score":6,"Answer_content":"\"Roughly speaking p-value gives a probability of the observed outcome of an experiment given the hypothesis (model).\"but it doesn't.  Not even roughly - this fudges an essential distinction.The model is not specified, as Raskolnikov points out, but let's assume you mean a binomial model (independent coin tosses, fixed unknown coin bias).  The hypothesis is the claim that the relevant parameter in this model, the bias or probability of heads, is 0.5.\"Having this probability (p-value) we want to judge our hypothesis (how likely it is)\"We may indeed want to make this judgement but a p-value will not (and was not designed to) help us do so.\"But wouldn't it be more natural to calculate the probability of the hypothesis given the observed outcome?\"Perhaps it would.  See all the discussion of Bayes above.\"[...] Now we calculate the p-value, that is equal to the probability to get 14 or more heads in 20 flips of coin.  OK, now we have this probability (0.058) and we want to use this probability to judge our model (how is it likely that we have a fair coin).\"'of our hypothesis, assuming our model to be true', but essentially: yes.  Large p-values indicate that the coin's behaviour is consistent with the hypothesis that it is fair.  (They are also typically consistent with the hypothesis being false but so close to being true we do not have enough data to tell; see 'statistical power'.)\"But if we want to estimate the probability of the model, why we do not calculate the probability of the model given the experiment? Why do we calculate the probability of the experiment given the model (p-value)?\"We actually don't calculate the probability of the experimental results given the hypothesis in this setup.  After all, the probability is only about 0.176 of seeing exactly 10 heads when the hypothesis is true, and that's the most probable value.  This isn't a quantity of interest at all.It is also relevant that we don't usually estimate the probability of the model either.  Both frequentist and Bayesian answers typically assume the model is true and make their inferences about its parameters.  Indeed, not all Bayesians would even in principle be interested in the probability of the model, that is: the probability that the whole situation was well modelled by a binomial distribution.  They might do a lot of model checking, but never actually ask how likely the binomial was in the space of other possible models.  Bayesians who care about Bayes Factors are interested, others not so much.","Display_name":"conjugateprior","Creater_id":1739,"Start_date":"2010-12-17 12:10:47","Question_id":5591}
{"_id":{"$oid":"5837a579a05283111e4d40d4"},"Last_activity":"2016-08-20 09:15:23","Creator_reputation":601,"Question_score":3,"Answer_content":"The trick here is to realize that  and  are in this case constants..Now the rest should be easy as the  are all well known. See here the exact variance/covariance values: How to derive variance-covariance matrix of coefficients in linear regression Edit: In response to your comment, it really is a matter of taste how far you take this from now on. I would proceed as follow:","Display_name":"user1357015","Creater_id":14104,"Start_date":"2014-04-23 23:17:56","Question_id":94970}
{"_id":{"$oid":"5837a579a05283111e4d40e3"},"Last_activity":"2016-01-02 09:03:28","Creator_reputation":323,"Question_score":1,"Answer_content":"A very specific suggestion to serve as an example: Check SGD classifier with class_weight='balanced' from scikit_learn. Make sure the label you want to predict comes out as 1 in your encoding and you use e.g. precision as scoring metric if you were to search for model hyperparameters with GridsearchCV or RandomsearchCV.","Display_name":"Diego","Creater_id":98469,"Start_date":"2016-01-02 09:03:28","Question_id":63134}
{"_id":{"$oid":"5837a579a05283111e4d40e4"},"Last_activity":"2016-01-01 23:37:20","Creator_reputation":74,"Question_score":1,"Answer_content":"These are classical challenges in any Big Data Machine Learning problems.  Most categories should have a lot of values that repeat themselves over and over and very few that appear very rarely.For categorical features where you have this occurs, you can use one-hot encoding to create additional features.  few interval data items (real numbers, less than 5 such items)For interval data items you can encode it using label encoders. If any business insights can be drawn like mean, median, mode, frequency etc. then the same should be reflected in your approach.  Some categories are also overcategories of others (like country and city). The outcome of each data is either 1 if the event occured or 0 if it did not occur.Since these dependent variables already cleanly reflect the independent variable there is little to be done here. I don't see a way to separate the features into hierarchy either.  What machine learning approaches and statistical models will perform well on such a task? My initial thoughts are logarithmic regression and support vector machines (with extensions like random forest)You can start with logistic regression. However since your data is big you might be able to find some distribution to the input feature vectors. In that case using Gaussian Discriminant Analysis might help. You can model p(y) as a Bernoulli random variable and predict p(x|y=0) and p(x|y=1) as Normal distribution.SVM algorithm can also be used but the training time will be high. Also, we don't know the best choice of the kernel before looking at the performance on learning curve.What is your evaluation measure through? Is it accuracy or precision/recall? If it is latter then using Random Forest will not work, instead you can go by simple decision trees.","Display_name":"SamParker","Creater_id":99358,"Start_date":"2016-01-01 23:37:20","Question_id":63134}
{"_id":{"$oid":"5837a579a05283111e4d40f1"},"Last_activity":"2015-10-04 15:42:28","Creator_reputation":2285,"Question_score":42,"Answer_content":"First, let's define a score.John, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:      Maths    Science    English    Music    John  80        85          60       55  Mike  90        85          70       45Kate  95        80          40       50In this case there are 12 scores in total. Each score represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.Now let's informally define a Principal Component.In the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables: Maths, Science, English, and Music), i.e.:You could plot two subjects in the exact same way you would with  and  co-ordinates in a 2D graph. You could even plot three subjects in the same way you would plot ,  and  in a 3D graph (though this is generally bad practice, because some distortion is inevitable in the 2D representation of 3D data). But how would you plot 4 subjects?At the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as Multidimensional scaling.Principal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could look at the types of subjects each student is maybe more suited to.A principal component is therefore a combination of the original variables after a linear transformation. In R, this is:DF\u0026lt;-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))prcomp(DF, scale = FALSE)Which will give you something like this (first two Principal Components only for sake of simplicity):                PC1         PC2Maths    0.27795606  0.76772853 Science -0.17428077 -0.08162874 English -0.94200929  0.19632732 Music    0.07060547 -0.60447104 The first column here shows coefficients of linear combination that defines principal component #1, and the second column shows coefficients for principal component #2.So what is a Principal Component Score?It's a score from the table at the end of this post (see below).The above output from R means we can now plot each person's score across all subjects in a 2D graph as follows. First, we need to center the original variables my subtracting column means:      Maths    Science    English    Music    John  -8.33       1.66       3.33       5  Mike   1.66       1.66      13.33      -5Kate   6.66       -3.33    -16.66       0And then to form linear combinations to get PC1 and PC2 scores:      x                                                    yJohn -0.28*8.33 + -0.17*1.66 + -0.94*3.33  + 0.07*5   -0.77*8.33 + -0.08*1.66 + 0.19*3.33   + -0.60*5 Mike 0.28*1.66  + -0.17*1.66 + -0.94*13.33 + -0.07*5   0.77*1.66 + -0.08*1.66 + 0.19*13.33  + -0.60*5Kate 0.28*6.66  + 0.17*3.33  + 0.94*16.66  + 0.07*0    0.77*6.66 +  0.08*3.33 + -0.19*16.66 + -0.60*0Which simplifies to:        x       yJohn   -5.39   -8.90Mike  -12.74    6.78Kate   18.13    2.12There are six principal component scores in the table above. You can now plot the scores in a 2D graph to get a sense of the type of subjects each student is perhaps more suited to.The same output can be obtained in R by typing prcomp(DF, scale = FALSE)$x.EDIT 1: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.EDIT 2: full credit to @drpaulbrewer for his comment in improving this answer.","Display_name":"Tony Breyal","Creater_id":81,"Start_date":"2010-07-20 05:02:26","Question_id":222}
{"_id":{"$oid":"5837a579a05283111e4d40f2"},"Last_activity":"2012-07-01 19:33:13","Creator_reputation":1779,"Question_score":4,"Answer_content":"Let  index the rows and  index the columns. Suppose you linearize the combination of variables (columns):Z_{i,1} = c_{i,1}\\cdot Y_{i,1} + c_{i,2}\\cdot Y_{i,2} + ... + c_{i,M}\\cdot Y_{i,M}The above formula basically says to multiply row elements with a certain value  (loadings) and sum them by columns. Resulting values ( values times the loading) are scores.A principal component (PC) is a linear combination ) (values by columns which are called scores). In essence, the PC should present the most important features of variables (columns). Ergo, you can extract as many PC as there are variables (or less).An output from R on PCA (a fake example) looks like this. PC1, PC2... are principal components 1, 2... The example below is showing only the first 8 principal components (out of 17). You can also extract other elements from PCA, like loadings and scores.Importance of components:                          PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8Standard deviation     1.0889 1.0642 1.0550 1.0475 1.0387 1.0277 1.0169 1.0105Proportion of Variance 0.0697 0.0666 0.0655 0.0645 0.0635 0.0621 0.0608 0.0601Cumulative Proportion  0.0697 0.1364 0.2018 0.2664 0.3298 0.3920 0.4528 0.5129","Display_name":"Roman Luštrik","Creater_id":144,"Start_date":"2010-07-19 23:24:32","Question_id":222}
{"_id":{"$oid":"5837a579a05283111e4d40f3"},"Last_activity":"2011-08-27 17:09:20","Creator_reputation":15777,"Question_score":6,"Answer_content":"I like to think of principal component scores as \"basically meaningless\" until you actually give them some meaning.  Interpretting PC scores in terms of \"reality\" is a tricky business - and there can really be no unique way to do it.  It depends on what you know about the particular variables that are going into the PCA, and how they relate to each other in terms of interpretations.As far as the mathematics goes, I like to interpret PC scores as the co-ordinates of each point, with respect to the principal component axes.  So in the raw variables you have   which is a \"point\" in p-dimensional space.  In these co-ordinates, this means along the  axis the point is a distance  away from the origin.  Now a PCA is basically a different way to describe this \"point\" - with respect to its principal component axis, rather than the \"raw variable\" axis.  So we have  , where  is the  matrix of principal component weights (i.e. eigenvectors in each row), and  is the \"centroid\" of the data (or mean vector of the data points).So you can think of the eigenvectors as describing where the \"straight lines\" which describe the PCs are.   Then the principal component scores describe where each data point lies on each straight line, relative to the \"centriod\" of the data.  You can also think of the PC scores in combination with the weights/eigenvectors as a series of rank 1 predictions for each of the original data points, which have the form:\\hat{x}_{ji}^{(k)}=\\overline{x}_j+z_{ki}A_{kj}Where  is the prediction for the th observation, for the th variable using the th PC.","Display_name":"probabilityislogic","Creater_id":2392,"Start_date":"2011-08-27 17:03:40","Question_id":222}
{"_id":{"$oid":"5837a579a05283111e4d40f4"},"Last_activity":"2010-07-26 12:58:28","Creator_reputation":111,"Question_score":4,"Answer_content":"The principal components of a data matrix are the eigenvector-eigenvalue pairs of its variance-covariance matrix.  In essence, they are the decorrelated pieces of the variance.  Each one is a linear combination of the variables for an observation -- suppose you measure w, x, y,z on each of a bunch of subjects.  Your first PC might work out to be something like0.5w + 4x + 5y - 1.5zThe loadings (eigenvectors) here are (0.5, 4, 5, -1.5).  The score (eigenvalue) for each observation is the resulting value when you substitute in the observed (w, x, y, z) and compute the total.This comes in handy when you project things onto their principal components (for, say, outlier detection) because you just plot the scores on each like you would any other data.  This can reveal a lot about your data if much of the variance is correlated (== in the first few PCs).","Display_name":"Tim","Creater_id":317,"Start_date":"2010-07-26 12:58:28","Question_id":222}
{"_id":{"$oid":"5837a579a05283111e4d40f5"},"Last_activity":"2010-07-20 01:20:36","Creator_reputation":8010,"Question_score":1,"Answer_content":"Principal component scores are a group of scores that are obtained following a Principle Components Analysis (PCA).  In PCA the relationships between a group of scores is analyzed such that an equal number of new \"imaginary\" variables (aka principle components) are created.  The first of these new imaginary variables is maximally correlated with all of the original group of variables.  The next is somewhat less correlated, and so forth until the point that if you used all of the principal components scores to predict any given variable from the initial group you would be able to explain all of its variance.  The way in which PCA proceeds is complex and has certain restrictions.  Among these is the restriction that the correlation between any two principal components (i.e. imaginary variables) is zero; thus it doesn't make sense to try to predict one principal component with another.","Display_name":"rpierce","Creater_id":196,"Start_date":"2010-07-20 01:20:36","Question_id":222}
{"_id":{"$oid":"5837a579a05283111e4d40f6"},"Last_activity":"2010-07-19 23:47:14","Creator_reputation":1088,"Question_score":6,"Answer_content":"Say you have a cloud of N points in, say, 3D (which can be listed in a 100x3 array). Then, the principal components analysis (PCA) fits an arbitrarily oriented ellipsoid into the data. The principal component score is the length of the diameters of the ellipsoid. In the direction in which the diameter is large, the data varies a lot, while in the direction in which the diameter is small, the data varies litte. If you wanted to project N-d data into a 2-d scatter plot, you plot them along the two largest principal components, because with that approach you display most of the variance in the data.","Display_name":"Jonas","Creater_id":198,"Start_date":"2010-07-19 23:47:14","Question_id":222}
{"_id":{"$oid":"5837a579a05283111e4d40f7"},"Last_activity":"2010-07-19 23:23:21","Creator_reputation":573,"Question_score":17,"Answer_content":"Principal component analysis (PCA) is one popular approach analyzing variance when you are dealing with multivariate data. You have random variables X1, X2,...Xn which are all correlated (positively or negatively) to varying degrees, and you want to get a better understanding of what's going on. PCA can help.What PCA gives you is a change of variable into Y1, Y2,..., Yn (i.e. the same number of variables) which are linear combinations of the Xs. For example, you might have Y1 = 2.1 X1 - 1.76 X2 + 0.2 X3...The Ys the nice property that each of these have zero correlation with each other. Better still, you get them in decreasing order of variance. So, Y1 \"explains\" a big chunk of the variance of the original variables, Y2 a bit less and so on. Usually after the first few Ys, the variables become somewhat meaningless. The PCA score for any of the Xi is just it's coefficient in each of the Ys. In my earlier example, the score for X2 in the first principal component (Y1) is 1.76.The way PCA does this magic is by computing eigenvectors of the covariance matrix.To give a concrete example, imagine X1,...X10 are changes in 1 year, 2 year, ..., 10 year Treasury bond yields over some time period. When you compute PCA you generally find that the first component has scores for each bond of the same sign and about the same sign. This tells you that most of the variance in bond yields comes from everything moving the same way: \"parallel shifts\" up or down. The second component typically shows \"steepening\" and \"flattening\" of the curve and has opposite signs for X1 and X10.","Display_name":"seancarmody","Creater_id":173,"Start_date":"2010-07-19 23:23:21","Question_id":222}
{"_id":{"$oid":"5837a579a05283111e4d4104"},"Last_activity":"2013-12-11 11:58:18","Creator_reputation":14825,"Question_score":14,"Answer_content":"You can find a pedagogical summary of the various methods available in (1)For some --recent-- numerical comparisons of the various methods listed there, you can check (2) and (3).there are many older (and less exhaustive) numerical comparisons, typically found in books. You will find one on pages 142-143 of (4), for example.Note that all the methods discussed here have an open source R implementation, mainly through the rrcov package.(1) P. Rousseeuw and M. Hubert (2013)  High-Breakdown Estimators ofMultivariate Location and Scatter.(2) M. Hubert, P. Rousseeuw, K. Vakili (2013).Shape bias of robust covariance estimators: an empirical study.Statistical Papers.(3) K. Vakili and E. Schmitt (2014). Finding multivariate outliers with FastPCS. Computational Statistics \u0026amp; Data Analysis.(4) Maronna R. A., Martin R. D. and Yohai V. J. (2006).Robust Statistics: Theory and Methods. Wiley, New York.","Display_name":"user603","Creater_id":603,"Start_date":"2010-07-29 09:13:41","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d4105"},"Last_activity":"2013-11-21 16:21:08","Creator_reputation":15777,"Question_score":12,"Answer_content":"I novel approach I saw was by IT Jolliffe Principal Components Analysis.  You run a PCA on your data (Note: PCA can be quite a useful data exploration tool in its own right), but instead of looking at the first few Principal Components (PCs), you plot the last few PCs.  These PCs are the linear relationships between your variables with the smallest variance possible.  Thus they detect \"exact\" or close to exact multivariate relationships in your data.A plot of the PC scores for the last PC will show outliers not easily detectable by looking individually at each variable.  One example is for height and weight - some who has \"above average\" height and \"below average\" weight would be detected by the last PC of height and weight (assuming these are positively correlated), even if their height and weight were not \"extreme\" individually (e.g. someone who was 180cm and 60kg).","Display_name":"probabilityislogic","Creater_id":2392,"Start_date":"2011-02-27 14:22:37","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d4106"},"Last_activity":"2012-05-04 17:50:18","Creator_reputation":25897,"Question_score":4,"Answer_content":"I didn't see anybody mention influence functions.  I first saw this idea in Gnanadesikan's multivariate book.In one dimension an outlier is either an extremely large or an extremely small value.  In multivariate analysis it is an observation removed from the bulk of the data.  But what metric should we use to define extreme for the outlier?  There are many choices. The Mahalanobis distance is just one.  I think that looking for every type of outlier is futile and counterproductive.  I would ask why do you care about the outlier?  In estimating a mean they can have a great deal of influence on that estimate.  Robust estimators downweight and accommodate outliers but they do not formally test for them.  Now in regression, the outliers--like leverage points--could have large effects on the slope parameters in the model.  With bivariate data they can unduly influence the estimated correlation coefficient and in three or more dimensions the multiple correlation coefficient.Influence functions were introduced by Hampel as a tool in robust estimation and Mallows wrote a nice unpublished paper advocating their use.  The influence function is a function of the point you are at in n-dimensional space and the parameter.  It essentially measures the difference between the parameter estimate with the point in the calculation and with the point left out. Rather than go to the trouble of doing the calculation of the two estimates and taking the difference, often you can derive a formula for it.  Then the contours of constant influence tell you the direction that is extreme with respect to the estimate of this parameter and hence tell you where in the n-dimensional space to look for the outlier.For more you can look at my 1983 paper in the American Journal of Mathematical and Management Sciences titled \"The influence function and its application to data validation.\"   In data validation we wanted to look for outliers that affected the intended use of the data.  My feeling is that you should direct your attention to outliers that greatly affect the parameters you are interested in estimating and not care so much about others that don't. ","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-05-04 14:49:04","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d4107"},"Last_activity":"2012-02-17 12:04:13","Creator_reputation":147383,"Question_score":11,"Answer_content":"You can find candidates for \"outliers\" among the support points of the minimum volume bounding ellipsoid.  (Efficient algorithms to find these points in fairly high dimensions, both exactly and approximately, were invented in a spate of papers in the 1970's because this problem is intimately connected with a question in experimental design.)","Display_name":"whuber","Creater_id":919,"Start_date":"2010-08-23 15:41:05","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d4108"},"Last_activity":"2011-06-14 11:26:43","Creator_reputation":4907,"Question_score":14,"Answer_content":"I would do some sort of \"leave one out testing algorithm\" (n is the number of data):for i=1 to ncompute a density estimation of the data set obtained by throwing  away. (This density estimate should be done with some assumption if the dimension is high, for example, a gaussian assumption for which the density estimate is easy: mean and covariance)Calculate the likelihood of  for the density estimated in step 1. call it . end for sort the  (for i=1,..,n) and use a multiple hypothesis testing procedure to say which are not good ... This will work if n is sufficiently large... you can also use \"leave k out strategy\" which can be more relevent when you have \"groups\" of outliers ...","Display_name":"robin girard","Creater_id":223,"Start_date":"2010-07-20 23:46:11","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d4109"},"Last_activity":"2010-08-27 09:44:31","Creator_reputation":7927,"Question_score":4,"Answer_content":"I'm not aware that anyone is doing this, but I generally like to try dimensionality reduction when I have a problem like this.  You might look into a method from manifold learning or non-linear dimensionality reduction.An example would be a Kohonen map.  A good reference for R is \"Self- and Super-organizing Maps in R: The kohonen Package\".","Display_name":"Shane","Creater_id":5,"Start_date":"2010-08-27 09:44:31","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d410a"},"Last_activity":"2010-08-23 03:48:27","Creator_reputation":37824,"Question_score":23,"Answer_content":"Have a look at the mvoutlier package which relies on ordered robust mahalanobis distances, as suggested by @drknexus.","Display_name":"chl","Creater_id":930,"Start_date":"2010-08-23 03:48:27","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d410b"},"Last_activity":"2010-07-22 18:44:22","Creator_reputation":26604,"Question_score":18,"Answer_content":"I think Robin Girard's answer would work pretty well for 3 and possibly 4 dimensions, but the curse of dimensionality would prevent it working beyond that. However, his suggestion led me to a related approach which is to apply the cross-validated kernel density estimate to the first three principal component scores. Then a very high-dimensional data set can still be handled ok.In summary, for i=1 to nCompute a density estimate of the first three principal component scores obtained from the data set without Xi. Calculate the likelihood of Xi for the density estimated in step 1.call it Li. end forSort the Li (for i=1,..,n) and the outliers are those with likelihood below some threshold. I'm not sure what would be a good threshold -- I'll leave that for whoever writes the paper on this! One possibility is to do a boxplot of the log(Li) values and see what outliers are detected at the negative end.","Display_name":"Rob Hyndman","Creater_id":159,"Start_date":"2010-07-22 18:44:22","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d410c"},"Last_activity":"2010-07-21 13:03:40","Creator_reputation":3102,"Question_score":6,"Answer_content":"For moderate dimensions, like 3, then some sort of kernel cross-validation technique as suggested elsewhere seems reasonable and is the best I can come up with.For higher dimensions, I'm not sure that the problem is solvable; it lands pretty squarely into 'curse-of-dimensionality' territory.  The issue is that distance functions tend to converge to very large values very quickly as you increase dimensionality, including distances derived from distributions.  If you're defining an outlier as \"a point with a comparatively large distance function relative to the others\", and all your distance functions are beginning to converge because you're in a high-dimensional space, well, you're in trouble.Without some sort of distributional assumption that will let you turn it into a probabilistic classification problem, or at least some rotation that lets you separate your space into \"noise dimensions\" and \"informative dimensions\", I think that the geometry of high-dimensional spaces is going to prohibit any easy -- or at least robust -- identification of outliers.","Display_name":"Rich","Creater_id":61,"Start_date":"2010-07-21 13:03:40","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d410d"},"Last_activity":"2010-07-21 05:59:00","Creator_reputation":351,"Question_score":1,"Answer_content":"One of the above answers touched in mahalanobis distances.... perhaps anpther step further and calculating simultaneous confidence intervals would help detect outliers!","Display_name":"Mojo","Creater_id":256,"Start_date":"2010-07-21 05:59:00","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d410e"},"Last_activity":"2010-07-20 02:30:51","Creator_reputation":17873,"Question_score":8,"Answer_content":"It may be an overshoot, but you may train an unsupervised Random Forest on the data and use the object proximity measure to detect outliers. More details here.","Display_name":"mbq","Creater_id":88,"Start_date":"2010-07-20 02:30:51","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d410f"},"Last_activity":"2010-07-20 00:56:06","Creator_reputation":8010,"Question_score":5,"Answer_content":"I'm not sure what you mean when you say you aren't thinking of a regression problem but of \"true multivariate data\".  My initial response would be to calculate the Mahalanobis distance since it doesn't require that you specify a particular IV or DV, but at its core (as far as I understand it) it is related to a leverage statistic.","Display_name":"rpierce","Creater_id":196,"Start_date":"2010-07-20 00:56:06","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d4110"},"Last_activity":"2010-07-20 00:55:30","Creator_reputation":846,"Question_score":3,"Answer_content":"My first response would be that if you can do multivariate regression on the data, then to use the residuals from that regression to spot outliers. (I know you said it's not a regression problem, so this might not help you, sorry !)I'm copying some of this from a Stackoverflow question I've previously answered which has some example R codeFirst, we'll create some data, and then taint it with an outlier;\u0026gt; testout\u0026lt;-data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5),Y=rnorm(50,mean=200,sd=25)) \u0026gt; #Taint the Data \u0026gt; testoutX2[10]\u0026lt;-5 \u0026gt; testoutX1, ylab=\"X1\") \u0026gt; boxplot(testoutY, ylab=\"Y\") You can then use stats to calculate critical cut off values, here using the Lund Test (See Lund, R. E. 1975, \"Tables for An Approximate Test for Outliers in Linear Models\", Technometrics, vol. 17, no. 4, pp. 473-476. and Prescott, P. 1975, \"An Approximate Test for Outliers in Linear Models\", Technometrics, vol. 17, no. 1, pp. 129-132.)\u0026gt; #Alternative approach using Lund Test \u0026gt; lundcrit\u0026lt;-function(a, n, q) { + # Calculates a Critical value for Outlier Test according to Lund + # See Lund, R. E. 1975, \"Tables for An Approximate Test for Outliers in Linear Models\", Technometrics, vol. 17, no. 4, pp. 473-476. + # and Prescott, P. 1975, \"An Approximate Test for Outliers in Linear Models\", Technometrics, vol. 17, no. 1, pp. 129-132. + # a = alpha + # n = Number of data elements + # q = Number of independent Variables (including intercept) + F\u0026lt;-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE) + crit\u0026lt;-((n-q)*F/(n-q-1+F))^0.5 + crit + } \u0026gt; testoutlm\u0026lt;-lm(Y~X1+X2,data=testout) \u0026gt; testoutresidual\u0026lt;-residuals(testoutlm) \u0026gt; testoutcoefficients) \u0026gt; crit\u0026lt;-lundcrit(0.1,n,q) \u0026gt; testoutstandardresid\u0026gt;crit,NA,testout$Y) \u0026gt; testout          X1         X2        Y    newX1   fitted    residual standardresid 1  44.20043  1.5259458 169.3296 44.20043 209.8467 -40.5171222  -1.009507695 2  40.46721  5.8437076 200.9038 40.46721 231.9221 -31.0183107  -0.747624895 3  48.20571  3.8243373 189.4652 48.20571 203.4786 -14.0134646  -0.335955648 4  60.09808  4.6609190 177.5159 60.09808 169.6108   7.9050960   0.190908291 5  50.23627  2.6193455 210.4360 50.23627 194.3285  16.1075799   0.391537883 6  43.50972  5.8212863 203.8361 43.50972 222.6667 -18.8306252  -0.452070155 7  44.95626  7.8368405 236.5821 44.95626 223.3287  13.2534226   0.326339981 8  66.14391  3.6828843 171.9624 66.14391 148.8870  23.0754677   0.568829360 9  45.53040  4.8311616 187.0553 45.53040 214.0832 -27.0279262  -0.646090667 10  5.00000  5.0000000 530.0000       NA 337.0535 192.9465135   5.714275585 11 64.71719  6.4007245 164.8052 64.71719 159.9911   4.8141018   0.118618011 12 54.43665  7.8695891 192.8824 54.43665 194.7454  -1.8630426  -0.046004311 13 45.78278  4.9921489 182.2957 45.78278 213.7223 -31.4266180  -0.751115595 14 49.59998  4.7716099 146.3090 49.59998 201.6296 -55.3205552  -1.321042392 15 45.07720  4.2355525 192.9041 45.07720 213.9655 -21.0613819  -0.504406009 16 62.27717  7.1518606 186.6482 62.27717 169.2455  17.4027250   0.430262983 17 48.50446  3.0712422 228.3253 48.50446 200.6938  27.6314695   0.667366651 18 65.49983  5.4609713 184.8983 65.49983 155.2768  29.6214506   0.726319931 19 44.38387  4.9305222 213.9378 44.38387 217.7981  -3.8603382  -0.092354925 20 43.52883  8.3777627 203.5657 43.52883 228.9961 -25.4303732  -0.634725264 \u0026lt;snip\u0026gt; 49 45.28317  5.0219647 208.1318 45.28317 215.3075  -7.1756966  -0.171560291 50 44.84145  3.6252663 251.5620 44.84145 213.1535  38.4084869   0.923804784        Ynew 1  169.3296 2  200.9038 3  189.4652 4  177.5159 5  210.4360 6  203.8361 7  236.5821 8  171.9624 9  187.0553 10       NA 11 164.8052 12 192.8824 13 182.2957 14 146.3090 15 192.9041 16 186.6482 17 228.3253 18 184.8983 19 213.9378 20 203.5657 \u0026lt;snip\u0026gt; 49 208.1318 50 251.5620 Obviosuly there are other outlier tests than the Lund test (Grubbs springs to mind), but I'm not sure which are better suited to multivariate data.","Display_name":"PaulHurleyuk","Creater_id":114,"Start_date":"2010-07-20 00:55:30","Question_id":213}
{"_id":{"$oid":"5837a579a05283111e4d4121"},"Last_activity":"2014-08-05 18:00:14","Creator_reputation":51,"Question_score":1,"Answer_content":"Although the FLOOR answer is probably nicer, you can do this with a vlookup.The first column of the lookup table (shown below) is the lower bound of each of your bins. Then you enter a formula to make the labels in the second column (formula shown, and result of the formula in the third column -- obviously you'd only have two columns.) Note the last two rows of the table are wonky -- the second last has a different formula because you want it to be the unbounded \"greater than\" case (unless you don't want that, then change the formula - you can set this up however you like). You would enter this formula (assuming the values you want to bin are in Col A, and the lookup table is in cols F:G somewhere. Note the TRUE parameter, so vlookup doesn't need to find an exact match. =VLOOKUP(A2,2:24,2,TRUE)This is what the lookup table would look like:Floor   Formula             Result of Formula (label)     0   =F2 \u0026amp; \" - \" \u0026amp;F3-1   0 - 4999950000   =F3 \u0026amp; \" - \" \u0026amp;F4-1   50000 - 99999100000  =F4 \u0026amp; \" - \" \u0026amp;F5-1   100000 - 149999150000  =F5 \u0026amp; \" - \" \u0026amp;F6-1   150000 - 199999200000  =F6 \u0026amp; \" - \" \u0026amp;F7-1   200000 - 249999250000  =F7 \u0026amp; \" - \" \u0026amp;F8-1   250000 - 299999300000  =F8 \u0026amp; \" - \" \u0026amp;F9-1   300000 - 349999....1100000 =\" \u0026gt;= \" \u0026amp;F25        \u0026gt;= 1100000","Display_name":"bjsalami","Creater_id":52861,"Start_date":"2014-08-05 18:00:14","Question_id":4341}
{"_id":{"$oid":"5837a579a05283111e4d4122"},"Last_activity":"2014-07-21 02:30:32","Creator_reputation":1,"Question_score":0,"Answer_content":"that is simply to use this formula in excel:Ceiling(Cell(i,j),1000)/1000Then you have group of 1000: 1, 2, 3, ...","Display_name":"Anh","Creater_id":52457,"Start_date":"2014-07-21 02:30:32","Question_id":4341}
{"_id":{"$oid":"5837a579a05283111e4d4123"},"Last_activity":"2014-02-16 21:10:27","Creator_reputation":184,"Question_score":0,"Answer_content":"To answer this for SQL, you really need to have a database that understands a window function (part of SQL:2003), such as MS SQL Server or PostgreSQL.To group val from table data into, say, 13 equal-width bins, use the ntile window function:SELECT val, ntile(13) OVER (ORDER BY val)FROM data;       val        | ntile------------------+------- 8908.96283090115 |     1 9090.72533249855 |     1  9620.1803535223 |     1  11068.768799305 |     1... 1994248.56621772 |    13 1994786.21594608 |    13 1995945.97052783 |    13 1997640.62557369 |    13(1000 rows)","Display_name":"Mike T","Creater_id":3929,"Start_date":"2014-02-16 21:10:27","Question_id":4341}
{"_id":{"$oid":"5837a579a05283111e4d4124"},"Last_activity":"2010-11-16 07:27:29","Creator_reputation":2487,"Question_score":2,"Answer_content":"Another option using Excel that gives you a fair amount of flexibility over the number \u0026amp; size of bins is the =frequency(data_array, bins_array) function. It is an array function that expects two arguments. The first argument is your data, the second is your bins that you define.Let's assume your data is in cells A1 - A1000 and create bins in cells B1 - B20. You would want to highlight cells C1 - C21 and then type something like =FREQUENCY(A1:A100, B1:B21). Unlike normal functions, array functions must be entered with the key combination SHIFT + CTRL + ENTER. You should see the counts fill down for all bins, if not - you most likely only hit enter and the first cell is calculated.There are plenty of good tutorials online explaining this in more detail, here's a decent one.","Display_name":"Chase","Creater_id":696,"Start_date":"2010-11-16 07:27:29","Question_id":4341}
{"_id":{"$oid":"5837a579a05283111e4d4125"},"Last_activity":"2010-11-12 18:32:33","Creator_reputation":147383,"Question_score":3,"Answer_content":"You have requested an Excel or SQL solution.  The easiest way in Excel is to use its \"Analysis\" add-in to create a histogram.  It will automatically create the bins (ranges of values) but, optionally, accepts a list of bin cutpoints as input and uses them.  The output includes a parallel list of bin counts.  This is especially handy for irregular-width bins.This is a one-off calculation: if the data change or the cutpoints change, you have to go through the entire dialog again.  A more flexible option is to use COUNTIF to count all values less than or equal to any given bin cutpoint.  The first differences of such an array give the bin counts.Here is a working example.  The data are in a column named \"Simulation_Z\" (which in this particular case is defined to be an entire column, such as C).  The formulae shown below are copied from columns L2:N10 of a sheet in the same workbook.  They were created by copying the first one downward (but notice the special formula for the first count in N3).Cut Count up                            Count-3.0    =COUNTIF(Simulation_Z, \"\u0026lt;=\" \u0026amp; L3)   =M3-2.0    =COUNTIF(Simulation_Z, \"\u0026lt;=\" \u0026amp; L4)   =M4-M3-1.0    =COUNTIF(Simulation_Z, \"\u0026lt;=\" \u0026amp; L5)   =M5-M4 0.0    =COUNTIF(Simulation_Z, \"\u0026lt;=\" \u0026amp; L6)   =M6-M5 1.0    =COUNTIF(Simulation_Z, \"\u0026lt;=\" \u0026amp; L7)   =M7-M6 2.0    =COUNTIF(Simulation_Z, \"\u0026lt;=\" \u0026amp; L8)   =M8-M7 3.0    =COUNTIF(Simulation_Z, \"\u0026lt;=\" \u0026amp; L9)   =M9-M8=MAX(Simulation_Z)  =COUNTIF(Simulation_Z, \"\u0026lt;=\" \u0026amp; L10)  =M10-M9Column L (\"Cut\") stipulates the upper limits of each bin.This procedure simultaneously defines the bins and computes their counts, which are then available for further testing (e.g., ) or plotting.","Display_name":"whuber","Creater_id":919,"Start_date":"2010-11-09 08:08:16","Question_id":4341}
{"_id":{"$oid":"5837a579a05283111e4d4126"},"Last_activity":"2010-11-10 07:07:28","Creator_reputation":764,"Question_score":1,"Answer_content":"In Excel, a simple way to group numeric data into bins is via the Pivot Table. Pull the numeric variable into the \"row labels\". Now right-click on any of the values in this right column and choose \"Group\". You can set the min and max of the overall range and the bin size (equal bins widths for all data). ","Display_name":"Galit Shmueli","Creater_id":1945,"Start_date":"2010-11-10 07:07:28","Question_id":4341}
{"_id":{"$oid":"5837a579a05283111e4d4127"},"Last_activity":"2010-11-09 06:57:35","Creator_reputation":17409,"Question_score":8,"Answer_content":"Why group them? Instead, how about estimate the probability density function (PDF) of the distributions from which the data arise? Here's an R-based example:set.seed(123)dat \u0026lt;- c(sample(2000000, 500), rnorm(100, 1000000, 1000),          rnorm(150, 1500000, 100),rnorm(150, 500000, 10),         rnorm(180, 10000, 10), rnorm(10, 1000, 5), 1:10)dens \u0026lt;- density(dat)plot(dens)If the data are strictly bounded (0, 2,000,000) then the kernel density estimate is perhaps not best suited. You could fudge things by asking it to only evaluate the density between the bounds:dens2 \u0026lt;- density(dat, from = 0, to = 2000000)plot(dens2)Alternatively there is the histogram - a coarse version of the kernel density. What you specifically talk about is binning your data. There are lots of rules/approaches to selecting equal-width bins (i.e. the number of bins) from the data. In R the default is Sturges rule, but it also includes the Freedman-Diaconis rule and Scott's rule. There are others as well - see the Wikipedia page on histograms.hist(dat)If you are not interested in the kernel density plot or the histogram per se, rather just the binned data, then you can compute the number of bins using the nclass.X family of functions where X is one of Sturges, scott or FD. And then use cut() to bin your data:cut.dat \u0026lt;- cut(dat, breaks = nclass.FD(dat), include.lowest = TRUE)table(cut.dat)which gives:\u0026gt; cut.dat  [-2e+03,2.21e+05] (2.21e+05,4.43e+05] (4.43e+05,6.65e+05] (6.65e+05,8.88e+05]                 247                  60                 215                  61 (8.88e+05,1.11e+06] (1.11e+06,1.33e+06] (1.33e+06,1.56e+06] (1.56e+06,1.78e+06]                 153                  51                 205                  50    (1.78e+06,2e+06]                  58in R.However, binning is fraught with problems, most notably; How do you know that your choice of bins hasn't influenced the resulting impression you get of the way the data are distributed?","Display_name":"Gavin Simpson","Creater_id":1390,"Start_date":"2010-11-09 06:00:47","Question_id":4341}
{"_id":{"$oid":"5837a579a05283111e4d4128"},"Last_activity":"2010-11-09 05:56:17","Creator_reputation":2345,"Question_score":6,"Answer_content":"I'll assume that you've already determined the number of categories you'll use. Let's say you want to use 20 categories. Then they will be:Category 1: [0 - 100,000)Category 2: [100,000 - 200,000)Category 3: [200,000 - 300,000)...Category 19: [1,800,000 - 1,900,000)Category 20: [1,900,000 - 2,000,000]Note that the label of each category can be defined as FLOOR (x / category_size) + 1This is trivial to define as a computed column in SQL or as a formula in Excel.Note that the last category is infinitesimally larger than the others, since it is closed on both sides. If you happen to get a value of exactly 2,000,000 you might erroneously classify it as falling into category 21, so you have to treat this exception with an ugly \"IF\" (in Excel) or \"CASE\" (in SQL).","Display_name":"Carlos Accioly","Creater_id":666,"Start_date":"2010-11-09 05:25:21","Question_id":4341}
{"_id":{"$oid":"5837a579a05283111e4d4135"},"Last_activity":"2016-08-20 07:23:00","Creator_reputation":21,"Question_score":2,"Answer_content":"Hmmm.  Lot's of issues here.a. Within-subject vs. between subject is quite different than fixed-vs-random.  Within subject is a design where ea. subject is exposed to all levels of the factor.  Between subject is where ea. subject is exposed to only one level of the factor.  This necessitates different modeling strategies which are largely orthogonal to the fixed-vs-random question.  b. The information inputed to the package, the design columns (or the design matrix), already code for whether a variable is within or between.  Consequently, the user need not worry about it.  I agree it does seem a bit magical at first, but if you think about it, you will see that the info is already there.c. While off topic, I would never argue that fixed-vs-random is moot in Bayesian.  They are the same in some cases and not the same in others.  For example, take a 2-by-2 ANOVA design.  If you dont include interactions, then I would argue whether you treat the factors as fixed or random is irrelevant.  But when you include the interactions it matters greatly.  See Rouder et al. (2012), JMP, http://pcl.missouri.edu/sites/default/files/Rouder.JMP_.2012.pdf","Display_name":"Jeff Rouder","Creater_id":128370,"Start_date":"2016-08-20 07:23:00","Question_id":230224}
{"_id":{"$oid":"5837a579a05283111e4d4136"},"Last_activity":"2016-08-17 02:09:48","Creator_reputation":16,"Question_score":0,"Answer_content":"In Bayesian Linear Mixed models, there is no distinction between random effects and fixed effects. This is a terminology inherited from the classical framework (which is often useful to understand in order to communicate the results). This is explained in Chapter 15 (page 383, footnote) of Gelman et al. book Bayesian data analysis.","Display_name":"Caf","Creater_id":127950,"Start_date":"2016-08-17 02:02:45","Question_id":230224}
{"_id":{"$oid":"5837a579a05283111e4d4143"},"Last_activity":"2016-08-20 07:20:17","Creator_reputation":5787,"Question_score":2,"Answer_content":"The eigenvalues of a covariance matrix are the variances in the independent coordinate frame. I.e, if there is a change of variables to rotate the covariance matrix to be aligned with the coordinate axes (so yes, the direction can be different), i.e., become a diagonal matrix, then these diagonal entries, which were the eigenvalues of the original covariance matrix, are its variances as well as its eigenvalues. Rotation preserves the eigenvalues.Your sample matrix is not positive semi-definite, and hence is not a valid covariance matrix. Among other things, a covariance matrix can not have any negative entries on the diagonal, but yours does.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-20 07:20:17","Question_id":230855}
{"_id":{"$oid":"5837a579a05283111e4d4152"},"Last_activity":"2016-08-20 06:13:22","Creator_reputation":61,"Question_score":6,"Answer_content":"I for one organize everything into 4 files for every project or analysis.(1) 'code' Where I store text files of R functions.(2) 'sql' Where I keep the queries used to gather my data.(3) 'dat' Where I keep copies (usually csv) of my raw and processed data.(4) 'rpt' Where I store the reports I've distributed.ALL of my files are named using very verbose names such as 'analysis_of_network_abc_for_research_on_modified_buffer_19May2011'I also write detailed documentation up front where I organize the hypothesis, any assumptions, inclusion and exclusion criteria, and steps I intend to take to reach my deliverable. All of this is invaluable for repeatable research and makes my annual goal setting process easier.","Display_name":"Will","Creater_id":4675,"Start_date":"2011-05-19 11:31:02","Question_id":10987}
{"_id":{"$oid":"5837a579a05283111e4d4153"},"Last_activity":"2013-06-25 10:42:31","Creator_reputation":5744,"Question_score":13,"Answer_content":"You are not the first person to ask this question.  Managing a statistical analysis project – guidelines and best practicesA workflow for RR Workflow: Slides from a Talk at Melbourne R Users by Jeromy Anglim (including another much longer list of webpages dedicated to R Workflow)My own stuff: Dynamic documents with R and LATEX as an important part of reproducible researchMore links to project organization: How to efficiently manage a statistical analysis project?","Display_name":"Bernd Weiss","Creater_id":307,"Start_date":"2011-05-19 13:53:08","Question_id":10987}
{"_id":{"$oid":"5837a579a05283111e4d4154"},"Last_activity":"2011-05-19 13:02:44","Creator_reputation":4963,"Question_score":2,"Answer_content":"Now that I've made the switch to Sweave I never want to go back.  Especially if you have plots as output, it's so much easier to keep track of the code used to create each plot.  It also makes it much easier to correct one minor thing at the beginning and have it ripple through the output without having to rerun anything manually.","Display_name":"Aaron","Creater_id":3601,"Start_date":"2011-05-19 13:02:44","Question_id":10987}
{"_id":{"$oid":"5837a579a05283111e4d4161"},"Last_activity":"2016-08-20 05:46:32","Creator_reputation":3619,"Question_score":0,"Answer_content":"From the help for anovaWhen given a sequence of objects,anova tests the models against one anotherin the order specified.In your first example your design is balanced so the order is immaterial but in the second example that is not true. Try doing both models with the order of terms reversed for further insight.You may want to investigate drop1","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-20 05:46:32","Question_id":230791}
{"_id":{"$oid":"5837a579a05283111e4d416d"},"Last_activity":"2016-08-20 05:45:29","Creator_reputation":19535,"Question_score":0,"Answer_content":"You can define the percentile in different way with respect to duplicate values.Let's choose a really simple corner case: we have 10 times the value 0.Then the empirical c.d.f. goes from 0 to 1 at 0. There is little mathematical reason to prefer the  to be 0 or 1. One could also define it as 0.5 but that causes just as much trouble as the other two. Thr most common choice (and in my experience the least problematic) is to choose the upper value. i.e. all observed values would be in the 100% quantile, which begins at 0 or larger, while values strictly less than 0 have the 0% quantile.A similar problem also exists with the Median (c.f. upper mefian, lower median).","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-08-20 05:45:29","Question_id":230827}
{"_id":{"$oid":"5837a579a05283111e4d417a"},"Last_activity":"2016-03-06 14:17:53","Creator_reputation":3636,"Question_score":4,"Answer_content":"One of the most thorough explanations of \"quasi-complete separation\" issues in maximum likelihood is Paul Allison's paper. He's writing about SAS software, but the issues he addresses are generalizable to any software:    Complete separation occurs whenever a linear function of x can generate perfect predictions of y  Quasi-complete separation occurs when (a) there exists some coefficient vector b such that bxi ≥ 0 whenever yi = 1,  and bxi ≤ 0* whenever **yi = 0 and this equality holds for at  least one case in each category of the dependent variable. In other  words in the simplest case, for any dichotomous independent variable  in a logistic regression, if there is a zero in the 2 × 2 table formed  by that variable and the dependent variable, the ML estimate for the  regression coefficient does not exist.  Allison discusses many of the solutions already mentioned including deletion of problem variables, collapsing categories, doing nothing, leveraging exact logistic regression, Bayesian estimation and penalized maximum likelihood estimation. http://www2.sas.com/proceedings/forum2008/360-2008.pdf","Display_name":"DJohnson","Creater_id":82102,"Start_date":"2015-11-26 10:08:10","Question_id":11109}
{"_id":{"$oid":"5837a579a05283111e4d417b"},"Last_activity":"2015-12-19 12:54:07","Creator_reputation":1,"Question_score":0,"Answer_content":"I am not sure that I agree with the statements in your question.I think that warning message means, for some of the observed X level in your data, the fitted probability is numerically 0 or 1. In other words, at the resolution, it shows as 0 or 1.You can run predict(yourmodel,yourdata,type='response') and you will find 0's or/and 1's there as predicted probabilities.As a result, I think it is ok to just use the results. ","Display_name":"Rwitch","Creater_id":98685,"Start_date":"2015-12-19 12:54:07","Question_id":11109}
{"_id":{"$oid":"5837a579a05283111e4d417c"},"Last_activity":"2015-08-10 04:36:00","Creator_reputation":18711,"Question_score":91,"Answer_content":"You've several options:Remove some of the bias.(a) By penalizing the likelihood as per @Nick's suggestion. Package logistf in R or the FIRTH option in SAS's PROC LOGISTIC implement the method proposed in Firth (1993), \"Bias reduction of maximum likelihood estimates\", Biometrika, 80,1.; which removes the first-order bias from maximum likelihood estimates. (Here @Gavin recommends the brglm package, which I'm not familiar with, but I gather it implements a similar approach for non-canonical link functions e.g. probit.)(b) By using median-unbiased estimates in exact conditional logistic regression. Package elrm or logistiX in R, or  the EXACT statement in SAS's PROC LOGISTIC.Exclude cases where the predictor category or value causing separation occurs. These may well be outside your scope; or worthy of further, focused investigation. (The R package safeBinaryRegression is handy for finding them.)Re-cast the model. Typically this is something you'd have done beforehand if you'd thought about it, because it's too complex for your sample size.(a) Remove the predictor from the model. Dicey, for the reasons given by @Simon: \"You're removing the predictor that best explains the response\".(b) By collapsing predictor categories / binning the predictor values. Only if this makes sense.(c) Re-expressing the predictor as two (or more) crossed factors without interaction. Only if this makes sense.Use a Bayesian analysis as per @Manoel's suggestion. Though it seems unlikely you'd want to just because of separation, worth considering on its other merits.The paper he recommends is Gelman et al (2008), \"A weakly informative default prior distribution for logistic \u0026amp; other regression models\", Ann. Appl. Stat., 2, 4: the default in question is an independent Cauchy prior for each coefficient, with a mean of zero \u0026amp; a scale of ; to be used after standardizing all continuous predictors to have a mean of zero \u0026amp; a standard deviation of . If you can elucidate strongly informative priors, so much the better.Do nothing. (But calculate confidence intervals based on profile likelihoods, as the Wald estimates of standard error will be badly wrong.) An often over-looked option. If the purpose of the model is just to describe what you've learnt about the relationships between predictors \u0026amp; response, there's no shame in quoting a confidence interval for an odds ratio of, say, 2.3 upwards. (Indeed it could seem fishy to quote confidence intervals based on unbiased estimates that exclude the odds ratios best supported by the data.) Problems come when you're trying to predict using point estimates, \u0026amp; the predictor on which separation occurs swamps the others.Use a hidden logistic regression model, as described in Rousseeuw \u0026amp; Christmann (2003),\"Robustness against separation and outliers in logistic regression\", Computational Statistics \u0026amp; Data Analysis, 43, 3, and implemented in the R package hlr. (@user603 suggests this.) I haven't read the paper, but they say in the abstract \"a slightly more general model is proposed under which the observed response is strongly related but not equal to the unobservable true response\", which suggests to me it mightn't be a good idea to use the method unless that sounds plausible.\"Change a few randomly selected observations from 1 to 0 or 0 to 1 among variables exhibiting complete separation\": @RobertF's comment. This suggestion seems to arise from regarding separation as a problem per se rather than as a symptom of a paucity of information in the data which might lead you to prefer other methods to maximum-likelihood estimation, or to limit inferences to those you can make with reasonable precision\u0026mdash;approaches which have their own merits \u0026amp; are not just \"fixes\" for separation. (Aside from its being unabashedly ad hoc, it's unpalatable to most that analysts asking the same question of the same data, making the same assumptions, should give different answers owing to the result of a coin toss or whatever.)","Display_name":"Scortchi","Creater_id":17230,"Start_date":"2013-09-01 08:39:35","Question_id":11109}
{"_id":{"$oid":"5837a579a05283111e4d417d"},"Last_activity":"2015-08-06 05:05:24","Creator_reputation":3256,"Question_score":32,"Answer_content":"This is an expansion of Scortchi and Manoel's answers, but since you seem to use R I thought I'd supply some code. :)I believe the easiest and most straightforward solution to your problem is to use a Bayesian analysis with non-informative prior assumptions as proposed by Gelman et al (2008). As Scortchi mentions, Gelman recommends to put a Cauchy prior with median 0.0 and scale 2.5 on each coefficient (normalized to have mean 0.0 and a SD of 0.5). This will regularize the coefficients and pull them just slightly towards zero. In this case it is exactly what you want. Due to having very wide tails the Cauchy still allows for large coefficients (as opposed to the short tailed Normal), from Gelman:How to run this analysis? Use the bayesglm function in arm package that implements this analysis!library(arm)set.seed(123456)# Faking some data where x1 is unrelated to y# while x2 perfectly separates y.d \u0026lt;- data.frame(y  =  c(0,0,0,0, 0, 1,1,1,1,1),                x1 = rnorm(10),                x2 = sort(rnorm(10)))fit \u0026lt;- glm(y ~ x1 + x2, data=d, family=\"binomial\")## Warning message:## glm.fit: fitted probabilities numerically 0 or 1 occurred summary(fit)## Call:## glm(formula = y ~ x1 + x2, family = \"binomial\", data = d)#### Deviance Residuals: ##       Min          1Q      Median          3Q         Max  ## -1.114e-05  -2.110e-08   0.000e+00   2.110e-08   1.325e-05  ## ## Coefficients:##               Estimate Std. Error z value Pr(\u0026gt;|z|)## (Intercept)    -18.528  75938.934       0        1## x1              -4.837  76469.100       0        1## x2              81.689 165617.221       0        1## ## (Dispersion parameter for binomial family taken to be 1)## ##     Null deviance: 1.3863e+01  on 9  degrees of freedom## Residual deviance: 3.3646e-10  on 7  degrees of freedom## AIC: 6## ## Number of Fisher Scoring iterations: 25Does not work that well... Now the Bayesian version:fit \u0026lt;- bayesglm(y ~ x1 + x2, data=d, family=\"binomial\")display(fit)## bayesglm(formula = y ~ x1 + x2, family = \"binomial\", data = d)##             coef.est coef.se## (Intercept) -1.10     1.37  ## x1          -0.05     0.79  ## x2           3.75     1.85  ## ---## n = 10, k = 3## residual deviance = 2.2, null deviance = 3.3 (difference = 1.1)Super-simple, no?ReferencesGelman et al (2008), \"A weakly informative default prior distribution for logistic \u0026amp; other regression models\", Ann. Appl. Stat., 2, 4http://projecteuclid.org/euclid.aoas/1231424214","Display_name":"Rasmus B\u0026#229;\u0026#229;th","Creater_id":6920,"Start_date":"2013-10-07 05:37:13","Question_id":11109}
{"_id":{"$oid":"5837a579a05283111e4d417e"},"Last_activity":"2013-11-04 08:52:42","Creator_reputation":8214,"Question_score":64,"Answer_content":"A solution to this is to utilize a form of penalized regression. In fact, this is the original reason some of the penalized regression forms were developed (although they turned out to have other interesting properties.Install and load package glmnet in R and you're mostly ready to go. One of the less user-friendly aspects of glmnet is that you can only feed it matrices, not formulas as we're used to. However, you can look at model.matrix and the like to construct this matrix from a data.frame and a formula...Now, when you expect that this perfect separation is not just a byproduct of your sample, but could be true in the population, you specifically don't want to handle this: use this separating variable simply as the sole predictor for your outcome, not employing a model of any kind.","Display_name":"Nick Sabbe","Creater_id":4257,"Start_date":"2011-05-22 04:14:19","Question_id":11109}
{"_id":{"$oid":"5837a579a05283111e4d417f"},"Last_activity":"2013-09-01 10:26:14","Creator_reputation":1280,"Question_score":0,"Answer_content":"Be careful with this warning message from R. Take a look at this blog post by Andrew Gelman, and you will see that it is not always a problem of perfect separation, but sometimes a bug with glm. It seems that if the starting values are too far from the maximum-likelihood estimate, it blows up. So, check first with other software, like Stata.If you really have this problem, you may try to use Bayesian modeling, with informative priors. But in practice I just get rid of the predictors causing the trouble, because I don't know how to pick an informative prior. But I guess there is a paper by Gelman about using informative prior when you have this problem of perfect separation problem. Just google it. Maybe you should give it a try.","Display_name":"Manoel Galdino","Creater_id":3058,"Start_date":"2011-05-22 17:00:20","Question_id":11109}
{"_id":{"$oid":"5837a579a05283111e4d418e"},"Last_activity":"2016-08-20 03:33:32","Creator_reputation":12907,"Question_score":0,"Answer_content":"The measures ME, RMSE, MAE, MPE, MAPE, and MASE reported in the model output are in-sample measures. They are not robust to overfitting as you can improve them simply by fitting a richer model. Therefore, they should not be central in guiding the model choice.Meanwhile, AIC, AICc and BIC are robust to overfitting, as long as you are not comparing too many models at once (see Hansen \"A winner’s curse for econometric models: on the joint distribution of in-sample fit and out-of-sample fit and its implications for model selection (2010)).AIC and AICc target one-step-ahead preditions. (AICc offers improvement over AIC in small samples, so you could just ignore AIC and stick to AICc.) If you want to select the model that should be better at forecasting (which seems to be your goal), look for the one with the lowest AIC and AICc values.Meanwhile, BIC may select the true model if it is among the candidate models. The true model need not be the one that predicts best (paradoxical as it may sound) but sometimes you are just interested in how the data was generated. Then look for a model with the lowest BIC value.However, for AIC, AICc and BIC to be directly comparable across models, you need the dependent variable to be exactly the same across the models. I suspect here it is not the case. The two models reported above include seasonal differencing; the seasonal periods differ (23 and 35). This way the model for the differenced data is fit on a longer time series in case of 23 than in case of 35.What you could to to circumvent this is cut the first 12 observations for the model with period 23. Then the AIC, AICc and BIC should be comparable.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-20 03:33:32","Question_id":229862}
{"_id":{"$oid":"5837a579a05283111e4d419b"},"Last_activity":"2016-08-20 01:37:19","Creator_reputation":1269,"Question_score":3,"Answer_content":"I skip the part about the possibility of errors in your codes because you haven't shown the codes. Concerning predictive performance of stepwise regression, you're not the first one to find that in a real world application, it performs better than one would expect: see here. However, there are good theoretical reasons why stepwise regression should tend to overfit the training data set, resulting in poor predictive performance. Thus the fact that on some specific data sets it does a good work, may well be due to chance. Have you addressed the possible high variance of the cross-validation estimator, by using repeated cross-validation? Have you tried other ways to estimate the generalization error, for example by using bootstrap? Even if the predictive performance happens to be good on a specific data set, I don't see why one would want to use stepwise regression today, when we have LASSO as a great way to estimate a sparse linear regression model (note that gamboostLSS doesn't estimate the linear regression model, but GAMLSS which is a much more complicated model). You cannot reliably make inference with stepwise regression, because the p-values don't have the standard interpretation. You could of course compute perfectly valid p-values for stepwise regression using sample splitting, but you would lose power that way. Instead, we have a significance test for LASSO which uses all the data.","Display_name":"DeltaIV","Creater_id":58675,"Start_date":"2016-08-20 00:22:29","Question_id":230809}
{"_id":{"$oid":"5837a579a05283111e4d41aa"},"Last_activity":"2016-08-20 01:57:33","Creator_reputation":12907,"Question_score":2,"Answer_content":"If you want to predict future values knowing only the current and past values, that is also how you specify the model. If the variable of interest is  and the variables that can be used for prediction are , you formulate the model as y_{t+1} = f(x_{1,t},\\dotsc,x_{K,t},\\dotsc,x_{1,t-p},\\dotsc,x_{K,t-p}) + \\varepsilon_{t+1} for some maximum lag , where  is the function the machine learning algorithm is trying to learn and  is something unpredictable. This way you can predict future  using data that is available today (at time ). When training your model, you will have a sample spanning . Then your  runs from  to  in the training sample. You can verify that by noticing that for  or  you would have to use data that you do not have within .Once you have formulated the model, training and prediction goes as usual.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-20 01:51:44","Question_id":230823}
{"_id":{"$oid":"5837a579a05283111e4d41bb"},"Last_activity":"2016-08-19 04:44:26","Creator_reputation":2088,"Question_score":2,"Answer_content":"you need to think about what makes a region similar, and produce an encoding that captures it. 2 things spring to mind: 'legal' hierarchies (council/county/state/ ...), geographic proximity.legal hierarchies could be done by more dummy variables ( and using regularisation eg L1/L2 regularisation ) to favour high level over low levels in hierarchy.geographic proximity - I am not so sure, one way would be using distances rather than dummy coding (ie input the distance from each region for each house).","Display_name":"seanv507","Creater_id":27556,"Start_date":"2016-08-19 04:44:26","Question_id":230636}
{"_id":{"$oid":"5837a579a05283111e4d41ca"},"Last_activity":"2016-08-19 23:40:02","Creator_reputation":424,"Question_score":8,"Answer_content":"Brief sketch of ARE for one-sample -test, signed test and the signed-rank testI expect the long version of @Glen_b's answer includes detailed analysis for two-sample signed rank test along with the intuitive explanation of the ARE. So I'll skip most of the derivation. (one-sample case, you can find the missing details in Lehmann TSH).Testing Problem: Let  be a random sample from location model , symmetric about zero. We are to compute ARE of signed test, signed rank test for the hypothesis  relative to t-test.To assess the relative efficiency of tests, only local alternatives are considered because consistent tests have power tending to 1 against fixed alternative. Local alternatives that give rise to nontrivial asymptotic power is often of the form  for fixed , which is called Pitman drift in some literature.Our task ahead isfind the limit distribution of each test statistic under the nullfind the limit distribution of each test statistic under the alternativecompute the local asymptotic power of each testTest statisics and asymptoticst-test (given the existence of )  t_n=\\sqrt{n}\\frac{\\bar{X}}{\\hat{\\sigma}}\\to_dN(0,1)\\quad \\text{under the null} t_n=\\sqrt{n}\\frac{\\bar{X}}{\\hat{\\sigma}}\\to_dN(h/\\sigma,1)\\quad \\text{under the alternative }\\theta=h/\\sqrt{n} so the test that rejects if  has asymptotic power function1-\\Phi\\left(z_\\alpha-h\\frac{1}{\\sigma}\\right)signed test \\sqrt{n}\\left(S_n-\\frac{1}{2}\\right)\\to_dN\\left(0,\\frac{1}{4}\\right)\\quad \\text{under the null }\\sqrt{n}\\left(S_n-\\frac{1}{2}\\right)\\to_dN\\left(hf(0),\\frac{1}{4}\\right)\\quad \\text{under the alternative } and has local asymptotic power1-\\Phi\\left(z_\\alpha-2hf(0)\\right)signed-rank test W_n=n^{-2/3}\\sum_{i=1}^{n}R_i1\\{X_i\u0026gt;0\\}\\to_dN\\left(0,\\frac{1}{3}\\right)\\quad \\text{under the null }W_n\\to_dN\\left(2h\\int f^2,\\frac{1}{3}\\right)\\quad \\text{under the alternative }and has local asymptotic power1-\\Phi\\left(z_\\alpha-\\sqrt{12}h\\int f^2\\right)Therefore, ARE(S_n)=(2f(0)\\sigma)^2ARE(W_n)=(\\sqrt{12}\\int f^2\\sigma)^2If  is standard normal density, , If  is uniform on [-1,1], , Remark on the derivation of distribution under the alternativeThere are of course many ways to derive the limiting distribution under the alternative. One general approach is to use Le Cam's third lemma. Simplified version of it states  Let  be the log of the likelihood ratio. For some statistic  , if    (W_n,\\Delta_n)\\to_d N\\left[\\left(\\begin{array}{c}\\mu\\\\-\\sigma^2/2\\end{array}\\right),\\left(\\begin{array}{cc}\\sigma^2_W \u0026amp; \\tau \\\\\\tau \u0026amp; \\sigma^2/2 \\end{array}\\right)\\right]\\\\  under the null, then W_n\\to_d N\\left(\\mu+\\tau,\\sigma^2_W\\right)\\quad\\text{under the alternative}For quadratic mean differentiable densities, local asymptotic normality and contiguity are automatically satisfied, which in turn implies Le Cam lemma. Using this lemma, we only need to compute  under the null.  obeys LAN \\Delta_n\\approx \\frac{h}{\\sqrt{n}}\\sum_{i=1}^{n}l(X_i)-\\frac{1}{2}h^2I_0 where  is score function,  is information matrix.Then, for instance, for signed test \\mathrm{cov}(\\sqrt{n}(S_n-1/2),\\Delta_n)=-h\\mathrm{cov}\\left(1\\{X_i\u0026gt;0\\},\\frac{f'}{f}(X_i)\\right)=h\\int_0^\\infty f'=hf(0)","Display_name":"Khashaa","Creater_id":42371,"Start_date":"2014-12-29 11:34:46","Question_id":130562}
{"_id":{"$oid":"5837a579a05283111e4d41cb"},"Last_activity":"2015-01-01 15:14:44","Creator_reputation":39281,"Question_score":3,"Answer_content":"This has nothing to do with explaining why  appears (which was explained nicely by others) but may help intuitively.  The Wilcoxon test is a -test on the ranks of  whereas the parametric test is computed on the raw data.  The efficiency of the Wilcoxon test with respect to the -test is the square of the correlation between the scores used for the two tests.  As  the squared correlation converges to .  You can easily see this empirically using R:n \u0026lt;- 1000000; x \u0026lt;- qnorm((1:n)/(n+1)); cor(1:n, x)^2; 3/pi[1] 0.9549402[1] 0.9549297n \u0026lt;- 100000000; x \u0026lt;- qnorm((1:n)/(n+1)); cor(1:n, x)^2; 3/pi[1] 0.9549298[1] 0.9549297","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2015-01-01 15:14:44","Question_id":130562}
{"_id":{"$oid":"5837a579a05283111e4d41cc"},"Last_activity":"2014-12-29 14:21:55","Creator_reputation":152503,"Question_score":4,"Answer_content":"Short version: The basic reason with the Wilcoxon-Mann-Whitney under a shift alternative is that finding the asymptotic relative efficiency (WMW/t) corresponds to evaluating  where  is the common density at the null and  is the common variance.So at the normal,  is effectively a scaled version of ; its integral will have a  term; when squared, that's the source of the .The same term - with the same integral - is involved in the ARE for the signed rank test, so it takes the same value.For the sign test relative to t, the ARE is ... and  again has a  in it.So essentially it's as I said in comments;  is in the ARE for the Wilcoxon-Mann-Whitney vs the two-sample t test, for the Wilcoxon signed rank test vs the one-sample t and the sign test vs the one-sample t test (in each case at the normal) quite literally because it appears in the normal density. Reference:J. L. Hodges and E. L. Lehmann (1956),\"The Efficiency of Some Nonparametric Competitors of the t-Test\",Ann. Math. Statist., 27:2, 324-335.  ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2014-12-29 08:59:32","Question_id":130562}
{"_id":{"$oid":"5837a579a05283111e4d41d8"},"Last_activity":"2016-08-19 23:35:32","Creator_reputation":1745,"Question_score":1,"Answer_content":"The first step is to plot the three histograms and see how much they overlap. If there is enough data to make a histogram (40+ samples) that can be used to  identify if the distribution in each category is normally distributed or not, and they are normally distributed then doing t-test for paired means and Levene's test for difference in variance (i.e., standard deviation squared) will be powerful and allow for determining whether or not there is an organized difference between the groups either with respect to location of the continuous variable or its variability. Doing these tests as one-sided tests would allow for identifying how they rank with respect to each other, e.g. A\u003eB\u003eC or not. If there are fewer than 40 samples in each group, or if there is no method by which one can normalize the data, for example by taking reciprocals, logarithms, etc., then one should use Wilcoxon for difference of location and Conover for difference of variance as those \"non-parametric\" tests rank the results and do not require normal conditions for their use.","Display_name":"Carl","Creater_id":99274,"Start_date":"2016-08-19 23:29:17","Question_id":131044}
{"_id":{"$oid":"5837a579a05283111e4d41d9"},"Last_activity":"2016-06-02 03:46:23","Creator_reputation":275,"Question_score":1,"Answer_content":"Very late answer but you might find this question, and this post helpful. Personally, I would just run a linear model with the categories represented as (ordered) integers, and examine the coefficient.There is also some nice guidance in the BMJ stats series.","Display_name":"drstevok","Creater_id":7746,"Start_date":"2016-06-02 03:38:56","Question_id":131044}
{"_id":{"$oid":"5837a579a05283111e4d41e6"},"Last_activity":"2016-08-19 23:33:27","Creator_reputation":2443,"Question_score":1,"Answer_content":"You should in general not use the unadjusted odds ratio from observational studies. If education were randomly allocated to subjects, an unadjusted (e.g. for key covariates) odds ratio would be only be slightly biased towards no effect and an unadjusted odds ratio may be okay. However, given that these studies presumably use observational data without random allocation to education levels, there are all sorts of selection biases etc. that typically make it impossible to attempt to say anything about the effect of education without some adjustments.How to correctly use adjusted odds ratios is another question. A simple network meta-analysis treating them as totally different interventions is not quite right, because some of the education levels are subsets or finer splits of others. However, if we know the number of each people at each (finely split) level of education in each study, then presumably we could write down a model for how they would combine. I could imagine that someone has already developed and programmed (and published) such a model, but if so, I am not aware of it.","Display_name":"Bj\u0026#246;rn","Creater_id":86652,"Start_date":"2016-08-19 23:33:27","Question_id":230595}
{"_id":{"$oid":"5837a579a05283111e4d41e7"},"Last_activity":"2016-08-19 04:28:04","Creator_reputation":3619,"Question_score":3,"Answer_content":"The gold standard when you have studies which have used different levels of the same explanatory variable is network meta-analysis (also known as multiple treatment comparison). If you do not want to delve into that level of complexity you have a number of more or less satisfactory ways of going forward. You could just select two levels and use them throughout while just ignoring all the other results. If you have all the raw frequencies you could, as you say, recompute unadjusted odds ratios for one comparison (say none versus more than none). If the unadjusted and adjusted ratios are close this might be convincing enough for your audience.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-19 04:28:04","Question_id":230595}
{"_id":{"$oid":"5837a579a05283111e4d41f8"},"Last_activity":"2016-08-19 22:29:34","Creator_reputation":8337,"Question_score":1,"Answer_content":"Negative numbers aren't a problem for PCA.In general, standardization will produce lots of negative numbers, because any value below the mean will be standardized to be negative.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-19 22:29:34","Question_id":230814}
{"_id":{"$oid":"5837a579a05283111e4d4205"},"Last_activity":"2016-08-19 22:12:59","Creator_reputation":527,"Question_score":1,"Answer_content":"I now sample this random voltage 10,000 times during the period of 1 second. With these samples recorded, is it possible to say with any confidence whatthe maximum voltage might be?You have a sequence  of i.i.d. random variable with . If I understood your question right, you are concerned with statistical properties of:Y_n = \\max\\{X_1,\\ldots,X_n\\}If the cdf of the  is differentiable (which is true when the density of  is normal) we have:f_{Y_n}(y) = nF_X(y)^{n-1}f_X(y),where I'm using  and  to denote the common cdf and density function of the . In your particular situation you have  as a  and  is the cumulative function of this density.I believe that it should be possible to determine something like it is \u0026lt;1.8 volts for 99% of the timeTo find  such that  you need to solve:\\int_{-\\infty}^{y_{\\alpha}}f_{Y_n}(y)dy = \\int_{-\\infty}^{y_{\\alpha}}nF_X(y)^{n-1}f_X(y)dy = \\alphaIf no analytical solution exist a numerical approximation would do the job.For the method just discussed you need values of  and . If you don't have them at hand estimating them with:\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^nX_i\\hat{\\sigma^2} = \\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\hat{\\mu})^2is usually the best option.","Display_name":"Mur1lo","Creater_id":120428,"Start_date":"2016-08-19 18:10:41","Question_id":230806}
{"_id":{"$oid":"5837a579a05283111e4d4206"},"Last_activity":"2016-08-19 17:23:29","Creator_reputation":8337,"Question_score":1,"Answer_content":"With 10,000 samples, the simple sample maximum should be quite a good estimate of the population maximum. However, if the variable in question is normally distributed, that means it isn't bounded above, so there is no maximum.  I believe that it should be possible to determine something like it is \u0026lt;1.8 volts for 99% of the timeThat would be the 99th percentile. Again, the sample 99th percentile is likely a good estimator in this situation. To find the sample 99th percentile, sort all your data points and then pick the value that's 99% of the way to the greatest (e.g., if you have 10,000 data points, pick the 9,900th).","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-19 17:23:29","Question_id":230806}
{"_id":{"$oid":"5837a579a05283111e4d4217"},"Last_activity":"2016-08-19 20:30:31","Creator_reputation":152503,"Question_score":0,"Answer_content":"The question you were asked is quite poorly worded; that doesn't help (in fact if a student wrote that way I'd be inclined to deduct marks for it). It seeks a test for a difference in means (\"average chemistry test score\"). The \"simple random sample of all students\" implies that the scores should be independent or close to it. Do you know of any tests for a difference in means with two independent samples?","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-19 20:30:31","Question_id":230818}
{"_id":{"$oid":"5837a579a05283111e4d4224"},"Last_activity":"2016-08-19 19:07:16","Creator_reputation":1745,"Question_score":2,"Answer_content":"What is needed is a probability distribution model that better fits the data. Sometimes, there are no defined moments. One such distribution is the Cauchy distribution. Although the Cauchy distribution has a median as an expected value, there is no stable mean value, and no stable higher moments. What this means is that when one collects data, actual measurements crop up that look like outliers, but are actual measurements. For example, if one has two normal distributions F and G, with mean zero, and one divides F/G, the result will have no first moment and is a Cauchy distribution. So we happily collect data, and it looks OK like 5,3,9,6,2,4 and we calculate a mean that looks stable, then, all of a sudden we get a -32739876 value and our mean value becomes meaningless, but note, the median is 4, stable. Such it is with long-tailed distributions. Find a more correct long tailed distribution for your data, and use the statistical measurements that that distribution implies, and your problem will go away.Edit: You might try Student's t-distribution with 2 degrees of freedom. That distribution has longer tails than the normal distribution, the skewness and kurtosis are unstable (Sic, do not exist), but the mean and variance are defined, i.e., are stable. Next edit: One possibility might be to use Theil regression. Anyway, it's a thought, because Theil will work well no matter what the tails look like. Theil can be done MLR (multiple linear regression using median slopes). I have never done Theil for histogram data fitting. But, I have done Theil with a jackknife variant to establish confidence intervals. The advantage of doing that is that Theil doesn't care what the distribution shapes are, and, the answers are generally less biased than with OLS because typically OLS is used when there is problematic independent axis variance. Not that Theil is totally unbaised, it is median slope. The answers have a different meaning as well, it finds a better agreement between the dependent and independent variables where OLS finds the least error predictor of the dependent variable, which latter is not always the question that we want an answer to. ","Display_name":"Carl","Creater_id":99274,"Start_date":"2016-01-08 14:32:00","Question_id":34565}
{"_id":{"$oid":"5837a579a05283111e4d4225"},"Last_activity":"2016-02-06 22:01:40","Creator_reputation":760,"Question_score":2,"Answer_content":"Have a look at heavy-tail Lambert W x F or skewed Lambert W x F  distributions a try (disclaimer: I am the author). In R they are implemented in the LambertW package.Related posts:What\u0026#39;s the distribution of these data?How to transform data to normality?One advantage over Cauchy or student-t distribution with fixed degrees of freedom is that the tail parameters can be estimated from the data -- so you can let the data decide what moments exist.  Moreover the Lambert W x F framework allows you to transform your data and remove skewness / heavy-tails.   Itt is important to note though that OLS does not require Normality of  or .  However, for your EDA it might be worthwhile.Here is an example of Lambert W x Gaussian estimates applied to equity fund returns.library(fEcofin)ret \u0026lt;- ts(equityFunds[, -1] * 100)plot(ret)The summary metrics of the returns are similar (not as extreme) as in OP's post.data_metrics \u0026lt;- function(x) {  c(mean = mean(x), sd = sd(x), min = min(x), max = max(x),     skewness = skewness(x), kurtosis = kurtosis(x))}ret.metrics \u0026lt;- t(apply(ret, 2, data_metrics))ret.metrics##          mean    sd    min   max skewness kurtosis## EASTEU 0.1300 1.538 -18.42 12.38   -1.855    28.95## LATAM  0.1206 1.468  -6.06  5.66   -0.434     4.21## CHINA  0.0864 0.911  -4.71  4.27   -0.322     5.42## INDIA  0.1515 1.502 -12.72 14.05   -0.505    15.22## ENERGY 0.0997 1.187  -5.00  5.02   -0.271     4.48## MINING 0.1315 1.394  -7.72  5.69   -0.692     5.64## GOLD   0.1098 1.855 -10.14  6.99   -0.350     5.11## WATER  0.0628 0.748  -5.07  3.72   -0.405     6.08Most series show clearly non-Normal characteristics (strong skewness and/or large kurtosis).  Let's Gaussianize each series using a heavy tailed Lambert W x Gaussian distribution (= Tukey's h) using a methods of momentsestimator (IGMM).library(LambertW)ret.gauss \u0026lt;- Gaussianize(ret, type = \"h\", method = \"IGMM\")colnames(ret.gauss) \u0026lt;- gsub(\".X\", \"\", colnames(ret.gauss))plot(ts(ret.gauss))The time series plots show much fewer tails and also more stable variation over time (not constant though).  Computing the metrics again on the Gaussianized time series yields:ret.gauss.metrics \u0026lt;- t(apply(ret.gauss, 2, data_metrics))ret.gauss.metrics##          mean    sd   min  max skewness kurtosis## EASTEU 0.1663 0.962 -3.50 3.46   -0.193        3## LATAM  0.1371 1.279 -3.91 3.93   -0.253        3## CHINA  0.0933 0.734 -2.32 2.36   -0.102        3## INDIA  0.1819 1.002 -3.35 3.78   -0.193        3## ENERGY 0.1088 1.006 -3.03 3.18   -0.144        3## MINING 0.1610 1.109 -3.55 3.34   -0.298        3## GOLD   0.1241 1.537 -5.15 4.48   -0.123        3## WATER  0.0704 0.607 -2.17 2.02   -0.157        3The IGMM algorithm achieved exactly what it was set forth to do: transform the data to have kurtosis equal to . Interestingly, all time series now have negative skewness, which is in line with most financial time series literature.  Important to point out here that Gaussianize() operates only marginally, not jointly (analogously to scale()).Simple bivariate regressionTo consider the effect of Gaussianization on OLS consider predicting \"EASTEU\" return from \"INDIA\" returns and vice versa.  Even though we are looking at same day returns between  on  (no lagged variables), it still provides value for a stock market prediction given the 6h+ time difference between India and Europe.layout(matrix(1:2, ncol = 2, byrow = TRUE))plot(ret[, \"INDIA\"], ret[, \"EASTEU\"])grid()plot(ret.gauss[, \"INDIA\"], ret.gauss[, \"EASTEU\"])grid()The left scatterplot of the original series shows that the strong outliers did not occur at the same days, but at different times in India and Europe; other than that it is not clear if the data cloud in the center supports no correlation or negative/positive dependency.  Since outliers strongly affect variance and correlation estimates, it is worthwhile to look at the dependency with heavy tails removed (right scatterplot).  Here the patterns are much more clear and the positive relation between India and Eastern Europe market becomes apparent.# try these models on your ownmod \u0026lt;- lm(EASTEU ~ INDIA * CHINA, data = ret)mod.robust \u0026lt;- rlm(EASTEU ~ INDIA, data = ret)mod.gauss \u0026lt;- lm(EASTEU ~ INDIA, data = ret.gauss)summary(mod)summary(mod.robust)summary(mod.gauss)Granger causalityA Granger causality test based on a   model (I use  to capture the week effect of daily trades) for \"EASTEU\" and \"INDIA\" rejects \"no Granger causality\" for either direction.library(vars)  mod.vars \u0026lt;- vars::VAR(ret[, c(\"EASTEU\", \"INDIA\")], p = 5)causality(mod.vars, \"INDIA\")Granger## ##  Granger causality H0: EASTEU do not Granger-cause INDIA## ## data:  VAR object mod.vars## F-Test = 4, df1 = 5, df2 = 3000, p-value = 0.003However, for the Gaussianized data the answer is different!  Here the test can not reject H0 that \"INDIA does not Granger-cause EASTEU\", but still rejects that \"EASTEU does not Granger-cause INDIA\".  So the Gaussianized data supports the hypothesis that European markets drive markets in India the following day.mod.vars.gauss \u0026lt;- vars::VAR(ret.gauss[, c(\"EASTEU\", \"INDIA\")], p = 5)causality(mod.vars.gauss, \"INDIA\")Granger## ##  Granger causality H0: EASTEU do not Granger-cause INDIA## ## data:  VAR object mod.vars.gauss## F-Test = 2, df1 = 5, df2 = 3000, p-value = 0.06Note that it is not clear to me which one is the right answer (if any), but it's an interesting observation to make.  Needless to say that this entire Causality testing is contingent on the  being the correct model -- which it is most likely not; but I think it serves well for illustratiton.","Display_name":"Georg M. Goerg","Creater_id":11476,"Start_date":"2016-02-06 22:01:40","Question_id":34565}
{"_id":{"$oid":"5837a579a05283111e4d4232"},"Last_activity":"2016-08-18 18:30:34","Creator_reputation":4307,"Question_score":0,"Answer_content":"This is not a full solution, but may help.According to extreme value theory, the CDF for the coordinate-wise maximum is the joint CDF evaluated \"isotropically\", i.e..Wikipedia gives a couple of references for numerically approximating a multivariate normal CDF:Botev, Z. I. (2016). \"The normal law under linear restrictions: simulation and estimation via minimax tilting\". Journal of the Royal Statistical Society: Series B (Statistical Methodology). doi:10.1111/rssb.12162.Genz, Alan (2009). Computation of Multivariate Normal and t Probabilities. Springer. ISBN 978-3-642-01689-9.","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-18 18:30:34","Question_id":230598}
{"_id":{"$oid":"5837a579a05283111e4d42c0"},"Last_activity":"2016-06-16 04:25:07","Creator_reputation":152503,"Question_score":18,"Answer_content":"With a complementary-log-log link function, it's not logistic regression -- the term \"logistic\" implies a logit link. It's still a binomial regression of course.  the estimate of time is 0.015. Is it correct to say the odds of mortality per unit time is multiplied by exp(0.015) = 1.015113 (~1.5% increase per unit time)No, because it doesn't model in terms of log-odds. That's what you'd have with a logit link; if you want a model that works in terms of log-odds, use a logit-link.The complementary-log-log link function says thatwhere .So  is not the odds ratio; indeed .Hence  and . As a result, if you need an odds ratio for some specific , you can compute one, but the parameters don't have a direct simple interpretation in terms of contribution to log-odds.Instead (unsurprisingly) a parameter shows (for a unit change in ) contribution to the complementary-log-log.As Ben gently hinted in his question in comments:  is it true to say that the probability of mortality per unit time (i.e. the hazard) is increased by 1.5% ?Parameters in the complementary log-log model do have a neat interpretation in terms of hazard ratio. We have that:, where  is the survival function.(So log-survival will drop by about 1.5% per unit of time in the example.)Now the hazard, , so indeed it seems that in the example given in the question, the probability of mortality* per unit of time is increased by about 1.5% *  (or for binomial models with cloglog link more generally, of )","Display_name":"Glen_b","Creater_id":805,"Start_date":"2015-01-08 00:29:24","Question_id":132627}
{"_id":{"$oid":"5837a579a05283111e4d42cc"},"Last_activity":"2016-08-19 07:28:35","Creator_reputation":5189,"Question_score":3,"Answer_content":"Since  is a vector,  is just , a scalar, for which 'positive definite' means 'positive'. And it is positive iff . But nothing prevents  from being  -- it's possible that . ","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-19 07:28:35","Question_id":230713}
{"_id":{"$oid":"5837a579a05283111e4d42d9"},"Last_activity":"2016-08-19 06:58:52","Creator_reputation":5445,"Question_score":3,"Answer_content":"There are common notations which can be used which make it very easy to know what is fixed and random, but the equation you posted does not adopt such a notation. In your case there is no way to tell, except for the residuals (which are very commonly denoted with  and since it is indexed the same way as the response variable, this makes it rather likely that is it a subject-level residual). Even the description beneath the variables does not help, so you have to rely on the text that follows on from there, where you find that  and  are also random ! Here is one easy notation scheme: denote fixed effects with ,  and  and random effects with lower case letters:  (typically reserved for subject residuals),  and .So a random intercepts model with 2  variables and a response  could be simply written as where  are the random intercepts and  are the subject residuals.To be more explicit, this type of model is often presented with indexing: Observations in each group are indexed with  and  referring to the th subject in the th group, where  ranges from  to  and  ranges from  to  where  is the total number of groups.Thus we have  is the response variable for the th subject in the th cluster. is the value of  for the th subject in the th cluster. is the value of  for the th subject in the th cluster.Then, the above random intercepts model is Y_{ij}= (\\beta_0 + u_{0j})+\\beta_1 X_{1ij} +\\beta_2 X_{2ij} +e_{ij},Since every subject has their own residual,  are indexed by , and since every group has it's own residual, these are indexed by  (where the  in the subscript corresponds to the zero in the  subscript because  is the random intercept and  is the global intercept).Using this notation we can easily introduce a random slope for  which will simply be another cluster-level residual,  y_{ij}= (\\beta_0 + u_{0j})+(\\beta_1 + u_{1j}) X_{1ij} +\\beta_2 X_{2ij} +e_{ij},and again for a random slope () for  Y_{ij}= (\\beta_0 + u_{0j})+(\\beta_1 + u_{1j}) X_{1ij} +(\\beta_2 +u_{2j}) X_{2ij} +e_{ij},Note how the subscripts on the beta-coefficient match those of the fixed effects  and  and the random effects  and We can further extend this with interaction, further  variables and further \"levels\". However it should be apparent that if we were to do that then this kind of notation quickly becomes difficult to work with, so there are 2 common ways to proceed.The first is to write:\\begin{align}\\\\Y_{ij}\u0026amp;= \\beta_{0j}+ \\beta_{1j} X_{1ij} + \\beta_{2j}X_{2ij} +e_{ij}\\\\ \\textrm{where}\\\\ \\beta_{0j}\u0026amp;=\\beta_0 + u_{0j}\\\\ \\beta_{1j}\u0026amp;=\\beta_1 + u_{1j}\\\\ \\beta_{2j}\u0026amp;=\\beta_{2} +u_{2j}\\end{align}and this is usually the approach taken by the multilevel and hierarchical linear modelling worlds (see for example the very well known book by Snjders and BoskerEdit: To address the comment, in the case of an interaction we would write:Y_{ij}= \\beta_{0j}+ \\beta_{1j} X_{1ij} + \\beta_{2j}X_{2ij}+ \\beta_{3}X_{1ij}X_{2ij} +e_{ij}where in this case only the fixed effect of the interaction is modelled. We could easily include a random slope for the interaction too, in the same way that we did for  and .A second way is to work in matrix form:\\mathbf{y}=\\mathbf{X\\beta} + \\mathbf{Z b} + \\mathbf{e}where  is the response vector,  is a design matrix for the fixed effects () and  is a block-diagonal design matrix for the random effects ().This is popular in the mixed effects world (see for example the book by Demidenko). To see how this notation works, we can partition the matrices for each group:\\begin{bmatrix} \\mathbf{y_1}\\\\ \\mathbf{y_2}\\\\ \\vdots \\\\ \\mathbf{y_J}\\end{bmatrix}= \\begin{bmatrix}X_1 \\\\ X_2 \\\\\\vdots \\\\ X_J \\end{bmatrix} \\begin{bmatrix}\\beta\\end{bmatrix}+\\begin{bmatrix}\\mathbf{Z_1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp;  \\mathbf{Z_2} \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \u0026amp;  \u0026amp; \\ddots  \u0026amp;  \\\\ 0 \u0026amp; 0  \u0026amp; 0 \u0026amp; \\mathbf{Z_J}\\end{bmatrix} +\\begin{bmatrix}b_1 \\\\ b_2  \\\\\\vdots \\\\ b_J \\end{bmatrix}+\\begin{bmatrix}e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_J\\end{bmatrix}where, in the case of the model without the interaction, but with random slopes for both  and , and  Note that for this model we have that  because we have random effects for all 3 fixed effects (intercept,  and ). If we had random slopes for only  then we would have:and if we had random intercepts only, we would have:","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-18 08:54:26","Question_id":229927}
{"_id":{"$oid":"5837a579a05283111e4d42e8"},"Last_activity":"2016-08-19 06:51:56","Creator_reputation":57702,"Question_score":0,"Answer_content":"You should use ordinal logistic regression, at least as a starting point.  If the proportional odds assumption is violated, you may need to go to some other method such as multinomial logisic. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-19 06:51:56","Question_id":230706}
{"_id":{"$oid":"5837a579a05283111e4d42f5"},"Last_activity":"2016-08-19 06:27:30","Creator_reputation":3354,"Question_score":0,"Answer_content":"No. Imagine the case where all the target values in a leaf are the same, there is no way to (striclty, as stressed by @whuber) reduce the error of this leaf.","Display_name":"RUser4512","Creater_id":73794,"Start_date":"2016-08-19 06:27:30","Question_id":230690}
{"_id":{"$oid":"5837a579a05283111e4d4302"},"Last_activity":"2016-08-19 06:26:20","Creator_reputation":57702,"Question_score":0,"Answer_content":"I don't know if there is a general answer that applies in all cases, but I think one big factor is that deviations are more noticeable in the tails. Suppose that you suspect that the variable in question should be normally distributed.  For concreteness, let's say it's height of adult human males. Now, in some reasonable sized sample (say N = 100) the Normal might predict a single person who  is over 6'3\".  But if there are none, or two, that will be very apparent.  On the other hand it might predict 10 people between 5'9 and 5'10\".  If there are 9, or 11, that won't be so apparent.It may also be that the actual distribution is not exactly normal.  But, again, deviations will be more apparent in the tails. set.seed(1234)  #Sets a random number seeddist1 \u0026lt;- rnorm(100,0, 1) #Random normal with N = 100, mean 0, sd 1dist2 \u0026lt;- c(dist1, 4) #Add a single extreme pointdist3 \u0026lt;- c(dist1, 0) #Add a point at the meanqqplot(dist1,dist2)qqplot(dist1, dist3)","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-19 06:26:20","Question_id":230693}
{"_id":{"$oid":"5837a579a05283111e4d430f"},"Last_activity":"2016-08-19 06:15:45","Creator_reputation":1139,"Question_score":0,"Answer_content":"You might want to look into association rules.  These are available in Statistics via the SPSSINC APRIORI extension command.  That requires the R Essentials for Statistics.","Display_name":"JKP","Creater_id":6768,"Start_date":"2016-08-19 06:15:45","Question_id":230329}
{"_id":{"$oid":"5837a57aa05283111e4d431d"},"Last_activity":"2016-08-19 05:58:55","Creator_reputation":12260,"Question_score":0,"Answer_content":"Since these are a time series, you have to be careful. If both time series are increasing (or decreasing) over the year, a straight-up correlation will be misleading. That's how various websites show clever correlations between things like coffee production in Peru and movies in which Kevin Bacon starred. Have you looked at this question in this forum?Would it be useful to aggregate to larger time periods? Have you already looked at the average usage per hour, per day, per month for each over the year? Are they reasonably close, just eyeballing it? Then there are tests for whether two groups have the same mean/variance, and it sounds like you have a paired example, that you can look at by week and by month.There are also time-series-specific comparison methods, as pionpi_ says in their answer.The key issue with time series is that they have autocorrelation: things tend to be the same from day to day and week to week, etc. If yesterday was hot, today will likely be hot. July temperatures are more like June temperatures than January temperatures. Etc. So each hour's electricity usage is not independent of the previous or next hour's usage.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-08-19 05:47:31","Question_id":230160}
{"_id":{"$oid":"5837a57aa05283111e4d431e"},"Last_activity":"2016-08-16 12:40:57","Creator_reputation":13,"Question_score":0,"Answer_content":"there are many measures of distance for time series data. see Fu 2011 and Liao 2005.","Display_name":"pionpi_","Creater_id":127808,"Start_date":"2016-08-16 12:40:57","Question_id":230160}
{"_id":{"$oid":"5837a57aa05283111e4d432b"},"Last_activity":"2015-11-11 14:47:37","Creator_reputation":147383,"Question_score":7,"Answer_content":"\"Covariance\" is used in many distinct senses.  It can bea property of a bivariate population,a property of a bivariate distribution,a property of a paired dataset, oran estimator of (1) or (2) based on a sample.Because any finite collection of ordered pairs  can be considered an instance of any one of these four things--a population, a distribution, a dataset, or a sample--multiple interpretations of \"covariance\" are possible.  They are not the same.  Thus, some non-mathematical information is needed in order to determine in any case what \"covariance\" means.In light of this, let's revisit three statements made in the two referenced posts:  If  are random vectors, then  is the matrix of elements This is complicated, because  can be viewed in two equivalent ways.  The context implies  and  are vectors in the same -dimensional real vector space and each is written , etc.  Thus \"\" denotes a bivariate distribution (of vectors), as in (2) above, but it can also be considered a collection of pairs , giving it the structure of a paired dataset, as in (3) above.  However, its elements are random variables, not numbers.  Regardless, these two points of view allow us to interpret \"\" ambiguously: would it be\\operatorname{Cov}(u,v) = \\frac{1}{n}\\left(\\sum_{i=1}^n u_i v_i\\right) - \\left(\\frac{1}{n}\\sum_{i=1}^n u_i\\right)\\left(\\frac{1}{n}\\sum_{i-1}^n v_i\\right),\\tag{1}which (as a function of the random variables  and ) is a random variable, or would it be the matrix\\left(\\operatorname{Cov}(u,v)\\right)_{ij} = \\operatorname{Cov}(u_i,v_j) = \\mathbb{E}(u_i v_j) - \\mathbb{E}(u_i)\\mathbb{E}(v_j),\\tag{2}which is an  matrix of numbers?  Only the context in which such an ambiguous expression appears can tell us which is meant, but the latter may be more common than the former.  If  are not random vectors, then  is the scalar .Maybe.  This assertion understands  and  in the sense of a population or dataset and assumes the averages of the  and  in that dataset are both zero.  More generally, for such a dataset, their covariance would be given by formula  above.Another nuance is that in many circumstances  represent a sample of a bivariate population or distribution.  That is, they are considered not as an ordered pair of vectors but as a dataset  wherein each  is an independent realization of a common random variable .  Then, it is likely that \"covariance\" would refer to an estimate of , such as\\operatorname{Cov}(u,v) = \\frac{1}{n-1}\\left(\\sum_{i=1}^n u_i v_i - \\frac{1}{n}\\left(\\sum_{i=1}^n u_i\\right)\\left(\\sum_{i-1}^n v_i\\right)\\right).This is the fourth sense of \"covariance.\"  If two vectors are not random, then their covariance is zero.This is an unusual interpretation.  It must be thinking of \"covariance\" in the sense of formula  above,\\left(\\operatorname{Cov}(u,v)\\right)_{ij} = \\operatorname{Cov}(u_i,v_j) = 0Each  and  is considered, in effect, a random variable that happens to be a constant.In a regression context (where vectors, numbers, and random variables all occur together) some of these distinctions are further elaborated in the thread on variance and covariance in the context of deterministic values. ","Display_name":"whuber","Creater_id":919,"Start_date":"2015-11-11 08:51:39","Question_id":179346}
{"_id":{"$oid":"5837a57aa05283111e4d432c"},"Last_activity":"2015-10-29 22:02:41","Creator_reputation":377,"Question_score":2,"Answer_content":"You have a misconception. Using the cov function on a pair of vectors computes the sample covariance of the two vectors. That is, it is assuming X and Y are vectors of observations of two one-dimensional random variables. That is why it doesn't give zero as an answer. What is zero (a matrix filled with zeroes, actually) is the covariance of two nonrandom numeric vectors. It would be silly to try to compute that covariance matrix though ^^EDIT: Here's an example, silliness and all:x \u0026lt;- matrix(rep(1:3,5), nrow=5, byrow=T)y \u0026lt;- matrix(rep(c(4.3,1.2,999),5), nrow=5, byrow=T)xycov(x,y) # All zeroes","Display_name":"Felipe Gerard","Creater_id":83177,"Start_date":"2015-10-29 21:35:28","Question_id":179346}
{"_id":{"$oid":"5837a57aa05283111e4d432d"},"Last_activity":"2015-10-29 21:08:49","Creator_reputation":100,"Question_score":1,"Answer_content":"Simply put a non-random vector has component elements that are constant -- they do not change by definition.  Since they do not change, there is no variability in their values.  Hence their variances are always zero.","Display_name":"3209Cigs","Creater_id":91444,"Start_date":"2015-10-29 21:08:49","Question_id":179346}
{"_id":{"$oid":"5837a57aa05283111e4d433a"},"Last_activity":"2016-08-19 05:50:05","Creator_reputation":3023,"Question_score":0,"Answer_content":"Assume you roll the dice, if you roll a one you win one euro, if you roll a two then you win 2 euro, ... if you roll six than you win six euro.  If the dice is fair then the probability to roll a one is 1/6, the same for rolling a two, ... the same for rolling a six. So for a dice we have . If you get one euro if you throw 1, two euro if you get two, ... then your ''expected win'' is  which is the expected value of , i.e. E(1X)=E(X)=3.51000XE(1000X)=1000E(X)=3500$. For sums it is simular, but you use two dice, one with an outcome X, the other one with an outcome Y. ","Display_name":"fcop","Creater_id":83346,"Start_date":"2016-08-19 05:50:05","Question_id":230672}
{"_id":{"$oid":"5837a57aa05283111e4d433b"},"Last_activity":"2016-08-19 05:13:39","Creator_reputation":216,"Question_score":0,"Answer_content":"To understand the meaning of E(-5X), I used the following R code:r=rnorm(100,5,5)library(sm)d=data.frame(r,-5*r)colnames(d)=c('newr','oldr')library(tidyr)do=gather(d)sm.density.compare(do[,2],as.factor(do[,1]))abline(v=5)text(28,.06,'old center')abline(v=-25)text(-50,.06,'new center')#new mean        mean(-5*r)To understand E(X+Y), I used the following R code:dist1=rnorm(100,5,5)dist2=rnorm(100,4,4)#expected value of E(X+Y)=E(X)+E(Y)=5+4=9XplusY=dist1+dist2d=data.frame(dist1,dist2,XplusY)colnames(d)=c('dist1','dist2','newdist')library(tidyr)do=gather(d)sm.density.compare(do[,2],as.factor(do[,1]))abline(v=9) #the center of multiplying values of two distsmean(XplusY)  #approximately 9","Display_name":"ahmedmar","Creater_id":100365,"Start_date":"2016-08-19 05:13:39","Question_id":230672}
{"_id":{"$oid":"5837a57aa05283111e4d4348"},"Last_activity":"2016-08-19 05:22:03","Creator_reputation":90,"Question_score":0,"Answer_content":"Did you modify the probability of dropout between the validation and training? This might be an important factor to explain the disparity in loss values.","Display_name":"LoulouChameau","Creater_id":127872,"Start_date":"2016-08-19 05:16:08","Question_id":230667}
{"_id":{"$oid":"5837a57aa05283111e4d4355"},"Last_activity":"2015-06-14 07:27:45","Creator_reputation":101,"Question_score":3,"Answer_content":"In order to be able to conduct a meta-analysis based on your data (this seems to me what you want to do), you first have to select what kind of effect size you want to use. If all your data has this structure you could for instance select log odds ratios as effect size measure. For more information about effect sizes see Borenstein, Hedges, Higgins, and Rothstein (2009). Introduction to meta-analysis, Chapter 5.Once you selected your effect size you have to compute for each study this effect size and its sampling variance. You could do this easily with for instance the metafor package in R.To give an example:ai = events_controlbi = no_events_controlci = events_activedi = no_events_activeAssuming that your effect sizes of interest are log odds ratios you can compute these and its sampling variances by library(metafor)ai \u0026lt;- c(25, 30)bi \u0026lt;- c(75, 120)ci \u0026lt;- c(38,45)di \u0026lt;- c(162, 355)es \u0026lt;- escalc(ai = ai, bi = bi, ci = ci, di = di, measure = \"OR\")\u0026gt; es      yi     vi1 0.3514 0.08582 0.6792 0.0667You could use then these log odds ratios (yi) and the sampling variances (vi) for conducting different types of meta-analyses. The code below gives a standard random-effects meta-analysis:\u0026gt; rma(yi = esvi)Random-Effects Model (k = 2; tau^2 estimator: REML)tau^2 (estimated amount of total heterogeneity): 0 (SE = 0.1079)tau (square root of estimated tau^2 value):      0I^2 (total heterogeneity / total variability):   0.00%H^2 (total variability / sampling variability):  1.00Test for Heterogeneity: Q(df = 1) = 0.7043, p-val = 0.4013Model Results:estimate       se     zval     pval    ci.lb    ci.ub            0.5358   0.1937   2.7657   0.0057   0.1561   0.9155       ** ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Another option to get the same output as presented above is not first computing the log odds ratios and its sampling variances via the escalc() function, but directly running the meta-analysis byrma(ai = ai, bi = bi, ci = ci, di = di, measure = \"OR\")For more information about meta-analysis please see the help functions of the metafor package or use the earlier mentioned book.","Display_name":"User33","Creater_id":79539,"Start_date":"2015-06-11 07:38:55","Question_id":156480}
{"_id":{"$oid":"5837a57aa05283111e4d4362"},"Last_activity":"2015-10-29 11:30:29","Creator_reputation":71,"Question_score":4,"Answer_content":"The agg function in MAd package actually can apply Borenstein et al. 2009 method for you. You just need to do:library(MAd)agg (id = id,es = es,var = var, cor=1,method = \"BHHR\", data = yourdata)With id as the column that contains the common name of all the rows you want to aggregate, and r as correlation. Depending on the type of study you are carrying out, you initial r would be different. Some people use r=0.5 as default. In my field (life sciences) I use r=1 when the non-independent outcomes are measurements across several time-points. What I would do is run the analysis with r=0, r=0.5 and r=1 and see if the conclusions are different. This is what some people would call a sensitivity analysis. ","Display_name":"fede_luppi","Creater_id":31979,"Start_date":"2015-10-29 10:46:57","Question_id":179242}
{"_id":{"$oid":"5837a57aa05283111e4d4370"},"Last_activity":"2016-08-19 04:57:43","Creator_reputation":3680,"Question_score":3,"Answer_content":"I think there are two potential sources of confusion here: (1) What the variance pertains to. (2) What kind of intervals are computed.(1) The variance is the predicted variance of the response and not the variance of the predicted mean. Thus, it does not correspond to predict(..., se.fit = TRUE) in an lm() regression. Instead it would correspond to the residual variance which is assumed to be constant in lm() regressions but not in beta regressions.(2) As the variance pertains to the response (see 1) you do should not use +/- 2 times the standard deviation to obtain a confidence interval. A normal approximation makes no sense here. Instead, it would be better to look at the, say, 2.5% and 97.5% quantiles of the predicted beta distribution. You can obtain these in betareg with predict(..., type = \"quantile\", at = c(0.025, 0.975)).Finally, if you are indeed looking for the equivalent of predict.lm(..., se.fit = TRUE) then betareg currently has no infrastructure for this. An alternative would be to bootstrap the predictions yourself. Or you could look at what the package lsmeans offers for betareg objects.","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2016-08-19 04:57:43","Question_id":230501}
{"_id":{"$oid":"5837a57aa05283111e4d437d"},"Last_activity":"2016-08-19 04:45:24","Creator_reputation":3680,"Question_score":2,"Answer_content":"I think your interpretation is correct - not sure why your colleague indicated that this is wrong. Unless I'm overlooking something, the only caveat in the interpretation is that this does not necessarily correspond to individual probabilities/odds due to the averaging within the region.Another useful approach to interpreting the output of betareg is to set up a new data frame with values of the regressor(s) that are \"of interest\" and then predict() the desired quantities, e.g., the expectation, variance, quantiles etc. And then you can graph these or present them in a table.For a worked example of this approach see Figure 2 in vignette(\"betareg\", package = \"betareg\") that shows the fitted mean of the response for varying the main regressor and keeping the other regressor fixed at a \"typical\" level. The constructed data frame with the values \"of interest\" is simply: data.frame(temp = 150:500, batch = \"6\"). See code chunk number 5 in edit(vignette(...)) for the exact replication code.","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2016-08-19 04:45:24","Question_id":230615}
{"_id":{"$oid":"5837a57aa05283111e4d438a"},"Last_activity":"2016-08-17 16:15:19","Creator_reputation":15542,"Question_score":2,"Answer_content":"For ATU, the weights on  would be w_i = \\begin{cases}      \\frac{1 - \\hat p(x_i)}{\\hat p(x_i)} \u0026amp; \\text{if}\\ d_i=1 \\\\      1 \u0026amp; \\text{if}\\ d_i=0,\\end{cases}where  is the binary treatment indicator.For ATT/ATET, the weights are w_i = \\begin{cases}      1 \u0026amp; \\text{if}\\ d_i=1 \\\\      \\frac{\\hat p(x_i)}{1-\\hat p(x_i)} \u0026amp; \\text{if}\\ d_i=0\\end{cases}For ATE, the weights are w_i = \\begin{cases}      \\frac{1}{\\hat p(x_i)} \u0026amp; \\text{if}\\ d_i=1 \\\\      \\frac{1}{1-\\hat p(x_i)} \u0026amp; \\text{if}\\ d_i=0\\end{cases}You can find these formulas derived on pages 67-69 of Micro-Econometrics for Policy, Program and Treatment Effects by Myoung-jae Lee, except that I broke them into two pieces here.Here's how I might do this in Stata, with native commands when possible and also by hand with a weighted regression of the outcome on a binary treatment dummy:clsset more offwebuse cattaneo2, clear/* (0) Get the phats */qui probit mbsmoke mmarried c.mage##c.mage fbaby medupredict double phat, pr/* (1a) ATE */teffects ipw (bweight) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), ate/* (1b) ATE By Hand */gen double ate_w =cond(mbsmoke==1,1/phat,1/(1-phat))reg bweight i.mbsmoke [pw=ate_w], vce(robust)/* (2a) ATT */teffects ipw (bweight) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), atet/* (2b) ATT by Hand */gen double att_w =cond(mbsmoke==1,1,phat/(1-phat))reg bweight i.mbsmoke [pw=att_w], vce(robust)/* (3) ATU by Hand Only */gen double atu_w =cond(mbsmoke==1,(1-phat)/phat,1)reg bweight i.mbsmoke [pw=atu_w], vce(robust)This gives the following three effects of maternal smoking on newborn weight:ATU = -231.8782 gramsATT = -225.1773 gramsATE  = -230.6886 grams","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-08-16 21:50:06","Question_id":230095}
{"_id":{"$oid":"5837a57aa05283111e4d4397"},"Last_activity":"2016-08-17 14:03:25","Creator_reputation":29935,"Question_score":9,"Answer_content":"This is because PCA scores are simply original data in a rotated coordinate frame.Below on the left I show some example 2D data (100 points in 2D) and on the right the corresponding PCA scores. The data cloud simply gets rotated clockwise by approximately 45 degrees.If it is not completely clear to you how one gets from the first subplot to the second one or why PCA amounts to rotation, take a look at our very informative thread Making sense of principal component analysis, eigenvectors \u0026amp; eigenvalues. In my answer there I am using exactly the same toy dataset as displayed here. Some other answers are very much worth reading too.Now, to your question.Clustering methods are usually based on Euclidean distances between points. The points that lie close to each other get clustered together; the ones that are far away get assigned to different clusters. As you can see above, all distances between all points stay exactly the same after PCA.Hence the identical clustering results. Here are both representations clustered with k-means with :As you see, the clustering results are identical.Can PCA make any difference at all?Yes. One can use it in two ways:Standardize all scores to unit variance; orUse only a subset of principal components, usually the ones that explain the most variance.Here is how it looks like in the same toy example. On the left I am using standardized scores (note how different the clusters become), on the right I am using only PC1.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-08-17 10:10:20","Question_id":230319}
{"_id":{"$oid":"5837a57aa05283111e4d43a4"},"Last_activity":"2016-08-19 04:03:52","Creator_reputation":26,"Question_score":1,"Answer_content":"This seems to me to be some kind of item position effect on the person side -- \"reverse test fatigue\" if you want. You might want to have a look at Davis and Ferdous (2005) and De Boeck and Wilson (2004).Also, there is a German-language dissertation on context effects in large-scale assessment by Sebastian Weirich (http://edoc.hu-berlin.de/dissertationen/weirich-sebastian-2015-07-13/PDF/weirich.pdf). Have a look at the appendices. You'll find the reference to Weirich, Hecht and Böhme (2014, APM), as well as an anglophone yet unpublished manuscript titled \"Item Position Effects are Moderated by Changes in Test-Taking Effort\" which might both be of interest to you.Davis, J. \u0026amp; Ferdous, A. (2005). Using item difficulty and item position to measure test fatigue. American Institutes for Research, Washington. Paris.De Boeck, P. \u0026amp; Wilson, M. (Hrsg.). (2004). Explanatory item response models: A generalized linear and nonlinear approach. New York: Springer.Weirich, S., Hecht, M., \u0026amp; Böhme, K. (2014). Modeling item position effects using generalized linear mixed models. Applied Psychological Measurement, 38(7), 535-548. doi: 10.1177/0146621614534955 ","Display_name":"user84164","Creater_id":84164,"Start_date":"2016-08-19 02:54:36","Question_id":230472}
{"_id":{"$oid":"5837a57aa05283111e4d43b1"},"Last_activity":"2016-08-19 03:47:26","Creator_reputation":6,"Question_score":0,"Answer_content":"Instead of doing univariate t-test you can look if your groups are different in multivariate analysis. Take a look at PLS-DA, it was developed for exactly this kind of medical questions and the results can be cross-validated to avoid false findings.","Display_name":"slakov","Creater_id":120897,"Start_date":"2016-08-19 03:47:26","Question_id":17803}
{"_id":{"$oid":"5837a57aa05283111e4d43b2"},"Last_activity":"2011-11-01 07:09:48","Creator_reputation":3506,"Question_score":2,"Answer_content":"This known as a multiplicity problem (a.k.a. alpha-inflation, and many other names).There are many methods to compensate for the fact you are testing many hypotheses, but first of all, you have to decide what is the measure of error you wish to control for. If you can allow for a proportion of false findings (chemicals found to differ when they actually do not), you should use the FDR measure. The most popular and simple procedure is the Benjamini-Hochberg procedure. If you wish to protect yourself from the possibility of any false finding, then look into FWE control. Some general purpose methods of correction are Bonferroni and Holm. If you are using R, look into the p.adjust function which also has good references. ","Display_name":"JohnRos","Creater_id":6961,"Start_date":"2011-11-01 07:09:48","Question_id":17803}
{"_id":{"$oid":"5837a57aa05283111e4d43bf"},"Last_activity":"2016-08-18 22:50:59","Creator_reputation":4307,"Question_score":1,"Answer_content":"Your check is not quite right: As you set the CDFs to be equal, then really you want to check that the quantiles match up (not the moments).So really you want the medians to match up. For a normal distribution (or any symmetric PDF), the mean and the median are the same. But for a Rayleigh PDF, the median is . And this is what you found in your check. So all is good!Note that in general, quantiles can be \"passed through\" a monotonic transform, whereas moments cannot (i.e. for ,  for any quantile , but  unless  is linear, and never for higher order moments).","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-18 22:50:59","Question_id":230623}
{"_id":{"$oid":"5837a57aa05283111e4d43d4"},"Last_activity":"2016-08-19 02:46:35","Creator_reputation":403,"Question_score":2,"Answer_content":"As Johnnyboycurtis has answerd, non-parametric methods are those if it makes no assumption on the population distribution or sample size to generate a model. A k-NN model is an example of a non-parametric model as it does not consider any assumptions to develop a model. A Naive Bayes or K-means is an example of parametric as it assumes a distribution for creating a model.For instance, K-means assumes the following to develop a model All clusters are spherical (i.i.d. Gaussian).All axes have the same distribution and thus variance.All clusters are evenly sized.As for k-NN, it uses the complete training set for prediction. It calculates the nearest neighbors from the test point for prediction. It assumes no distribution for creating a model. For more info:http://pages.cs.wisc.edu/~jerryzhu/cs731/stat.pdfhttp://stats.stackexchange.com/a/133841/86202http://stats.stackexchange.com/a/133694/86202","Display_name":"prashanth","Creater_id":86202,"Start_date":"2016-08-18 02:13:07","Question_id":230044}
{"_id":{"$oid":"5837a57aa05283111e4d43d5"},"Last_activity":"2016-08-16 09:20:20","Creator_reputation":738,"Question_score":1,"Answer_content":"So, I think you're missing a few points. First, and most importantly,   A statistical method is called non-parametric if it makes no  assumption on the population distribution or sample size.Here is a simple (applied) tutorial on some nonparmetric models: http://www.r-tutor.com/elementary-statistics/non-parametric-methodsA researcher may decide to use a nonparemtric model vs a parametric model, say, nonparamtric regression vs linear regression, is because the data violates assumptions held by the parametric model. Since you're coming from a ML background, I'll just assume you never learned the typical linear regression model assumptions. Here is a reference: https://statistics.laerd.com/spss-tutorials/linear-regression-using-spss-statistics.phpViolating assumptions can skew your parameter estimates, and ultimately increase the risk of invalid conclusions. A nonparametric model is more robust to outliers, nonlinear relationships, and does not depend on many population distribution assumptions, hence, can provide more trust worthy results when trying to make inferences or predictions.For a quick tutorial on nonparametric regression, I recommend these slides:http://socserv.socsci.mcmaster.ca/jfox/Courses/Oxford-2005/slides-handout.pdf","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-16 09:20:20","Question_id":230044}
{"_id":{"$oid":"5837a57aa05283111e4d43e2"},"Last_activity":"2016-08-19 02:02:46","Creator_reputation":506,"Question_score":1,"Answer_content":"Your question is somewhat similar to this Why doesn\u0026#39;t Random Forest handle missing values in predictors? You can modify the algorithm to take missing values into consideration as used in CART C4.5 implementation but it is adds computational cost because of modification that you make. Adding this computational cost may not outweigh the improvement \u0026amp; so imputation is generally preferred.","Display_name":"Nishad ","Creater_id":115514,"Start_date":"2016-08-19 02:02:46","Question_id":230638}
{"_id":{"$oid":"5837a57aa05283111e4d43ef"},"Last_activity":"2016-08-18 03:24:24","Creator_reputation":15,"Question_score":0,"Answer_content":"Well the coding answer IMHO is to use coxph on the Surv object, with the right hand side of the formula includes the factors and (maybe) other variables of interest. So something like ...   coxph.fit \u0026lt;- coxph(Surv(time2013,event2013) ~ \u0026lt;contract type\u0026gt; + \u0026lt;other variables\u0026gt;, data=mydata)   summary(coxph.fit)Where \u0026lt;...\u0026gt; indicates the actual factor/variable names.For just two levels of a factor the resulting summary compares one level with the reference level. The p-value and confidence interval gives you maybe some idea of 'enough-ness' (i.e. statistical significance), but then you are getting into stats theory.There's lots behind all this, so a bit of reading might be useful, to be aware of the assumptions and limitations.....https://www.openintro.org/stat/surv.php and maybe the guides here...http://www.ats.ucla.edu/stat/r/examples/asa/asa_ch3_r.htmOr follow a few threads here, to see the kinds of issues people have.","Display_name":"Big Old Dave","Creater_id":83548,"Start_date":"2016-08-18 03:14:54","Question_id":230644}
{"_id":{"$oid":"5837a57aa05283111e4d43fe"},"Last_activity":"2016-08-19 01:40:39","Creator_reputation":506,"Question_score":2,"Answer_content":"Actually both ways are valid \u0026amp; selection of the approach is based on performance on particular dataset.For competitions on Kaggle, winners have used both of them together as well to get small jump the leaderboard.Refer to below practical guide on model ensemble -http://mlwave.com/kaggle-ensembling-guide/The effectiveness of the method will vary from problem to problem but in most cases I have seen method2 being more effective than one.","Display_name":"Nishad ","Creater_id":115514,"Start_date":"2016-08-19 01:40:39","Question_id":230637}
{"_id":{"$oid":"5837a57aa05283111e4d440b"},"Last_activity":"2016-08-19 01:33:17","Creator_reputation":3619,"Question_score":2,"Answer_content":"The situation here is often covered under the heading of scoring rules. See for example this question and answer. One popular way of doing this is the Brier score. The Brier score is basically the average squared discrepancy between what you predicted (which is a proportion) and what did happen (which is 0 or 1).","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-19 01:33:17","Question_id":230499}
{"_id":{"$oid":"5837a57aa05283111e4d440c"},"Last_activity":"2016-08-18 07:24:14","Creator_reputation":67,"Question_score":1,"Answer_content":"You can use cross entropy.  See the wikipedia page especially the part on logistic regression.  The idea is if Boston wins you evaluate  and when Boston losses you evaluate  where  is the probability your model outputs.  You can combine this into one formula: .  is the actual result (win or loss).  You can use this to evaluate your model and use this as your loss function.","Display_name":"www3","Creater_id":93111,"Start_date":"2016-08-18 07:24:14","Question_id":230499}
{"_id":{"$oid":"5837a57aa05283111e4d4418"},"Last_activity":"2016-08-19 01:00:56","Creator_reputation":506,"Question_score":1,"Answer_content":"Please refer to CV wiki -https://en.wikipedia.org/wiki/Coefficient_of_variation#AlternativeFirst, CV may not be relevant for Celsius scale as it takes both positive \u0026amp; negative values. You can convert it to Kelvin \u0026amp; compute.There are alternatives provided on the same page","Display_name":"Nishad ","Creater_id":115514,"Start_date":"2016-08-19 01:00:56","Question_id":230639}
{"_id":{"$oid":"5837a57aa05283111e4d4427"},"Last_activity":"2016-08-19 00:53:31","Creator_reputation":25275,"Question_score":3,"Answer_content":"Using Bayes theorem is not the same as using Bayesian statistics. You are mixing two different things.If you knew what is the conditional probability of person's gender given his luck in lottery  and the unconditional probability distribution of winning , then you could apply Bayes theorem to compute . Notice that I did not use anywhere here terms such as prior, likelihood, or posterior, since they have nothing to do with such problems. (You could use naive Bayes classifier for such problems, but first is is not Bayesian since it does not use priors, and second you have insufficient data for it.)As your quote mentioned, in Bayesian approach we have prior, likelihood and posterior. Likelihood is a conditional distribution of data given some parameter. Prior is distribution of this parameter that you assume a priori before seeing the data. Posterior is the estimate given the data you have and your prior.To give concrete example illustrating it, let's assume that you have data about some coin since you threw it once and observed a head, let's call it . Obviously,  follows Bernoulli distribution parametrized by some parameter  that is unknown and we want to estimate it. We do not know what is , but we have likelihood function , that is probability mass function of Bernoulli distribution over  parametrized by . To learn about  Bayesian way, we assume prior for . Since we have no clue what  could be, we can decide to use weekly informative \"uniform\" Beta(1,1) prior. So out model becomes X \\sim \\mathrm{Bernoulli}(p) \\\\p \\sim \\mathrm{Beta}(\\alpha, \\beta) where  are parameters of beta distribution. Since beta is conjugate prior for Bernoulli distribution, we can easily compute the posterior distribution of  p \\sim \\mathrm{Beta}(\\alpha + 1, \\beta) and it's expected value E(p \\mid X) - \\frac{\\alpha + 1}{\\alpha + \\beta + 1} = 0.66   so given the data we have and assuming Beta(1,1) prior, expected value of  is .","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-19 00:46:20","Question_id":230629}
{"_id":{"$oid":"5837a57aa05283111e4d4438"},"Last_activity":"2016-06-08 14:41:55","Creator_reputation":5445,"Question_score":3,"Answer_content":"One reason to avoid such a transformation is that it will make the interpretation of the regression coefficient very difficult.Moreover, there is no requirement for independent variables to be normally distributed, and as a rule you should avoid doing so unless there are substantive reasons for it, such as a known nonlinear relationship, to deal with heteroscedasticity, or to help interpretation","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-06-08 14:32:48","Question_id":217941}
{"_id":{"$oid":"5837a57aa05283111e4d4447"},"Last_activity":"2016-04-07 04:20:26","Creator_reputation":20442,"Question_score":1,"Answer_content":"  Aren't we predicting the true conditional mean  rather than the actual realization ?Yes indeed we are.  If so, we are committing a measurement error when using   in place of the unobserved  when evaluating forecast accuracy. Isn't this problematic?On the one hand, you are right. We are forecasting an unobservable quantity and want to assess the accuracy of this forecast. We have a problem here.The apparently only way out is to assess the accuracy of forecasts based on observables, and then deal with the fact that our forecast accuracy inevitably again only is one realization of a random variable, by invoking asymptotic arguments.Now, whether and how this works for a given point forecast depends on what you want to use the point forecast for. Or, from a different perspective, it depends on your loss function.If your loss function is quadratic, then forecasting the conditional mean is the best you can do.If your loss function depends on the absolute error without squares involved, you want to forecast the conditional median.If your loss function involves \"skewed\" absolute errors, you want to forecast conditional quantiles. Loss functions for forecast errors have been a topic for quite a while now (Fildes \u0026amp; Makridakis, 1988, IJF). In the area I am most familiar with, forecasting retail sales, the conditional mean (quadratic loss) is most useful for planning promotions, whereas store replenishment requires high quantiles.Now, all the above relies on the fact that there always remains unexplained variance. (In some areas, like physics, we have a sufficiently good handle on the data generating process that we can explain almost all the variance and forecast extremely well, say, the trajectory of a bullet in vacuum.) People have been arguing that in non-physics situation, point forecasts alone are not overly helpful, and we should really aim for density forecasts, also known as predictive distributions. This ties into your parenthetical remark at the end of the question.This is commonly accepted in financial and macroeconomic forecasting (in finance, driven by Value at Risk and options pricing) - not so much in supply chain and sales forecasting, where people happily calculate conditional means, estimate variances and assume a homoskedastic normal distribution in setting safety stocks. I have argued that predictive distributions make more sense in supply chains, too. The problem is that evaluating a density forecast is a bit more involved than evaluating point forecasts. I give a few pointers in this earlier answer of mine.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2016-04-07 04:20:26","Question_id":206020}
{"_id":{"$oid":"5837a57aa05283111e4d4456"},"Last_activity":"2016-08-18 22:11:58","Creator_reputation":176,"Question_score":2,"Answer_content":"Let's recall the main aim of regularization is to reduce over fitting.What other techniques are currently being used to reduce over fitting:1) Weight sharing- as done in CNN's, applying the same filters across the image.2) Data Augmentation- Augmenting existing data and generate synthetic data with generative models3) Large amount of training data- thanks to ImageNet etc.4) Pre-training- For example say Use ImageNet learnt weights before training classifier on say Caltech dataset.5) The use of RelU's in Neural Nets by itself encourages sparsity as they allow for zero activations. In fact for more complex regions in feature space use more RelU's, deactivate them for simple regions. So basically vary model complexity based on problem complexity.Use of a bunch of such techniques in addition to dropout and early stopping seems sufficient for the problems being solved today. However for novel problems with lesser data you may find other regularization techniques useful.","Display_name":"Amitoz Dandiana","Creater_id":103889,"Start_date":"2016-08-18 22:01:09","Question_id":230544}
{"_id":{"$oid":"5837a57aa05283111e4d4465"},"Last_activity":"2016-08-17 05:34:52","Creator_reputation":3619,"Question_score":2,"Answer_content":"You could view this as a meta-analysis problem. Each bin gives you an estimate of the prevalence (which you might want to transform) and its standard error. Using the standard inverse variance procedure for meta-analysis you would then get an overall estimate and also an estimate of its heterogeneity. You might be interested in the CRAN Task View on MetaAnalysis for some hints. I use metafor but your mileage may vary. I have added the meta-analysis tag, feel free to edit it out if you disagree.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-17 05:34:52","Question_id":230206}
{"_id":{"$oid":"5837a57aa05283111e4d4478"},"Last_activity":"2016-08-18 18:35:53","Creator_reputation":15542,"Question_score":3,"Answer_content":"At first glance, both the propensity score matching DID (PSM DID) and the inverse probability weighting (IPW DID) that you want to do are sensible ways to approach this: matching and weighting will take care of the selection into treatment based on observables, and the DID will deal with selection on unobservables as long as the bias from it is time-invariant, conditional on covariates. Matching is just an attempt to approximate what weighting is doing directly. If you're just worried about covariate imbalance, you can include them as controls, as long as they are variables that affect both treatment and the outcome, and are not affected by the treatment itself or by anticipation of treatment. Also, as an aside, you can never test the parallel paths or trends/growth assumptions, even if you had more data. All you can say is that is seems to hold in the pre-treatment period, so you would be more willing to believe it holds in the post-treatment future. But is is still an assumption. In the example below, your IPW approach will produce estimates that match what Stata's own teffects ipw would give you for the ATET/ATT and ATE. I will also demonstrate some alternative matching methods that you might find useful as a robustness check. Why? With IPW, things can go south if the estimated propensity scores are near zero or 1 since you would be dividing by small numbers. This is not the case in this example, and so the estimates are fairly similar. Asymptotically, both PSM and IPW should give you the same answer (though I cannot recall where I saw this result). Below I will employ a user-written command that can do two-period Kernel PSM DID called diff. I will also do a regression version with the kernel-based weights from the command. Finally, you can also trick matching estimators, like user-written psmatch2 or Stata's own teffects ipw, to do DID with two periods, or you can also do a regression version of ipw.Here's the annotated example: . set more off. estimates clear. use http://fmwww.bc.edu/repec/bocode/c/CardKrueger1994.dta, clear(Dataset from Card\u0026amp;Krueger (1994)). /* Need to balance the panels so that all the methods use the same sample */. drop if missing(fte)(19 observations deleted). bys id (t): keep if _N==2(23 observations deleted). . /* (1) -diff- to get ATT */. diff fte, treated(treated) period(t) id(id) cov(bk kfc roys) kernel ktype(gaussian) support bw(0.05) cluster(id) robust // reportKERNEL PROPENSITY SCORE MATCHING DIFFERENCE-IN-DIFFERENCES    Estimation on common support    Matching iterations.................................................................................................................................................................................\u0026gt; ............................................................................................................................................DIFFERENCE-IN-DIFFERENCES ESTIMATION RESULTSNumber of observations in the DIFF-IN-DIFF: 778            Baseline       Follow-up   Control: 75             75          150   Treated: 314            314         628            389            389------------------------------------------------------ Outcome var.   | fte     | S. Err. |   t   |  P\u0026gt;|t|----------------+---------+---------+-------+---------Baseline        |         |         |       |    Control      | 20.471  |         |       |    Treated      | 17.069  |         |       |    Diff (T-C)   | -3.402  | 1.481   | -2.30 | 0.022**Follow-up       |         |         |       |    Control      | 17.792  |         |       |    Treated      | 17.518  |         |       |    Diff (T-C)   | -0.274  | 1.067   | -0.26 | 0.798                |         |         |       | Diff-in-Diff    | 3.128   | 1.388   | 2.25  | 0.025**------------------------------------------------------R-square:    0.02* Means and Standard Errors are estimated by linear regression**Robust Std. Errors**Clustered Std. Errors**Inference: *** p\u0026lt;0.01; ** p\u0026lt;0.05; * p\u0026lt;0.1. assert _support==1. . /* (2a) Regression Using Kernel Weights for ATE */. reg fte i.treated##i.t [aw=_weights], cluster(id) robust(sum of wgt is   1.2560e+03)Linear regression                               Number of obs     =        778                                                F(3, 388)         =       2.04                                                Prob \u0026gt; F          =     0.1075                                                R-squared         =     0.0193                                                Root MSE          =        9.5                                   (Std. Err. adjusted for 389 clusters in id)------------------------------------------------------------------------------             |               Robust         fte |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------     treated |         NJ  |  -3.401828   1.481415    -2.30   0.022    -6.314433   -.4892233         1.t |  -2.679224   1.307741    -2.05   0.041    -5.250369   -.1080794             |   treated#t |       NJ#1  |   3.128269   1.388281     2.25   0.025     .3987744    5.857763             |       _cons |    20.4711    1.39505    14.67   0.000     17.72829     23.2139------------------------------------------------------------------------------. xtset id t       panel variable:  id (strongly balanced)        time variable:  t, 0 to 1                delta:  1 unit. xtreg fte i.treated##i.t [aw=_weights], fe cluster(id) robustnote: 1.treated omitted because of collinearityFixed-effects (within) regression               Number of obs     =        778Group variable: id                              Number of groups  =        389R-sq:                                           Obs per group:     within  = 0.0376                                         min =          2     between = 0.0065                                         avg =        2.0     overall = 0.0000                                         max =          2                                                F(2,388)          =       2.57corr(u_i, Xb)  = -0.1104                        Prob \u0026gt; F          =     0.0781                                   (Std. Err. adjusted for 389 clusters in id)------------------------------------------------------------------------------             |               Robust         fte |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------     treated |         NJ  |          0  (omitted)         1.t |  -2.679224   1.306897    -2.05   0.041     -5.24871   -.1097388             |   treated#t |       NJ#1  |   3.128269   1.387385     2.25   0.025     .4005359    5.856002             |       _cons |   18.77018   .3468462    54.12   0.000     18.08825    19.45211-------------+----------------------------------------------------------------     sigma_u |  8.0179596     sigma_e |  6.8873514         rho |  .57541878   (fraction of variance due to u_i)------------------------------------------------------------------------------. . /* Create ATT and ATE Weights for regressions*/. /* You can install -xfill- by typing net from http://www.sealedenvelope.com/ and clicking on the name */. xfill _ps, i(id). gen double w_att = cond(treated==1,1,_ps/(1-_ps)). gen double w_ate = cond(treated==1,1/_ps,1/(1-_ps)). . /* (2b) Regression Using ATT and ATE IPW Weights */. reg fte i.treated##i.t [pw=w_att] /*if _support==1*/, cluster(id) robust(sum of wgt is   1.2560e+03)Linear regression                               Number of obs     =        778                                                F(3, 388)         =       1.48                                                Prob \u0026gt; F          =     0.2187                                                R-squared         =     0.0079                                                Root MSE          =     9.4359                                   (Std. Err. adjusted for 389 clusters in id)------------------------------------------------------------------------------             |               Robust         fte |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------     treated |         NJ  |  -1.897508   1.458586    -1.30   0.194    -4.765229    .9702128         1.t |  -2.182001   1.170245    -1.86   0.063    -4.482815     .118813             |   treated#t |       NJ#1  |   2.631046   1.259607     2.09   0.037     .1545361    5.107555             |       _cons |   18.96678   1.370783    13.84   0.000     16.27168    21.66187------------------------------------------------------------------------------. reg fte i.treated##i.t [pw=w_ate] /*if _support==1*/, cluster(id) robust(sum of wgt is   1.5560e+03)Linear regression                               Number of obs     =        778                                                F(3, 388)         =       1.53                                                Prob \u0026gt; F          =     0.2068                                                R-squared         =     0.0086                                                Root MSE          =     9.4409                                   (Std. Err. adjusted for 389 clusters in id)------------------------------------------------------------------------------             |               Robust         fte |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------     treated |         NJ  |  -2.014025   1.455524    -1.38   0.167    -4.875727    .8476772         1.t |  -2.247811   1.184075    -1.90   0.058    -4.575816    .0801949             |   treated#t |       NJ#1  |   2.712311   1.275497     2.13   0.034     .2045602    5.220061             |       _cons |    19.1994   1.367706    14.04   0.000     16.51036    21.88845------------------------------------------------------------------------------. xtreg fte i.treated##i.t [pw=w_att], fe cluster(id) robustnote: 1.treated omitted because of collinearityFixed-effects (within) regression               Number of obs     =        778Group variable: id                              Number of groups  =        389R-sq:                                           Obs per group:     within  = 0.0275                                         min =          2     between = 0.0065                                         avg =        2.0     overall = 0.0000                                         max =          2                                                F(2,388)          =       2.21corr(u_i, Xb)  = -0.1023                        Prob \u0026gt; F          =     0.1116                                   (Std. Err. adjusted for 389 clusters in id)------------------------------------------------------------------------------             |               Robust         fte |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------     treated |         NJ  |          0  (omitted)         1.t |  -2.182001   1.169489    -1.87   0.063    -4.481331    .1173282             |   treated#t |       NJ#1  |   2.631046   1.258794     2.09   0.037     .1561344    5.105957             |       _cons |   18.01802   .3146986    57.25   0.000     17.39929    18.63675-------------+----------------------------------------------------------------     sigma_u |  8.0031528     sigma_e |  6.6447787         rho |  .59194417   (fraction of variance due to u_i)------------------------------------------------------------------------------. xtreg fte i.treated##i.t [pw=w_ate], fe cluster(id) robustnote: 1.treated omitted because of collinearityFixed-effects (within) regression               Number of obs     =        778Group variable: id                              Number of groups  =        389R-sq:                                           Obs per group:     within  = 0.0287                                         min =          2     between = 0.0065                                         avg =        2.0     overall = 0.0000                                         max =          2                                                F(2,388)          =       2.28corr(u_i, Xb)  = -0.1037                        Prob \u0026gt; F          =     0.1032                                   (Std. Err. adjusted for 389 clusters in id)------------------------------------------------------------------------------             |               Robust         fte |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------     treated |         NJ  |          0  (omitted)         1.t |  -2.247811   1.183311    -1.90   0.058    -4.574314    .0786925             |   treated#t |       NJ#1  |   2.712311   1.274674     2.13   0.034     .2061786    5.218443             |       _cons |   18.19239   .3186684    57.09   0.000     17.56586    18.81892-------------+----------------------------------------------------------------     sigma_u |  8.0054923     sigma_e |  6.6977382         rho |  .58824522   (fraction of variance due to u_i)------------------------------------------------------------------------------. . /* (3) psmatch2 for ATE and ATT */. keep id fte treated t bk kfc roys. qui reshape wide fte, i(id treated bk kfc roys) j(t). gen did_fte = fte1 - fte0. psmatch2 treated bk kfc roys, outcome(fte0 fte1 did_fte) kernel kerneltype(normal) bw(0.05) common ateProbit regression                               Number of obs     =        389                                                LR chi2(3)        =       2.97                                                Prob \u0026gt; chi2       =     0.3956Log likelihood = -189.22405                     Pseudo R2         =     0.0078------------------------------------------------------------------------------     treated |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------          bk |   .1368372   .2190829     0.62   0.532    -.2925573    .5662318         kfc |   .4092482   .2579891     1.59   0.113    -.0964012    .9148976        roys |   .2448943   .2415267     1.01   0.311    -.2284893    .7182778       _cons |   .6744898   .1889631     3.57   0.000     .3041288    1.044851----------------------------------------------------------------------------------------------------------------------------------------------------------------------        Variable     Sample |    Treated     Controls   Difference         S.E.   T-stat----------------------------+-----------------------------------------------------------            fte0  Unmatched | 17.0692675   20.1733333  -3.10406582   1.21654594    -2.55                        ATT | 17.0692675   20.4710986  -3.40183108    1.4673544    -2.32                        ATU | 20.1733333   17.6299058  -2.54342757            .        .                        ATE |                          -3.23632912            .        .----------------------------+-----------------------------------------------------------            fte1  Unmatched | 17.5183121        17.65  -.131687898   1.11242821    -0.12                        ATT | 17.5183121   17.7918742  -.273562085   1.05193873    -0.26                        ATU |      17.65   18.0632041   .413204067            .        .                        ATE |                          -.141152158            .        .----------------------------+-----------------------------------------------------------         did_fte  Unmatched | .449044586  -2.52333333   2.97237792    1.1318176     2.63                        ATT | .449044586  -2.67922441     3.128269   1.35190664     2.31                        ATU |-2.52333333   .433298308   2.95663164            .        .                        ATE |                           3.09517696            .        .----------------------------+-----------------------------------------------------------Note: S.E. does not take into account that the propensity score is estimated.           | psmatch2: psmatch2: |   Common Treatment |  supportassignment | On suppor |     Total-----------+-----------+---------- Untreated |        75 |        75    Treated |       314 |       314 -----------+-----------+----------     Total |       389 |       389 . . /* (4) IPW Matching ATT=ATET and ATE */. teffects ipw (did_fte) (treated bk kfc roys, probit), atet vce(robust) // osample(ipw1)  Iteration 0:   EE criterion =  1.142e-28  Iteration 1:   EE criterion =  3.433e-31  Treatment-effects estimation                    Number of obs     =        389Estimator      : inverse-probability weightsOutcome model  : weighted meanTreatment model: probit------------------------------------------------------------------------------             |               Robust     did_fte |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------ATET         |     treated | (NJ vs PA)  |   2.631046   1.220372     2.16   0.031     .2391604    5.022931-------------+----------------------------------------------------------------POmean       |     treated |         PA  |  -2.182001   1.130429    -1.93   0.054      -4.3976    .0335979------------------------------------------------------------------------------. teffects ipw (did_fte) (treated bk kfc roys, probit), ate vce(robust) // osample(ipw2)  Iteration 0:   EE criterion =  1.147e-28  Iteration 1:   EE criterion =  1.002e-31  Treatment-effects estimation                    Number of obs     =        389Estimator      : inverse-probability weightsOutcome model  : weighted meanTreatment model: probit------------------------------------------------------------------------------             |               Robust     did_fte |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------ATE          |     treated | (NJ vs PA)  |   2.712311   1.238966     2.19   0.029      .283982     5.14064-------------+----------------------------------------------------------------POmean       |     treated |         PA  |  -2.247811   1.146997    -1.96   0.050    -4.495884    .0002625------------------------------------------------------------------------------To summarize, Kernel PSM DID and its regression analogue as well as PSM with psmatch2 on differences yield an ATT of 3.13 FTEs. The weights from this program are more involved than the IPW weights.The IPW on differences and the regression analogue you propose are similar to each other with an ATT of 2.63 FTEs, but smaller than the PSM based-estimates, though the differences are not significant.Code:clsset more offestimates clearuse http://fmwww.bc.edu/repec/bocode/c/CardKrueger1994.dta, clear/* Need to balance the panels so that all the methods use the same sample */drop if missing(fte)bys id (t): keep if _N==2/* (1) -diff- to get ATT */diff fte, treated(treated) period(t) id(id) cov(bk kfc roys) kernel ktype(gaussian) support bw(0.05) cluster(id) robust // reportassert _support==1/* (2a) Regression Using Kernel Weights for ATE */reg fte i.treated##i.t [aw=_weights], cluster(id) robustxtset id txtreg fte i.treated##i.t [aw=_weights], fe cluster(id) robust/* Create ATT and ATE Weights for regressions*//* You can install -xfill- by typing net from http://www.sealedenvelope.com/ and clicking on the name */xfill _ps, i(id)gen double w_att = cond(treated==1,1,_ps/(1-_ps))gen double w_ate = cond(treated==1,1/_ps,1/(1-_ps))/* (2b) Regression Using ATT and ATE IPW Weights */reg fte i.treated##i.t [pw=w_att] /*if _support==1*/, cluster(id) robustreg fte i.treated##i.t [pw=w_ate] /*if _support==1*/, cluster(id) robustxtreg fte i.treated##i.t [pw=w_att], fe cluster(id) robustxtreg fte i.treated##i.t [pw=w_ate], fe cluster(id) robust/* (3) psmatch2 for ATE and ATT */keep id fte treated t bk kfc roysqui reshape wide fte, i(id treated bk kfc roys) j(t)gen did_fte = fte1 - fte0psmatch2 treated bk kfc roys, outcome(fte0 fte1 did_fte) kernel kerneltype(normal) bw(0.05) common ate/* (4) IPW Matching ATT=ATET and ATE */teffects ipw (did_fte) (treated bk kfc roys, probit), atet vce(robust) // osample(ipw1)  teffects ipw (did_fte) (treated bk kfc roys, probit), ate vce(robust) // osample(ipw2)  ","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-08-05 00:00:25","Question_id":228367}
{"_id":{"$oid":"5837a57aa05283111e4d4485"},"Last_activity":"2013-10-23 07:34:28","Creator_reputation":241,"Question_score":12,"Answer_content":"It is a measure of precision just as  is a measure of dispersion. More elaborately,  is a measure of how the variables are dispersed around the mean (the diagonal elements) and how they co-vary with other variables (the off-diagonal) elements. The more the dispersion the farther apart they are from the mean and the more they co-vary (in absolute value) with the other variables the stronger is the tendency for them to 'move together' (in the same or opposite direction depending on the sign of the covariance). Similarly,   is a measure of how tightly clustered the variables are around the mean (the diagonal elements) and the extent to which they do not co-vary with the other variables (the off-diagonal elements). Thus, the higher the diagonal element, the tighter the variable is clustered around the mean. The interpretation of the off-diagonal elements is more subtle and I refer you to the other answers for that interpretation.","Display_name":"prop","Creater_id":31126,"Start_date":"2013-10-22 06:42:11","Question_id":73463}
{"_id":{"$oid":"5837a57aa05283111e4d4486"},"Last_activity":"2013-10-22 10:57:30","Creator_reputation":1593,"Question_score":14,"Answer_content":"Using superscripts to denote the elements of the inverse,  is the variance of the component of variable  that is uncorrelated with the  other variables, and  is the partial correlation of variables  and , controlling for the  other variables.","Display_name":"Ray Koopman","Creater_id":20776,"Start_date":"2013-10-22 10:57:30","Question_id":73463}
{"_id":{"$oid":"5837a57aa05283111e4d4493"},"Last_activity":"2016-08-18 17:58:46","Creator_reputation":44,"Question_score":1,"Answer_content":"Connections/Dominance between distances is not that straightforward to evaluate. In particular, there does not seem to be an obvious connection between the energy distance and the  distance. See:  On choosing and bounding probability metrics.","Display_name":"Luck","Creater_id":128185,"Start_date":"2016-08-18 17:58:46","Question_id":230601}
{"_id":{"$oid":"5837a57aa05283111e4d44a2"},"Last_activity":"2016-08-18 17:24:42","Creator_reputation":7682,"Question_score":4,"Answer_content":"In the  operation,  is an  matrix, and  an . Hence, the matrix multiplication will yield a  matrix.Addressing the comments and the underlying issue, let's pretend that we have a matrix corresponding to returns of different stocks (in the columns) versus 5 consecutive years (in the rows) - completely fictitious stocks and years. Let's call the matrix, :A = \\begin{bmatrix}       \u0026amp; \\color{red}{\\text{yah(y)}}  \u0026amp; \\color{blue}{\\text{goog(g)}}  \u0026amp;  \\color{green}{\\text{ms(m)}} \\\\    \\text{Yr.1} \u0026amp;  1 \u0026amp;8 \u0026amp; 1\\\\    \\text{Yr.2} \u0026amp; -4 \u0026amp;9 \u0026amp; 3 \\\\    \\text{Yr.3} \u0026amp; 5 \u0026amp;  10 \u0026amp; 4 \\\\    \\text{Yr.4} \u0026amp; 7 \u0026amp; 3 \u0026amp; 5\\\\    \\text{Yr.5} \u0026amp; 8 \u0026amp; 7\u0026amp; 6\\end{bmatrix}We want to calculate the correlations between the different vectors of returns, one for each company, \"packaged\" in the matrix .The variance-covariance matrix of the portfolio assuming equal holdings will be: with  being the mean-centered observations and  corresponding to the number of observations minus .The mean-centered (or demeaned) matrix  is:\\begin{bmatrix}       \u0026amp; \\color{red}{\\text{y}}  \u0026amp; \\color{blue}{\\text{g}}  \u0026amp;  \\color{green}{\\text{m}} \\\\    \\text{Yr.1} \u0026amp;  -2.4 \u0026amp;0.6 \u0026amp; -2.8\\\\    \\text{Yr.2} \u0026amp; -7.4 \u0026amp;1.6 \u0026amp; -0.8 \\\\    \\text{Yr.3} \u0026amp; 1.6 \u0026amp;  2.6 \u0026amp; 0.2 \\\\    \\text{Yr.4} \u0026amp; 3.6 \u0026amp; -4.4 \u0026amp; 1.2\\\\    \\text{Yr.5} \u0026amp; 4.6 \u0026amp; -0.4\u0026amp; 2.2\\end{bmatrix}And the variance-covariance matrix:\\begin{bmatrix}       \u0026amp; \\color{red}{y}  \u0026amp; \\color{blue}{g}  \u0026amp;  \\color{green}{m} \\\\    \\color{red}{y}  \u0026amp;  24.30 \u0026amp;-6.70 \u0026amp; 6.85\\\\    \\color{blue}{g} \u0026amp; -6.70 \u0026amp; 7.30 \u0026amp; -2.15 \\\\    \\color{green}{m} \u0026amp;   6.85 \u0026amp; -2.15 \u0026amp; 3.70 \\\\\\end{bmatrix}So it went from the   matrix to a  matrix.The operations involved in calculating the correlation matrix are similar, but the data points are standardized by dividing each one by the standard deviation of the returns of each company (column vectors), right after centering the data points by subtracting the column means as in the covariance matrix:\\small cor(A)=\\tiny\\frac{1}{n-1}\\small\\begin{bmatrix}\\frac{\\color{red}{y_1 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp; \\frac{\\color{red}{y_2 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp; \\frac{\\color{red}{y_3 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp; \\frac{\\color{red}{y_4 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp;\\frac{\\color{red}{y_5 - \\bar{y}}}{\\color{red}{sd(y)}} \\\\\\frac{\\color{blue}{g_1 - \\bar{g}}}{\\color{blue}{sd(g)}} \u0026amp; \\frac{\\color{blue}{g_2 - \\bar{g}}}{\\color{blue}{sd(g)}} \u0026amp; \\frac{\\color{blue}{g_3 - \\bar{g}}}{\\color{blue}{sd(g)}} \u0026amp; \\frac{\\color{blue}{g_4 - \\bar{g}}} {\\color{blue}{sd(g)}}\u0026amp; \\frac{\\color{blue}{g_5 - \\bar{g}}}{\\color{blue}{sd(g)}}\\\\\\frac{\\color{green}{m_1 - \\bar{m}}}{\\color{green}{sd(m)}}\u0026amp; \\frac{\\color{green}{m_2 - \\bar{m}}}{\\color{green}{sd(m)}}  \u0026amp;\\frac{\\color{green}{m_3 - \\bar{m}}}{\\color{green}{sd(m)}}  \u0026amp; \\frac{\\color{green}{m_4 - \\bar{m}}}{\\color{green}{sd(m)}}  \u0026amp; \\frac{\\color{green}{m_5 - \\bar{m}}}{\\color{green}{sd(m)}}\\\\\u0026amp;\u0026amp;\\color{purple} {3\\times 5 \\,\\text{matrix}}\\end{bmatrix}\\begin{bmatrix}\\frac{\\color{red}{y_1 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp; \\frac{\\color{blue}{g_1 - \\bar{g}}}{\\color{blue}{sd(g)}} \u0026amp; \\frac{\\color{green}{m_1 - \\bar{m}}}{\\color{green}{sd(m)}} \\\\\\frac{\\color{red}{y_2 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp; \\frac{\\color{blue}{g_2 - \\bar{g}}}{\\color{blue}{sd(g)}} \u0026amp; \\frac{\\color{green}{m_2 - \\bar{m}}}{\\color{green}{sd(m)}} \\\\\\frac{\\color{red}{y_3 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp;\\frac{\\color{blue}{g_3 - \\bar{g}}}{\\color{blue}{sd(g)}} \u0026amp; \\frac{\\color{green}{m_3 - \\bar{m}}}{\\color{green}{sd(m)}} \\\\\\frac{\\color{red}{y_4 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp; \\frac{\\color{blue}{g_4 - \\bar{go}}}{\\color{blue}{sd(g)}} \u0026amp; \\frac{\\color{green}{m_4 - \\bar{m}}}{\\color{green}{sd(m)}} \\\\\\frac{\\color{red}{y_5 - \\bar{y}}}{\\color{red}{sd(y)}} \u0026amp; \\frac{\\color{blue}{g_5 - \\bar{g}}}{\\color{blue}{sd(g)}} \u0026amp; \\frac{\\color{green}{m_5 - \\bar{m}}}{\\color{green}{sd(m)}} \\\\\u0026amp;\\color{purple} {5\\times 3 \\,\\text{matrix}}\\end{bmatrix}One more quick thing for completeness sake: We have so far a clunky matrix as the result, but in general we want to estimate the portfolio variance:  portfolio;  variance. To do that we multiply the matrix of variance-covariance of  to the left and to the right by the vector containing the proportions or weightings in each stock - . Since we want to end up with a scalar single number, it is unsurprising that the algebra will be: , with the vector of weights (fractions) being in this case  to match perfectly on the left as , and on the right as .Code in R:Fictitious data set of returns in billions, percentage (?) - the matrix A:yah = c(1, - 4, 5, 7, 8)goog = c(8, 9, 10, 3, 7)ms = c(1, 3, 4, 5, 6)returns \u0026lt;- cbind(yah, goog, ms)row.names(returns) =c(\"Yr.1\",\"Yr.2\",\"Yr.3\",\"Yr.4\", \"Yr.5\")Centered matrix (G) of demeaned returns:demeaned_returns \u0026lt;- scale(returns, scale = F, center = T)Manual and R function calculation of the variance-covariance matrix:(var_cov_A = (t(demeaned_returns)%*%demeaned_returns)/(nrow(returns)-1))cov(returns)   # the R in-built function cov() returns the same results.Correlation matrix calculation:We need to divide by the standard deviation column-wise:demeaned_scaled_returns \u0026lt;- scale(returns, scale = T, center = T)and then proceed as above:(corr_A = (t(demeaned_scaled_returns) %*% demeaned_scaled_returns)/(nrow(returns)-1))cor(returns) # Again, the R function returns the same matrix.","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-02-01 10:46:12","Question_id":193502}
{"_id":{"$oid":"5837a57aa05283111e4d44a3"},"Last_activity":"2016-02-01 09:55:17","Creator_reputation":384,"Question_score":18,"Answer_content":"Correlation is calculated between columns and not between rows.The output should be read as, correlation between column-i and column j.Since you have 6 columns, you get a 6x6 correlation matrix. All 8 rows have been considered while calculating these correlations.","Display_name":"Bach","Creater_id":28897,"Start_date":"2016-02-01 09:55:17","Question_id":193502}
{"_id":{"$oid":"5837a57aa05283111e4d44b0"},"Last_activity":"2016-08-18 14:05:59","Creator_reputation":5787,"Question_score":2,"Answer_content":"This can be formulated as a Nonlinear SDP (Semidefinite Program) using YALMIP, and solved by YALMIP calling PENLAB as a solver.It is a non-trivial problem. It is non-convex due to multiplication of optimization variables, specifically involving  and .  If  were assumed known, and  held at a fixed value, it could be formulated and easily solved in CVX or YALMIP as a convex Linear SDP (then an overarching 1-D search on  could be conducted, with a convex Linear SDP solved at each value of ).  For the remainder of this answer, I will presume  is not known, and show the formulation and solution as a nonlinear SDP.Regardless of whether an exact solution to all equations and constraints can be found, the problem can be formulated as an optimization problem having an objective function which is a non-convex function, which could be some (weighted sums of) norm of differences between LHS and RHS of the variosu equations, subject to semidefinite constraints on covariance matrices being solved for, and any other constraints. The covariance matrix optimization variables are declared so as to be symmetric, and the semidefinite constraint imposed on them is that the matrix is positive semi-definite (PSD), which is equivalent to it being a covariance matrix, hence the semidefinite constraint constrains the matrix to be a valid covariance matrix (i.e., symmetric PSD).Because the problem is non-convex, the solution you obtain may not be globally optimal, and may depend on the initial (starting values) you can provide for the optimization variables (unknowns). The initial values which you said in a comment you can obtain via basic estimation of the factor model, could help a lot.Below is a sample YALMIP formulation. To enter it, you will need to have installed YALMIP http://users.isy.liu.se/johanl/yalmip/, and to run it (the optimize command), you will need to have obtained and installed PENLAB http://web.mat.bham.ac.uk/kocvara/penlab/ . To get started on YALMIP, read http://users.isy.liu.se/johanl/yalmip/pmwiki.php?n=Tutorials.Basics  and follow links. You can get help from YALMIP's developer at https://groups.google.com/forum/#!forum/yalmip .The comments provide some explanations.  You may wish to change the formulation of the objective function, for instance how to combine the norms of the various errors, as well as adding or deleting constraints as appropriate. But it should get you started.  I chose the frobenius norm for matrices and the 2-norm for vectors.  At some point you may wish to set PENLAB options to non-default values, which can be done by adding fields to sdpsettings.  No guarantees that I didn't made any typos in the code.% n is the relevant dimension, assumed already set to a numerical value% Optimization variable declarations in YALMIP.% Because matrices are square and not declared 'full', they are by default symmetric.% sdpvar is declaring an optimization variabletau = sdpvarmu_a = sdpvar(n,1)mu_b = sdpvar(n,1)theta_a = sdpvar(n,n)theta_b = sdpvar(n,n)psi = sdpvar(n,n)lambda = sdpvar(n,n,'diagonal')% f is the relative weighting factor for matrix vs. vector norms, assumed already set% mu_a_hat, mu_b_hat, sigma_a_hat, sigma_b_hat assumed already set% \u0026gt;= 0 constraint on the square (symmetric) matrices are interpreted in LMI sense, i.e., constrains matrix to be positive semidefinite% I put in constraint tau \u0026gt;= 0, but omit it if it doesn't belong$ I put  in constraint diag(lambda) \u0026gt;= 0, but omit it if it doesn't belongConstraints = [theta_a \u0026gt;= 0,theta_b \u0026gt;= 0,psi \u0026gt;= 0,diag(lambda) \u0026gt;= 0, tau\u0026gt;= 0]% Put the unnecessary transpose on post-multiplying lambda in case lambda is ever made non-symmetricObjective = f*norm(tau*mu_a+(1-tau)*mu_b) + f*norm(mu_a_hat-lambda*mu_a) + f*norm(mu_b_hat-lambda*mu_b) + norm(lambda*theta_a*lambda'+psi-sigma_a_hat,'fro') + norm(lambda*theta_b*lambda'+psi-sigma_b_hat,'fro')% Specify PENLAB as the solver, and 'usex0' set to 1 means use assigned initial valuesops = sdpsettings('solver','penlab','usex0',1)% All variables ending in _initial are the initial values to use in the optimizer and assumed already setassign(tau,tau_initial)assign(mu_a,mu_a_initial)assign(mu_b,mu_b_initial)assign(lambda,diag(vector_of_lambda_diagonal_initial))assign(psi,psi_initial)assign(sigma_a,sigma_a_initial)assign(sigma_b,sigma_b_initial)sol = optimize(Constraints,Objective,ops) % invokes the optimizer to solve the problem","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-18 13:36:31","Question_id":230354}
{"_id":{"$oid":"5837a57aa05283111e4d44bd"},"Last_activity":"2016-08-18 16:46:07","Creator_reputation":15542,"Question_score":1,"Answer_content":"I wonder if you can fit this with a model of value likeE[v \\vert n]=\\alpha + \\beta \\cdot \\frac{1}{n}The parameter  will give you the value asymptote. This functional form also has the benefit that the value only becomes positive only above some threshold , which might be reasonable if there is some sort of fixed cost.","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-08-18 16:46:07","Question_id":230477}
{"_id":{"$oid":"5837a57aa05283111e4d44be"},"Last_activity":"2016-08-18 16:20:27","Creator_reputation":4307,"Question_score":0,"Answer_content":"If you have access to the economic model itself, one possibility is to first calibrate a surrogate model (using a design of computational experiments approach, e.g. DACE). This would give a cheap way to get approximate model results quickly, which you could then use in your VOI calculations. This type of approach is useful if e.g. VOI is a post-processing of several model outputs, and the model has many more inputs than just sample size as input.If the model is literally VOI[n], but just slow to compute, then the \"design of experiments approach\" essentially reduces to adaptive sampling. In that case you could for example just choose new  values to evaluate based on where your spline has high curvature. This would me more akin to adaptive mesh refinement.","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-18 16:20:27","Question_id":230477}
{"_id":{"$oid":"5837a57aa05283111e4d44cd"},"Last_activity":"2016-08-18 16:40:33","Creator_reputation":527,"Question_score":2,"Answer_content":"Maybe I have a statistical test that might help you.Let me first note that you want to know if your sequence of observations is a independent and identically distributed (i.i.d.) sequence of Bernoulli random variables. The problem is that a sequence can fail to be i.i.d. for a variety of reasons and there are a lot of randomness tests out there, each one better suited for a particular way in which a sequence fails to be i.i.d.. I believe that the number of runs test or the longest runs test (you can find its description in this book ) are two good candidates for you problem.If none of the previous tests work let me propose a alternative approach. Let's suppose we have a sequence of i.i.d. Bernoulli r.v. , i.e., , for all .Now we will define the following random variables:  is the number of  we shaw before the first ;  is the number of  after the first  that we shaw before the second . In a similar way we define  for any value of .The interesting thing here is that (if we assume a i.i.d. sequence) all  have the same distribution which is a geometric distribution with success probability , :\\mathbb P (Y_j = k) = (1-p)^kp Now we have our test. If your sequence is not a i.i.d. one, the distribution of the  you constructed from it would not be close to a  distribution. This can be tested with any goodness of fit test. If you have no previous knowledge of the value of  you can resort to estimating it from the sample.","Display_name":"Mur1lo","Creater_id":120428,"Start_date":"2016-08-18 16:40:33","Question_id":230087}
{"_id":{"$oid":"5837a57aa05283111e4d44ce"},"Last_activity":"2016-08-16 14:23:58","Creator_reputation":10423,"Question_score":1,"Answer_content":"As said in comments, any stricly binary (yes/no) variable is a Bernoulli variable, which is the same as an Binomial variable with parameter  (and some ). There is no more possibilities, since such a variable has its distribution totally identified by the probability  of occurence.But when you have more than one bernoulli variable, and sum them up, to get the total number of occurences, there are other possibilities. Let there be  such variables, with probability of occurence  each. When all the probabilities  are identical , and the events are independent, the sum  has the binomial distribution with parameters . But with unequal probabilities and/or dependence, we get other possibilities.First, the case with independence but enequal probabilities ,  \\DeclareMathOperator{\\E}{\\mathbb{E}}\\DeclareMathOperator{\\V}{\\mathbb{V}}\\bar{p} = \\frac1n \\sum p_i. Then the expectation  still has the binomial value  (by linearity of expectation), but the variance  differs:\\V X = \\V (\\sum X_i) = \\sum \\V X_i = \\sum p_i(1-p_i) \\\\     =  \\dots = n\\bar{p}(1-\\bar{p}) - \\sum (p_i - \\bar{p})^2 \\\\     \u0026lt;  n \\bar{p}(1-\\bar{p})(after some algebra), so it is always smaller than the binomial variance!There are of course many other possibilities, let us take the case of , equal probabilities, but dependence:Suppose that    P(X_1=0,X_2=0)= (1-p)^2 (1+\\theta) \\\\    P(X_1=0,X_2=1)= p(1-p) (1-\\psi) \\\\    P(X_1=1,X_2=0)= p(1-p) (1-\\psi) \\\\    P(X_1=1,X_2=1)= p^2 (1+\\theta) (If you fix , you can determine . I leave that as an exercise ...) with a positive , this is a kind of positive dependence, positive covariance. If I got my algebra right, the variance of  can be found to be:\\V X = 2 p (1-p) + \\theta p^2so that, with a positive , the variance is larger than binomial, but with a negative , the variance is smaller than binomial!","Display_name":"kjetil b halvorsen","Creater_id":11887,"Start_date":"2016-08-16 13:30:21","Question_id":230087}
{"_id":{"$oid":"5837a57aa05283111e4d44dd"},"Last_activity":"2016-08-18 16:27:42","Creator_reputation":8337,"Question_score":1,"Answer_content":"Rather than tell you how to do this, I'm going to tell you that you shouldn't do it, and why.Your proposal is an example of data-dredging. Rather than trying to answer a question about the data, you're looking for questions the data could answer. The problem with data-dredging is that given enough things to check, you're going to find spurious relationships, which won't generalize beyond the sample you happen to be looking at. It's not useful to point out that \"Females with BMI between 24.8 and 28.2 have scores between 2.3 and 3.7\" in your sample if this says nothing about the population, assuming you want to do some kind of inference.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-18 16:27:42","Question_id":230582}
{"_id":{"$oid":"5837a57aa05283111e4d44ec"},"Last_activity":"2016-08-18 15:53:14","Creator_reputation":7965,"Question_score":0,"Answer_content":"Input of the LSTM: Output of the LSTM:  (a.k.a. hidden states)Some network architectures use all the hidden states:Others use only a few ones (most commonly the last one):(source of the images) ","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-18 15:53:14","Question_id":229861}
{"_id":{"$oid":"5837a57aa05283111e4d44f8"},"Last_activity":"2016-08-18 01:43:29","Creator_reputation":403,"Question_score":3,"Answer_content":"As per the literature,Schmidhuber, J. (2015). \"Deep Learning in Neural Networks: An Overview\". Neural Networks. 61: 85–117. arXiv:1404.7828free to read. doi:10.1016/j.neunet.2014.09.003.https://en.wikipedia.org/wiki/Deep_learningIt is said that   There is no universally agreed upon threshold of depth dividing  shallow learning from deep learning, but most researchers in the field  agree that deep learning has multiple nonlinear layers (CAP \u003e 2) and  Schmidhuber considers CAP \u003e 10 to be very deep learningA chain of transformations from input to output is a Credit Assignment Path or CAP. For a feedforward neural network, the depth of the CAPs, and thus the depth of the network, is the number of hidden layers plus one. ","Display_name":"prashanth","Creater_id":86202,"Start_date":"2016-08-18 01:43:29","Question_id":229619}
{"_id":{"$oid":"5837a57aa05283111e4d44f9"},"Last_activity":"2016-08-13 06:11:55","Creator_reputation":7965,"Question_score":11,"Answer_content":"\"Deep\" is a marketing term: you can therefore use it whenever you need to market your multi-layered neural network.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-13 06:11:55","Question_id":229619}
{"_id":{"$oid":"5837a57aa05283111e4d44fa"},"Last_activity":"2016-08-13 00:47:00","Creator_reputation":2762,"Question_score":6,"Answer_content":"\"Deep\"One of the earliest deep neural networks has three densely connected hidden layers (Hinton et al. (2006)).\"Very Deep\"In 2014 the \"very deep\" VGG netowrks Simonyan et al. (2014) consist of 16+ hidden layers.\"Extremely Deep\"In 2016 the \"extremely deep\" residual networks He et al. (2016) consist of 50 up to 1,000+ hidden layers.","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-08-13 00:47:00","Question_id":229619}
{"_id":{"$oid":"5837a57aa05283111e4d4509"},"Last_activity":"2016-08-18 15:20:59","Creator_reputation":738,"Question_score":0,"Answer_content":"Your key question is \"is there any simple way to test and report the significance of my 1000 GLMs, and all the 1000 t values and coefficient distributions for my variables?\"What you want to report is the distribution of the estimated parameters and confidence intervals.Example:I went ahead and generated some data, and a simple t-test is performed. Say, I'm REALLY interested in seeing what the t-statistics look like. Given that there is a significant difference between the sample sizes, and you want to comparable sample sizes, you can sample n (400 in my example) from each group with replacement, as long as you boost the number of simulations. Anything above 10^4 is fine, but I use 10^5.s1 \u0026lt;- rnorm(n = 400, mean = 3, sd = 2) ## I'll assume there's a higher rate heres2 \u0026lt;- rnorm(n = 1000, mean = 2, sd = 2)t_stats \u0026lt;- numeric(length = 10^5)for(i in 1:10^5){  boot_s1 \u0026lt;- s1[sample(x = 1:400, size = 400, replace = TRUE)]  boot_s2 \u0026lt;- s2[sample(x = 1:1000, size = 400, replace = TRUE)]  temp \u0026lt;- t.test(x = boot_s1, y = boot_s2, var.equal = TRUE)  t_stats[i] \u0026lt;- temp$statistic}After producing my bootstrapped samples, I can perform a statistical test.qt(p = 0.95, df = 399, lower.tail = TRUE)## null hypothesis: 95% of bootstrapped t-statistics are not beyond 1.648682summary(t_stats)From a summary, we see that 100% of above 1.65, so you can iterate that in your report or whatevs.Next, visualize the distributionhist(x = t_stats, breaks = \"scott\",      probability = TRUE, main = \"Bootstrapped t-Statistics\",      xlim = c(-1, 15), xlab = \"t-Statistics\")abline(v = 1.648682, lty = 2, col = 2)And include the confidence intervals of your statistics. I'm lazy, so I use the quantiles.## boot CIquantile(x = t_stats, probs = c(0.025, 0.975))That's pretty much it. The rest of the job is for you to provide some context as to what the analysis is showing with respect to your data.Here's a reference for further reading/examples: http://www.statoo.com/en/publications/bootstrap_scgn_v131.pdfHope this helps","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-18 15:12:49","Question_id":230493}
{"_id":{"$oid":"5837a57aa05283111e4d4518"},"Last_activity":"2016-08-18 14:46:59","Creator_reputation":5445,"Question_score":0,"Answer_content":"Does each crab appear only in one tank ?Does each tank appear only in one trial ?If the answer to both of these is yes then you have a nested design for which you can specify the random structure as (1|Trial/TankID/Crab), assuming you have sufficient number of trials, tanks and crabs (it wasn't clear from your question how many of each you have) - see the answer here for more details.As for the interactions, if you think that the effect of Treatment is different for different levels of exposure then you would include the interaction term Treatment:exposure. Similarly for the other interaction. There is a warning:  fixed-effect model matrix is rank deficient so dropping 1 column / coefficientThere is probably linear dependence among your fixed effects. You can investigate this with:require(caret)findLinearCombos(model.matrix(actfit.glmm.interact))The warning about convergence should also be checked, but you should write another question about that (after searching for possible answers, since this is a common problem)","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-11 01:24:30","Question_id":229260}
{"_id":{"$oid":"5837a57aa05283111e4d4527"},"Last_activity":"2016-08-18 14:24:15","Creator_reputation":1486,"Question_score":0,"Answer_content":"The most obvious choice should be an one-sided paired t-test, since what you want to tell is if the difference of your strategy return and the market return for the same days is significantly greater than zero.","Display_name":"Pere","Creater_id":123561,"Start_date":"2016-08-18 14:24:15","Question_id":230572}
{"_id":{"$oid":"5837a57aa05283111e4d4534"},"Last_activity":"2016-08-18 14:09:48","Creator_reputation":1486,"Question_score":0,"Answer_content":"First of all, please notice that is false that \"the distribution of the unobserved factors in the population is zero\". The expected value (e.g. the mean) of the effects of the unobserved values is zero, which is not the same.Assuming  is a matter of convenience and convention more than a strong underlying theoretical reason. In fact, you could choose to equal  to any (constant) given number and all the regression analysis wouldn't be very different than usual - the main difference is that you would get the constant  with your constant subtracted.Anyway, please notice that in the usual formulation all effects have zero expectation, even the effect of the observed factor (). You can see that the regression line passes through the point  and for other points regression line just describes the effect on yield above or below mean fertilizer.Furthermore, we shouldn't read  as meaning that the unobserved factors (e.g. rainfall) don't have a nett effect compared to zero. The regression line describes a prediction for the mean unobserved factors (e.g. for the mean rainfall), not a the prediction for zero value of unobserved factors (e.g. assuming no rainfall). Therefore, the unobserved factors (e.g. rainfall) don't have a nett effect compared to the mean.","Display_name":"Pere","Creater_id":123561,"Start_date":"2016-08-18 14:09:48","Question_id":230565}
{"_id":{"$oid":"5837a57aa05283111e4d4535"},"Last_activity":"2016-08-18 13:51:07","Creator_reputation":165,"Question_score":0,"Answer_content":"Since all those factors are not measured, you have no idea about:1) The magnitude of their effect on the yield;2) The direction of that effect.For instance, for a given plot you can say that the yield is expected to be higher than average because the level of fertilizer is higher than average, but you have no idea whether the amount of rainfall has been higher or lower than the average precipitation. It can go either way. Therefore, your best guess is to say that, on average (across all the plots with varying level of rainfall), rainfall factor has zero effect. In technical terms, that statement corresponds to .","Display_name":"Nik Tuzov","Creater_id":123427,"Start_date":"2016-08-18 13:51:07","Question_id":230565}
{"_id":{"$oid":"5837a57aa05283111e4d4546"},"Last_activity":"2016-08-18 13:43:46","Creator_reputation":44,"Question_score":1,"Answer_content":"Hint: Look at each entry of , then, look at their average. Then, use the Law of large numbers. Finally, use the continuous mapping theorem.","Display_name":"Luck","Creater_id":128185,"Start_date":"2016-08-18 13:43:46","Question_id":230568}
{"_id":{"$oid":"5837a57aa05283111e4d4555"},"Last_activity":"2016-08-18 13:13:56","Creator_reputation":11400,"Question_score":4,"Answer_content":"If your data and model fit the Cox proportional hazards assumption, then you are much better off using the Cox model to predict 3-month or 6-month PFS. The logistic model would throw away all information about actual event times and also ignore cases that were censored (lost to follow up) before 3 or 6 months, respectively. If there was no censoring before your time of interest and the proportional hazards assumption was substantially violated then I suppose you could consider logistic regression, but you would have to recognize that your results might be highly dependent on the particular time that you chose. No harm in trying the logistic regression, but I think most would prefer to see Cox regression results instead.","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-08-18 13:13:56","Question_id":230555}
{"_id":{"$oid":"5837a57aa05283111e4d4564"},"Last_activity":"2016-08-12 16:40:50","Creator_reputation":21,"Question_score":1,"Answer_content":"Typically, when using the EM algorithm you would consider missing observations as hidden variables that would be estimated. Instead of setting missing observations to 0, you would have to estimate them in the M-step.However, for the case of HMM, given the current state, each observation is independent. Because of this,  it turns out that we do not need to estimate missing observations, as long as we estimate all the hidden states.The update equations for the M-step turn out to be what you've guessed!This is explained in detail in the below paper:Roberta Paroli and Luigi Spezia, \"Parameter estimation of Gaussian hidden Markov models when missing observations occur\", Metron-International Journal of Statistics, 2002 (https://www.researchgate.net/publication/5182349_Parameter_estimation_of_Gaussian_hidden_Markov_models_when_missing_observations_occur)","Display_name":"badim","Creater_id":127289,"Start_date":"2016-08-12 16:40:50","Question_id":229580}
{"_id":{"$oid":"5837a57aa05283111e4d4571"},"Last_activity":"2016-08-18 05:32:18","Creator_reputation":41,"Question_score":3,"Answer_content":"Based on the formula, I would code it as :R-squared R2 \u0026lt;- 1 - (sum((actual-predict )^2)/sum((actual-mean(actual))^2))RMSEmy_RMSE\u0026lt;-sqrt(mean((predict -actual)^2))or use the function of caretmy_RMSE\u0026lt;-caret::RMSE(pred,y)There is a relation between the two, you have R²=1-n*RMSE²/TSS, where TSS is the total sum of square.I would rather use the RMSE which indicate the average deviation of the estimates from the actual values.I think that your second question should be asked in Cross Validated, where similar topics can be found.","Display_name":"Arault","Creater_id":123067,"Start_date":"2016-08-18 05:32:18","Question_id":230556}
{"_id":{"$oid":"5837a57aa05283111e4d458a"},"Last_activity":"2012-04-03 07:35:08","Creator_reputation":3970,"Question_score":2,"Answer_content":"DataOne provides a helpful set of data management best practices that can be filtered by tag. The best practices tagged with \"quality\", found at http://www.dataone.org/best-practices/quality, reiterating and expanding on many of the points made by @whuber. Here is a list of the topics covered there (in alphabetical order):Communicate data qualityConfirm a match between data and their description in metadataConsider the compatibility of the data you are integratingDevelop a quality assurance and quality control planDouble-check the data you enterEnsure basic quality controlEnsure integrity and accessibility when making backups of dataIdentify outliersIdentify values that are estimatedProvide version information for use and discovery","Display_name":"David LeBauer","Creater_id":1381,"Start_date":"2012-04-03 07:35:08","Question_id":7467}
{"_id":{"$oid":"5837a57aa05283111e4d458b"},"Last_activity":"2011-02-21 16:27:05","Creator_reputation":147383,"Question_score":25,"Answer_content":"This response focuses on the second question, but in the process a partial answer to the first question (guidelines for a QA/QC procedure) will emerge.By far the best thing you can do is check data quality at the time entry is attempted.  The user checks and reports are labor-intensive and so should be reserved for later in the process, as late as is practicable.Here are some principles, guidelines, and suggestions, derived from extensive experience (with the design and creation of many databases comparable to and much larger than yours).  They are not rules; you do not have to follow them to be successful and efficient; but they are all here for excellent reasons and you should think hard about deviating from them.Separate data entry from all intellectually demanding activities.  Do not ask data entry operators simultaneously to check anything, count anything, etc.  Restrict their work to creating a computer-readable facsimile of the data, nothing more.  In particular, this principle implies the data-entry forms should reflect the format in which you originally obtain the data, not the format in which you plan to store the data.  It is relatively easy to transform one format to another later, but it's an error-prone process to attempt the transformation on the fly while entering data.Create a data audit trail: whenever anything is done to the data, starting at the data entry stage, document this and record the procedure in a way that makes it easy to go back and check what went wrong (because things will go wrong).  Consider filling out fields for time stamps, identifiers of data entry operators, identifiers of sources for the original data (such as reports and their page numbers), etc.  Storage is cheap, but the time to track down an error is expensive.Automate everything.  Assume any step will have to be redone (at the worst possible time, according to Murphy's Law), and plan accordingly.  Don't try to save time now by doing a few \"simple steps\" by hand.In particular, create support for data entry: make a front end for each table (even a spreadsheet can do nicely) that provides a clear, simple, uniform way to get data in.  At the same time the front end should enforce your \"business rules:\" that is, it should perform as many simple validity checks as it can.  (E.g., pH must be between 0 and 14; counts must be positive.)  Ideally, use a DBMS to enforce relational integrity checks (e.g., every species associated with a measurement really exists in the database).Constantly count things and check that counts exactly agree.  E.g., if a study is supposed to measure attributes of 10 species, make sure (as soon as data entry is complete) that 10 species really are reported.  Although checking counts is simple and uninformative, it's great at detecting duplicated and omitted data. If the data are valuable and important, consider independently double-entering the entire dataset.  This means that each item will be entered at separate times by two different non-interacting people.  This is a great way to catch typos, missing data, and so on.  The cross-checking can be completely automated.  This is faster, better at catching errors, and more efficient than 100% manual double checking.  (The data entry \"people\" can include devices such as scanners with OCR.)Use a DBMS to store and manage the data.  Spreadsheets are great for supporting data entry, but get your data out of the spreadsheets or text files and into a real database as soon as possible.  This prevents all kinds of insidious errors while adding lots of support for automatic data integrity checks.  If you must, use your statistical software for data storage and management, but seriously consider using a dedicated DBMS: it will do a better job.After all data are entered and automatically checked, draw pictures: make sorted tables, histograms, scatterplots, etc., and look at them all.  These are easily automated with any full-fledged statistical package.Do not ask people to do repetitive tasks that the computer can do.  The computer is much faster and more reliable at these.  Get into the habit of writing (and documenting) little scripts and small programs to do any task that cannot be completed immediately.  These will become part of your audit trail and they will enable work to be redone easily.  Use whatever platform you're comfortable with and that is suitable to the task.  (Over the years, depending on what was available, I have used a wide range of such platforms and all have been effective in their way, ranging from C and Fortran programs through AWK and SED scripts, VBA scripts for Excel and Word, and custom programs written for relational database systems, GIS, and statistical analysis platforms like R and Stata.)If you follow most of these guidelines, approximately 50%-80% of the work in getting data into the database will be database design and writing the supporting scripts.  It is not unusual to get 90% through such a project and be less than 50% complete, yet still finish on time: once everything is set up and has been tested, the data entry and checking can be amazingly efficient.","Display_name":"whuber","Creater_id":919,"Start_date":"2011-02-21 16:27:05","Question_id":7467}
{"_id":{"$oid":"5837a57aa05283111e4d4598"},"Last_activity":"2016-08-18 11:35:25","Creator_reputation":738,"Question_score":1,"Answer_content":"If you do not mind doing some reading, I recommend looking up Sampling: Design and Analysis by Lohr or Sampling by Thompson for examples on model based weighting schemes for mean squared error (MSE). I'm sure you'll find copies online by doing a simple Google search. Since your data seems deal with area (location), I recommend reviewing the chapters on Spatial Sampling in Sampling.Note that you should try to understand how your data was sampled (obtained) as that will affect the weights.","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-18 11:35:25","Question_id":230517}
{"_id":{"$oid":"5837a57aa05283111e4d4599"},"Last_activity":"2016-08-18 09:20:49","Creator_reputation":25275,"Question_score":2,"Answer_content":"As already noticed by whuber in a comment, it is not clear if your procedure of setting weights is valid. Notice that in non-weighted RMSE larger areas already have greater weight on the estimate since they are larger, so they appear more often in your data. That is why, as suggested, people rather down-weight such subpopulations, so that the final estimate treats all the subpopulations more evenly.However if you wanted to use weighted RMSE, then recall that RMSE is by design pretty close to standard deviation, so why not look at how weighted variance is calculated? \\sigma^2 = \\sum_{i=1}^n w_i (x_i - \\bar x)^2 where weights are non-negative and . The same you can take weighted RMSE as \\text{RMSE} = \\sqrt{\\sum_{i=1}^n w_i (\\hat x_i - x_i)^2} Notice that we take sum of weighted differences, not the mean. Unweighted mean is the same as weighted mean with weights that are all equal to , so if you took arithmetic mean, it would be like dividing RMSE by  second time.Check also:Weighted Variance, one more time","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-18 08:24:09","Question_id":230517}
{"_id":{"$oid":"5837a57aa05283111e4d45aa"},"Last_activity":"2016-08-18 11:17:08","Creator_reputation":21,"Question_score":2,"Answer_content":"I had similar issue. Take a look at this example for some advice:https://github.com/Lasagne/Lasagne/blob/master/examples/recurrent.pyThe solution that worked for me was to change the cost function by flattening the output, e.g changecost=T.nnet.categorical_crossentropy(network_output,target_values).mean()to something like this:# lasagne.layers.get_output produces a variable for the output of the netnetwork_output = lasagne.layers.get_output(l_out)# The network output will have shape (n_batch, 1); let's flatten to get a# 1-dimensional vector of predicted valuespredicted_values = network_output.flatten()# Our cost will be mean-squared errorcost = T.mean((predicted_values - target_values)**2)","Display_name":"user128173","Creater_id":128173,"Start_date":"2016-08-18 11:17:08","Question_id":208502}
{"_id":{"$oid":"5837a57aa05283111e4d45b7"},"Last_activity":"2016-08-18 11:05:16","Creator_reputation":null,"Question_score":138,"Answer_content":"First, we need to understand what is a Markov chain. Consider the following weather example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:Since, the next day's weather is either sunny or rainy it follows that:Similarly, let:Therefore, it follows that:The above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:We might ask several questions whose answers follow:Q1: If the weather is sunny today then what is the weather likely to be tomorrow?A1: Since, we do not know what is going to happen for sure, the best we can say is that there is a  chance that it is likely to be sunny and  that it will be rainy. Q2: What about two days from today?A2: One day prediction:  sunny,  rainy. Therefore, two days from now:First day it can be sunny and the next day also it can be sunny. Chances of this happening are: . OrFirst day it can be rainy and second day it can be sunny. Chances of this happening are: .Therefore, the probability that the weather will be sunny in two days is:Similarly, the probability that it will be rainy is:In linear algebra (transition matrices) these calculations correspond to all the permutations in transitions from one step to the next (sunny-to-sunny (), sunny-to-rainy (), rainy-to-sunny () or rainy-to-rainy ()) with their calculated probabilities:On the lower part of the image we see how to calculate the probability of a future state ( or ) given the probabilities (probability mass function, ) for every state (sunny or rainy) at time zero (now or ) as simple matrix multiplication.If you keep forecasting weather like this you will notice that eventually the -th day forecast, where  is very large (say ), settles to the following 'equilibrium' probabilities:and In other words, your forecast for the -th day and the -th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.The above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' Markov chain (nice = transition probabilities satisfy conditions):Irrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.Markov Chain Monte Carlo exploits the above feature as follows: We want to generate random draws from a target distribution. We then identify a way to construct a 'nice' Markov chain such that its equilibrium probability distribution is our target distribution. If we can construct such a chain then we arbitrarily start from some point and iterate the Markov chain many times (like how we forecast the weather  times). Eventually, the draws we generate would appear as if they are coming from our target distribution. We then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the Monte Carlo component.There are several ways to construct 'nice' Markov chains (e.g., Gibbs sampler, Metropolis-Hastings algorithm).","Display_name":"user28","Creater_id":null,"Start_date":"2010-07-19 21:00:14","Question_id":165}
{"_id":{"$oid":"5837a57aa05283111e4d45b8"},"Last_activity":"2016-08-18 05:18:10","Creator_reputation":11,"Question_score":1,"Answer_content":"MCMC is typically used as an alternative to crude Monte Carlo simulation techniques. Both MCMC and other Monte Carlo techniques are used to evaluate difficult integrals but MCMC can be used more generally.For example, a common problem in statistics is to calculate the mean outcome relating to some probabilistic/stochastic model. Both MCMC and Monte Carlo techniques would solve this problem by generating a sequence of simulated outcomes that we could use to estimate the true mean. Both MCMC and crude Monte Carlo techniques work as the long-run proportion of simulations that are equal to a given outcome will be equal* to the modelled probability of that outcome. Therefore, by generating enough simulations, the results produced by both methods will be accurate.*I say equal although in general I should talk about measurable sets. A layperson, however, probably wouldn't be interested in this*However, while crude Monte Carlo involves producing many independent simulations, each of which is distributed according to the modelled distribution, MCMC involves generating a random walk that in the long-run \"visits\" each outcome with the desired frequency.The trick to MCMC, therefore, is picking a random walk that will \"visit\" each outcome with the desired long-run frequencies.A simple example might be to simulate from a model that says the probability of outcome \"A\" is 0.5 and of outcome \"B\" is 0.5. In this case, if I started the random walk at position \"A\" and prescribed that in each step it switched to the other position with probability 0.2 (or any other probability that is greater than 0), I could be sure that after a large number of steps the random walk would have visited each of \"A\" and \"B\" in roughly 50% of steps--consistent with the probabilities prescribed by our model.This is obviously a very boring example. However, it turns out that MCMC is often applicable in situations in which it is difficult to apply standard Monte Carlo or other techniques.You can find an article that covers the basics of what it is and why it works here:http://wellredd.uk/basics-markov-chain-monte-carlo/","Display_name":"Richard Redding","Creater_id":127887,"Start_date":"2016-08-16 10:37:34","Question_id":165}
{"_id":{"$oid":"5837a57aa05283111e4d45b9"},"Last_activity":"2015-02-15 23:06:58","Creator_reputation":3102,"Question_score":64,"Answer_content":"I'd probably say something like this:\"Anytime we want to talk about probabilities, we're really integrating a density.  In Bayesian analysis, a lot of the densities we come up with aren't analytically tractable: you can only integrate them -- if you can integrate them at all -- with a great deal of suffering.  So what we do instead is simulate the random variable a lot, and then figure out probabilities from our simulated random numbers.  If we want to know the probability that X is less than 10, we count the proportion of simulated random variable results less than 10 and use that as our estimate.  That's the \"Monte Carlo\" part, it's an estimate of probability based off of random numbers.  With enough simulated random numbers, the estimate is very good, but it's still inherently random.\"So why \"Markov Chain\"?  Because under certain technical conditions, you can generate a memoryless process (aka a Markovian one) that has the same limiting distribution as the random variable that you're trying to simulate.  You can iterate any of a number of different kinds of simulation processes that generate correlated random numbers (based only on the current value of those numbers), and you're guaranteed that once you pool enough of the results, you will end up with a pile of numbers that looks \"as if\" you had somehow managed to take independent samples from the complicated distribution you wanted to know about.\"So for example, if I want to estimate the probability that a standard normal random variable was less than 0.5, I could generate ten thousand independent realizations from a standard normal distribution and count up the number less than 0.5; say I got 6905 that were less than 0.5 out of 10000 total samples; my estimate for P(Z\u0026lt;0.5) would be 0.6905, which isn't that far off from the actual value.  That'd be a Monte Carlo estimate.\"Now imagine I couldn't draw independent normal random variables, instead I'd start at 0, and then with every step add some uniform random number between -0.5 and 0.5 to my current value, and then decide based on a particular test whether I liked that new value or not; if I liked it, I'd use the new value as my current one, and if not, I'd reject it and stick with my old value.  Because I only look at the new and current values, this is a Markov chain.  If I set up the test to decide whether or not I keep the new value correctly (it'd be a random walk Metropolis-Hastings, and the details get a bit complex), then even though I never generate a single normal random variable, if I do this procedure for long enough, the list of numbers I get from the procedure will be distributed like a large number of draws from something that generates normal random variables.  This would give me a Markov Chain Monte Carlo simulation for a standard normal random variable.  If I used this to estimate probabilities, that would be a MCMC estimate.\"","Display_name":"Rich","Creater_id":61,"Start_date":"2010-07-19 17:52:13","Question_id":165}
{"_id":{"$oid":"5837a57aa05283111e4d45ba"},"Last_activity":"2013-10-02 05:17:36","Creator_reputation":4042,"Question_score":10,"Answer_content":"So there are plenty of answers here paraphrased from statistics/probability textbooks, Wikipedia, etc. I believe we have \"laypersons\" where I work; I think they are in the marketing department. If I ever have to explain anything technical to them, I apply the rule \"show don't tell.\" With that rule in mind, I would probably show them something like this.The idea here is to try to code an algorithm that I can teach to spell--not by learning all of the hundreds (thousands?) of rules like When adding an ending to a word that ends with a silent e, drop the final e if the ending begins with a vowel. One reason that won't work is I don't know those rules (i'm not even sure the one I just recited is correct). Instead I am going to teach it to spell by showing it a bunch of correctly spelled words and letting it extract the rules from those words, which is more or less the essence of Machine Learning, regardless of the algorithm--pattern extraction and pattern recognition.The success criterion is correctly spelling a word the algorithm has never seen before (i realize that can happen by pure chance, but that won't occur to the marketing guys, so i'll ignore--plus I am going to have the algorithm attempt to spell not one word, but a lot, so it's not likely we'll be deceived by a few lucky guesses).An hour or so ago, I downloaded (as a plain text file) from the excellent Project Gutenberg Site, the Herman Hesse novel Siddhartha. I'll use the words in this novel to teach the algorithm how to spell.So I coded the algorithm below that scanned this novel, three letters at a time (each word has one additional character at the end, which is 'whitespace', or the end of the word). Three-letter sequences can tell you a lot--for instance, the letter 'q' is nearly always followed by 'u'; the sequence 'ty' usually occurs at the end of a word; z rarely does, and so forth. (Note: I could just as easily have fed it entire words in order to train it to speak in complete sentences--exactly the same idea, just a few tweaks to the code.)None of this involves MCMC though, that happens after training, when we give the algorithm a few random letters (as a seed) and it begins forming 'words'. How does the algorithm build words? Imagine that it has the block 'qua'; what letter does it add next? During training, the algorithm constructed a massive l*etter-sequence frequency matrix* from all of the thousands of words in the novel. Somewhere in that matrix is the three-letter block 'qua' and the frequencies for the characters that could follow the sequence. The algorithm selects a letter based on those frequencies that could possibly follow it. So the letter that the algorithm selects next depends on--and solely on--the last three in its word-construction queue.So that's a Markov Chain Monte Carlo algorithm. I think perhaps the best way to illustrate how it works is to show the results based on different levels of training. Training level is varied by changing the number of passes the algorithm makes though the novel--the more passes thorugh the greater the fidelity of its letter-sequence frequency matrices.  Below are the results--in the form of 100-character strings output by the algorithm--after training on the novel 'Siddharta'. A single pass through the novel, Siddhartha:  then whoicks ger wiff all mothany stand ar you livid theartim mudded  sullintionexpraid his sible his(Straight away, it's learned to speak almost perfect Welsh; I hadn't expected that.)After two passes through the novel:  the ack wor prenskinith show wass an twor seened th notheady theatin land  rhatingle was the ov thereAfter 10 passes:  despite but the should pray with ack now have water her dog lever pain feet  each not the weak memory And here's the code (in Python, i'm nearly certain that this could be done in R using an MCMC package, of which there are several, in just 3-4 lines)def create_words_string(raw_string) :  \"\"\" in case I wanted to use training data in sentence/paragraph form;       this function will parse a raw text string into a nice list of words;      filtering: keep only words having  more than 3 letters and remove       punctuation, etc.  \"\"\"  pattern = r'\\b[A-Za-z]{3,}\\b'  pat_obj = re.compile(pattern)  words = [ word.lower() for word in pat_obj.findall(raw_string) ]  pattern = r'\\b[vixlm]+\\b'  pat_obj = re.compile(pattern)  return \" \".join([ word for word in words if not pat_obj.search(word) ])def create_markov_dict(words_string):  # initialize variables  wb1, wb2, wb3 = \" \", \" \", \" \"  l1, l2, l3 = wb1, wb2, wb3  dx = {}  for ch in words_string :    dx.setdefault( (l1, l2, l3), [] ).append(ch)    l1, l2, l3 = l2, l3, ch  return dxdef generate_newtext(markov_dict) :  simulated_text = \"\"  l1, l2, l3 = \" \", \" \", \" \"  for c in range(100) :    next_letter = sample( markov_dict[(l1, l2, l3)], 1)[0]    simulated_text += next_letter    l1, l2, l3 = l2, l3, next_letter  return simulated_textif __name__==\"__main__\" :  # n = number of passes through the training text  n = 1  q1 = create_words_string(n * raw_str)  q2 = create_markov_dict(q1)  q3 = generate_newtext(q2)  print(q3)","Display_name":"doug","Creater_id":438,"Start_date":"2010-08-05 02:15:34","Question_id":165}
{"_id":{"$oid":"5837a57aa05283111e4d45bb"},"Last_activity":"2013-03-06 06:08:10","Creator_reputation":4586,"Question_score":10,"Answer_content":"Excerpt from Bayesian Methods for HackersThe Bayesian landscapeWhen we setup a Bayesian inference problem with  unknowns, we are implicitly creating a  dimensional space for the prior distributions to exist in. Associated with the space is an additional dimension, which we can describe as the surface, or curve, of the space, that reflects the prior probability of a particular point. The surface of the space is defined by our prior distributions. For example, if we have two unknowns  and , and both are uniform on [0,5], the space created is the square of length 5 and the surface is a flat plane that sits ontop of the square (representing that every point is equally likely).Alternatively, if the two priors are  and , then the space is all postive numbers on the 2-D plane, and the surface induced by the priors looks like a water fall that starts at the point (0,0) and flows over the positive numbers. The visualization below demonstrates this. The more dark red the color, the more prior probability that the unknowns are at that location. Conversely, areas with darker blue represent that our priors assign very low probability to the unknowns being there. These are simple examples in 2D space, where our brains can understand surfaces well. In practice, spaces and surfaces generated by our priors can be much higher dimensional. If these surfaces describe our prior distributions on the unknowns, what happens to our space after we have observed data . The data  does not change the space, but it changes the surface of the space by pulling and stretching the fabric of the surface to reflect where the true parameters likely live. More data means more pulling and stretching, and our original shape becomes mangled or insignificant compared to the newly formed shape. Less data, and our original shape is more present. Regardless, the resulting surface describes the posterior distribution. Again I must stress that it is, unfortunately, impossible to visualize this in larger dimensions. For two dimensions, the data essentially pushes up the original surface to make tall mountains. The amount of pushing up is resisted by the prior probability, so that less prior probability means more resistance. Thus in the double exponential-prior case above, a mountain (or multiple mountains) that might erupt near the (0,0) corner would be much higher than mountains that erupt closer to (5,5), since there is more resistance near (5,5). The mountain, or perhaps more generally, the mountain ranges, reflect the posterior probability of where the true parameters are likely to be found.Suppose the priors mentioned above represent different parameters  of two Poisson distributions. We observe a few data points and visualize the new landscape. The plot on the left is the deformed landscape with the  priors, and the plot on the right is the deformed landscape with the exponential priors. The posterior landscapes look different from one another. The exponential-prior landscape puts very little posterior weight on values in the upper right corner: this is because the prior does not put much weight there, whereas the uniform-prior landscape is happy to put posterior weight there. Also, the highest-point, corresponding the the darkest red, is biased towards (0,0) in the exponential case, which is the result from the exponential prior putting more prior wieght in the (0,0) corner.The black dot represents the true parameters. Even with 1 sample point, as what was simulated above, the mountains attempts to contain the true parameter. Of course, inference with a sample size of 1 is incredibly naive, and choosing such a small sample size was only illustrative. Exploring the landscape using the MCMCWe should explore the deformed posterior space generated by our prior surface and observed data to find the posterior mountain ranges. However, we cannot naively search the space: any computer scientist will tell you that traversing -dimensional space is exponentially difficult in : the size of the space quickly blows-up as we increase  (see the curse of dimensionality ). What hope do we have to find these hidden mountains? The idea behind MCMC is to perform an intelligent search of the space. To say \"search\" implies we are looking for a particular object, which perhaps not an accurate description of what MCMC is doing. Recall: MCMC returns samples from the posterior distribution, not the distribution itself. Stretching our mountainous analogy to its limit, MCMC performs a task similar to repeatedly asking  \"How likely is this pebble I found to be from the mountain I am searching for?\", and completes its task by returning thousands of accepted pebbles in hopes of reconstructing the original mountain. In MCMC and PyMC lingo, the returned sequence of \"pebbles\" are the samples, more often called the traces. When I say MCMC intelligently searches, I mean MCMC will hopefully converge towards the areas of high posterior probability. MCMC does this by exploring nearby positions and moving into areas with higher probability. Again, perhaps \"converge\" is not an accurate term to describe MCMC's progression. Converging usually implies moving towards a point in space, but MCMC moves towards a broader area in the space and randomly walks in that area, picking up samples from that area.At first, returning thousands of samples to the user might sound like being an inefficient way to describe the posterior distributions. I would argue that this is extremely efficient. Consider the alternative possibilities::Returning a mathematical formula for the \"mountain ranges\" would involve describing a N-dimensional surface with arbitrary peaks and valleys. Returning the \"peak\" of the landscape, while mathematically possible and a sensible thing to do as the highest point corresponds to most probable estimate of the unknowns, ignores the shape of the landscape, which we have previously argued is very important in determining posterior confidence in unknowns. Besides computational reasons, likely the strongest reason for returning samples is that we can easily use The Law of Large Numbers to solve otherwise intractable problems. I postpone this discussion for the next chapter.  Algorithms to perform MCMCThere is a large family of algorithms that perform MCMC. Simplestly, most algorithms can be expressed at a high level as follows: 1. Start at current position.2. Propose moving to a new position (investigate a pebble near you ).3. Accept the position based on the position's adherence to the data and prior distributions (ask if the pebble likely came from the mountain).4. If you accept: Move to the new position. Return to Step 1.5. After a large number of iterations, return the positions.This way we move in the general direction towards the regions where the posterior distributions exist, and collect samples sparingly on the journey. Once we reach the posterior distribution, we can easily collect samples as they likely all belong to the posterior distribution. If the current position of the MCMC algorithm is in an area of extremely low probability, which is often the case when the algorithm begins (typically at a random location in the space), the algorithm will move in positions that are likely not from the posterior but better than everything else nearby. Thus the first moves of the algorithm are not reflective of the posterior.","Display_name":"Cam.Davidson.Pilon","Creater_id":11867,"Start_date":"2013-03-06 06:08:10","Question_id":165}
{"_id":{"$oid":"5837a57aa05283111e4d45bc"},"Last_activity":"2011-07-05 13:15:25","Creator_reputation":271,"Question_score":27,"Answer_content":"Imagine you want to find a better strategy to beat your friends at the board game Monopoly. Simplify the stuff that matters in the game to the question: which properties do people land on most? The answer depends on the structure of the board, the rules of the game and the throws of two dice. One way to answer the question is this. Just follow a single piece around the board as you throw the dice and follow the rules. Count how many times you land on each property (or program a computer to do the job for you). Eventually, if you have enough patience or you have programmed the rules well enough in you computer, you will build up a good picture of which properties get the most business. This should help you win more often.What you have done is a Markov Chain Monte Carlo (MCMC) analysis. The board defines the rules. Where you land next only depends on where you are now, not where you have been before and the specific probabilities are determined by the distribution of throws of two dice. MCMC is the application of this idea to mathematical or physical systems like what tomorrow's weather will be or where a pollen grain being randomly buffeted by gas molecules will end up.","Display_name":"matt_black","Creater_id":5294,"Start_date":"2011-07-05 13:15:25","Question_id":165}
{"_id":{"$oid":"5837a57aa05283111e4d45bd"},"Last_activity":"2011-07-05 03:03:20","Creator_reputation":1426,"Question_score":51,"Answer_content":"I think there's a nice and simple intuition to be gained from the (independence-chain) Metropolis-Hastings algorithm.  First, what's the goal?  The goal of MCMC is to draw samples from some probability distribution without having to know its exact height at any point.  The way MCMC achieves this is to \"wander around\" on that distribution in such a way that the amount of time spent in each location is proportional to the height of the distribution.  If the \"wandering around\" process is set up correctly, you can make sure that this proportionality (between time spent and height of the distribution) is achieved.Intuitively, what we want to do is to to walk around on some (lumpy) surface in such a way that the amount of time we spend (or # samples drawn) in each location is proportional to the height of the surface at that location.  So, e.g., we'd like to spend twice as much time on a hilltop that's at an altitude of 100m as we do on a nearby hill that's at an altitude of 50m.  The nice thing is that we can do this even if we don't know the absolute heights of points on the surface: all we have to know are the relative heights.  e.g., if one hilltop A is twice as high as hilltop B, then we'd like to spend twice as much time at A as we spend at B.The simplest variant of the Metropolis-Hastings algorithm (independence chain sampling) achieves this as follows: assume that in every (discrete) time-step, we pick a random new \"proposed\" location (selected uniformly across the entire surface).  If the proposed location is higher than where we're standing now, move to it.  If the proposed location is lower, then move to the new location with probability p, where p is the ratio of the height of that point to the height of the current location.  (i.e., flip a coin with a probability p of getting heads; if it comes up heads, move to the new location; if it comes up tails, stay where we are).  Keep a list of the locations you've been at on every time step, and that list will (asyptotically) have the right proportion of time spent in each part of the surface.  (And for the A and B hills described above, you'll end up with twice the probability of moving from B to A as you have of moving from A to B).There are more complicated schemes for proposing new locations and the rules for accepting them, but the basic idea is still: (1) pick a new \"proposed\" location; (2) figure out how much higher or lower that location is compared to your current location; (3) probabilistically stay put or move to that location in a way that respects the overall goal of spending time proportional to height of the location.  What is this useful for? Suppose we have a probabilistic model of the weather that allows us to evaluate A*P(weather), where A is an unknown constant. (This often happens--many models are convenient to formulate in a way such that you can't determine what A is).  So we can't exactly evaluate P(\"rain tomorrow\").  However, we can run the MCMC sampler for a while and then ask: what fraction of the samples (or \"locations\") ended up in the \"rain tomorrow\" state.  That fraction will be the (model-based) probabilistic weather forecast.","Display_name":"jpillow","Creater_id":5289,"Start_date":"2011-07-05 01:57:48","Question_id":165}
{"_id":{"$oid":"5837a57aa05283111e4d45be"},"Last_activity":"2010-07-21 09:42:43","Creator_reputation":4116,"Question_score":26,"Answer_content":"OK here's my best attempt at an informal and crude explanation.A Markov Chain is a random process that has the property that the future depends only on the current state of the process and not the past i.e. it is memoryless. An example of a random process could be the stock exchange. An example of a Markov Chain would be a board game like Monopoly or Snakes and Ladders where your future position (after rolling the die) would depend only on where you started from before the roll, not any of your previous positions. A textbook example of a Markov Chain is the \"drunkard's walk\". Imagine somebody who is drunk and can move only left or right by one pace. The drunk moves left or right with equal probability. This is a Markov Chain where the drunk's future/next position depends only upon where he is at present.Monte Carlo methods are computational algorithms (simply sets of instructions) which randomly sample from some process under study. They are a way of estimating something which is too difficult or time consuming to find deterministically. They're basically a form of computer simulation of some mathematical or physical process. The Monte Carlo moniker comes from the analogy between a casino and random number generation. Returning to our board game example earlier, perhaps we want to know if some properties on the Monopoly board are visited more often than others. A Monte Carlo experiment would involve rolling the dice repeatedly and counting the number of times you land on each property. It can also be used for calculating numerical integrals. (Very informally, we can think of an integral as the area under the graph of some function.)  Monte Carlo integration works great on a high-dimensional functions by taking a random sample of points of the function and calculating some type of average at these various points. By increasing the sample size, the law of large numbers tells us we can increase the accuracy of our approximation by covering more and more of the function.These two concepts can be put together to solve some difficult problems in areas such as Bayesian inference, computational biology, etc where multi-dimensional integrals need to be calculated to solve common problems. The idea is to construct a Markov Chain which converges to the desired probability distribution after a number of steps. The state of the chain after a large number of steps is then used as a sample from the desired distribution and the process is repeated. There many different MCMC algorithms which use different techniques for generating the Markov Chain. Common ones include the Metropolis-Hastings and the Gibbs Sampler.","Display_name":"Graham Cookson","Creater_id":215,"Start_date":"2010-07-21 09:42:43","Question_id":165}
{"_id":{"$oid":"5837a57aa05283111e4d45cb"},"Last_activity":"2016-08-18 10:59:38","Creator_reputation":147383,"Question_score":4,"Answer_content":"Because the  all have a uniform distribution, all (unordered) variables are assumed independent, and no other order statistic lies between  and ,  has a truncated uniform distribution supported on the interval .  Its mean obviously is , QED.If you would like a formal demonstration, note that when the  are iid with an absolutely continuous distribution , the conditional density of  (conditional on all the other order statistics) is , which is the truncated distribution.  (When ,  is taken to be ; and when ,  is taken to be .)  This follows from Joint pdf of functions of order statistics, for instance, together with the definition of conditional densities.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-18 10:51:19","Question_id":230528}
{"_id":{"$oid":"5837a57aa05283111e4d45d8"},"Last_activity":"2016-08-18 10:45:24","Creator_reputation":21578,"Question_score":2,"Answer_content":"You may be confusing the population vs. the sample moments, on one hand, andthe population variance and the variance of the estimate of the mean (which involves the strata variances... unfortunately... as well as a lot of other stuff like finite population corrections ).The population moment of order  is, obviously (with some abuse of notation switching between the flat population and the stratified population):M_k = \\frac{1}{N} \\sum_{i=1}^N x_i^k = \\sum_{h=1}^L \\sum_{i=1}^{N_h} x_{hi}^k \\Bigl/ \\sum_{h=1}^L N_hThe population variance isV = M_2 - M_1^2The population skewness (I made up the letter) is\\Gamma = \\frac{M_3 - 3 M_2 M_1 + 2 M_1^3}{(M_2 - M_1^2)^{3/2}}The population kurtosis isK = \\frac{M_4 - 2 M_2 M_1^2 + M_1^4}{(M_2 - M_1^2)^2}The raw moments are estimable with their plug-in analogues (with abuse of notation, again, with the index  running over the sample now):\\hat M_k = \\sum_{h=1}^L \\sum_{i=1}^{n_h} w_{hi} x_{hi}^k \\Bigl/ \\sum_{h=1}^L \\sum_{i=1}^{n_h} w_{hi}( are the analysis weights that you referred to as elevation factors  -- I honestly wonder what discipline you come from to call them these way; please, please comment below and let me know; in the simplest case of a simple random sample with no nonresponse, ).Unless your sample sizes were fixed by the sampling design, the estimated moments , being ratios of random variables, are biased estimators of their target quantities. The biases are of the order , vs. the sampling error of the order , and hence disappear asymptotically. The plug-in estimates of skewness and kurtosis are also biased, and their biases also disappear asymptotically. The standard errors around , ,  can be obtained by the delta method / Taylor series linearization. If I needed them (I am a lucky Stata user), I would just svyset [pw=fe_h], strata(strata)forvalues k=2/4 {   generate x_`k' = x^`k'}svy: mean x x_2 x_3 x_4nlcom (skew: (_b[x_3]-3*_b[x_2]*_b[x]+2*_b[x]*_b[x]*_b[x])/( (_b[x_2]-_b[x]*_b[x])^(3/2) ) )and Stata would produce the required standard errors that take stratification into account. Note that the standard errors for the moments of order  (implicitly) rely on the population moments of order , so they won't be very stable/accurate.Depending on what's available to you, some references may be helpful as introductory reading on analysis of survey data:Korn and Graubard (1995 JRSS-A)Korn and Graubard (1999 Wiley book)Heeringa, West and Berglund (2010 Chapman and Hall)Kolenikov and Pitblado (2014 chapter in Wiley handbook) -- that would be me; a copy can be found on ResearchGate.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-09 10:16:09","Question_id":227052}
{"_id":{"$oid":"5837a57aa05283111e4d45e9"},"Last_activity":"2010-11-17 15:01:08","Creator_reputation":201,"Question_score":1,"Answer_content":"For reference, these are the values that are included in the table:Df refers to Degrees of freedom, \"the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.\"  The Sum of Sq column refers to the sum of squares (or more precisely sum of squared deviations). In short this is a measure of the amount that each individual value deviates from the overall mean of those values.RSS is the Residual Sum of Squares. These are a measure of how much the predicted value of the dependent (or output) variable varies from the true value for each data point in the set (or more colloquially: each \"line\" in the data table). AIC is the Akaike information criterion which is generally regarded as \"too complex to explain\" but is, in short, a measure of the goodness of fit of an estimated statistical model. If you require further details, you will have to turn to dead trees with words on them. Or Wikipedia and the resources there.   The F value is used to perform what's called an F-test and from it is derived the Pr(F) value, which describes how likely (or Probable = Pr) that F value is. A Pr(F) value close to zero (indicated by ***) is indicative of an input variable that is in some way important to include in a good model, that is, a model that does not include it is \"significantly\" different than the one that does.All of these values are, in the context of the drop1 command, calculated to compare the overall model (including all the input variables) with the model resulting from removing that one specific variable per each line in the output table. Now, if this can be improved upon, please feel free to add to it or clarify any issues. My goal is only to clarify and provide a better \"reverse lookup\" referene from the output of an R command to the actual meaning of it. ","Display_name":"gakera","Creater_id":1994,"Start_date":"2010-11-17 11:07:33","Question_id":4639}
{"_id":{"$oid":"5837a57aa05283111e4d45ea"},"Last_activity":"2010-11-17 10:07:15","Creator_reputation":3755,"Question_score":7,"Answer_content":"drop1 gives you a comparison of models based on the AIC criterion, and when using the option test=\"F\" you add a \"type II ANOVA\" to it, as explained in the help files. As long as you only have continuous variables, this table is exactly equivalent to  summary(lm1), as the F-values are just those T-values squared. P-values are exactly the same.So what to do with it? Interprete it in exactly that way: it expresses in a way if the model without that term is \"significantly\" different from the model with that term. Mind the \"\" around significantly, as the significance here cannot be interpreted as most people think. (multi-testing problem and all...)And regarding the AIC : the lower the better seems more like it. AIC is a value that goes for the model, not for the variable. So the best model from that output would be the one without the variable examination.Mind you, the calculation of both AIC and the F statistic are different from the R functions AIC(lm1) resp. anova(lm1). For AIC(), that information is given on the help pages of extractAIC(). For the anova() function, it's rather obvious that type I and type II SS are not the same.I'm trying not to be rude, but if you don't understand what is explained in the help files there, you shouldn't be using the function in the first place. Stepwise regression is incredibly tricky, jeopardizing your p-values in a most profound manner. So again, do not base yourself on the p-values. Your model should reflect your hypothesis and not the other way around. ","Display_name":"Joris Meys","Creater_id":1124,"Start_date":"2010-11-17 09:28:20","Question_id":4639}
{"_id":{"$oid":"5837a57aa05283111e4d45fd"},"Last_activity":"2016-08-18 09:00:42","Creator_reputation":376,"Question_score":20,"Answer_content":"It's by definition.The empirical distribution function of a set of observations  is defined byF_e(t) = \\frac{\\#\\{X_n \\mid X_n \\le t\\}}nWhere  is the set cardinality. This is, by nature, a step function.It converges to the actual CDF almost surely.Also note that for any distribution with  for at least two  (especially nondegenerate discrete distributions), your variant of ECDF does not converge to the actual CDF.For example consider a Bernoulli distribution with CDFF_X(x) = p \\chi_{x \\ge 0} + (1-p) \\chi_{x \\ge 1}this is a step function whereas ecdf2 will converge to  (a piecewise linear function connecting  and .","Display_name":"AlexR","Creater_id":78492,"Start_date":"2016-08-18 03:10:48","Question_id":230458}
{"_id":{"$oid":"5837a57aa05283111e4d460e"},"Last_activity":"2016-08-18 08:43:29","Creator_reputation":7965,"Question_score":2,"Answer_content":"A classifier is a specific type of model, the output variable of which is discrete, often nominal. As pointed out by others, the terminology is loose. ","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-18 07:45:25","Question_id":230505}
{"_id":{"$oid":"5837a57aa05283111e4d460f"},"Last_activity":"2016-08-18 07:34:37","Creator_reputation":90,"Question_score":0,"Answer_content":"I'm definitely no expert in the domain so take my answer with a grain of salt but from what I have understood you have:Classifier : The algorithm, the core of your machine learning process. It can be an SVM, Naive bayes or even a neural network classifier. Basically it's a big \"set of rules\" on how you want to classify your input.Model : It is what you get once you have finished training your classifier, it's the resulting object of the training phase. You can see it as an \"intelligent\" black box to whom you feed and input sample and it gives you a label as an ouput. Hope my answer is clear enough, but yeah the difference is rather subtle between the two terms.","Display_name":"LoulouChameau","Creater_id":127872,"Start_date":"2016-08-18 07:34:37","Question_id":230505}
{"_id":{"$oid":"5837a57aa05283111e4d4610"},"Last_activity":"2016-08-18 07:32:50","Creator_reputation":2544,"Question_score":2,"Answer_content":"I don't think there's an unified terminology here, but usually classifier refers to the algorithm to assess classification rules, while the rules themselves is what people often call a model. Otherwise, people call the rules a classifier too, and the algorithms are also referred as models. Also, you can refer to your modelling framework as a model in itself.","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-08-18 07:32:50","Question_id":230505}
{"_id":{"$oid":"5837a57aa05283111e4d461d"},"Last_activity":"2016-08-05 16:04:19","Creator_reputation":26,"Question_score":1,"Answer_content":"The autologistic model has an intractable likelihood in the sense that it involves a normalizing constant that cannot be obtained in closed form (this is, you cannot derive the full likelihood and its evaluation is computationally expensive). There is a recent paper that describes this difficulty and proposes an approximation to the maximum likelihood estimator that does not require the evaluation of the likelihood function:  Bee, M., Espa, G., \u0026amp; Giuliani, D. (2015). Approximate maximum likelihood estimation of the autologistic model. Computational Statistics \u0026amp; Data Analysis, 84, 14-26.","Display_name":"Refugee","Creater_id":126791,"Start_date":"2016-08-05 15:53:14","Question_id":228504}
{"_id":{"$oid":"5837a57aa05283111e4d462a"},"Last_activity":"2016-03-12 04:20:53","Creator_reputation":12907,"Question_score":1,"Answer_content":"A good source of information on diagnostic testing of univariate GARCH models is \"rugarch\" vignette by Alexios Ghalanos.I can't tell why it is NaN, but the excess kurtosis is quite pronounced so that normality should be rejected. (JB test statistic is a weighted sum of squared skewness and squares excess kurtosis, see Wikipedia.)The table seems to be telling the opposite: standardized residuals are autocorrelated (the null of no autocorrelation can be rejected) while squared standardized residuals are not autocorrelated (the null cannot be rejected). If the test rejects for either levels or squares, it is not clear whether the conditional mean or the conditional variance model is at fault (because their effects interact from the perspective of the test); what is clear is that the current combination of cond. mean and cond. variance models is lacking.?Yes, ARCH-LM test seems to be telling you that. However, ARCH-LM is not applicable on standardized residuals from a GARCH model; it is only applicable on raw data where no GARCH model has been fit yet. (But this is often ignored in software implementations.) The right test here would be Li-Mak test.I think the parameters should ideally be constant, so Nyblom test results are quite disappointing. Hence, your model might not be very good for forecasting.? (But check the \"rugarch\" vignette for information about this test.)","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-03-11 13:02:01","Question_id":201165}
{"_id":{"$oid":"5837a57aa05283111e4d4639"},"Last_activity":"2016-08-17 04:15:45","Creator_reputation":36,"Question_score":1,"Answer_content":"On page 90 of McCullagh \u0026amp; Nelder, they state that many covariate selection procedures, including AIC minimization and tests using the F statistic, are equivalent to minimizing . Here  is the deviance,  is a function of the number of data points,  is the number of covariates, and  is the dispersion parameter. They cite a paper of Atkinson which I can't access for this statement. From the introduction, it seems Atkinson's actual statement is that the quantity to be minimized is , where  is the maximized log likelihood of the model. Note the relationship , where  is a constant, from pages 33 and 34 of McCullagh and Nelder. It seems to me that in order to obtain their formulation from Atkinson's they already assume  is the same across all candidate models. I think there are basically two issues here. First, if you are estimating a dispersion parameter in some way other than MLE, it's not even clear that Atkinson's quantity is defined, since  should be the maximized log likelihood. If you are estimating a dispersion parameter for a binomial distribution, as McCullagh and Nelder do, you are already not using MLE to fit all the parameters of the model.Second, you may still want to use a criterion of this kind anyway by minimizing  among your candidate models.  can be calculated for the model even if the regression parameters weren't obtained by maximizing the likelihood. If  is constant across all candidate models this has heuristic value because it measures a trade off between model fit (with decreasing ) and complexity (with increasing ). This seems to be what McCullagh and Nelder are suggesting.However if the estimate for  is not constant across all candidate models, then even this heuristic value is lost.  is no longer solely a measure of how well each candidate model fits the data but is also affected by changes in each model's dispersion parameter estimate, and the nature of the complexity-fit trade off becomes less clear. In fact if the differences in your scale parameter estimates are sufficiently large, minimizing this quantity amounts to minimizing the estimated scale parameter.Of course, the scale parameter estimate for each model will depend both on the number of parameters and the fit of the model. So I think the best answer to this question is: If you believe the complexity-fit trade off implied by your method of estimating  will produce a better model than the quantity above, then allow it to vary for each model, or just use it directly. I would be skeptical of this belief, since any method of estimating  was likely not intended to be a good procedure for model selection.","Display_name":"Jonny Lomond","Creater_id":85938,"Start_date":"2016-08-16 09:02:34","Question_id":226152}
{"_id":{"$oid":"5837a57aa05283111e4d463a"},"Last_activity":"2016-08-12 03:14:52","Creator_reputation":1417,"Question_score":0,"Answer_content":"I suspect the reason for the recommendation is that, in the old days, first the model was fit, then the dispersion parameter was calculated, and then the likelihood was adjusted for overdispersion. Deriving suitable test statistics for a LRT with adjusted dispersion parameters seems difficult, so it may be that people just said: whatever, we'll just develop a test conditional on a fixed dispersion, and that's it. Still, keeping the dispersion fixed seems weird to me. As you say, the most complex model likely fits tighter to the data, so using its dispersion parameter also for the simpler models should lead to suboptimal likelihoods, which would seem to create a bias towards larger complexity.A more sensible approach that makes use of modern computing power seems to me to fit both models including the dispersion with a full likelihood, and then do a simulated LRT for the comparison. ","Display_name":"Florian Hartig","Creater_id":48591,"Start_date":"2016-08-11 08:53:49","Question_id":226152}
{"_id":{"$oid":"5837a57aa05283111e4d4647"},"Last_activity":"2016-05-03 19:33:10","Creator_reputation":882,"Question_score":1,"Answer_content":"You ask a very important question, Alan, and have received some fine answers above. I would like to offer a simpler answer, and also indicate an additional dimension to the distinction that the above answers have not addressed. For simplicity, everything I'll say here relates to parametric statistical models.First of all, you may find the idea of a family helpful for connecting your question with things you've learned in high school. (I am surprised that this word has not yet appeared on this page!) You long ago learned about the quadratic family of curves, . You can think of a parametric statistical model in the same way, as a family of distributions. You have probably done lab experiments in chemistry or physics classes, where you collected data and plotted them in order to identify parameters from a simple family of models like  or . At the highest level, estimating the parameters of a statistical model very much resembles the process of finding the slope  and intercept , or finding the spring constant . As you continue to study mathematics, you will see 'families' of various sorts of entities pop up everywhere.So, my brief Answer #1 to your question is: a statistical model is a family of distributions.The further point I wanted to make relates to the qualifier, statistical. As Judea Pearl points out in his \"golden rule of causal analysis\" [1,p350],  No causal claim can be established by a purely statistical method, be it propensity scores, regression, stratification, or any other distribution-based design.(For present purposes, I would invite you to read \"statistical\" in place of \"distribution-based,\" and \"model\" in place of \"design.\") What Pearl is keen to convey is that our models of causal effects in the world (think , for example!) necessarily embody more than purely statistical ideas. Thus, taking your question as titled---i.e., without the qualification statistical attached to model---a full answer requires the further admonition that models generally incorporate causal ideas that lie inherently outside the province of statistics, i.e. of statements about probability distributions.Thus, my Answer #2 to your question is: models usually embody causal ideas that cannot be expressed in purely distributional terms.[1]: Pearl, Judea. Causality: Models, Reasoning and Inference. 2nd edition. Cambridge, U.K. ; New York: Cambridge University Press, 2009.","Display_name":"David C. Norris","Creater_id":41404,"Start_date":"2016-05-03 19:33:10","Question_id":210403}
{"_id":{"$oid":"5837a57aa05283111e4d4648"},"Last_activity":"2016-05-03 03:24:52","Creator_reputation":25275,"Question_score":21,"Answer_content":"Probability distribution is a mathematical function that describes a random variable. A little bit more precisely, it is a function that assigns probabilities to numbers and it's output has to agree with axioms of probability.Statistical model is an abstract, idealized description of some phenomenon in mathematical terms using probability distributions. Quoting Wasserman (2013):  A statistical model  is a set of distributions (or  densities or regression functions). A parametric model is a set   that can be parameterized by a finite number of  parameters. [...]      In general, a parametric model takes the form     \\mathfrak{F} = \\{ f (x; \\theta) : \\theta \\in \\Theta \\}     where  is an unknown parameter (or vector of parameters) that  can take values in the parameter space . If  is a  vector but we are only interested in one component of , we  call the remaining parameters nuisance parameters. A nonparametric  model is a set  that cannot be parameterized by a  finite number of parameters.In many cases we use distributions as models (you can check this example). You can use binomial distribution as a model of counts of heads in series of coin throws. In such case we assume that this distribution describes, in simplified way, the actual outcomes. This does not mean that this is an only way on how you can describe such phenomenon, neither that binomial distribution is something that can be used only for this purpose. Model can use one or more distributions, while Bayesian models specify also prior distributions.More formally this is discussed by McCullaugh (2002):  According to currently accepted theories [Cox and Hinkley (1974),  Chapter 1; Lehmann (1983), Chapter 1; Barndorff-Nielsen and Cox  (1994), Section 1.1; Bernardo and Smith (1994), Chapter 4] a  statistical model is a set of probability distributions on the sample  space . A parameterized statistical model is a parameter   set together with a function , which assigns to each parameter point   a probability distribution  on  . Here  is the set of all  probability distributions on . In much of the following, it is  important to distinguish between the model as a function , and the associated set of  distributions .So statistical models use probability distributions to describe data in their terms. Parametric models are also described in terms of finite set of parameters.This does not mean that all statistical methods need probability distributions. For example, linear regression is often described in terms of normality assumption, but in fact it is pretty robust to departures from normality and we need assumption about normality of errors for confidence intervals and hypothesis testing. So for regression to work we don't need such assumption, but to have fully specified statistical model we need to describe it in terms of random variables, so we need probability distributions. I write about this because you can often hear people saying that they used regression model for their data -- in most such cases they rather mean that they describe data in terms of linear relation between target values and predictors using some parameters, than insisting on conditional normality.McCullagh, P. (2002). What is a statistical model? Annals of statistics, 1225-1267.Wasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-05-02 03:16:33","Question_id":210403}
{"_id":{"$oid":"5837a57aa05283111e4d4649"},"Last_activity":"2016-05-02 11:06:13","Creator_reputation":147383,"Question_score":8,"Answer_content":"Think of  as a set of tickets.  You can write stuff on a ticket.  Usually a ticket starts out with the name of some real-world person or object that it \"represents\" or \"models.\"  There's lots of blank space on each ticket for writing other things.You can make as many copies of each ticket as you want.  A probability model  for this real-world population or process consists of making one or more copies of every ticket, mixing them up, and putting them in a box.  If you--the analyst--can establish that the process of drawing one ticket randomly from this box emulates all the important behavior of what you are studying, then you can learn much about the world by thinking about this box.  Because some tickets may be more numerous in the box than others, they may have difference chances of being drawn.  Probability theory studies these chances.When numbers are written on the tickets (in a consistent way), they give rise to (probability) distributions.  A probability distribution merely describes the proportion of tickets in a box whose numbers lie within any given interval.Because we usually don't know exactly how the world behaves, we have to imagine different boxes in which the tickets appear with different relative frequencies. The set of these boxes is . We view the world as being adequately described by the behavior of one of the boxes in .  It is your objective to make reasonable guesses as to which box it is, based on what you see on the tickets you have pulled out of it.As an example (which is practical and realistic, not a textbook toy), suppose you are studying the rate  of a chemical reaction as it varies with temperature.  Suppose that the theory of chemistry predicts that within the range of temperatures between  and  degrees, the rate is proportional to the temperature. You plan to study this reaction at both  and  degrees, making several observations at each temperature.  You therefore make up a very, very large number of boxes.  You are going to fill each box with tickets.  There is a rate constant written on each one.  All the tickets in any given box have the same rate constant written on them.  Different boxes use different rate constants.  Using the rate constant written on any ticket, you also write down the rate at  and the rate at  degrees: call these  and .  But this is not yet enough for a good model.   Chemists also know that no substance is pure, no quantity is exactly measured, and other forms of observational variability occur.  To model these \"errors,\" you make very, very many copies of your tickets.  On each copy you change the values of  and .  On most of them you change them only a little.  On a very few, you might change them a lot.  You write down as many changed values as you plan to observe at each temperature.  These observations represent possible observable outcomes of your experiment. Into the box go each such set of these tickets: it is a probability model for what you might observe for a given rate constant.  What you do observe is modeled by drawing a ticket from that box and reading only the observations written there.  You don't get to see the underlying (true) values of  or .  You don't get to read the (true) rate constant.  Those aren't afforded by your experiment.Every statistical model must make some assumptions about the tickets in these (hypothetical) boxes.  For instance, we hope that when you modified the values of the  and , you did so without consistently increasing or consistently decreasing either one (as a whole, within the box): that would be a form of systematic bias.  Because the observations written on each ticket are numbers, they give rise to probability distributions.  The assumptions made about the boxes typically are phrased in terms of properties of those distributions, such as whether they must average out to zero, be symmetric, have a \"bell curve\" shape, are uncorrelated, or whatever.That's really all there is to it.  Much in the way that a primitive twelve-tone scale gave rise to all of Western classical music, a collection of ticket-containing boxes is a simple concept that can be used in extremely rich and complex ways.  It can model just about anything, ranging from a coin flip to a library of videos, databases of Website interactions, quantum mechanical ensembles, and anything else that can be observed and recorded.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-05-02 11:06:13","Question_id":210403}
{"_id":{"$oid":"5837a57aa05283111e4d464a"},"Last_activity":"2016-05-02 09:32:50","Creator_reputation":550,"Question_score":2,"Answer_content":"A model is specified by a PDF, but it is not a PDF.Probability distribution (PDF) is a function that assigns probabilities to numbers and its output has to agree with axioms of probability, like Tim explained.A model is fully defined by a probability distribution, but it is more than that. In the coin tossing example, our model could be \"coin is fair\" + \"each throw is independent\". This model is specified by a PDF that is a binomial with p=0.5.However, one could imagine a model where the throws are not independent, in which case it is no longer described by the binomial PDF. Still, the model is specified by the joint distribution (a PDF) of all events .The point being, formally, a model is always specified by the joint distribution over events.One distinction between the model and the PDF is that a model can be interpreted as a statistical hypothesis. For example, in coin tossing, we can consider the model where the coin is fair (p=0.5), and that each throw is independent (binomial), and say that this is our hypothesis, which we want to test against a competing hypothesis.You can also have competing models (e.g. we don't know  and we want to compute which  is the best fit). It does not make sense to speak of competing PDFs because they are just a mathematical object.","Display_name":"J. C. Leit\u0026#227;o","Creater_id":12100,"Start_date":"2016-05-02 09:32:50","Question_id":210403}
{"_id":{"$oid":"5837a57aa05283111e4d464b"},"Last_activity":"2016-05-02 08:33:44","Creator_reputation":121,"Question_score":2,"Answer_content":"A probability distribution gives all the information about how a random quantity fluctuates. In practice we usually do not have the full probability distribution of our quantity of interest. We may know or assume something about it without knowing or assuming that we know everything about it. For example, we might assume that some quantity is normally distributed but know nothing about the mean and variance. Then we have a collection of candidates for the distribution to choose from; in our example, it is all possible normal distributions. This collection of distributions forms a statistical model. We use it by gathering data and then restricting our class of candidates so that all the remaining candidates are consistent with the data in some appropriate sense. ","Display_name":"Ian","Creater_id":103264,"Start_date":"2016-05-02 08:33:44","Question_id":210403}
{"_id":{"$oid":"5837a57aa05283111e4d464c"},"Last_activity":"2016-05-02 05:03:59","Creator_reputation":2443,"Question_score":3,"Answer_content":"The definition of a distribution as assigning probabilities to each possible event works for discrete distribution, but becomes trickier for continuous distributions, where e.g. any number on the real line could be the outcome. Very often when talking about distributions, we think of them as having fixed parameters such as a binomial distribution having two parameters: firstly, the number of observations and secondly a probability  of a single observation being an event.Typical parametric statistical models describe how the parameter(s) of a distribution depend on certain things such as factors (a variable that has discrete values) and covariates (continuous variables). For example, if in a normal distribution you assume that the mean can be described by some fixed number (an \"intercept\") and some number (a \"regression coefficient\") times the value of a covariate, you obtain a linear regression model with a normally distributed error term. For a binomial distribution, one commonly used model (\"logistic regression\") is to assume that the logit of the probability  of an event () can be described by a regression equation such as . Similarly, for a Poisson distribution a common model is to assume this for the logarithm of the rate parameter (\"Poisson regression\").","Display_name":"Bj\u0026#246;rn","Creater_id":86652,"Start_date":"2016-05-02 05:03:59","Question_id":210403}
{"_id":{"$oid":"5837a57aa05283111e4d4659"},"Last_activity":"2016-08-18 08:04:32","Creator_reputation":19081,"Question_score":0,"Answer_content":"The current trend in forecasting is to forecast the distribution of outcomes, e.g. take a look at fan charts popularized by Bank of England.The expectation (mean) is a popular choice for a point forecast. However, there are many other kinds of point forecasts. For instance, the naive forecast: your forecast equals the last observed value. Hence, yes, you can use a median.The reason why mean is used so much is because it happens to be an optimal point forecast, i.e. it minimizes the expected forecast error. However, depending on your lost function and the forecast error distribution the mean may be suboptimal, and some other point forecast would be better. In some cases that could be the median which is optimal.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-08-18 08:04:32","Question_id":230377}
{"_id":{"$oid":"5837a57aa05283111e4d465a"},"Last_activity":"2016-08-18 06:17:47","Creator_reputation":152503,"Question_score":2,"Answer_content":"Yes. It's quite possible to use a median as a forecast. In situations where the location (level) and variability is not really changing over time, and the serial dependence between consecutive observations is weak, means or medians (or various other possible estimates of location) may be quite reasonable as forecasts.Medians would tend to be more suitable than means when the distribution is peaky/heavy tailed, for example. If you have a lot of zeros you may end up forecasting 0. That may be good or not so good depending on what kind of properties you seek.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-18 06:10:27","Question_id":230377}
{"_id":{"$oid":"5837a57aa05283111e4d4669"},"Last_activity":"2016-08-18 07:45:04","Creator_reputation":328,"Question_score":1,"Answer_content":"Training deeper networks becomes harder as you can incur into the Vanishing Gradient Problem, especially if the activation function you are using is the sigmoid.If you look at the derivative of the sigmoid function you will see that it's bell-shaped:This means that there is plenty of possibilities for the gradient of the sigmoid to be very low. The gradients of the parameters of earlier layers are calculated via multiplication with the gradients of further layers. If the gradients are typically less than 1, you can see how, as you add more layers, the gradient of earlier layers gets pushed towards 0. This is explained in detail from Michael Nielsen's Neural Networks and Deep Learning book (chapter 5).One solution would be to use alternative activation functions, such as ReLu, or Exponential Linear Unit, which do not have low gradients for large inputs.Another \"solution\" would be to just employ a much larger number of epochs.Moreover, make sure that proper initialization of the weights (Xavier) is employed.","Display_name":"fstab","Creater_id":40719,"Start_date":"2016-08-18 07:41:36","Question_id":230193}
{"_id":{"$oid":"5837a57aa05283111e4d466a"},"Last_activity":"2016-08-18 02:03:38","Creator_reputation":403,"Question_score":1,"Answer_content":"For a deep neural network that you mention, finding an effective local minima is the key. As per the paper, Gülçehre, Çağlar, and Yoshua Bengio. \"Knowledge matters: Importance of prior information for optimization.\" Journal of Machine Learning Research 17.8 (2016): 1-32.It says  Deeper Networks are Harder Hypothesis: Although solutions may exist,  effective local minima are generally more likely to hamper learning as  the required depth of the architecture increases.","Display_name":"prashanth","Creater_id":86202,"Start_date":"2016-08-18 02:03:38","Question_id":230193}
{"_id":{"$oid":"5837a57aa05283111e4d466b"},"Last_activity":"2016-08-17 23:17:11","Creator_reputation":195,"Question_score":1,"Answer_content":"More hidden layers shouldn't prevent convergence,  although it becomes more challenging to get a learning rate that updates all layer weights efficiently. However, if you are using full-batch update, you should be able to determine a learning rate low enough to make your neural network progress or always decrease the objective function.Assuming that you are using full-batch update, at a given iteration, in order to guarantee sufficient objective function decrease without manually specifying a learning rate, you can perform line search to find a learning rate that satisfies the two Wolfe conditions. You can also use L-BFGS with line search to optimise your neural network efficiently. L-BFGS uses an approximation of the Hessian (second order gradient) which in a way sets a learning rate for every parameter. minFunc for Matlab and scipy.optimise for python have L-BFGS.However, before going further I would do the following two things:1) I would check that the implementation is correct by checking that the gradient function is correct. This link 1 shows a method for checking that the gradient function is correct. First, numerically approximate the gradient for a parameter using the function value, then compare it with the value returned by the gradient function. The two values should be approximately the same.2) I would also compare the neural network (NN) results with easy-to-use NN code available online. The autograd python library shows a quick implementation of multi-layer perceptron and its performance on a toy example. You can set the learning rate and the number of hidden layers fairly easy.Hope this helps!","Display_name":"Curious","Creater_id":26696,"Start_date":"2016-08-17 23:17:11","Question_id":230193}
{"_id":{"$oid":"5837a57aa05283111e4d466c"},"Last_activity":"2016-08-17 04:23:30","Creator_reputation":176,"Question_score":2,"Answer_content":"If your error function diverges try reducing your learning rate from 0.7 to say 0.007 or 0.07. More or less hidden layers should not affect convergence though the generalization power of the two would be different.","Display_name":"Amitoz Dandiana","Creater_id":103889,"Start_date":"2016-08-17 04:23:30","Question_id":230193}
{"_id":{"$oid":"5837a57aa05283111e4d4679"},"Last_activity":"2016-08-18 07:44:58","Creator_reputation":5445,"Question_score":1,"Answer_content":"The statements quoted from the document you linked to are consistent with each other.  The means of the random effects are usually assumed to be zero.This is the usual assumption in mixed effects model, and very often a multivariate normal distribution with mean vector of zeros is assumed.  If a factor is random, then it is assumed to be a variable that is sampled from a population that has a particular mean and varianceHere they are talking about the population that the random effects are a sample from. The mean is typically estimated as a fixed effect, and the random effects are the deviations from that mean, so that the mean of the random effect is zero. For example, with a random intercept, a global intercept (fixed effect) would be estimated along with the variance of the random intercepts, which then allows each level of the factor to have it's own intercept (the global intercept plus the factor-level intercept). Edit: As another example, consider a random intercepts and slopes model: y_{ij}= (\\beta_0 + u_{0j})+(\\beta_1 + u_{1j}) x_{1ij} +\\epsilon_{ij},where  are the random intercepts and  are the random slopes. We can think of these either on their own, with a mean of zero, or together with their respective fixed effects,  and  respectively, where  and  would be their means.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-15 08:41:14","Question_id":229932}
{"_id":{"$oid":"5837a57aa05283111e4d4686"},"Last_activity":"2016-08-18 07:22:38","Creator_reputation":328,"Question_score":0,"Answer_content":"Classifiers work generally better with lower dimensionalities, but of higher quality. Maybe the additional features are too noisy and are detrimental to the learning?What about the training accuracy, does it go down as well as the testing accuracy? If not, then this is surely a case of overfitting. This might happen when the classifier learns from the noise.In order to reduce the noise, you might try dimensionality reduction techniques, such as PCA or autoencoders (I strongly suggest for autoencoders, as they learn nonlinear mappings). Then you can run your classification algorithm on the learned codes.","Display_name":"fstab","Creater_id":40719,"Start_date":"2016-08-18 07:22:38","Question_id":230271}
{"_id":{"$oid":"5837a57aa05283111e4d4692"},"Last_activity":"2016-08-18 06:57:29","Creator_reputation":25275,"Question_score":2,"Answer_content":"If you are looking for average distance from the mean, then the most common choice of such measure is variance. If  is your sample and  is it's arithmetic mean, then we define variance as \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar x)^2  so we take average squared difference from the mean. To have your measure in the same scale as your data, you can take square root of it, i.e. calculate standard deviation.There are also other measures as well, for example if you were working with medians, you could use more robust median absolute deviation, that is defined as \\operatorname{median}\\left(\\ \\left| X_{i} - \\operatorname{median} (X) \\right|\\ \\right) However squaring the difference and using variance (or standard deviation) is preferred since it has many nice properties, what you could learn from following threads:Why square the difference instead of taking the absolute value in standard deviation?Why should I prefer the standard deviation over other measures of variance?Understanding \u0026quot;variance\u0026quot; intuitivelyor Why is variance calculated by squaring the deviations? thread on ResearchGate.net.Moreover, in your case squaring penalizes the extreme outlying cases more then taking absolute value, so it better serves for your purpose.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-18 06:46:20","Question_id":230462}
{"_id":{"$oid":"5837a57aa05283111e4d469f"},"Last_activity":"2016-08-18 06:35:29","Creator_reputation":1357,"Question_score":1,"Answer_content":"2:) makes no sense to me. You would only do a t-test between control/treated if you want to test for difference in the sample means, but not for calculating the fold-change.Fold change is typically calculated by simply average of group 2/ average of group 1. I'll give you a proof, in http://seqanswers.com/forums/showthread.php?t=49101, the author of DESeq2 wrote:(average in group2)/(average in group1)The question is why would you want to do this? There are good Bioconductor packages that can do that for you. For example, DESeq2 applies shrinkage methods to the fold-changes. Raw fold-change is not informative in bioinformatic statistical analysis, because it doesn't address the expression level (and variance) of the gene. Highly and lowly expressed genes can give you the same fold-change, and you don't want this to happen.","Display_name":"Student T","Creater_id":34623,"Start_date":"2016-08-18 06:35:29","Question_id":230484}
{"_id":{"$oid":"5837a57aa05283111e4d46ae"},"Last_activity":"2016-08-18 06:28:04","Creator_reputation":3619,"Question_score":1,"Answer_content":"The classical definition considered a finite set of outcomes each of which was considered equally likely. So for example by symmetry you consider the chances of each face of a die as being equally likely. The probability is then one over the number of possible events (so 1/6 for a standard cubic die). The frequentist interpretation used the concept of long-run frequency so could deal with infinite sequences.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-18 06:28:04","Question_id":230438}
{"_id":{"$oid":"5837a57ba05283111e4d46c3"},"Last_activity":"2016-08-18 04:51:43","Creator_reputation":1096,"Question_score":0,"Answer_content":"Lets denote a discrete uniform RV in  by  where we assume  and  is total number of observations, both integers. On the same way we denote a RV from DUNIF(0,n) by  where again  is the total number of observations. Let do randomizations. 1- in the first one we sample one observation from each . then repeating  times to get samples which are assigned to the test(validation) set.2- in the second one we select  samples from  and assign them to the test set.The first one is equivalent to sampling 10 times from . That is you are sampling from . the second one is k samples directly from . Notice that uniform distribution is consistence with respect to the change in location. As a result, both methods are the same. But in actions, the first one is heavier in term of running by a computer as it needs three steps,1) dividing data into folds, 2)sampling from folds (a loop) and storing the result.But the second one needs two steps,1)sampling from data,2)storing data.Note that if  is not an integer the second strategy is better, obviousely.","Display_name":"TPArrow","Creater_id":46139,"Start_date":"2016-08-18 04:51:43","Question_id":230475}
{"_id":{"$oid":"5837a57ba05283111e4d46c4"},"Last_activity":"2016-08-18 04:37:09","Creator_reputation":618,"Question_score":1,"Answer_content":"The latter is better because it guarantees your folds are of equal size (or within one row of equal). In the case of not being divisible by ten then having some folds be one record smaller is fine.","Display_name":"Dex Groves","Creater_id":109345,"Start_date":"2016-08-18 04:37:09","Question_id":230475}
{"_id":{"$oid":"5837a57ba05283111e4d46d1"},"Last_activity":"2016-08-18 04:25:27","Creator_reputation":12907,"Question_score":1,"Answer_content":"See structural VAR (SVAR) modelling, e.g. in Lütkepohl \"New Introduction to Multiple Time Series Analysis\" (2005), Chapter 9;Pfaff \"Analysis of Integrated and Cointegrated Time Series with R\" (2008), Section 2.3; orthe vignette of \"vars\" package in R, Section 2.2; among other sources. SVAR is a form of VAR that allows for contemporaneous relations between variables. Actually, to estimate a SVAR you first turn it into a corresponding VAR representation and then turn the results back into the SVAR representation. The latter step can be done under special conditions related to the concept of identification.If your series happen to be integrated and cointegrated, a special form of SVAR, namely, a structural VECM (SVECM) could be the relevant model. It is also covered in the sources cited above.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-18 03:12:45","Question_id":230450}
{"_id":{"$oid":"5837a57ba05283111e4d46e0"},"Last_activity":"2016-08-18 04:06:49","Creator_reputation":176,"Question_score":0,"Answer_content":"Depends on what you want to achieve from the classification?Say it is cancer v/s non cancer, then detecting cancer is vital. However since non-cancer will form majority of your data the classifier can essentially send all cases to non-cancer class and get very high accuracy. But we can't afford that, so we essentially down sample non-cancer cases, essentially moving the decision boundary away from cancer region into the non-cancer region.Even in use cases where accuracy is our only aim, balancing can be essential if the test time balance is expected to be different from train time.For example say you want to classify mangoes and oranges, you have a training dataset with 900 mangoes and 30 oranges, but you expect to deploy it in a marketplace with equal mangoes and oranges, then ideally you should sample in the expected sample ratio to maximize accuracy.","Display_name":"Amitoz Dandiana","Creater_id":103889,"Start_date":"2016-08-18 04:06:49","Question_id":227088}
{"_id":{"$oid":"5837a57ba05283111e4d46ed"},"Last_activity":"2016-08-18 03:54:36","Creator_reputation":23,"Question_score":0,"Answer_content":"DIY (R code):# - log likelihoodll = function(par){if(par[1]\u0026gt;0 \u0026amp; par[2]\u0026gt;0) return(-sum(dgamma(data,shape=par[1],rate=par[2],log=T)))else return(Inf)}# initial pointinit0 = c(1,0.1)# Optimisation stepOPT = optim(init0,ll)# -log likelihoodOPTpar# fitted densityfit.den = function(x) dgamma(x,shape=OPTpar[2])hist(data,probability=T)curve(fit.den,0,8000,add=T,col=\"red\")","Display_name":"Tim Allen","Creater_id":128118,"Start_date":"2016-08-18 03:46:43","Question_id":230420}
{"_id":{"$oid":"5837a57ba05283111e4d46ee"},"Last_activity":"2016-08-18 02:45:24","Creator_reputation":3680,"Question_score":2,"Answer_content":"I would definitely recommend scaling your data, probably even with 1000 and not just 10. I can't think of any reason why it would be necessary (let alone vital to the future of mankind) to compute the log-likelihood on the original scale rather than the transformed scale. Simply do all calculations on the transformed scale and you are fine.Having said that it is easy to simply call sum(dgamma(..., log = TRUE)) with the transformed parameters to obtain the log-likelihood on the desired scale. For illustration let's look at the fit on the scale divided by 10 as you did and divided by 1000 as I would recommend:## scaled datad10 \u0026lt;- data/10d1000 \u0026lt;- data/1000## corresponding fitdistrf10 \u0026lt;- fitdistr(d10, \"gamma\")f1000 \u0026lt;- fitdistr(d1000, \"gamma\")Then we can look at the shape parameters which essentially agrees10 \u0026lt;- coef(f10)[\"shape\"]s1000 \u0026lt;- coef(f1000)[\"shape\"]c(s10, s1000)##     shape     shape ## 0.8971837 0.8973881 And then we compute the rate parameters on the scale divided by 10 and they also essentially agree:r10_10 \u0026lt;- coef(f10)[\"rate\"]r10_1000 \u0026lt;- coef(f1000)[\"rate\"]/100c(r10_10, r10_1000)##       rate       rate ## 0.00648812 0.00649091 And then we can compute the log-likelihoods by hand or extract them from f10. l10_10 \u0026lt;- sum(dgamma(d10, shape = s10, rate = r10_10, log = TRUE))l10_1000 \u0026lt;- sum(dgamma(d10, shape = s1000, rate = r10_1000, log = TRUE))c(l10_10, l10_1000, logLik(f10))## [1] -59.25158 -59.25158 -59.25158They also essentially agree but the one from the fit on the scale divided by 1000 leads to a very slightly higher (= better) log-likelihood. This reflects that the numerical properties are better on this scale as both parameters (and their standard errors) are in the same order of magnitude. c(l10_10, logLik(f10)) \u0026lt; l10_1000## [1] TRUE TRUETo conclude we can also compute the log-likelihood on the original scale:r1_10 \u0026lt;- coef(f10)[\"rate\"]/10r1_1000 \u0026lt;- coef(f1000)[\"rate\"]/1000l1_10 \u0026lt;- sum(dgamma(data, shape = s10, rate = r1_10, log = TRUE))l1_1000 \u0026lt;- sum(dgamma(data, shape = s1000, rate = r1_1000, log = TRUE))c(l1_10, l1_1000)## [1] -82.27743 -82.27743Of course, this differs from logLik(f10) and logLik(f1000) just by constants and all inference (e.g., likelihood ratio tests) or information criteria would lead to idential conclusions on any of the scales.","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2016-08-18 02:45:24","Question_id":230420}
{"_id":{"$oid":"5837a57ba05283111e4d46fb"},"Last_activity":"2016-08-18 03:38:09","Creator_reputation":197,"Question_score":1,"Answer_content":"I think you inversed the numerator with the denominator. If we want to test :H_0:\\theta\\in\\Theta_0 \\space vs\\space H_1:\\theta\\in\\Theta_0^cwhere  is the parametric space, then: \\Lambda(x)=\\frac{\\sup_{\\Theta_0}L(\\theta|x)}{\\sup_{\\Theta}L(\\theta|x)} Assuming that,  should be written like this:\\Lambda = \\frac{\\hat p^{\\sum_{i=1}^mx_i}(1-\\hat p)^{\\sum_{i=1}^m n_i-x_i}}{\\prod_{i = 1}^m \\hat p_i^{x_i}(1-\\hat p_i)^{n_i-x_i}}.","Display_name":"Toney Shields","Creater_id":109437,"Start_date":"2016-08-18 03:38:09","Question_id":230455}
{"_id":{"$oid":"5837a57ba05283111e4d4708"},"Last_activity":"2016-08-18 03:37:52","Creator_reputation":152503,"Question_score":5,"Answer_content":"Since  and  are both proportional to a known expression* (the unscaled probabilities, , with the same unknown constant of proportionality, ) and you know the  values must add to , then . Which is to say .(This notion is widely used in Bayesian statistics with many kinds of discrete variables.)Note that  (always, since the power is ), so  and *(these 's are  in the paper, for  and  respectively) ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-17 22:25:27","Question_id":230413}
{"_id":{"$oid":"5837a57ba05283111e4d4715"},"Last_activity":"2016-08-18 03:35:58","Creator_reputation":176,"Question_score":1,"Answer_content":"Recall where this term actually comes from: the amount of weight decay we want to have for each weight at every iterationdel(E)/del(w(j,k,l))= del(left cross entropy error) + λ*(w(j,k,l)say the λ is 0.001 essentially it means if Error is not affected by this particular weight, decay it by 0.1%. The number of units and layers does not affect the decay we want in a particular weight.In his paper I don't think he is normalizing loss by n samples as you are doing:but rather adding a 1/nsamples term to the gradient where nsamples is the minibatch size. So effectively his λ[his]= λ[yours]*nsamples. As you correctly said λ[yours] doesn't depend on mini batch size, but λ[his] depends, so he assumes λ is the regulartization parameter he would use for full batch gradient descent, and normalizes it by (nsamples/training set size) if doing it with mini-batches.","Display_name":"Amitoz Dandiana","Creater_id":103889,"Start_date":"2016-08-18 03:35:58","Question_id":227074}
{"_id":{"$oid":"5837a57ba05283111e4d4724"},"Last_activity":"2015-10-23 01:59:43","Creator_reputation":25275,"Question_score":1,"Answer_content":"Assuming that the only thing that you have is an empirical distribution, the simplest way to go is to draw values from  and reject if  and . Less naive implementation would be to subset  values so to drop the values below threshold and draw from , the same way as you would do with any other discrete distribution. Simple example in R of such approach can be find below.set.seed(123)c1 \u0026lt;- -1c2 \u0026lt;- 0.5X \u0026lt;- data.frame(X1 = rnorm(100), X2 = rnorm(100)) # creating fake dataX_trunc \u0026lt;- subset(X, X1 \u0026gt; c1 \u0026amp; X2 \u0026gt; c2) # subset# draw 1000 samplesX_trunc[sample(nrow(X_trunc), 1000, replace = TRUE), ]The above example assumes that you have the full data, however if the only thing that you have is the empirical distribution tables with probabilities for , than the procedure is the same but you draw the  pairs with  probabilities as in the example below.library(dplyr)# lets calculate the probabilities for x1,x2 pairsFX \u0026lt;- group_by(X, X1, X2) %\u0026gt;%  summarise(n = n()) %\u0026gt;%  ungroup() %\u0026gt;%  mutate(prob = n/sum(n))# next, we subset and sample as above but from F(X1,X2)FX_trunc \u0026lt;- subset(FX, X1 \u0026gt; c1 \u0026amp; X2 \u0026gt; c2)# notice that here we sample with parameter prob set to F(x1,x2)FX_trunc[sample(nrow(FX_trunc), 1000, replace = TRUE, prob = FX_trunc$prob), ]Drawing from bivariate distribution does not differ in here from drawing from univariate distribution, the values to be drawn are pairs, or more precisely indexes for those pairs.","Display_name":"Tim","Creater_id":35989,"Start_date":"2015-10-22 13:17:54","Question_id":178119}
{"_id":{"$oid":"5837a57ba05283111e4d4731"},"Last_activity":"2016-08-18 03:05:28","Creator_reputation":150,"Question_score":0,"Answer_content":"I'm not sure if I understand your overall question, but I might answer some.You're asking if you need all 4 independent variables? You're saying dependent, but if they were dependent your 4 variables would be redundant - just pick the one with the highest desirable effect. If they are independent you can use a Rough Set approach to pick out the variables with the highest desirable effect. I'll explain how if I've understood your question correctly.","Display_name":"Lennart","Creater_id":85590,"Start_date":"2016-08-18 03:05:28","Question_id":230439}
{"_id":{"$oid":"5837a57ba05283111e4d4740"},"Last_activity":"2016-08-18 02:49:31","Creator_reputation":403,"Question_score":0,"Answer_content":"To check the performance of a split, as you mentioned MSE and RMSE are the popular approaches. RMSLE penalizes an under-predicted estimate greater than an over-predicted estimate ϵ is the RMSLE value (score) n is the total number of observations in the (public/private) data set, pi is your prediction, and ai is the actual response for i. log(x) is the natural logarithm of x.Ref:https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicErrorhttp://stats.stackexchange.com/a/110610/86202","Display_name":"prashanth","Creater_id":86202,"Start_date":"2016-08-18 02:49:31","Question_id":230316}
{"_id":{"$oid":"5837a57ba05283111e4d474f"},"Last_activity":"2016-08-18 01:40:50","Creator_reputation":56,"Question_score":3,"Answer_content":"Summarising from McHugh (2013), the  test has six conditions specific to the test. Two relevent assumptions here are:\"Each subject may contribute data to one and only one cell in the test\"\"The value of the cell expecteds should be 5 or more in at least 80% of the cells, and no cell should have an expected of less than one\"The first assumption is usually used to caution about applying to panel data, but would also apply here, depending on how you tabulated your 21 items (assuming each subject could have multiple 'items'). The second assumption is self explanatory, and a practical example is given here.  A reproducible example / clearer description would help with suggestions for alternatives. However, common options are Fisher's exact test, and (according to McHugh) the Maximum likelihood ratio  test. If you have matched pairs, then McNemar's test.NB. Fisher's exact test assumes fixed totals. Read more here. ","Display_name":"Simon","Creater_id":67446,"Start_date":"2016-08-14 17:30:27","Question_id":229833}
{"_id":{"$oid":"5837a57ba05283111e4d4750"},"Last_activity":"2016-08-16 22:16:03","Creator_reputation":1357,"Question_score":0,"Answer_content":"Your experiment looks like a 4x4 contingency table, where the rows and columns represent expected and observed cell counts.Since your input size is small, you should consider the Fischer's Exact test. The chi-square test is not recommended because your data set will give poor approximation to normality (the test is only valid with a large enough data set).","Display_name":"Student T","Creater_id":34623,"Start_date":"2016-08-16 22:16:03","Question_id":229833}
{"_id":{"$oid":"5837a57ba05283111e4d475d"},"Last_activity":"2016-08-18 02:05:46","Creator_reputation":150,"Question_score":0,"Answer_content":"What you want to do: \"..To compare the coefficients of each sets of data that I have to be able to tell how different they were*.\" So the question is, how do you figure out the coefficients? The dataset is obviously not linear, in other words the model have variables with an exponantiation higher than 1 e.g x^2+x+5=yYou can find out the coefficients with Newton's method - there are many resources on how to do it. There are certainly packages in matlab/R.You also want to find out which model is the better one: the red or the green one with respect to the actual data, yes? You can compare and evaluate how good the model is by calculating the Root mean square error (RMSE) of each model and pick the model with the least RMSE.Did this answer your question?","Display_name":"Lennart","Creater_id":85590,"Start_date":"2016-08-18 02:05:46","Question_id":230275}
{"_id":{"$oid":"5837a57ba05283111e4d476a"},"Last_activity":"2016-08-18 02:03:08","Creator_reputation":2762,"Question_score":0,"Answer_content":"The numerator is the summation over a -dimensional one-hot vector, so the indicator variable only equal  to 1 once.The denominator is the unnormalized zeroth moment, and can be shown to be equal to the summation over  summations of such -dimensional vectors.See this question for more details Derivation of expectation-maximization in General - PRML","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-08-18 01:58:07","Question_id":210815}
{"_id":{"$oid":"5837a57ba05283111e4d477b"},"Last_activity":"2016-08-18 01:11:30","Creator_reputation":962,"Question_score":0,"Answer_content":"I give an example how this would look like, when you work with the two subset-PC's (PCs of subset A1,A2, PCs of subset B1,B2) in a joint vectorspace.       I just generate manually a correlation matrix with an example, where the subsets have strong correlation in them, but small correlation between them. For manipulation as \"selective pca\" I use my homebrewn matrix-tool \"MatMate\", I think the matrix-language is also nice pseudocode...     0) We begin with the cholesky-decomposition of the correlation-matrix to have a common vector space for all items[23] lad = cholesky(cor):      A1:   1.000   0.000   0.000   0.000:      A2:   0.981   0.195   0.000   0.000:      B2:   0.335  -0.726   0.601   0.000:      B2:   0.396  -0.706   0.573   0.1261.1a) next we find the principal directions of the subset A; the \"1..2\" in the rotation-call means: use rows 1..2 only for the computation of the rotation-angles:[27] lad = rot(lad,\"pca\",1..2):      A1:   0.995  -0.098   0.000   0.000:      A2:   0.995   0.098   0.000   0.000:      B2:   0.263  -0.755   0.601   0.000:      B2:   0.325  -0.741   0.573   0.1261.1b) to be able to work later with the according principal components we add latent items to the matrix which point into the principal directions[28] lad= {lad,marksp(lad,{1,2}}:      A1:   0.995  -0.098   0.000   0.000:      A2:   0.995   0.098   0.000   0.000:      B2:   0.263  -0.755   0.601   0.000:      B2:   0.325  -0.741   0.573   0.126------------------------------------------:   A-pc1:   1.000   0.000   0.000   0.000:   A-pc2:   0.000   1.000   0.000   0.0001.2a) next we find the principal directions of the subset B:[32] lad = rot(lad,\"pca\",3..4):      A1:   0.367   0.423  -0.480  -0.675:      A2:   0.220   0.442  -0.398  -0.773:      B2:   0.997  -0.072   0.000   0.000:      B2:   0.997   0.072   0.000   0.000------------------------------------------:   A-pc1:   0.295   0.435  -0.441  -0.728:   A-pc2:  -0.750   0.095   0.420  -0.5021.2b) ... and immediately we add the two new latent items to the loadings matrix:[33] lad= {lad,marksp(lad,{1,2}}:      A1:   0.367   0.423  -0.480  -0.675:      A2:   0.220   0.442  -0.398  -0.773:      B2:   0.997  -0.072   0.000   0.000:      B2:   0.997   0.072   0.000   0.000------------------------------------------:   A-pc1:   0.295   0.435  -0.441  -0.728:   A-pc2:  -0.750   0.095   0.420  -0.502------------------------------------------:   B-pc1:   1.000   0.000    .       .   :   B-pc2:   0.000   1.000    .       .   2.1) Now we look at the principal components \"second order\": the principal components of the two main directions of each subset (marked with \"***\" here)         [37] lad = rot(lad,\"pca\",5´7):      A1:   0.846   0.529  -0.043   0.042:      A2:   0.755   0.653   0.043  -0.042:      B2:   0.783  -0.619  -0.044  -0.047:      B2:   0.822  -0.566   0.044   0.047------------------------------------------:***A-pc1:   0.805   0.594    .       .   ******:   A-pc2:  -0.466   0.632   0.443  -0.433------------------------------------------:***B-pc1:   0.805  -0.594    .       .   ******:   B-pc2:   0.270   0.366   0.611   0.648This is very similar to the idea of canonical correlations: the main-directions of subsets of items are pca'ed (in canonical correlations: regressed on each other).But as the correlation between them (0.805*0.805 - 0.594*0.594) is small, it might be more interesting to assume they rather model orthogonal (or slightly oblique) directions.2.2) So we rotate to \"quartimax\" or \"varimax\"   structure between them:[40] lad = rot(lad,\"varimax\",5´7,1..2):      A1:   0.224   0.973  -0.043   0.042:      A2:   0.072   0.996   0.043  -0.042:      B2:   0.991   0.116  -0.044  -0.047:      B2:   0.981   0.181   0.044   0.047------------------------------------------:***A-pc1:   0.149   0.989    .       .   ******:   A-pc2:  -0.776   0.117   0.443  -0.433------------------------------------------:***B-pc1:   0.989   0.149    .       .   ******:   B-pc2:  -0.068   0.450   0.611   0.648This looks more convincing as some true model; and while we have the varimax-directions of the main-directions of the two subsets, also the two subsets show the same clear structure of loadings; (besides the small residual-covariance (columns 3 and 4) which remains when we work in a two-dimensional PC-space).[update] Just for the record. Using the PAF 2-Factor solution we have itempecific variance; this is shown here as additional columns/dimensions in the joint vectorspace. Since the iemspecific variances in my example data were small we do not see significant change in the results, so I only show here the initial PAF solution for the 4 items and the final solution based on the subset-principal components taken in the common-variance-vectorspace onlyThe initial PAF(2) solution:PAF      A1:   0.842   0.534    .       .    ||    0.083    .       .       .         A2:   0.749   0.657    .       .    ||     .      0.087    .       .         B2:   0.786  -0.611    .       .    ||     .       .      0.091    .         B2:   0.825  -0.558    .       .    ||     .       .       .      0.089The intermediate steps are not displayed, except of the final \"varimax\" rotated solution (varimax PAF-factor 1 and varimax PAF-factor 2 change position compared to the PC-solution above):PAF = rot(PAF,\"varimax\",5´7,1..2)      A1:   0.971   0.225    .       .    ||    0.083    .       .       .         A2:   0.994   0.073    .       .    ||     .      0.087    .       .         B2:   0.116   0.989    .       .    ||     .       .      0.091    .         B2:   0.181   0.979    .       .    ||     .       .       .      0.089------------------------------------------***A-pc1:   0.989   0.150    .       .    ||     .       .       .       .      A-pc2:   0.149  -0.985    .       .    ||     .       .       .       .   ------------------------------------------***B-pc1:   0.150   0.989    .       .    ||     .       .       .       .      B-pc2:   0.947  -0.143    .       .    ||     .       .       .       .   ------------------------------------------","Display_name":"Gottfried Helms","Creater_id":1818,"Start_date":"2016-08-17 03:37:13","Question_id":230180}
{"_id":{"$oid":"5837a57ba05283111e4d477c"},"Last_activity":"2016-08-16 19:25:54","Creator_reputation":338,"Question_score":0,"Answer_content":"You seem to have two different questions here.(1) Is this approach valid?Yes - you will have two variables for each sample and that will be used to predict your output. Assuming you are correct that A1 and A2 measure similar things and B1 and B2 measure similar things, your resulting model should be good (assuming, of course, you use the right model, there is a relationship between A, B, and Y, etc...).(2) Will it capture any relationship between A and B?Your dimensionality reduction procedure will not capture any relationship between A and B. However, this is not necessarily a problem. If  and  are correlated, you might have to deal with problems that come from having correlated features, but this is a different problem, not a result or your procedure.","Display_name":"roundsquare","Creater_id":122754,"Start_date":"2016-08-16 19:25:54","Question_id":230180}
{"_id":{"$oid":"5837a57ba05283111e4d478b"},"Last_activity":"2015-01-21 00:46:07","Creator_reputation":24971,"Question_score":1,"Answer_content":"I quote from the link you have provided:   The best linear predictor in terms of minimum mean squared error  (MSE), of  or 1-step forecast based on information  available at time  is     \\mathbf{Y}_{T+1|T}=c+\\mathbf{\\Pi}_1\\mathbf{Y}_T+...+\\mathbf{\\Pi}_p\\mathbf{Y}_{T-p+1}This is for VAR(p) model. You have VAR(1). Further quote  Forecasts for longer horizons  (-step forecasts) may be obtained  using chain-rule of forecasting as    \\mathbf{Y}_{T+h|T}=c+\\mathbf{\\Pi}_1\\mathbf{Y}_{T+h-1|T}+...+\\mathbf{\\Pi}_p\\mathbf{Y}_{T+h-p|T}Such type of forecasting is the commonly accepted way of forecasting VAR models. It is implemented for example in vars R package. If you compare it to the univariate case you will see that for AR(p) model the forecasts are the same. Furthermore thery are optimal in a sense that minimise the MSE, you can consult Lütkepohl (New introduction to multiple time series analysis, 2005) for more details.","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2015-01-21 00:46:07","Question_id":134280}
{"_id":{"$oid":"5837a57ba05283111e4d4798"},"Last_activity":"2016-08-17 17:47:17","Creator_reputation":26604,"Question_score":1,"Answer_content":"In general, forecast intervals from ARIMA models will increase as the forecast horizon increases. For stationary models (i.e., with no differencing), they will converge so forecast intervals for long horizons are all essentially the same. If there is some differencing, you are assuming the series is non-stationary, and the forecast intervals will continue to diverge into the future.","Display_name":"Rob Hyndman","Creater_id":159,"Start_date":"2016-08-17 17:47:17","Question_id":230269}
{"_id":{"$oid":"5837a57ba05283111e4d47a4"},"Last_activity":"2016-08-17 23:54:33","Creator_reputation":295,"Question_score":2,"Answer_content":"Let  be a (non-homogeneous) Poisson process with intensity \\lambda(t) = \\lambda_1\\mathsf 1_{[0,t_1)}(t) + \\lambda_2\\mathsf 1_{[t_1,\\infty)}(t). Then for , we have \\begin{align}\\mathbb P(N(t_2)=0) \u0026amp;= \\mathbb P(N(t_1)=0,N(t_2)-N(t_1)=0)\\\\\u0026amp;= \\mathbb P(N(t_1)=0)\\mathbb P(N(t_2)-N(t_1)=0)\\\\\u0026amp;= e^{-\\lambda_1 t_1}e^{-\\lambda_2(t_2-t_1)}.\\end{align}","Display_name":"Math1000","Creater_id":63691,"Start_date":"2016-08-17 23:54:33","Question_id":230345}
{"_id":{"$oid":"5837a57ba05283111e4d47b1"},"Last_activity":"2016-08-17 23:40:43","Creator_reputation":12907,"Question_score":1,"Answer_content":"This happens because you generate rtn and vxr independently and vxr does not affect the conditional variance of rtn in any way. When you estimate the model and look at the effect of vxr on the conditional variance of rtn, you naturally find there is no statistically significant effect. This is in line with the true data generating process (there is no effect). If you tried a large number of times, you could finally get a statistically significant result due to pure chance, though.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-17 23:40:43","Question_id":230417}
{"_id":{"$oid":"5837a57ba05283111e4d47be"},"Last_activity":"2016-08-17 23:05:11","Creator_reputation":1357,"Question_score":0,"Answer_content":"When thinking about a statistical problem, you should ask yourself what experiment you have.You have two groups, with different sample size. You can calculate arithmetic mean over all your replicates. You will be able to use the means for comparison.1:) Check if you can assume normality. Because your data is simple, you can just do a simple QQ plot or histogram.2:) If you can assume normality, you should use paired t-test.3:) If you can't assume normality, you should Wilcox signed-rank test","Display_name":"Student T","Creater_id":34623,"Start_date":"2016-08-17 23:05:11","Question_id":230343}
{"_id":{"$oid":"5837a57ba05283111e4d47bf"},"Last_activity":"2016-08-17 13:28:36","Creator_reputation":738,"Question_score":1,"Answer_content":"From reading your previous post, I see that you have two groups with 15 subjects, each with multiple observations (3 each). Each subject appears in each group, except subject 15 who has 0 observation in group 1.So, basically, you have a paired design. A way to test whether Group 1 and Group 2 are different is by using a paired wilcoxon signed rank sum test. In R, this can be done using the following code:df\u0026lt;- structure(list(Group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,                               1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,                               1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,                               1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,                               2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,                               2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), Subject = c(1L,                                                                                                    1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L,                                                                                                    6L, 7L, 7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L,                                                                                                    11L, 12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 1L, 1L, 1L,                                                                                                    2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 6L, 6L, 6L, 7L,                                                                                                    7L, 7L, 8L, 8L, 8L, 9L, 9L, 9L, 10L, 10L, 10L, 11L, 11L, 11L,                                                                                                    12L, 12L, 12L, 13L, 13L, 13L, 14L, 14L, 14L, 15L, 15L, 15L),                     Value = c(29.89577946, 29.51885854, 29.77429604, 33.20695108,                               32.09027292, 31.90909894, 30.88358173, 30.67547731, 30.82494595,                               31.70128247, 31.57217504, 31.61359752, 30.51371055, 30.42241945,                               30.44913954, 26.90850496, 0, 0, 0, 0, 0, 28.94047335, 29.27188604,                               29.78511206, 28.18475423, 27.54266717, 26.99873401, 29.26941344,                               28.50457189, 28.78050443, 31.39038527, 31.19237052, 30.74053275,                               28.68618888, 28.42109545, 28.58222544, 28.99337177, 29.31797,                               28.4541501, 28.18475423, 27.54266717, 26.99873401, 28.07576794,                               28.96344894, 28.48358437, 27.02527663, 27.1308483, 26.96091103,                               27.04019758, 27.51900858, 28.14559621, 26.83569136, 26.90724462,                               26.82675, 0, 0, 0, 27.62449786, 26.82335228, 26.66925534,                               0, 25.81254792, 26.61666776, 26.12545858, 0, 0, 0, 0, 0,                               28.84580419, 29.11003424, 29.24723895, 28.72919768, 29.70673437,                               29.31274377, 30.73133587, 30.44805655, 30.61561583, 27.06896964,                               27.04249553, 27.15990629, 31.54738209, 31.51643714, 31.8055509,                               31.291867, 31.89146186, 31.65812735)), .Names = c(\"Group\",                                                                                 \"Subject\", \"Value\"), class = \"data.frame\", row.names = c(NA,                                                                                                                                          -87L))dfValue == 0] \u0026lt;- NAdf[is.na(dfGroup, dfSubject == 15,]) ## omit the 15th patientWilcoxon rank sum test with continuity correctiondata:  Value by GroupW = 900, p-value = 0.0006732alternative hypothesis: true location shift is not equal to 0Warning message:In wilcox.test.default(x = c(29.89577946, 29.51885854, 29.77429604,  :                               cannot compute exact p-value with ties## we can reject the null hypothesis that both groups are equalFrom the R documentation,   If exact p-values are available, an exact confidence interval is obtained by the algorithm described in Bauer (1972), and the Hodges-Lehmann estimator is employed. Otherwise, the returned confidence interval and point estimate are based on normal approximations. These are continuity-corrected for the interval but not the estimate (as the correction depends on the alternative).","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-17 12:06:53","Question_id":230343}
{"_id":{"$oid":"5837a57ba05283111e4d47c0"},"Last_activity":"2016-08-17 12:08:23","Creator_reputation":1,"Question_score":0,"Answer_content":"For comparing means of populations with different samples, the standard practice is applying a two-sample location test. Depending on the size of your data, you could implement either a t-test or a z-test (t-test is recommended for smaller sample sizes). In any case, since you want to find out if the groups are different, a two-sided test would be ideal for the purpose. Since the data has different sizes, it is generally accepted that a Welch's test is the best way of performing such comparison. The Welch's test assumes unequal variances, an assumption you could test yourself. In case you have reasons to believe that the variances are equal instead, it is recommended that you use a paired t-test.In R, assuming your data is stored un vectors x and y#Welch's t-testt.test(x,y)#paired sample testt.test(x,y, paired = TRUE) #test for equality of variancesvar.test(x,y) ","Display_name":"Carlo Mazzaferro","Creater_id":128057,"Start_date":"2016-08-17 12:08:23","Question_id":230343}
{"_id":{"$oid":"5837a57ba05283111e4d47cd"},"Last_activity":"2016-08-17 22:11:04","Creator_reputation":152503,"Question_score":2,"Answer_content":"There are two possibilities here: scaling the sample range to get an approximate idea of the sample standard deviation, and using the sample range to produce an estimate of the population  (my earlier comment failed to make the distinction clear). I think your question is asking about the second, but I'll deal with both.I'll tackle these two in order.The ratio of sample range () to sample standard deviation is sometimes call the studentized range. So if the middle of the distribution of studentized range was 3, it could make sense to approximate the sample standard deviation from the range by dividing the range by 3.[Some people are no doubt wondering why we'd bother - after all why not just calculate the standard deviation instead of a noisy approximation of it? Perhaps we're trying to an eyeball estimate from a scatterplot or something, and the range is relatively quick to get by eye. Sometimes we can get the minimum and maximum but not the standard deviation.]So for the normal distribution, how is the studentized range distributed?Here are simulated distributions, for 100000 simulations at various sample sizes for normal data:         mean        sdn=6   2.663658 0.2198905n=10  3.164088 0.3085827n=30  4.119035 0.4355710n=100 5.025396 0.4884935If instead we're trying to estimate the population standard deviation, what matters is the distribution of the range at  (since we can work out the distribution for other  is obtained by direct scaling):          mean        sdn=6   2.536377 0.8495843n=10  3.080769 0.8013887n=30  4.088134 0.6918155n=100 5.016027 0.6044998This results in somewhat different distributional shape and different mean and standard deviation (though Slutsky's theorem suggests that as  becomes large they should be more and more similar).-The answers to this question discusses the interesting property that at n=6, for many different distributions the range divided by 2.5 provides a reasonable approximation to the standard deviation.Tippett produced extensive tables of the expected value of the sample range for the standardized normal in 1925 (and briefer tables of the standard deviation and standardized 3rd and 4th moments). Tippett, L.H.C. (1925),On the Extreme Individuals and the Range of Samples Taken from a Normal Population,Biometrika 17 (3-4): 364-387 In either case (approximating  or estimating  for samples from normal distributions), around sample sizes of 8-9, dividing by 3 produces a reasonably good estimate.In very large samples, you'd expect a kind of 6-sigma rule to apply, since the \"3-sigma\" rule is 3 standard deviations either side of the mean. But it's not an asymptotic result since in sufficiently large samples you expect to see the range exceed . Indeed, by n=1000, we're already close to , and by n=10,000 its near , and for n=100,000 it's somewhere above ; the mean of the distribution of the range continues to increase as  increases, but at what appears to be roughly as the square-root of the log of the sample size (that's a pretty slow increase). (Edit indeed, it seems that it's been known for a long time that the growth in mean range is asymptotically proportional to )Update based on discussion in comments:It sounds from your comments that the distribution of values are actually reasonably right skew. I just did a quick simulation to see (at sample size 100) what gamma shape parameter would result in a typical ratio of (max-mean)/(mean-min) of around 1.76 (it turns out to be about ). So then the question is, for that shape, and at that sample size, how much difference does it make if you use the normal values above?. The somewhat surprising answer is 'hardly any at all'.You'd want to check at some other sample sizes, but the upshot is - if the actual distribution of values on which your extremes and mean are based is a shifted gamma with shape parameter around 7 (which is moderately skew) - then, at least near n=100, estimates of  you produce from scaling max-min as if they were normal should be about right.That surprises me that it had even this level of robustness, but it should reassure you a little, at least. Having repeated the simulation exercise at n=30, and a shape parameter of 7, if the ratio of (max-mean)/(mean-min) doesn't tend to be much larger than 1.76, you should be pretty safe - at least on average - using that normal-based rule to estimate  if the distribution of results isn't much heavier tailed than gamma.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2013-12-23 17:29:50","Question_id":80444}
{"_id":{"$oid":"5837a57ba05283111e4d47ce"},"Last_activity":"2013-12-23 17:23:32","Creator_reputation":null,"Question_score":0,"Answer_content":"I don't think this can be correct. The range of a normal random variable is , hence  and . However, your denominator remains constant at 3, so this estimator will diverge as your sample gets larger, and is hence inconsistent. This has nothing to do with the three sigma rule.","Display_name":"user31668","Creater_id":null,"Start_date":"2013-12-23 17:23:32","Question_id":80444}
{"_id":{"$oid":"5837a57ba05283111e4d47df"},"Last_activity":"2016-08-17 19:14:31","Creator_reputation":41,"Question_score":1,"Answer_content":"I believe you have confused the meaning of expected value.  We can now assume that after 17.64 samples that we have a 50% chance to obtain said monster because we know 17.64 is the average number of samples it takes to obtain said monster.This statement is incorrect, and doesn't make fundamental sense (you cannot do 17.64 trials)An expected value of 17.64 in this case does not mean this, (I used 18 as approximation):X~B(18, 0.0567)P(X=1) = 0.5 or P(X\u003e=1) = 0.5.The expected value of 17.64 means that if you draw many many trials from this process, we expect 1 success with the average interval of 17.64 trials. Note that this says nothing about the probability of seeing success or not.In fact to find the probability that in 18 trials you will observe 1 or more successes:X~B(18, 0.0567)P(X=1) = 0.378357850443551P(X\u003e=1) = 0.65029888269312i.e. your chance of getting exactly one success for 18 trials is 38%, and getting 1 or more is 65%.However if you are insistent on defining an \"average\" as number of trials needed for the cumulative chance of 50% to obtain at least 1 success or more, then General Abrial's answer is the perfect way to calculate it. i.e.\"Average\" = What is the needed number of trials n, to have a 50% shot at getting at least 1 success? Although, from a gamer perspective, why is it so important to use 50% cumulative chance? Personally I would rather go for something more significant, like 80~95% cumulative chance to get at least 1 success.Say if the summon (presumably powerful) is needed to beat a boss (or win a PvP), would you rather calculate for 50% chance of making it or 80% chance of making it.","Display_name":"Kim","Creater_id":127845,"Start_date":"2016-08-17 17:37:03","Question_id":230014}
{"_id":{"$oid":"5837a57ba05283111e4d47e0"},"Last_activity":"2016-08-17 07:36:15","Creator_reputation":1300,"Question_score":1,"Answer_content":"I think you have mean and median mixed up.The mean (or expected value) is, indeed about 17.  But this value is skewed high by the small odds of taking a very large number of turns.The formula you use looks like a continuous approximation of the discrete CMF (cumulative mass function) 1-(1-p)^nBut solving the CMF = 0.5, does NOT give you the expected value.  It gives you the median (equal chance of greater or lesser outcome) value, which is indeed about 12.","Display_name":"MikeP","Creater_id":36115,"Start_date":"2016-08-17 07:27:25","Question_id":230014}
{"_id":{"$oid":"5837a57ba05283111e4d47e1"},"Last_activity":"2016-08-16 08:27:34","Creator_reputation":17799,"Question_score":1,"Answer_content":"Most of your question is premised on a misunderstanding of how these probability models work. I'm not going to directly engage with that -- instead, I'll give examples of how to work out these problems correctly.Suppose that we model this process as a binomial distribution. This model assumes that each trial is independent and has a finite, fixed integer number of trials and a fixed probability of success for each trial. All trials are attempted, and the number of successes are totaled up at the end.The probability of obtaining the monster in a sequence of  trials is  which has densityf(x;n,p)=\\binom{n}{x}p^x(1-p)^{n-x}. The density formula gives the probability of obtaining exactly  monsters in  trials.For example, the probability of obtaining it in one trial is 0.0567. But in the more general case, you need to be more specific -- when you say \"obtain\" do you mean obtain exactly once? Or at least once? If you mean at least once, the probability of obtaining the monster is , i.e. the complement of the event that you obtain the monster 0 times in all  trials.In the scenario where you take 17 trials, the probability of obtaining the monster at least once is .On the other hand, the outcome I think you're really interested in might be better modeled as a negative binomial distribution. This model has a fixed probability of success for each trial, the trials are independent, and the trials continue until the desired number of successes are obtained (specifically, the last trial is a success).So we want to know how many trials to carry out to have some probability of having one success at summoning the monster. We can do this computation in R using the qnbinom function. The quantile function takes a probability, i.e. a value of the CDF, and returns the corresponding value of the random variable (in the case of discrete r.v.s, the smallest value of the r.v. that includes the probability).qnbinom(0.5, size=1, prob=0.0567)=11This means that there's a 50% shot of summoning the monster on the 12th attempt: 11 failures followed by the successful summoning.","Display_name":"Sycorax","Creater_id":22311,"Start_date":"2016-08-15 20:41:42","Question_id":230014}
{"_id":{"$oid":"5837a57ba05283111e4d47f2"},"Last_activity":"2016-08-17 17:13:52","Creator_reputation":152503,"Question_score":2,"Answer_content":"You don't compute relative efficiency by taking the ratio of the estimators.For unbiased estimators, relative efficiency is the ratio of their precision (inverse of variances). Equivalently, the efficiency of the first to the second would be the variance of the second estimator divided by the variance of the first (so estimators with larger variance are less efficient).More generally, relative efficiency either looks at the ratio of their mean square errors e(T_1,T_2)=\\frac {\\mathrm{E} \\left[ (T_2-\\theta)^2 \\right]} {\\mathrm{E} \\left[ (T_1-\\theta)^2 \\right]}or sometimes they divide by  instead -- taking ratios of coefficients of variation squared. Since you're computing asymptotic relative efficiency for consistent estimators, you could take the ratio of the variances.In the case of the scaled mean deviation vs the standard deviation as an estimate of  in the normal, Fisher derived the ARE to be . This will be the result Tukey refers to. Some details of the derivation (and other references) are given in Pham-Gia and Hung (2001)[1].Note, however, that Ripley is talking not about comparing a multiple of  with  as estimates of  but comparing a multiple of  with  to estimate ; that's a different calculation, and it surprises me that the asymptotic relative efficiency should come out the same. (There may be some obvious reason why that should work out that way that I have missed.)[1] Pham-Gia, T. and T.L. Hung (2001),\"The mean and median absolute deviations\"Mathematical and Computer Modelling, Vol 34, No 7–8 (Oct), pp 921-936(a pdf link to the paper is available on the paper's web-page here)","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-17 06:12:47","Question_id":230248}
{"_id":{"$oid":"5837a57ba05283111e4d47fe"},"Last_activity":"2016-08-17 16:19:35","Creator_reputation":16,"Question_score":1,"Answer_content":"Both analyses (non-linear regression and binomial GLM) have their advantages and disadvantages. They will produce quite similar LD50s if the data aren't variable but can produce wildly different results if the data are quite variable or if the bottom of the curve is not found (e.g. 100% lethality in the highest treatment). I'd recommend trying both to see what makes the most sense with your data. Personally I find nonlinear regression gives a more realistic fit of the data given the experiment is a controlled lab-based assay, and therefore can better meet some of the requirements such as normality of residuals, homogeneity of variances, minimal deviation from the model (i.e run/replicate tests), reasonably balanced sample units. Usually at least 5 treatments levels are needed for model convergence. Appropriate constraints are also needed. In addition to the drc package, a very easy to use program is Graphpad Prism which requires no coding. I'd recommend using it's free online Help-section to better understand non-linear regression.On the other hand, binomial GLMs (logistic regressions) need to meet fewer criteria and may be stronger for an unbalanced designs and smaller sample sizes, utilising the underlying probability distribution (the binomial) to inform estimates. Random variation, which is common in biological data, can also be incorporated using a binomial GLMM. Also they also don't need artificial constraints like non-linear regression often do. Hope this helps. ","Display_name":"Tom","Creater_id":105672,"Start_date":"2016-08-17 16:19:35","Question_id":37676}
{"_id":{"$oid":"5837a57ba05283111e4d480d"},"Last_activity":"2016-08-17 14:15:51","Creator_reputation":416,"Question_score":1,"Answer_content":"Using the first approach of the average score of your CV loop would give overly optimistic accuracies because when you selected the hyper-parameters, you chose the ones which gave you the highest accuracy.  Effectively, you would have no out of sample testing using that method.  It would be like trying to find the highest score of a basketball game in a season by summing up the number of points each player got in the game where they individually scored the most points (where the game would be the hyper-parameters and the players the folds of cross-validation).The most standard way to do the evaluation would be your second suggestion where you do cross-validation on a training set and then do the evaluation on a held out test set.  This ensures that the model selection is independent of your testing data giving you a more realistic metric for the generalization of your model.One final method I have seen, particularly in small samples where data is limited, is to use a nested cross-validation.  In this version, your data are divided up into N groups.  In the first fold, group 1 would be the testing group and groups 2-N would be the training groups.  Cross-validation would be performed on groups 2-N to select model hyper-parameters.  The model is then trained on that training set (groups 2-N) and tested on group 1.  In the next fold group 2 is held out and groups 1,3-N are used to train the model.  This continues for the total of N-folds.While it is clear to see the advantage of this method in small data (you can use a large amount of your data to train the models and still have all of the data available for evaluation) it can cause problems in later evaluation.   First off, your results now come from N different models so the errors can't easily be compared (what if there was just one fold that chose really bad parameters but the rest were great).  Secondly, while the results come from different models, those models are not independent as (N-2)/(N-1) of the training data were the same between any two pairs of models.  Most statistical tests require an assumption of independence so that can lead to difficulty selection a metric for evaluation.In general, I would go with your approach 2, but if data is really limiting, it may be worth doing nested cross-validation.  I cannot imagine a circumstance where the first approach would be appropriate.","Display_name":"Barker","Creater_id":127913,"Start_date":"2016-08-17 14:15:51","Question_id":229509}
{"_id":{"$oid":"5837a57ba05283111e4d481a"},"Last_activity":"2016-08-17 16:13:39","Creator_reputation":3680,"Question_score":1,"Answer_content":"One obvious option is, of course, to make the plotting device larger so that all the nodes have enough space on the device. Another possible way to save space is to just show the prediction in the terminal nodes (without the (n = ..., err = ...) part. This is possible through the FUN argument to the node_terminal panel function. For example, you could try:plot(as.simpleparty(pfit40),  tp_args = list(FUN = function(info)    format(round(info$prediction, digits = 1), nsmall = 1)  ))The first line has a similar effect as type = \"simple\" that you used. However, it also enriches the info available for printing. The second and third line set up the function that computes the information for display in the terminal node. In my example, it just shows the predicted mean (rounded to 1 digit).(Additional remark: In your particular case it might be worth to explore different specifications of MonthNr/Hour. Currently, these are apparently unordered factors which leads to many splits.)","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2016-08-17 16:13:39","Question_id":219651}
{"_id":{"$oid":"5837a57ba05283111e4d4826"},"Last_activity":"2016-08-17 16:04:06","Creator_reputation":8337,"Question_score":0,"Answer_content":"In general, yes, a maximum likelihood estimate can be negative. For example, the maximum likelihood estimate of the mean of a normal distribution when the data is just the singleton vector (-1) is -1.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-17 16:04:06","Question_id":230375}
{"_id":{"$oid":"5837a57ba05283111e4d4833"},"Last_activity":"2016-08-17 14:22:45","Creator_reputation":5787,"Question_score":0,"Answer_content":"My interpretation of your situation:   and  each take the values 0 or 1. You know ,i.e., . You have observations of . And you want to know . If so, make use of P(X|Y) = \\frac{P(X,Y)}{P(Y)}Estimate P(X,Y) from the data.for Y = y values of 0 and 1, and calculate the corresponding . Note that you have  as ordered pairs. That is (should be) taken into account when estimating  from the data. The marginal distributions of  and , whatever they happen to be, even if different, and whatever their dependency, are\"automatically\" modeled by \"nature\" and reflected in the paired data. Explicit Solution: is estimated as \\frac{P(X=1,Y=1)}{q} where  is the sample fraction of all pairs in which both  and . is estimated as \\frac{P(X=1,Y=0)}{(1-q)} where  is the sample fraction of all pairs in which both  and .  and  where the estimates on the left-hand sides are obtained using the estimates on the right-hand side.Addressing a comment: We also have , so we can estimate the left-hand side using estimated values on the right-hand side  Note however, that we could have estimated just  more directly as the sample fraction of all pairs  having . And of course .","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-17 14:22:45","Question_id":230366}
{"_id":{"$oid":"5837a57ba05283111e4d4846"},"Last_activity":"2016-05-21 13:40:24","Creator_reputation":19535,"Question_score":4,"Answer_content":"PCA decomposes the covariance matrix into rotation and scaling.If you only use rotation, you should get the exact same result with k-means. So you gained nothing.Two ways of using the scaling information:scale every projected attribute to unit variancediscard attributes with low varianceboth.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-05-21 13:40:24","Question_id":213471}
{"_id":{"$oid":"5837a57ba05283111e4d4853"},"Last_activity":"2016-08-17 14:35:04","Creator_reputation":416,"Question_score":1,"Answer_content":"There are advantages and disadvantages to both methods of model evaluation.  While it is clear to see the advantage of cross-validation in small data (you can use a large amount of your data to train the models and still have all of the data available for evaluation) it can cause problems in later evaluation. First off, your results now come from N different models so the errors can't easily be compared (if your model ends up being highly effected by the training set this can produce wildly different accuracies in different folds). Secondly, while the results come from different models, those models are not independent as (N-2)/(N-1) of the training data were the same between any two pairs of models. Most statistical tests require an assumption of independence so that can lead to difficulty selection a metric for evaluation.In general, I would go with the hold out method, but if data is really limiting, it may be worth doing cross-validation.","Display_name":"Barker","Creater_id":127913,"Start_date":"2016-08-17 14:35:04","Question_id":229431}
{"_id":{"$oid":"5837a57ba05283111e4d4854"},"Last_activity":"2016-08-11 16:29:34","Creator_reputation":143,"Question_score":1,"Answer_content":"Holdout is essentially a 2-fold cross validation. If you perform k-fold cross validation correctly, no extra holdout set is necessary. Make sure that your predictors are chosen based on the test sets (and not in advance on all the samples). You may also want to think about stratification if appropriate.","Display_name":"oW_","Creater_id":127417,"Start_date":"2016-08-11 16:29:34","Question_id":229431}
{"_id":{"$oid":"5837a57ba05283111e4d4861"},"Last_activity":"2016-08-17 14:24:47","Creator_reputation":3680,"Question_score":1,"Answer_content":"(a) Assess the performance(s) you are interested in. Thus, if you are mainly interested in getting the expectation of the response E(y) right, then MAE or RMSE are useful. Similarly, you could also the conditional expectation E(y | y \u003e 0), i.e., the expected amount of precipitation given that there is precipitation. If you are mostly interested in the probability of any precipitation P(y \u003e 0) you could look at the corresponding misclassification rate or the Brier score etc. And if you are interested in the entire distribution, the scoring rules like the log-likelihood (or log-score) or the CRPS (continuous ranked probability score) would be natural.(b) Instead of a two-step model with binary first step and zero-truncated second step, you could also use a single regression model with a response that is censored at zero. A worked example with precipitation in a weather forecasting context is available in a paper about our crch R package (see https://journal.R-project.org/archive/accepted/messner-mayr-zeileis.pdf).","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2016-08-17 14:24:47","Question_id":230334}
{"_id":{"$oid":"5837a57ba05283111e4d4872"},"Last_activity":"2016-08-17 13:41:15","Creator_reputation":8337,"Question_score":2,"Answer_content":"The book you linked (Gelman et al.) is eminently practical: the authors spend quite a few pages discussing real data and the gory details of choosing, fitting, and evaluating models. That's where I learned Bayesian stuff myself. If what you're looking for is something less mathematically intimidating, try John K. Kruschke's Doing Bayesian Data Analysis.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-17 13:41:15","Question_id":230368}
{"_id":{"$oid":"5837a57ba05283111e4d487f"},"Last_activity":"2016-08-17 13:40:57","Creator_reputation":738,"Question_score":1,"Answer_content":"If your data is not MCAR or MAR, I do not recommend statistical imputation. From your previous posts, your data does not look like it is MAR/MCAR and hence, imputation is NOT appropriate.Here's a referrence on missing data that might be at your level: https://liberalarts.utexas.edu/prc/_files/cs/Missing-Data.pdf","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-17 13:40:57","Question_id":230369}
{"_id":{"$oid":"5837a57ba05283111e4d488b"},"Last_activity":"2016-08-17 13:23:44","Creator_reputation":156,"Question_score":0,"Answer_content":"The Poisson does not fall within the MDA of any EV distribution (not possible to find shifting and scaling sequences that provide a non-degenerate limit).  Consequence of a theorem in Leadbetter, Lindgren and Rootzen (1983).","Display_name":"Mark D","Creater_id":35961,"Start_date":"2016-08-17 13:23:44","Question_id":73563}
{"_id":{"$oid":"5837a57ba05283111e4d488c"},"Last_activity":"2013-10-23 11:20:32","Creator_reputation":null,"Question_score":3,"Answer_content":"I don't know a definitive answer for your primary question. Although I found the following two references:Anderson, C. W., “Extreme value theory for a class of discrete distributions with applications to some stochastic processes”, Journal of Applied Probability, vol 7, 1970, pp. 99–113.Anderson, C. W., “Local limit theorems for the maxima of discrete random variables”,Mathematical Proceedings of the Cambridge Philosophical Society, vol 88, 1980, pp. 161–165.For your secondary question, the CDF of the Poisson is  so . Apply the difference operator (lag1) and you get the PMF of the max.","Display_name":"user31668","Creater_id":null,"Start_date":"2013-10-23 11:20:32","Question_id":73563}
{"_id":{"$oid":"5837a57ba05283111e4d4899"},"Last_activity":"2016-08-17 13:08:36","Creator_reputation":91,"Question_score":1,"Answer_content":"If by range you mean the difference between the maximum and minimum value, then the answer is quite simple.You're simulating data from a Normal distribution with mean 0 and standard deviation of 10. Range is directly related to the most extreme(minimum and maximum) values of your data. The probability of getting very extreme data is small, hence the \"extreme\" part, but as your sample size goes to infinity, your probability of getting an extreme data point will go to 1. (Where you've defined extreme as greater than 5 standard deviations or something else you feel is \"extreme\")Intuitively as your sample from a distribution grows you are more likely to experience extreme data, which has a direct positive impact on the range. As an example I just simulated 100 million data points from a N(0,10) distribution in R and got a range of (-56.36, to 55.69), so the difference here would be about 111. I got values 5 standard deviations away from my mean, because I simulated so much data! Sure if I sat around all day tomorrow and simulated small data set after small data set of N(0,10) data I might see a range this big, but I probably wouldn't because I probably wouldn't simulate 100 million data points worth of small data sets.As an aside, this is not really a terribly interesting phenomenon since it is exactly what we would expect to happen. ","Display_name":"Robert Montgomery","Creater_id":102554,"Start_date":"2016-08-17 13:08:36","Question_id":230302}
{"_id":{"$oid":"5837a57ba05283111e4d48a6"},"Last_activity":"2012-09-12 12:54:53","Creator_reputation":75845,"Question_score":44,"Answer_content":"The advent of generalized linear models has allowed us to build regression-type models of data when the distribution of the response variable is non-normal--for example, when your DV is binary.  (If you would like to know a little more about GLiMs, I wrote a fairly extensive answer here, which may be useful although the context differs.)  However, a GLiM, e.g. a logistic regression model, assumes that your data are independent.  For instance, imagine a study that looks at whether a child has developed asthma.  Each child contributes one data point to the study--they either have asthma or they don't.  Sometimes data are not independent, though.  Consider another study that looks at whether a child has a cold at various points during the school year.  In this case, each child contributes many data points.  At one time a child might have a cold, later they might not, and still later they might have another cold.  These data are not independent because they came from the same child.  In order to appropriately analyze these data, we need to somehow take this non-independence into account.  There are two ways:  One way is to use the generalized estimating equations (which you don't mention, so we'll skip).  The other way is to use a generalized linear mixed model.  GLiMMs can account for the non-independence by adding random effects (as @MichaelChernick notes).  Thus, the answer is that your second option is for non-normal repeated measures (or otherwise non-independent) data.  (I should mention, in keeping with @Macro's comment, that general-ized linear mixed models include linear models as a special case and thus can be used with normally distributed data.  However, in typical usage the term connotes non-normal data.)  Update:  (The OP has asked about GEE as well, so I will write a little about how all three relate to each other.)  Here's a basic overview:  a typical GLiM (I'll use logistic regression as the prototypical case) lets you model an independent binary response as a function of covariates  a GLMM lets you model a non-independent (or clustered) binary response conditional on the attributes of each individual cluster as a function of covariates  the GEE lets you model the population mean response of non-independent binary data as a function of covariates  Since you have multiple trials per participant, your data are not independent; as you correctly note, \"[t]rials within one participant are likely to be more similar than as compared to the whole group\".  Therefore, you should use either a GLMM or the GEE.  The issue, then, is how to choose whether GLMM or GEE would be more appropriate for your situation.  The answer to this question depends on the subject of your research--specifically, the target of the inferences you hope to make.  As I stated above, with a GLMM, the betas are telling you about the effect of a one unit change in your covariates on a particular participant, given their individual characteristics.  On the other hand with the GEE, the betas are telling you about the effect of a one unit change in your covariates on the average of the responses of the entire population in question.  This is a difficult distinction to grasp, especially because there is no such distinction with linear models (in which case the two are the same thing).  One way to try to wrap your head around this is to imagine averaging over your population on both sides of the equals sign in your model.  For example, this might be a model:\\text{logit}(p_i)=\\beta_{0}+\\beta_{1}X_1+b_iwhere:\\text{logit}(p)=\\ln\\left(\\frac{p}{1-p}\\right),~~~~~\\\u0026amp;~~~~~~b\\sim\\mathcal N(0,\\sigma^2_b)There is a parameter that governs the response distribution (, the probability, with binary data) on the left side for each participant.  On the right hand side, there are coefficients for the effect of the covariate[s] and the baseline level when the covariate[s] equals 0.  The first thing to notice is that the actual intercept for any specific individual is not , but rather .  But so what?  If we are assuming that the 's (the random effect) are normally distributed with a mean of 0 (as we've done), certainly we can average over these without difficulty (it would just be ).  Moreover, in this case we don't have a corresponding random effect for the slopes and thus their average is just .  So the average of the intercepts plus the average of the slopes must be equal to the logit transformation of the average of the 's on the left, mustn't it?  Unfortunately, no.  The problem is that in between those two is the , which is a non-linear transformation.  (If the transformation were linear, they would be equivalent, which is why this problem doesn't occur for linear models.)  The following plot makes this clear:Imagine that this plot represents the underlying data generating process for the probability that a small class of students will be able to pass a test on some subject with a given number of hours of instruction on that topic.  Each of the grey curves represents the probability of passing the test with varying amounts of instruction for one of the students.  The bold curve is the average over the whole class.  In this case, the effect of an additional hour of teaching conditional on the student's attributes is --the same for each student (that is, there is not a random slope).  Note, though, that the students baseline ability differs amongst them--probably due to differences in things like IQ (that is, there is a random intercept).  The average probability for the class as a whole, however, follows a different profile than the students.  The strikingly counter-intuitive result is this: an additional hour of instruction can have a sizable effect on the probability of each student passing the test, but have relatively little effect on the probable total proportion of students who pass.  This is because some students might already have had a large chance of passing while others might still have little chance.  The question of whether you should use a GLMM or the GEE is the question of which of these functions you want to estimate.  If you wanted to know about the probability of a given student passing (if, say, you were the student, or the student's parent), you want to use a GLMM.  On the other hand, if you want to know about the effect on the population (if, for example, you were the teacher, or the principal), you would want to use the GEE.  For another, more mathematically detailed, discussion of this material, see this answer by @Macro.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2012-07-16 17:11:40","Question_id":32419}
{"_id":{"$oid":"5837a57ba05283111e4d48a7"},"Last_activity":"2012-07-16 19:33:37","Creator_reputation":2520,"Question_score":1,"Answer_content":"I suggest you also examine answers of a question I asked some time ago:General Linear Model vs. Generalized Linear Model (with an identity link function?)","Display_name":"Behacad","Creater_id":3262,"Start_date":"2012-07-16 19:33:37","Question_id":32419}
{"_id":{"$oid":"5837a57ba05283111e4d48a8"},"Last_activity":"2012-07-16 18:20:23","Creator_reputation":25897,"Question_score":2,"Answer_content":"The key is the introduction of random effects.  Gung's link mentions it.  But I think it should have been mentioned directly. That is the main difference.","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-07-16 18:20:23","Question_id":32419}
{"_id":{"$oid":"5837a57ba05283111e4d48b5"},"Last_activity":"2016-08-17 12:48:14","Creator_reputation":7682,"Question_score":0,"Answer_content":"I've been thinking about the Bayesian approach, but I believe now that it may end up being more theoretical or academic than intended in this very particular instance.I wonder if what you are after (and I realize that I am working around some of the more specific questions at the end of your post) is to set up some statistical power calculations. You can do this with R, or online stats calculators, but the important concept is the flexibility in generating results that tell you in what percentage of cases you can expect to correctly call the coin biased depending on: 1. The number of tosses; 2. The degree of bias in the coin; and 3. The percentage of cases we are ready to accept as the risk of actually calling the coin biased when it is not (risk alpha).Knowing that even with a fair coin you can get extremely surprising results, you can settle for a compromise that you will only call the coin \"biased\" if it goes beyond a threshold that guarantees you are going to make a mistake in only  of the times when the coin is in fact unbiased (risk alpha).But you want to know what is in some respects a complementary idea: If the coin is biased, in what percentage of cases will you be able to make the call under the self-imposed constraints in the prior paragraph? In other words, the power. I have some notes in here if you are interested.Evidently, your power to confidently reject the coin fairness will increase (will make itself more easily manifest) as you increase the number of tosses, or if you happen to be dealing with more extremely biased coins.I am not going to re-invent the wheel by reformulating what is a very straightforward chunk of code that you can find in many guises, for example in this Berkeley University post, and that I'm pasting for ease of access:set.seed(17) # Today's date.coin.power = function(ntoss=100,nsim=1000,prob=.5){     lower = qbinom(.025,ntoss,.5)     upper = qbinom(.975,ntoss,.5)     rr = rbinom(nsim,ntoss,prob)     sum(rr \u0026lt; lower | rr \u0026gt; upper) / nsim }ntosses = c(10,100,200,500,600,800,1000,1500,2000,2500)res = sapply(ntosses, coin.power, prob=.55) names(res) = ntossesres10     100    200    500   600   800   1000  1500  2000  25000.032  0.133  0.259  0.634 0.653 0.799 0.867 0.969 0.994 0.999 In the case of a coin with a true biased probability of heads of , then, you can expect to reject correctly the hypothesis that the coin is fair  of the times if you toss the coin  times, and  of the times with  tosses.If the coin is more biased, for example, , you can expect virtual certainty with  tosses:res = sapply(ntosses,coin.power,prob=.70)names(res) = ntossesres10    100   200   500   600   800   1000  1500  2000  25000.163 0.976 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-08-17 10:02:22","Question_id":229966}
{"_id":{"$oid":"5837a57ba05283111e4d48b6"},"Last_activity":"2016-08-16 06:50:56","Creator_reputation":21,"Question_score":2,"Answer_content":"It is fairly well explained here:https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fairBasically, using the Bayesian inference method and assuming an uniform prior distribution, which is reasonable (it represents maximum initial uncertainty about the fairness of the coin), the posterior probability for the actual probability  of obtaining heads in a single toss after having observed  number of heads and  number of tails (therefore  is the total number of tosses) is a Beta distribution with parameters  and f(r|H=h,T=t)=\\frac{(h+t+1)!}{h!t!}r^h(1-r)^tThis will give you an idea of how r is distributed. The maximum-a-posteriori estimate (mode) isr^*(h,t)=\\frac{h}{h+t}And the expected value isE[r](h,t)=\\frac{h+1}{h+t+2}One can use the standard deviation as estimation of the uncertainty\\sigma(h,t)=\\sqrt{\\frac{(h+1)(t+1)}{(h+t+2)^2(h+t+3)}}As you can see it doesn't depend just on the total number of tosses  but also on , so the criterion for a given confidence interval will be different depending on the sequence of results of the tosses.","Display_name":"J. R. C.","Creater_id":127493,"Start_date":"2016-08-15 12:19:15","Question_id":229966}
{"_id":{"$oid":"5837a57ba05283111e4d48c3"},"Last_activity":"2016-08-17 12:47:36","Creator_reputation":7682,"Question_score":1,"Answer_content":"I am not going to attempt to provide final answers to your question; I believe the topic is more than addressed after the comprehensive response given by Glen. However, and apropos of his comment about a Bayesian approach, I'd like to post some illustrations about the way our preconceptions about the \"fairness\" of the coin (or the experiment in general) affects the posterior probability density, i.e. the , where  stands for the probability of heads in the coin toss.Luckily, we have a conjugate prior distribution for the binomial case that occupies us - the beta distribution, facilitating the calculation of the posterior distribution.First scenario - The Fair-Minded Player:We walk into the game (not a very exciting game, but still...), and we have absolutely no reason to assume that there is foul play going on. Things being by nature less than perfect, we have it in our mind that the coin is fair-ish. In other words, we think that the probability of heads, , falls somewhere around . Later, the unexpected single tail out of  tosses, will force us to move the posterior probability of  to the left (the arrows indicate the influence of the data on the prior distribution):Second Scenario - The Shrewd Player:We strongly suspect from insider's leaked information that the game is markedly biased towards tails, and we not only are about to make a killing, but also in need to further reinforce our conviction after the first round, doubling down our bet:Third Scenario - Losing Your Shirt:We've never played before, but we have read a manual, and we feel ready. All signs clearly indicate that the coin is markedly biased towards , a mistake that we will soon start to correct at a high \\\\beta(1,1)U\\,(0,1)\\theta$. As brought up to my attention, a Jeffreys prior is close and possibly more correct:So I hope this provides a bit of a light-hearted visual depiction of our approach to estimating the chances of this game being rigged, perhaps encapsulating more of a real scenario than calculations of the type pbinom(1, 6, 0.5). If you want the code in R, and the credits to a great video with Matlab illustrations, I posted it here.","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2015-09-08 23:17:05","Question_id":171451}
{"_id":{"$oid":"5837a57ba05283111e4d48c4"},"Last_activity":"2015-09-08 23:32:38","Creator_reputation":152503,"Question_score":6,"Answer_content":"[I think I'd start by asking for a whiteboard, markers -- and an eraser, because one boardful isn't enough to explain everything wrong with the question.]I'm going to answer this question by rejecting its premises. The \"coin\" itself is just a coin; by itself it doesn't do anything, and so it cannot be fair or not-fair. What we're talking about is the process of tossing a particular coin in some fashion -- that can be discussed in terms of whether it's fair or not.Data can't show you that a coin-tossing process applied to some coin is exactly fair. Sometimes it can show you that your coin-tossing-process on a given coin is inconsistent with fairness, but failure to identify any inconsistency with fairness doesn't imply fairness (failure to reject is because your sample size is small, not because the coin is actually fair). [e.g. Consider it in terms of a confidence interval for P(head), the fact that  is in the CI doesn't mean that P(head)=, since there are always other values - distinct from  - in there too. Or think in terms of power: on the experiment given in the interview question - 6 tosses - what's the probability that you'd reject as unfair the case where the tossing process applied to a particular coin had  at some typical significance level? That's clearly an unfair coin, but you'll reject barely more often than your type I error rate, and a large fraction of those rejections in a two tailed test would be \"in the wrong tail\"!]No coin-tossing process on a given coin will be perfectly fair. (For example, changing the side facing up slightly alters the chances associated with the resulting face on the toss, as experiments run by Persi Diaconis have shown.)Could the coin be close to fair? Possibly; it may even be possible to get very close to fair. Exactly fair? No, it's not possible in practice. But then to discuss whether it's \"close to fair\" we'd have to define what we mean by 'close'. [If we were to give some usable definition, while some people might suggest some form of equivalence test, or perhaps considering whether some CI lay entirely inside some \"close to fair\" bounds, I'd be inclined toward a Bayesian approach to deciding whether the coin is sufficiently close to fair. Note that with the tiny sample size mentioned, the data are quite consistent with p(head) so far from  that this exercise on that data would not conclude \"close to fair\" on any of the three mentioned approaches.]So:  Given a coin you don’t know it’s fair or unfair. Yes, actually, I do. In fact I don't even need to see data. It's not fair.  Throw it 6 times and get 1 tail and 5 heads. Determine whether it’s fair or not. I really don't care what the data are. It makes no difference to my answer, since the data could not possibly demonstrate fairness, even if fairness were a realistically possible state to be in.  What’s your confidence value?100% (in a sense similar to almost surely)(In any case, even if there were a way to do this statistically I don't know of any statistical procedure that gives anything I'd agree to call \"confidence values\", so I also reject the form of that question. What does that term even mean? If I were asked a question phrased that way in an interview, I'd have serious concerns about working there, because it seems to suggest the people conducting the interview don't really understand what they're even asking - and that suggests either nobody there knows this stuff, or they don't care enough about this position to make sure the interview is being conducted by someone who does. Either way, it would certainly influence my willingness to work there.)Forgetting everything I just said for the moment, some comments on your hypothesis test:Your process for a hypothesis test is wrong. Why do you compare your significance level with 0.05? You've chosen a significance level of 0.21 (which I have no objection to in this experiment, the sample size is so low you only have 3% or 21% and =3% will be too low-powered to be much use) -- 0.05 doesn't relate to anything here. Do you see that in your test when it came time to reject or not reject, you made no reference at all to the sample statistic (5 heads)? Indeed you ignored your rejection rule.The rejection rule you stated algebraically  is inconsistent with the rejection region you mentioned (). That's a lot of errors in a few lines! If I was involved in such an interview**, I might forgive the error with the rejection rule as something one could overlook under interview pressure, but the first two errors would suggest some fundamental problems. ** leaving aside that I'd never ask such a poor question, nor would I likely care enough about hypothesis testing to even think to ask a question about it. ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2015-09-07 14:45:08","Question_id":171451}
{"_id":{"$oid":"5837a57ba05283111e4d48d3"},"Last_activity":"2016-08-17 12:40:02","Creator_reputation":17374,"Question_score":3,"Answer_content":"The simple answer to your question is that what you want can't be done. You were interested in hourly salary, but you didn't ask participants that. Obviously, creating ratios would create some bizarre overlapping categories. Sometimes they can be coarsened into a larger category, so for instance, individuals working full time for \\100 or less for an hourly salary. But I have also found you will obtain interesting discrepancies from how participants would have responded to \"what is your hourly salary?\". All signs point to not creating a transformed variable.There's no reason why salary can't be treated as a grouped linear variable where a 1 indicates the lowest salary and 5 indicates the highest, in prediction and inference this approach has been discussed extensively as a valid approach. Similarly, with enough participants, it can be treated ordinally without a problem.Furthermore, there's no reason why hours worked per week couldn't be considered a separate factor in any model. Suppose for instance you were interested in modeling self-reported happiness, both hours-worked-per-week would be predictors of happiness as well as weekly salary for different albeit correlated reasons. Allowing an interaction (eq. a product term) between these values would tell you if there is a time-money tradeoff for happiness. ","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-17 12:40:02","Question_id":230348}
{"_id":{"$oid":"5837a57ba05283111e4d48df"},"Last_activity":"2016-08-03 14:06:09","Creator_reputation":152503,"Question_score":6,"Answer_content":"One important thing to note is that your data don't appear to be consistent with having been drawn from a Rayleigh population -- the right tail is considerably too heavy.Nevertheless, I'll continue as if the shifted-Rayleigh were a suitable model.If the offset is unknown you can estimate it as a parameter.The density for a one-parameter Rayleigh is:If we introduce a shift , it becomes:[NB Here  is the lower bound on the random variable, not the mean.]Dey et al, 2014 [1] discuss estimation in the two-parameter Rayleigh case. (However, you should carefully note that in the parameterization there, the second parameter, , is not the scale - even though they say it is - in fact,  (or anything proportional to it) is a scale parameter, where .)They provide a simple iterative estimator for the MLE of the shift parameter, : {\\mu}^{[j+1]}=\\enclose{horizontalstrike}{2\\sum_{i=1}^n(x_i-\\mu^{[j]})^2\\times \\sum_{i=1}^n(x_i-\\mu^{[j]})\\times \\sum_{i=1}^n(x_i-\\mu^{[j]})^{-1}}[Edit: It looks like this formula (found in both the working and published versions!) cannot be correct, since it's in squared units-of-. Clearly a shift/location parameter has to be in units-of-.For the moment (until I see if I can derive it correctly myself), probably the best thing to do is optimize the profile log likelihood for  in equation 7 using a univariate optimizer:a quick check of the algebra seems to suggest this formula is correct up to an additive constant. Running a few dozen examples on randomly generated Rayleigh data - both in small (n=10) and moderately large samples (n=1000) - suggests that simply optimizing the profile log likelihood directly seems to work quite well. I used Brent's method but any number of reasonable optimization methods should work adequately.]Then  is taken to be the value of  at the last iterate obtained at convergence, , say.These iterations could be started () at the method of moments estimator of  where  and  are the sample mean and standard deviation respectively, and , or at some suitably small distance below the smallest observation (e.g.  with  near  should work reasonably well as a start point). Note that the method of moments estimator may sometimes exceed the smallest observation (and should be avoided/modified in that case).If the data are then shifted by , , the scale parameter may be estimated from the back-shifted data in the usual fashion for a Rayleigh distribution. Standard errors and confidence intervals follow in the same fashion.Interestingly, (but not entirely surprisingly for a shift parameter on a random variable bounded below by it), the shift parameter doesn't have the \"usual\" asymptotics for MLEs, in that the variance isn't proportional to . (The paper gives asymptotic confidence intervals for the parameters - but again, note that they don't use the same parameterization for the main parameter. The same paper discusses other estimators, but since the MLEs are fairly simple, I'd suggest sticking with them)[1]: Dey, S., T. Dey, and D. Kundu (2014),Two-Parameter Rayleigh Distribution: Different Methods of Estimation,American Journal of Mathematical and Management Sciences, Vol 33, No 1, p55-74(working paper here)","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-18 17:23:50","Question_id":224416}
{"_id":{"$oid":"5837a57ba05283111e4d48ee"},"Last_activity":"2016-08-17 11:56:10","Creator_reputation":280,"Question_score":0,"Answer_content":"I've talked with Eric Zivot the author and he confirmed: \"I assume pi is the true parameter in the derivation\"","Display_name":"ihadanny","Creater_id":37793,"Start_date":"2016-08-17 11:56:10","Question_id":229679}
{"_id":{"$oid":"5837a57ba05283111e4d48fd"},"Last_activity":"2016-08-13 19:10:04","Creator_reputation":117,"Question_score":3,"Answer_content":"Hi thank you for the question. It is useful to think of the possible set of values and determine it's probability distribution, as oppose to jumping into binomial formulas. As Mark has has mentioned inside the comments, binomial is only not suitable for when there is no replacement. For example, flipping a fair coin n times is equivalent to sampling a red or a blue ball from a bag with replacement (not without).There are four possible situations describing which set of two students you have selected.Note:Sequence matters in the \"without replacement\" case.   In other words, the probability of sampling  men with 2 draws from the original sample are as belowAll the probabilities ofthe outcomes of  sum to 1, so we know it gives a valid probability distribution. This answer question a)As for question b). Now, that you know the probability distribution of , you can quite easily derive the expected value and the standard deviation with these formulas.Attempt the rest by yourself. Good Luck :)","Display_name":"tintinthong","Creater_id":121671,"Start_date":"2016-08-13 18:53:03","Question_id":229718}
{"_id":{"$oid":"5837a57ba05283111e4d490a"},"Last_activity":"2015-05-30 06:50:54","Creator_reputation":12722,"Question_score":10,"Answer_content":"1) The definition of eigenvector  is ambidextrous.  If  is an eigenvector, so is , for then A(-x) = -Ax = -\\lambda x = \\lambda (-x)So the definition of an eigenbasis is ambiguous of sign.2) It's hard to know for sure, but I have a strong suspicion of what is happening here.  Your equation (A - \\lambda)x = 0 is technically incorrect.  The correct equation is (A - \\lambda I)xThe first equation is often used as a shorthand for the second.  In general, this is unambiguous, because there is no real mathematical way to subtract a vector from a square matrix, but it is abuse of notation.  In R though, you have broadcasting.  So if you do\u0026gt; M \u0026lt;- matrix(c(1, 1, 1, 1), nrow=2)\u0026gt; M - .5     [,1] [,2][1,]  0.5  0.5[2,]  0.5  0.5its not really what you want.  The proper way would be\u0026gt; M - diag(.5, 2)     [,1] [,2][1,]  0.5  1.0[2,]  1.0  0.5The reason you are getting zero solutions is that the matrix you are starting with  is invertible.  More than likely (almost surely), the matrix you get by subtracting the same number from every entry will also be invertible.  For invertible matrices, the only solution to  is the zero vector.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2015-05-30 06:50:54","Question_id":154716}
{"_id":{"$oid":"5837a57ba05283111e4d4917"},"Last_activity":"2016-08-17 11:25:09","Creator_reputation":738,"Question_score":0,"Answer_content":"Given the context of your data, I recommend the KS test, or a Wilcoxon Rank Sum (aka Mann-Whitney U) test. Please see assumptions: https://statistics.laerd.com/spss-tutorials/wilcoxon-signed-rank-test-using-spss-statistics.phpThe use of a Fisher's exact test makes sense if you have two categorical variables producing something something like a 2 x 3 contingency table. From your question, it seems you may potentially have a 2 x 80 contingency table if you treat each monetary value as a level in the Money category by the Group factor. Hence, it doesn't make sense to use a Fisher's exact test for your analysis.To briefly explain each test:Kolmogorov-Smirnov  An attractive feature of this test is that the distribution of the K-S test statistic itself does not depend on the underlying cumulative  distribution function being tested. Another advantage is that it is an  exact test (the chi-square goodness-of-fit test depends on an adequate  sample size for the approximations to be valid). Despite these  advantages, the K-S test has several important limitations:       It only applies to continuous distributions.   It tends to be more sensitive near the center of the distribution than at the tails.   Perhaps the most serious limitation is that the distribution must be fully  specified. That is, if location, scale, and shape parameters are  estimated from the data, the critical region of the K-S test is no  longer valid. It typically must be determined by simulation.  http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htmContingency tables with Fisher's exact test   \"is used when you want to conduct a chi-square test but one or more of  your cells has an expected frequency of five or less.  Remember that  the chi-square test assumes that each cell has an expected frequency  of five or more, but the Fisher's exact test has no such assumption  and can be used regardless of how small the expected frequency is.\"(http://www.ats.ucla.edu/stat/spss/whatstat/whatstat.htm)","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-16 15:54:32","Question_id":230140}
{"_id":{"$oid":"5837a57ba05283111e4d4926"},"Last_activity":"2016-08-17 11:00:00","Creator_reputation":147383,"Question_score":2,"Answer_content":"You asked for alternative approaches.  Here is one you might find useful.Let's begin by stating the obvious: you are implicitly assuming the five probabilities are equal.  The expected total in the showcase equals the sum of those probabilities, whence it is five times any one of them.  Yet the expected total is the average value of all possible totals, weighted by their chances of occurring.  Since by design the possible total is always , its average must be .  Therefore each probability is .The power of this reasoning about expectations becomes clear when you generalize the question to  cars in the showcase to be chosen (with equal probabilities) out of  cars backstage.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-17 11:00:00","Question_id":221728}
{"_id":{"$oid":"5837a57ba05283111e4d4927"},"Last_activity":"2016-07-01 11:50:22","Creator_reputation":8337,"Question_score":0,"Answer_content":"I assume that all cars are equally likely to be chosen.Suppose without loss of generality the cars are labeled 1 through 5. The probability of not choosing car 1 is (4/5) * (3/4) * (2/3) = 2/5, so the probability of choosing it is 1 - 2/5 = 3/5. Of course, by my assumption above, the same argument applies to any of the cars. The answer 3/5 makes intuitive sense since we're drawing 3 things from a set of 5.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-01 11:50:22","Question_id":221728}
{"_id":{"$oid":"5837a57ba05283111e4d4936"},"Last_activity":"2016-08-17 10:47:44","Creator_reputation":41,"Question_score":3,"Answer_content":"Thanks for the help in the comments, here is a solution:Do a variable substitution :]We use the expression for the absolute moment for the normal, , to calculate the ratio between the Laplace and normal moments:.Since  for  and  for , we have, for :.","Display_name":"Carlos Stein","Creater_id":127977,"Start_date":"2016-08-17 09:23:31","Question_id":230256}
{"_id":{"$oid":"5837a57ba05283111e4d4946"},"Last_activity":"2013-12-20 00:46:37","Creator_reputation":152503,"Question_score":12,"Answer_content":"You should not do a calculation of probability for an event deemed surprising post hoc as if it were an event specified before it was rolled (observed).It's very difficult to to do a proper calculation of post hoc probability, because what other events would have been deemed at least as surprising depends on what the context is, and also on the person doing the deeming. Would three ones twice in a row at an earlier or later stage of the game have been as surprising? Would you rolling three ones have been as surprising as him rolling them? Would three sixes be as surprising as three ones? and so on... What is the totality of all the events would have been surprising enough to generate a post like this one?To take an extreme example, imagine a wheelbarrow-full-of-dice (ten thousand, say), each with a tiny individualized serial number. We tip the barrow out and exclaim \"Whoah, what are the chances of getting this?\" --  and if we work it out,  is . Astronomically small. If we repeat the experiment, we get an equally unusual event. In fact, every single time we do it, we get an event so astronomically unbelievably small that we could almost power a starship with it. The problem is that the calculation is meaningless, because we specified the event post-hoc.(Even if it were legitimate to do the calculation as if it were a pre-specified event, it looks like you have that calculation incorrect. Specifically, the probability (for an event specified before the roll) of taking three dice and rolling  is , because the three rolls are independent, not , and the probability of doing it twice out of a total of two rolls is the square of that - but neither the condition of being pre-specified nor the \"out of two rolls\" actually hold)","Display_name":"Glen_b","Creater_id":805,"Start_date":"2010-08-08 17:37:45","Question_id":1424}
{"_id":{"$oid":"5837a57ba05283111e4d4957"},"Last_activity":"2015-09-15 06:59:18","Creator_reputation":3354,"Question_score":1,"Answer_content":"I think we all need more information. What are the features you are extracting ? What is the label you are trying to predict ? Type of music ? Singer ?If KNN and RFs do not perform better, maybe the predictors are not relevant with respect to the target.Besides, you did not mention parameter tuning, which has an important impact on the performances of the SVM.Edit.As you seem to be performing mutliclass prediction, 50% is not that bad (it would be awful in the case of a binary classification).Now, if you want to improve the performance of the model, you could try various kernels for your SVM and change their hyper-parameters. The cost parameter C of SVMs plays an important role as well.If the performance reached with your model after parameter tuning is not satisfying, you should try to add more features. Not being an expert in sound classification, I cannot help you.","Display_name":"RUser4512","Creater_id":73794,"Start_date":"2015-09-15 01:47:23","Question_id":172556}
{"_id":{"$oid":"5837a57ba05283111e4d4966"},"Last_activity":"2016-08-17 10:03:46","Creator_reputation":25275,"Question_score":4,"Answer_content":"When using R's runif by design you won't be able to sample neither 0, nor 1. Check it's source code:double u;    /* This is true of all builtin generators, but protect against       user-supplied ones */    do {u = unif_rand();} while (u \u0026lt;= 0 || u \u0026gt;= 1);return a + (b - a) * u;","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-17 10:03:46","Question_id":230317}
{"_id":{"$oid":"5837a57ba05283111e4d4975"},"Last_activity":"2016-08-17 10:01:19","Creator_reputation":26,"Question_score":0,"Answer_content":"I think all you need to do is \"score\" (create a new column in your database that contains the predicted values for each record in your database) using the regression model coefficients and functional form of the model (for linear regression example, y = XB where y is the predicted value from the regression model, X is your database, and B is a vector with the model coefficients).I'm not sure of the exact functional form of your regression model but in R you can write the equation from the command line:  y \u0026lt;- a + b*xHope this helps","Display_name":"Brian Griner","Creater_id":93865,"Start_date":"2016-08-17 10:01:19","Question_id":230218}
{"_id":{"$oid":"5837a57ba05283111e4d4981"},"Last_activity":"2016-08-17 10:00:50","Creator_reputation":5787,"Question_score":0,"Answer_content":"I offer a candidate solution which satisfies your requirements on the means. You will have to determine whether this satisfies your other desires.Let  = desired mean of tr, and  = desired mean of t.Solution: Draw all exponentials below independentlyGenerate tr per an exponential random variable having mean of For each sample value of tr, generate corresponding sample t = tr + [exponential random variable having mean of It might be easier to view the solution as follows: If tr is an n by 1 vector of exponential random numbers having mean of , and  is a vector of exponential random numbers having mean of , then  will be a vector of sample values of t.tr is exponentially distributed. t is exponentially distributed conditional on tr, or equivalently is a shifted exponential, in which the shift is by the amount tr, which of course differs in numerical value for each random variate of tr.Applying this solution to your stated values of , results in Edit: In an attempt to satisfy all your requirements, including negative correlation between tr and t, I generated tr and X according to a Gaussian copula (rather than independently), but otherwise as per the solution above.  For tr and t having respective means of 1 and 2, the smallest correlation I was able to achieve was 0.42, using a correlation of essentially -1 in the Gaussian for the copula.Alternatively, if you were willing to forgo the requirement that t \u003e tr, then your desired correlation between tr and t of -0.5 could be achieved by using a Gaussian copula to generate tr and t (without the \"t= tr + X\" method in my solution above) with correlation of approximately -0.724 in the Gaussian for the copula.I don't think it's possible to simultaneously satisfy all your requirements with the numerical values you have provided for mean(t), mean(tr), and corr(tr,t). I'd be very interested if someone can show that it is possible. Or perhaps someone can prove that it is not possible.However, it is possible to meet all your requirements for other values.  For instance, if mean(tr) = 1 and mean(tr) \u003e 5.9, the corr(tr,t) = -0.5 can be achieved using my \"t = tr + X\" method combined with the Gaussian copula, per the method described above, and that satisfies all your requirements (except that mean(t) must be \u003e 5.9).  Similarly, if you only require corr(tr,t) be negative, but not that it equal -0.5, then if mean(tr) = 1, this approach will work if mean(t) \u003e 2.6.  Perhaps use of this approach with some copula other than Gaussian would further \"improve\" things. I leave that as an exercise to you.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-16 10:11:08","Question_id":230085}
{"_id":{"$oid":"5837a57ba05283111e4d498e"},"Last_activity":"2016-08-17 09:58:03","Creator_reputation":136,"Question_score":0,"Answer_content":"I have seen the following paper cited before for Naive Bayes:   Hand, D. J., \u0026amp; Yu, K. (2001). Idiot's Bayes—not so stupid after all?. International statistical review, 69(3), 385-398.It is a bit of a review and discussion of the topic.","Display_name":"dpritch","Creater_id":93528,"Start_date":"2016-08-17 09:21:50","Question_id":18212}
{"_id":{"$oid":"5837a57ba05283111e4d498f"},"Last_activity":"2012-01-03 21:43:56","Creator_reputation":null,"Question_score":5,"Answer_content":"A naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions. Bayes' theorem was named after the Reverend Thomas Bayes (1702–61), who studied how to compute a distribution for the probability parameter of a binomial distribution. After Bayes' death, his friend Richard Price edited and presented this work in 1763, as An Essay towards solving a Problem in the Doctrine of Chances.So it is safe to say that Bayes classifiers have been around since the 2nd half of the 18th century.Especially as Stephen Stigler suggested (in 1983, Stephen M. Stigler, \"Who Discovered Bayes' Theorem?\" The American Statistician 37(4):290–296) that Bayes' theorem was discovered by Nicholas Saunderson some time before Bayes. On the other hand Edwards (1986) disputed that interpretation (in 1986, A. W. F. Edwards, \"Is the Reference in Hartley (1749) to Bayesian Inference?\", The American Statistician 40(2):109–110).Which takes us back to the safe assumption of \"2nd half of the 18th century\" again, as a naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem... which makes it \"naive\" is that it comes with strong (naive) independence assumptions. But practically, it's the same theorem.","Display_name":"user7997","Creater_id":null,"Start_date":"2012-01-03 14:29:57","Question_id":18212}
{"_id":{"$oid":"5837a57ba05283111e4d499b"},"Last_activity":"2016-08-17 09:56:46","Creator_reputation":111,"Question_score":0,"Answer_content":"Here's an answer from a survival package vignette I found helpful - it's linked in the first answer to the first question you linked to:Best packages for Cox models with time varying covariatesThey're referring to the long form data setup, or data with repeated entries for subjects.  One common question with this data setup is whether we need to worry about correlated data, since a given subject has multiple observations. The answer is no, we do not. The reason is that this representation is simply a programming trick. The likelihood equations at any time point use only one copy of any subject, the program picks out the correct row of data at each time. There two exceptions to this rule:       When subjects have multiple events, then the rows for the events are correlated within subject and a cluster variance is needed.    When a subject appears in overlapping intervals. This however is almost always a data error, since it corresponds to two copies of the subject being present in the same strata at the same time, e.g., she could meet herself at a party.  The example they give is fit \u0026lt;- coxph(Surv(time1, time2, status) ~ age + creatinine, data=mydata)suggesting that if you provide two times (start and end of period) to Surv instead of one, coxph() will figure out the rest.","Display_name":"BLT","Creater_id":81981,"Start_date":"2016-08-17 09:56:46","Question_id":178944}
{"_id":{"$oid":"5837a57ba05283111e4d499c"},"Last_activity":"2015-11-09 01:36:51","Creator_reputation":1031,"Question_score":5,"Answer_content":"Including cluster(ID) does not change the point estimates of the parameters. It does change the way that the standard errors are computed however. More details can be found in Therneau \u0026amp; Grambsch's book Extending the Cox Model, chapter 8.2. Note that in their example, they use method = \"breslow\" as correction for ties, but also with the default (method = \"efron\") a similar calculation for the se's will be used, and appears in the summary as \"robust se\". If cluster(ID) is used, a \"robust\" estimate of standard errors is imposed and possible dependence between subjects is measured (e.g. by standard errors and variance scores). Not using cluster(ID), on the other hand, imposes independence on each observation and more \"information\" is assumed in the data. In more technical terms, the score function for the parameters does not change, but the variance of this score does. A more intuitive argument is that 100 observations on 100 individuals provide more information than 100 observations on 10 individuals (or clusters).Vague indeed. In short, +frailty(ID) in coxph() fits standard frailty models with gamma or log-normal random effects and with non-parametric baseline hazard / intensity. frailtypack uses parametric baseline (also flexible versions with splines or piecewise constant functions) and also fits more complicated models, such as correlated frailty, nested frailty, etc.Finally, +cluster() is somewhat in the spirit of GEE, in that you take the score equations from a likelihood with independent observations, and use a different \"robust\" estimator for the standard errors. edit: Thanks @Ivan for the suggestions regarding the clarity of the post.","Display_name":"Theodor","Creater_id":55082,"Start_date":"2015-10-28 08:52:12","Question_id":178944}
{"_id":{"$oid":"5837a57ba05283111e4d49b1"},"Last_activity":"2016-08-17 09:12:14","Creator_reputation":738,"Question_score":0,"Answer_content":"If you can perform a bootstrapped analysis, I would. I wouldn't know how to do this in SPSS though. Sorry.Basically,Resample n observations for each group.Caluculate the t-statistic (or ANOVA parameter estimate)Repeat these steps about 1000+ timesPlot the histogram of your statisticsIf 95% of your simulated statistics are above a meaningful cut off, then you're overall results are meaningful. However, say only 80% of your generated statistics are above a meaningful cut-off, then your results may not be valid. From here, your options are to either bootstrap more or conclude that there is no significant difference between group 1 and group 2.","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-17 09:12:14","Question_id":228888}
{"_id":{"$oid":"5837a57ba05283111e4d49bf"},"Last_activity":"2016-08-17 09:02:12","Creator_reputation":1,"Question_score":0,"Answer_content":"In addition, sensitivity analysis can be considered as a tool to enhance model validity by choosing proper values(calibration) for the most critical input parameters.By using sensitivity analysis and defining input parameters values, we add more credibility to the model in hand.","Display_name":"user3637844","Creater_id":128025,"Start_date":"2016-08-17 09:02:12","Question_id":194262}
{"_id":{"$oid":"5837a57ba05283111e4d49c0"},"Last_activity":"2016-02-05 18:04:13","Creator_reputation":2788,"Question_score":8,"Answer_content":"This is a bit of an oversimplification, but model validation generally tells one about how well the current model fits the data at hand.Sensitivity analyses tell one how likely your results based upon that model would change given new information or changes to your assumptions.For example, someone could develop a model aimed at determining the impact that an intervention has on an outcome, and that model could validate well under their collected data (i.e. it seems very good at predicting response).  However, that model rests on a number of assumptions – one being that all covariates are accounted for. A sensitivity analysis could tell one how much your model results would change if this new, \"imaginary\" variable, with certain properties, existed.","Display_name":"StatsStudent","Creater_id":7962,"Start_date":"2016-02-05 17:43:41","Question_id":194262}
{"_id":{"$oid":"5837a57ba05283111e4d49cc"},"Last_activity":"2016-08-17 08:56:13","Creator_reputation":168,"Question_score":5,"Answer_content":"Here are a few observations:Your first layer of a single sigmoid neuron is a big bottleneck. Unless you are very lucky and the neuron is initialised to map your input onto the near-linear central part of the sigmoid, you end up with unnecessary information loss and vanishing gradient in the first layer right from the start. You maybe added this because a basic ANN is usually described to have 3 layers: input, hidden, and output, but the input layer is not actually modelled. It is better to think in the number of sets of weights are needed: input to hidden, and hidden to output.You only run a single epoch. That is not enough to learn the full function. At each epoch the weights are only updated a little bit to follow the local gradient, this needs to be repeated many more times to converge at the final result.In this case your batch size is fairly big, which means less updates are done per epoch. Usually smaller batch sizes can be more efficient, but this can differ per problem.After removing the first layer, increasing nb_epoch to 100 and decreasing batch_size to 10, I get a much better result:","Display_name":"sgvd","Creater_id":87605,"Start_date":"2016-08-17 08:56:13","Question_id":230298}
{"_id":{"$oid":"5837a57ba05283111e4d49d9"},"Last_activity":"2016-08-17 05:28:48","Creator_reputation":57702,"Question_score":1,"Answer_content":"A recursive partition is not going to yield a model that can be represented in a regression formula - that's one big advantage of trees.  A tree can have different interactions in different parts of the tree.LASSO does not deal well with collinearity. LASSO deals with overfitting. And a large part of Harrell's point is, I think, that you cannot get computers to do your thinking for you. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-17 05:28:48","Question_id":230303}
{"_id":{"$oid":"5837a57ba05283111e4d49e6"},"Last_activity":"2016-08-17 08:52:01","Creator_reputation":4307,"Question_score":0,"Answer_content":"This is not a full answer, but may help you get there.Say  is the total number of people invited to the party, and  is your position in the arrival sequence. Then from your setup we have that  is Poisson distributed with mean 10. So you can answer your questions in two steps: First, you can compute the answers conditional on knowing . Then, you must compute the final answers by incorporating the distribution of .Your post addresses only the first part. You are correct that , and . To finish, you now need to incorporate the distribution of .(You also need to account for some unit offsets, i.e. it is  people who arrive before you, and it is  which is Poisson distributed).Hope this helps!","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-17 08:52:01","Question_id":230134}
{"_id":{"$oid":"5837a57ba05283111e4d49f3"},"Last_activity":"2016-08-17 08:49:54","Creator_reputation":90,"Question_score":0,"Answer_content":"Ok so i solved my problem, just posting my solution here in case somebody has the same issue. Basically what I did is construct \"by hand\" a confusion matrix which is a 2D list of 20 rows/20 columns (20 because I had 20 categories). I filled this matrix at every step of the training by comparing the predicted category and the labeled category. Example when predicted category is number 16 and the labeled category is 7:confusion_matrix[16][7]+=1This confusion matrix allowed me to compute recall and precision values in the end by using the classic formula you can see here : https://en.wikipedia.org/wiki/Precision_and_recallCheers.","Display_name":"LoulouChameau","Creater_id":127872,"Start_date":"2016-08-17 08:49:54","Question_id":230125}
{"_id":{"$oid":"5837a57ba05283111e4d4a04"},"Last_activity":"2016-08-15 09:43:38","Creator_reputation":324,"Question_score":1,"Answer_content":"Instead of using a Z-Test, you should use the T-Test for difference between means.  You should not use the Z-Test because you do not know the true variances for groups Control, V1, V2,...,V6.  The T-Test uses sample estimates for variances:Z-Statistic:Z = \\frac{(\\bar{X}_2 - \\bar{X_1})}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}}T-Statistic:T = \\frac{(\\bar{X}_2 - \\bar{X_1})}{\\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}}The two key differences are: – true population variance for group k – sample variance from group kOnce you have computed your T-test statistic, you can compute the p-value here with the Student's T-Distribution withdf = \\min\\big(n_1 - 1, n_2 - 1\\big) Or this can be done in R with the following commands:t.test(Control,V1) t.test(Control,V2) t.test(Control,V3) t.test(Control,V4) t.test(Control,V5) t.test(Control,V6) ","Display_name":"David C","Creater_id":96030,"Start_date":"2016-08-15 06:23:36","Question_id":229698}
{"_id":{"$oid":"5837a57ba05283111e4d4a15"},"Last_activity":"2016-08-17 08:12:09","Creator_reputation":3619,"Question_score":1,"Answer_content":"The sensitivity is calculated purely from the true cases so the number of the true non-cases is not relevant and vice versa. However if the decision is being made by a human diagnostician and s/he knows the prevalence of the condition it may affect his/her criterion. The positive and negative predictive values are of course strongly affected by the prevalence.You may be interested in@ARTICLE{lijmer99,  author = {Lijmer, J G and Mol, B W and Heisterkamp, S and     Bonsel, G J and Prins, M H and van der Meulen, J H P      and Bossuyt, P M M},  year = {1999},  title = {Empirical evidence of design-related bias in     studies of diagnostic tests},  journal = {Journal of the American Medical Association},  volume = 282,  pages = {1061--1066}}which discusses a number of ways in which studies of diagnostic tests can be subject to bias induced by poor design.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-17 08:12:09","Question_id":230273}
{"_id":{"$oid":"5837a57ba05283111e4d4a22"},"Last_activity":"2016-08-17 07:43:35","Creator_reputation":17799,"Question_score":4,"Answer_content":"As you note, your definition of data-dredging implicitly excludes RF there's no significance testing taking place during the RF model construction.Your implicit question here, especially in the second paragraph, is whether RF is \"trustworthy\" in some sense. Fortunately, we have several reasons to believe that they are. A regular old vanilla decision tree is constructed as a forward-wise greedy algorithm that looks for the best split at every node. This is very unstable because the greedy nature of it means that a slight perturbation of the data could cause a dramatically different tree to be built: measurement noise or the exclusion of a small number of data points can entirely change the tree. In that sense, it is a high-variance estimator. On the other hand, the RF algorithm is explicitly designed to counter both of these in three ways (1) bootstrapping the sample (2) randomly subsetting features for each split and (3) averaging over many trees.This, of course, is all theoretical. As a practical matter, we can approximate how well a RF model is doing by making the standard out-of-sample measurements you make for any other model.To your specific claims, though, note that the \"data dredging\" definition doesn't mention \"bias\" at all. It doesn't even state that the relationships uncovered are invalid. It just says that data dredging is characterized by not forming a hypothesis formed ahead of time. I would say that by fitting the RF model, you've implicitly specified that you think the features you've presented have some bearing on the outcome, and that the relationship might be well-approximated by several decision trees (vice the relationship being linear as in a regression model). The RF hypothesis is much more flexible than in the linear regression context (the linear regression model has the hypothesis that the response is a multilinear function of the features), and I think that's more or less the source of your concern.I might also point out that by using this specific definition of data-dredging, you've implicitly adopted the conventions of significance testing and p-values as being an important part of the enterprise. By contrast, in my work, which has involved a considerable number of RF models, doesn't care about significance testing (at least in the sense of regression coefficients).It's difficult for me to directly address your claim that RF is \"biased\" because I don't think you mean statistical bias -- I think you mean that RF is untrustworthy because it engages in data dredging, but in a way that I can't quantify. Maybe you could sharpen your language here? I don't think that RF engages in data dredging, so I don't think that RF is untrustworthy for that reason. (I don't grant your premise.)The question here is one of model calibration. You've outlined the problem of the RF predicted probabilities not matching the true probability of the event in some reference population which is external to your training data. The simplest way to correct this would be to collect samples from whatever reference population you desire (and disjoint from your training data) and construct a logistic regression model which takes the RF output as the independent variable. Under specific conditions which are outlined extensively elsewhere, logistic regression yields calibrated probability estimates of the outcome.","Display_name":"Sycorax","Creater_id":22311,"Start_date":"2016-08-17 07:29:09","Question_id":230057}
{"_id":{"$oid":"5837a57ba05283111e4d4a2f"},"Last_activity":"2016-08-17 07:50:14","Creator_reputation":2088,"Question_score":0,"Answer_content":"When calculating the AUC, you need to provide a range of values (as @General Abrial mentioned), not the class labels (0/1)..  so you need to find how to get the non thresholded output from your SVM. For instance in scikit learn you would use decision_function, rather than the predict functionhttp://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html","Display_name":"seanv507","Creater_id":27556,"Start_date":"2016-08-17 07:50:14","Question_id":230279}
{"_id":{"$oid":"5837a57ba05283111e4d4a30"},"Last_activity":"2016-08-17 07:49:19","Creator_reputation":17799,"Question_score":3,"Answer_content":"I gather from the comments that you only have 2 unique predicted values. When you present some data to the classifier to make predictions, it gives either A or B as an output, rather than a continuous outcome in some range. For example, logistic regression gives output in (0,1), so any real number between 0 and 1 (such as 0.2, 0.123123, or 0.996) is a legitimate prediction. But your model might only give 0.0 or 1.0.Because your ROC curve must go from (0,0) to (1,1) and be non-decreasing, and you have 2 unique prediction values, you can only have 2 points in between. Thus, the plot you have. Most ROC curves are produced by models that give continuous outcomes. Because there are more decision points, there are more opportunities to measure error rates rates. That produces the kind of ROC curve in the second figure. By way of contrast, for example, logistic regression gives output in (0,1), so any real number between 0 and 1 (such as 0.2, 0.123123, or 0.996) is a legitimate prediction. But your model might only give 0.0 or 1.0.","Display_name":"Sycorax","Creater_id":22311,"Start_date":"2016-08-17 07:49:19","Question_id":230279}
{"_id":{"$oid":"5837a57ba05283111e4d4a31"},"Last_activity":"2016-08-17 07:25:16","Creator_reputation":2544,"Question_score":3,"Answer_content":"You have four point estimates of sensitivity and specificity. This could be due to really grouped estimates or because you have too few samples.","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-08-17 07:25:16","Question_id":230279}
{"_id":{"$oid":"5837a57ba05283111e4d4a3e"},"Last_activity":"2016-08-17 07:58:13","Creator_reputation":5445,"Question_score":1,"Answer_content":"The ICC (intra-class correlation) is interpretable and useful for random intercepts models. It is the correlation between two observations within the same cluster. The higher the correlation within the clusters (ie. the larger the ICC) the lower the variability is within the clusters and consequently the higher the variability is between the clusters.Alternatively, it is also measure of how much variation there is at each level, and this is why it is also called the variance partition coefficient (VPC). Therefore, as you rightly point out, in a random intercepts model, when the ICC is large, this is evidence in favour of retaining the random intercepts, while when it is small, this is evidence in favour of discarding random intercepts. However, as is often the case in applied statistics, what determines \"small\" and \"large\" is context-specific and discipline-specific. Once we introduce random slopes/coefficients, things get more complicated. The ICC is no longer the same as the VPC, because the ICC will be a function of the variable(s) for which random slopes are specified. Therefore there can be an infinite number of values for the ICC is the variable in question is continuous, and as many as the number of levels if it is categorical or a count. Thus any interpretation of the ICC in a random slopes model becomes more difficult. Stata, for example, will calculate a single value for the ICC but in a random slopes model, this is accompanied by the warning:  Note: ICC is conditional on zero values of random-effects covariates.In other words, it has computed the ICC based on a value of zero for the random slope variable(s), so any interpretation of the ICC is also based on a value of zero for the slope variable(s).Regarding your question:   If the ICC the small, then it means theres little variability between the clusters, which might suggest that their means are similar and thus a random intercepts model may not be needed, but does that automatically mean a random slopes model is also not warranted?No, because it is possible for each cluster to have the same intercept (no random intercept) while the slopes may indeed vary, which we can visualise like this:If we want to know whether random slopes are supported by the data, one approach is to fit models with and without random slopes and use a likelihood ratio test.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-17 07:52:21","Question_id":230214}
{"_id":{"$oid":"5837a57ba05283111e4d4a4b"},"Last_activity":"2016-08-17 07:56:47","Creator_reputation":7965,"Question_score":3,"Answer_content":"In the neural network terminology:one epoch = one forward pass and one backward pass of all the training examplesbatch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.number of iterations =  number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-17 07:56:47","Question_id":230120}
{"_id":{"$oid":"5837a57ba05283111e4d4a58"},"Last_activity":"2016-08-17 07:40:43","Creator_reputation":2088,"Question_score":1,"Answer_content":"In database applications/data warehouse/BI it is common to refer to additive measures additive example: moneysemi-additive: balance (can aggregate across eg departments but not time)non-additive: ratios (eg growth rate etc)http://stackoverflow.com/questions/34295293/whats-the-difference-between-additive-semi-additive-and-non-additive-measures http://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/additive-semi-additive-non-additive-fact/","Display_name":"seanv507","Creater_id":27556,"Start_date":"2016-08-17 05:16:13","Question_id":154463}
{"_id":{"$oid":"5837a57ba05283111e4d4a59"},"Last_activity":"2015-05-28 09:17:12","Creator_reputation":28606,"Question_score":11,"Answer_content":"Properties that are physically additive are called extensive. Mass is extensive, as when you add (literally!) weights to a balance. A feature of extensive properties is that totals make sense. In your example, gas used, measured in kWh, is one instance. The word physically is not meant restrictively here. My income in April and my income in May can be added, as can my expenditures. Both are extensive properties. So, there are other non-physical situations in which addition makes sense. If totals make sense, then means make sense too. Whether they are the measures you want to use, however, depends on your purpose. Otherwise properties that are not physically additive are called intensive. Temperature is intensive. If you mix bodies, the resulting temperature is some kind of weighted mean, and certainly not the total. This Wikipedia article says much more from a physical science point of view. One source emphasising the importance of this distinction in statistical science is Cox, D.R. and Snell, E.J. 1981. Applied statistics: principles and examples. London: Chapman and Hall. See p.14. (They use the term non-extensive, which I do not find attractive.) ","Display_name":"Nick Cox","Creater_id":22047,"Start_date":"2015-05-28 09:17:12","Question_id":154463}
{"_id":{"$oid":"5837a57ba05283111e4d4a68"},"Last_activity":"2016-08-17 07:15:42","Creator_reputation":12260,"Question_score":5,"Answer_content":"Per Cliff AB's comment to the OP, it sounds like they are heading towards an Empirical Bayesian philosophy. There are three main Bayesian schools of thought, and Empirical Bayes estimates priors from data, often with frequentist methods. That doesn't conform exactly to the quote (which implies Bayes up front, frequentist-like concerns afterwards), but we shouldn't overlook Cliff AB's excellent comment.Also, there was, and may still be, a school of Bayesian thought that you don't have to check anything after a Bayesian procedure. More modern thought would use posterior predictive checks, and perhaps that kind of double-check-your-answers approach is what the quote is referring to.Also, frequentist philosophy is concerned with procedures rather than inferences from data. So perhaps that is also a clue to the quote's meaning.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-08-17 07:15:42","Question_id":230097}
{"_id":{"$oid":"5837a57ba05283111e4d4a69"},"Last_activity":"2016-08-16 14:51:47","Creator_reputation":1009,"Question_score":2,"Answer_content":"In the context of this data science class, my interpretation of \"check like a frequentist\" is that you evaluate the performance of your prediction function or decision function on held-out validation data.  The advice to \"think like a Bayesian\" expresses the opinion that a prediction function derived from a Bayesian approach will generally give good results. ","Display_name":"DavidR","Creater_id":6640,"Start_date":"2016-08-16 14:51:47","Question_id":230097}
{"_id":{"$oid":"5837a57ba05283111e4d4a6a"},"Last_activity":"2016-08-16 14:49:18","Creator_reputation":56,"Question_score":1,"Answer_content":"It sounds like \"think like a Bayesian, check like a frequentist\" refers to one's approach in statistical design and analysis.  As I understand it, Bayesian thinking involves some belief about prior situations (experimentally or statistically), let's say for example that the mean reading scores for 4th-graders is 80 words per minute, and that some intervention might increase this to 90 words per minute. These are beliefs based on prior studies and hypotheses. Frequentist thinking extrapolates the findings (of the intervention) to obtain confidence intervals or other statistics that are based on the theoretical and practical frequency or probability of these results happening again (i.e., how \"frequently\"). For example the post-intervention reading score might be 91 words per minute with a 95% confidence interval of 85 to 97 words per minute and an associated p-value (probability value) of this being different from the pre-intervention score. So 95% of the time, the new reading scores would be between 85 and 97 words per minute after the intervention.   Therefore \"think like a Bayesian\"---i.e., theorize, hypothesize, look at previous evidence, and \"check like a frequentist\"---i.e., how frequently would these experimental results occur, and how likely are they to be due to chance rather than the intervention.   ","Display_name":"Jeremy","Creater_id":115931,"Start_date":"2016-08-16 14:49:18","Question_id":230097}
{"_id":{"$oid":"5837a57ba05283111e4d4a6b"},"Last_activity":"2016-08-16 09:28:01","Creator_reputation":17374,"Question_score":13,"Answer_content":"Bayesian statistics summarize beliefs whereas frequentist statistics summarize evidence. The Bayesians view probability as a degree of belief. This inclusive and generative type of reasoning is useful for formulating hypotheses. For instance, Bayesians may be able to arbitrarily assign some probability to the notion that the moon is made of green cheese, regardless of whether astronauts have actually been able to travel there to verify this. This hypothesis is perhaps supported by the idea that, from afar, the moon looks like green cheese. Frequentists cannot singularly conceive of a hypothesis that is more than a strawman, nor can they say evidence favors one hypothesis over another. Even maximum likelihood only generates a statistic which is \"most consistent with what was observed\". Formally, Bayesian statistics allows us to think outside the box and propose defensible ideas from data. But this is strictly hypothesis generating in nature. Frequentist statistics are best applied to confirm hypotheses. When an experiment is conducted well, frequentist statistics provide an \"independent observer\" or \"empirical\" context to the findings by eschewing priors. This is consistent with the Karl Popper philosophy of science. The point of evidence is not to promulgate a certain idea. Plenty of evidence is consistent with incorrect hypotheses. Evidence can merely falsify beliefs. The influence of priors is generally regarded as a bias in statistical reasoning. As you know, we can make up any great number of reasons for why things happen. Psychologically, many people believe that our observer bias is the result of priors in our brain that keep us from truly weighting what we see. \"Hope clouds observation\" as the Reverend Mother said in Dune. Popper made this idea rigorous.This had great historical importance in some of the greatest scientific experiments of our time. For instance, John Snow meticulously collected evidence for the Cholera epidemic and concluded astutely that Cholera is not caused by moral deprivation, and pointed out that the evidence was highly consistent with sewage contamination: note he did not conclude this, Snow's findings predated the discovery of bacteria, and there was no mechanistic or etiologic understanding. A similar discourse is found in Origin of Species. We didn't actually know whether the moon was made of green cheese until astronauts actually landed on the surface and collected samples. At that point, Bayesian posteriors have assigned very, very low probability to any other possibility, and Frequentists at best can say that the samples are highly inconsistent with anything except moon dust.In summary, Bayesian statistics are amenable to hypothesis generating and frequentist statistics are amenable to hypothesis confirmation. Ensuring that data are collected independently in these endeavors is one of the greatest challenges modern statisticians face. ","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-16 09:28:01","Question_id":230097}
{"_id":{"$oid":"5837a57ba05283111e4d4a6c"},"Last_activity":"2016-08-16 09:00:24","Creator_reputation":2761,"Question_score":28,"Answer_content":"The main difference between the Bayesian and frequentist schools of statistics arises due to a difference in interpretation of probability. A Bayesian probability is a statement about personal belief that an event will (or has) occurred. A frequentist probability is a statement about the proportion of similar events that occur in the limit as the number of those events increases. For me, to \"think like a Bayesian\" means to update your personal belief as new information arises and to \"check [or worry] like a frequentist\" means to be concerned with performance of statistical procedures aggregated across the times those procedures are used, e.g. what is the coverage of credible intervals, what is the Type I/II error rates, etc. ","Display_name":"jaradniemi","Creater_id":40440,"Start_date":"2016-08-16 09:00:24","Question_id":230097}
{"_id":{"$oid":"5837a57ca05283111e4d4a83"},"Last_activity":"2016-07-15 08:36:46","Creator_reputation":1966,"Question_score":1,"Answer_content":"I can confirm the results by Kontorus with the following script:# Define numbersnums \u0026lt;- 0:9counter \u0026lt;- 0for(i in nums){  for(j in nums){    for(k in nums){      for(l in nums){        if(i+j == k+l){          counter \u0026lt;- counter + 1        }      }    }  }}(counter)Yields 670. You can change the range of numbers to try non-decimal, but then you need to define an addition operator for them, but I assume that addition is meant as addition for base 10.","Display_name":"Gumeo","Creater_id":68057,"Start_date":"2016-07-15 08:36:46","Question_id":223954}
{"_id":{"$oid":"5837a57ca05283111e4d4a84"},"Last_activity":"2016-07-15 08:34:19","Creator_reputation":309,"Question_score":3,"Answer_content":"This is a permutation problem, and it is like the sum of a dice roll problem. Consider that the sum of the first two digits is 0, then the combination is 0000. If the sum of the first two digits is 18, then the combination is 9999. Now consider if the sum of the first two digits is 1, then the possible combinations are 1001, 1010, 0110, 0101, and if the sum of the first two digits is 17, then we have 9889, 9898, 8998, 8989. Now you should start to see a pattern. (Sum of 2 Digit Numbers = Sums, Number of 2 Digit Combinations = 2Dig, Number of 4 Digit Combinations = 4Dig)Sums    2Dig    4Dig0       1       11       2       42       3       93       4       164       5       25...9       10      100...14      5       2515      4       1616      3       917      2       418      1       1Now you can see that the total number of 4 digit combinations should be the sum of . This equals 670","Display_name":"akash87","Creater_id":120392,"Start_date":"2016-07-15 08:34:19","Question_id":223954}
{"_id":{"$oid":"5837a57ca05283111e4d4a85"},"Last_activity":"2016-07-15 08:31:01","Creator_reputation":272,"Question_score":2,"Answer_content":"Assuming decimal digits:The first two digits can be anything from 00 to 99. There are a total of 100 possibilities for these first two digits. You would then need to test the remaining two digits for each of the possible first two digits. The number of times you would need to test would be equal to the number of possible sum combinations. The sum of these two digits can range from 0 to 18. Sum            0  1  2  3  4  5  6  7  8   9  10  11  12  13  14  15  16  17  18 Combinations   1  2  3  4  5  6  7  8  9  10   9   8   7   6   5   4   3   2   1For 00, the sum is 0. You would only need to test sums of 0, which there are 1. For 09, you would need to test 10 different combinations of sums of 9. For example, you would test 0909, 0918, 0927, etc. Note that the first two digits are also seen in the last two digits (09 and 09). There are a total of 10*10 = 100 different combinations where the first and last two digits have a sum of 9. The total number of combinations should be 1*1 + 2*2 + 3*3 + ... + 9*9 + 8*8 + ... + 1*1. This is equal to 670.","Display_name":"Kontorus","Creater_id":90858,"Start_date":"2016-07-15 08:21:54","Question_id":223954}
{"_id":{"$oid":"5837a57ca05283111e4d4a94"},"Last_activity":"2016-08-09 11:46:33","Creator_reputation":3398,"Question_score":2,"Answer_content":"Since it looks like you're sampling without replacement, the answer is:\\binom{m-1}{n-1},which in your example is:\\binom{3}{1}=3.This is easy to see: set aside an element, say \"A\". Now pick from the remaining elements. Since order doesn't matter, you get the above binomial coefficient.","Display_name":"Alex R.","Creater_id":61092,"Start_date":"2016-08-09 11:46:33","Question_id":229044}
{"_id":{"$oid":"5837a57ca05283111e4d4aa1"},"Last_activity":"2016-08-16 01:50:05","Creator_reputation":5189,"Question_score":1,"Answer_content":"Your method (the one that produces the wrong result) consists of summing the following three probabilitiesThe first card hitsThe first card hits and the second card hitsThe first card does not hit and the second card hits But these events are not mutually exclusive so the probability of their union (probability that at least one of them occurs) is not obtained by summing the probabilities of the individual events. You are double-counting the possibility of both cards hitting, since it is contained both in the first term and the second term. ","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-16 01:50:05","Question_id":229972}
{"_id":{"$oid":"5837a57ca05283111e4d4ab0"},"Last_activity":"2016-08-17 05:45:14","Creator_reputation":11,"Question_score":1,"Answer_content":"The likelihood function you have is actually based on . Then, instead of having , you have , where  denotes a summary statistic which is the sequence . If the summary statistic  is sufficient, then the estimator is in fact the MLE. Otherwise, they are different.Reference: https://en.wikipedia.org/wiki/Sufficient_statistic#Fisher.E2.80.93Neyman_factorization_theoremThere is an area that uses summary statistics to obtain inferences on the parameters called ABC:https://en.wikipedia.org/wiki/Approximate_Bayesian_computation","Display_name":"Alpha","Creater_id":127991,"Start_date":"2016-08-17 05:45:14","Question_id":230263}
{"_id":{"$oid":"5837a57ca05283111e4d4abd"},"Last_activity":"2016-08-17 05:17:23","Creator_reputation":3619,"Question_score":4,"Answer_content":"If you convert  to  then your analysis will be conservative but you will at least have been able to include all the studies. Similarly for  and so on. The problem comes from the ones which say  as the only safe option here is to convert them to , your suggestion of  cannot really be justified. Be careful with the method you use as some of them do not allow for  or  so you would need to use a value very close to the boundary.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-17 05:17:23","Question_id":230237}
{"_id":{"$oid":"5837a57ca05283111e4d4ace"},"Last_activity":"2010-12-08 12:16:01","Creator_reputation":37824,"Question_score":8,"Answer_content":"Log-linear models might be another option to look at, if you want to study your two-way data structure.If you assume that the two samples are matched (i.e., there is some kind of dependency between the two series of locutions) and you take into consideration that data are actually counts that can be considered as scores or ordered responses (as suggested by @caracal), then you can also look at marginal models for matched-pairs, which usually involve the analysis of a square contingency table. It may not be necessarily the case that you end up with such a square Table, but we can also decide of an upper-bound for the number of, e.g. passive sentences. Anyway, models for matched pairs are well explained in Chapter 10 of Agresti, Categorical Data Analysis; relevant models for ordinal categories in square tables are testing for quasi-symmetry (the difference in the effect of a category from one case to the other follows a linear trend in the category scores), conditional symmetry ( or , ), and quasi-uniform association (linear-by-linear association off the main diagonal, which in the case of equal-interval scores means an uniform local association). Ordinal quasi-symmetry (OQS) is a special case of linear logit model, and it can be compared to a simpler model where only marginal homogeneity holds with an LR test, because ordinal quasi-symmetry + marginal homogeneity  symmetry.Following Agresti's notation (p. 429), we consider  ordered scores for variable  (in rows) and variable  (in columns);  or  denotes any row or column. The OQS model reads as the following log-linear model:\r\\log\\mu_{ab}=\\lambda+\\lambda_a+\\lambda_b+\\beta u_b +\\lambda_{ab}\r\rwhere  for all . Compared to the usual QS model for nominal data which is , where  would mean independence between the two variables, in the OQS model we impose  (hence introducing the idea of a linear trend). The equivalent logit representation is , for .If , then we have symmetry as a special case of this model. If , then we have stochastically ordered margins, that is  means that column mean is higher compared to row mean (and the greater , the greater the differences between the two joint probabilities distributions  and  are, which will be reflected in the differences between row and column marginal distributions). A test of  corresponds to a test of marginal homogeneity. The interpretation of the estimated  is straightforward: the estimated probability that score on variable  is  units more positive than the score on  is  times the reverse probability. In your particular case, it means that  might allow to quantify the influence that one particular speaker exerts on the other.Of note, all R code was made available by Laura Thompson in her S Manual to Accompany Agresti's Categorical Data Analysis.Hereafter, I provide some example R code so that you can play with it on your own data. So, let's try to generate some data first:set.seed(56)d \u0026lt;- as.data.frame(replicate(2, rpois(420, 1.5)))colnames(d) \u0026lt;- paste(\"S\", 1:2, sep=\"\")d.tab \u0026lt;- table(dS2, dnn=names(d)) # or xtabs(~S1+S2, d)library(vcdExtra)structable(~S1+S2, data=d)# library(ggplot2)# ggfluctuation(d.tab, type=\"color\") + labs(x=\"S1\", y=\"S2\") + theme_bw()Visually, the cross-classification looks like this:   S2  0  1  2  3  4  5  6S1                        0     17 35 31  8  7  3  01     41 41 30 23  7  2  02     19 43 18 18  5  0  13     11 21  9 15  2  1  04      0  3  4  1  0  0  05      1  0  0  2  0  0  06      0  0  0  1  0  0  0Now, we can fit the OQS model. Unlike Laura Thompson which used the base glm() function and a custom design matrix for symmetry, we can rely on the gnm package; we need, however, to add a vector for numerical scores to estimate  in the above model.library(gnm)d.long \u0026lt;- data.frame(counts=c(d.tab), S1=gl(7,1,7*7,labels=0:6),                     S2=gl(7,7,7*7,labels=0:6))d.long\\hat\\beta=0.123\\exp(0.123)=1.13\\hat\\betascores \u0026lt;- rep(1:4,each=4)summary(mod.oqs \u0026lt;- gnm(counts~scores+Symm(PreSex,ExSex), data=table.10.5,                        family=poisson)) # beta = -2.857anova(mod.oqs) # G^2(5)=2.10","Display_name":"chl","Creater_id":930,"Start_date":"2010-12-08 07:39:39","Question_id":5171}
{"_id":{"$oid":"5837a57ca05283111e4d4acf"},"Last_activity":"2010-12-06 04:00:31","Creator_reputation":8181,"Question_score":2,"Answer_content":"You seem to have ordered categorical data, therefore I suggest a linear-by-linear test as described by Agresti (2007, p229 ff). Function lbl_test() of package coin implements it in R.Agresti, A. (2007). Introduction to Categorical Data Analysis. 2nd Ed. Hoboken, New Jersey: John Wiley \u0026amp; Sons. Hoboken, NJ: Wiley.","Display_name":"caracal","Creater_id":1909,"Start_date":"2010-12-06 04:00:31","Question_id":5171}
{"_id":{"$oid":"5837a57ca05283111e4d4ad0"},"Last_activity":"2010-12-05 22:01:20","Creator_reputation":1186,"Question_score":0,"Answer_content":"I would maybe start with a rank correlation analysis.The issue is that you may have very low correlations as the effects you are trying to capture are small.Both Kendall and Spearman correlation coefficients are implemented in R incor(x=A, y=B, method = \"spearman\")  cor(x=A, y=B, method = \"kendall\")","Display_name":"RockScience","Creater_id":1709,"Start_date":"2010-12-05 22:01:20","Question_id":5171}
{"_id":{"$oid":"5837a57ca05283111e4d4adf"},"Last_activity":"2016-08-17 04:45:17","Creator_reputation":1,"Question_score":0,"Answer_content":"I choose to solve my problem like this. First i simulated my data following this tutorial .To assign the integer values per the original frequency distrobution i did the following:a1_cut \u0026lt;- quantile(rdataint_a1[(rdataint_a1[(rdataa1 \u0026lt;= a1_cut[2])] \u0026lt;- 2rdataa1 \u0026gt; a1_cut[2]) \u0026amp; (rdataint_a1[(rdataa1 \u0026lt;= a1_cut[4])] \u0026lt;- 4rdataa \u0026gt; a1_cut[4])] \u0026lt;- 5Any opinions on this way to solve it would be great.","Display_name":"Wasser","Creater_id":127118,"Start_date":"2016-08-17 04:45:17","Question_id":228975}
{"_id":{"$oid":"5837a57ca05283111e4d4aec"},"Last_activity":"2016-08-17 02:06:04","Creator_reputation":24971,"Question_score":2,"Answer_content":"This will not be an answer, only the explanation of how I would approach this problem. Asymptotics is usually always about the sums, so it is better to express all the quantities in term of sums. The main sum has 4 summands, where each summand depends on , so it is clear that we need to deal with each summand independently. Assume that we have a sample ,  where  and  and  are independent Bernoulli random variables. Then we have\\hat{p}_{1*}=\\frac{\\sum_{i=1}^nX_i}{n},\\;\\; \\hat{p}_{0*}=\\frac{\\sum_{i=1}^n(1-X_i)}{n}.The definitions for  and  are respectively the same. Now \\hat{p}_{11}=\\frac{\\sum_{i=1}^nX_iY_i}{n},\\;\\; \\hat{p}_{00}=\\frac{\\sum_{i=1}^n (1-X_i)(1-Y_i)}{n},and \\hat{p}_{01}=\\frac{\\sum_{i=1}^n(1-X_i)Y_i}{n},\\;\\; \\hat{p}_{10}=\\frac{\\sum_{i=1}^n X_i(1-Y_i)}{n}.By substituting these expressions into the initial sum and doing some algebra, the resulting expressions either will look as the selfnormalizing sums of independent random variables, which will give you asymptotic results immediately, or you will get some form of  statistic for which again you can find some sort of asymptotic results. These are of course some sort of educated guesses.","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2016-08-17 02:06:04","Question_id":230130}
{"_id":{"$oid":"5837a57ca05283111e4d4afb"},"Last_activity":"2016-08-17 04:31:41","Creator_reputation":51,"Question_score":1,"Answer_content":"Yes, the normal is a particular case of the skew normal distribution:s(x;\\alpha) = 2\\phi(x)\\Phi(\\alpha x),then, No, the skew normal is not a particular case of the sinh-arcsinh normal with unit kurtosis parameter. There are several types of \"skew normal\" distributions, it is just that Azzalini called his distribution \"the skew normal\" for marketing purposes. A good summary of skew distributions can be found in this pdf.                        ","Display_name":"Thanis","Creater_id":97249,"Start_date":"2015-12-06 14:02:31","Question_id":153762}
{"_id":{"$oid":"5837a57ca05283111e4d4b0a"},"Last_activity":"2016-08-17 02:54:59","Creator_reputation":46,"Question_score":3,"Answer_content":"I will assume that by \"uniform\", you mean uniform(0,1). Plot the theoretical quantiles (using your favourite rule) vs the empirical quantiles (R code):n =100y = runif(n)seq(0.05,1,length(y))plot(1:n/(n+1),sort(y))Alternatively, you can plot a QQ envelope to see how well the uniform distribution reproduces the data (this is just a visual inspection):n =100y = runif(n)seq(0.05,1,length(y))plot(1:n/(n+1),sort(y),col=\"red\")for(i in 1:1000){points(sort(runif(length(y))),sort(y),pch=\".\")}points(1:n/(n+1),sort(y),col=\"red\",pch=16)You can test the assumption of uniform distribution using the Kolmogorov-Smirnov test:ks.test(y,\"punif\",0,1)","Display_name":"Uncle Ben","Creater_id":127965,"Start_date":"2016-08-17 02:49:24","Question_id":230226}
{"_id":{"$oid":"5837a57ca05283111e4d4b17"},"Last_activity":"2015-12-16 17:34:30","Creator_reputation":11,"Question_score":1,"Answer_content":"You want a distribution for  each quarter (given a state),  each state (given a region), and  each region.That means you'll need at least some state parameters indexed by s (in your model b0, b1, b2), region parameters indexed by r (which I'll call c0, c1, c2), and global parameters (which I'll call d0, d1, d2). Your model should look something like this:modelstring = \"  model {    for ( i in 1:N ) {      y[i] ~ dnorm(y.hat[i], tau.i)      y.hat[i] \u0026lt;- b0[s[i]] + b1[s[i]] * x1[i] + b2[s[i]] * x2[i] + b3[s[i], a1[i]]    }    for ( s in 1:Nstate ) {      b0[s] ~ dnorm(c0[r[s]], b0t)      b1[s] ~ dnorm(c1[r[s]], b1t)      b2[s] ~ dnorm(c2[r[s]], b2t)      for ( j in 1:Nqtr ) {        b3[s, j] ~ dnorm(0, tau.sq.alpha)      }    }    for ( r in 1:Nregion ) {      c0[r] ~ dnorm(d0, c0t)      c1[r] ~ dnorm(d1, c1t)      c2[r] ~ dnorm(d2, c2t)    }    b0t ~ dgamma(0.001, 0.001)    b1t ~ dgamma(0.001, 0.001)    b2t ~ dgamma(0.001, 0.001)    c0t ~ dgamma(0.001, 0.001)    c1t ~ dgamma(0.001, 0.001)    c2t ~ dgamma(0.001, 0.001)    d0 ~ dnorm(0, .0001)    d1 ~ dnorm(0, .0001)    d2 ~ dnorm(0, .0001)    tau.i ~ dgamma(0.001, 0.001)    tau.sq.alpha ~ dgamma(0.001, 0.001)    sigma.sq.alpha \u0026lt;- 1 / tau.sq.alpha    }\"where r[s] gives a region index, given a state index.","Display_name":"user98453","Creater_id":98453,"Start_date":"2015-12-16 17:34:30","Question_id":185254}
{"_id":{"$oid":"5837a57ca05283111e4d4b24"},"Last_activity":"2016-08-17 04:07:02","Creator_reputation":1,"Question_score":0,"Answer_content":"You can input the dataframe with those columns for which you need bivariate associations and use the cor() in R to get the job done.If you also need the significance of correlation between two variables, you can use rcorr() Hmisc library as followslibrary(Himsc)df\u0026lt;-your_data_framecor_matrix\u0026lt;-rcorr(as.matrix(df),type=\"pearson\")cor_values\u0026lt;-cor_matrixp","Display_name":"Geethasai Krishna","Creater_id":127959,"Start_date":"2016-08-17 02:31:02","Question_id":229979}
{"_id":{"$oid":"5837a57da05283111e4d4bed"},"Last_activity":"2016-08-16 14:05:11","Creator_reputation":31,"Question_score":3,"Answer_content":"A function to compute omega squared is straightforward to write. This function takes the object returned by the aov test, and calculates and returns and omega squared:omega_sq \u0026lt;- function(aovm){    sum_stats \u0026lt;- summary(aovm)[[1]]    SSm \u0026lt;- sum_stats[[\"Sum Sq\"]][1]    SSr \u0026lt;- sum_stats[[\"Sum Sq\"]][2]    DFm \u0026lt;- sum_stats[[\"Df\"]][1]    MSr \u0026lt;- sum_stats[[\"Mean Sq\"]][2]    W2 \u0026lt;- (SSm-DFm*MSr)/(SSm+SSr+MSr)    return(W2)}edit: updated function for n-way aov models:omega_sq \u0026lt;- function(aov_in, neg2zero=T){    aovtab \u0026lt;- summary(aov_in)[[1]]    n_terms \u0026lt;- length(aovtab[[\"Sum Sq\"]]) - 1    output \u0026lt;- rep(-1, n_terms)    SSr \u0026lt;- aovtab[[\"Sum Sq\"]][n_terms + 1]    MSr \u0026lt;- aovtab[[\"Mean Sq\"]][n_terms + 1]    SSt \u0026lt;- sum(aovtab[[\"Sum Sq\"]])    for(i in 1:n_terms){        SSm \u0026lt;- aovtab[[\"Sum Sq\"]][i]        DFm \u0026lt;- aovtab[[\"Df\"]][i]        output[i] \u0026lt;- (SSm-DFm*MSr)/(SSt+MSr)        if(neg2zero \u0026amp; output[i] \u0026lt; 0){output[i] \u0026lt;- 0}    }    names(output) \u0026lt;- rownames(aovtab)[1:n_terms]    return(output)}","Display_name":"Janak Mayer","Creater_id":61568,"Start_date":"2014-11-26 16:13:08","Question_id":2962}
{"_id":{"$oid":"5837a57da05283111e4d4bee"},"Last_activity":"2014-12-03 15:38:26","Creator_reputation":11,"Question_score":1,"Answer_content":"I had to recently report an .partialOmegas \u0026lt;- function(mod){    aovMod \u0026lt;- mod    if(!any(class(aovMod) %in% 'aov')) aovMod \u0026lt;- aov(mod)    sumAov     \u0026lt;- summary(aovMod)[[1]]    residRow   \u0026lt;- nrow(sumAov)    dfError    \u0026lt;- sumAov[residRow,1]    msError    \u0026lt;- sumAov[residRow,3]    nTotal     \u0026lt;- nrow(model.frame(aovMod))    dfEffects  \u0026lt;- sumAov[1:{residRow-1},1]    ssEffects  \u0026lt;- sumAov[1:{residRow-1},2]    msEffects  \u0026lt;- sumAov[1:{residRow-1},3]    partOmegas \u0026lt;- abs((dfEffects*(msEffects-msError)) /                  (ssEffects + (nTotal -dfEffects)*msError))    names(partOmegas) \u0026lt;- rownames(sumAov)[1:{residRow-1}]    partOmegas}It is a messy function that can easily be cleaned up. It computes the partial , and should probably only be used on between-subjects factorial designs.","Display_name":"Stephen Martin","Creater_id":62045,"Start_date":"2014-12-03 15:21:51","Question_id":2962}
{"_id":{"$oid":"5837a57da05283111e4d4bef"},"Last_activity":"2012-11-02 11:18:38","Creator_reputation":186,"Question_score":2,"Answer_content":"I found an omega squared function in somebody's .Rprofile that they made available online:http://www.estudiosfonicos.cchs.csic.es/metodolo/1/.Rprofile","Display_name":"Jim","Creater_id":11675,"Start_date":"2012-11-02 11:18:38","Question_id":2962}
{"_id":{"$oid":"5837a57da05283111e4d4bf0"},"Last_activity":"2010-09-22 05:32:55","Creator_reputation":6669,"Question_score":3,"Answer_content":"I'd suggest that generalized eta square is considered (ref, ref) a more appropriate measure of effect size. It is included in the ANOVA output in the ez package for R.","Display_name":"Mike Lawrence","Creater_id":364,"Start_date":"2010-09-22 05:32:55","Question_id":2962}
{"_id":{"$oid":"5837a57da05283111e4d4c01"},"Last_activity":"2016-08-16 13:24:10","Creator_reputation":738,"Question_score":1,"Answer_content":"For a nonparametric approach for pairwise comparisons, I recommend using a Holm correction to the Wilcoxon Rank Sum (aka Mann-Whitney U) test. Please follow the assumptions for the Wilcoxon Rank Sum test. Reference: https://statistics.laerd.com/spss-tutorials/mann-whitney-u-test-using-spss-statistics.phpThe Holm correction (https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method)In R, this can be implemented as: \u0026gt; data(\"PlantGrowth\")\u0026gt; head(PlantGrowth)  weight group1   4.17  ctrl2   5.58  ctrl3   5.18  ctrl4   6.11  ctrl5   4.50  ctrl6   4.61  ctrl\u0026gt; \u0026gt; pairwise.wilcox.test(x = PlantGrowthgroup, p.adjust.method = \"holm\", paired = FALSE)    Pairwise comparisons using Wilcoxon rank sum test data:  PlantGrowthgroup      ctrl  trt1 trt1 0.199 -    trt2 0.126 0.027P value adjustment method: holm Warning message:In wilcox.test.default(xi, xj, paired = paired, ...) :  cannot compute exact p-value with ties\u0026gt; \u0026gt; ## see: ?wilcox.test\u0026gt; ## By default (if exact is not specified), an exact p-value is computed if the samples contain less than 50 finite values and there are no ties. \u0026gt; ## Otherwise, a normal approximation is used.\u0026gt; ","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-16 13:24:10","Question_id":157742}
{"_id":{"$oid":"5837a57da05283111e4d4c02"},"Last_activity":"2015-06-19 10:54:28","Creator_reputation":121,"Question_score":1,"Answer_content":"You can try nonparametric Mann-Whitney U-test, which is generally applicable for two samples of 8 and 8 elements (some handbooks recommend 20 and 20). It was described in H. B. Mann and D. R. Whitney. 1947. On a test of whether one of two random variables is stochastically larger than the other. Annals of Mathematical Statistics, 18:50-60 (openly available here). Table 1 of the paper contains critical values for samples of the smaller sizes. Minimal sample sizes with significance level  (one-tailed) are 3 and 3.","Display_name":"Andrey","Creater_id":44668,"Start_date":"2015-06-19 10:54:28","Question_id":157742}
{"_id":{"$oid":"5837a57da05283111e4d4c0f"},"Last_activity":"2016-08-16 11:56:51","Creator_reputation":197,"Question_score":2,"Answer_content":"Don't you mean Stochastic process?A stochastic process is a time sequence representing the evolution of some system represented by a variable whose change is subject to a random variation. 1- https://en.wikipedia.org/wiki/Stochastic_process .","Display_name":"Toney Shields","Creater_id":109437,"Start_date":"2016-08-16 11:56:51","Question_id":230152}
{"_id":{"$oid":"5837a57da05283111e4d4c1e"},"Last_activity":"2016-08-16 12:36:27","Creator_reputation":13,"Question_score":1,"Answer_content":"depends on what you are trying to achive. more data is not always good, because there could be structural change in the process, and if you use too much data, you will incorrectly include that in your calibration process.","Display_name":"pionpi_","Creater_id":127808,"Start_date":"2016-08-16 12:36:27","Question_id":230158}
{"_id":{"$oid":"5837a57da05283111e4d4c2b"},"Last_activity":"2016-08-16 12:27:02","Creator_reputation":156,"Question_score":14,"Answer_content":"In linear regression the Maximize Likelihood Estimation (MLE) solution for estimating  has the following closed form solution (assuming that A is a matrix with full column rank):\\hat{x}_\\text{lin}=\\underset{x}{\\text{argmin}} \\|Ax-b\\|_2^2 = (A^TA)^{-1}A^TbThis is read as \"find the  that minimizes the objective function, \". The nice thing about representing the linear regression objective function in this way is that we can keep everything in matrix notation and solve for  by hand. As Alex R. mentions, in practice we often don't consider  directly because it is computationally inefficient and  often does not meeting the full rank criteria. Instead, we turn to the Moore-Penrose pseudoinverse. The details of computationally solving for the pseudo-inverse can involve the Cholesky decomposition or the Singular Value Decomposition. Alternatively, the MLE solution for estimating the coefficients in logistic regression is:\\hat{x}_\\text{log} = \\underset{x}{\\text{argmin}} \\sum_{i=1}^{N} \\log(1+e^{-x^Ta^{(i)}}) + (1-y^{(i)})\\log(1+e^{x^T a^{(i)}})where (assuming each sample of data is stored row-wise): is a vector represents regression coefficients  is a vector represents the  sample/ row in data matrix  is a scalar in , and the  label corresponding to the  sample is the number of data samples / number of rows in data matrix .Again, this is read as \"find the  that minimizes the objective function\". If you wanted to, you could take it a step further and represent  in matrix notation as follows: \\hat{x}_\\text{log} = \\underset{x}{\\text{argmin}} \\begin{bmatrix} 1 \u0026amp; (1-y^{(1)}) \\\\ \\vdots \u0026amp; \\vdots \\\\ 1 \u0026amp; (1-y^{(N)})\\\\\\end{bmatrix} \\begin{bmatrix} \\log(1+e^{-x^Ta^{(1)}})  \u0026amp; ... \u0026amp; \\log(1+e^{-x^Ta^{(N)}}) \\\\\\log(1+e^{x^Ta^{(1)}})  \u0026amp; ... \u0026amp; \\log(1+e^{x^Ta^{(N)}})\\end{bmatrix} but you don't gain anything from doing this. Logistic regression does not have a closed form solution and does not gain the same benefits as linear regression does by representing it in matrix notation. To solve for  estimation techniques such as gradient descent and the Newton-Raphson method are used. Through using some of these techniques (i.e. Newton-Raphson),  is approximated and is represented in matrix notation (see link provided by Alex R.).","Display_name":"joceratops","Creater_id":73458,"Start_date":"2016-08-09 10:22:01","Question_id":229014}
{"_id":{"$oid":"5837a57da05283111e4d4c2c"},"Last_activity":"2016-08-10 11:19:25","Creator_reputation":17409,"Question_score":11,"Answer_content":"@joceratops answer focuses on the optimization problem of maximum likelihood for estimation. This is indeed a flexible approach that is amenable to many types of problems. For estimating most models, including linear and logistic regression models, there is another general approach that is based on the method of moments estimation.The linear regression estimator can also be formulated as the root to the estimating equation:0 = \\mathbf{X}^T(Y - \\mathbf{X}\\beta)In this regard  is seen as the value which retrieves an average residual of 0. It needn't rely on any underlying probability model to have this interpretation. It is, however, interesting to go about deriving the score equations for a normal likelihood, you will see indeed that they take exactly the form displayed above. Maximizing the likelihood of regular exponential family for a linear model (e.g. linear or logistic regression) is equivalent to obtaining solutions to their score equations.0 = \\sum_{i=1}^n S_i(\\alpha, \\beta) = \\frac{\\partial}{\\partial \\beta} \\log \\mathcal{L}( \\beta, \\alpha, X, Y) = \\mathbf{X}^T (Y - g(\\mathbf{X}\\beta))Where  has expected value . In GLM estimation,  is said to be the inverse of a link function. In normal likelihood equations,  is the identity function, and in logistic regression  is the logit function. A more general approach would be to require  which allows for model misspecification.Additionally, it is interesting to note that for regular exponential families,  which is called a mean-variance relationship. Indeed for logistic regression, the mean variance relationship is such that the mean  is related to the variance by . This suggests an interpretation of a model misspecified GLM as being one which gives a 0 average Pearson residual. This further suggests a generalization to allow non-proportional functional mean derivatives and mean-variance relationships.A generalized estimating equation approach would specify linear models in the following way:0  = \\frac{\\partial g(\\mathbf{X}\\beta)}{\\partial \\beta} \\mathbf{V}^{-1}\\left(Y - g(\\mathbf{X}\\beta)\\right)With  a matrix of variances based on the fitted value (mean) given by . This approach to estimation allows one to pick a link function and mean variance relationship as with GLMs. In logistic regression  would be the inverse logit, and  would be given by . The solutions to this estimating equation, obtained by Newton-Raphson, will yield the  obtained from logistic regression. However a somewhat broader class of models is estimable under a similar framework. For instance, the link function can be taken to be the log of the linear predictor so that the regression coefficients are relative risks and not odds ratios. Which--given the well documented pitfalls of interpreting ORs as RRs--behooves me to ask why anyone fits logistic regression models at all anymore.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-09 14:16:22","Question_id":229014}
{"_id":{"$oid":"5837a57da05283111e4d4c3b"},"Last_activity":"2015-06-09 11:44:50","Creator_reputation":3680,"Question_score":0,"Answer_content":"In the party package (or its recommended reimplementation in the partykit package) this is not directly possible. You would have to first estimate the root stump with only the desired variable and maxdepth = 1. And then you would have to make separate ctree() calls in the resulting subsamples.","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2015-06-09 11:44:50","Question_id":156097}
{"_id":{"$oid":"5837a57da05283111e4d4c49"},"Last_activity":"2016-08-16 11:53:11","Creator_reputation":12937,"Question_score":0,"Answer_content":"Citing Rob J. Hyndman \"Measuring forecast accuracy\" (2014),   Measures based on percentage errors have the disadvantage of being infinite or undefined if  for any observation in the test set, and having extreme values when any  is close to zero.Since the other accuracy measures seem OK (e.g. MASE well below 1), it might be the specifics of the data rather than a faulty model that is producing the very high MAPE.Also note that (again citing from the same source)  [a]nother problem with percentage errors \u0026lt;...\u003e is that they assume a scale based on quantity. If  is measured in dollars, or kilograms, or some other quantity, percentages make sense. On the other hand, a percentage error makes no sense when measuring the accuracy of temperature forecasts on the Fahrenheit or Celsius scales, because these are not measuring a quantity.So perhaps you could discard MAPE right at the start based on what your raw data is measuring.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-16 11:46:49","Question_id":229892}
{"_id":{"$oid":"5837a57da05283111e4d4c4a"},"Last_activity":"2016-08-15 05:39:34","Creator_reputation":14039,"Question_score":0,"Answer_content":"Daily data is often better modeled with a mixture of ARIMA and deterministic effects. Day-of-the-week , month-of-the-year , holiday effects etc. while accounting for pulses/level shifts/time trends. Look at http://www.autobox.com/cms/index.php/afs-university/intro-to-forecasting/doc_download/53-capabilities-presentation slide 47 for an example of this.","Display_name":"IrishStat","Creater_id":3382,"Start_date":"2016-08-15 05:39:34","Question_id":229892}
{"_id":{"$oid":"5837a57da05283111e4d4c57"},"Last_activity":"2016-08-16 11:36:53","Creator_reputation":31,"Question_score":3,"Answer_content":"Jackknife often refers to 2 related but different processes, both of which rely on a leave-one-out approach -- leading to this very confusion.In one context, jackknife can be used to estimate population parameters and their standards errors.  For example, to use a jackknife approach to estimate the slope and intercept of a simple regression model one would:Estimate the slope and intercept using all available data.Leave out 1 observation and estimate the slope and intercept (also known as the \"partial estimate\" of the coefficients).Calculate the difference between the \"partial estimate\" and the \"all data\" estimate of the slope and the intercept (also know as the \"pseudo value\" of the coefficients).Repeat steps 2 \u0026amp; 3 for the entire data set.Compute the mean of the pseudo values for each coefficient -- these are the jackknife estimates of the slope and interceptThe pseudo values and the jackknife estimates of the coefficients can also be used to determine the standard errors and thus confidence intervals. Typically this approach gives wider confidence intervals for the coefficients because it's a better, more conservative, measure of uncertainty. Also, this approach can be used to get a jackknife estimate of bias for the coefficients too.In the other context, jackknife is used to evaluate model performance.  In this case jackknife = leave-one-out cross validation.  Both refer to leaving one observation out of the calibration data set, recalibrating the model, and predicting the observation that was left out.  Essentially, each observation is being predicted using its \"partial estimates\" of the predictors.Here's a nice little write-up about jackknife I found online:https://www.utdallas.edu/~herve/abdi-Jackknife2010-pretty.pdf","Display_name":"jcmb","Creater_id":127895,"Start_date":"2016-08-16 11:36:53","Question_id":144064}
{"_id":{"$oid":"5837a57da05283111e4d4c58"},"Last_activity":"2015-03-30 07:31:00","Creator_reputation":748,"Question_score":4,"Answer_content":"In cross-validation you compute a statistic on the left-out sample(s). Most often, you predict the left-out sample(s) by a model built on the kept samples. In jackknifing, you compute a statistic from the kept samples only.","Display_name":"Tommy L","Creater_id":61931,"Start_date":"2015-03-30 07:31:00","Question_id":144064}
{"_id":{"$oid":"5837a57da05283111e4d4c67"},"Last_activity":"2016-08-16 11:12:10","Creator_reputation":4052,"Question_score":45,"Answer_content":"What enforces more separation than there should be is each discipline's lexicon. There are many instances where ML uses one term and Statistics uses a different term--but both refer to the same thing--fine, you would expect that, and it doesn't cause any permanent confusion (e.g., features/attributes versus expectation variables, or neural network/MLP versus projection-pursuit).What's much more troublesome is that both disciplines use the same term to refer to completely different concepts.A few examples:Kernel FunctionIn ML, kernel functions are used in classifiers (e.g., SVM) and of course in kernel machines. The term refers to a simple function (cosine, sigmoidal, rbf, polynomial) to map non-linearly separable to a new input space, so that the data is now linearly separable in this new input space. (versus using a non-linear model to begin with).In statistics, a kernel function is weighting function used in density estimation to smooth the density curve.RegressionIn ML, predictive algorithms, or implementations of those algorithms that return class labels \"classifiers\" are (sometimes) referred to as machines--e.g., support vector machine, kernel machine. The counterpart to machines are regressors, which return a score (continuous variable)--e.g., support vector regression. Rarely do the algorithms have different names based on mode--e.g., a MLP is the term used whether it returns a class label or a continuous variable.In Statistics, regression, if you are attempting to build a model based on empirical data, to predict some response variable based on one or more explanatory variables or more variables--then you are doing regression analysis. It doesn't matter whether the output is a continuous variable or a class label (e.g., logistic regression). So for instance, least-squares regression refers to a model that returns a continuous value; logistic regression on the other hand, returns a probability estimate which is then discretized to a class labels.BiasIn ML, the bias term in the algorithm is conceptually identical to the intercept term used by statisticians in regression modeling.In Statistics, bias is non-random error--i.e., some phenomenon influenced the entire data set in the same direction, which in turn means that this kind of error cannot be removed by resampling or increasing the sample size.","Display_name":"doug","Creater_id":438,"Start_date":"2010-08-09 03:12:35","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c68"},"Last_activity":"2016-04-14 08:25:12","Creator_reputation":993,"Question_score":2,"Answer_content":"I think machine learning needs to be a sub-branch under statistics, just how in my view chemistry needs to be a sub-branch under physics.I think physics-inspired view into chemistry is pretty solid (I guess). I don't think there is any chemical reaction that its equivalent is not known in physical terms. I think physics has done an amazing job by explaining everything we can see at a chemistry-level. Now the physicists challenge seems to be explaining the tiny mysteries at the quantum level, under extreme conditions that are not observable. Now back to machine learning. I think it too should be a sub-branch under statistics (just how chemistry is a sub-branch of physics).But it seems to me that, somehow, either the current state of machine learning, or statistics, is not mature enough to perfectly realize this. But in the long run, I think one must become a sub-branch of the other. I think it's ML that will to get under statistics.I personally think that \"learning\" and \"analyzing samples\" to estimate/infer functions or predictions is all essentially a question of statistics.","Display_name":"caveman","Creater_id":100507,"Start_date":"2016-04-14 08:25:12","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c69"},"Last_activity":"2016-03-21 13:33:00","Creator_reputation":1024,"Question_score":3,"Answer_content":"This question can also be extended to the so-called super-culture of data science in 2015 David Donoho paper 50 years of Data Science, where he confronts different points of view from statistics and computer science (including machine learning), for instance direct standpoints (from different persons) such that:Why Do We Need Data Science When We've Had Statistics for Centuries?Data Science is statistics.Data Science without statistics is possible, even desirable.Statistics is the least important part of data science.and assorted with historical, philosophical considerations, for instance:  It is striking how, when I review a presentation on today's data  science, in which statistics is super\u000ccially given pretty short  shrift, I can't avoid noticing that the underlying tools, examples,  and ideas which are being taught as data science were all literally  invented by someone trained in Ph.D. statistics, and in many cases the  actual software being used was developed by someone with an MA or  Ph.D. in statistics. The accumulated e\u000borts of statisticians over  centuries are just too overwhelming to be papered over completely, and  can't be hidden in the teaching, research, and  exercise of Data Science. This essay has generated many responses and contributions to the debate.","Display_name":"Laurent Duval","Creater_id":83945,"Start_date":"2016-03-21 13:33:00","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c6a"},"Last_activity":"2015-03-18 05:14:33","Creator_reputation":39301,"Question_score":12,"Answer_content":"The largest differences I've been noticing in the past year are:Machine learning experts do not spend enough time on fundamentals, and many of them do not understand optimal decision making and proper accuracy scoring rules.  They do not understand that predictive methods that make no assumptions require larger sample sizes than those that do.We statisticians spend too little time learning good programming practice and new computational languages.  We are too slow to change when it comes to computing and adopting new methods from the statistical literature.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2015-03-18 05:14:33","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c6b"},"Last_activity":"2015-03-18 05:06:25","Creator_reputation":81,"Question_score":4,"Answer_content":"There is an area of application of statistics where focus on the data generating model makes a lot of sense. In designed experiments, e.g., animal studies, clinical trials, industrial DOEs, statisticians can have a hand in what the data generating model is. ML tends not to spend much time on this very important problem as ML usually focuses on another very important problem of prediction based on “large” observational data. That is not to say that ML can’t be applied to “large” designed experiments, but it is important to acknowledge that statistics has particular expertise on “small” data problems arising from resource constrained experiments. At the end of the day I think we can all agree to use what works best to solve the problem at hand. E.g., we may have a designed experiment that produces very wide data with the goal of prediction. Statistical design principles are very useful here and ML methods could be useful to build the predictor.","Display_name":"Clark","Creater_id":13586,"Start_date":"2015-03-18 05:06:25","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c6c"},"Last_activity":"2013-12-11 23:28:14","Creator_reputation":1,"Question_score":10,"Answer_content":"The real problem is that this question is misguided. It is not machine learning vs statistics, it is machine learning against real scientific advance. If a machine learning device gives the right predictions 90% of the time but I cannot understand \"why\", what is the contribution of machine learning to science at large? Imagine if machine learning techniques were used to predict the positions of planets: there would be a lot of smug people thinking that they can accurately predict a number of things with their SVMs, but what would they really know about the problem they have in their hands? Obviously, science does not really advance by numerical predictions, it advances by means of models (mental, mathematical) who let us see far beyond than just numbers.","Display_name":"user36080","Creater_id":36080,"Start_date":"2013-12-11 23:28:14","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c6d"},"Last_activity":"2013-10-18 12:35:16","Creator_reputation":405,"Question_score":47,"Answer_content":"Bayesian: \"Hello, Machine Learner!\"Frequentist:  \"Hello, Machine Learner!\"Machine Learning: \"I hear you guys are good at stuff.  Here's some data.\"F: \"Yes, let's write down a model and then calculate the MLE.\"B: \"Hey, F, that's not what you told me yesterday!  I had some univariate data and I wanted to estimate the variance, and I calculated the MLE.  Then you pounced on me and told me to divide by  instead of by .\"F: \"Ah yes, thanks for reminding me.  I often think that I'm supposed to use the MLE for everything, but I'm interested in unbiased estimators and so on.\"ML: \"Eh, what's this philosophizing about?  Will it help me?\"F: \" OK, an estimator is a black box, you put data in and it gives you some numbers out.  We frequentists don't care about how the box was constructed, about what principles were used to design it.  For example, I don't know how to derive the  rule.\"ML: \" So, what do you care about?\"F: \"Evaluation.\"ML: \"I like the sound of that.\"F: \"A black box is a black box.  If somebody claims a particular estimator is an unbiased estimator for , then we try many values of  in turn, generate many samples from each based on some assumed model, push them through the estimator, and find the average estimated .  If we can prove that the expected estimate equals the true value, for all values, then we say it's unbiased.\"ML: \"Sounds great!  It sounds like frequentists are pragmatic people.  You judge each black box by its results.  Evaluation is key.\"F: \"Indeed!  I understand you guys take a similar approach.  Cross-validation, or something?  But that sounds messy to me.\"ML: \"Messy?\"F: \"The idea of testing your estimator on real data seems dangerous to me.  The empirical data you use might have all sorts of problems with it, and might not behave according the model we agreed upon for evaluation.\"ML: \"What?  I thought you said you'd proved some results?  That your estimator would always be unbiased, for all .\"F: \"Yes.  While your method might have worked on one dataset (the dataset with train and test data) that you used in your evaluation, I can prove that mine will always work.\"ML: \"For all datasets?\"F: \"No.\"ML: \"So my method has been cross-validated on one dataset.  You haven't test yours on any real dataset?\"F: \"That's right.\"ML: \"That puts me in the lead then!  My method is better than yours.  It predicts cancer 90% of the time.  Your 'proof' is only valid if the entire dataset behaves according to the model you assumed.\"F: \"Emm, yeah, I suppose.\"ML: \"And that interval has 95% coverage.  But I shouldn't be surprised if it only contains the correct value of  20% of the time?\"F: \"That's right.  Unless the data is truly i.i.d Normal (or whatever), my proof is useless.\"ML: \"So my evaluation is more trustworthy and comprehensive? It only works on the datasets I've tried so far, but at least they're real datasets, warts and all.  There you were, trying to claim you were more 'conservative' and 'thorough' and that you were interested in model-checking and stuff.\"B: (interjects) \"Hey guys, Sorry to interrupt.  I'd love to step in and balance things up, perhaps demonstrating some other issues, but I really love watching my frequentist colleague squirm.\"F: \"Woah!\"ML: \"OK, children.  It was all about evaluation.  An estimator is a black box.  Data goes in, data comes out.  We approve, or disapprove, of an estimator based on how it performs under evaluation.  We don't care about the 'recipe' or 'design principles' that are used.\"F: \"Yes.  But we have very different ideas about which evaluations are important.  ML will do train-and-test on real data.  Whereas I will do an evaluation that is more general (because it involves a broadly-applicable proof) and also more limited (because I don't know if your dataset is actually drawn from the modelling assumptions I use while designing my evaluation.)\"ML: \"What evaluation do you use, B?\"F: (interjects) \"Hey. Don't make me laugh. He doesn't evaluate anything.  He just uses his subjective beliefs and runs with it.  Or something.\"B: \"That's the common interpretation.  But it's also possible to define Bayesianism by the evaluations preferred.  Then we can use the idea that none of us care what's in the black box, we care only about different ways to evaluate.\"B continues:   \"Classic example:  Medical test.  The result of the blood test is either Positive or Negative.  A frequentist will be interested in, of the Healthy people, what proportion get a Negative result.  And similarly, what proportion of Sick people will get a Positive. The frequentist will calculate these for each blood testing method that's under consideration and then recommend that we use the test that got the best pair of scores.\"F: \"Exactly. What more could you want?\"B: \"What about those individuals that got a Positive test result?  They will want to know 'of those that get a Positive result, how many will get Sick?' and 'of those that get a Negative result, how many are Healthy?' \"ML: \"Ah yes, that seems like a better pair of questions to ask.\"F: \"HERESY!\"B: \"Here we go again.  He doesn't like where this is going.\"ML: \"This is about 'priors', isn't it?\"F: \"EVIL\".B: \"Anyway, yes, you're right ML.  In order to calculate the proportion of Positive-result people that are Sick you must do one of two things.  One option is to run the tests on lots of people and just observe the relevant proportions. How many of those people go on to die of the disease, for example.\"ML: \"That sounds like what I do.  Use train-and-test.\"B: \"But you can calculate these numbers in advance, if you are willing to make an assumption about the rate of Sickness in the population.  The frequentist also makes his calcuations in advance, but without using this population-level Sickness rate.\"F:  \"MORE UNFOUNDED ASSUMPTIONS.\"B: \"Oh shut up.  Earlier, you were found out.  ML discovered that you are just as fond of unfounded assumptions as anyone.  Your 'proven' coverage probabilities won't stack up in the real world unless all your assumptions stand up.  Why is my prior assumption so diffent?  You call me crazy, yet you pretend your assumptions are the work of a conservative, solid, assumption-free analysis.\"B (continues): \"Anyway, ML, as I was saying. Bayesians like a different kind of evaluation.  We are more interested in conditioning on the observed data, and calculating the accuracy of our estimator accordingly.  We cannot perform this evaluation without using a prior.  But the interesting thing is that, once we decide on this form of evaluation, and once we choose our prior, we have an automatic 'recipe' to create an appropriate estimator.  The frequentist has no such recipe.  If he wants an unbiased estimator for a complex model, he doesn't have any automated way to build a suitable estimator.\"ML: \"And you do? You can automatically build an estimator?\"B: \"Yes.  I don't have an automatic way to create an unbiased estimator, because I think bias is a bad way to evaluate an estimator.  But given the conditional-on-data estimation that I like, and the prior, I can connect the prior and the likelihood to give me the estimator.\"ML: \"So anyway, let's recap.  We all have different ways to evaluate our methods, and we'll probably never agree on which methods are best.\"B: \"Well, that's not fair.  We could mix and match them.  If any of us have good labelled training data, we should probably test against it.  And generally we all should test as many assumptions as we can.  And some 'frequentist' proofs might be fun too, predicting the performance under some presumed model of data generation.\"F: \"Yeah guys.  Let's be pragmatic about evaluation.  And actually, I'll stop obsessing over infinite-sample properties.  I've been asking the scientists to give me an infinite sample, but they still haven't done so.  It's time for me to focus again on finite samples.\"ML: \"So, we just have one last question.  We've argued a lot about how to evaluate our methods, but how do we create our methods.\"B: \"Ah. As I was getting at earlier, we Bayesians have the more powerful general method.  It might be complicated, but we can always write some sort of algorithm (maybe a naive form of MCMC) that will sample from our posterior.\"F(interjects): \"But it might have bias.\"B: \"So might your methods.  Need I remind you that the MLE is often biased?  Sometimes, you have great difficulty finding unbiased estimators, and even when you do you have a stupid estimator (for some really complex model) that will say the variance is negative.  And you call that unbiased.  Unbiased, yes.  But useful, no!\"ML: \"OK guys. You're ranting again.  Let me ask you a question, F.  Have you ever compared the bias of your method with the bias of B's method, when you've both worked on the same problem?\"F: \"Yes.  In fact, I hate to admit it, but B's approach sometimes has lower bias and MSE than my estimator!\"ML: \"The lesson here is that, while we disagree a little on evaluation, none of us has a monopoly on how to create estimator that have properties we want.\"B: \"Yes, we should read each other's work a bit more.  We can give each other inspiration for estimators.  We might find that other's estimators work great, out-of-the-box, on our own problems.\"F: \"And I should stop obsessing about bias.  An unbiased estimator might have ridiculous variance.  I suppose all of us have to 'take responsibility' for the choices we make in how we evaluate and the properties we wish to see in our estimators.  We can't hind behind a philosophy.  Try all the evaluations you can.  And I will keep sneaking a look at the Bayesian literature to get new ideas for estimators!\"B:\"In fact, a lot of people don't really know what their own philosophy is.  I'm not even sure myself.  If I use a Bayesian recipe, and then proof some nice theoretical result, doesn't that mean I'm a frequentist?  A frequentist cares about above proofs about performance, he doesn't care about recipes.  And if I do some train-and-test instead (or as well), does that mean I'm a machine-learner?\"ML: \"It seems we're all pretty similar then.\"","Display_name":"Aaron McDaid","Creater_id":7817,"Start_date":"2013-10-18 12:17:41","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c6e"},"Last_activity":"2013-09-16 13:21:23","Creator_reputation":1,"Question_score":-4,"Answer_content":"As as Computer Scientist, I am always intrigued when looking to statistical approaches. To me many times it looks like the statistical models used in the statistical analysis are way too complex for the data in many situations!For example there is a strong link between data compression and statistics. Basically one needs a good statistical model which is able to predict the data well and this brings a very good compression of the data. In computer science when compressing the data always the complexity of the statistical model and the accuracy of the prediction are very important. Nobody wants to get have EVER a data file (containing sound data or image data or video data) becoming bigger after the compression!I find that there are more dynamic things in computer science regarding statistics, like for example Minimum Description Length and Normalized Maximum Likelihood.","Display_name":"cerb","Creater_id":30363,"Start_date":"2013-09-16 12:53:06","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c6f"},"Last_activity":"2013-06-06 23:41:05","Creator_reputation":407,"Question_score":6,"Answer_content":"Clearly, the two fields clearly face similar but different problems, in similar but not identical ways with analogous but not identical concepts, and work in different departments, journals and conferences.When I read Cressie and Read's Power Divergence Statistic  it all snapped into place for me. Their formula generalizes commonly used test statistics into one that varies by one exponent, lambda.  There are two special cases, lambda=0 and lambda=1.   Computer Science and Statistics fit along a continuum (that presumably could include other points).    At one value of lambda, you get statistics commonly cited in Statistics circles, and at the other you get statistics commonly cited in Comp Sci circles.  StatisticsLambda = 1 Sums of squares appear a lotVariance as a measure of variabilityCovariance as a measure of associationChi-squared statistic as a measure of model fitComputer science:Lambda = 0Sums of logs appear a lotEntropy as a measure of variabilityMutual information as a measure of associationG-squared statistic as a measure of model fit","Display_name":"prototype","Creater_id":8797,"Start_date":"2012-02-23 13:59:04","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c70"},"Last_activity":"2013-06-06 23:37:55","Creator_reputation":3083,"Question_score":121,"Answer_content":"I think the answer to your first question is simply in the affirmative. Take any issue of Statistical Science, JASA, Annals of Statistics of the past 10 years and you'll find papers on boosting, SVM, and neural networks, although this area is less active now. Statisticians have appropriated the work of Valiant and Vapnik, but on the other side, computer scientists have absorbed the work of Donoho and Talagrand. I don't think there is much difference in scope and methods any more. I have never bought Breiman's argument that CS people were only interested in minimizing loss using whatever works. That view was heavily influenced by his participation in Neural Networks conferences and his consulting work; but PAC, SVMs, Boosting have all solid foundations. And today, unlike in 2001, Statistics is more concerned with finite-sample properties, algorithms and massive datasets.But I think that there are still three important differences that are not going away soon. Methodological Statistics papers are still overwhelmingly formal and deductive, whereas Machine Learning researchers are more tolerant of new approaches even if they don't come with a proof attached;The ML community primarily shares new results and publications in conferences and related proceedings, whereas statisticians use journal papers. This slows down progress in Statistics and identification of star researchers. John Langford a nice post on the subject from a while back;Statistics still covers areas that are (for now) of little concern to ML, such as survey design, sampling, industrial Statistics etc.","Display_name":"gappy","Creater_id":30,"Start_date":"2010-07-24 20:29:41","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c71"},"Last_activity":"2013-06-06 19:41:57","Creator_reputation":181,"Question_score":10,"Answer_content":"Statistical learning (AKA Machine Learning) has its origins in the quest to create software by \"learning from examples\".  There are many tasks that we would like computers to do (e.g., computer vision, speech recognition, robot control) that are difficult to program but for which it is easy to provide training examples.  The machine learning/statistical learning research community developed algorithms to learn functions from these examples. The loss function was typically related to the performance task (vision, speech recognition). And of course we had no reason to believe there was any simple \"model\" underlying these tasks (because otherwise we would have coded up that simple program ourselves). Hence, the whole idea of doing statistical inference didn't make any sense. The goal is predictive accuracy and nothing else.  Over time, various forces started driving machine learning people to learn more about statistics. One was the need to incorporate background knowledge and other constraints on the learning process. This led people to consider generative probabilistic models, because these make it easy to incorporate prior knowledge through the structure of the model and priors on model parameters and structure. This led the field to discover the rich statistical literature in this area. Another force was the discovery of the phenomenon of overfitting. This led the ML community to learn about cross-validation and regularization and again we discovered the rich statistical literature on the subject. Nonetheless, the focus of most machine learning work is to create a system that exhibits certain performance rather than the make inferences about an unknown process.  This is the fundamental difference between ML and statistics.","Display_name":"Tom Dietterich","Creater_id":26598,"Start_date":"2013-06-06 19:41:57","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c72"},"Last_activity":"2012-05-05 09:59:11","Creator_reputation":21588,"Question_score":3,"Answer_content":"You run a fancy computer algorithm once -- and you get a CS conference presentation/statistics paper (wow, what a fast convergence!). You commercialize it and run it 1 million times -- and you go broke (ouch, why am I getting useless and irreproducible results all the time???) unless you know how to employ probability and statistics to generalize the properties of the algorithm.","Display_name":"StasK","Creater_id":5739,"Start_date":"2012-05-05 09:59:11","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c73"},"Last_activity":"2012-05-04 14:08:26","Creator_reputation":25897,"Question_score":16,"Answer_content":"I have spoken on this at a different forum the ASA Statistical Consulting eGroup.  My response was more specifically to data mining but the two go hand in hand.  We statisticians have snubbed our noses at data miners, computer scientists, and engineers. It is wrong. I think part of the reason it happens is because we see some people in those fields ignoring the stochastic nature of their problem.  Some statisticians call data mining  data snooping or data fishing.  Some people do abuse and misuse the methods but statisticians have fallen behind in data mining and machine learning because we paint them with a broad brush.  Some of the big statistical results have come from outside the field of statistics.  Boosting is one important example.  But statisticians like Brieman, Friedman, Hastie, Tibshirani, Efron, Gelman and others got it and their leadership has brought statisticians into the analysis of microarrays and other large scale inference problems. So while the cultures may never mesh there is now more cooperation and collabortion between the computer scientists, engineers and statisticians.","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-05-04 14:08:26","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c74"},"Last_activity":"2011-02-14 15:09:41","Creator_reputation":3016,"Question_score":96,"Answer_content":"The biggest difference I see between the communities is that statistics emphasizes inference, whereas machine learning emphasized prediction.  When you do statistics, you want to infer the process by which data you have was generated.  When you do machine learning, you want to know how you can predict what future data will look like w.r.t. some variable.  Of course the two overlap.  Knowing how the data was generated will give you some hints about what a good predictor would be, for example.  However, one example of the difference is that machine learning has dealt with the p \u003e\u003e n problem (more features/variables than training samples) since its infancy, whereas statistics is just starting to get serious about this problem.  Why?  Because you can still make good predictions when p \u003e\u003e n, but you can't make very good inferences about what variables are actually important and why.","Display_name":"dsimcha","Creater_id":1347,"Start_date":"2011-02-14 15:09:41","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c75"},"Last_activity":"2010-08-09 06:56:39","Creator_reputation":2853,"Question_score":11,"Answer_content":"Ideally one should have a thorough knowledge of both statsitics and machine learning before attempting to answer his question. I am very much a neophyte to ML, so forgive me if wat I say is naive.I have limited experience in SVMs and regression trees. What strikes me as lacking in ML from a stats point of view is a well developed concept of inference. Inference in ML seems to boil down almost exclusively to the predictice accuracy, as measured by (for example) mean classification error (MCE), or balanced error rate (BER) or similar. ML is in the very good habit of dividing data randomly (usually 2:1) into a training set and a test set. Models are fit using the training set and performance (MCE, BER etc) is assessed using the test set. This is an excellent practice and is only slowly making its way into mainstream statistics. ML also makes heavy use of resampling methods (especially cross-validation), whose origins appear to be in statistics.However, ML seems to lack a fully developed concept of inference - beyond predictive accuracy. This has two results.1) There does not seem to be an appreciation that any prediction (parameter estimation etc.) is subject to a random error and perhaps systemmatics error (bias). Statisticians will accept that this is an inevitable part of prediction and will try and estimate the error. Statistical techniques will try and find an estimate that has minimum bias and random error. Their techniques are usually driven by a model of the data process, but not always (eg. Bootstrap).2) There does not seem to be a deep understanding in ML of the limits of applying a model to new data to a new sample from the same population (in spite of what I said earlier about the training-test data set approach). Various statistical techniques, among them cross validation and penalty terms applied to likelihood-based methods, guide statisticians in the trade-off between parsimony and model complexity. Such guidelines in ML seem much more ad hoc.I've seen several papers in ML where cross validation is used to optimise a fitting of many models on a training dataset - producing better and better fit as the model complexity increases. There appears little appreciation that the tiny gains in accuracy are not worth the extra complexity and this naturally leads to over-fitting. Then all these optimised models are applied to the test set as a check on predictive performance and to prevent overfitting. Two things have been forgotten (above). The predictive performance will have a stochastic component. Secondly multiple tests against a test set will again result in over-fitting. The \"best\" model will be choisen by the ML practitioner without a full appreciation he/she has cherry picked from one realisation of many possible outomes of this experiment. The best of several tested models will almost certainly not reflect the true performance on new data.Any my 2 cents worth. We have much to learn from each other.","Display_name":"Thylacoleo","Creater_id":521,"Start_date":"2010-08-09 06:51:29","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c76"},"Last_activity":"2010-07-21 06:48:15","Creator_reputation":4907,"Question_score":9,"Answer_content":"I don't really know what the conceptual/historical difference between machine learning and statistic is but I am sure it is not that obvious... and I am not really interest in knowing if I am a machine learner or a statistician, I think 10 years after Breiman's paper, lots of people are both...Anyway, I found  interesting the question about predictive accuracy of models. We have to remember that it is not always possible to measure the accuracy of a model and more precisely we are most often implicitly making some modeling when measuring errors.For Example, mean absolute error in time series forecast is a mean over time and it measures the performance of a procedure to forecast the median with the assumption that performance is, in some sense, stationary and shows some ergodic property. If (for some reason) you need to forecast the mean temperature on earth for the next 50 years and if your modeling performs well for the last 50 years... it does not means that... More generally, (if I remember, it is called no free lunch) you can't do anything without modeling... In addition, I think statistic is trying to find an answer to the question : \"is something significant or not \", this is a very important question in science and can't be answered through a learning process. To state John Tukey (was he a statistician ?) :  The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data Hope this helps ! ","Display_name":"robin girard","Creater_id":223,"Start_date":"2010-07-21 06:48:15","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c77"},"Last_activity":"2010-07-21 05:43:00","Creator_reputation":351,"Question_score":19,"Answer_content":"I disagree with this question as it suggests that machine learning and statistics are different or conflicting sciences.... when the opposite is true!machine learning makes extensive use of statistics... a quick survey of any Machine learning or data mining software package will reveal Clustering techniques such as k-means also found in statistics.... will also show dimension reduction techniques such as Principal components analysis also a statistical technique... even logistic regression yet another.In my view the main difference is that traditionally statistics was used to proove a pre conceived theory and usually the analysis was design around that principal theory. Where with data mining or machine learning the opposite approach is usually the norm in that we have the outcome we just want to find a way to predict it rather than ask the question or form the theory is this the outcome!","Display_name":"Mojo","Creater_id":256,"Start_date":"2010-07-21 05:43:00","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c78"},"Last_activity":"2010-07-19 12:51:34","Creator_reputation":17883,"Question_score":45,"Answer_content":"In such a discussion, I always recall the famous Ken Thompson quote   When in doubt, use brute force.In this case, machine learning is a salvation when the assumptions are hard to catch; or at least it is much better than guessing them wrong. ","Display_name":"mbq","Creater_id":88,"Start_date":"2010-07-19 12:51:34","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c79"},"Last_activity":"2010-07-19 12:18:56","Creator_reputation":376,"Question_score":18,"Answer_content":"Machine Learning seems to have its basis in the pragmatic - a Practical observation or simulation of reality.  Even within statistics, mindless \"checking of models and assumptions\" can lead to discarding methods that are useful.For example, years ago, the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome.  Technically, that's a bad approach, but practically, it worked.","Display_name":"Jay Stevens","Creater_id":23,"Start_date":"2010-07-19 12:18:56","Question_id":6}
{"_id":{"$oid":"5837a57da05283111e4d4c8c"},"Last_activity":"2016-08-16 10:22:15","Creator_reputation":123,"Question_score":1,"Answer_content":"You can rewrite a likelihood-maximization problem as a loss-minimization problem by defining the loss as the negative log likelihood. If the likelihood is a product of independent probabilities or probability densities, the loss will be a sum of independent terms, which can be computed efficiently. Furthermore, if the stochastic variables are normally distributed, the corresponding loss-minimization problem will be a least squares problem.If it is possible to create a loss-minimization problem by rewriting a likelihood-maximization, this should be to prefer to creating a loss-minimization problem from scratch, since it will give rise to a loss-minimization problem that is (hopefully) more theoretically founded and less ad hoc. For example, weights, such as in weighted least squares, which you usually have to guesstimate values for, will simply emerge from the process of rewriting the original likelihood-maximization problem and already have (hopefully) optimal values.","Display_name":"HelloGoodbye","Creater_id":38055,"Start_date":"2016-08-16 10:22:15","Question_id":886}
{"_id":{"$oid":"5837a57da05283111e4d4c8d"},"Last_activity":"2015-08-09 19:13:37","Creator_reputation":2459,"Question_score":11,"Answer_content":"If statistics is all about maximizing likelihood, then machine learning is all about minimizing loss. Since you don't know the loss you will incur on future data, you minimize an approximation, ie empirical loss.For instance, if you have a prediction task and are evaluated by the number of misclassifications, you could train parameters so that resulting model produces the smallest number of misclassifications on the training data. \"Number of misclassifications\" (ie, 0-1 loss) is a hard loss function to work with because it's not differentiable, so you approximate it with a smooth \"surrogate\". For instance, log loss is an upper bound on 0-1 loss, so you could minimize that instead, and this will turn out to be the same as maximizing conditional likelihood of the data. With parametric model this approach becomes equivalent to logistic regression.In a structured modeling task, and log-loss approximation of 0-1 loss, you get something different from maximum conditional likelihood, you will instead maximize product of (conditional) marginal likelihoods.To get better approximation of loss, people noticed that training model to minimize loss and using that loss as an estimate of future loss is an overly optimistic estimate. So for more accurate (true future loss) minimization they add a bias correction term to empirical loss and minimize that, this is known as structured risk minimization.In practice, figuring out the right bias correction term may be too hard, so you add an expression \"in the spirit\" of the bias correction term, for instance, sum of squares of parameters. In the end, almost all parametric machine learning supervised classification approaches end up training the model to minimize the followingwhere  is your model parametrized by vector ,  is taken over all datapoints ,  is some computationally nice approximation of your true loss and  is some bias-correction/regularization termFor instance if you , , a typical approach would be to let , , , and choose  by cross validation","Display_name":"Yaroslav Bulatov","Creater_id":511,"Start_date":"2010-07-28 11:25:00","Question_id":886}
{"_id":{"$oid":"5837a57da05283111e4d4c8e"},"Last_activity":"2010-08-04 02:07:44","Creator_reputation":2853,"Question_score":7,"Answer_content":"I can't post a comment (the appropriate place for this comment) as I don't have enough reputation, but the answer accepted as the best answer by the question owner misses the point.\"If statistics is all about maximizing likelihood, then machine learning is all about minimizing loss.\"The likelihood is a loss function. Maximising likelihood is the same as minimising a loss function: the deviance, which is just -2 times the log-likelihood function. Similarly finding a least squares solution is about minimising the loss function describing the residual sum of squares. Both ML and stats use algorithms to optimise the fit of some function (in the broadest terms) to data. Optimisation necessarily involves minimising some loss function.","Display_name":"Thylacoleo","Creater_id":521,"Start_date":"2010-08-04 02:07:44","Question_id":886}
{"_id":{"$oid":"5837a57da05283111e4d4c8f"},"Last_activity":"2010-08-01 22:16:40","Creator_reputation":3083,"Question_score":14,"Answer_content":"I will give an itemized answer. Can provide more citations on demand, although this is not really controversial.Statistics is not all aboutmaximizing (log)-likelihood. That'sanathema to principled bayesians whojust update their posteriors orpropagate their beliefs through anappropriate model.A lot of statistics is about lossminimization. And so is a lot ofMachine Learning. Empirical lossminimization has a different meaningin ML. For a clear, narrative view,check out Vapnik's \"The nature ofstatistical learning\"Machine Learning is not all aboutloss minimization. First, becausethere are a lot of bayesians in ML;second, because a number ofapplications in ML have to do withtemporal learning and approximate DP.Sure, there is an objective function,but it has a very different meaningthan in \"statistical\" learning.I don't think there is a gap between the fields, just many different approaches, all overlapping to some degree. I don't feel the need to make them into systematic disciplines with well-defined differences and similarities, and given the speed at which they evolve, I think it's a doomed enterprise anyway.","Display_name":"gappy","Creater_id":30,"Start_date":"2010-08-01 22:16:40","Question_id":886}
{"_id":{"$oid":"5837a57da05283111e4d4c90"},"Last_activity":"2010-07-28 10:28:47","Creator_reputation":9266,"Question_score":2,"Answer_content":"I don't think there is a fundamental idea around parameter estimation in Machine Learning.  The ML crowd will happily maximize the likelihood or the posterior, as long as the algorithms are efficient and predict \"accurately\".  The focus is on computation, and results from statistics are widely used.If you're looking for fundamental ideas in general, then in computational learning theory, PAC is central; in statistical learning theory, structural risk miniminization; and there are other areas (for example, see the Prediction Science post by John Langford).On bridging statistics/ML, the divide seems exagerrated.  I liked gappy's answer to the \"Two Cultures\" question.","Display_name":"ars","Creater_id":251,"Start_date":"2010-07-28 10:28:47","Question_id":886}
{"_id":{"$oid":"5837a57da05283111e4d4c91"},"Last_activity":"2010-07-28 08:29:33","Creator_reputation":17883,"Question_score":3,"Answer_content":"There is a trivial answer -- there is no parameter estimation in machine learning! We don't assume that our models are equivalent to some hidden background models; we treat both reality and the model as black boxes and we try to shake the model box (train in official terminology) so that its output will be similar to that of the reality box.The concept of not only likelihood but the whole model selection based on the training data is replaced by optimizing the accuracy (whatever defined; in principle the goodness in desired use) on the unseen data; this allows to optimize both precision and recall in a coupled manner. This leads to the concept of an ability to generalize, which is achieved in different ways depending on the learner type.The answer to the question two depends highly on definitions; still I think that the nonparametric statistics is something that connects the two. ","Display_name":"mbq","Creater_id":88,"Start_date":"2010-07-28 08:29:33","Question_id":886}
{"_id":{"$oid":"5837a57da05283111e4d4ca0"},"Last_activity":"2016-08-16 09:54:19","Creator_reputation":152603,"Question_score":2,"Answer_content":"In both cases 12 appears when approximating the distribution of the test statistic with a normal and chi-square respectively because the statistic must first be written in a standardized form. With continuous data, the ranks from  to  are used, and the variance of a randomly chosen value from  is . [Consider that the expectation of a randomly chosen rank from  to  is  and the expected value of the square of a randomly chosen rank is ; the variance of a randomly chosen rank is therefore . Similarly, the covariance of two randomly selected values (chosen without replacement) from  is .]As a result the variance of either the sum or the average of  randomly chosen ranks from  will have  in it.There's no \"inversion\" of the 12 differently in either formula - in both cases there's a denominator term involving a function of (something/12). If you divide by something/12 that's the same as multiplying by 12/something. So in the Kruskal-Wallis we see it simplified to be written that \"12/something\" way. In the Wilcoxon rank sum test when you're using the normal approximation you divide by the square root of the variance, so there's a (something/12) in the denominator; it could as easily be written as (12/something).For similar reasons (at heart, because an expected squared rank involves ), either 12 or 6 appears in formulas related to many other rank-based statistics.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-16 09:38:09","Question_id":230089}
{"_id":{"$oid":"5837a57da05283111e4d4cb1"},"Last_activity":"2016-08-16 09:12:07","Creator_reputation":90,"Question_score":2,"Answer_content":"From what I have understood, SVM, Linear regression and Naive Bayes classifier performs best on simple pattern complexity (e.g. \u0026lt;10 classes). So it's only natural that your accuracy starts dropping when you add more and more categories (here the materials).If you want to achieve top level accuracy (\u003e0.9) you might want to implement a neural networks and in your case, since you are treating image data, I suggest looking into CNN.","Display_name":"LoulouChameau","Creater_id":127872,"Start_date":"2016-08-16 09:12:07","Question_id":230127}
{"_id":{"$oid":"5837a57da05283111e4d4cbe"},"Last_activity":"2016-08-16 09:03:32","Creator_reputation":2542,"Question_score":0,"Answer_content":"Perhaps too obvious, but you could use the standard deviation of the predictions, or a confidence interval if you have enough predictions.","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-08-16 09:03:32","Question_id":230103}
{"_id":{"$oid":"5837a57da05283111e4d4ccf"},"Last_activity":"2016-08-16 08:27:17","Creator_reputation":755,"Question_score":0,"Answer_content":"Your assumptions quickly leads to contradictions except for certain values of  and .   Keeping in mind that the joint probability mass function and the conditional distributions are proportional, we can derive an expression for  in terms of  in two ways.  From the assumed conditional Poisson distributions it follows that\\begin{equation}\\frac{p(0,x_2)}{p(0,0)} = \\frac{p_{X_2|X_1=0}(x_2)}{p_{X_2|X_1=0}(0)} = \\frac{e^{-\\lambda_2}\\lambda_2^{x_2}/x_2!}{e^{-\\lambda_2}} = \\lambda_2^{x_2}/x_2!.\\end{equation} Similarly,\\begin{equation}\\frac{p(x_1,x_2)}{p(0,x_2)} = \\frac{p_{X_1|X_2=x_2}(x_1)}{p_{X_1|X_2=x_2}(0)} = \\frac{e^{-(\\lambda_1+a x_2)}(\\lambda_1+a x_2)^{x_1}/x_1!}{e^{-(\\lambda_1+a x_2)}} = (\\lambda_1+a x_2)^{x_1}/x_1!\\end{equation} Hence,\\begin{equation}p(x_1,x_2) = \\frac{(\\lambda_1+a x_2)^{x_1}\\lambda_2^{x_2}}{x_1!x_2!}p(0,0).\\end{equation}Doing the same argument but going via  leads to\\begin{equation}p(x_1,x_2) = \\frac{\\lambda_1^{x_1}(\\lambda_2+b x_1)^{x_2}}{x_1!x_2!}p(0,0).\\end{equation}The two last equations can both be true only if\\begin{equation}(\\lambda_1+a x_2)^{x_1}\\lambda_2^{x_2} = \\lambda_1^{x_1}(\\lambda_2+b x_1)^{x_2}\\end{equation}for all  which is only possible if , that is, if  and  are independent.  Otherwise, the assumption that the conditional distributions are Poisson lead to a contradiction and are thus inconsistent.","Display_name":"Jarle Tufto","Creater_id":77222,"Start_date":"2016-08-16 03:54:53","Question_id":229901}
{"_id":{"$oid":"5837a57da05283111e4d4cde"},"Last_activity":"2015-07-02 08:14:57","Creator_reputation":103,"Question_score":0,"Answer_content":"Try features in the frequency domain (fft related features), but also general statistics (mean, std, max, min,...).An algorithm that works generally well when you don't really know which features are most significant and you have quite a few of those, is Random Forests. They're easy to train and available in most ML libraries.This approach is commonly used for activity recognition based on IMU sensor data (accelerometer, gyroscope,...). ","Display_name":"Joren Van Severen","Creater_id":41911,"Start_date":"2015-07-02 08:14:57","Question_id":159441}
{"_id":{"$oid":"5837a57da05283111e4d4ced"},"Last_activity":"2014-04-23 00:44:37","Creator_reputation":1457,"Question_score":2,"Answer_content":"Your goal of obtaining a combined effect estimate is the central problem of meta-analysis, as you probably know. Before going too far, it may help you to have a textbook such as this one at hand to use as a reference and to give you some illustrative examples. Regarding your question:As you correctly state later, simply averaging the studies will not give appropriate weight to more accurate studies. So scratch this approach.This is closer to the typically taken approach in meta-analysis (good intuition). Often a weighted average of the observed effect sizes is calculated where the weights correspond to the inverse standard deviations of the effect size estimates from your studies. This is also the approach used in a fixed-effects regression that meta-analyses often include.It is not entirely clear what you mean by \"reported mean weights\" but I suppose it is the observed effect sizes from your studies. As mentioned in the previous bullet, it is indeed common to perform a weighted fixed effects regression, using the inverse standard deviation or variance as weight. Below, I will point you to some options.A very useful R package for performing meta-analysis is metafor. This will provide you with many tools. Of particular interest might be the following.If you have the observed effect sizes and their variances, you can use the function rma.uni with method = \"FE\" to perform a fixed effects meta-analysis.A fixed effect analysis is not appropriate if there is heterogeneity in the studies. Use the Q-test to assess this. If there is heterogeneity, you should consider using a random effects model if you have enough studies. Set method = \"REML\" in rma.uni to perform one. You may also want to examine your data for publication bias. Once you have a model from rma.uni you can create a funnel plot to see if you might have this problem (funnel plots do not prove publication bias is present, but indicate visually that it might be).You can also use your output from rma.uni to create a forest plot, which may be helpful in visualizing your studies.","Display_name":"Deathkill14","Creater_id":17672,"Start_date":"2014-04-23 00:44:37","Question_id":94791}
{"_id":{"$oid":"5837a57da05283111e4d4cf8"},"Last_activity":"2016-08-16 08:02:37","Creator_reputation":1326,"Question_score":2,"Answer_content":"SIR uses two ideas. The first idea is importance sampling. The main idea is that you draw from one probability distribution (in your case, it's the uniform), in order to get information about another. You do this by drawing from one distribution, then weighting the samples. Generally you are trying to get information about a tricky distribution, but since this is a tutorial, you're trying to get information about a normal distribution with samples from a uniform.Say you sample . In your code these are called draw1. Then you can weight these with the unnormalized weights . You can use these particles/weighted-samples to approximate things like expectations:E_{\\text{normal}}[h(X)] \\approx \\sum_{i=1}^N \\tilde{w}_i h(x_i),where  are the normalized weights.However these samples (without the weights) are not distributed according to the normal distribution. If you want samples that are distributed normally, you need to resample from your weighted samples. Samples with higher weights are more likely to be picked. But at the end, all resampled things will have equal weight, as you are sampling with replacement.So say you draw . Your code calls these draws1. These are distributed approximately normally. You can verify this empirically with a command like hist(draws1); it will probably look like a bell curve. Now, you may have drawn duplicates here. Even though you might have two s, they all are treated equal. Every resampled bit has equal weight. It should be  for each of them. I don't know for sure off the top of my head why the code writes length(unique(draws1))/m. It's probably something akin to effective sample size, or something like that. This also has to do with Matthew Gunn's answer. If you have a large number of unique samples, then you didn't have much of a degeneracy problem (few samples with high probability). The higher the better. ","Display_name":"Taylor","Creater_id":8336,"Start_date":"2016-08-09 15:32:46","Question_id":229036}
{"_id":{"$oid":"5837a57da05283111e4d4d07"},"Last_activity":"2016-02-08 07:29:14","Creator_reputation":3619,"Question_score":2,"Answer_content":"I think you need to use the metafor package and look at the function rma.mv and its documentation. It would also be worth looking at his website which has many examples. Perhaps you would like to try and then ask again? The author does post on Cross Validated but it may also be worth your while trying to post on the R-help mailing list. Sorry I cannot be too specific here but this is at the limits of my expertise.There seems to have been a pause here so I will try to expand my answer with the benefit of the extra information which you have provided. I am going to suggest how to do this using metafor and the rma.mv function.I assume you have all your variables in a data.frame and I have given them what I hope are obvious names below.You start by specifying fit \u0026lt;- rma.mv(yi = outcome, V = se^2,The you need to specify the random effects, which I think from your description will be random = ~ responsetype | study You need to make sure that responsetype is a factor or character variable.You now need to specify your moderator variables with mods = ~ beaklength + responsetype This suppose that there is enough overlap between studies in the response types other wise you can only use beaklength here.I would recommend setting slab = paste(stduy, responsetype) to get good labels in your forest plot. I would also strongly recommend using the profile function on the object to check the fit. With this sort of model it is sometimes the case that you do not have enough information in your dataset to identify the parameters. You may, as a biologist, be interested in@article{nakagawa12,   author = {Nakagawa, S and Santos, E S A},   title = {Methodological issues and advances in biological meta--analysis},   journal = {Evolutionary Ecology},   year = {2012},   volume = {26},   pages = {1253--1274},   keywords = {meta-analysis, general}}which gives some of the theory. I think I downloaded my copy from one of the author's site in case you do not have acces to that journal.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-02-03 14:37:21","Question_id":193853}
{"_id":{"$oid":"5837a57da05283111e4d4d08"},"Last_activity":"2016-02-03 17:37:45","Creator_reputation":1340,"Question_score":1,"Answer_content":"I'll try to address a few different components of your questionYou seem to be under the impression that you want to meta-analyze means of some bird-related variable, but after reading...  I want to look at if, across birds, these response measures increase along with another variable that I have for each birdit instead appears as though what you're really interested in meta-analyzing are the correlations between that bird-related variable, and some other variable that you have identified (potentially looking at differences across birds). Meta-analyzing correlations is certainly more typical than meta-analyzing means, though the latter is certainly possible--you can meta-analyze virtually any statistic with a corresponding variance or standard error. If it is, in fact, the case that you want to meta-analyze correlations between variables, then recording the means and standard errors of the one variable within each study won't help much. Instead, you will want to collect the correlations between the variables you are interested in, as well as the size of the sample for each correlation, as you'll use sample size to calculate the standard error and/or variance of each correlation. Once you have this information, it appears as though you'll run into another issue: dependency of effect sizes. Meta-analysis, in many ways, is just a fancy weighted regression, and so many of the same assumptions apply--including assuming that all observations are independent. As you have indicated that...  Some studies use 10 response measures while others use one..it seems likely that you will have some studies contributing multiple correlations, and therefore will be violating this assumption. Though it sounds scary (especially if you haven't don't meta-analysis before), you can use a method of 3-level meta-analysis (using multilevel structural equation models) to account for this dependency; this is easily accomplished using Cheung's metaSEM package for R (it's syntax is very similar to metafor and other meta-analysis packages). In effect, metaSEM allows you to specify a clustering variable that corresponds to how effect sizes (your correlations) are nested, so you could assign each study an ID number, and just use that as your clustering variable. Bringing it all together, based on your description, it seems as though you would be interested in at least two models:Model 1: An \"intercept-only\" model, whereby you estimate the meta-analytic average correlation between your two bird-related variables of interest.Model 2: A model whereby you test if/how this correlation is moderated by some other variable(s) (e.g., type of bird, type of outcome measure used, etc.,). The corresponding metaSEM code for each model is below. In the hypothetical example, you'd be using a data frame called \"mydata\", with columns called \"ID\", \"corrs\", \"corrs_v\", \"bird_type\", and \"measure_type\", corresponding to the ID # you assigned each study, the correlation(s) from the study, the variance of each correlation (metaSEM also allows you to specify standard errors instead), the type of bird, and the type of outcome measure respectively. #Install and call metaSEM packageinstall.packages(\"metaSEM\")library(metaSEM)#Fit Model 1-Intercept-onlymodel.1=meta3(y = corrs, v = corrs_v, cluster = ID, data = mydata, model.name = \"Intercept-only\")summary(model.1)#Fit Model 2a-Moderation by Bird Typemodel.2a=meta3(y = corrs, v = corrs_v, cluster = ID, data = mydata, x = cbind(bird_type), model.name = \"Moderation by Bird Type\")summary(model.2a)#Fit Model 2b-Moderation by Outcome Measure Typemodel.2a=meta3(y = corrs, v = corrs_v, cluster = ID, data = mydata, x = cbind(measure_type), model.name = \"Moderation by Outcome Measure Type\")summary(model.2a)What's nice about the 3-level meta-analysis approach (and metaSEM) is that you will get the descriptive statistics of effect size variability that you normally would from a random-effects model (e.g., , and ), except broken down for each level of clustering (i.e., one of each for within-study variability, and one for between study variability). Then, for your moderation models, you also get the benefit of having  for each level of clustering, so you have an idea of how much variance in effect sizes your moderator(s) are explaining at each level.So, to summarize, collect the effect sizes you are actually interested in meta-analyzing (it doesn't sound like you're interested in means, but rather, correlations of some sort) and their corresponding sample sizes (so you can calculate standard error or variance of the correlation; any introductory meta-analysis text will have these formulas). Then, once you have all this information entered, you can code an ID variable corresponding to which correlation(s) came from which study, and use metaSEM to estimate your meta-analytic model while appropriately accounting for the dependency among the effect sizes that come from the same study. That should be plenty to get you started; if you have remaining questions, or if I misunderstood something, just comment this response and I can edit it as needed. ","Display_name":"jsakaluk","Creater_id":53456,"Start_date":"2016-02-03 17:37:45","Question_id":193853}
{"_id":{"$oid":"5837a57da05283111e4d4d09"},"Last_activity":"2016-02-03 17:19:07","Creator_reputation":216,"Question_score":0,"Answer_content":"You can use the standard mean difference for pooling different scales. for further reading check : 'metaanalysis with r' a book writen by the author of the package 'meta' pg 25  However, in many settings different studies use different outcome  scales, e.g. different depression scales or quality of life scales. In  such cases we cannot pool the effect estimates (mean differences)  directly. Instead, we calculate a dimensionless effect measure from  every study and use this for pooling. A very popular dimensionless  effect measure is the standardised mean difference which is the  study’s mean difference divided by a standard deviation based either  on a single treatment group or both treatment groups.He then used this command#Ne,Nc sample size for both intervention,control,Me,Se for mean and standard deviation for each group, SMD for standardized mean differencemc2 \u0026lt;- metacont(Ne, Me, Se, Nc, Mc, Sc, sm=\"SMD\",+ data=data2)To get the result use summary(mc2), forest(mc2)","Display_name":"ahmedmar","Creater_id":100365,"Start_date":"2016-02-03 15:14:50","Question_id":193853}
{"_id":{"$oid":"5837a57da05283111e4d4d18"},"Last_activity":"2016-03-22 07:12:33","Creator_reputation":3619,"Question_score":0,"Answer_content":"You need to convert everything to mean differences with their standard error. If you only have means and sd before and after you will need to impute a value for the correlation coefficient between before and after scores. You may be able to estimate this if any studies have presented their results in both the formats you describe.With the means and standard errors you can in principle compare the drugs using a meta-regression although as you say these will be indirect comparisons.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-03-22 07:12:33","Question_id":193967}
{"_id":{"$oid":"5837a57da05283111e4d4d26"},"Last_activity":"2016-08-16 07:28:09","Creator_reputation":1,"Question_score":0,"Answer_content":"I have had the same issue (SPSS v23) with missing p-values when running multiple t-tests. A workaround was to only test the variables with missing values one at a time. I find this odd as the analysis was set to 'analysis-by-analysis' so should have been pairwise. Sample size and mean difference was the same in the output of both analyses but Bootstrap numbers (Bias, SE, Sig, 95%CI) were different or in the case of the p-value missing when the test was run with multiple variables.","Display_name":"Andrew Roberts","Creater_id":122375,"Start_date":"2016-08-16 06:37:57","Question_id":92734}
{"_id":{"$oid":"5837a57da05283111e4d4d27"},"Last_activity":"2014-04-06 12:02:46","Creator_reputation":1124,"Question_score":3,"Answer_content":"It is possible, even if very rare, a resampled sample to be composed by all same values; so the variance would be zero and the t statistics infinity","Display_name":"Giorgio Spedicato","Creater_id":6547,"Start_date":"2014-04-06 12:02:46","Question_id":92734}
{"_id":{"$oid":"5837a57da05283111e4d4d34"},"Last_activity":"2016-08-16 07:10:31","Creator_reputation":12937,"Question_score":0,"Answer_content":"I do not see a big problem there. Technically you may have GARCH(1,2) with the coefficient on  being equal to zero (similarly, there is nothing wrong with ARMA models that have some zero coefficients). It is a matter of how good the model approximates the data, and apparently GARCH(1,2) does a better job that its competitors for your data.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-16 07:10:31","Question_id":162888}
{"_id":{"$oid":"5837a57da05283111e4d4d43"},"Last_activity":"2016-08-16 07:07:59","Creator_reputation":550,"Question_score":1,"Answer_content":"This is a bit of a shameless plug of a publication of mine, but we do exactly this in this work (arxiv). Amongst other things, we propose adapting the variance of the exponential distribution to improve the acceptance (step S3.2 in algorithm in the paper).In our case, asymptotically the adaptation does not change the proposal distribution (which in the paper is when ). Thus, asymptotically, the process is still Markovian in the same spirit as the Wang-Landau algorithm. We numerically verify that the process is ergodic and the chain samples from the target distribution we choose (e.g. left bottom panel of Fig. 4).We don't use information about the acceptance rate, but we obtain an acceptance independent of the quantity we are interested in (equivalent to the energy of a spin system, bottom-right of Fig. 4).","Display_name":"J. C. Leit\u0026#227;o","Creater_id":12100,"Start_date":"2016-08-16 07:07:59","Question_id":7286}
{"_id":{"$oid":"5837a57da05283111e4d4d44"},"Last_activity":"2012-03-04 11:36:02","Creator_reputation":138,"Question_score":3,"Answer_content":"The approaches suggested by users wok and robertsy cover the most commonly cited examples of what you're looking for that I know of.  Just to expand on those answers, Haario and Mira wrote a paper in 2006 that combines the two approaches, an approach they call DRAM (delayed rejection adaptive Metropolis).Andrieu has a nice treatment of various different adaptive MCMC approaches (pdf) which covers Haario 2001 but also discusses various alternatives that have been proposed in recent years.","Display_name":"redmoskito","Creater_id":1938,"Start_date":"2012-03-04 11:36:02","Question_id":7286}
{"_id":{"$oid":"5837a57da05283111e4d4d45"},"Last_activity":"2011-02-16 09:18:35","Creator_reputation":650,"Question_score":3,"Answer_content":"You can improve the acceptance rate using delayed rejection as described in Tierney, Mira (1999). It is based on a second proposal function and a second acceptance probability, which guarantees the Markov chain is still reversible with the same invariant distribution: you have to be cautious since \"it is easy to construct adaptive methods that might seem to work but in fact sample from the wrong distribution\".","Display_name":"Wok","Creater_id":1351,"Start_date":"2011-02-16 09:18:35","Question_id":7286}
{"_id":{"$oid":"5837a57da05283111e4d4d46"},"Last_activity":"2011-02-16 08:35:20","Creator_reputation":141,"Question_score":7,"Answer_content":"I think that this paper from Heikki Haario et al. will give you the answer you need. The markovianity of the chain is affected by the adaptation of the proposal density, because then a new proposed value depends not only of the previous one but on the whole chain. But it seems that the sequence has still the good properties if great care is taken. ","Display_name":"robertsy","Creater_id":3108,"Start_date":"2011-02-16 08:15:42","Question_id":7286}
{"_id":{"$oid":"5837a57da05283111e4d4d53"},"Last_activity":"2016-08-16 06:45:48","Creator_reputation":12260,"Question_score":2,"Answer_content":"As Brian says in his answer: there's no simple rule as to which is better. For example, the UK's Office for National Statistics switched from HW to ARIMA and wrote a paper on it and while they chose to switch it was probably because of the power of the X12 (now X13) software package, which is ARIMA-based and very powerful, rather than the technique itself.Also, you should compare State Space (Kalman Filter) solutions, which is even more general. R's arima, for example, uses a State Space solution under the hood.Holt-Winters has three parameters, so it's simple, but they're basically smoothing factors so it doesn't tell you much if you know them. ARIMA has more parameters, and some of them have some intuitive meaning, but it still doesn't tell you much. State Space can be complex, but you can also explicitly model things for greater explanatory power. In my opinion, anyhow.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-08-11 12:05:50","Question_id":229384}
{"_id":{"$oid":"5837a57da05283111e4d4d54"},"Last_activity":"2016-08-11 11:40:36","Creator_reputation":180,"Question_score":1,"Answer_content":"I have seen people with different data sets compare results from both algorithms and get different results.  In some cases the Holt-Winters algorithms gives better results than the ARIMA and in others cases it is the other way around.  I don't think you will find and an explicit answer on when to use one over the other.","Display_name":"Brian O\u0026#39;Donnell","Creater_id":108167,"Start_date":"2016-08-11 11:40:36","Question_id":229384}
{"_id":{"$oid":"5837a57da05283111e4d4d61"},"Last_activity":"2016-03-11 11:40:32","Creator_reputation":147538,"Question_score":3,"Answer_content":"This has a nice geometric explanation.  A VIF of  indicates there is no detectable linear relationship among the regressors, while a large VIF indicates a tight linear relationship.  The question therefore asks  How is it possible for data to appear uncorrelated while subsets of the data are strongly correlated?Answer: create the subsets first.For example, consider two regressors  and  divided into groups wherein  and  are strongly related to each other, as in this picture:The groups are distinguished by color.The correlation among the entire collection of points is low (essentially zero in this example), while the correlation coefficients within six of these groups are either larger than  or smaller than , indicating extremely high correlation (and leading to very high VIFs if each group were used alone in a regression).It should be evident there's nothing to worry about.  After all, you could take any point cloud (representing a set of regressor values) and find several points--maybe a large number of them--that come close to lining up.  If you were to make that a group, you could create a regression based on them with a high VIF.  But, when there are other points in the model lying far from such point collections, the usual problems associated with high VIFs do not occur.Here is the R code used to create these data, showing how they were built up from individual strongly-correlated groups:X \u0026lt;- expand.grid(Group=1:7, X=seq(-10, 10, length.out=20))XX*(4 - XGroupplot(XY, col=terrain.colors(7)[X$Group], pch=16, xlab=\"X\", ylab=\"Y\")","Display_name":"whuber","Creater_id":919,"Start_date":"2016-03-11 11:40:32","Question_id":201205}
{"_id":{"$oid":"5837a57da05283111e4d4d6e"},"Last_activity":"2016-08-16 06:23:34","Creator_reputation":2608,"Question_score":1,"Answer_content":"Using your notation, if you have a time series of length  and you want to compute the mutual information between this time series and the time series delayed by  positions, you should match  values:t_1 , t_{k+1}\\\\t_2 , t_{k+2}\\\\\\vdots \\\\t_{n-k} , t_{n}Mutual information between delayed time series is often computed to infer biological networks (See here and here).So you will compute mutual information on  data points rather than . Be careful though when comparing mutual information estimated on a different number of data points: its estimates might be inflated if  is small (See here).","Display_name":"Simone","Creater_id":2719,"Start_date":"2016-08-16 06:23:34","Question_id":230088}
{"_id":{"$oid":"5837a57da05283111e4d4d7d"},"Last_activity":"2016-08-16 05:49:33","Creator_reputation":639,"Question_score":2,"Answer_content":"Firstly,  I think it's worth noting that the description of what ridge does assumes that the data matrix is orthonormal.Secondly, the answer to your question is yes under those circumstances.  The details may be found in \"Elements of Statistical Learning\" on p. 69 bis (section 3.4.3) .  The short story is that  is the formula. Please see the book for the complete discussion, better formatting, and details.  ","Display_name":"aginensky","Creater_id":39770,"Start_date":"2016-07-25 09:15:29","Question_id":225319}
{"_id":{"$oid":"5837a57da05283111e4d4d8a"},"Last_activity":"2016-08-16 05:28:58","Creator_reputation":3619,"Question_score":1,"Answer_content":"You can also convert the equation you have into one where the regression will give you a direct test of the coefficients.Intercept: if you subtract your hypothesised intercept (6.5) from your  variable (child height) and then run a regression the test whether the intercept is zero in the new model is the test that it has the hypothesised intercept with the unshifted .Slope: you need to fit a model including the  variable with a known coefficient (0.5). This is called an offset. Use  as the offset and also include  in the model as a covariate. The test of whether the coefficient of  is zero in the new model is the same as the test of whether it is 0.5 in the model without offset.Of course you would do both simultaneously.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-16 05:28:58","Question_id":230042}
{"_id":{"$oid":"5837a57da05283111e4d4d8b"},"Last_activity":"2016-08-16 01:30:09","Creator_reputation":865,"Question_score":0,"Answer_content":"You want to know whether there is a linear relationship between a boy's height and the combined height of their parents. More specifically, you have a theory about what the slope and intercept is in this linear relationship.One way of approaching the problem would seem to be to set those predictions up as your null hypothesis, and then try to reject it. You could fit the linear regression to this height data and then inspect the coefficients and their standard errors. If the predicted values (i.e. 0.5 and 6.5) do not sit within the standard errors (or CIs) then you can reject the null, and this would provide evidence that your hypothesised relationship between parent and child height is incorrect.","Display_name":"Ian_Fin","Creater_id":29532,"Start_date":"2016-08-16 01:30:09","Question_id":230042}
{"_id":{"$oid":"5837a57da05283111e4d4d98"},"Last_activity":"2016-08-16 05:27:28","Creator_reputation":28,"Question_score":1,"Answer_content":"Kolmogorov–Smirnov statistic may help you in this case.Following is an implementation which uses Kolmogorov-Smirnov statistic and the function returns the probability of similarity.#include \u0026lt;math.h\u0026gt;#define EPS1 0.001#define EPS2 1.0e-8float kstest(float alam) {    int j;    float a2, fac = 2.0, sum = 0.0, term, termbf = 0.0;    a2 = -2.0 * alam * alam;    for (j = 1; j \u0026lt;= 100; j++) {    term = fac * exp(a2 * j * j);    sum += term;    if (fabs(term) \u0026lt;= EPS1 * termbf || fabs(term) \u0026lt;= EPS2 * sum)        return sum;    fac = -fac;    termbf = fabs(term);    }    return 1.0;}void checkSameDist(float data1[], unsigned long n1, float data2[],          unsigned long n2, float *d, float *prob) {    float           kstest(float alam);    void            sort(unsigned long n, float arr[]);    unsigned long   j1 = 1,    j2 = 1;    float d1, d2, dt, en1, en2, en, fn1 = 0.0, fn2 = 0.0;    sort(n1, data1);    sort(n2, data2);    en1 = n1;    en2 = n2;    *d = 0.0;    while (j1 \u0026lt;= n1 \u0026amp;\u0026amp; j2 \u0026lt;= n2) {        if ((d1 = data1[j1]) \u0026lt;= (d2 = data2[j2]))            fn1 = j1++ / en1;        if (d2 \u0026lt;= d1)            fn2 = j2++ / en2;        if ((dt = fabs(fn2 - fn1)) \u0026gt; *d)            *d = dt;    }    en = sqrt(en1 * en2 / (en1 + en2));    *prob = kstest((en + 0.12 + 0.11 / en) * (*d));}Also check, following function checks if a particular distribution is normal, you could modify it a little bit (this would give you more intuition about the statistic and how can you implement it from scratch(https://walteis.wordpress.com/2012/04/26/a-kolmogorov-smirnov-implementation/)public bool IsNormal{    get    {        // This method uses the Kolmogorov-Smirnov test to determine a normal distribution.        // The level of significance (alpha) used is .05, and the critical values used are from Table 1 of:         // The Kolmogorov-Smirnov Test for Goodness of Fit        // Frank J. Massey, Jr.        // Journal of the American Statistical Association        // Vol. 46, No. 253 (Mar., 1951) (pp. 68-78)        if (DataSet.Count == 0)            return false;        List\u0026lt;double\u0026gt; vals = DataSet.Values.ToList();        Accumulator acc = new Accumulator(vals.ToArray());        double dmax = double.MinValue;        double cv = 0;        MathNet.Numerics.Distributions.NormalDistribution test = new MathNet.Numerics.Distributions.NormalDistribution(acc.Mean, acc.Sigma);        // the 0 entry is to force the list to be a base 1 index table.        List\u0026lt;double\u0026gt; cvTable = new List\u0026lt;double\u0026gt;() { 0, .975, .842, .708, .624, .565,                                                .521, .486, .457, .432, .410,                                                .391, .375, .361, .349, .338,                                                .328, .318, .309, .301, .294};        test.EstimateDistributionParameters(DataSet.Values.ToArray());        vals.Sort();        for (int i = 0; i \u0026lt; vals.Count; i++)        {            double dr = Math.Abs(((i + 1) / (double)vals.Count) - test.CumulativeDistribution(vals[i]));            double dl = Math.Abs(test.CumulativeDistribution(vals[i]) - (i / (double)vals.Count));            dmax = Math.Max(dmax, Math.Max(dl, dr));        }        // get critical value and compare to d(N)        if (vals.Count \u0026lt;= 10)            cv = cvTable[vals.Count];        else if (vals.Count \u0026gt; 10)            cv = 1.36 / Math.Sqrt(vals.Count);        return (dmax \u0026lt; cv);    }}Best of Luck","Display_name":"Parth Raghav","Creater_id":110367,"Start_date":"2016-08-16 05:27:28","Question_id":229977}
{"_id":{"$oid":"5837a57da05283111e4d4da5"},"Last_activity":"2016-08-16 04:52:15","Creator_reputation":9959,"Question_score":1,"Answer_content":"Let my give an answer to your last question in the comments:We have the stochastic trend model  for which, as discussed in the comments,E(\\Delta Y_t)=\\alpha On the other hand, the deterministic trend model has  and thus\\Delta Y_t=\\beta_0+\\beta_1t+v_t-[\\beta_0+\\beta_1(t-1)+v_{t-1}]=\\beta_1+\\Delta v_tso that, indeed,E(\\Delta Y_t)=\\beta_1Hence, the two processes are indeed equivalent in terms of their expected drift.They are not in terms of their variances, however. By recursive substitution (and assuming ), you can write the stochastic trend model asY_t=t\\alpha+\\sum_{s=1}^tv_s,which has varianceVar(Y_t)=t\\sigma^2_vwhereas the deterministic trend process has varianceVar(Y_t)=\\sigma^2_vPlaying around with the codealpha \u0026lt;- beta1 \u0026lt;- .1beta0 \u0026lt;- 0t \u0026lt;- 100v \u0026lt;- rnorm(t)Y.ST \u0026lt;- 1:t*alpha+cumsum(v)Y.DT \u0026lt;- beta0 + beta1*1:t + vplot(Y.ST, type=\"l\", col=\"gold\", lwd=2)lines(Y.DT, col=\"purple\", lwd=2)will show differences between the two processes. If you pick a large value for the drift or a small standard deviation for the errors, the differences will tend to get blurred for moderate , though.","Display_name":"Christoph Hanck","Creater_id":67799,"Start_date":"2016-08-16 04:52:15","Question_id":229879}
{"_id":{"$oid":"5837a57da05283111e4d4db2"},"Last_activity":"2016-08-16 04:45:46","Creator_reputation":6391,"Question_score":0,"Answer_content":"Addressing your questionAs you have stated correctly, the groups are independent, hence Dirichlet does not make any sense. When you have more than one group, you can boil the comparison down to 2 by one of these strategies:You have a reference / baseline group, which shall be improved. Compare all groups to that one.Compare the current best to the current second best to decide whether you have a global winner. In the same way, compare the worst to the second-worst to decide whether a group shall dropped from the competition.General remarkNote: Maybe I have misunderstood what you meant with \"comparison by mean\", I just add this for sake of completeness for other readers.Comparing the posteriors is good, but the current mean is rather volatile and hence may be bigger or smaller just by chance. IMHO it is recommended to compare the credible intervals of the means using a two-dimensional monte carlo integration to calculate the subjective probability of the true unknown mean of group B is greater than the true unknown mean of group A, i.e. group B \u003e group A.Example code forA: 401 trials, 125 successful trials B: 441 trials, 141 successful trialswhich istrials \u0026lt;- 10000resDat\u0026lt;-data.frame(\"a\"=rbeta(trials,125+1,401-125+1),                   \"b\"=rbeta(trials,144+1,441-144+1))length(which(resData))/trialsBut even using this approach one does only control the type II error, i.e. falsely missing a difference. To control the type I error, i.e. falsely stating finding a difference, I recommend a classical frequentist style test (e.g. G-Test) as followup.","Display_name":"steffen","Creater_id":264,"Start_date":"2016-08-16 04:45:46","Question_id":216027}
{"_id":{"$oid":"5837a57da05283111e4d4dbf"},"Last_activity":"2016-08-16 04:30:23","Creator_reputation":673,"Question_score":1,"Answer_content":"I got the effect sizes from , and followed these formulas:","Display_name":"qed","Creater_id":7787,"Start_date":"2016-08-16 04:30:23","Question_id":229926}
{"_id":{"$oid":"5837a57da05283111e4d4dd0"},"Last_activity":"2016-08-16 03:52:17","Creator_reputation":3354,"Question_score":0,"Answer_content":"I doubt there is a general formula for any distribution :n \u0026lt;- 100000x \u0026lt;- rpois(n, 1)y \u0026lt;- rchisq(n, df = 2)/2mean(x/(x+y))sd(x/(x+y))x2 \u0026lt;- rexp(n,1)y2 \u0026lt;- rexp(n,1)mean(x2/(x2+y2))sd(x2/(x2+y2))Though the expected values and variances are all the same:\u0026gt; mean(x/(x+y))[1] 0.4179734\u0026gt; sd(x/(x+y))[1] 0.3608732And: \u0026gt; mean(x2/(x2+y2))[1] 0.4996955\u0026gt; sd(x2/(x2+y2))[1] 0.2889206","Display_name":"RUser4512","Creater_id":73794,"Start_date":"2016-08-16 03:52:17","Question_id":230062}
{"_id":{"$oid":"5837a57da05283111e4d4ddd"},"Last_activity":"2016-08-15 08:31:43","Creator_reputation":152603,"Question_score":2,"Answer_content":"The usual assumption of constant variance are not usually reasonable for percentage data (they might be okay in some situations). [Linearity might also matter if you're trying to infer anything but the means at those two particular times, or the change in mean there.]In any case, if you're prepared to assume the population standard deviations are about the same at the two time periods then one easy way to get a standard error of the slope would be to fit a least squares regression line to the entire six values (i.e. don't average them first).[i.e. Set up a y column by stacking the six y-values, and an x-column by stacking the times (three 1 values on top of three 2 values).]There are a variety of equivalent available formulas for the standard error of the slope, but the following approach will probably be the easiest for you to implement. In simple linear regression,\\widehat{\\text{Var}}(\\hat{\\beta})=\\frac{(1-R^2)}{n-2}\\frac{\\text{Var}(y)}{\\text{Var}(x)}The standard error of the slope is the square root of that.[In Excel, you can get the slope using the SLOPE function, and the  value using the RSQ function between the y and x values. The ordinary VAR functions are applied to the y and x columns and then COUNT to (say) the y-values (here assuming there are no missing values in the x-column) to get the .]In the simple case you have here the slope and its standard error can be calculated a bit more simply, but the approach here is more general. ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-15 07:01:22","Question_id":229756}
{"_id":{"$oid":"5837a57da05283111e4d4dea"},"Last_activity":"2014-09-14 22:10:56","Creator_reputation":9472,"Question_score":1,"Answer_content":"I have found two measures of effect size in the literature for Cochran's  test of  blocks (subjects) and  treatments (groups):Serlin, Carr and Marascuillo's (2007) maximum-corrected measure of effect size (), which is given by:\\eta^{2}_{Q} = \\frac{Q}{b(k-1)},where .Berry, Johnston and Mielke (2007) offer a chance-corrected measure of effect size (), which is given by:\\mathcal{R} = 1 - \\frac{\\delta}{\\mu_{\\delta}},where:\\delta = \\left[k {b\\choose 2}\\right]^{-1}\\sum_{i=1}^{k}\\sum_{j=1}^{b-1}\\sum_{l=j+1}^{b}{\\left|x_{ji}-x_{li}\\right|}for observations  in data matrix , \\mu_{\\delta} = \\frac{2}{b\\left(b-1\\right)}\\left[\\left(\\sum_{i=1}^{b}{p_{i}}\\right)\\left(b-\\sum_{i=1}^{b}{p_{i}}\\right)-\\sum_{i=1}^{n}{p_{i}\\left(1-p_{i}\\right)}\\right],and  is the proportions of successes across all treatments in the  block.Update: From personal correspondence with Berry, the published Equation [7] contains a typographical error, and the  term in the equation for  should be replaced with  as I have represented above.Berry \u0026amp;Co. also make a critique of  versus , writing (I substitute the symbols  and  for the symbols  and  appearing in their paper):  Chance-corrected measures of effect size, such as , possess distinct advantages in interpretation over maximum-corrected measures of effect size,  such as . The problem lies in the manner in which  is maximized. The denominator of , , standardizes the observed value of  for the sample size and the number of treatments. Unfortunately,  does not standardize  for the data on which  is based but rather standardizes  on another unobserved hypothetical set of data.A little farther, they sell the merits of  over those of :   is completely data dependent, whereas  relies on an unobserved, idealized data set for its maximum value. Thus,  can achieve an effect size of unity for the observed data, while this is usually impossible for  . Second,  is a chance-corrected measure of effect size. Furthermore,  is zero under chance conditions, unity when agreement among the  subjects is perfect, and negative under conditions of disagreement. Therefore,  has a clear interpretation corresponding to Cohen's coefficient of agreement (1960) and other chance-corrected measures that is familiar to most researchers. On the other hand,  possesses no meaningful interpretation except for values of 0 and 1. Although takes the form of a correlation ratio, it cannot be interpreted as a correlation coefficient unless the marginal frequency totals are identicalI have implemented both of these effect size measures in Stata in the cochranq package, which can be accessed within Stata by typing net describe cochranq, from(http://www.doyenne.com/stata).ReferencesBerry, K. J., Johnston, J. E., and Jr., P. W. M. (2007). An alternative measure of effect size for Cochran’s  test for related proportions. Perceptual and Motor Skills, 104:1236–1242.Serlin, R. C., Carr, J., and Marascuillo, L. A. (2007). A measure of association for selected nonparametric procedures. Psychological Bulletin, 92:786–790.","Display_name":"Alexis","Creater_id":44269,"Start_date":"2014-09-07 15:18:40","Question_id":9867}
{"_id":{"$oid":"5837a57da05283111e4d4deb"},"Last_activity":"2011-05-24 13:07:50","Creator_reputation":37824,"Question_score":2,"Answer_content":"I found this paper with Google but I cannot access it, so I don't really know what it is about really:  Berry KJ, Johnston JE, Mielke PW Jr.  An alternative measure of effect size  for Cochran's Q test for related  proportions. Percept Mot Skills.  2007 Jun;104(3 Pt 2):1236-42.I initially thought that using pairwise multiple comparisons with Cochran or McNemar test* (if the overall test is significant) would give you further indication of where the differences lie, while reporting simple difference for your binary outcome would help asserting the magnitude of the observed difference. * I found an online tutorial with R.","Display_name":"chl","Creater_id":930,"Start_date":"2011-05-24 13:07:50","Question_id":9867}
{"_id":{"$oid":"5837a57da05283111e4d4df7"},"Last_activity":"2016-08-16 02:33:04","Creator_reputation":1,"Question_score":0,"Answer_content":"I think the problem is in the word 'true'.  The reality of the natural world is innately un-knowable as it's infinitely complex and infinitely variable over time, so 'truth' applied to nature is always conditional.  All we can do is try to find levels of probable correspondence between variables by repeated experiment.  In our attempt to make sense of reality, we look for what seems like order in it and construct conceptually-conscious models of it in our mind to help us make sensible decisions BUT it's very much a hit-and-miss affair because there's always the unexpected.  The null hypothesis is the only reliable starting point in our attempt to make sense of reality. ","Display_name":"John Faupel","Creater_id":127825,"Start_date":"2016-08-16 02:33:04","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4df8"},"Last_activity":"2016-08-16 01:09:19","Creator_reputation":1,"Question_score":0,"Answer_content":"Isn't all statistics premised on the assumption that nothing is certain in the natural world (as distinct from the man-made world of games \u0026amp;c).  In other words, the only way we can get near to understanding it is by measuring the probability that one thing correlates with another and this varies between 0 and 1 but can only be 1 if we could test the hypothesis an infinite number of times in an infinite number of different circumstances, which of course is impossible.  And we can never know it was zero for the same reason.  It's a more reliable approach to understanding the reality of nature, than mathematics, which deal in absolutes and mostly relies on equations, which we know are idealistic because if, literally, the LH side of an equation really = the RH side, the two sides could be reversed and we wouldn't learn anything.  Strictly speaking it applies only to a static world, not a 'natural' one which is intrinsically turbulent.  Hence, the null hypothesis should even underwrite mathematics - whenever it is used to understand  nature itself.  ","Display_name":"John Faupel","Creater_id":127825,"Start_date":"2016-08-16 01:09:19","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4df9"},"Last_activity":"2015-12-10 03:50:39","Creator_reputation":21,"Question_score":2,"Answer_content":"If I'm understanding you correctly, you're in agreement with the late, great Paul Meehl. See Meehl, P.E. (1967). Theory-testing in psychology and physics: A methodological paradox. Philosophy of Science, 34:103-115.","Display_name":"Doc","Creater_id":13388,"Start_date":"2012-08-17 12:17:19","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4dfa"},"Last_activity":"2013-12-12 09:55:50","Creator_reputation":1026,"Question_score":2,"Answer_content":"I'll expand on the mention of Paul Meehl by @Doc:1) Testing the opposite of your research hypothesis as the null hypothesis makes it so you can only affirm the consequent which is a \"formally invalid\" argument. The conclusions do not necessarily follow from the premise.If Bill Gates owns Fort Knox, then he is rich.Bill Gates is rich.Therefore, Bill Gates owns Fort Knox.http://rationalwiki.org/wiki/Affirming_the_consequentIf the theory is \"This drug will improve recovery\" and you observe improved recovery this does not mean you can say your theory is true. The appearance of improved recovery could have occurred for some other reason. No two groups of patients or animals will be exactly the same at baseline and will change further over time during the study. This is a greater problem for observational than experimental research because randomization \"defends\" against severe imbalances of unknown confounding factors at baseline. However, randomization does not really resolve the problem. If the confounds are unknown we have no way to tell the extent to which the \"randomization defense\" has been successful.Also see table 14.1 and the discussion of why no theory can be tested on it's own (there are always auxiliary factors that tag along) in:Paul Meehl. \"The Problem Is Epistemology, Not Statistics: Replace Significance Tests by Confidence Intervals and Quantify Accuracy of Risky Numerical Predictions\" In L. L. Harlow, S. A. Mulaik, \u0026amp; J. H. Steiger (Eds.), What If There Were No Significance Tests? (pp. 393–425) Mahwah, NJ : Erlbaum, 1997.2) If some type of bias is introduced (e.g., imbalance on some confounding factors) we do not know which direction this bias will lie or how strong it is. The best guess we can give is that there is a 50% chance of biasing the treatment group in the direction of higher recovery. As sample sizes get large there is also 50% chance that your significance test will detect this difference and you will interpret the data as corroborating your theory.This situation is totally different from the case of a null hypothesis that \"This drug will improve recovery by x%\". In this case the presence of any bias (which I would say always exist in comparing groups of animals and humans) makes it more likely for you to reject your theory. Think of the \"space\" (Meehl calls it the \"Spielraum\") of possible results bounded by the most extreme measurements possible. Perhaps there can be 0-100% recovery, and you can measure with resolution of 1%. In the common significance testing case, the space consistent with your theory will be 99% of the possible outcomes you could observe. In the case when you predict a specific difference the space consistent with your theory will be 1% of the possible outcomes.Another way of putting it is that finding evidence against a null hypothesis of mean1=mean2 is not a severe test of the research hypothesis that a drug does something. A null of mean1 \u0026lt; mean2 is better but still not very good.See figure 3 and 4 here:(1990). Appraising and amending theories: The strategy of Lakatosian defense and two principles that warrant using it. Psychological Inquiry, 1, 108-141, 173-180","Display_name":"Flask","Creater_id":31334,"Start_date":"2013-12-12 09:55:50","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4dfb"},"Last_activity":"2012-08-18 01:24:28","Creator_reputation":55,"Question_score":-1,"Answer_content":"We must select null hypothesis the one which we want to reject.Because in our hypothesis testing scenario, there is a critical region, if the region under hypothesis come in critical region , we reject the hypothesis otherwise we accept the hypothesis.So suppose we select the null hypothesis , the one we want to accept. And the region under null hypothesis does not come under critical region, So we will accept the null hypothesis. But the problem here is if region under null hypothesis come under acceptable region, then it does not mean that the region under alternate hypothesis will not come under acceptable region. And if this is the case then our interpretation about result will be wrong. So we must only take that hypothesis as a null hypothesis which we want to reject. If we are able to reject null hypothesis, then it means that alternate hypothesis is true. But if we are not able to reject null hypothesis, then it means that any of the two hypothesis can be correct. May be we can then take another test, in which we can can take our alternate hypothesis as null hypothesis, and then we can attempt to reject it. If we are able to reject the alternate hypothesis(which now is null hypothesis.) , then we can say that our initial null hypothesis was true.","Display_name":"mohit khanna","Creater_id":13010,"Start_date":"2012-08-18 01:24:28","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4dfc"},"Last_activity":"2011-11-21 08:57:45","Creator_reputation":27790,"Question_score":4,"Answer_content":"I think this is another case where frequentist statistics can't give a direct answer to the question you actually want to ask, and so answers a (no so) subtly different question, and it is easy to misinterpret this as a direct answer to the question you actually wanted to ask.What we would really like to ask is normally what is the probability that the alternative hypothesis is true (or perhaps how much more likely to be true is it than the null hypothesis).  However a frequentist analysis fundamentally cannot answer this question, as to a frequentist a probability is a long run frequency, and in this case we are interested in the truth of a particular hypothesis, which doesn't have a long run frequency - it is either true or it isn't.  A Bayesian on the other hand can answer this question directly, as to a a Bayesian a probability is a measure of the plausibility of some proposition, so it is perfectly reasonable in a Bayesian analysis to assign a probability to the truth of a particular hypothesis.The way frequentists deal will particular events is to treat them as a sample from some (possibly fictitious) population and make a statement about that population in place of a statement about the particular sample.  For example, if you want to know the probability that a particular coin is biased, after observing N flips and observing h heads and t tails, a frequentist analysis cannot answer that question, however they could tell you the proportion of coins from a distribution of unbiased coins that would give h or more heads when flipped N times.  As the natural definition of a probability that we use in everyday life is generally a Bayesian one, rather than a frequentist one, it is all too easy to treat this as the pobability that the null hypothesis (the coin is unbiased) is true.Essentially frequentist hypothesis tests have an implicit subjectivist Bayesian component lurking at its heart.  The frequentist test can tell you the likelihood of observing a statistic at least as extreme under the null hypothesis, however the decision to reject the null hypothesis on those grounds is entirely subjective, there is no rational requirement for you to do so.  Essentiall experience has shown that we are generally on reasonably solid ground to reject the null if the p-value is suffciently small (again the threshold is subjective), so that is the tradition.  AFAICS it doesn't fit well into the philosophy or theory of science, it is essentially a heuristic.That doesn't mean it is a bad thing though, despite its imperfections frequentist hypothesis testing provides a hurdle that our research must get over, which helps us as scientists to keep our self-skepticism and not get carried away with enthusiasm for our theories.  So while I am a Bayesian at heart, I still use frequentists hypothesis tests on a regular basis (at least until journal reviewers are comfortable with the Bayesain alternatives).  ","Display_name":"Dikran Marsupial","Creater_id":887,"Start_date":"2011-11-21 08:57:45","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4dfd"},"Last_activity":"2011-08-03 12:30:53","Creator_reputation":57712,"Question_score":3,"Answer_content":"I think you've got a fundamental error here (not that the whole area of hypothesis testing is clear!) but you say the alternative is what we try to prove. But this is not right. We attempt to reject (falsify) the null. If the results we obtain would be very unlikely if the null were true, we reject the null.Now, as others said, this is not usually the question we want to ask: We don't usually care how likely the results are if the null is true, we care how likely the null is, given the results. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2011-08-03 12:30:53","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4dfe"},"Last_activity":"2011-08-03 07:46:32","Creator_reputation":2383,"Question_score":3,"Answer_content":"You are right that, in a sense, frequentist hypothesis testing has it backwards.  I'm not saying that that approach is wrong, but rather that the results are often not designed to answer the questions that the researcher is most interested in.  If you want a technique more similar to the scientific method, try Bayesian inference.Instead of talking about a \"null hypothesis\" that you can reject or fail to reject, with Bayesian inference you begin with a prior probability distribution based upon your understanding of the situation at hand.  When you acquire new evidence, Bayesian inference provides a framework for you to update your belief with the evidence taken into account.  I think this is how more similar to how science works.","Display_name":"Michael McGowan","Creater_id":2485,"Start_date":"2011-08-03 07:46:32","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4dff"},"Last_activity":"2011-08-03 06:34:51","Creator_reputation":287,"Question_score":3,"Answer_content":"To add to Gavin's answer, a couple of things:First, I've heard this idea that propositions can only be falsified, but never proven.  Could you post a link to a discussion of this, because with our wording here it doesn't seem to hold up very well - if X is a proposition, then not(X) is a proposition too.  If disproving propositions is possible, then disproving X is the same as proving not(X), and we've proven a proposition.Second, your analogy between the P(effective|) and P(dog|4 legs) is interesting.  The wording should be changed a little bit though:   The drug is effective (i.e.: iff the drug is effective you will see an effect).In fact, P(effective|) is often greater than P(|effective), as long as you use hypothesis testing and the right statistical model.  Hypothesis testing formalizes the unlikelihood of positive test results under .  But an effective drug doesn't guarentee a positive test; when the drug is effective and variance is high the effect can be masked in the test.   If you observe  you can infer effectiveness, because the alternative is , and the hypothesis testing is set up so that P(|) \u0026lt; 0.05.So the difference between the dog case and the effectiveness case is in the appropriateness of the inference from the evidence to the conclusion.  In the dog case, you have observed some evidence that doesn't strongly imply a dog.  But in the clinical trial case you have observed some evidence that does strongly imply efficacy.","Display_name":"ImAlsoGreg","Creater_id":5539,"Start_date":"2011-08-03 06:34:51","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4e00"},"Last_activity":"2011-08-03 04:35:36","Creator_reputation":17439,"Question_score":13,"Answer_content":"In statistics there are tests of equivalence as well as the more common test the Null and decide if sufficient evidence against it. The equivalence test turn this on its head and posits that effects are different as the Null and we determine if their is sufficient evidence against this Null.I'm not clear on your drug example. If the response is a value/indicator of the effect, then an effect of 0 would indicate not effective. One would set that as the Null and evaluate the evidence against this. If the effect is sufficiently different from zero we would conclude that the no-effectiveness hypothesis is inconsistent with the data. A two-tailed test would count sufficiently negative values of effect as evidence against the Null. A one tailed test, the effect is positive and sufficiently different from zero, might be a more interesting test.If you want to test if the effect is 0, then we'd need to flip this around and use an equivalence test where the H0 is the effect is not equal to zero, and the alternative is that H1 = the effect = 0. That would evaluate the evidence against the idea that effect was different from 0.","Display_name":"Gavin Simpson","Creater_id":1390,"Start_date":"2011-08-03 03:59:58","Question_id":13797}
{"_id":{"$oid":"5837a57da05283111e4d4e0d"},"Last_activity":"2016-08-16 02:22:05","Creator_reputation":308,"Question_score":0,"Answer_content":"I am not sure about your notation, however:Using the regression identity: It is out of scope but you can read more about the regression identity e.g. here.It is relatively trivial to show that: , where  and  is the coefficient of indetermination.Which ultimately gives: . If we assume  we can deduce that it is then a matter to divide by the nobs () less the number of variables in the model ; and then take the sqrt of the right hand and left hand side of the identify above. where  is just a sample standard deviation of the y's. This means that the relationship suggested by the OP is not entirely correct.","Display_name":"IcannotFixThis","Creater_id":26474,"Start_date":"2016-08-15 05:00:05","Question_id":229882}
{"_id":{"$oid":"5837a57da05283111e4d4e1a"},"Last_activity":"2016-08-16 00:57:41","Creator_reputation":604,"Question_score":6,"Answer_content":"I think simple cross validation is the best fit. Both AIC and BIC consider the balance between model complexity and the amount of information available. With more data, more complex models can be learned. However, this balance is fixed and not based on the data. Cross validation is based on the data. It also balances model complexity with the amount of information available. With more data more complex models can be learned. The performance on unseen data quantifies how well the model works. Implicitly, models that are to complex (overfitting) are penalized because they make bad predictions. In the case of many variables the highly correlated ones can be chosen during training. During testing however it becomes apparent that the learned relations do not generalize to unseen data. Another advantage of cross validation is that you can choose your own performance measurement. ","Display_name":"Pieter","Creater_id":93550,"Start_date":"2016-08-15 14:45:22","Question_id":229987}
{"_id":{"$oid":"5837a57da05283111e4d4e2b"},"Last_activity":"2016-08-16 00:10:06","Creator_reputation":1230,"Question_score":2,"Answer_content":"It hard to answer such a question since it involves both \"What was X thinking? and the subjective concept of being interesting.My guess will be Kolmogorov complexity.The Kolmogorov complexity of a string is the length of the shortest program that can produce that string.I think that having a short Kolmagorv complexity with respect to the string is good definition of being interesting since:Most of the times when we understand something, we can describe it in a more efficient way. The connection between short description, and predicatibility (as a formal representation of \"understanding\") is well known and has many aspects such as Occam's_razor It is rare to have a Kolmogorov complexity shorter than the string. That can be observed due to a counting argument. Specifically, randomly generated strings are likely to have a Kolmogorov complexity of about their length.The Kolmogorov complexity is not computable. That mean's that there is no algorithm that can give us the actual Kolmogorov complexity of every string. We can just upper bound it. That make the measure itself interesting...Let choose the Kolmogorov complexity as a measure of being interesting and \"lexical white noise\" as a string of random lexical (by some common definition of randomness).We get that Randall Munroe is correct by these definitions and also by the common sense. ","Display_name":"Dan Levin","Creater_id":81056,"Start_date":"2016-08-16 00:10:06","Question_id":229719}
{"_id":{"$oid":"5837a57da05283111e4d4e2e"},"Last_activity":"2016-08-15 23:49:27","Creator_reputation":5189,"Question_score":1,"Answer_content":"The \"intuitive\" claim in the beginning is correct:\\begin{equation}P(h(X,Y) = w \\mid X = x) = P(f(X,Y) \\in \\{z \\mid g(z)=w\\} \\mid X = x) \\\\ = \\sum_{z_i \\in \\{z \\mid g(z)=w\\}} P(f(X,Y) = z_i \\mid X=x) = \\sum_{z_i \\in \\{z \\mid g(z)=w\\}} P(f(X,Y) = z_i) = P(f(X,Y) \\in \\{z \\mid g(z)=w\\}) = P(h(X,Y) = w).\\end{equation}An alternative way of stating it would be that if  and  are independent, then  and  are independent. Applying a deterministic mapping to one of independent random variables cannot lose independence.The counterexample does not work since there is no function  with the property that  for all values   may attain here (as @whuber pointed out in the comments). The example is equivalent to your first try (mentioned in comments) with  upto relabeling the range of . As you found that there was no function in that case the first version, there is no applicable function in the new case, either. not being such a function could be interpreted as being due to it having 'multiple branches': if we want  to imply ,   should equal both  and , which is a proof-by-contradiction for nonexistence of such a function. We may decide to define if as a multivalued function or perhaps define a function  of  that picks the correct \"branch\" based on  -- but then the premises of the initial claim do not hold. ","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-15 23:36:33","Question_id":229991}
{"_id":{"$oid":"5837a57da05283111e4d4e3b"},"Last_activity":"2011-11-28 01:29:02","Creator_reputation":24971,"Question_score":2,"Answer_content":"According to documentation, you need function classify_models (excerpt from the documentation page):  Description    Uses a trained model from the train_models function to classify new  data.    Usage    classify_models(corpus, models, ...)    Arguments    corpus   Class of type matrix_container-class generated by the  create_corpus function.    models   List of models to be used for classification generated by  train_models.    ...  Other parameters to be passed on to classify_model.Here is the reproducible example:###Train model (taken out of `classify_model` man page)library(RTextTools)    set.seed(123)alldata \u0026lt;- read_data(system.file(\"data/NYTimes.csv.gz\",package=\"RTextTools\"),type=\"csv\")smpl \u0026lt;- sample(1:3100,size=100)data \u0026lt;- alldata[smpl,]matrix \u0026lt;- create_matrix(cbind(dataSubject), language=\"english\", removeNumbers=TRUE, stemWords=FALSE, weighting=weightTfIdf)corpus \u0026lt;- create_corpus(matrix,dataTitle,newdataTopic.Code,trainSize=1:1, testSize=2:101, virgin=FALSE)newresults1 \u0026lt;- classify_model(newcorpus,models[[1]])newresults2 \u0026lt;- classify_model(newcorpus,models[[2]])The last line should produce the error, since the new data has some terms which were not present in the data used to produce the models. To circumvent this, it is better to use create_matrix on all the data you have, and then to train model on the sample of result of create_matrix, instead of the original data.This solution is a bit quirky, since the convenience function classify_models demands a corpus, where train and test data sets must be explicitly defined. Since the goal is to use the classifier, all data is test data. In this solution I sidestepped the problem by selecting only one observation for train  and all the others for the test. To sidestep this it is possible to use predict method directly:predict(models[[1]],as.compressed.matrix(newmatrix))","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2011-11-28 01:29:02","Question_id":18935}
{"_id":{"$oid":"5837a57da05283111e4d4e48"},"Last_activity":"2016-08-15 22:33:33","Creator_reputation":8337,"Question_score":2,"Answer_content":"  Well, the parameters that represent higher exponentials (x3,x4) are drasticly increasing the complexity of our model. So shouldn't we penalize more for high w3,w4 values than we penalize for high w1,w2 values?The reason we say that adding quadratic or cubic terms increases model complexity is that it leads to a model with more parameters overall. We don't expect a quadratic term to be in and of itself more complex than a linear term. The one thing that's clear is that, all other things being equal, a model with more covariates is more complex.For the purposes of regularization, one generally rescales all the covariates to have equal mean and variance so that, a priori, they are treated as equally important. If some covariates do in fact have a stronger relationship with the dependent variable than others, then, of course, the regularization procedure won't penalize those covariates as strongly, because they'll have greater contributions to the model fit.But what if you really do think a priori that one covariate is more important than another, and you can quantify this belief, and you want the model to reflect it? Then what you probably want to do is use a Bayesian model and adjust the priors for the coefficients to match your preexisting belief. Not coincidentally, some familiar regularization procedures can be construed as special cases of Bayesian models. In particular, ridge regression is equivalent to a normal prior on the coefficients, and lasso regression is equivalent to a Laplacian prior.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-15 22:33:33","Question_id":230013}
{"_id":{"$oid":"5837a57da05283111e4d4e49"},"Last_activity":"2016-08-15 21:52:26","Creator_reputation":900,"Question_score":0,"Answer_content":"Great observations. To answer your question \"Should we penalise 'more'?\" Well, do we gain anything from imposing a priori penalty on some variables?We kind of do the opposite in practice, remember re-scaling the input variables to the same magnitude. Different magnitudes gives different a priori 'importance' to some of the variables. We don't know which ones are important and which not. There's an entire line of research about finding the right 'features' or feature selection / representation learning.So, here's two ways to think about it.One could start with a simple linear basis hypothesis and no regularisation. Then have a different hypothesis of the model, taking quadratic and other interactions of the input space. Sure. Then add regularisation and so on. So this 'search' is simple to complex. More of a parametric way to do it since you produce the hypotheses about the basis.Or, an alternative 'non-parametric' way would be to start with a really complex hypothesis, and let the regularisation do the work (e.g. penalise the complexity and arrive at something simpler) via cross-validation.The point of regularisation and nonparametrics is to do things  automatically. Let the machine do the work.Here is a good resource on basis functions.And finally,  spaces and norms will clear things up even more.","Display_name":"Florin Schimbinschi","Creater_id":91213,"Start_date":"2016-08-15 21:52:26","Question_id":230013}
{"_id":{"$oid":"5837a57da05283111e4d4e62"},"Last_activity":"2012-08-14 03:52:12","Creator_reputation":25897,"Question_score":0,"Answer_content":"The approach is just one of many possible ways to adjust for multiplicity.  You could also apply bootstrap p-value adjustment ala Westfall and Young.","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-08-09 18:24:02","Question_id":19073}
{"_id":{"$oid":"5837a57da05283111e4d4e63"},"Last_activity":"2012-06-10 16:02:53","Creator_reputation":180,"Question_score":1,"Answer_content":"You can report the -values, or can make a figure with the means (plus/minus the standard error) and indicate with different letters those means that are statistically different according to G\u0026amp;H test.Games and Howell procedure uses the Studentized maximum modulus distribution. Hope it help! and excuse my english","Display_name":"Damian","Creater_id":11843,"Start_date":"2012-06-09 17:03:46","Question_id":19073}
{"_id":{"$oid":"5837a57da05283111e4d4e70"},"Last_activity":"2015-01-06 13:58:20","Creator_reputation":1025,"Question_score":2,"Answer_content":"In your situation, the t-test will likely be robust in terms of Type I error rate, but not Type II error rate.  You would probably achieve more power through either a) a Kruskal-Wallis test, or b) a normalizing transformation prior to a t-test.I'm basing this conclusion on two Monte Carlo studies. In the first (Khan \u0026amp; Rayner, 2003), skew and kurtosis were indirectly manipulated via the parameters of the g-and-k distribution family, and the resulting power was examined.  Importantly, the Kruskal- Wallis test's power was less damaged by non-normality, particularly for n\u003e=15.A few caveats/qualifications about this study: Power was often hurt by high kurtosis, but it was less affected by skew.   At first glance, this pattern might seem less relevant to your situation given that you noted a problem with skew, not kurtosis.  However, I'm betting that excess kurtosis is also extreme in your case.  Keep in mind that excess kurtosis will be at least as high as skew^2 - 2.  (Let excess kurtosis equal the 4th standardized moment minus 3, so that excess kurtosis=0 for a normal distribution.)  Note also that Khan and Rayner (2003) examined ANOVAs with 3 groups, but their results are likely to generalize to a two-sample t-test.A second relevant study (Beasley, Erikson, \u0026amp; Allison, 2009) examined both Type I and Type II errors with various non-normal distributions, such as a Chi-squared(1) and Weibull(1,.5).  For sample sizes of at least 25, the t-test adequately controlled the Type I error rate at or below the nominal alpha level.  However, power was highest with either a Kruskal-Wallis test or with a Rank-based Inverse Normal transformation (Blom scores) applied prior to the t-test.  Beasley and colleagues generally argued against the normalizing approach, but it should be noted that the the normalizing approach controlled the Type I error rate for n\u003e=25, and its power sometimes slightly exceeded that of the Kruskal-Wallis test.  That is, the normalizing approach seems promising for your situation.  See tables 1 and 4 in their article for details.References:Khan, A., \u0026amp; Rayner, G. D. (2003). Robustness to non-normality of common tests for the many-sample location problem. Journal of Applied Mathematics and Decision Sciences, 7, 187-206.Beasley, T. M., Erickson, S., \u0026amp; Allison, D. B. (2009). Rank-based inverse normal transformations are increasingly used, but are they merited? Behavioral Genetics, 39, 580-595.","Display_name":"Anthony","Creater_id":27148,"Start_date":"2015-01-06 13:58:20","Question_id":38967}
{"_id":{"$oid":"5837a57da05283111e4d4e71"},"Last_activity":"2015-01-06 07:11:26","Creator_reputation":9063,"Question_score":5,"Answer_content":"@PeterFlom has already mentioned that simulation studies can never cover all scenarios and possibilities and therefore cannot lead to a definite answer. However, I still find it useful to actually explore an issue like this by conducting some simulations (this also happens to be exactly the type of exercise that I like to use when introducing the idea of Monte Carlo simulation studies to students). So, let's actually try this out. I'll use R for this.The Coden1 \u0026lt;- 33n2 \u0026lt;- 45mu1 \u0026lt;- 0mu2 \u0026lt;- 0sd1 \u0026lt;- 1sd2 \u0026lt;- 1iters \u0026lt;- 100000p1 \u0026lt;- p2 \u0026lt;- p3 \u0026lt;- p4 \u0026lt;- p5 \u0026lt;- rep(NA, iters)for (i in 1:iters) {   ### normal distributions   x1 \u0026lt;- rnorm(n1, mu1, sd1)   x2 \u0026lt;- rnorm(n2, mu2, sd2)   p1[i] \u0026lt;- t.test(x1, x2)p.value   ### both variables skewed to the left   x1 \u0026lt;- -1 * (rchisq(n1, df=1) - 1)/sqrt(2) * sd1 + mu1   x2 \u0026lt;- -1 * (rchisq(n2, df=1) - 1)/sqrt(2) * sd2 + mu2   p3[i] \u0026lt;- t.test(x1, x2)p.value   ### first skewed to the right, second skewed to the left   x1 \u0026lt;- (rchisq(n1, df=1) - 1)/sqrt(2)      * sd1 + mu1   x2 \u0026lt;- -1 * (rchisq(n2, df=1) - 1)/sqrt(2) * sd2 + mu2   p5[i] \u0026lt;- t.test(x1, x2)\\alpha = .05$). When the skewness is in opposite directions, there is some slight inflation in the Type I error rate.If we change the code to mu1 \u0026lt;- .5, then we get:   p1    p2    p3    p4    p5 0.574 0.610 0.606 0.592 0.602So, compared to the case where both distributions are normal (as assumed by the test), power actually appears to be slightly higher when the skewness is in the same direction! If you are surprised by this, you may want to rerun this a few times (of course, each time getting slightly different results), but the pattern will remain.Note that we have to be careful with interpreting the empirical power values under the two scenarios where the skewness is in opposite directions, since the Type I error rate is not quite nominal (as an extreme case, suppose I always reject regardless of what the data show; then I will always have a test with maximal power, but of course the test also has a rather inflated Type I error rate).One could start exploring a range of values for mu1 (and mu2 -- but what really matters is the difference between the two) and, more importantly, start changing the true standard deviations of the two groups (i.e., sd1 and sd2) and especially making them unequal. I also stuck to the sample sizes mentioned by the OP, but of course that could be adjusted as well. And skewness could of course take many other forms than what we see in a chi-squared distribution with one degree of freedom. I still think approaching things this way is useful, despite the fact that it cannot yield a definite answer.","Display_name":"Wolfgang","Creater_id":1934,"Start_date":"2015-01-06 06:21:49","Question_id":38967}
{"_id":{"$oid":"5837a57da05283111e4d4e72"},"Last_activity":"2015-01-06 03:17:57","Creator_reputation":301,"Question_score":0,"Answer_content":"First of all, if you assume that the distribution of the two samples is different, make sure you are using Welch's version of the t-test which assumes unequal variances between the groups. This will at least attempt to account for some of the differences that occur because of the distribution.If we look at the formula for the Welch's t-test:t = {\\overline{X}_1 - \\overline{X}_2 \\over s_{\\overline{X}_1 - \\overline{X}_2}}where  iss_{\\overline{X}_1 - \\overline{X}_2} = \\sqrt{{s_1^2 \\over n_1} + {s_2^2  \\over n_2}}we can see that everytime there is an s we know the variance is being taken into account. Let's imagine that the two variances are in fact the same, but one is skewed, leading to a different variance estimate. If this estimate of the variance is not actually representative of your data because of skew, then the actually biasing effect will essentially be the square-root of that bias divided by the number of data points used to calculate it. Thus the effect of bad estimators of variance is muffled a bit by the square-root and a higher n, and that is probably why the consensus is that it remains a robust test.The other issue of skewed distributions is that mean calculation will also be affected, and this is probably where the real problems of test assumption violations are since the means are relatively sensitive to skew. And the robustness of the test can be determined roughly by calculating the difference in means, compared to the difference in medians (as an idea). Perhaps you could even try replacing the difference in means by the difference in medians in the t-test as a more robust measure (I'm sure someone has discussed this but I couldn't find something on google quickly enough to link to).I would also suggest running a permutation test if all you are doing is a t-test. The permutation test is an exact test, independent of distribution assumptions. Most importantly, the permutation tests and t-test will lead to identical results if the assumptions of the parametric test are met. Therefore, the robustness measure you seek can be 1 - the difference between the permutation and t-test p-values, where a score of 1 implies perfect robustness and 0 implies not robust at all.","Display_name":"Mensen","Creater_id":42917,"Start_date":"2015-01-06 03:17:57","Question_id":38967}
{"_id":{"$oid":"5837a57da05283111e4d4e73"},"Last_activity":"2012-10-08 17:55:32","Creator_reputation":152603,"Question_score":12,"Answer_content":"@PeterFlom hit the nail dead on with his first sentence.I'll try to give a rough summary of what studies I have seen (if you want links it could be a while):Overall, the two sample t-test is reasonably power-robust to symmetric non-normality (the true type-I-error-rate is affected somewhat by kurtosis, the power is impacted mostly by that).When the two samples are mildly skew in the same direction, the one-tailed t-test is no longer unbiased. The t-statistic is skewed oppositely to the distribution, and has much more power if the test is in one direction than if it's in the other. If they're skew in opposite directions, the type I error rate can be heavily affected.Heavy skewness can have bigger impacts, but generally speaking, moderate skewness with a two-tailed test isn't too bad if you don't mind your test in essence allocating more of its power to one direction that the other.In short - the two-tailed, two-sample t-test is reasonably robust to those kinds of things if you can tolerate some impact on the significance level and some mild bias.There are many, many, ways for distributions to be non-normal, though, which aren't covered by those comments.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2012-10-08 17:55:32","Question_id":38967}
{"_id":{"$oid":"5837a57da05283111e4d4e74"},"Last_activity":"2012-10-08 17:36:13","Creator_reputation":57712,"Question_score":15,"Answer_content":"Questions about robustness are very hard to answer well - because the assumptions may be violated in so many ways, and in each way to different degrees. Simulation work can only sample a very small portion of the possible violations.Given the state of computing, I think it is often worth the time to run both a parametric and a non-parametric test, if both are available. You can then compare results.If you are really ambitious, you could even do a permutation test.What if Alan Turing had done his work before Ronald Fisher did his? :-). ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2012-10-08 17:36:13","Question_id":38967}
{"_id":{"$oid":"5837a57da05283111e4d4e85"},"Last_activity":"2016-08-15 18:27:35","Creator_reputation":4307,"Question_score":0,"Answer_content":"I have not read the paper you cite, but according to Wikipedia:\"In complete-linkage clustering, the link between two clusters contains all element pairs, and the distance between clusters equals the distance between those two elements (one in each cluster) that are farthest away from each other.\"So in CLC the distance between two clusters is equal to some inter-element distance, and in your case inter-element distances are computed via EMD. As far as I can tell, this would be the straightforward application of EMD for CLC. However I am not sure that that this implies any particular notion of \"cluster center\". For a given inter-element distance metric, the cluster center will typically by optimizing some function of the element-to-\"center\" distances, but this could vary (e.g. a \"mean\" would minimize the sum of distances to all elements, but a \"midpoint\" would minimize the maximum distance). Independent of how it is defined, a cluster center will be of the same form as a cluster element. So here each cluster center would be a multidimensional distribution.From the quote you give, it seems like the paper may be doing a two-stage clustering. In stage 1, clusters are computed via CLC (using EMD for inter-element distances). Then they compute \"cluster centers\" for each of these clusters (how this is done is indeterminate from the information you provide). In stage 2, these centers are then used to re-cluster the data, where data points are assigned to the cluster whose center they are closest to (where \"close\" is again measured by EMD). ","Display_name":"GeoMatt22","Creater_id":127790,"Start_date":"2016-08-15 18:27:35","Question_id":229950}
{"_id":{"$oid":"5837a57da05283111e4d4e94"},"Last_activity":"2016-08-15 17:29:30","Creator_reputation":136,"Question_score":2,"Answer_content":"You mention two problems: 1. lack of convergence of the fitting procedure for truncate distributions; 2. choice of a suitable distribution for your data.Regarding point 1: the lack of convergence is in your case due to the fact that the \"true\" solution of the maximum likelihood optimization problem lies too far from the starting parameter values provided. You can see this by playing with the lower bound a of the truncation:## select a few lower boundsaa \u0026lt;- c(-10,-1,-0.5,-0.2,-0.15)## fit by MLE for the various lower boundsfits \u0026lt;- lapply(aa, function(a){    fitdist(testData, \"truncnorm\", fix.arg=list(a=a),            start = list(mean = mean(testData), sd = sd(testData)))})## fit by BFGS for lower bound a=0fit0.BFGS \u0026lt;- fitdist(testData, \"truncnorm\", fix.arg=list(a=0),                     start = list(mean = mean(testData), sd = sd(testData)),                     optim.method=\"L-BFGS-B\", lower=c(0, 0))## quantile-quantile plotting utility functionqqpl \u0026lt;- function(fit, lims=c(-0.8,3.5), del=0.05){    a \u0026lt;- fitestimate[\"mean\"]    fitsdev \u0026lt;- fitaic),3),                  collapse=\"\")    if(!is.null(fitdotsestimate))This often fixes the convergence problems. However, to address point 2 (which is the most important, IMHO), you should try several other distributions and see which fits the best. As discussed above, the previous attempts yield heavily truncated normal distributions: this suggests to try a Generalised Pareto Distribution (which is used to model upper tails): library(fExtremes)fit \u0026lt;- gpdFit(testData, u=0)##plot(fit, which=4)par(mfrow=c(1,2))lims \u0026lt;- c(0,3.5)qqpl(fit0.start, lims=lims)del \u0026lt;- 0.05probs \u0026lt;- seq(del, 1-del, by=del)qempir \u0026lt;- quantile(testData, probs)param \u0026lt;- attr(fit, \"fit\")llh,3) )plot(qempir, qtheor, xlim=lims, ylim=lims,     xlab=\"Empirical\", ylab=\"Theoretical\", main=tit)abline(a=0, b=1, lty=2)The GPD actually provides a MUCH better fit than the normal truncated at zero. The fitted tail parameter is not large (0.35), suggesting that decent results might also be obtained by a Weibull or a lognormal. One last remark: your data looks a bit \"suspicious\"!plot(testData)The \"funnelling\" for the second half of the dataset might indicate heterogeneity (i.e., the data comes from different generating processes). This might suggest a mixture model, but I would not attempt that without knowing more about the data (collection, generating process, etc.).","Display_name":"renato vitolo","Creater_id":126995,"Start_date":"2016-08-15 17:29:30","Question_id":229624}
{"_id":{"$oid":"5837a57da05283111e4d4ea1"},"Last_activity":"2016-08-15 17:22:04","Creator_reputation":4443,"Question_score":2,"Answer_content":"This is because for any fold, there are two stages: training and testing. For example, you are running 10 fold. Fold 1 training would print a plus sign and take a longer time. Fold 1 testing will print a minus sign and take shorter time. ","Display_name":"hxd1011","Creater_id":113777,"Start_date":"2016-08-15 17:22:04","Question_id":230006}
{"_id":{"$oid":"5837a57da05283111e4d4eae"},"Last_activity":"2016-08-15 16:48:29","Creator_reputation":5787,"Question_score":1,"Answer_content":"The entire probability distribution for X, namely P(X = i) for all values of i from 0 to N, can be obtained using a discrete-time (time-inhomogeneous) Markov Chain having  states. The states are:\"Initial state of 0 trials having been conducted\"For each i from  0 to N-1, there is a state for \"i trials have had value 1, with most recent trial being 0\"For each i from 0 to N-1, there is a state for \"i trials have had value 1, with most recent trial being 1\"There is a single state for \"N trials have had value 1\"The one-step transition matrix M is formed in the obvious fashion for the transitions between the states, based on the data you say is available.The Markov Chain is started in the initial state of 0 trials having been conducted, and is run for N steps.  The first row of  contains the probabilities of being in the various states after N trials. For each i from 0 to N-1, P(X = i) = sum of the entries in the first row of  for the states \"i trials have had value 1, with most recent trial being 0\" and \"i trials have had value 1, with most recent trial being 1\". P(X = N) = entry in the first row of  for the state \"N trials have had value 1\"Here is a sample formulation and results:Let N = 3, so there are 8 states, which I have ordered:\"Initial state of 0 trials having been conducted\"\"0 trials have had value 1, with most recent trial being 0\"\"0 trials have had value 1, with most recent trial being 1\"\"1 trial has had value 1, with most recent trial being 0\"\"1 trial has had value 1, with most recent trial being 1\"\"2 trials have had value 1, with most recent trial being 0\"\"2 trials have had value 1, with most recent trial being 1\"\"3 trials have had value 1\"In this example,let p = 0.5P(trial has same value as previous trial)  = dP(trial has different value than previous trial) = 1-dA value of d = 0.5 corresponds to independent (binomial) trials, d = 1 corresponds to perfectly correlated trials, and d = 0 corresponds to perfectly anti-correlated (i.e., flip-flopping) trials.Here is the single step transition matrix M for d = 0.8. 0    0.5000         0         0    0.5000         0         0         0 0    0.8000         0         0    0.2000         0         0         0 0    0.2000         0         0    0.8000         0         0         0 0         0         0    0.8000         0         0    0.2000         0 0         0         0    0.2000         0         0    0.8000         0 0         0         0         0         0    0.8000         0    0.2000 0         0         0         0         0    0.2000         0    0.8000 0         0         0         0         0         0         0    1.0000Here are the resulting probability distributions for X for values of d from 0 to 1 in 0.1 increments.  All the results make perfect sense to me.   d      P(X=0)    P(X=1)    P(X=2)    P(X=3)0.0000         0    0.5000    0.5000         00.1000    0.0050    0.4950    0.4950    0.00500.2000    0.0200    0.4800    0.4800    0.02000.3000    0.0450    0.4550    0.4550    0.04500.4000    0.0800    0.4200    0.4200    0.08000.5000    0.1250    0.3750    0.3750    0.12500.6000    0.1800    0.3200    0.3200    0.18000.7000    0.2450    0.2550    0.2550    0.24500.8000    0.3200    0.1800    0.1800    0.32000.9000    0.4050    0.0950    0.0950    0.40501.0000    0.5000         0         0    0.5000","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-15 08:51:33","Question_id":229931}
{"_id":{"$oid":"5837a57ea05283111e4d4eba"},"Last_activity":"2016-08-15 16:02:33","Creator_reputation":17414,"Question_score":0,"Answer_content":"The interpretation of a squared term is a difference in differences comparing groups differing in 1 unit of the outcome. For instance if the model is linear, and we fit  then  as well as . But if the true model is quadratic, then that difference is not constant among all values. Case 1: model misspecification, if the true model is  then . Kind of complicated, but you see the \"linear slope\" is now a linear function of the . But if you do a difference in differences for . So basically the  is the tangent slope of the quadratic curve at the origin, and  is a quadratic slope.With Poisson GLMs, the coefficients are exponentiated and interpreted as relative rates. So  would be called a ratio of ratio of rates comparing groups differing by 1 unit differing by 1 unit of . It's basically an interaction term.The only distinction in mixed models is the interpretation of individual level effects.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-15 16:02:33","Question_id":229959}
{"_id":{"$oid":"5837a57ea05283111e4d4ebb"},"Last_activity":"2016-08-15 13:05:09","Creator_reputation":12308,"Question_score":1,"Answer_content":"Going through your results a bit at a time:Random effects: Groups      Name        Variance    Std.Dev.  Corr  Firm:Industry (Intercept) 1.787e+04 1.337e+02                   Year          4.434e-03 6.659e-02 -1.00 Industry      (Intercept) 7.749e-01 8.803e-01                   Year          1.923e-07 4.385e-04 -1.00The very small standard deviations of Year, and the perfect negative correlation between year and intercept within each group, suggest that your model is overfitted. I would recommend dropping the Year term and reverting to an intercept-only model (although it probably won't hurt anything if you insist on leaving it there). The among-firm-within-industry standard deviation is very large (approx. 130); if this is really a Poisson regression with a log link, it suggests that your response variable consists of very large and variable numbers of counts. I would definitely check for outliers in your data, and strongly consider checking for overdispersion/fitting a negative-binomial model/if the counts are large enough, consider just a linear model on log-transformed data.The variation among Industries is much smaller than the variability at the level of Firm-within-Industry -- not negligible (it's larger than some of your effect sizes). Number of obs: 436, groups:  Firm:Industry, 109; Industry, 37This is a reasonable-sized (I would call it 'small to medium') data set for mixed modeling.Fixed effects:             Estimate Std. Error z value Pr(\u0026gt;|z|)    (Intercept) 51.697639   9.178129   5.633 1.77e-08 ***Year        -0.027132   0.004535  -5.983 2.19e-09 ***X            1.322702   0.334092   3.959 7.52e-05 ***Xsquared    -0.277335   0.141129  -1.965   0.0494 *  Size         0.321026   0.043825   7.325 2.39e-13 ***Everything is highly significant (except for your X-squared term, which is below the magic  but just barely!). The intercept is huge, but this is an artifact of the fact that you have Year in your model (probably) as years-since-AD 0; see below. When X=0, the effect of X on the log-counts is positive (1.32 log-counts per unit increase in X), but the curve decelerates/effect of X decreases with increasing X (the effect of X is zero, or the predicted curve of log-counts vs. X peaks, at ).Correlation of Fixed Effects:         (Intr) Year   DOI    DOI2  Year     -0.999                     DOI       0.004 -0.019              DOI2     -0.009  0.020 -0.953       LNAssets -0.162  0.119  0.003 -0.007You're probably putting year in as a Gregorian-calendar year (e.g. 2015); this puts the intercept of the model at AD 0, which results in a very strong correlation between the intercept and the Year slope. This probably isn't messing anything up, but it would be clearer and would reduce the chances of numerical problems to set cYear=Year-mean(Year) (or subtract the minimum value, or the median, or any other reasonable value that means the zero value of Year is somewhere near your data).For graphical solutions, try using predict() to plot predicted values from the model vs. X.","Display_name":"Ben Bolker","Creater_id":2126,"Start_date":"2016-08-15 13:05:09","Question_id":229959}
{"_id":{"$oid":"5837a57ea05283111e4d4ec8"},"Last_activity":"2016-08-15 14:49:59","Creator_reputation":17414,"Question_score":3,"Answer_content":"McNemar's test is for paired data but CMH is not. What kinds of differences are you looking for? If you're interested in heterogeneity of the prevalence of the outcome, fit a logistic model with site effects for the reference values only (this will give you independent data). If you're interested in heterogeneity in the agreements, just mutually compare the 95% CIs for the matched ORs or fit a conditional logistic regression model.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-15 14:49:59","Question_id":229969}
{"_id":{"$oid":"5837a57ea05283111e4d4ed5"},"Last_activity":"2016-08-15 15:29:15","Creator_reputation":1795,"Question_score":2,"Answer_content":"I think what the reviewers are asking is to take the pseudo--values and \"unbias\" them for number of samples in the quantile range, , and number of parameters in the model, . In other words, adjusted- in its usual context. That is that the corrected unexplained fraction is larger than the gross unexplained fraction by a factor of , i.e.,, or, I agree with you about taking things too far, because this is already a pseudo--value and an adjusted-pseudo--value might leave the reader with an impression of performing a pseudo-adjustment. One alternative is to do the calculations and show the reviewers what the results are and NOT include them in the paper, by explaining that it goes beyond what the published methods are that you are using and you do not want the responsibility for inventing an otherwise unpublished adjusted-pseudo- procedure. However, you should realize that the reason that the reviewers are asking is because they want assurances that they are not seeing gibberish numbers. Now, if you can think of another way of doing exactly that, assuring the reviewer(s) that the results are reliable, then the problem should go away... One alternative is to include more references or information about the pseudo- values you are using, especially if you can show robustness, or precision. For example A Lack-of-Fit Test for Quantile Regression. Are the pseudo- values essential to the paper, or are there other ways to accomplish the same goal? Sometimes, just deleting the problem is the simplest thing to do. Yes, we agree with you, exalted reviewer, your majestic  infallibility is worshiped, grovel, grovel problem deleted.","Display_name":"Carl","Creater_id":99274,"Start_date":"2016-08-12 12:57:20","Question_id":222084}
{"_id":{"$oid":"5837a57ea05283111e4d4ed6"},"Last_activity":"2016-07-12 23:35:49","Creator_reputation":17,"Question_score":0,"Answer_content":"You had better use  to compare two quantile regression models, because the quantile regression model's loss function is not base on MSE. You can try AIC or BIC.","Display_name":"Tony Huang","Creater_id":111861,"Start_date":"2016-07-12 23:35:49","Question_id":222084}
{"_id":{"$oid":"5837a57ea05283111e4d4ee3"},"Last_activity":"2016-08-15 15:28:31","Creator_reputation":21,"Question_score":2,"Answer_content":"First, note that observed variables and latent variables both have probability distributions, parameters are fixed. A helpful example can be found in Koller and Friedman's PGM textbook (see below). Note that incorporating the latent variable H in the left-hand model reduces the parameter space of the overall graphical model. An I-equivalent graph can be drawn without the latent variable H (as in the right-hand model), but it may require many more parameters than a model that incorporates latent variables. Choosing between the two is a modeling decision (that can come down to statistical vs. computational simplicity). H in the right-hand model need not have any physical representation, but it may be included or removed in the model for interpretability, or other requirements of the problem (e.g., sampling, inference). That is, there are often context-specific trade-offs that need to be made in determining the graph's structure. Hope this helps! ","Display_name":"blanca","Creater_id":127774,"Start_date":"2016-08-15 15:28:31","Question_id":49061}
{"_id":{"$oid":"5837a57ea05283111e4d4ee4"},"Last_activity":"2013-03-03 01:19:12","Creator_reputation":51,"Question_score":2,"Answer_content":"The only reasonable answer to me seems that latent variables are the parameters of a distribution written as they were real variables, while they haven't any physical interpretation.Bishop is always very precise and clear, I wonder why this time he didn't use the single word \"parameters\", that would have been enlightening.","Display_name":"Gino Strato","Creater_id":20359,"Start_date":"2013-03-03 01:19:12","Question_id":49061}
{"_id":{"$oid":"5837a57ea05283111e4d4ef1"},"Last_activity":"2016-08-15 15:13:21","Creator_reputation":738,"Question_score":1,"Answer_content":"For a simple linear regression model with 1 covariate, I've heard the rule of thumb being no less than 30 observations. However, this can be flexible depending on how well the data fits compared to a  distribution. But for discussion purposes, I recommend the following paper:http://sites.stat.psu.edu/~ajw13/stat501/SpecialTopics/SampleSizes_MLR.pdfPlease refer to tables 1 and 2 in the paper.","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-15 15:13:21","Question_id":58596}
{"_id":{"$oid":"5837a57ea05283111e4d4efe"},"Last_activity":"2016-08-15 14:55:32","Creator_reputation":738,"Question_score":2,"Answer_content":"Since the data is ordinal (not continuous) and does not follow a Normal distribution, I recommend using a Wilcoxon Rank Sum test (aka Mann–Whitney U test) instead of a t-test. Wilcoxon Rank Sum test is a nonparametric approach to the t-test.You can find more information about the assumptions here:https://statistics.laerd.com/spss-tutorials/mann-whitney-u-test-using-spss-statistics.phpIn R, you can implement the test with the following commands. Example:\u0026gt; fake_data \u0026lt;- rpois(n = 100, lambda = 5)\u0026gt; wilcox.test(x = fake_data, mu = 3.5)Wilcoxon signed rank test with continuity correctiondata:  fake_dataV = 3986, p-value = 4.204e-07alternative hypothesis: true location is not equal to 3.5## reject H_0: true location is equal to 3.5","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-15 14:49:40","Question_id":229995}
{"_id":{"$oid":"5837a57ea05283111e4d4f0b"},"Last_activity":"2016-08-15 15:12:06","Creator_reputation":21,"Question_score":2,"Answer_content":"The Nash-Sutcliffe model efficiency coefficient is nearly identical to the coefficient of determination. The primary difference is how it is used.The coefficient of determination () is a measure of the goodness of fit of a statistical model. \\begin{equation}\\begin{aligned}R^2 = 1 - \\frac{\\sum (y_i - \\hat{y_i})^2}{\\sum (y_i - \\bar{y})^2}\\end{aligned}\\end{equation}Where  are the observed values of the variable of interest,  are the predicted values, and  is the mean of the observations. For example, if we have a set of obervations  and , we might assume a linear model  to predict this relationship, resulting in set of predicted values, . The smallest  occurs when there is no correlation between  and  and the best prediction is to assume  and . This corresponds to an  value of 0, which is the lower limit of  because the the sum of squares of the rediduals, , will never be greater than the total sum of squares, .The Nash-Sutcliffe model efficiency coefficient () is used to quantify how well a model simulation can predict the outcome variable. \\begin{equation}\\begin{aligned}E = 1 - \\frac{\\sum (y_i - y_{i,sim})^2}{\\sum (y_i - \\bar{y})^2}\\end{aligned}\\end{equation}The variables are the same as described above, but  are the predictions from the simulation (instead of the  from a statistical model). The model may be calibrated, but the predicted values of the outcome variable  are not inferred from the observed values.Unlike with a statistical model, the sum of squares of the model error, , may be greater than the total sum of squares, , and the coefficient can therefore be negative.","Display_name":"Gopal Penny","Creater_id":127777,"Start_date":"2016-08-15 15:05:59","Question_id":185898}
{"_id":{"$oid":"5837a57ea05283111e4d4f0c"},"Last_activity":"2016-05-02 17:33:58","Creator_reputation":1,"Question_score":0,"Answer_content":"I don't think there is any ... looks same to me","Display_name":"Abdul Raheem","Creater_id":114341,"Start_date":"2016-05-02 17:33:58","Question_id":185898}
{"_id":{"$oid":"5837a57ea05283111e4d4f1c"},"Last_activity":"2016-08-15 14:36:41","Creator_reputation":5787,"Question_score":3,"Answer_content":"Your linked question is addressing using weights as a shortcut for dealing with equally weighted per data point variance in which some data points occur more than once.@whuber has addressed in a comment the situation in which the variances of all data points are equal. So I will address the situation in which they are not equal.  It is in this situation that the optimal weighted mean produces a lower variance than the unweighted, i.e., equally weighted, mean.The weighted mean, using weights , equals , and has variance = .  So we wish to minimize , subject to  and  for all i.The Karush-Kuhn-Tucker conditions, which are necessary and sufficient for a global minimum for this problem, given that it is a convex Quadratic Programming problem, result in a closed form solution, namely:The optimal   for 1 = 1 .. n. The variance of the corresponding optimal weighted mean = .By contrast, equal weighting means  for all i, where n is the number of data points. As pointed out by whuber, equal weights are optimal if all data point variances are equal, which can be seen from the above formula for optimal . However, as evident by that formula, equal weights are not optimal if the data point variances are not all equal, and indeed result in larger variance (of the weighted mean) than the optimal weights. The  variance of the equally weighted mean, i.e., the variance of the weighted mean using equal weights = .Here are some example numerical results:There are two data points, having variances respectively of 1 and 4.  The unweighted mean has variance = 1.25.  The weighted mean using the optimal weights of 0.8 and 0.2 respectively, has variance = 0.8, which of course is less than 1.25.There are three data points, having variances respectively of 1, 4,and 9.  The unweighted mean has variance = 1.5556.  The weighted meanusing the optimal weights of 0.7347, 0.1837, 0.0816 respectively, hasvariance = 0.7347, which of course is less than 1.5556.Of course, it is possible for the weighted mean to have a greater variance than the unweighted mean, if the weights are chosen in a poor manner. By choosing weight of 1 on the data point with largest variance, and 0 for all other data points, the weighted mean would have variance = the largest variance of any data point. This extreme example would be the result of maximizing rather than minimizing in the optimization problem I laid out.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-14 14:55:50","Question_id":229781}
{"_id":{"$oid":"5837a57ea05283111e4d4f1d"},"Last_activity":"2016-08-14 09:24:48","Creator_reputation":13288,"Question_score":0,"Answer_content":"Here is a simple example using the  and  forms of the variance: Suppose your population has measurements . Unweighted the mean is  and variance is   With respective weights  the weighted mean is  and the weighted variance is With respective weights  the weighted mean is  and the weighted variance is This example is consistent with my comment that the your statistician's quote is likely to be true for a population with a unimodal distribution, though it need not be true in general. I suppose the point is that if you are quoting the weighted mean, you should probably be associating it with the weighted variance.  If in fact your mean is the result of the sample, the standard error of the weighted sample mean is a more complicated calculation. ","Display_name":"Henry","Creater_id":2958,"Start_date":"2016-08-14 09:24:48","Question_id":229781}
{"_id":{"$oid":"5837a57ea05283111e4d4f30"},"Last_activity":"2016-08-15 10:52:02","Creator_reputation":11523,"Question_score":3,"Answer_content":"To clarify notation, lets start with the equation:E[Y] = \\beta_0 \\cdot Z^{\\beta_1} \\cdot \\exp( \\beta_n X_n)Where  is just short hand for all of your other independent variables. Lets take the logs of each side, so we are simply talking about the linear predictors on the right hand side:\\log ( E[Y] ) = \\beta_n X_n + \\log(\\beta_0 Z^{\\beta_1})Note it only makes sense for the exposure term to be positive, and so  can be rewritten as . Since  is just a constant, if you include an intercept in the usual regression equation it will capture this (but not be identified). To estimate  you can simply include  on the right hand side of the regression equation and estimate its effect - same as all the other independent variables on the right hand side.","Display_name":"Andy W","Creater_id":1036,"Start_date":"2016-08-15 10:38:09","Question_id":229942}
{"_id":{"$oid":"5837a57ea05283111e4d4f3f"},"Last_activity":"2016-08-15 09:13:11","Creator_reputation":25370,"Question_score":1,"Answer_content":"Error measures are based on residuals. If you denote  as predicted variable and  as your estimate for it, then we define residuals as r = y - \\hat y Error measures such as MAE, or RMSE are defined as  and . If you take  as your prediction, then what you are doing is you take y - y - \\hat y = -\\hat y as your \"residuals\". So MAE becomes  and RMSE becomes , this applies also to other error measures. This means that you are not measuring error at all.So by predicting all zeros, you would conclude that they perfectly fit to any data (MAE = RMSE = 0). Small predictions would lead to small \"errors\" and vice versa. You can easily extrapolate this example to other cases.If you say that this approach works for you in financial setting then this means that betting on random noise is better then being on predictions from your model. If regression assumptions are met then residuals are random around zero and there is no trend in them. So if this works then either you use regression for data that does not qualify for it, or your model is really poor.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-14 19:37:32","Question_id":229846}
{"_id":{"$oid":"5837a57ea05283111e4d4f4e"},"Last_activity":"2016-08-15 12:54:47","Creator_reputation":7682,"Question_score":2,"Answer_content":"  \"I want to create a code for plotting ACF and PACF from time-series  data\".Although the OP is a bit vague, it may possibly be more targeted to a \"recipe\"-style coding formulation than a linear algebra model formulation. The ACF is rather straightforward: we have a time series, and basically make multiple \"copies\" (as in \"copy and paste\") of it, understanding that each copy is going to be offset by one entry from the prior copy, because the initial data contains  data points, while the previous time series length (which excludes the last data point) is only . We can make virtually as many copies as there are rows. Each copy is correlated to the original, keeping in mind that we need identical lengths, and to this end, we'll have to keep on clipping the tail end of the initial data series to make them comparable. For instance, to correlate the initial data to  we'll need to get rid of the last  data points of the original time series (the first  chronologically).Example:We'll concoct a times series with a cyclical sine pattern superimposed on a trend line, and noise, and plot the R generated ACF. I got this example from an online post by Christoph Scherber, and just added the noise to it:x=seq(pi, 10 * pi, 0.1)y = 0.1 * x + sin(x) + rnorm(x)y = ts(y, start=1800)Ordinarily we would have to test the data for stationarity (or just look at the plot above), but we know there is a trend in it, so let's skip this part, and go directly to the de-trending step:model=lm(y ~ I(1801:2083))st.y = y - predict(model)Now we are ready to takle this time series by first generating the ACF with the acf() function in R, and then comparing the results to the makeshift loop I put together:ACF = 0                  # Starting an empty vector to capture the auto-correlations.ACF[1] = cor(st.y, st.y) # The first entry in the ACF is the correlation with itself (1).for(i in 1:30){          # Took 30 points to parallel the output of `acf()`  lag = st.y[-c(1:i)]    # Introducing lags in the stationary ts.  clipped.y = st.y[1:length(lag)]    # Compensating by reducing length of ts.  ACF[i + 1] = cor(clipped.y, lag)   # Storing each correlation.}acf(st.y)                            # Plotting the built-in function (left)plot(ACF, type=\"h\", main=\"ACF Manual calculation\"); abline(h = 0) # and my results (right).OK. That was successful. On to the PACF. Much more tricky to hack... The idea here is to again clone the initial ts a bunch of times, and then select multiple time points. However, instead of just correlating with the initial time series, we put together all the lags in-between, and perform a regression analysis, so that the variance explained by the previous time points can be excluded (controlled). For example, if we are focusing on the PACF ending at time , we keep , ,  and , as well as , and we regress  through the origin and keeping only the coefficient for :PACF = 0          # Starting up an empty storage vector.for(j in 2:25){   # Picked up 25 lag points to parallel R `pacf()` output.  cols = j          rows = length(st.y) - j + 1 # To end up with equal length vectors we clip.  lag = matrix(0, rows, j)    # The storage matrix for different groups of lagged vectors.for(i in 1:cols){  lag[ ,i] = st.y[i : (i + rows - 1)]  #Clipping progressively to get lagged ts's.}  lag = as.data.frame(lag)  fit = lm(lag$V1 ~ . - 1, data = lag) # Running an OLS for every group.  PACF[j] = coef(fit)[j - 1]           # Getting the slope for the last lagged ts.}And finally plotting again side-by-side, R-generated and manual calculations:That the idea is correct, beside probable computational issues, can be seen comparing PACF to pacf(st.y, plot = F).code here.","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-08-14 21:22:29","Question_id":129052}
{"_id":{"$oid":"5837a57ea05283111e4d4f4f"},"Last_activity":"2016-06-20 02:11:50","Creator_reputation":11,"Question_score":1,"Answer_content":"Well, in the practise we found error (noise) which is represented by  the confidence bands help you to figure out if a level can be considerate as only noise (because about the 95% times will be into the bands).","Display_name":"user120580","Creater_id":120580,"Start_date":"2016-06-20 02:11:50","Question_id":129052}
{"_id":{"$oid":"5837a57ea05283111e4d4f50"},"Last_activity":"2014-12-16 14:24:26","Creator_reputation":6503,"Question_score":8,"Answer_content":"AutocorrelationsThe correlation between two variables  is defined as:\\rho = \\frac{\\hbox{E}\\left[(y_1-\\mu_1)(y_2-\\mu_2)\\right]}{\\sigma_1 \\sigma_2} =\\frac{\\hbox{Cov}(y_1, y_2)}{\\sigma_1 \\sigma_2} \\,,where E is the expectation operator,  and  are the means respectively for  and  and  are their standard deviations.In the context of a single variable, i.e. auto-correlation,  is the original series and  is a lagged version of it. Upon the above definition, sample autocorrelations of order  can be obtained by computing the following expression with the observed series , :\\rho(k) = \\frac{\\frac{1}{n-k}\\sum_{t=k+1}^n (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sqrt{\\frac{1}{n}\\sum_{t=1}^n (y_t - \\bar{y})}\\sqrt{\\frac{1}{n-k}\\sum_{t=k+1}^n (y_{t-k} - \\bar{y})}} \\,,where  is the sample mean of the data.Partial autocorrelationsPartial autocorrelations measure the linear dependence of one variable after removing the effect of other variable(s) that affect to both variables. For example, the partial autocorrelation of order measures the effect (linear dependence) of  on  after removing the effect of  on both  and .Each partial autocorrelation could be obtained as a series of regressions of the form:\\tilde{y}_t = \\phi_{21} \\tilde{y}_{t-1} + \\phi_{22} \\tilde{y}_{t-2} + e_t \\,,where  is the original series minus the sample mean, . The estimate of  will give the value of the partial autocorrelation of order 2. Extending the regression with  additional lags, the estimate of the last term will give the partial autocorrelation of order . An alternative way to compute the sample partial autocorrelations by solving the following system for each order :\\begin{eqnarray}\\left(\\begin{array}{cccc}\\rho(0) \u0026amp; \\rho(1) \u0026amp; \\cdots \u0026amp; \\rho(k-1) \\\\\\rho(1) \u0026amp; \\rho(0) \u0026amp; \\cdots \u0026amp; \\rho(k-2) \\\\\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\\rho(k-1) \u0026amp; \\rho(k-2) \u0026amp; \\cdots \u0026amp; \\rho(0) \\\\\\end{array}\\right)\\left(\\begin{array}{c}\\phi_{k1} \\\\\\phi_{k2} \\\\\\vdots \\\\\\phi_{kk} \\\\\\end{array}\\right)= \\left(\\begin{array}{c}\\rho(1) \\\\\\rho(2) \\\\\\vdots \\\\\\rho(k) \\\\\\end{array}\\right) \\,,\\end{eqnarray}where  are the sample autocorrelations. This mapping between the sample autocorrelations and the partial autocorrelations is known as the Durbin-Levinson recursion. This approach is relatively easy to implement for illustration. For example, in the R software, we can obtain the partial autocorrelation of order 5 as follows:# sample datax \u0026lt;- diff(AirPassengers)# autocorrelationssacf \u0026lt;- acf(x, lag.max = 10, plot = FALSE)acf[,,1]res2# [1]  0.30285526 -0.21344644 -0.16044680 -0.22163003  0.01008379all.equal(res1[5], res2[5])# [1] TRUEConfidence bandsConfidence bands can be computed as the value of the sample autocorrelations , where  is the quantile  in the Gaussian distribution, e.g. 1.96 for 95% confidence bands.Sometimes confidence bands that increase as the order increases are used. In this cases the bands can be defined as .","Display_name":"javlacalle","Creater_id":48766,"Start_date":"2014-12-16 13:47:00","Question_id":129052}
{"_id":{"$oid":"5837a57ea05283111e4d4f5d"},"Last_activity":"2014-02-25 03:20:30","Creator_reputation":57712,"Question_score":4,"Answer_content":"The sampling weights are designed to account for the non-simple random sample nature of your sample. Therefore, they are just as needed in one form of regression as another. Exactly how to do this may be complicated; e.g. in SAS there is PROC SURVEYLOGISTIC to deal with various sorts of samples. In R there is the survey package which I think does similar things (but I have not used it). ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2014-02-25 03:20:30","Question_id":87779}
{"_id":{"$oid":"5837a57ea05283111e4d4f6a"},"Last_activity":"2016-08-15 12:50:32","Creator_reputation":21588,"Question_score":2,"Answer_content":"Good question; I see that you are planning an implementation, and are concerned about the storage requirements. I suspect there is no flat  answer possible, as it would depend on the distribution of weights. If all weights are the same, approximately all items are competitive. If there are  large weights (say 100) and  small weights (say 1), then whenever a high-weight item enters the pool, its key is nearly always competitive, overshadowing a small-weight item. Thus the first  items would be small, and keep fighting each other; but as soon as large items begin entering the sample, the competition for the lowest key whittles down. So my conjecture is, the number of competitive items can be anywhere from  to .","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-15 12:50:32","Question_id":133132}
{"_id":{"$oid":"5837a57ea05283111e4d4f77"},"Last_activity":"2016-08-15 12:42:40","Creator_reputation":75925,"Question_score":6,"Answer_content":"Although you are modeling counts, your data cannot be Poisson.  The easy way to see this is because it cannot be , which is a possible value for the Poisson distribution.  But this isn't really a shifted or truncated Poisson, either.  Nor could the number of resits be Poisson.  What you are calling \"the number of resits\" could equally well be called \"the number of failures until  successes\".  That is the definition of a negative binomial.  In your case, .  In short, I would model \"the number of resits\" using a regression model with a negative binomial distribution.  Typically with a regression model, the predicted values should be the expected value at each point in the covariate space.  Note that the expected value of a negative binomial is:E[Y] = \\frac{(1-p)r}{p}where  is the probability of success.  Since  for you, the predicted values are the odds of failing a test.  You could convert that to the probability of passing a test via .  It's actually a little more complicated than that in practice, unfortunately.  First, as you probably know, models that use something other than the normal distribution for the response (e.g., the negative binomial), need to use a link function.  So to get to what I'm calling , you will need to exponentiate the values computed from the model equation.  Next, 'using a negative binomial distribution' sounds like you need negative binomial regression.  Actually, that is a slightly different animal from what we have here.  Negative binomial regression estimates not only the regression coefficients but also a dispersion parameter.  We know the appropriate parameter value for your case a-priori.  What you need is called the geometric model.  Note also that there are different ways of specifying and parameterizing these things, so you need to read the documentation very carefully.  I can walk through a simple example using R.  Let's compare people who did not take a prep class, x = 0, to those who did, x = 1.  We will imagine that those who didn't take a prep class have only a 30% chance of passing, but those who did have a 70% chance.  library(MASS)   # you need this library for the negative.binomial() function belowset.seed(1082)  # this makes the example exactly reproducibley = c(rnbinom(1000, size=1, prob=.3),   # number of resits for those who didn't       rnbinom(1000, size=1, prob=.7) )  #   or did take prep classx = rep(c(0,1), each=1000)              # prep class indicatorm = glm(y~x, family=negative.binomial(theta=1))summary(m)# Call:# glm(formula = y ~ x, family = negative.binomial(theta = 1))# # Deviance Residuals: #     Min       1Q   Median       3Q      Max  # -1.5805  -0.8358  -0.6336   0.4447   3.3044  # # Coefficients:#             Estimate Std. Error t value Pr(\u0026gt;|t|)    # (Intercept)  0.91108    0.03883   23.46   \u0026lt;2e-16 ***# x           -1.78335    0.07180  -24.84   \u0026lt;2e-16 ***# ---# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1# # (Dispersion parameter for Negative Binomial(1) family taken to be 1.075368)# #     Null deviance: 2646  on 1999  degrees of freedom# Residual deviance: 1913  on 1998  degrees of freedom# AIC: 5902.8# # Number of Fisher Scoring iterations: 4cm = c(coef(m)[1], sum(coef(m)) )1 - ( exp(cm)/(1 + exp(cm)) )  # the model's estimated probabilities of passing the test# (Intercept)             #   0.2867795   0.7052186 ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-08-12 16:13:07","Question_id":229547}
{"_id":{"$oid":"5837a57ea05283111e4d4f78"},"Last_activity":"2016-08-12 08:23:15","Creator_reputation":19111,"Question_score":2,"Answer_content":"You have to use the distribution that explains the phenomenon. Sometime a simple rewording can do the trick of thinking outside the box. For instance, instead of framing the question as \"how many attempts to pass?\" you frame it as \"How many re-takes?\".You see, in your case it's impossible to have an observations of a \"number of attempts\" variable less that 1. Hence, Poisson is simply not a good fit at all. However, if you frame the question the way I did, you observations of \"number of retakes\" variable include 0, and Poisson could be reasonable distribution to consider. You're not truncating anything or shifting, you are answering a different question about a different variable, namely, a \"number of re-takes\" not attempts like in the original question.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-08-12 08:23:15","Question_id":229547}
{"_id":{"$oid":"5837a57ea05283111e4d4f87"},"Last_activity":"2016-08-15 12:37:38","Creator_reputation":21588,"Question_score":1,"Answer_content":"Sampling with replacement is boring. Sampling without replacement is very interesting. That's why the authors of library(sampling) restricted their attention to sampling WOR. So inclusionprobabilities() takes the baseline rates in your y, and figure out what would the inclusion probabilities be should a proper unequal probability WOR sampling algorithm applied to these numbers.Looking at the source code, I imagine that your snippet of code reproduces the \"regular\" case of inclusionprobabilities() when none of the inclusion probabilities exceed 1. In that regular case, the inclusion probabilities are simply the input probabilities scaled up so that their sum is equal to the target sample size. Note that inclusion probabilities refer to the units on the frame, rather than the specific samples, as your code does.For sampling with replacement, I believe your calculations are correct, in that probability of each pair is the product of probabilities. Then what inclusionprobabilities refers to are the sums across all rows where either X1 or X2 are equal to 1, 2, 3 or 4 (the indices of the original units):for(k in 1:4) {  print(sum(dfX1==k|df$X2==k]))}This is to say, unit 1 appears in 1.8% of the samples, while unit 3, in 77.3% of the samples. However, these numbers sum up neither to 1 (as base probabilities should) nor to 2 (as correct inclusion probabilities should), and so they are kinda weird, in the end.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-15 12:37:38","Question_id":134624}
{"_id":{"$oid":"5837a57ea05283111e4d4f96"},"Last_activity":"2015-04-13 23:24:21","Creator_reputation":15542,"Question_score":4,"Answer_content":"svr is a user-written alternative to Stata's native svy, which uses Taylor series linearization. The later can now accommodate multilevel mixed-effects complementary log-log regression, GLMs, vanilla and ordered logistic/probit, Poisson and negative binomial regression, and parametric survival analysis models. You can read about replication weights in the svy manual.","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2015-04-13 23:09:45","Question_id":145930}
{"_id":{"$oid":"5837a57ea05283111e4d4fa3"},"Last_activity":"2016-08-15 11:48:04","Creator_reputation":21588,"Question_score":1,"Answer_content":"If people say they have post-stratified weights, it does not necessarily mean they implemented post-stratification, proper (as in, rescaled the weights in each demographic cell to the known population total). About 80% of usage that I hear of \"post-stratified weights\" actually refers to calibrated weights (i.e., rather than trying to adjust each and every cell in a five-way table, the weights are only adjusted to match each of the five variables of the table individually). I produced what somebody referred to as a methodological rant on the distinction. The distinction, however, plays a role in standard error calculations, as Anthony noted in another answer. With properly post-stratified weights, you can apply the regular variance estimation formulae, more or less treating your post-strata as sampling strata (minor technicalities aside). With weights that are only calibrated on each table margin, computations are somewhat more involved. Either one is internalized in survey package, anyway, though. You just need to feed your post-stratification/calibration variables to the appropriate design object/formula.library(survey)data(api)# cross-classified post-stratification variable in populationapipopstype) +as.integer(apipopstype.sch.wide \u0026lt;-   10*as.integer(apiclus1sch.wide)# population totals(pop.totals \u0026lt;- xtabs(~stype.sch.wide, data=apipop))# reference designdclus1 \u0026lt;- svydesign(id=~dnum,weights=~pw,data=apiclus1,fpc=~fpc)# post-stratification of the original designdclus1p \u0026lt;- postStratify(dclus1,~stype.sch.wide, pop.totals)# design with post-stratified weights, but no evidence of post-stratificationdclus1pfake \u0026lt;- svydesign(id=~dnum,weights=~weights(dclus1p),data=apiclus1,fpc=~fpc)# taking off the design with known weights, add post-stratification interactiondclus1pp \u0026lt;- postStratify(dclus1pfake,~stype.sch.wide, pop.totals)# estimates and standard errors: starting pointsvymean(~api00,dclus1)# post-stratification reduces standard errors a bitsvymean(~api00,dclus1p)# but here we are not aware of the survey being post-stratifiedsvymean(~api00,dclus1pfake)# if we just add post-stratification variables to the design object# that only had post-stratified weights, the result is the same# as for post-stratified object based on the original weightssvymean(~api00,dclus1pp)","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-15 11:48:04","Question_id":146841}
{"_id":{"$oid":"5837a57ea05283111e4d4fa4"},"Last_activity":"2015-04-17 22:57:55","Creator_reputation":417,"Question_score":2,"Answer_content":"bad news :)  your standard errors, confidence intervals, and tests of significance will be incorrect if you do not account for the relationship between the original and post-stratified weights.i believe you can back-calculate the original sampling weights if you have the sampling clusters (although you'd have to invest a lot of time reversing the method in the postStratify command).rather than spending more money, ask whoever creates this data to provide both sets of weights.  this is information that the original survey administrator has, and can send to you for the price of an e-mail.","Display_name":"Anthony Damico","Creater_id":16939,"Start_date":"2015-04-17 22:57:55","Question_id":146841}
{"_id":{"$oid":"5837a57ea05283111e4d4fb1"},"Last_activity":"2016-08-15 10:37:19","Creator_reputation":175,"Question_score":2,"Answer_content":"First, scale all the differences to obtain iid observations to estimate . For instance, If we denote scaled observations as , the estimate of  is:Then you can get a CI using that This approach ignores the uncertainty in  estimation. If you want to take that into account as well, you'll have to use simulation.","Display_name":"Nik Tuzov","Creater_id":123427,"Start_date":"2016-08-15 10:37:19","Question_id":229953}
{"_id":{"$oid":"5837a57ea05283111e4d4fbf"},"Last_activity":"2016-08-15 11:16:44","Creator_reputation":738,"Question_score":1,"Answer_content":"Variances do not look equal, this can violate the t-test assumptions. See below from SPSS documentation:  Assumption #6: There needs to be homogeneity of variances. You can  test this assumption in SPSS Statistics using Levene’s test for  homogeneity of variances. In our enhanced independent t-test guide, we  (a) show you how to perform Levene’s test for homogeneity of variances  in SPSS Statistics, (b) explain some of the things you will need to  consider when interpreting your data, and (c) present possible ways to  continue with your analysis if your data fails to meet this assumption  https://statistics.laerd.com/spss-tutorials/independent-t-test-using-spss-statistics.phpIn R you can test homogeneity of variances via library(car)leveneTestI recommend you use pairwise.wilcox.test for nonparametic approach of testing equality of central values for multiple groups.","Display_name":"Jon","Creater_id":82338,"Start_date":"2016-08-15 11:16:44","Question_id":229944}
{"_id":{"$oid":"5837a57ea05283111e4d4fc0"},"Last_activity":"2016-08-15 10:38:58","Creator_reputation":324,"Question_score":0,"Answer_content":"Would recommend using a T-Test for comparison of means for each individual comparison against the Med0 group: T_k = \\frac{\\bar{X}_2 - \\bar{X}_1}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}Where\\qquad k = \\min( n_1 - 1, n_2 - 1)\\qquad s^2 = \\frac{\\sum (X_{i} - \\bar{X})^2}{n-1}This can be done in R with the following command:t.test(Med0,Med1) ","Display_name":"David C","Creater_id":96030,"Start_date":"2016-08-15 09:38:58","Question_id":229944}
{"_id":{"$oid":"5837a57ea05283111e4d4fcc"},"Last_activity":"2016-08-15 10:56:30","Creator_reputation":825,"Question_score":3,"Answer_content":"Hope this will help (the section 2 of the paper)\"Variable Selection in Random Forest with Application to Quantitative Structure-Activity Relationship\", Vladimir Svetnik, Andy Liaw, and Christopher Tong, Biometrics Research ","Display_name":"Metariat","Creater_id":78313,"Start_date":"2015-07-17 06:14:02","Question_id":161733}
{"_id":{"$oid":"5837a57ea05283111e4d4fdb"},"Last_activity":"2016-08-15 10:46:45","Creator_reputation":21588,"Question_score":3,"Answer_content":"You can compare variances from first principles, i.e., by calculating the variance as the difference between value-squared and mean-squared.webuse nhanes2, cleargen bpsystol_sq = bpsystol* bpsystolsvy : mean bpsystol*, over( female )* estimated variance in group 0nlcom ( _b[bpsystol_sq:0] - _b[bpsystol:0]*_b[bpsystol:0])* estimated variance in group 1nlcom ( _b[bpsystol_sq:1] - _b[bpsystol:1]*_b[bpsystol:1])* equality testtestnl ( _b[bpsystol_sq:1] - _b[bpsystol:1]*_b[bpsystol:1]) ///     = ( _b[bpsystol_sq:0] - _b[bpsystol:0]*_b[bpsystol:0])","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-15 10:46:45","Question_id":148314}
{"_id":{"$oid":"5837a57ea05283111e4d4fdc"},"Last_activity":"2015-10-07 07:52:54","Creator_reputation":26,"Question_score":2,"Answer_content":"In R you can calculate weighted variance for each group using function \"wt.var\" from the package \"SDMTools\".As a result you'll have two weighted variances - var1 and var2F statistic equals var1/var2And the degrees of freedom for that F value are WEIGHTED n1 - 1 and WEIGHTED n2 - 1.Done","Display_name":"user2323534","Creater_id":38098,"Start_date":"2015-10-07 07:52:54","Question_id":148314}
{"_id":{"$oid":"5837a57ea05283111e4d4fe9"},"Last_activity":"2016-08-15 10:41:37","Creator_reputation":21588,"Question_score":0,"Answer_content":"If you are willing to assume that the vocabulary is independent of the text structure, then you can add up the  you are getting from the word distribution to the  from sentence length distribution to the  from dialogue-narration ratio. Be mindful of unique words -- Pearson test assumes that cell sizes are sufficiently large.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-15 10:41:37","Question_id":154156}
{"_id":{"$oid":"5837a57ea05283111e4d4ff8"},"Last_activity":"2015-06-18 06:07:20","Creator_reputation":172,"Question_score":0,"Answer_content":"You could provide a scaling variable for each individual dataset.  Or, you could analyze the log of the deviations.","Display_name":"Maddenker","Creater_id":78846,"Start_date":"2015-06-18 06:07:20","Question_id":157208}
{"_id":{"$oid":"5837a57ea05283111e4d5007"},"Last_activity":"2015-07-17 07:29:29","Creator_reputation":21588,"Question_score":1,"Answer_content":"If you are working with a typical labor force survey (LFS), chances are it has more than just the weights in the mix, like stratification and clustering. These have to be accounted for appropriately. R's survey package can create the bootstrap weights that account for these aspects of the survey; I doubt boot package does this properly. That said, you may have to code Oaxaca-Binder decomposition from scratch using the results from svyglm.To my amazement and despair, econometric references often give technically incorrect advice on how to deal with complex survey data. It seems like few if any econometricians have ever taken a course in sampling, and understand finite population inference, and why survey statisticians create all these complications of weights, clusters and strata. (The answer is, to make the damn thing work at all; it is all nice to assume rational expectations for a dissertation paper, but in the real world, you have to deal with limited budgets, non-existent lists of observation units, and requests to optimize the survey for this and that and yet another statistic.) (To be fair, that's a complete rehaul of one's thinking; econometricians are all about models, while finite population inference is all about how to avoid models and deal with non-parametric sampling inference; it took me some 2-3 years to get into the groove; I started as an econometrician, and now do surveys for living.) For a 50-page introduction, you can take a look at my chapter in Handbook of Health Survey Methods. For a book length treatment, you would want to start with Heeringa, West and Berglund (2010) or Lumley (2010).As a side comment, there may be better decompositions available on the market. ","Display_name":"StasK","Creater_id":5739,"Start_date":"2015-07-17 07:29:29","Question_id":157777}
{"_id":{"$oid":"5837a57ea05283111e4d5014"},"Last_activity":"2016-08-15 10:21:56","Creator_reputation":21588,"Question_score":1,"Answer_content":"What DHS does with weights is beyond me. I think their intent with division by 100,000 is to make weights sum up to the nominal sample size of 9,000. But this is an awkward scale of weights. The proper scale should be the population of the country (or, rather, as DHS surveys the specific population of women in their fertile ages, total number of women aged 15-49). ICF computes the weights properly stringing the probabilities of selection, but chooses to destroy the scale. Ah well. So as your first step, you would need to scale the weights in the DHS sample up so that they sum up to the population total. (My guess though is since that total is usually not known very accurately, DHS sweeps the issue under the carpet, and just makes a poker face with the weights that sum up to the sample size.)Likewise, your additional sample should be scaled so that the weights sum up to the target population in your three provinces.Once that is done, you can combine the weights using a version of the single frame estimation method (Lohr 2009). Since weights are inverse probabilities of selection, the combined weight should be the inverse of the combined probability of selection:w_i^c = 1/\\pi_i^c = 1/[1-(1-\\pi_1^1)(1-\\pi_i^2)] \\approx 1/(\\pi_i^1 + \\pi_i^2) = 1/(1/w_i^1 + 1/w_i^2)for the observations in the three provinces that were sampled twice, while the observations in the remaining provinces just retain their DHS weight.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-15 10:21:56","Question_id":161684}
{"_id":{"$oid":"5837a57ea05283111e4d5021"},"Last_activity":"2016-08-15 10:03:43","Creator_reputation":169,"Question_score":0,"Answer_content":"I believe that your data would best be analyzed using, in order of preference, a time-to-event method such as Kaplan-Meier or Cox's model or, less optimal, a Poisson regression. Provided that you are interested in the differences in time until completion, survival analysis will give you the best sense of the difference between completion rates of your groups. The main role of survival analysis is to understand differences in an underlying process ( the \"force of mortality\" in healthcare applications) leading some units to fail (or not fail) deferentially as a function of time. Survival analysis takes into account that not all units are under observation at all time points in the analysis- some have already failed or, in your case, completed the task. This is called censoring. Your analysis may not succeed in identifying differences in the underlying completion rate at any given time if you do not account for the fact that some persons have already completed the task. Thus, one of the methods of survival analysis is most appropriate. Also, skewness is often seen in survival analysis and, as long as the proportional hazards assumption is met, survival analysis is particularly well-suited to handle this skewness, while still preserving the ability to differentiate between rates over time between groups or covariates.Poisson regression can also be used to model failure time data under special circumstances (see: Does Cox Regression have an underlying Poisson distribution?). However, it has never been clear to me why this offers an advantage over survival analysis, as this method requires the data fits the Poisson distribution and does not account for changing membership in the at-risk group over time. ","Display_name":"Todd","Creater_id":64263,"Start_date":"2016-08-15 10:03:43","Question_id":229775}
{"_id":{"$oid":"5837a57ea05283111e4d502e"},"Last_activity":"2016-08-15 09:36:14","Creator_reputation":2788,"Question_score":0,"Answer_content":"If you need to model longitudinal data where the outcome is a count, I'd recommend using a mixed model approach or a Generalized Estimating Equations (GEE) approach.  Both of these methods will account for the serial correlation of your outcomes and you will not have to worry about transformations or taking differences of measurements.  A search on this site for GEE/Poisson or Mixed Models/Poisson will lead you in the right direction.","Display_name":"StatsStudent","Creater_id":7962,"Start_date":"2016-08-15 09:36:14","Question_id":229945}
{"_id":{"$oid":"5837a57ea05283111e4d503b"},"Last_activity":"2016-08-14 18:21:47","Creator_reputation":6377,"Question_score":0,"Answer_content":"I've thought about this some now, and I'm realizing we can sort this out. Let  denote your time variable,  denote your rv response variable, and let  be the response you actually used in the model. Thendr/dt = dy^2/dt =2y\\cdot dy/dtand note that lstrends can calculate values of . It follows that SE(dr/dt) \\approx |2y|\\cdot SE(dy/dt) Note also that  depends on  (as well as other variables), so you need to take due care and probably include some specific key values of  in the reference grid (use the at argument). Once you do that, you can use summary(lsmeans()) to calculate a data frame lsms whose least-squares means estimate the needed values of ; and usesummary(lstrends()) to create a data frame slopes whose least-squares means are the corresponding estimates of . Finally, the estimated slopes  will be 2 * lsmslsmean, and estimates of  will be abs(2 * lsmsSE.In principle, this idea could be incorporated in the lstrends function (e.g., as its way of supporting type = \"response\" situations). I'll poke around with that and see how easily this feature could be added.","Display_name":"rvl","Creater_id":52554,"Start_date":"2016-08-14 18:21:47","Question_id":228958}
{"_id":{"$oid":"5837a57ea05283111e4d5047"},"Last_activity":"2010-12-17 00:11:15","Creator_reputation":24971,"Question_score":3,"Answer_content":"If these 3 models are estimated from independent samples, then you can assume that  are independent for these 3 models. Then you can average them. The standard error of the average then will be the square root from the average of the squares of the standard errors. However you should check if you do not have omited-variables problem. If you do, then one of the  in your models is biased, so it should be discarded. Of course if you have all the data for these 3 models, I suggest estimate Model 3 with all the data and then use the coefficient  with its standard error, assuming that your model is adequate.","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2010-12-17 00:11:15","Question_id":5586}
{"_id":{"$oid":"5837a57ea05283111e4d5056"},"Last_activity":"2016-08-15 09:08:38","Creator_reputation":1618,"Question_score":0,"Answer_content":"I think I finally have the answer. I need an additional requirement on the distribution of . Before, I only required that  is uniform between 0 and 1. In this case my example is correct and Bonferroni would be too liberal. However, if I additionally require the uniformity of  then it is easy to derive that Bonferroni can never be too conservative. My example violates this assumption. In more general terms, the assumption is that the distribution of all p-values given that all null hypotheses are true must have the form of a copula: Jointly they don't need to be uniform, but marginally they do. Comment: If anyone can point me to a source where this assumption is clearly stated (textbook, paper), I'll accept this answer. ","Display_name":"fabee","Creater_id":6000,"Start_date":"2016-08-15 09:08:38","Question_id":80112}
{"_id":{"$oid":"5837a57ea05283111e4d5057"},"Last_activity":"2016-07-20 16:26:24","Creator_reputation":263,"Question_score":3,"Answer_content":"Bonferroni can't be liberal, regardless of dependence, if your p-values are computed correctly.Let A be the event of Type I error in one test and let B be the event of Type I error in another test. The probability that A or B (or both) will occur is:P(A or B) = P(A) + P(B) - P(A and B)Because P(A and B) is a probability and thus can't be negative, there’s no possible way for that equation to produce a value higher than P(A) + P(B). The highest value the equation can produce is when P(A and B) = 0, i.e. when A and B are perfectly negatively dependent. In that case, you can fill in the equation as follows, assuming both nulls true and a Bonferroni-adjusted alpha level of .025:P(A or B) = P(A) + P(B) - P(A and B) = .025 + .025 - 0 = .05Under any other dependence structure, P(A and B) \u003e 0, so the equation produces a value even smaller than .05. For example, under perfect positive dependence, P(A and B) = P(A), in which case you can fill in the equation as follows:P(A or B) = P(A) + P(B) - P(A and B) = .025 + .025 - .025 = .025Another example: under independence, P(A and B) = P(A)P(B). Hence:P(A or B) = P(A) + P(B) - P(A and B) = .025 + .025 - .025*.025 = .0494As you can see, if one event has a probability of .025 and another event also has a probability of .025, it’s impossible for the probability of “one or both” events to be greater than .05, because it’s impossible for P(A or B) to be greater than P(A) + P(B). Any claim to the contrary is logically nonsensical.\"But that's assuming both nulls are true,\" you might say. \"What if the first null is true and the second is false?\" In that case, B is impossible because you can't have a Type I error where the null hypothesis is false. Thus, P(B) = 0 and P(A and B) = 0. So let's fill in our general formula for the FWER of two tests:P(A or B) = P(A) + P(B) - P(A and B) = .025 + 0 - 0 = .025So once again the FWER is \u0026lt; .05. Note that dependence is irrelevant here because P(A and B) is always 0. Another possible scenario is that both nulls are false, but it should be obvious that the FWER would then be 0, and thus \u0026lt; .05.","Display_name":"Bonferroni","Creater_id":109785,"Start_date":"2016-07-19 07:47:58","Question_id":80112}
{"_id":{"$oid":"5837a57ea05283111e4d5068"},"Last_activity":"2015-07-17 06:36:55","Creator_reputation":21588,"Question_score":1,"Answer_content":"Population weights reflect the probability of selection of an individual. If you do use weights, they are attached to a person. Think about this: representing a choice model like multinomial logit in a \"long\" format, with one line per person-by-alternative, and an indicator for the chosen alternative, vs. \"wide\" format, with one line per person and the dependent variable showing just the chosen alternative, is an implementation issue, not a statistical issue. The latter one clearly has only one weight attached to it, and that is the person's weight.I am not quite sure I would understand what the (differential) weights of the alternatives mean, at least in the context of finite population sampling.I wrote far more extensively about analysis of complex survey data here, although that was in the context of health applications. There was a section on subsamples -- you often need to treat them carefully as you may mislead the software into misinterpreting your survey design. There are also some schools of thought that the weights are only needed for descriptive analysis, and are anywhere between useless and irrelevant for complex models... but these schools of thought may be missing a point of what the regressions with survey data are estimating... see also here. Regressions with weights and other elements of complex survey designs intend on estimating the census regression, i.e., what you would have obtained should you have run the regression for the full finite population of interest. Whether that's a relevant target quantity may be a different issue; I think a big part of the argument is that, especially for the counterfactual models, the finite population may not be that well defined. Ah well.","Display_name":"StasK","Creater_id":5739,"Start_date":"2015-07-17 06:36:55","Question_id":161945}
{"_id":{"$oid":"5837a57ea05283111e4d5074"},"Last_activity":"2015-07-20 18:17:10","Creator_reputation":7427,"Question_score":1,"Answer_content":"Not sure I completely follow the description, but presumably the program is just using the probability transform.The idea here is that if  is a distribution function and we define , then  is a realization of a random variable with distribution  when  uniform.  The reason the distribution function is used is because of this result.If we wanted to use the mass function instead, we would have to \"chop up\" the unit interval  into slices with lengths equal to the probabilities associated with that mass function and then map these to the values of the random variable.  This is a lot of work and it turns out to just be a manual way of calculating .","Display_name":"dsaxton","Creater_id":78861,"Start_date":"2015-07-20 18:17:10","Question_id":162383}
{"_id":{"$oid":"5837a57ea05283111e4d5085"},"Last_activity":"2016-08-15 07:34:13","Creator_reputation":7559,"Question_score":6,"Answer_content":"To give a counter-example, you almost certainly want the mean and not the median when calculating returns in finance.Examples where the median is horribly misleading compared to the mean:If you're looking at a set of bond returns, the median will effectively ignore those observations where the bond defaults and you lose half or more of your money. If you're looking at venture capital returns, it's in some sense the reverse. The median company in VC or angel investing is a bust, and the median will effectively ignore the big winners like Google. The return for Ron Conway's first angel fund came largely from one company, Google. Sometimes insensitivity to outliers is NOT what you want!Good luck explaining to investors, \"I know our fund is down 40 percent this year because nearly half our bonds went bust with no recovery, but our median bond is returning one percent!\"","Display_name":"Matthew Gunn","Creater_id":97925,"Start_date":"2016-08-12 10:27:44","Question_id":229576}
{"_id":{"$oid":"5837a57ea05283111e4d5092"},"Last_activity":"2016-08-15 07:28:33","Creator_reputation":21588,"Question_score":0,"Answer_content":"Generally, unequal weighting reduces precision and power. One easy way to think about it is that, say, the maximum likelihood estimates of the normal mean have equal weights of , so anything that deviates from that uniform weighting scheme cannot be optimal. Same can be said about BLUEs. (But this is not to say that equal weighting always makes better sense than unequal weighting; robust statistics starts with the premise that robustness is a more important property than efficiency, to which most economists would subscribe. But I digress.)Efficiency losses are characterized by design effects (due to Kish 1965). For unequal weights, the design effect is (Korn and Graubard 1999, sec. 4.4)\\mbox{UWE DEFF} = \\frac{n\\sum_i w_i^2}{\\bigl(\\sum_i w_i\\bigr)^2}and the effective sample size is n_{\\rm eff} = \\frac{\\bigl(\\sum_i w_i\\bigr)^2}{\\sum_i w_i^2}So if two thirds of the weights are equal to 1, and the remaining third, to 2, you haven_{\\rm eff} = \\frac{\\bigl(1\\cdot 2n/3 + 2 \\cdot n/3 )^2}{1 \\cdot 2n/3 + 4 \\cdot n/3}=\\frac{16n^2/9}{2n}=\\frac{8n}{9}That would be the quantity to go into your denominator.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-15 07:28:33","Question_id":164409}
{"_id":{"$oid":"5837a57ea05283111e4d50a2"},"Last_activity":"2016-08-15 07:16:47","Creator_reputation":7965,"Question_score":4,"Answer_content":"[1] addresses the question:First, weights shouldn't be set to zeros in order to break the symmetry when backprogragating:  Biases can generally be initialized to zero but weights need to be initialized carefully to break the symmetry between hidden units of the same layer. Because different output units receive different gradient signals, this symmetry breaking issue does not concern the output weights (into the output units), which can therefore also be set to zero.Some initialization strategies:[2] and [3] recommend scaling by the inverse of the square root of the fan-inGlorot and Bengio (2010) and the Deep Learning Tutorials use a combination of the fan-in and fan-out:for hyperbolic tangent units: sample a Uniform(-r, r) with  (fan-in is the number of inputs of the unit).for sigmoid units : sample a Uniform(-r, r) with  (fan-in is the number of inputs of the unit).in the case of RBMs, a zero-mean Gaussian with a small standard deviation around 0.1 or 0.01 works well (Hinton, 2010) to initialize the weights.Orthogonal random matrix initialization, i.e. W = np.random.randn(ndim, ndim); u, s, v = np.linalg.svd(W) then use u as your initialization matrix.Also, unsupervised pre-training may help in some situations:  An important choice is whether one should use  unsupervised pre-training (and which unsupervised  feature learning algorithm to use) in order  to initialize parameters. In most settings  we have found unsupervised pre-training to help  and very rarely to hurt, but of course that  implies additional training time and additional  hyper-parameters.Some ANN libraries also have some interesting lists, e.g. Lasagne:Constant([val]) Initialize weights with constant value.Normal([std, mean]) Sample initial weights from the Gaussian distribution.Uniform([range, std, mean]) Sample initial weights from the uniform distribution.Glorot(initializer[, gain, c01b])   Glorot weight initialization.GlorotNormal([gain, c01b])  Glorot with weights sampled from the Normal distribution.GlorotUniform([gain, c01b]) Glorot with weights sampled from the Uniform distribution.He(initializer[, gain, c01b])   He weight initialization.HeNormal([gain, c01b])  He initializer with weights sampled from the Normal distribution.HeUniform([gain, c01b]) He initializer with weights sampled from the Uniform distribution.Orthogonal([gain])  Intialize weights as Orthogonal matrix.Sparse([sparsity, std]) Initialize weights as sparse matrix.[1] Bengio, Yoshua. \"Practical recommendations for gradient-based training of deep architectures.\" Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 437-478.[2] LeCun, Y., Bottou, L., Orr, G. B., and Muller, K. (1998a). Efficient backprop. In Neural Networks, Tricks of the Trade.[3] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" International conference on artificial intelligence and statistics. 2010.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2015-12-11 20:19:54","Question_id":47590}
{"_id":{"$oid":"5837a57ea05283111e4d50a3"},"Last_activity":"2013-01-14 13:42:42","Creator_reputation":6885,"Question_score":15,"Answer_content":"I assume you are using logistic neurons, and that you are training by gradient descent/back-propagation. The logistic function is close to flat for large positive or negative inputs. The derivative at an input of  is about , but at  the derivative is about  . This means that if the input of a logistic neuron is  then, for a given training signal, the neuron will learn about  times slower that if the input was . If you want the neuron to learn quickly, you either need to produce a huge training signal (such as with a cross-entropy loss function) or you want the derivative to be large. To make the derivative large, you set the initial weights so that you often get inputs in the range . The initial weights you give might or might not work. It depends on how the inputs are normalized. If the inputs are normalized to have mean  and standard deviation , then a random sum of  terms with weights uniform on  will have mean  and variance , independent of . The probability that you get a sum outside of  is small. That means as you increase , you are not causing the neurons to start out saturated so that they don't learn. With inputs which are not normalized, those weights may not be effective at avoiding saturation. ","Display_name":"Douglas Zare","Creater_id":11981,"Start_date":"2013-01-12 16:35:49","Question_id":47590}
{"_id":{"$oid":"5837a57ea05283111e4d50a4"},"Last_activity":"2013-01-14 01:24:40","Creator_reputation":5957,"Question_score":5,"Answer_content":"The following explanation is taken from the book: Neural Networks for Pattern Recognition by Christopher Bishop. Great book!Assume you have previously whitened the inputs to the input units, i.e. \u0026lt;x_{i}\u0026gt; = 0 and \u0026lt;x_{i}^{2}\u0026gt; = 1The question is: how to best choose the weights?. The idea is to pick values of the weights at random following a distribution which helps the optimization process to converge to a meaningful solution.You have for the activation of the units in the first layer, y = g(a)  where  a = \\sum_{i=0}^{d}w_{i}x_{i}. Now, since you choose the weights independently from the inputs,\u0026lt;a\u0026gt; = \\sum_{i=0}^{d}\u0026lt;w_{i}x_{i}\u0026gt; = \\sum_{i=0}^{d}\u0026lt;w_{i}\u0026gt;\u0026lt;x_{i}\u0026gt; = 0 and  \u0026lt;a^2\u0026gt; = \\left\u0026lt;\\left(\\sum_{i=0}^{d}w_{i}x_{i}\\right) \\left(\\sum_{i=0}^{d}w_{i}x_{i}\\right)\\right\u0026gt; = \\sum_{i=0}^{d}\u0026lt;w_{i}^{2}\u0026gt;\u0026lt;x_{i}^{2}\u0026gt; = \\sigma^{2}d  where sigma is the variance of the distribution of weights. To derive this result you need to recall that weights are initialized independently from each other, i.e. \u0026lt;w_{i}w_{j}\u0026gt; = \\delta_{ij}","Display_name":"jpmuc","Creater_id":17908,"Start_date":"2013-01-13 13:45:07","Question_id":47590}
{"_id":{"$oid":"5837a57ea05283111e4d50b1"},"Last_activity":"2016-08-15 07:00:50","Creator_reputation":31,"Question_score":0,"Answer_content":"First, do these 8 constructs make sense? Are there theoretical reasons for the individual tests to hang together in these 8 constructs?I would say you need to do some CFA to confirm what you found with your explorations with EFA. And what do you mean by move straight to multiple regression analysis? Is this to use the 8 constructs with other variables?","Display_name":"jason","Creater_id":127406,"Start_date":"2016-08-15 07:00:50","Question_id":229912}
{"_id":{"$oid":"5837a57ea05283111e4d50c0"},"Last_activity":"2016-08-15 05:32:23","Creator_reputation":536,"Question_score":3,"Answer_content":"Yes, you're right a lower learning rate should find a better optimum than a higher learning rate. But you should tune the hyper-parameters using grid search to find the best combination of learning rate along with the other hyper-parameters.The GBM algorithm uses multiple hyper parameters in addition to the learning rate (shrinkage), these are:Number of treesInteraction depthMinimum observation in a nodeBag fraction (fraction of randomly selected observations)The grid search needs to check all of these in order to determine the most optimal set of parameters.For example, on some data-sets I've tuned with GBM, I've observed that accuracy varies widely as each hyper-parameter is changed. I haven't run GBM on your sample data-set, but I'll refer to a similar tuning exercise for another data-set. Refer to this graph on a classification problem with highly imbalanced classes.Although the accuracy is highest for lower learning rate, e.g. for max. tree depth of 16, the Kappa metric is 0.425 at learning rate 0.2 which is better than 0.415 at learning rate of 0.35.But when you look at learning rate at 0.25 vs. 0.26 there is a sharp but small increase in Kappa for max tree depth of 14, 15 and 16; whereas it continues decreasing for tree depth 12 and 13.Hence, I would suggest you should try the grid search.Additionally, as you mentioned, this situation could also have been aggravated by a smaller sample size of the data-set. ","Display_name":"Sandeep S. Sandhu","Creater_id":55831,"Start_date":"2016-08-14 23:07:34","Question_id":229855}
{"_id":{"$oid":"5837a57ea05283111e4d50cf"},"Last_activity":"2016-08-15 05:13:07","Creator_reputation":11400,"Question_score":1,"Answer_content":"If you have counts of a type of event as your dependent variable then you should use an analysis that is designed for that type of dependent variable. Poisson regression is the simplest choice to start. Your predictor of main interest and the covariates would be the independent variables, similarly to the standard multiple linear regression you might perform with a truly continuous dependent variable, but they now help predict the probability of an event's occurrence. Several Cross Validated pages explain the advantages of Poisson regression or related count-specific analyses, for example here and here. Unless you have so many events that your dependent variable is very close to continuous, then a multiple regression treating your dependent variable as having Poisson, overdispersed Poisson, or negative binomial characteristics would seem to make the most sense.  (These are examples of generalized linear models; be careful with the choice of terminology.)","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-08-15 05:13:07","Question_id":229897}
{"_id":{"$oid":"5837a57fa05283111e4d5157"},"Last_activity":"2016-08-14 06:45:29","Creator_reputation":149,"Question_score":0,"Answer_content":"I would say that it is not possible to say that  since we do not know that the values of the sequence of arithmetic means we obtain by iteratively calculating it for every  are normally distributed around  as , and  only for enough large  and normally distributed data. That's why I think to prove the original statement by substituting  for every  is impossible.","Display_name":"ahra","Creater_id":125486,"Start_date":"2016-08-14 06:45:29","Question_id":229764}
{"_id":{"$oid":"5837a57fa05283111e4d5158"},"Last_activity":"2016-08-14 05:45:56","Creator_reputation":197,"Question_score":1,"Answer_content":"  https://en.wikipedia.org/wiki/Expected_value#Properties","Display_name":"Toney Shields","Creater_id":109437,"Start_date":"2016-08-14 05:45:56","Question_id":229764}
{"_id":{"$oid":"5837a57fa05283111e4d5165"},"Last_activity":"2016-08-14 06:25:25","Creator_reputation":158,"Question_score":0,"Answer_content":"I think the issue is that you are printing out the accuracy and cost after each mini-batch.  This is very noisy.  Notice that on average the loss tends to decrease which the accuracy tends to increase.  You should try to take the average over many batches.","Display_name":"chasep255","Creater_id":110043,"Start_date":"2016-08-14 06:25:25","Question_id":229767}
{"_id":{"$oid":"5837a57fa05283111e4d5166"},"Last_activity":"2016-08-13 13:01:18","Creator_reputation":null,"Question_score":0,"Answer_content":"It could happen because you are probably over fitting the network.For example:You train you network with a training set TR and you check accuracy on a test set TE. After several weight updates (training epochs) to TR entries you check the error of the network on, for example, all entries of TE. In the beginning the error on TE is decreasing but after a while your error suddenly starts to increase. This happens because your network beginnings to over fit to your training set and hence, loses the ability to generalize to new data.","Display_name":"Bastian Schoettle","Creater_id":null,"Start_date":"2016-08-13 13:01:18","Question_id":229767}
{"_id":{"$oid":"5837a57fa05283111e4d5173"},"Last_activity":"2016-08-14 06:06:21","Creator_reputation":1,"Question_score":0,"Answer_content":"Based on the predicting the likely time using multivariate Bayesian  scan statistic (MBSS) could be of assistance. This MBSS has advantage of improving the timeliness and accuracy of event detection. ","Display_name":"Esan","Creater_id":127647,"Start_date":"2016-08-14 06:06:21","Question_id":16302}
{"_id":{"$oid":"5837a57fa05283111e4d5174"},"Last_activity":"2011-09-30 19:20:15","Creator_reputation":5187,"Question_score":8,"Answer_content":"Hidden Markov models would apply if the data were random emissions from some underlying unobserved Markov model; I wouldn't rule that out, but it doesn't seem a very natural model.I would think about point processes, which match your particular data well.  There is a great deal of work on predicting earthquakes (though I don't know much about it) and even crime.  If there are many different people printing, and you're just seeing the times but not the individual identities, a Poisson process might work well (the superposition of multiple independent point processes is approximately Poisson), though it would have to be inhomogeneous (the chance of a point varies over time): people are less likely to be printing at 3am than at 3pm.  For the inhomogeneous Poisson process model, the key would be getting a good estimate of the chance of a print job at a particular time on a particular day.  If these print times are for students in a classroom, though, it could be quite tricky, as they're not likely to be independent and so the Poisson process wouldn't work well.Here's a link to a paper on the crime application.","Display_name":"Karl","Creater_id":5862,"Start_date":"2011-09-30 16:46:49","Question_id":16302}
{"_id":{"$oid":"5837a57fa05283111e4d5181"},"Last_activity":"2016-08-14 05:18:09","Creator_reputation":233,"Question_score":0,"Answer_content":"As you describe in your own answer above, plm gives the residuals of the (quasi-)demeaned model. To get the \"outer\"/\"overall\" residuals of the random effects model, use:X \u0026lt;- model.matrix(grun.re, model = \"pooling\")y \u0026lt;- pmodel.response(grun.re, model = \"pooling\")est \u0026lt;-  X %*% coef(grun.re)res \u0026lt;- y - esthead(res)       [,1]1  -53.696662 -135.068743 -192.349084 -155.949515 -181.426566 -146.61723This matches Stata's uRE.","Display_name":"Helix123","Creater_id":94889,"Start_date":"2016-08-08 08:30:25","Question_id":183206}
{"_id":{"$oid":"5837a57fa05283111e4d5182"},"Last_activity":"2015-12-10 19:29:23","Creator_reputation":33,"Question_score":1,"Answer_content":"Thank you Helix.I expect don't breaking any code of politeness answering my own question.In fact, this question is related to this. Yet, I wil try give an answer from the econometrician point fo view now.After long time, I realized that in a Random effects estimates you are running a demeaned regression as is said in equation 6 of the plm package paper here. However, I think their notation a litle \"unrelated\" to the rest of the paper.Folowing Cameron and Triverdi, Microeconometrics Methods, the feasible GLS estimator can be implemented making OLS in the demeaned equation. That is Cameron and Triverdi 21.43 demeaned equation (which is the same as the cited above). y_{it}-\\widehat{\\theta}{\\overline{y}_{it}}=(1-\\widehat{\\theta})\\mu+(x_{it}-\\widehat{\\theta}{\\overline{x}_{i}})'\\beta+\\upsilon_{it}Where:\\widehat{\\theta}=1-\\frac{\\sigma_\\epsilon}{(T\\sigma_\\alpha^2+\\sigma_\\epsilon^2)^{1/2}}The plm package calculates theta and stores it in the regression object.And where,\\upsilon=(1-\\widehat{\\theta})\\alpha_i+(\\epsilon_{it}-\\widehat{\\theta}{\\overline{\\epsilon}_{i}})In a Random Effects model, the plm regression residuals are, in fact, the upsilon as above.However, if we calculate the residuals by hand, u=Y-XB, we will obtain what Stata calls the overall error of the model. In a fixed effect model it is u_{it}=\\alpha_{i}+\\epsilon_{it}.Where alpha is the individual especif effect and epsilon the idiosyncratic error. Once we obtain alpha of random effects by the shrinkage factor (as the cited related question does), we can recover the idiosyncratic error. In summary, what plm package returns as the residuals from random effets model are the residuals of the OLS demeaned regression.Wooldridge, Hasiao and Baltagi books about econometrics panel data derive the same result for the feseable GLS.","Display_name":"Rodrigo Remedio","Creater_id":83090,"Start_date":"2015-12-10 19:29:23","Question_id":183206}
{"_id":{"$oid":"5837a57fa05283111e4d5191"},"Last_activity":"2016-08-14 05:09:27","Creator_reputation":57712,"Question_score":1,"Answer_content":"PCA always explains all the variance, if you include all the components. Therefore, I am guessing that what you mean is that the first (or perhaps first several) components explain less of the variance than you  think they should.What this means is that the variables do not \"go together\" as well as you think they do.  This might simply mean that you were overly optimistic. A higher sample in one area will not, by itself, make the first (few) principal components lower. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-14 05:09:27","Question_id":229741}
{"_id":{"$oid":"5837a57fa05283111e4d5192"},"Last_activity":"2016-08-14 02:39:08","Creator_reputation":56,"Question_score":0,"Answer_content":"PCA is a decomposition process.Meaning it takes an existing vector space and transforms it into another vector space.If i understand correctly, you chose the first N components of the transformed vector space. If N is lower than the original vector space shape(number of features) then the explained variance might be lower than 100% and can basically range from 0-100.It you used a specific package for the PCA, you can change the explained variance by setting the hyper-parameter(n_components in Sklrean.PCA) to something different.Another thing to consider, explained variance lower than 50% is not that bad, depending on your thoughts on how good the features describe your problem domain.","Display_name":"DaFanat","Creater_id":123020,"Start_date":"2016-08-14 02:39:08","Question_id":229741}
{"_id":{"$oid":"5837a57fa05283111e4d519f"},"Last_activity":"2016-08-11 14:35:35","Creator_reputation":166,"Question_score":5,"Answer_content":"R reports the V-statistic, which is the sum of the positive ranks. The Wikipedia example computes it slightly differently, as the sum of all ranks, regardless of sign. In other words, both versions are correct (and equivalent). This CrossValidated post might be helpful.","Display_name":"jdobres","Creater_id":127174,"Start_date":"2016-08-11 14:35:35","Question_id":229760}
{"_id":{"$oid":"5837a57fa05283111e4d51ae"},"Last_activity":"2016-08-14 04:20:48","Creator_reputation":56,"Question_score":1,"Answer_content":"Its common to assume that the distribution-variance of the sample and that of the entire population are similar, so no surprise you didn't find an answer quickly.None the less you can use this formula to calculate the CI of the standard deviation(as a factor of your sample size)CI:=[SDSQRT((n-1)/CHIINV((alpha/2), n-1)), SDSQRT((n-1)/CHIINV(1-alpha/2), n-1))]Where alpha is your confidence target and CHIINV(CHI Inverse Function) returns the one-tailed chi-statistic given a target probability.","Display_name":"DaFanat","Creater_id":123020,"Start_date":"2016-08-14 04:20:48","Question_id":229752}
{"_id":{"$oid":"5837a57fa05283111e4d51bb"},"Last_activity":"2016-08-14 03:55:24","Creator_reputation":15579,"Question_score":0,"Answer_content":"In addition to @jake-westfall's answer which gives you an easily applied rule of thumb:The variance you observe between the accuracies/errors of the different folds is composed (at least) of some variance due to the limited number of cases tested by that surrogate model and some variance due to the variations between the surrogate models (instability).  The latter is the variance you want to trade off against bias with your regularization, while the former hampers your ability to detect improvements. So for a second look, I recommend to check that the variance due to the number of tested cases is low enough to sensibly allow the comparison.","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2016-08-14 03:55:24","Question_id":229450}
{"_id":{"$oid":"5837a57fa05283111e4d51bc"},"Last_activity":"2016-08-11 18:48:04","Creator_reputation":4856,"Question_score":2,"Answer_content":"Sort of. There is the so-called \"one standard error rule,\" which does use the standard deviations of the prediction error estimates (i.e., their standard errors), although not in quite the way you mentioned.The one standard error rule says: pick the simplest model whose mean estimated prediction error is within 1 standard error of the best-performing model's estimated prediction error. In practice, the \"simplest model\" usually means \"the most strongly regularized model.\" And of course, the \"best-performing model\" is the one with the lowest mean estimated prediction error of all models tested.Stated a bit more plainly, the rule says that we want to pick the simplest model that still does essentially as good a job as the best-looking model -- the best-looking model could be far more complicated, despite only a marginal increase in performance.","Display_name":"Jake Westfall","Creater_id":5829,"Start_date":"2016-08-11 18:48:04","Question_id":229450}
{"_id":{"$oid":"5837a57fa05283111e4d51c9"},"Last_activity":"2016-08-14 03:43:23","Creator_reputation":271,"Question_score":17,"Answer_content":"The confusion matrix is a way of tabulating the number of misclassifications, i.e., the number of predicted classes which ended up in a wrong classification bin based on the true classes.While sklearn.metrics.confusion_matrix provides a numeric matrix, I find it more useful to generate a 'report' using the following:import pandas as pdy_true = pd.Series([2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2])y_pred = pd.Series([0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2])pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)which results in:Predicted  0  1  2  AllTrue                   0          3  0  0    31          0  1  2    32          2  1  3    6All        5  2  5   12This allows us to see that:The diagonal elements show the number of correct classifications for each class: 3, 1 and 3 for the classes 0, 1 and 2.The off-diagonal elements provides the misclassifications: for example, 2 of the class 2 were misclassified as 0, none of the class 0 were misclassified as 2, etc.The total number of classifications for each class in both y_true and y_pred, from the \"All\" subtotalsThis method also works for text labels, and for a large number of samples in the dataset can be extended to provide percentage reports.import numpy as npimport pandas as pd# create some datalookup = {0: 'biscuit', 1:'candy', 2:'chocolate', 3:'praline', 4:'cake', 5:'shortbread'}y_true = pd.Series([lookup[_] for _ in np.random.random_integers(0, 5, size=100)])y_pred = pd.Series([lookup[_] for _ in np.random.random_integers(0, 5, size=100)])pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted']).apply(lambda r: 100.0 * r/r.sum())The output then is:Predicted     biscuit  cake      candy  chocolate    praline  shortbreadTrue                                                                    biscuit     23.529412    10  23.076923  13.333333  15.384615    9.090909cake        17.647059    20   0.000000  26.666667  15.384615   18.181818candy       11.764706    20  23.076923  13.333333  23.076923   31.818182chocolate   11.764706     5  15.384615   6.666667  15.384615   13.636364praline     17.647059    10  30.769231  20.000000   0.000000   13.636364shortbread  17.647059    35   7.692308  20.000000  30.769231   13.636364where the numbers now represent the percentage (rather than number of cases) of the outcomes that were classified.Although note, that the sklearn.metrics.confusion_matrix output can be directly visualized using:import matplotlib.pyplot as pltconf = sklearn.metrics.confusion_matrix(y_true, y_pred)plt.imshow(conf, cmap='binary', interpolation='None')plt.show()","Display_name":"achennu","Creater_id":52634,"Start_date":"2014-07-23 04:58:32","Question_id":95209}
{"_id":{"$oid":"5837a57fa05283111e4d51ca"},"Last_activity":"2014-04-25 11:06:59","Creator_reputation":1074,"Question_score":1,"Answer_content":"On y-axis confusion matrix has the actual values, and on the x-axis the values given by the predictor. Therefore, the counts on the diagonal are the number of correct predictions. And elements of the diagonal are incorrect predictions. In your case:\u0026gt;\u0026gt;\u0026gt; confusion_matrix(y_true, y_pred)    array([[2, 0, 0],  # two zeros were predicted as zeros           [0, 0, 1],  # one 1 was predicted as 2           [1, 0, 2]]) # two 2s were predicted as 2, and one 2 was 0","Display_name":"Akavall","Creater_id":11708,"Start_date":"2014-04-25 10:52:53","Question_id":95209}
{"_id":{"$oid":"5837a57fa05283111e4d51d7"},"Last_activity":"2016-08-14 03:30:25","Creator_reputation":15579,"Question_score":1,"Answer_content":"First, to add one more point to your list of drawbacks of the proposed plot: as the folds are (usually) drawn at random and interpreted to be random, there is no reason why fold 2 should be in between folds 1 and 3: the order along the x axis is also random. Here is what I do: source: C. Beleites, K. Geiger, M. Kirsch, S. B. Sobottka, G. Schackert and R. Salzer: Raman spectroscopic grading of astrocytoma tissues: using soft reference information, Anal. Bioanal. Chem., 400 (2011), 2801 - 2816. DOI: 10.1007/s00216-011-4985-4AAM versionNote that the data are spectra, so there is an inherent relationship between the variates that makes a line plot of the coefficients a sensible and common choice.Also, that plot was produced from 125 repeated/iterated -fold cross validation. An overlay plot of 1000 lines would have been a mess, so I plotted median and quartiles instead.If the model (co)variates do not have such an inherent relationship lines are clearly not indicated. I'd preferrably go for facets unless that makes the plot too crowded. If only few surrogate models were calculated, I'd go for a dot plot, with many surrogate models for a box plot or something similar.If you facet (co)variates, and have an idea how the coefficient values are distributed, you could put QQ-plots into the facets.","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2016-08-14 03:30:25","Question_id":229729}
{"_id":{"$oid":"5837a57fa05283111e4d51e4"},"Last_activity":"2016-08-14 02:50:56","Creator_reputation":36,"Question_score":0,"Answer_content":"There are many corrected versions of AIC which aim at reducing bias. They typically depend on the dimension, the sample size, or the number of covariates in linear regression models. If you want to find a corrected AIC version in your context, you may need to specify more details on your model.The use of AIC is not recommended to select the number of components in mixture models. See for instance:  Choice of the Number of Component Clusters in Mixture Models by Information Criteria.    Generating Gaussian Mixture Models by Model Selection For Speech Recognition","Display_name":"Fito","Creater_id":127635,"Start_date":"2016-08-14 02:50:56","Question_id":229629}
{"_id":{"$oid":"5837a57fa05283111e4d51f1"},"Last_activity":"2016-08-14 02:30:24","Creator_reputation":12982,"Question_score":0,"Answer_content":"It looks like you have estimated an ARMA(1,1)-EGARCH(2,1) rather than just an EGARCH(1,1). That gives you the following model equations:\\begin{aligned}(r_t-\\mu) \u0026amp;= \\varphi_1 (r_{t-1}-\\mu) + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}; \\\\\\varepsilon_t \u0026amp;= \\sigma_t u_t; \\\\\\ln(\\sigma_t^2) \u0026amp;= \\omega + \\alpha_1(\\varepsilon_{t-1}+\\gamma_1(|\\varepsilon_{t-1}|-\\mathbb{E}\\varepsilon_{t-1})) + \\alpha_2(\\varepsilon_{t-2}+\\gamma_2(|\\varepsilon_{t-2}|-\\mathbb{E}\\varepsilon_{t-2})) + \\beta_1 \\ln(\\sigma_{t-1}^2); \\\\u_t \u0026amp;\\sim i.i.d(0,1). \\\\\\end{aligned}In the estimation output, ar1 stands for  and ma1 for .When you scale the data by 100, (the mean of ) and its standard error get scaled by 100; (associated with the mean of , but not equal to it) and its standard error get scaled by a nontrivial amount; arriving to a precise expression would involve some tedious arithmetics; and  and their standard errors remain the same because they characterize the autoregressive behaviour which is independent of the level; and their standard errors remain the same analogously to 3.These are analytical results. Meanwhile, empirical results may differ slightly. Optimization is involved in estimating the model parameters, which means the estimates are subject to rounding errors and probably convergence issues. As can be seen in your estimation output, the coefficients that are supposed to remain unchanged indeed remain roughly unchanged. However, some of their standard errors get affected considerably. My guess is, they suffer more from rouding errors because of being smaller. But it is a bit puzzling.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-14 02:30:24","Question_id":228781}
{"_id":{"$oid":"5837a57fa05283111e4d51fe"},"Last_activity":"2016-08-14 02:18:20","Creator_reputation":2256,"Question_score":1,"Answer_content":"When I understand your question correctly you are asking which class is the positive one and which is the negative one.The answer is that this is to a certain extent arbitrary, so you have to decide that considering the problem at hand. From \"Machine Learning with R\" by Brett Lantz, 2.nd edition, 2015, p. 318:  The most common performance measures consider the model's ability to  discern one class versus all others. The class of interest is known as  the positive class, while all others are known as negative.    The use of the terms positive and negative is not intended to imply  any value judgment (that is, good versus bad), nor does it necessarily  suggest that the outcome is present or absent (such as birth defect  versus none). The choice of the positive outcome can even be  arbitrary, as in cases where a model is predicting categories such as  sunny versus rainy or dog versus cat.    The relationship between the positive class and negative class  predictions can be depicted as a 2 x 2 confusion matrix that tabulates  whether predictions fall into one of the four categories:    • True Positive (TP): Correctly classified as the class of  interest • True Negative (TN): Correctly classified as not the  class of interest • False Positive (FP): Incorrectly  classified as the class of interest • False Negative (FN):  Incorrectly classified as not the class of interestThis is the reason that you e.g. have to specify the positive class when using generic performance measure functions, like ConfusionMatrix in the caret package in R.Now another complicating factor is of course your multiclass setting, but this is answered here:How to compute precision/recall for multiclass-multilabel classification?In general the most popular approach is to calculate these measures for each class by comparing each class level to the remaining levels (i.e. a \"one versus all\" approach).","Display_name":"vonjd","Creater_id":230,"Start_date":"2016-08-14 02:03:22","Question_id":187724}
{"_id":{"$oid":"5837a57fa05283111e4d51ff"},"Last_activity":"2015-12-21 01:18:36","Creator_reputation":25400,"Question_score":0,"Answer_content":"You simply have to merge the categories while calculating (e.g.  and , where  is  or ). True positive prediction for category  is when  was predicted and it was observed, true negative when it was not predicted and not observed, false negative when  was observed but not predicted and false positive when it was predicted but not observed. You can find nice worked example of  confusion matrix on Wikipedia.","Display_name":"Tim","Creater_id":35989,"Start_date":"2015-12-21 01:18:36","Question_id":187724}
{"_id":{"$oid":"5837a57fa05283111e4d520b"},"Last_activity":"2016-08-14 02:12:28","Creator_reputation":36,"Question_score":2,"Answer_content":"Your data may be truly multimodal. The bandwidths that you mention work well asymptotically. If your data is large enough, and does not contain many outliers, then your data may be multimodal.Your data contain many outliers. This is a big issue with KDE since the bandwidth is sensitive to the presence of outliers. The fitted KDE may be way off in such cases.If you believe your data is unimodal, you may want to compare the fit of the KDE with that of a log concave estimator. These are implemented in the R packages logcondens and LogConcDEAD.Try a parametric alternative such a log Student t distribution.Fit your data in the log scale. This may help you visualise some features more clearly and remove the effect of outliers.","Display_name":"Fito","Creater_id":127635,"Start_date":"2016-08-14 02:12:28","Question_id":229743}
{"_id":{"$oid":"5837a57fa05283111e4d5218"},"Last_activity":"2013-09-20 03:14:37","Creator_reputation":1653,"Question_score":3,"Answer_content":"There are many possible clustering methods, and none of them can be considered \"best\", everything depends on the data, as always:If you would like to use spectral clustering, but do not know the number of clusters before hand I suggest taking a look at the self-tuning spectral clustering or some methods of determining the number of clustersIf you consider other algorithms you could try:DBScanOpticsDensity-Link-ClusteringHierarchical clustering","Display_name":"lejlot","Creater_id":28903,"Start_date":"2013-09-20 03:14:37","Question_id":70568}
{"_id":{"$oid":"5837a57fa05283111e4d5225"},"Last_activity":"2016-08-13 23:45:34","Creator_reputation":152613,"Question_score":2,"Answer_content":"This is what you're asking R for:  qnorm(.075, mean=11,sd=8.58)  qnorm(.16, mean=11,sd=8.58) == 2.48  Its also strange to me that the first line is negative, but I assume that its a technical issue.It's not a \"technical issue\", that's actually what the quantile is. It's negative because the 0.075 point of a normal distribution is about 1.44 standard deviations below the mean. When the mean is 11 and the standard deviation is 8.58, you get 11.0 - 1.44 x 8.58 ≈ -1.35  Shouldnt the area under the pdf equal to .85 have a larger interval than the area equal to .68?Indeed it does, but you didn't compute that interval with that call to qnorm. You only computed the left end of that interval. The left end of the wider (/longer) interval is lower down, just as it should be.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-13 21:17:39","Question_id":229726}
{"_id":{"$oid":"5837a57fa05283111e4d5226"},"Last_activity":"2016-08-13 20:00:34","Creator_reputation":8337,"Question_score":1,"Answer_content":"qnorm(.075, mean = 11, sd = 8.58) returns −1.35 because 7.5% of the mass of a normal distribution with mean 11 and SD 8.58 lies between −∞ and −1.35. If what you wanted is to compute the proprtion of this distribution's mass that lies between −∞ and .075, you want the CDF, which in R would be written pnorm(.075, mean = 11, sd = 8.58).","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-13 20:00:34","Question_id":229726}
{"_id":{"$oid":"5837a57fa05283111e4d5233"},"Last_activity":"2016-08-14 00:37:44","Creator_reputation":19594,"Question_score":2,"Answer_content":"k-means is very sensitive to data distribution.In particular the use of k-means on binary (e.g. dummy) variables is questionable, because the mean does not make too much sense anymore. You cannot easily map back cluster centers to attribute values!As for using box-cox with k-means that is actually a good thing. k-means does not handle skewed distributions well. So e.g. an attribute \"income\" (which is notoriously skewed) such a transformation may improve results a lot.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-08-14 00:37:44","Question_id":229627}
{"_id":{"$oid":"5837a57fa05283111e4d5240"},"Last_activity":"2016-08-14 00:11:14","Creator_reputation":2782,"Question_score":2,"Answer_content":"Some of my understandings, may not be correct.  The cause of the vanishing gradient problem is that sigmoid tanh (and RBF) saturate on both sides (-inf and inf), so it's very likely for the input of such non-linearity to fall on the saturated regions.The effect of BN is that it \"pulls\" the input of the non-linearity towards a small range around 0  as a starting point , where such non-linearities don't saturate. So I guess it will work with RBF as well.To remove the non-linearity of ReLU, we can use the softplus funtion , which is very close to ReLU, and was used in Geoffrey Hinton`s papper to explain why ReLU would work.Also the residual networks or the highway networks provide another way of addressing vanishing gradients (via shortcuts). From my experience such architecture gets trained way faster than only connecting the loss to the last layer.Moreover the difficulty of training deep networks is not solely because of the vanishing gradient, but other factors as well (e.g. the internal covariate shift). There's a recent paper layer normalization about another way of doing normalization, it doesn't say about vanishing gradients though, but maybe you'll be interested.","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-08-11 03:57:20","Question_id":227114}
{"_id":{"$oid":"5837a57fa05283111e4d5241"},"Last_activity":"2016-08-13 23:45:11","Creator_reputation":393,"Question_score":1,"Answer_content":"Have you looked into RMSProp? Take a look at this set of slides from Geoff Hinton:Overview of mini-batch gradient descentSpecifically page 29, entitled 'rmsprop: A mini-batch version of rprop', although it's probably worth reading through the full set to get a fuller idea of some of the related ideas.Also related is Yan Le Cun's No More Pesky Learning Ratesand Brandyn Webb's SMORMS3.The main idea is to look at the sign of gradient and whether it's flip-flopping or not; if it's consistent then you want to move in that direction, and if the sign isn't flipping then whatever step you just took must be OK, provided it isn't vanishingly small, so there are ways of controlling the step size to keep it sensible and that are somewhat independent of the actual gradient.So the short answer to how to handle vanishing or exploding gradients is simply - don't use the gradient's magnitude!","Display_name":"redcalx","Creater_id":7789,"Start_date":"2016-08-13 23:45:11","Question_id":227114}
{"_id":{"$oid":"5837a57fa05283111e4d5250"},"Last_activity":"2016-08-13 21:36:58","Creator_reputation":470,"Question_score":0,"Answer_content":"Have you considered looking into the literature for references? I have never heard \"significant non-significance\" and that phrase made me chuckle. If you have a t-statistic and N, I would look up Jeff Rouder Bayes factor calculator online bayes factor calculator. This way you can compute evidence for the null. However, be sure to read a few of his papers so that you understand the approach and what you are doing.","Display_name":"D_Williams","Creater_id":110487,"Start_date":"2016-08-13 21:36:58","Question_id":229723}
{"_id":{"$oid":"5837a57fa05283111e4d525d"},"Last_activity":"2016-08-13 20:30:13","Creator_reputation":16,"Question_score":0,"Answer_content":"I have noted this before in another question here.My guess is that in some cases the model is over-parametrized and the model cannot be constructed. Or it is possible that the data needs regularization (adding a small constant to the co-variance matrices) to make them invertible.Please let us know if you find this or other reasons and/or any work around.BTW, scaling data to have mean zero and standard deviation of one does not make it uniform but actually preserves normality. Here is an example for that:x \u0026lt;- rnorm(1000, mean=10, sd=3)par(mfrow=c(1,2))hist(x)hist(scale(x))EDITLooking at the Mclust documentation (page 13) enabling the conjugate prior withe the argument \"prior=priorControl()\" should produce fewer missing BIC values. ","Display_name":"kap","Creater_id":8757,"Start_date":"2016-08-05 12:04:50","Question_id":209364}
{"_id":{"$oid":"5837a57fa05283111e4d526a"},"Last_activity":"2016-08-10 16:31:45","Creator_reputation":104,"Question_score":1,"Answer_content":"You could take a look at the actuar packagehttps://cran.r-project.org/web/packages/actuar/actuar.pdf Distributions available :BurrInverse BurrGeneralized BetaTransformed BetaParetoGeneralized ParetoInverse ParetoInverse ExponentialInverse GammaLog GammaInverse ParalogisticLog logisticInverse Transformed GammaTransformed GammaInverse WeibullI know that most of then are heavy tailed distributions, but I can't \"order\" them to help you.","Display_name":"\u0026#201;tienne Vanasse","Creater_id":126899,"Start_date":"2016-08-10 16:31:45","Question_id":229279}
{"_id":{"$oid":"5837a57fa05283111e4d5277"},"Last_activity":"2016-08-13 18:19:17","Creator_reputation":8283,"Question_score":2,"Answer_content":"Frame the problem this way:You have a distribution  over a set , a classification loss function , and the gradient of the loss function with respect to an input sample . You want to know  in order to take a step in your optimization algorithm.The typical way to do this in machine learning is to suppose that you have a sample, and estimate \\mathbb E_{X \\sim P} [ g(X) ] \\approx \\frac{1}{N} \\sum_{i=1}^N g(x_i).(Thinking about it this way makes clear the motivation for stochastic gradient descent, where you simply take a subset of your training sample at each step.)What you're proposing instead is that instead of just taking a random sample , you carefully choose a set of points to represent the distribution. It turns out this is an idea that makes a lot of sense, and is generally known as quasi-Monte Carlo. If you assume that the function  is relatively smooth, and the dimension of  is not too high, then there are specially designed sequences of points you can use to guarantee that your estimate is not too different from the true expectation.Note that everything I just said only applies to a single step of the optimization algorithm. As you noted in the question, if you choose the same  every time, you may end up at a weird optimum that works well for those points but not in general. A satisfying theoretical answer to this problem will probably take some work, but I think in practice if you just randomly shift the points (as you suggested) then it should be okay in \"reasonable\" cases.","Display_name":"Dougal","Creater_id":9964,"Start_date":"2016-08-13 14:22:46","Question_id":181705}
{"_id":{"$oid":"5837a57fa05283111e4d5286"},"Last_activity":"2016-08-13 16:55:54","Creator_reputation":8337,"Question_score":1,"Answer_content":"Not so long as you keep treating it as categorical. Dummy variables (and other, trivially different coding schemes) are how categories are input as predictors to regression models. Doing something else with a predictor (in a regression model, at least) requires treating it as having a more than purely categorical structure.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-13 16:55:54","Question_id":229714}
{"_id":{"$oid":"5837a57fa05283111e4d5293"},"Last_activity":"2016-08-13 16:49:14","Creator_reputation":8337,"Question_score":2,"Answer_content":"Yes. \"Multivariate\" and \"univariate\", when they're used to describe models, refer to the number of dependent variables, not the number of independent variables. A multivariate linear regression model, for example, predicts several different variables, and the residuals are multivariate normal rather than univariate normal. See, for example, the Wikipedia article \"Linear regression\":  For more than one explanatory variable, the process is called multiple linear regression. (This term should be distinguished from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.)Hence, your model is a multiple linear regression model, but also a univariate linear regression model.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-13 15:35:02","Question_id":229700}
{"_id":{"$oid":"5837a57fa05283111e4d5294"},"Last_activity":"2016-08-13 14:12:50","Creator_reputation":518,"Question_score":0,"Answer_content":"No that is a multivariate regression. x and x^2 considered two separate variables here. A univariate regression is one with only one independent variable (ignoring the intercept). ","Display_name":"VCG","Creater_id":124896,"Start_date":"2016-08-13 14:12:50","Question_id":229700}
{"_id":{"$oid":"5837a57fa05283111e4d52a1"},"Last_activity":"2016-08-13 16:17:41","Creator_reputation":8893,"Question_score":1,"Answer_content":"Using RMSE or even standard  is somewhat unnatural for the case of a count response variable. Median absolute error/deviation (MAD) would be definitely more natural for integer values but would not directly reflect a \"variance explained\" quantity. Given you are particularly interested in GLMMs (instead of GLMs) I think it will be appropriate to look at something like specific for GLMMs like using the r.squaredGLMM function implemented in R's MuMIn package. This is essentially the  for GLMMs as this is described by  Nakagawa \u0026amp; Schielzeth on their paper on A general and simple method for obtaining R² from generalized linear mixed-effects models. Because you are having a mixed model with fixed () and random () covariates it makes sense to have a conditional  as well as a marginal .You could also report Nagelkerke's  (eg. using fmsb::NagelkerkeR2), it is somewhat standard for GL(M)Ms too.Having mentioned the above  quantities, please note that it is debatable whether or not  measurements are really relevant for Poisson regression (or GLMMs in general). Pseudo-, generalised-  come in many variants; see for example a list here and an excellent discussion in this CV thread here. My advice would be to use MAD as well as a specialised  but do not focus much on the . Reporting -values in regards to a model's performance is a bit pointless....Regarding your latest comment: Using the caret package and cross-validation in general is an excellent idea; you should do it. Notice though that cross-validation is best to select a model's coefficients and not to directly estimate a model's performance for unseen data. Furthermore, irrespective of the  used for your -fold cross-validation scheme, run at least 100 rounds/repeats of your -fold procedure to ensure your results are stable. To estimate out-of-sample performance use hold-out data; a chunk of your data that you never touched during training. Report MAD, (specialized)  on the model's performance on that data. See Zack's answer on the (awesome) thread here for more details about this.","Display_name":"usεr11852","Creater_id":11852,"Start_date":"2016-08-13 16:12:23","Question_id":229370}
{"_id":{"$oid":"5837a57fa05283111e4d52af"},"Last_activity":"2016-08-13 15:49:32","Creator_reputation":946,"Question_score":3,"Answer_content":"To see how T behaves you should plot a histogram of T. For comparison think of the Z test; if you simulated 100 Z scores you would expect that 95% of them would be in the region [-1.96,1.96]. If you got the mean of 100 Z scores it would be pretty close to 0, but usually 1.96 is the critical value of Z so the averages of many Z scores don't tell you anything useful about where your test statistic will lie.Also remember that the distribution of T will vary depending on the size of your data so when simulating T you should simulate it for the same amount of data as your sample size.","Display_name":"Hugh","Creater_id":25299,"Start_date":"2016-08-13 15:49:32","Question_id":229711}
{"_id":{"$oid":"5837a57fa05283111e4d52bc"},"Last_activity":"2016-08-13 15:41:59","Creator_reputation":152613,"Question_score":1,"Answer_content":"The notation may be a little misleading.Note that  is a constant, so its variance is . However,  is the value of the variance function at  rather than the variance of . That is, it specifies the variance component of the model: .The variance function follows from the distribution specification in a GLM. In a quasi-model you'd specify the variance function directly. ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-13 15:41:59","Question_id":228557}
{"_id":{"$oid":"5837a57fa05283111e4d52c9"},"Last_activity":"2016-08-13 14:55:30","Creator_reputation":26229,"Question_score":2,"Answer_content":"Unlike Pearson correlation, covariance itself is not a measure of magnitude of linear relationship. It is a measure of co-variation (which could be just monotonic). This is because covariance depends not only on the strength of linear association but also on he magnitude of the variances. In order for covariance to be only the measure of linear association the variances must be controlled for somehow, without that control covariance might occur stronger under nonlinear underlying relationship than under linear one.Example: let there be completely linearly tied variables X and Y. Without touching Y move quite apart two polar utmost values of X. Now the relationship is only monotonic, but due to widening the range of X the covariance has enhanced.But covariance has theoretical upper limit equal  which is attainable only under exact linear relationship. In the example, if we back-rescale the widened X data to its original variance the newer value of covariance will drop lower, not higher, than the very initial value. And this is because we had abandoned the linear relationship for monotonic one. Linearity coefficient, the Pearson  is nothing else than the covariance relative that its upper limit.But the limiting fact that - under controlling the variances (such as standardizing them) - covariance is maximized when the bond is linear, does not make covariance the measure of the magnitude of linear association. It would be improper to call the covariance coefficient the \"linear covariance coefficient\".However, covariance is often used in place of Pearson correlation in analyses which assume linear models. For example, you can do factor analysis based on covariance matrix rather than correlation matrix. While covariance can tap not just linearity among the manifest variables, latent factors yet effect the variables but linearly (Pt 2), therefore accounting or taking responsibility only for linear bonds between them.Covariance the higher the...more monotonic is the association (i.e. the fewer are theinstances of inversions in the datagreater is the combined variability more equal are the two variabilitiesmore equal or proportional are the variables' values: undercondition that  cov will be maximal when (considering already centered variables), or, equivalently,under  cov will be maximal when .Linearity.","Display_name":"ttnphns","Creater_id":3277,"Start_date":"2016-08-13 13:59:51","Question_id":229667}
{"_id":{"$oid":"5837a57fa05283111e4d52ca"},"Last_activity":"2016-08-13 08:26:13","Creator_reputation":57712,"Question_score":2,"Answer_content":"As @RichardHardy points out in his comment, correlation is simply scaled covariance. So, they are useful for exactly the same types of relationships, but correlations are comparable across different relationships and correlations will not be affected by choice of units, while covariances will. set.seed(123)htin \u0026lt;- rnorm(100,68,3)wtpound \u0026lt;- htin*2.5 + rnorm(100,0,5)htm \u0026lt;- htin*0.0254wtkg \u0026lt;- wtpound/2.2cor(htin,wtpound) #0.81cov(htin,wtpound) #18.09cor(htm,wtkg) #0.81cov(htm,wtkg) #0.21If you have a perfect U shaped relation, both cov and corr will be 0:x \u0026lt;- seq(-4,4,by = 0.1)y \u0026lt;- x^2cor(x,y) #1.63*10^-16cov(x,y) #1.89*10^-15","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-13 08:26:13","Question_id":229667}
{"_id":{"$oid":"5837a57fa05283111e4d52d7"},"Last_activity":"2016-08-13 14:22:30","Creator_reputation":261,"Question_score":0,"Answer_content":"In continuous time hazard rate is the exit probability per unit time (given still in the initial state).  It may exceed one.  In discrete time, you look at probability to exit during the time interval, and obviously, as a probability, it must be .  I think you have to be clear of this distinction to understand the problem.It seems to me that you model the (continuous time) hazard rate as  (constant in time).  In this case the probability to have exited the initial state by  is  (this is exponential CDF).  If your hazard rate is not constant, you will have an integral instead of .Sorry, I am not quite familiar what role does the link function play in your specification.","Display_name":"Ott Toomet","Creater_id":94588,"Start_date":"2016-08-13 14:22:30","Question_id":229626}
{"_id":{"$oid":"5837a57fa05283111e4d52e4"},"Last_activity":"2016-08-13 14:07:14","Creator_reputation":91,"Question_score":0,"Answer_content":"The Fourier transform of discrete data with sampling rate  is periodic with period  (if we're talking about angular frequency, ) OR with period  (if we're talking about linear frequency, ).  The relationship between angular and linear frequency is .  We'll assume for simplicity that  for now and I'll be using linear frequency, .In fact, with real-valued data the Fourier transform (which is complex) is conjugate-symmetric about . If  is the Fourier transform of , then .  The spectrum is proportional to , so the spectrum is symmetric about .So, with a discretely sampled data (\"index limited\") we end up with a \"band limited\" Fourier transform.  The Nyquist frequency (the highest frequency which is not an alias of lower frequencies.  The below is an example of aliasing (can't tell one sinusoid from another - see Aliasing on Wikipedia).If  then the Nyquist frequency is  and the frequencies where the spectrum is calculated (if you have  data points) isseq(0, 1/(2*dt), by = 1/N)Usually you want to zeropad for the FFT, so you add some zeroes on at the end.One of the better options for estimating the spectrum is to use the Multitaper Method (package multitaper in R from CRAN).Sorry, this was all over the place... An example is below.  The plot that it creates demonstrates the bias (broadband bias) properties of the periodogram vs. the multitaper.An example I thoroughly enjoy comes from Percival and Walden, page 46 - an order 4 autoregressive simulation (so we know the theoretical spectrum):x_{t} = 2.7607x_{t-1} - 3.8106x_{t-2} + 2.6535x_{t-3} - 0.9238x_{t-4}library('TSA')library('multitaper')phi \u0026lt;- c(2.7607, -3.8106, 2.6535, -0.9238)# our dataN \u0026lt;- 500 # number of data pointsM \u0026lt;- 2048 # zeropadded length of seriesfreq \u0026lt;- seq(0, 0.5, by = 1/M)x \u0026lt;- arima.sim(n = N, model = list(ar = phi))# theoretical spectrumspec.thry \u0026lt;- ARMAspec(model = list(ar = phi), plot = FALSE)h.pgram \u0026lt;- rep(1/sqrt(N), N) #periodogram taper / window# prepared dataxh.pgram \u0026lt;- x * h.pgram# calculate the periodogramspec.pgram \u0026lt;- abs(fft(c(xh.pgram, rep(0, M-N)))[1:(M/2+1)])^2# multitaper - better solution very oftenspec.mtm \u0026lt;- spec.mtm(x, nFFT = M, plot = FALSE)# plottingpar(mar = c(4,4,1,1))plot(spec.thryspec, type = 'l', log='y', ylab = \"Spectrum\", xlab = \"Frequency\", lwd = 2)lines(freq, spec.pgram, col = rgb(1,0,0,0.7))lines(spec.mtmspec, col = rgb(0,0,1,0.7))legend(\"topright\", c(\"Theory\", \"Periodogram\", \"Multitaper\"), col = c(\"black\", rgb(1,0,0,0.7), rgb(0,0,1,0.7)), lwd = c(2,1,1))","Display_name":"driegert","Creater_id":70002,"Start_date":"2016-08-13 14:07:14","Question_id":229530}
{"_id":{"$oid":"5837a57fa05283111e4d52f1"},"Last_activity":"2016-08-13 14:02:50","Creator_reputation":518,"Question_score":0,"Answer_content":"First you should graph the correlation between the average rating and each of the variables separately to get an idea of the data. You also want to check the amount of variation in your Y variable (you don't want too much) and the variation in your X variable (you want a lot). Next run a linear regression. You said in the comments that the coefficients were negative, but what were the standard errors? I am guessing the variables are correlated so you may suffer from multicollinearity making inference about specific coefficients difficult. Just because the coefficients aren't what you ``want'' at first glance doesn't mean the functional form is wrong. Next, consider grouping the categories into larger categories and doing tests of joint significance for the larger categories. Or you could try other kinds of functional forms like squaring your variables to bring out more variation. Also check the R^2 to see if your RHS is getting at most of the variation in Y. If it is low, while the marginal effects may be meaningful, there is much more determining Y than what you are estimating. ","Display_name":"VCG","Creater_id":124896,"Start_date":"2016-08-13 14:02:50","Question_id":229692}
{"_id":{"$oid":"5837a57fa05283111e4d5304"},"Last_activity":"2016-08-13 12:11:50","Creator_reputation":12982,"Question_score":1,"Answer_content":"  Can someone please explain the link between fitting a GARCH model and the optimal exponential smoothing parameter?There are quite a few versions of exponential smoothing methods (for a taxonomy, see Table 7.8 in Section 7.6 of Hyndman \u0026amp; Athanasopoulos \"Forecasting: Principles and Practice\"). I assume you are talking about the simple exponential smoothing (coded  in the table) where \\hat x_t = \\lambda x_{t-1} + (1-\\lambda)\\hat x_{t-1} using the notation of the weigted average form of the model as introduced in Section 7.1 of the textbook.Meanwhile, the GARCH(1,1) model in its simplest form (w.r.t. the conditional mean equation) is \\begin{aligned}r_t \u0026amp;= \\sigma_t\\varepsilon_t; \\\\\\sigma_t^2 \u0026amp;= \\omega + \\alpha r_{t-1}^2 + \\beta\\sigma_{t-1}^2; \\\\\\varepsilon_t \u0026amp;\\sim i.i.d(0,1). \\\\\\end{aligned} The conditional variance equation of the GARCH(1,1) model is indeed similar to the simple exponential smoothing, especially once we put hats on the conditional variances to denote predicted values () and estimated values (): \\hat\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta\\hat\\sigma_{t-1}^2. However, this equation exhibits a few differences from the simple exponential smoothing:It includes a constant .It includes the lagged squared return  in place of the lagged conditional variance . Including  would make the equation infeasible in practice since the conditional variance is not observed, while the return  is.Its coefficients need not sum to zero.An IGARCH(1,1) model would come the closest to exponential smoothing. There we have  and , hence \\hat\\sigma_t^2 = \\alpha r_{t-1}^2 + (1-\\alpha)\\hat\\sigma_{t-1}^2; but still there is  in place of .  is a proxy for , albeit a noisy one, so this looks pretty much like simple exponential smoothing. But strictly speaking, I don't think you can directly claim this:  I am guessing that  and  also happen to be the optimal smoothing parameters.However, if  were observable and you could estimate a simple exponential smoothing model for it, you would probably get parameter estimates similar to the ones from IGARCH(1,1); namely,  would be close to .","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-13 12:00:42","Question_id":229685}
{"_id":{"$oid":"5837a57fa05283111e4d5311"},"Last_activity":"2016-08-13 12:56:23","Creator_reputation":21,"Question_score":2,"Answer_content":"If  and  are discretized vector representations of analog signals that are each normalized,  is the normalized cross correlation function. Then the density function for , at fixed time , is a transform of the noncentral Beta distribution. You can compute this density function from the relative error in approximating the vectorized representation of  with . In what follows, I'll change your notation: I am going to call the digital form of  vector  and the digital form of  vector . I will also assume  is a reference, or template signal with effectively infinite SNR. I will assume  is noisy. Unless  is entirely noise, it will include a signal  immersed in noise . In general,  is partially correlated with , and is not necessarily an \"amplitude scaled copy of \" (e.g., the matched filtering hypothesis is somewhat erroneous). Call the deterministic correlation between the signals (I'll get to noise shortly) . The  indicates the \"infinite\" SNR of the deterministic portion of the signals. The total correlation coefficient (your ) is my .Using these definitions, the relative square error in approximating a data stream    with a waveform template , is the ratio of the least-squares  error  to measured signal energy . This error may be re-written as a ratio of quadratic forms:\\begin{equation}\\begin{split}\\cfrac{\\vert \\vert \\mathbf{e} \\vert \\vert^{2}}{\\vert \\vert \\mathbf{x} \\vert \\vert^{2}} \u0026amp;= \\cfrac{\\vert \\vert  \\mathbf{x} - \\hat{A} \\mathbf{w }  \\vert \\vert^{2} }{ \\vert \\vert \\mathbf{x} \\vert \\vert^{2}}\\\\ \u0026amp;=  \\cfrac{ \\Vert  \\mathbf{x} - \\cfrac{\\langle   \\mathbf{x},\\,  \\mathbf{w} \\rangle}{ \\Vert \\mathbf{w }  \\Vert^{2}} \\mathbf{w } \\Vert^{2}} {\\vert \\vert \\mathbf{x} \\vert \\vert^{2}}\\\\\u0026amp;=1- \\cfrac{\\langle   \\mathbf{x},\\,  \\mathbf{w} \\rangle^{2} }{ \\vert \\vert \\mathbf{w }  \\vert \\vert^{2} \\vert \\vert \\mathbf{x }  \\vert \\vert^{2}}\\\\\u0026amp;=1 - r^{2}\\left( \\mathbf{x}\\right)\\end{split}\\label{eq:betaRelError}\\end{equation}where  is the maximum likelihood estimate for template waveform amplitude. I will rewrite the right hand side of the second equality in the preceding Equation as a ratio of subspace projections:\\begin{equation}\\begin{split} \\cfrac{ \\Vert  \\mathbf{x} - \\cfrac{\\langle   \\mathbf{x},\\,  \\mathbf{w} \\rangle}{ \\Vert \\mathbf{w }  \\Vert^{2}} \\mathbf{w } \\Vert^{2}} {\\vert \\vert \\mathbf{x} \\vert \\vert^{2}}=  \\cfrac{ \\Vert  P_{W}^{\\perp} \\left( \\mathbf{x} \\right) \\Vert ^{2}} { \\Vert P_{W}^{\\perp}\\left( \\mathbf{x} \\right) \\Vert ^{2} + \\vert \\vert P_{W} \\left( \\mathbf{x} \\right) \\vert \\vert ^{2}}%\\end{split}\\label{eq:betaSubspace}\\end{equation}where  is the subspace ,  is the orthogonal complement to ,  is the projector onto  and  is the projector onto . The denominator follows from the Pythagorean identity for Hilbert Spaces.  I define two noncentrality parameters from these terms:\\begin{equation}\\begin{split}\\lambda \u0026amp;=  \\cfrac{  \\Vert P_{W}^{}\\left( \\mathbb{E} \\left\\{  \\mathbf{x} \\right\\}  \\right) \\Vert ^{2}   }{\\sigma^{2}} = \\cfrac{\\vert \\vert P_{W} \\left( \\mathbf{u} \\right) \\vert \\vert ^{2}}{\\sigma^{2}} = \\rho_{\\infty}^{2}\\cfrac{\\Vert  \\mathbf{u} \\Vert ^{2}}{\\sigma^{2}}\\\\\\lambda^{\\perp}  \u0026amp;= \\cfrac{   \\Vert P_{W}^{\\perp}\\left(  \\mathbb{E} \\left\\{  \\mathbf{x} \\right\\}   \\right) \\Vert ^{2}   }{\\sigma^{2}} =  \\cfrac{\\vert \\vert P_{W}^{\\perp} \\left( \\mathbf{u} \\right) \\vert \\vert ^{2}}{\\sigma^{2}} = \\left(1 - \\rho_{\\infty}^{2}\\right)\\cfrac{\\Vert  \\mathbf{u} \\Vert ^{2}}{\\sigma^{2}}\\label{eq:lambdaDefs}\\end{split}\\end{equation}where the expected value and linear-projection operators commute. Now combine the previous three equations to rewrite :\\begin{equation}\\begin{split}1-\\left(1 - r^{2}\\left(\\mathbf{x} \\right)\\right) \u0026amp;=  \\frac{\\Vert P_{W}^{\\perp}\\left( \\mathbf{x} \\right) \\Vert ^{2} + \\vert \\vert P_{W} \\left( \\mathbf{x} \\right) \\vert \\vert ^{2}}{\\Vert P_{W}^{\\perp}\\left( \\mathbf{x} \\right) \\Vert ^{2} + \\vert \\vert P_{W} \\left( \\mathbf{x} \\right) \\vert \\vert ^{2}}\\\\\u0026amp;\\quad -\\cfrac{ \\Vert  P_{W}^{\\perp} \\left( \\mathbf{x} \\right) \\Vert ^{2}} { \\Vert P_{W}^{\\perp}\\left( \\mathbf{x} \\right) \\Vert ^{2} + \\vert \\vert P_{W} \\left( \\mathbf{x} \\right) \\vert \\vert ^{2}}\\\\\u0026amp;= \\cfrac{ \\Vert  P_{W}\\left( \\mathbf{x} \\right) \\Vert ^{2}} { \\Vert P_{W}^{\\perp}\\left( \\mathbf{x} \\right) \\Vert ^{2} + \\vert \\vert P_{W} \\left( \\mathbf{x} \\right) \\vert \\vert ^{2}}\\\\\u0026amp;\\overset{d}{=}  \\cfrac{ \\chi_{1}^{2}( \\lambda )} { \\chi_{1}^{2}( \\lambda ) +  \\chi_{N_{E} - 1}^{2}( \\lambda^{\\perp} ) } \\end{split}\\end{equation}where  is distributional equality,  is the noncentral Chi-square distribution with  degrees of freedom and noncentrality parameter , and  is the noncentral Chi-square distribution with one degree of freedom and noncentrality parameter . From the definition of the Beta distribution:\\begin{equation}\\begin{split}%r^{2}\\left(\\mathbf{x} \\right) \\sim \\text{B} \\left(t, \\frac{1}{2}, \\frac{1}{2}(N_{E}-1) ; \\lambda, \\lambda^{\\perp} \\right)\\end{split}\\end{equation}where  is the doubly noncentral Beta distribution function. It is evaluated at  (with the same domain as ), has  and  degrees of freedom, and noncentrality parameters  and . The scalar  denotes the effective number of independent samples within .  Now derive the PDF  for  from the density of  use a variable transformation; additionally consider values , but fix :\\begin{equation}\\begin{split}f_{R}\\left(r  ; \\tau \\right) \u0026amp;=  \\lvert r \\left( \\mathbf{x} \\right) \\rvert \\left[ \\text{B}\\left( r^{2}\\left(\\mathbf{x}\\right); \\,\\frac{1}{2}, \\,\\frac{1}{2}\\left( N_{E}-1\\right), \\,\\lambda,\\,\\lambda^{\\perp}\\right) + \\text{B}\\left( -r^{2}\\left(\\mathbf{x}\\right); \\,\\frac{1}{2}, \\,\\frac{1}{2}\\left( N_{E}-1\\right),\\, \\lambda,\\, \\lambda^{\\perp}\\right) \\right].%\\nonumber\\label{eq:rBetaDist}\\end{split}\\end{equation}Remember, your  is my .One minor self promotional note: there is a seismology paper in press with this derivation in it; please cite if you use (Bulletin of the Seismological Society of America, ''A Waveform Detector that Targets Template-Decorrelated Signals and Achieves its Predicted Performance: Demonstration with IMS Data (Part I)''; Vol 106, Issue 5. I'll omit my name).","Display_name":"Joshua D Carmichael","Creater_id":127407,"Start_date":"2016-08-11 14:56:49","Question_id":23962}
{"_id":{"$oid":"5837a57fa05283111e4d531e"},"Last_activity":"2016-08-13 11:34:34","Creator_reputation":8337,"Question_score":0,"Answer_content":"You're right to be confused. The exercise is unclear on two points:How is the mean daily count of customers who order tomatoes (8) supposed to be reconciled with the mean daily count of orders for tomatoes (25)? Is it that a given customer can make more than one order? If so, isn't the number of customers irrelevant, and only the number of orders matters? For that matter, is it guaranteed that only one tomato is needed per order? And what quantity is 1.2 the SD of?What exactly are we trying to estimate? Is a portion the same thing as an order? Do we want the probability that the landlord will run out of tomatoes within a single day, or over a longer period? Do unused tomatoes carry over?You could try asking your instructor.Maybe the sanest way to interpret the question is as follows. Over 20 days, the daily occurrence of an instantaneous event was counted. The daily counts have a mean of 25 and an SD of 1.2. What is the probability that the count of events on a given day will exceed 10? (I have completely ignored the 8.)A natural way to approach that problem is to construe the daily counts as i.i.d. draws from a negative binomial distribution. Estimate the parameters and then use the cumulative distribution function to compute the probability that a new draw will exceed 10.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-13 11:21:49","Question_id":229662}
{"_id":{"$oid":"5837a57fa05283111e4d532b"},"Last_activity":"2016-08-13 11:30:39","Creator_reputation":8337,"Question_score":2,"Answer_content":"Broadly, two approaches to dimension reduction in a supervised-learning problem are:Do the dimension reduction first, without consideration of the DV, and then use the dimension-reduced IVs for predicting the DV.Apply a predictive model that does dimension reduction in the process of trying to predict the DV using the IVs.The lasso belongs to approach 2. If you want approach 1, use an unsupervised dimension-reduction method such as principal components analysis.That said, if predictive accuracy is your goal, approach 2 is generally a better bet, because the relationships among the IVs may not have much to do with their relationship with the DV.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-13 11:30:39","Question_id":229681}
{"_id":{"$oid":"5837a57fa05283111e4d533a"},"Last_activity":"2016-08-13 11:23:11","Creator_reputation":1,"Question_score":0,"Answer_content":"Firstly, Variance Inflation Factor is inversely proportional to the  value. So checking this might help , but multi-collinearity is a thing to look at.Secondly , instead of  , adjusted  is a better way to validate a model. ","Display_name":"user127601","Creater_id":127601,"Start_date":"2016-08-13 11:23:11","Question_id":229672}
{"_id":{"$oid":"5837a57fa05283111e4d5349"},"Last_activity":"2016-08-13 10:59:21","Creator_reputation":8337,"Question_score":0,"Answer_content":"Since you want parameter values that work for any value of , you should fit the model to the whole dataset, as you mentioned first. That you get different estimates on a subset shouldn't be a surprise, and may be a manifestation of Simpson's paradox, but isn't a reason to believe those estimates in preference to the estimates from the whole sample.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-13 10:59:21","Question_id":229664}
{"_id":{"$oid":"5837a57fa05283111e4d5356"},"Last_activity":"2016-08-13 10:51:36","Creator_reputation":8337,"Question_score":1,"Answer_content":"It's ultimately a matter of taste. How conservative do you want to be? What kind of error rates are you after—are you okay with a type-I error rate of .05 for each comparison, or do you want a rate of .05 for the whole set of comparisons, or something else?","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-13 10:51:36","Question_id":229677}
{"_id":{"$oid":"5837a57fa05283111e4d5363"},"Last_activity":"2016-08-13 07:03:22","Creator_reputation":650,"Question_score":1,"Answer_content":"To get the factors, use:cut(dataset, quantile(dataset))From the help:## Default S3 method:cut(x, breaks)x          a numeric vector which is to be converted to a factor by cutting.breaks   either a numeric vector of two or more cut points or a single number (greater than or equal to 2) giving the number of intervals into which x is to be cut.To use it on multiple columns you could use the data.table package:df \u0026lt;- data.table(df)cut_quantile \u0026lt;- function (x) cut(x, quantile(x))df[, lapply(.SD, cut_quantile), .SDcols = c('V7', 'V8', 'V9', 'V10')]","Display_name":"Wok","Creater_id":1351,"Start_date":"2011-03-11 06:38:19","Question_id":8151}
{"_id":{"$oid":"5837a57fa05283111e4d5364"},"Last_activity":"2016-08-13 05:11:12","Creator_reputation":2256,"Question_score":1,"Answer_content":"You can use the content method in the bin function in the OneR package for that. It works on vectors and dataframes.library(OneR)set.seed(2)df \u0026lt;- data.frame(a = rnorm(900), b = rnorm(900))df_bin \u0026lt;- bin(df, nbins = 3, method = \"content\")table(df_binb)## ## (-4.08,-0.432] (-0.432,0.399]   (0.399,3.17] ##            300            300            300(Full disclosure: I am the author of this package)","Display_name":"vonjd","Creater_id":230,"Start_date":"2016-08-13 05:11:12","Question_id":8151}
{"_id":{"$oid":"5837a57fa05283111e4d5373"},"Last_activity":"2016-08-13 10:21:05","Creator_reputation":21588,"Question_score":0,"Answer_content":"I disagree with @DJohnson -- this does not look like a probability sample -- simply because the probabilities of selection are not quantifiable for quota samples. The phrasing \"unique and rigorous\" is another obvious smell. Face-to-face surveys can be designed using the established sampling techniques (e.g., Kish's classic sampling book) based on the reliable ONS data, without the need to come up with anything \"unique\". In these samples, interviewers are instructed by the sampling statistician who to interview, rather than allowed to pick up somebody from the crowd (which is what quota sampling by location would eventually end up being) -- and the interviewers will probably pick up the easier-to-interview cases that way, biasing the sample.That does not mean that the ultimate sample is awful. It may be reasonably close to the target population on demographics, so whatever is closely related to demographics may work out OK. But many behaviors are rather poorly explained by demographics, and nonprobability samples often have a poor record of tracking behaviors. Then again, for marketing purposes you may not need the fully rigorous sample if all you need is to figure out what audience your product should target.For more information, you can check out the following reports by the American Association for Public Opinion Research, the leading professional society that deals with issues of survey methodology:Task Force on Non-probability Samples (2013) -- of which quota sampling is an example.Task Force on Evaluating Survey Quality (2016) -- checklist of questions that a consumer of survey data should be asking.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-13 10:21:05","Question_id":178313}
{"_id":{"$oid":"5837a57fa05283111e4d5380"},"Last_activity":"2016-08-12 11:45:28","Creator_reputation":75990,"Question_score":3,"Answer_content":"In general, the reason we use non-parametric analyses is that we don't want to have to make (or count on) any distributional assumptions.  In other words, you might choose the Mann-Whitney U-test because you think the variances are unequal, for example.  In short, non-parametric analyses do not require equal variance.  Let me speculate on what may have been the source of confusion.  The Mann-Whitney U-test is a test of one distribution is stochastically larger.  That means that if you drew a single value from each population, it is likely that the value from the one distribution will be higher than the other value. Now if the two distributions are the same, except that one is shifted up relative to the other, then stochastically larger implies the higher population has a higher mean.  This is only true if the distributions are identical except for shifted.  Note further that identical distributions entails equal variances.  If your goals would only be satisfied by a test of the equality of means, and your distributions are neither sufficiently normal nor identically shaped, you could try bootstrapping.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-08-11 19:38:04","Question_id":229439}
{"_id":{"$oid":"5837a57fa05283111e4d538f"},"Last_activity":"2012-11-22 11:40:23","Creator_reputation":21588,"Question_score":1,"Answer_content":"I have looked at this recently, and concluded that nothing works in R the way I wanted to. So I analyzed my multilevel complex survey data in Stata using gllamm package, which can account for weights at multiple levels and clustering (but still can't do stratification). I would be happy to hear of otherwise available packages, but generally the multilevel people and the survey people do not overlap that much.","Display_name":"StasK","Creater_id":5739,"Start_date":"2012-11-22 11:40:23","Question_id":41917}
{"_id":{"$oid":"5837a57fa05283111e4d539c"},"Last_activity":"2015-11-12 13:52:31","Creator_reputation":39311,"Question_score":4,"Answer_content":"Propensity scores are usually developed using logistic regression and we usually use a \"kitchen sink\" approach.  I don't believe in doing univariable analysis to decide which variables to include, and you may easily have a power problem that prevents you from seeing a real imbalance.  It is typical to adjust for observed variables, imbalanced or not.  I am liberal about using regression splines in the propensity model so as to not assume linearity (which translates as a shift in the means only; a quadratic effect would allow means and variances to differ by treatment group).Make sure that a weighted analysis is efficient, as compared with covariate adjustment using regression splines of the logit of propensity.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2015-11-12 13:52:31","Question_id":181505}
{"_id":{"$oid":"5837a57fa05283111e4d53af"},"Last_activity":"2016-08-13 08:16:26","Creator_reputation":3619,"Question_score":3,"Answer_content":"If the excess of zeroes in the continuous predictor represent the fact that those cases are fundamentally different then you could consider using two variables, your existing X1 plus a zero/one variable which represents zero versus the rest. So an example might be expense on drugs where if you are well it is zero and of you are ill it can have any value. You might be able to do something similar with X2 as well. You feeling that the residuals are important is of course correct whatever you decide to do.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-13 08:16:26","Question_id":229650}
{"_id":{"$oid":"5837a57fa05283111e4d53bb"},"Last_activity":"2016-08-13 08:12:54","Creator_reputation":12260,"Question_score":0,"Answer_content":"I'll leave my old answer at the end to provide context for your comment.It seems to me that your rectangular-versus-ellipsoid thought experiment gives an interesting hint of a problem with multiple comparisons: your multiple test example is in some sense projecting information down in dimensionality, then back up, losing information in the process.That is, the joint probability is ellipsoid precisely because you have two Gaussian distributions, which will jointly yield an ellipsoid, whose circularity is determined by the relative variance of the two distributions, and whose major axis' slope is determined by the correlation of the two sets of data. Since you specify the two datasets are independent, the major axis is parallel to the x or y axis.On the other hand, your two-test example projects Gaussian distributions down to a 1-D range and when you then combine the two tests into a single, 2-D graph (projecting back up), you have lost information and the resulting 95% area is a rectangular rather than the appropriate ellipsoid. And things get worse if the two datasets are correlated.So it seems to me that this might be an indication that multiple testing is losing information due to what we might describe as projecting information down -- losing information in the process -- then back up. So the shape of the resulting pseudo-joint density is incorrect and attempting to scale its axes via something like a Boneferroni can't fix that.So in answer to your question, I'd say yes, we prefer an ellipse in our joint distribution rather than the incorrect (due to loss of information) rectangle of our pseudo-joint distribution. Or perhaps the issue is that you've created a pseudo-joint density in the first place.BUT your question is more philosophical than that, and I have to support Amoeba's answer that it's not simply a matter of the math. For example, what if you pre-registered your jellybean experiment with a precise \"green jelly beans\" as part of your hypothesis, rather than an imprecise \"greenish\". You perform the experiment and find no statistically significant effect. Then your lab assistant shows you a photo they took of themselves in front of all of the jellybean doses -- what a Herculean task they performed! And something you say leads the assistant to realize that you are partially colorblind.It turns out that what you called \"green\" we're actually green and aqua jellybeans! With the help of the photo, the assistant properly codes the results and it turns out green jellybeans are significant! Your career is saved! Except you've just done a multiple comparison: you took two swipes at the data, and if you had found significance in the first place, no one would have ever known any different.This isn't a matter of you p-value-hacking. It was an honest correction, but your motivation doesn't matter here.And if we're being totally honest, \"green\" is no more specific than \"greenish\". First, in terms of the actual color, and then in terms of the fact that green is most likely a proxy for other ingredients.And what if you had never discovered your error, but for some reason your assistant replicated the experiment and the second results were significant? Basically the same case, though you did collect two sets of data. At this point, I'm starting to wander, so let me summarize by again saying I believe Amoeba has it right and your \"it is or isn't because of mathematics\" idea is technically correct, but not tractable in the real world.OLD answer: Is this question actually about correlation? I'm thinking more of a Mahalanobis Distance kind of issue, where independently looking at the 95% x1 and the 95% x2 yields a rectangle, but this assumes that x1 and x2 are not correlated. While using the Mahalanobis Distance (an ellipse that is shaped based on the correlation between x1 and x2) is superior. The ellipse extends outside of the rectangle, so it accepts some points that are outside of the rectangle, but it also rejects points inside the rectangle. Assuming x1 and x2 are correlated to some degree.Otherwise, if you assume x1 and x2 have 0 correlation, what distribution are you assuming for each? If uniform, you'd get a rectangular region, if normal you'll get an elliptical region. Again, this would be independent of multiple testing corrections or not.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-08-10 07:52:28","Question_id":229193}
{"_id":{"$oid":"5837a57fa05283111e4d53bc"},"Last_activity":"2016-08-12 00:20:26","Creator_reputation":3038,"Question_score":1,"Answer_content":"@amoeba: on the example with the jelly beans I would like to argue as follows (note, I just want to understand): Let's say that there are 20 different colors of jelly beans, let's call these , and let  be the color 'green'. So, with your example the p-values for color  (we note this as ) will be  when  and . Theory 1: green jelly beans cause acneIf you have developed a theory that green jelly beans cause acne, then you should test the hypothesis : ''jelly beans of color  have no effect on acne'' versus : ''jelly beans of color  cause acne''. This is obviously not a multiple testing problem, so you do not have to adjust the p-values. Theory 2: only green jelly beans cause acneIn that case you should have '': green jelly beans cause acne AND jelly beans of color  do not cause acne'' and  is then ''green jelly beans do not cause acne OR  such that beans of color  cause acne''. This is a multiple testing problem and requires adjusted p-values. Theory 3: jelly beans (of whatever color) cause acneIn that case : ''jelly beans of color  cause acne AND ''jelly beans of color  cause acne AND .... AND ''jelly beans of color  cause acne'' and  is the opposite.  This is again a multiple testing problem.  Theory ...ConclusionAnyhow, it can be seen that these theories are fundamentally different and whether or not p-value adjustment is required depends on that, not on ''philosophy'', at least that is my understanding. P.S. for the reaction to the example of @FrankHarrell see ''EDIT'' at the bottom of my answer to What\u0026#39;s wrong with Bonferroni adjustments?","Display_name":"fcop","Creater_id":83346,"Start_date":"2016-08-11 09:02:17","Question_id":229193}
{"_id":{"$oid":"5837a57fa05283111e4d53bd"},"Last_activity":"2016-08-11 16:57:32","Creator_reputation":29995,"Question_score":3,"Answer_content":"I think you are missing @FrankHarrell's point here (I do not currently have access to the Perneger's paper discussed in the linked thread, so cannot comment on it).The debate is not about math, it is about philosophy. Everything you wrote here is mathematically correct, and clearly Bonferroni correction allows to control the familywise type I error rate, as your \"joint test\" also does. The debate is not at all about the specifics of Bonferroni itself, it is about multiple testing adjustments in general.Everybody knows an argument for multiple testing corrections, as illustrated by the famous XKCD jelly beans comic:Here is a counter-argument: if I developed a really convincing theory predicting that specifically green jelly beans should cause acne; and if I ran experiment to test for it and got nice and clear ; and if it so happened that some other PhD student in the same lab for whatever reason ran nineteen tests for all other jelly beans colors  getting  every time; and if now our advisor wants to put all of that in one single paper; -- then I would be totally against \"adjusting\" my p-value from  to .Note that the experimental data in the Argument and in the Counter-Argument might be exactly the same. But the interpretation differs. This is fine, but illustrates that one should not be obliged by doing multiple testing corrections in all situations. It is ultimately a matter of judgment. Crucially, real-life scenarios are usually not as clear cut as here and tend to be in between #1 and #2. See also Frank's example in his answer.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-08-11 06:34:17","Question_id":229193}
{"_id":{"$oid":"5837a57fa05283111e4d53ce"},"Last_activity":"2014-11-21 03:32:22","Creator_reputation":111,"Question_score":0,"Answer_content":"Receiver Operating Characteristic curves and associated Area Under Curve measures work with binary classifiers. There is more complexity when multiple values for the labels are possible (see Wikipedia link)I believe the R package PerfMeas supports this so you could take advantage of the RapidMiner R extension to get access to this.","Display_name":"awchisholm","Creater_id":28188,"Start_date":"2014-11-21 03:32:22","Question_id":124496}
{"_id":{"$oid":"5837a57fa05283111e4d53db"},"Last_activity":"2016-08-13 07:25:57","Creator_reputation":324,"Question_score":2,"Answer_content":"It doesn't really seem like your proof makes sense, but it might just be that you are skipping steps, which is making it difficult to understand.  Here is a more complete proof of \\begin{align*}Var(\\bar{X}) \u0026amp;= E\\Big( (\\bar{X} - \\mu)^2 \\Big)  \\\\\u0026amp;= E\\Big( \\big(\\dfrac{1}{n}\\sum_{i=1}^n X_i - \\mu\\big)^2 \\Big)  \\\\\u0026amp;= E\\Big( \\big(\\dfrac{1}{n}(X_1+\\ ...\\ + X_n) - \\mu\\big)^2 \\Big)  \\\\\u0026amp;= E\\Big( \\big(\\dfrac{1}{n}\\big((X_1-\\mu)+\\ ...\\ + (X_n-\\mu)\\big)\\big)^2 \\Big)  \\\\\u0026amp;= \\dfrac{1}{n^2}E\\Big( \\big((X_1-\\mu)+\\ ...\\ + (X_n-\\mu)\\big)^2 \\Big)  \\\\\u0026amp;= \\dfrac{1}{n^2}E\\Big( \\sum_{i=1}^n \\sum_{j=1}^n(X_i - \\mu)(X_j-\\mu) \\Big)  \\\\\u0026amp;= \\dfrac{1}{n^2}\\sum_{i=1}^n \\sum_{j=1}^n E\\Big((X_i - \\mu)(X_j-\\mu)\\Big)  \\\\\\end{align*}Note, however, that since the s are independent, then if , then .  Thus:\\begin{align*}\u0026amp;= \\dfrac{1}{n^2}\\sum_{i=1}^n  E\\Big((X_i - \\mu)^2\\Big)  \\\\\u0026amp;= \\dfrac{1}{n^2}\\sum_{i=1}^n  Var(X_i)  \\\\\u0026amp;= \\dfrac{1}{n^2}\\sum_{i=1}^n  \\sigma^2  \\\\\u0026amp;= \\dfrac{1}{n^2}  n\\sigma^2  \\\\\u0026amp;= \\dfrac{\\sigma^2}{n}\\end{align*}","Display_name":"David C","Creater_id":96030,"Start_date":"2016-08-13 07:03:53","Question_id":229653}
{"_id":{"$oid":"5837a57fa05283111e4d53dc"},"Last_activity":"2016-08-13 05:48:55","Creator_reputation":3337,"Question_score":2,"Answer_content":"\\begin{align*}Var(\\bar{X}) \u0026amp;= Var\\left(\\dfrac{1}{n}\\sum_{i=1}^n X_i\\right)\\\\\u0026amp;= \\dfrac{1}{n^2}Var\\left(\\sum_{i=1}^n X_i\\right)\\end{align*}variance of sum is equal to sum of variances because  are independent\\begin{align*}\u0026amp;= \\dfrac{1}{n^2}\\sum_{i=1}^n Var(X_i) \\\\\u0026amp;= \\dfrac{n\\sigma^2}{n^2}\\\\\u0026amp;= \\dfrac{\\sigma^2}{n}\\end{align*}","Display_name":"bdeonovic","Creater_id":17661,"Start_date":"2016-08-13 05:48:55","Question_id":229653}
{"_id":{"$oid":"5837a57fa05283111e4d53e9"},"Last_activity":"2016-08-13 07:11:39","Creator_reputation":39311,"Question_score":13,"Answer_content":"What is wrong with the Bonferroni correction besides the conservatism mentioned by others is what's wrong with all multiplicity corrections.  They do not follow from basic statistical principles and are arbitrary; there is no unique solution to the multiplicity problem in the frequentist world.  Secondly, multiplicity adjustments are based on the underlying philosophy that the veracity of one statement depends on which other hypotheses are entertained.  This is equivalent to a Bayesian setup where the prior distribution for a parameter of interest keeps getting more conservative as other parameters are considered.  This does not seem to be coherent.  One could say that this approach comes from researchers having been \"burned\" by a history of false positive experiments and now they want to make up for their misdeeds.To expand a bit, consider the following situation.  An oncology researcher has made a career of studying efficacy of chemotherapies of a certain class.  All previous 20 of her randomized trials have resulted in statistically insignificant efficacy.  Now she is testing a new chemotherapy in the same class.  The survival benefit is significant with .  A colleague points out that there was a second endpoint studied (tumor shrinkage) and that a multiplicity adjustment needs to be applied to the survival result, making for an insignificant survival benefit.  How is it that the colleague emphasized the second endpoint but couldn't care less about adjusting for the 20 previous failed attempts to find an effective drug?  And how would you take into account prior knowledge about the 20 previous studies if you weren't Bayesian?  What if there had been no second endpoint.  Would the colleague believe that a survival benefit had been demonstrated, ignoring all previous knowledge?","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2014-10-17 06:32:32","Question_id":120362}
{"_id":{"$oid":"5837a57fa05283111e4d53ea"},"Last_activity":"2016-08-11 05:55:56","Creator_reputation":1259,"Question_score":11,"Answer_content":"  He summarized saying that Bonferroni adjustment have, at best, limited applications in biomedical research and should not be used when assessing evidence about specific hypothesis.The Bonferroni correction is one of the simplest and most conservative multiple comparisons technique. It is also one of the oldest and has been improved upon greatly over time. It is fair to say that the Bonferroni adjustments have limited application in almost all situations. There is almost certainly a better approach. That is to say, you will need to correct for multiple comparisons but you can choose a method that is less conservative and more powerful.Less ConservativeMultiple comparisons methods protect against getting at least one false positive in a family of tests. If you perform one test at the  level then you are allowing a 5% chance of getting a false positive. In other words, you reject your null hypothesis erroneously. If you perform 10 tests at the  level then this increases to   = ~40% chance of getting a false positive With the Bonferroni method you use an  at the lowest end of the scale  (i.e. ) to protect your family of  tests at the  level. In other words, it is the most conservative. Now, you can increase  above the lower limit set by Bonferroni (i.e. make your test less conservative) and still protect your family of tests at the  level. There are many ways to do this, the Holm-Bonferroni method for example or better still False Discovery RateMore PowerfulA good point brought up in the paper referenced is that the likelihood of type II errors is also increased so that truly important differences are deemed non-significant.This is very important. A powerful test is one that finds significant results if they exist. By using the Bonferroni correction you end up with a less powerful test. As Bonferroni is conservative, the power is likely to be considerable reduced. Again, one of the alternative methods eg False Discovery Rate, will increase the power of the test. In other words, not only do you protect against false positives, you also improve your ability to find truly significant results.So yes, you should apply some correction technique when you have multiple comparisons. And yes, Bonferroni should probably be avoided in favour of a less conservative and more powerful method","Display_name":"martino","Creater_id":9233,"Start_date":"2014-10-17 05:27:19","Question_id":120362}
{"_id":{"$oid":"5837a57fa05283111e4d53eb"},"Last_activity":"2016-08-07 00:08:01","Creator_reputation":3038,"Question_score":5,"Answer_content":"Maybe it's good to explain the  ''reasoning behind'' multiple testing corrections like the one of Bonferroni.  If that is clear then you will be able to judge yourself whether you should apply them or not. In a hypothesis test one tries to find evidence for some known or assumed fact about the real world.  It is similar to ''proof by contradiction'' in mathematics, i.e. if one wants to prove that e.g. a parameter  is non-zero, then one will assume that the opposite is true, i.e. one assumes that  and one tries to find something that is impossible under that assumption.  In statistics things are rarely impossible, but they can be very improbable.  So if we want to show that  then we assume the opposite namely  and we try to find something very improbable.  Very improbable is defined in terms of a probability lower than an a priori fixed significance level . Note that, because of the analogy I will use terms such as ''statistically proven'' or ''statistical evidence'', these terms aree just used for didactical reasons and are not used in general. In order to find that ''low probability'' we draw a random sample form a distribution that is known when  (our assumption of the ''opposite'' of what we want to prove) is true.  As we assumted  te be true we can compute the probability of this outcome (more precise something that is at least as extreme as this outcome). As the sample is a random draw from a distribution, it may be that we obtain a low probability just by ''bad luck with the sample'' and then we reject  just because we had bad luck with the sample.  Rejecting  means that we consider to have found evidence for  but it is false evidence in these cases where we have bad luck with the sample.  False evidence is a bad thing in science because we believe to have gained true knowledge about the world,  but in fact we may have had bad luck with the sample.  This kinds of errors should consequently be controled.  Therefore one should put an upper limit on the probability of this kind of evidence, or one should control the type I error.  This is done by fixing an acceptable significance level in advance.  So if we fix our significance level at  then we are saying that we are ready to reject  when it is true (because of bad luck with the sample) with a chance of .  As (see supra) rejecting  is ''statistical evidence'' for  this means that we falsely consider  as ''statistically proven''.Assume now that we have two parameters, and we want to show that that at least one is different from zero.  Follwing the logic of ''proof by contradiction'' we will assume  versus  and that we use a signficance level . One possibility to do this is to split this hypothesis test and to test  versus  and to test  versus  both at the significance level . To do both tests we draw one sample , so we use one and the same sample to do both of these tests.  I may have bad luck with that one sample and erroneously reject  but with that same sample I may also have bad luck with the sample for the second test and erroneously reject Therefore, the chance that at least one of the two is an erroneous rejection is 1 minus the probability that both are not rejected, i.e. , where it was assumed that both tests are independent.  In other words, the type I error has ''inflated'' to 0.0975 which is almost double . The important fact here is that the two tests are based on one and the sampe sample !Note that we have assumed independence.  If you can not assume independence then you can show, using the Bonferroni inequalityH_0^{(1)}: \\mu_1=0H_1^{(1)}: \\mu_1 \\ne 0H_0^{(2)}: \\mu_1=0H_1^{(2)}: \\mu_2 \\ne 0H_0^{(12)}: \\mu_1=0 \\\u0026amp; \\mu_2 = 0H_1^{(12)}: \\mu_1 \\ne 0 | \\mu_2 \\ne 0H_0^{(1)}H_1^{(1)}H_0^{(2)}H_1^{(2)}$ at the 2.5% level.","Display_name":"fcop","Creater_id":83346,"Start_date":"2016-08-05 00:52:13","Question_id":120362}
{"_id":{"$oid":"5837a57fa05283111e4d53ec"},"Last_activity":"2016-08-06 11:40:34","Creator_reputation":263,"Question_score":5,"Answer_content":"Thomas Perneger is not a statistician and his paper is full of mistakes. So I wouldn't take it too seriously. It's actually been heavily criticized by others.For example, Aickin said Perneger's paper \"consists almost entirely of errors\": Aickin, \"Other method for adjustment of multiple testing exists\", BMJ. 1999 Jan 9; 318(7176): 127.Also, none of the p-values in the original question are \u0026lt; .05 anyway, even without multiplicity adjustment. So it probably doesn't matter what adjustment (if any) is used.","Display_name":"Bonferroni","Creater_id":109785,"Start_date":"2016-08-04 18:23:35","Question_id":120362}
{"_id":{"$oid":"5837a57fa05283111e4d53ed"},"Last_activity":"2014-10-17 11:37:05","Creator_reputation":1349,"Question_score":4,"Answer_content":"A nice discussion of Bonferroni correction and effect size http://beheco.oxfordjournals.org/content/15/6/1044.full.pdf+htmlAlso, Dunn-Sidak correction and Fisher's combined probabilities approach are worth considering as alternatives. Regardless of the approach, it is worth reporting both adjusted and raw p-values plus effect size, so that the reader can have the freedom of interpreting them.","Display_name":"katya","Creater_id":57390,"Start_date":"2014-10-17 11:37:05","Question_id":120362}
{"_id":{"$oid":"5837a57fa05283111e4d53ee"},"Last_activity":"2014-10-17 08:41:45","Creator_reputation":111,"Question_score":3,"Answer_content":"One should look at the \"False Discovery Rate\" methods as a less conservative alternative to Bonferroni.  SeeJohn  D. Storey, \"THE POSITIVE FALSE DISCOVERY RATE: A BAYESIANINTERPRETATION AND THE q-VALUE,\"The Annals of Statistics2003, Vol. 31, No. 6, 2013–2035.","Display_name":"John Mark","Creater_id":8111,"Start_date":"2014-10-17 08:41:45","Question_id":120362}
{"_id":{"$oid":"5837a57fa05283111e4d53ef"},"Last_activity":"2014-10-16 13:47:00","Creator_reputation":4284,"Question_score":3,"Answer_content":"For one, it's extremely conservative. The Holm-Bonferroni method accomplishes what the Bonferonni method accomplishes (controlling the Family Wise Error Rate) while also being uniformly more powerful. ","Display_name":"TrynnaDoStat","Creater_id":23801,"Start_date":"2014-10-16 13:47:00","Question_id":120362}
{"_id":{"$oid":"5837a57fa05283111e4d5404"},"Last_activity":"2016-07-08 04:39:46","Creator_reputation":31,"Question_score":3,"Answer_content":"update: I´ve crossposted the question at statalist.org and got an answer there:http://www.statalist.org/forums/forum/general-stata-discussion/general/1348073-f-test-differences-stata-and-rbasically the problem was that the contrast settings differed between R and Stata \u0026amp; I did not compute the same SS type.By default Stata computes type 3 SS, but I specified type 2 SS in R. But when computing type 3 SS in R, you should NOT use the default contrasts (contr.treatment), but instead use some orthogonal contrast (like contr.sum), see this link:http://www.mail-archive.com/r-help@s.../msg69781.htmlThus when I did the ANOVA with type 3 SS and contr.sum, I got the same output as in Stata where I didn´t specify anything.","Display_name":"user115328","Creater_id":115328,"Start_date":"2016-07-08 04:39:46","Question_id":212068}
{"_id":{"$oid":"5837a57fa05283111e4d5413"},"Last_activity":"2016-08-13 03:30:19","Creator_reputation":29995,"Question_score":35,"Answer_content":"PCA computes eigenvectors of the covariance matrix (\"principal axes\") and sorts them by their eigenvalues (amount of explained variance). The centered data can then be projected onto these principal axes to yield principal components (\"scores\"). For the purposes of dimensionality reduction, one can keep only a subset of principal components and discard the rest. (See here for a layman's introduction to PCA.)Let  be the  data matrix with  rows (data points) and  columns (variables, or features). After subtracting the mean vector  from each row, we get the centered data matrix . Let  be the  matrix of some  eigenvectors that we want to use; these would most often be the  eigenvectors with the largest eigenvalues. Then the  matrix of PCA projections (\"scores\") will be simply given by .This is illustrated on the figure below: the first subplot shows some centered data (the same data that I use in my animations in the linked thread) and its projections on the first principal axis. The second subplot shows only the values of this projection; the dimensionality has been reduced from two to one: In order to be able to reconstruct the original two variables from this one principal component, we can map it back to  dimensions with . Indeed, the values of each PC should be placed on the same vector as was used for projection; compare subplots 1 and 3. The result is then given by . I am displaying it on the third subplot above. To get the final reconstruction , we need to add the mean vector  to that:\\boxed{\\text{PCA reconstruction} = \\text{PC scores} \\cdot \\text{Eigenvectors}^\\top + \\text{Mean}}Note that one can go directly from the first subplot to the third one by multiplying  with the  matrix; it is called a projection matrix. If all  eigenvectors are used, then  is the identity matrix (no dimensionality reduction is performed, hence \"reconstruction\" is perfect). If only a subset of eigenvectors is used, it is not identity.This works for an arbitrary point  in the PC space; it can be mapped to the original space via .Discarding (removing) leading PCs Sometimes one wants to discard (to remove) one or few of the leading PCs and to keep the rest, instead of keeping the leading PCs and discarding the rest (as above). In this case all the formulas stay exactly the same, but  should consist of all principal axes except for the ones one wants to discard. In other words,  should always include all PCs that one wants to keep.Caveat about PCA on correlationWhen PCA is done on correlation matrix (and not on covariance matrix), the raw data  is not only centered by subtracting   but also scaled by dividing each column by its standard deviation . In this case, to reconstruct the original data, one needs to back-scale the columns of  with  and only then to add back the mean vector .Image processing exampleThis topic often comes up in the context of image processing. Consider Lenna -- one of the standard images in image processing literature (follow the links to find where it comes from). Below on the left, I display the grayscale variant of this  image (file available here).We can treat this grayscale image as a  data matrix . I perform PCA on it and compute  using the first 50 principal components. The result is displayed on the right. Reverting SVDPCA is very closely related to singular value decomposition (SVD), see Relationship between SVD and PCA. How to use SVD to perform PCA? for more details. If a  matrix  is SVD-ed as  and one selects a -dimensional vector  that represents the point in the \"reduced\" -space of  dimensions, then to map it back to  dimensions one needs to multiply it with .Examples in R, Matlab, Python, and StataI will conduct PCA on the Fisher Iris data and then reconstruct it using the first two principal components. I am doing PCA on the covariance matrix, not on the correlation matrix, i.e. I am not scaling the variables here. But I still have to add the mean back. Some packages, like Stata, take care of that through the standard syntax. Thanks to @StasK and @Kodiologist for their help with the code.We will check the reconstruction of the first datapoint, which is:5.1        3.5         1.4        0.2Matlabload fisheririsX = meas;mu = mean(X);[eigenvectors, scores] = pca(X);nComp = 2;Xhat = scores(:,1:nComp) * eigenvectors(:,1:nComp)';Xhat = bsxfun(@plus, Xhat, mu);Xhat(1,:)Output:5.083      3.5174      1.4032     0.21353RX = iris[,1:4]mu = colMeans(X)Xpca = prcomp(X)nComp = 2Xhat = Xpcarotation[,1:nComp])Xhat = scale(Xhat, center = -mu, scale = FALSE)Xhat[1,]Output:Sepal.Length  Sepal.Width Petal.Length  Petal.Width    5.0830390    3.5174139    1.4032137    0.2135317For worked out R example of PCA reconstruction of images see also this answer.Pythonimport numpy as npimport sklearn.datasets, sklearn.decompositionX = sklearn.datasets.load_iris().datamu = np.mean(X, axis=0)pca = sklearn.decomposition.PCA()pca.fit(X)nComp = 2Xhat = np.dot(pca.transform(X)[:,:nComp], pca.components_[:nComp,:])Xhat += muprint(Xhat[0,])Output:[ 5.08718247  3.51315614  1.4020428   0.21105556]Note that this differs slightly from the results in other languages. That is because Python's version of the Iris dataset contains mistakes. Statawebuse iris, clearpca sep* pet*, components(2) covariancepredict _seplen _sepwid _petlen _petwid, fitlist in 1  iris   seplen   sepwid   petlen   petwid    _seplen    _sepwid    _petlen    _petwid  setosa      5.1      3.5      1.4      0.2   5.083039   3.517414   1.403214   .2135317  ","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-08-09 16:52:47","Question_id":229092}
{"_id":{"$oid":"5837a57fa05283111e4d5420"},"Last_activity":"2016-08-13 02:35:40","Creator_reputation":null,"Question_score":9,"Answer_content":"At least on linux, RKWard offers the best functionality. The new RStudio appears quite promising as well.","Display_name":"user3502","Creater_id":null,"Start_date":"2011-03-01 22:24:33","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5421"},"Last_activity":"2015-04-14 13:23:35","Creator_reputation":154,"Question_score":1,"Answer_content":"Having worked with the  (Base) R  RStudio  Revolution R Enterprise   in Windows environment, I strongly suggest \"Revolution R Enterprise\".I accept that its installing takes little longer (it is 600-700MB) if compared with BaseR and RStudio, but anyway, the Object Browser of Revo R, the easiness of package installation procedure, management of variables, etc. etc. there are many things that - according to me - makes Revo R the best one (acc. to  me).That said, Revo R being purchased by Microsoft is - to me- one of its drawbacks since MS is eventually profit-oriented firm and may change its free nature sooner or later.","Display_name":"Erdogan CEVHER","Creater_id":30822,"Start_date":"2015-04-14 13:23:35","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5422"},"Last_activity":"2015-01-19 19:24:32","Creator_reputation":1,"Question_score":-2,"Answer_content":"If you don't want to code R, but want graphical user interface like SPSS, there is a new cloud based software, Number Analytics (). It is based on cloud so you don't need to install the program. It is freemium model starting free. It is for beginners who don't have much knowledge about statistics. The biggest selling point is that it does interpret the statistical results. Color table, and built-in graphs also helps. ","Display_name":"Nam","Creater_id":66982,"Start_date":"2015-01-19 19:24:32","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5423"},"Last_activity":"2014-03-06 10:03:55","Creator_reputation":1895,"Question_score":115,"Answer_content":"You can also try the brand-new RStudio.  Reasonably full-featured IDE with easy set-up.  I played with it yesterday and it seems nice.UpdateI now like RStudio even more.  They actively implement feature requests, and it shows in the little things getting better and better.  It also includes Git support (including remote syncing so Github integration is seamless).A bunch of big names just joined so hopefully things will continue getting even better.Update againAnd indeed things have only gotten better, in rapid fashion.  Package build-check cycles are now point-and-click, and the little stuff continues to improve as well. It now comes with an integrated debugging environment, too.","Display_name":"Ari B. Friedman","Creater_id":3488,"Start_date":"2011-03-01 07:19:56","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5424"},"Last_activity":"2013-06-05 01:50:32","Creator_reputation":1,"Question_score":2,"Answer_content":"I would recommend having a look at AirXcell. It's an online (Web 2.0) calculation software based on R which provides a quite usable R GUI with a command line interface (The R console) a code editor, and various other things (data frame editor, etc.), all online from within the web browser. See Use AirXcell as an online R console.","Display_name":"Jerome","Creater_id":26521,"Start_date":"2013-06-05 01:50:32","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5425"},"Last_activity":"2012-10-04 12:39:57","Creator_reputation":218,"Question_score":3,"Answer_content":"GUI  != ProgrammingAsking which GUI will help you learn programming is like asking which grocery store will help you learn how to hunt for your own food.  Using a GUI is not a way to learn programming.The power of R is that it's not GUI driven, it uses scripts which fundamentally allow for more more reproducible results.GUIs to demonstrate specific topics / Brief backpedalingThat having been said, I do think it's useful to use a GUI to demonstrate a single specific topic.The latticist package is awesome for creating lattice graphs and learning lattice syntax.  The PBSmodelling package has some wonderful examples of GUIs that allow you to run specific models, such as MCMV models.The TeachingDemos package is also seems to have some good demos of specific topics.Roll your own GUIThe PBSmodelling package also has tools that allow you to make your own GUIs.  This includes some amazing tools for project organization and documentation.  Thank you Pacific Biological Station!Also, by using Rook and Apache you can also make powerful web-based GUI applications.Making your own GUI is not appropriate for beginners or the direct answer to your question.  However, if you're an advanced user then you might want to consider making a GUI to demonstrate a particular topic.The installed \"R\" is a GUI (technically)It's worth noting that the installed version of R is a shortcut to Rgui.exe.  I know that you're asking for a GUI that let's you access all of the base functionality of R by pointing and clicking, not a glorified wrapper for the command line.However, it's important to realize that a GUI wrapper for the command line is a GUI, and it's a valid answer to your question.  The command line is the only way that you can get access to the rapidly evolving functionality of the power of R and the freshly packages authored daily.So...Again, the best GUI is R StudioThe best interface for R is definitely R Studio.  For some people the StatET / Eclipse interface is important for it's powerful features, but R Studio is rapidly overtaking those features and adding new ones.Revolution R (the commercial version) also has a GUI, but it's not so great unless you are deeply passionate about the design of MS Visual Studio.  However, you can access Revolution's build of R though R Studio or Eclipse, so that's a pretty neat trick too.","Display_name":"geneorama","Creater_id":10826,"Start_date":"2012-10-04 12:39:57","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5426"},"Last_activity":"2011-11-04 05:49:03","Creator_reputation":21,"Question_score":3,"Answer_content":"I used Rattle to both learn how to use R and for quick and dirty data mining tasks.","Display_name":"tom","Creater_id":5037,"Start_date":"2011-11-04 05:49:03","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5427"},"Last_activity":"2011-10-15 00:24:40","Creator_reputation":7937,"Question_score":24,"Answer_content":"This has been answered several times on StackOverflow.  The top selections on there seem to consistently be Eclipse with StatET or Emacs with ESS.I wouldn't say that there are any good gui's to make it easier to learn the language.  The closest thing would be deducer from Ian Fellows.  But there are plenty of other resources (books, papers, blogs, packages, etc.) available for learning.","Display_name":"Shane","Creater_id":5,"Start_date":"2010-12-09 07:32:36","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5428"},"Last_activity":"2011-10-15 00:24:31","Creator_reputation":null,"Question_score":37,"Answer_content":"I would second @Shane's recommendation for Deducer, and would also recommend the R Commander by John Fox.  The CRAN package is here.  It's called the R \"Commander\" because it returns the R commands associated with the point-and-click menu selections, which can be saved and run later from the command prompt.  In this way, if you don't know how to do something then you can find it in the menus and get an immediate response for the proper way to do something with R code. It looks like Deducer operates similarly, though I haven't played with Deducer for a while.The base R Commander is designed for beginner-minded tasks, but there are plugins available for some more sophisticated analyses (Deducer has plugins, too). Bear in mind, however, that no GUI can do everything, and at some point the user will need to wean him/herself from pointing-and-clicking.  Some people (myself included) think that is a good thing.","Display_name":"user1108","Creater_id":null,"Start_date":"2010-12-09 08:05:19","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5429"},"Last_activity":"2011-03-01 09:16:25","Creator_reputation":12260,"Question_score":2,"Answer_content":"Despite all of the good recommendations, I've not found anything radically better than the default Mac GUI. R-Studio shows promise, but it's not currently that much more customizable or featureful than R and, say, BBEdit to edit.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2011-03-01 09:16:25","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d542a"},"Last_activity":"2010-12-10 00:08:08","Creator_reputation":1186,"Question_score":2,"Answer_content":"I recommend Tinn-R (Which is the acronym for Tinn is not Notepad)","Display_name":"RockScience","Creater_id":1709,"Start_date":"2010-12-09 19:48:55","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d542b"},"Last_activity":"2010-12-09 14:31:56","Creator_reputation":163,"Question_score":4,"Answer_content":"Personally ESS, but as stated above i have found Rcmdr very easy to use. ","Display_name":"idclark","Creater_id":2262,"Start_date":"2010-12-09 14:31:56","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d542c"},"Last_activity":"2010-12-09 10:39:07","Creator_reputation":6409,"Question_score":2,"Answer_content":"I used JGR for a short while, until it became apparent it would quickly consume all the memory on my system. I have not used it since, and recommend you do not use it.","Display_name":"shabbychef","Creater_id":795,"Start_date":"2010-12-09 10:39:07","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d542d"},"Last_activity":"2010-12-09 10:11:31","Creator_reputation":17883,"Question_score":13,"Answer_content":"I think that the command line is the best interface, and especially for the beginners. The sooner you'll start with console, the sooner you'll find out that this is the fastest, the most comfortable and what's most important the only fully non-limiting way of using R.","Display_name":"mbq","Creater_id":88,"Start_date":"2010-12-09 10:11:31","Question_id":5292}
{"_id":{"$oid":"5837a57fa05283111e4d5439"},"Last_activity":"2016-08-13 01:43:40","Creator_reputation":111,"Question_score":1,"Answer_content":"[I am assuming this is an issue with one of your independent variables. If it's in your dependent variable, linear regression is not the way to go]You are right that assigning numeric variables is the wrong way to go. In linear regression with non-numeric (or categorical) independent variables, you want a coefficient for each category (except a default one). You need the variable to be a factor. You can either let R do this for you, by just adding the variable as-is to the model, or convert it to a factor yourself. That way, you can set which mode of operation is the default.","Display_name":"Heroka","Creater_id":86185,"Start_date":"2015-08-17 23:27:58","Question_id":167600}
{"_id":{"$oid":"5837a57fa05283111e4d543a"},"Last_activity":"2016-07-11 06:15:06","Creator_reputation":506,"Question_score":0,"Answer_content":"You can change the categorical variables to numeric based on the typeCategorical variables - Create column for each unique \u0026amp; create binary flag for themOrdinal variables (categories with logical order e.g., high low medium) can be converted to values 1,2,3 etcLanguage - You can use Word2Vec or Document Term matrixCities - You can create actual distance from respect to one citySo all of them will be numeric \u0026amp; you can use Linear regression Hope this helps.","Display_name":"Nishad ","Creater_id":115514,"Start_date":"2016-07-11 06:15:06","Question_id":167600}
{"_id":{"$oid":"5837a57fa05283111e4d5449"},"Last_activity":"2016-08-13 01:20:40","Creator_reputation":3619,"Question_score":0,"Answer_content":"For general information on meta-analysis (MA) you could look at the tag-info. I have not read the sources it quotes so cannot recommend one over another.The basic MA paradigm is that from each study (on drug A) you extract the statistic you wish to summarise  and its variance . You then form a weighted average using . In your case there is a complication that the proportions take the role of the  and they are supposed to be normally distrbuted which they clearly are not. The answer is to transform them first. At this point I strongly recommend choosing a software system to do all this. MA is certainly available in R (which I use), see the CRAN Task View for more details I use metafor but meta also does this, available in Stata (search for Stata metaprop) and in various stand-alone packages. It is possible to program it in SAS but I have no idea how. I have never seen anyone do it in SPSS but it may be possible.When you have done that you will have an estimate from each set of studies on drug A and on drug B so you can proceed to compute the probability of their simultaneous occurrence conditional on their being independent.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-13 01:20:40","Question_id":228947}
{"_id":{"$oid":"5837a57fa05283111e4d5456"},"Last_activity":"2016-08-13 00:27:52","Creator_reputation":298,"Question_score":0,"Answer_content":"\"I am not sure which one is right\"There is no right or wrong here. A classifier's performance can be represented using an  matrix. When trying to represent the performance using a single metric you lose some information.In other words, since it is impossible to recover the confusion matrix based on a single metric, there is a loss of information when we consider only a single metric to interpret the performance of a classifier.But still... to decide which classifier is better among several alternatives - we need a single metric...Which single metric best represents the performance? That's a subjective questions. This is where statisticians become creative. This is why so many metrics have been purposed.Different metrics 'prefer' different types of information that can be extracted from the confusion matrix. It is up to you to decide which one captures the information your regard as 'most important'.Some criteria you may consider:Are all classes are equally important / are all instances are equally important?Are classification and misclassifications are equally 'important'?Are false positives and false negatives are equally 'important'?Should the performance be absolute, or relative to some random classifier?Should the metric be linear in some sense?etc.","Display_name":"Lior Kogan","Creater_id":6559,"Start_date":"2016-08-13 00:17:59","Question_id":229630}
{"_id":{"$oid":"5837a57fa05283111e4d5464"},"Last_activity":"2016-08-12 20:37:07","Creator_reputation":152613,"Question_score":3,"Answer_content":"This would indicate a clear seasonal effect of period 7 (such as you might see with a \"day-of-week\" type effect and daily data)The fact that it extends for a long time suggests you might need seasonal differencing.If you look at the plots for seasonally differenced series you'll probably see indication of a small ARMA model -- perhaps an MA(1), say.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-12 20:37:07","Question_id":229577}
{"_id":{"$oid":"5837a57fa05283111e4d5471"},"Last_activity":"2016-08-12 22:55:45","Creator_reputation":133,"Question_score":2,"Answer_content":"I was a little bit confused at first when thinking about the matter. However, the two main reason why you won't be able to harmonize forecasts on hourly and daily basis is the following.The events for rain in breakdown perspective are not independent. Aggregating probabilities would require knowledge on how the events are related, but this will vary from case to case.On my blog you'll find the article Rain Risk: Too much detail? which deals with the question and provides some neat images.","Display_name":"Jan","Creater_id":69600,"Start_date":"2016-08-11 11:15:00","Question_id":209699}
{"_id":{"$oid":"5837a57fa05283111e4d547e"},"Last_activity":"2016-08-11 21:18:16","Creator_reputation":191,"Question_score":2,"Answer_content":"First, it is important to note that you are not talking about the complexity of the model, but the complexity of the relation between X and Y, which has nothing to do with modelling. When we try to figure out such relations, we use models, which typically involve some kind of assumptions, etc. in order to track down that relationship. but there is an inherent difference between the complexity of the underlying relationship and the complexity of a model.Given that, there is always a tradeoff between model complexity (in some intuitive sense) and model accuracy. when there is underlying complex relationship we try to model, the simpler the model is, the less accurate are its prediction likely to be. So the underlying relationship complexity is in some sense the \"sum\" of actual model complexity and its lack of fit.I think that the AIC measure is an attempt to capture such complexity (that takes into account both inaccuracy and the model's \"degrees of freedom\" so to speak. I use it often to compare models that fall under common \"strategies\" even if they do not contain the same variables (e.g. two logistic regressions with the same response, but with different set of predictors), but tend to find it less useful/reliable comparing models of different nature.Second, the example you bring about the circle, emphasize the importance of feature representation and two different sources of \"complexity\". after all, with a simple polar transformation, the circle with fixed radius become analogous to your first example. so what is worse? another explanatory variable or a transformation on an existing one? and is sin(x) is more complex that x^2 as transformations?This is to emphasize how \"measure of complexity\" is totally dependent on the context. you must have some kind of a \"loss function\" that will quantify/penalize you for any \"step\" you do in order to get from X to Y. and this function is unlikely to have universal features. it is context specific.I know this is not the kind of answer that you expected (sorry), But I do think there is no hope for the expectation you expressed in your question.HTH","Display_name":"amit","Creater_id":25986,"Start_date":"2016-08-11 21:18:16","Question_id":229378}
{"_id":{"$oid":"5837a57fa05283111e4d547f"},"Last_activity":"2016-08-11 09:37:20","Creator_reputation":21588,"Question_score":4,"Answer_content":"I believe this has been discussed in the literature. In regression context, the measures of model complexity utilize the linear regression relation of the rank of the projection matrix being equal to the number of (non-collinear) regressors. So Ye (1998) generalized this by perturbing the data , running your favorite machine learning model (these were called data mining back then) on these perturbed data, getting predictions , and measuring how much the predictions changed in response to perturbations. Repeating this many times, you collect a big data set with such predictions as outcomes, and the magnitude of perturbations as regressors, . Then the sum of regression coefficients  gives you the degrees of freedom.The HTF ESL book repeats this argument in Section 7.6, although without attribution to Ye. They further discuss MDL and VC dimension, which are probably relevant to your case, too.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-11 09:37:20","Question_id":229378}
{"_id":{"$oid":"5837a57fa05283111e4d548c"},"Last_activity":"2014-08-18 06:10:32","Creator_reputation":208,"Question_score":1,"Answer_content":"Here is one approach I came up with, but if there's a more elegant/accurate approach please let me know.Re-sample the 2D histogram to very small bins of wave height andperiod, and normalize so the sum of the resampled probability is 1. Smaller bins make the summation below more accurate.Loop through different probability values. For each probability value, plot the contour of thatprobability and identify enclosed bins of wave height and period(I am doing this in Matlab and used the approach in thisanswer). Sum the probability in the enclosed bins and store the value.After the loop is finished, interpolate to find the contour that would enclose 10%. If x=[summed probability from step 3] and y=[contoured probability value from step 2], interpolate on xi=[desired summed probability], here 10%, to find yi=[contour enclosing desired summed probability].Delete all prior tested contours from the plot, then plot the final contour of yi which encloses the height/period combinations that happen 10% of the time.","Display_name":"KAE","Creater_id":18578,"Start_date":"2014-08-15 12:39:37","Question_id":112015}
{"_id":{"$oid":"5837a57fa05283111e4d5499"},"Last_activity":"2016-08-12 16:39:54","Creator_reputation":8337,"Question_score":0,"Answer_content":"I would recommend instead the equivalent model that is in standard form for linear regression:Y = c + \\beta_1X_{t=1} + \\beta_2X_{t=0} + \\epsilon then represents the effect of a one-unit change in  controlling for , as desired.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-12 16:39:54","Question_id":229616}
{"_id":{"$oid":"5837a57fa05283111e4d54a5"},"Last_activity":"2016-08-12 16:19:25","Creator_reputation":19594,"Question_score":1,"Answer_content":"It depends on your task. But usually you want all three to be high.high support: should apply to a large amount of caseshigh confidence: should be correct oftenhigh lift: indicates it is not just a coincidenceConsider e.g. \"rain\" and \"day\". Assuming we live in a very unfortunate place at the Equator, where it is raining 50% of the time, and it is day 50% of the time, and these are independent of each other. I.e. in 25% of the time it is raining and it is day.We then have a support of 25% - that is pretty high for most data sets. We also have a confidence of 50% - that is also pretty good. If 50% of my visitors buy a product I recommend I would be a billionaire. But the lift is just 1, i.e. no improvement.Beware that on other data sets, you won't get anywhere near 25% support. Consider a supermarket with diverse prodcuts. How many % of customers do you think buy toilet paper?","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-08-12 16:19:25","Question_id":229523}
{"_id":{"$oid":"5837a57fa05283111e4d54b1"},"Last_activity":"2016-08-12 16:11:04","Creator_reputation":143,"Question_score":-1,"Answer_content":"Exploratory solution would be to try a couple of sub-sets of your data, (3000, 30K, 300K, 3m, 30m) and see what happens to your confidence intervals as your data set gets larger. It may be that nothing happens, or it may be that your confidence interval gets larger in a predictable way. ","Display_name":"Mox","Creater_id":64122,"Start_date":"2016-08-12 16:11:04","Question_id":229608}
{"_id":{"$oid":"5837a57fa05283111e4d54be"},"Last_activity":"2016-08-12 15:53:32","Creator_reputation":7975,"Question_score":1,"Answer_content":"Fuzzy logic can be considered a concept under the purview of of AI. Keep in mind that AI is a quite subjective term. E.g., see AI effect.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-12 15:53:32","Question_id":229607}
{"_id":{"$oid":"5837a57fa05283111e4d54cb"},"Last_activity":"2016-08-12 15:52:35","Creator_reputation":6813,"Question_score":3,"Answer_content":"Just some speculations. If we plot the coefficients of binned high temperature versus those of binned mean temperature we can see a curve linear relationship:My guesses are:i) While the correlation is high, the very high number of sample size might have masked some subtle non-linear relationship. In some climates, it's possible that there is a wider variability in summer compared to winter. In that sense, it's not like all cases are just moved to the next bin, there could have been some reshuffling, causing the difference.ii) Following up with the point above, in the graph we can also see the associations with the outcome seems to be diminishing compared to that of the mean temperature. A possibility is that at a very high temperature (e.g. when a heat wave strikes,) people might change behaviors that mitigated their risk. For instance they may have stayed indoor and turned on the AC, so that they were less likely to get heat stroke or dehydration, etc.And I think that the 5-degree binning approach seems a bit wasteful. With so many data points it may be worth to examine the association between temperature and the log odds of the events (can be done in single degree), and evaluate if binning really needs to be done, or can temperature be included in some other functional forms.","Display_name":"Penguin_Knight","Creater_id":13047,"Start_date":"2016-08-12 15:47:18","Question_id":229598}
{"_id":{"$oid":"5837a57fa05283111e4d54d8"},"Last_activity":"2016-08-12 14:58:57","Creator_reputation":21,"Question_score":1,"Answer_content":"Typically learning with neural networks uses the back-propagation algorithm to minimize SSE of the errors. This will generally find a local minimum (https://en.wikipedia.org/wiki/Local_optimum) of SSE and is not guaranteed to find the global minimum. Depending on the initialization of parameters the algorithm may converge to a different local minimum.In your example, when training the full model it seems that the algorithm has got stuck on a local minimum, as there is clearly another model (using a subset of available data) that has smaller SSE.","Display_name":"badim","Creater_id":127289,"Start_date":"2016-08-12 14:58:57","Question_id":229592}
{"_id":{"$oid":"5837a57fa05283111e4d54e7"},"Last_activity":"2016-08-12 15:16:50","Creator_reputation":356,"Question_score":0,"Answer_content":"After thinking about it, I realized ordinal regression is indeed appropriate for this situation.While the response variable is a continuous real, it isn't an infinitely precise continuous real. In all likelihood, it's coming from some physical measure that has an associated uncertainty. Even if the measurement were to be infinitely precise, the formulation of the regression problem implies that there is some measurement inaccuracy in the response variable, making the precision a false one. Finally, if you're only concerned about rank order, any change in precision is \"pointless\" as long as it doesn't change the effective order of the entries.For example, imagine you're modeling the weight of widgets in a manufacturing process. Theoretically, there weight is a continuous real, but the scale on which you measure them has a fixed precision (e.g. milligrams). There's also a practical level of intrinsic variability (e.g. two widgets within a tenth of a gram of each other might be considered \"the same weight\" with respect to the downstream process.). Finally, there's the limitations of the input data. If one widget in the training set is 5.57 g and then next heavier is 5.89 g, no rank-order based method will show a practical distinction between a prediction of 5.65 g versus 5.75 g. In this light, it should be possible to choose some \"precision\" under which changes in the response variable do not substantially affect the point of the regression problem. You can then \"round to the nearest\" to get discreet response levels you can then subject to ordinal regression.The one consideration may be the number of output levels with respect to the number of input variables. Apparently, good ordinal regression packages are able to handle situations where the number of output levels is on the same order or even equal to the number of input cases. (That is, if each input case has it's own unique output level.) As I understand it, the total ordering of the output levels is a rather weak requirement, in terms of information costs, so you don't really risk over fitting with a large number of response levels.With discretized output levels - especially with one level per input case - ordinal regression becomes practically equivalent to generalized linear regression with an arbitrary monotonic link function.","Display_name":"R.M.","Creater_id":69382,"Start_date":"2016-08-12 15:16:50","Question_id":226955}
{"_id":{"$oid":"5837a57fa05283111e4d54f4"},"Last_activity":"2014-03-18 07:52:11","Creator_reputation":441,"Question_score":4,"Answer_content":"After exploring this exercise you can try the easier ways in R.  There are two popular functions for doing PCA: princomp and prcomp.  The princomp function does the eigenvalue decomposition as done in your exercise.  The prcomp function uses singular value decomposition.  Both methods will give the same results almost all of the time: this answer explains the differences in R, whereas this answer explains the math. (Thanks to TooTone for comments now integrated into this post.)Here we use both to reproduce the exercise in R.  First using princomp: d = data.frame(x=c(2.5,0.5,2.2,1.9,3.1,2.3,2.0,1.0,1.5,1.1),                y=c(2.4,0.7,2.9,2.2,3.0,2.7,1.6,1.1,1.6,0.9))# compute PCsp = princomp(d,center=TRUE,retx=TRUE)# use loadings and scores to reproduce with only first PCloadings = t(pscores[,1] reproduce = scores %*% loadings  + colMeans(d)# plotsplot(reproduce,pch=3,ylim=c(-1,4),xlim=c(-1,4))abline(h=0,v=0,lty=3)mtext(\"Original data restored using only a single eigenvector\",side=3,cex=0.7)biplot(p)Second using prcomp:d = data.frame(x=c(2.5,0.5,2.2,1.9,3.1,2.3,2.0,1.0,1.5,1.1),                y=c(2.4,0.7,2.9,2.2,3.0,2.7,1.6,1.1,1.6,0.9))# compute PCsp = prcomp(d,center=TRUE,retx=TRUE)# use loadings and scores to reproduce with only first PCloadings = t(px[,1]reproduce = scores %*% loadings  + colMeans(d)# plotsplot(reproduce,pch=3,ylim=c(-1,4),xlim=c(-1,4))abline(h=0,v=0,lty=3)mtext(\"Original data restored using only a single eigenvector\",side=3,cex=0.7)biplot(p)Clearly the signs are flipped but the explanation of variation is equivalent. ","Display_name":"mrbcuda","Creater_id":41867,"Start_date":"2014-03-17 18:34:04","Question_id":90331}
{"_id":{"$oid":"5837a57fa05283111e4d54f5"},"Last_activity":"2014-03-17 18:03:11","Creator_reputation":386,"Question_score":4,"Answer_content":"I think you have the right idea but stumbled over a nasty feature of R. Here again the relevant code piece as you've stated it:trans_data = final_datatrans_data[,2] = 0row_orig_data = t(t(feat_vec[1,]) %*% t(trans_data))plot(row_orig_data, asp=T, pch=16)Essentially final_data contains the coordinates of the original points with respect to the coordinate system defined by the eigenvectors of the covariance matrix. To reconstruct the original points one therefore has to multiply each eigenvector with the associated transformed coordinate, e.g.(1) final_data[1,1]*t(feat_vec[1,] + final_data[1,2]*t(feat_vec[2,])which would yield the original coordinates of the first point. In your question you set the second component correctly to zero, trans_data[,2] = 0. If you then (as you already edited) calculate(2) row_orig_data = t(t(feat_vec) %*% t(trans_data))you calculate formula (1) for all points simultaneously. Your first approach row_orig_data = t(t(feat_vec[1,]) %*% t(trans_data))calculates something different and only works because R automatically drops the dimension attribute for feat_vec[1,], so it is not a row vector anymore but treated as a column vector. The subsequent transpose makes it a row vector again and that's the reason why at least the calculation does not produce an error, but if you go through the math you will see that it is something different than (1). In general it is a good idea in matrix multiplications to suppress dropping of the dimension attribute which can be achieved by the drop parameter, e.g. feat_vec[1,,drop=FALSE].Your edited solution seems correct, but you calculated the slope if PCA1 wrongly. The slope is given by  , hence s1 = evectors[1,1] # PC1s2 = evectors[1,2] # PC2","Display_name":"Georg","Creater_id":40608,"Start_date":"2014-03-17 17:22:29","Question_id":90331}
{"_id":{"$oid":"5837a57fa05283111e4d54f6"},"Last_activity":"2014-03-17 17:31:25","Creator_reputation":2664,"Question_score":10,"Answer_content":"You were very very nearly there and got caught by a subtle issue in working with matrices in R. I worked through from your final_data and got the correct results independently. Then I had a closer look at your code. To cut a long story short, where you wroterow_orig_data = t(t(feat_vec[1,]) %*% t(trans_data))you would have been ok if you had writtenrow_orig_data = t(t(feat_vec) %*% t(trans_data))instead (because you'd zeroed out the part of trans_data that was projected on the second eigenvector). As it was you were trying to multiply a  matrix by a  matrix but R did not give you an error. The problem is that t(feat_vec[1,]) is treated as . Trying row_orig_data = t(as.matrix(feat_vec[1,],ncol=1,nrow=2) %*% t(trans_data)) would have given you a non-conformable arguments error. The following, possibly more along the lines of what you intended, would also have workedrow_orig_data = t(as.matrix(feat_vec[1,],ncol=1,nrow=2) %*% t(trans_data)[1,])as it multiplies a  matrix by a  matrix (note that you could have used the original final_data matrix here). It isn't necessary to do it this way, but it's nicer mathematically because it shows that you are getting  values in row_orig_data from  values on the right hand side.I've left my original answer below, as someone might find it useful, and it does demonstrate getting the required plots. It also shows that the code can be a bit simpler by getting rid of some unnecessary transposes:  so t(t(p) %*% t(q)) = q %*% t.Re your edit, I've added the principal component line in green to my plot below. In your question you got had the slope as  not .Writed_in_new_basis = as.matrix(final_data)then to get your data back in its original basis you needd_in_original_basis = d_in_new_basis %*% feat_vecYou can zero out the parts of your data that are projected along the second component usingd_in_new_basis_approx = d_in_new_basisd_in_new_basis_approx[,2] = 0and you can then transform as befored_in_original_basis_approx = d_in_new_basis_approx %*% feat_vecPlotting these on the same plot, together with the principal component line in green, shows you how the approximation worked.plot(x=d_in_original_basis[,1]+mean(dy),     pch=16, xlab=\"x\", ylab=\"y\", xlim=c(0,3.5),ylim=c(0,3.5),     main=\"black=original data\\nred=original data restored using only a single eigenvector\")points(x=d_in_original_basis_approx[,1]+mean(dy),       pch=16,col=\"red\")points(x=c(mean(dvectors[1,1]*10,mean(dvectors[1,1]*10), c(y=mean(dvectors[2,1]*10,mean(dvectors[2,1]*10), type=\"l\",col=\"green\")Let's rewind to what you had. This line was okfinal_data = data.frame(t(feat_vec %*% row_data_adj))The crucial bit here is feat_vec %*% row_data_adj which is equivalent to  where  is the matrix of eigenvectors and  is your data matrix with your data in rows, and  is the data in the new basis. What this is saying is that the first row of  is the sum of (rows of  weighted by the first eigenvector). And the second row of  is the sum of (rows of  weighted by the second eigenvector).Then you hadtrans_data = final_datatrans_data[,2] = 0This is ok: you're just zeroing out the parts of your data that are projected along the second component. Where it goes wrong isrow_orig_data = t(t(feat_vec[1,]) %*% t(trans_data))Writing  for the matrix of data  in the new basis, with zeros in the second row, and writing  for the first eigenvector, the business end of this code t(feat_vec[1,]) %*% t(trans_data) comes down to .As explained above (this is where I realised the subtle R problem and wrote the first part of my answer), mathematically you are trying to multiply a  vector by a  matrix. This doesn't work mathematically. What you should do is take the first row of  = the first row of : call this . Then multiply  and  together. The th column of the result  is the eigenvector  weighted by the 1st coordinate only of the th point in the new basis, which is what you want.","Display_name":"TooTone","Creater_id":25936,"Start_date":"2014-03-17 17:14:48","Question_id":90331}
{"_id":{"$oid":"5837a57fa05283111e4d5503"},"Last_activity":"2014-12-10 08:50:14","Creator_reputation":1568,"Question_score":3,"Answer_content":"This is very similar to this previous questionFollowing your analysis, I use the same pca object.  Looking at summary(pca) I can see that at 20 components, 90%  of the variation is explained.  So for demonstration purposes, that sounds like a good number to work with.# reconstruct matrixrestr \u0026lt;- pcarotation[,1:20])# unscale and uncenter the dataif(pcascale)    }    if(all(pcacenter, scale=FALSE)}# plot your original image and reconstructed imagepar(mfcol=c(1,2), mar=c(1,1,2,1))im \u0026lt;- matrix(data=rev(im.train[2,]), nrow=96, ncol=96)image(1:96, 1:96, im, col=gray((0:255)/255))rst \u0026lt;- matrix(data=rev(restr[2,]), nrow=96, ncol=96)image(1:96, 1:96, rst, col=gray((0:255)/255))","Display_name":"cdeterman","Creater_id":37428,"Start_date":"2014-12-10 08:50:14","Question_id":127502}
{"_id":{"$oid":"5837a57fa05283111e4d5510"},"Last_activity":"2016-08-12 15:04:47","Creator_reputation":21,"Question_score":1,"Answer_content":"Had the exact same question. According to  this post and Wilcox (p166) you shouldn't calculate Z and Cohen's D respectively because their calculation is based on the normality assumption. Wilcox provides robust alternatives to Cohen's D (p166-170) including R code in his book. Got the book from my library. It is also available as e-book in both library networks I have access to. http://store.elsevier.com/Introduction-to-Robust-Estimation-and-Hypothesis-Testing/Rand-Wilcox/isbn-9780123870155/ There are a couple of other posts in this forum asking very similar questions (effect size for Wilcoxon, one sample Wilxocon or Mann-Whitney-U tests just to name a few). You might like to look at those too.","Display_name":"Simone","Creater_id":115835,"Start_date":"2016-08-11 01:16:02","Question_id":208139}
{"_id":{"$oid":"5837a580a05283111e4d551d"},"Last_activity":"2016-08-12 14:57:48","Creator_reputation":19594,"Question_score":0,"Answer_content":"60 instances is much to little.You probably need 60 instances to get a meaningful location estimate of a single cluster. At 10 clusters, the average cluster size is just 6. Many clusters will likely be based on a single data point!You may even have duplicates? Then at this \"flipping\" point, every point is exactly encoded in your model. Afterwards, you even get a lot of redundancy, which is probably why the quality decreases again - it is becoming ambiguous, with no more fresh data available at all.You are badly overfitting your data.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-08-12 14:57:48","Question_id":228490}
{"_id":{"$oid":"5837a580a05283111e4d552a"},"Last_activity":"2016-08-12 14:39:52","Creator_reputation":11,"Question_score":0,"Answer_content":"Below is a dplyr option using pipes. It does seems strange that this is so hard to access.library(dplyr)col_index \u0026lt;- varImp(modelFit)names[1:3]You can also subset the original data frame like this:Sonar_imp \u0026lt;- Sonar[,imp_names]","Display_name":"Scott Worland","Creater_id":114321,"Start_date":"2016-08-12 14:39:52","Question_id":113618}
{"_id":{"$oid":"5837a580a05283111e4d552b"},"Last_activity":"2016-06-11 17:49:57","Creator_reputation":11,"Question_score":0,"Answer_content":"This should do the trick:  rownames(varImp(modelFit)$importance)[1:3]","Display_name":"Chipmunk","Creater_id":119686,"Start_date":"2016-06-11 17:44:35","Question_id":113618}
{"_id":{"$oid":"5837a580a05283111e4d552c"},"Last_activity":"2014-08-28 13:05:44","Creator_reputation":1098,"Question_score":9,"Answer_content":"What is the issue with #1? It runs fine for me and the result of the call to varImp() produces the following, ordered most to least important:\u0026gt; varImp(modelFit)rpart variable importance   OverallV5 100.000V4  38.390V3  38.362V2   5.581V1   0.000EDIT Based on Question clarification:I am sure there are better ways, but here is how I might do it:ImpMeasure\u0026lt;-data.frame(varImp(modelFit)Vars\u0026lt;-row.names(ImpMeasure)ImpMeasure[order(-ImpMeasure$Overall),][1:3,]Regarding #2, you need to add importance=TRUE in order to tell randomForest to calculate them.\u0026gt; modelFit \u0026lt;- train( V6~.,data=training, method=\"rf\" ,importance = TRUE)\u0026gt; varImp(modelFit)rf variable importance   OverallV5 100.000V3  22.746V2  21.136V4   3.797V1   0.000","Display_name":"B_Miner","Creater_id":2040,"Start_date":"2014-08-28 12:26:05","Question_id":113618}
{"_id":{"$oid":"5837a580a05283111e4d553b"},"Last_activity":"2016-08-12 13:45:54","Creator_reputation":17864,"Question_score":3,"Answer_content":"The x-axis is the size of the coefficient relative to the  norm of the coefficient vector.... probably. I'm assuming that you created that plot with the default plot commands in lars and this is what the documentation describes.The bars in figures 2 and 3 represent an estimate of the sampling error around the mean estimate of MSE. I don't know how you generated them, so I don't know how wide the interval is. Default values are described in the documentation. The red points in figure 3 are the mean estimates of the MSE. The colored vertical lines are the locations of the minimum MSE and minimum MSE plus 1 standard error (by default).","Display_name":"Sycorax","Creater_id":22311,"Start_date":"2016-08-12 12:50:28","Question_id":229593}
{"_id":{"$oid":"5837a580a05283111e4d554a"},"Last_activity":"2016-08-12 13:00:26","Creator_reputation":2542,"Question_score":1,"Answer_content":"There's no reason at all to believe any feature importance would coincide among algorithms so apart. Random Forest importance is based on expected decrease in performance when said predictor is used in a tree. GLM importance is based on the scale of coefficients.","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-08-12 13:00:26","Question_id":203565}
{"_id":{"$oid":"5837a580a05283111e4d5559"},"Last_activity":"2016-08-12 12:23:04","Creator_reputation":126,"Question_score":1,"Answer_content":"I think you have 1200*2 in-between layers. So, total connection will be (1200^2)*2 (what you wrote is correct.) This is correct, if you are not considering any biased weights. If you add biased weights to each neuron then it would be (1200^2)*2+1200*2 number of connections.In RNN, each node will be connected twice with itself (Node1 --\u003e Node1 --\u003e Node1). So, (1200^2)*2+(1200^2)*2 (without considering any biased weights). You may generalize it.","Display_name":"Coder","Creater_id":127347,"Start_date":"2016-08-12 12:23:04","Question_id":229591}
{"_id":{"$oid":"5837a580a05283111e4d5566"},"Last_activity":"2016-08-12 11:53:47","Creator_reputation":3398,"Question_score":0,"Answer_content":"You want a nonlinear fit model. Check out: http://www.mathworks.com/help/stats/nonlinearmodel.fit.htmlandhttp://www.mathworks.com/help/stats/fitnlm.html","Display_name":"Alex R.","Creater_id":61092,"Start_date":"2016-08-12 10:41:31","Question_id":229572}
{"_id":{"$oid":"5837a580a05283111e4d5573"},"Last_activity":"2016-08-12 11:50:03","Creator_reputation":17414,"Question_score":1,"Answer_content":"It depends on the question. Interrater agreement has been studied extensively, so there is not one single way to do this. I'll pose a question to you: if rater 1 predicts Swimmer A will take 6th place and Swimmer B will take 1st place whereas rater 2 predicts Swimmer A will take 1st place and swimmer B will take 6th place, would you or should you consider that a worse ranking than a similar setting where the disagreement is over 1st versus 2nd place? I can say with some confidence it would be erI would think that a weighted kappa is the best approach to evaluating rater agreement for ranks. The choice of weight is determined by the problem. A higher weight considers adjacent ranks to be \"ball park\" close so that the second scenario of my example is considered an \"okay\" agreement but the first scenario is considered \"awful\". Unweighted kappas classify any ranking disagreement as a complete miss, regardless of how close it was. Confidence intervals and hypothesis tests are available for kappas using standard software. However, it would require that you actually produce an experiment for this question, and simulate examples from many types of races/events.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-12 11:50:03","Question_id":229459}
{"_id":{"$oid":"5837a580a05283111e4d5580"},"Last_activity":"2016-08-12 11:49:35","Creator_reputation":7559,"Question_score":7,"Answer_content":"Directly examining the cost function can be useful, but be aware of some basic issues:Units: Eg. if you measured house price in a less valuable currency (eg. Yen) all the numbers would be higher. What you regard as \"high\" must be relative to the units used.Number of observations: you want to normalize by the number of observations so that more data doesn't mechanically give you higher cost!Some basic measures of overall error:root mean square errormean absolute deviationRoot mean square error is a monotonic transformation of the sum of squares, so minimizing the sum of squares is the same thing as minimizing root mean square error (and minimizing mean absolute deviation is the same as minimizing the sum of absolute error). is 1 minus the sum of squared error divided by the total sum of squares. For a linear regression with a constant, this essentially gives you the proportion of the variance explained by the model.What does a high root mean square error, high mean absolute deviation mean, or low  imply?In some sense it means that you have a lot of forecast error. What's reasonable to expect in terms of forecast error is entirely problem dependent. In physics with good data and precisely modeled problems, you may have almost no error.  In economics (eg. forecasting home prices etc...) you tend have a LOT of error. In fact, if you have implausibly good forecasts, too little error, it probably means you've overfit the data!Beware of overfitting...In general, a huge problem in empirical research, machine learning etc.. is overfitting. If you give yourself enough parameters to estimate, you can end up with a model that fits the training data too well... it fits your sample, but if you try it on new data, the model may perform horribly. If there's overfitting, your algorithm is picking up random, meaningless peculiarities of your particular data set.Note there's a big conceptual difference between error on: (1) the data used to estimate your model and (2) new data.General note on solving least squaresThe solution to minimizing a sum of squares can be expressed as a solution to a linear system of equations. (See derivation here: Understanding linear algebra in Ordinary Least Squares derivation.) Systems of linear equations can be efficiently solved and you can check the accuracy of your gradient descent algorithm by simply comparing it to the solution you get by solving the linear system.Eg.b_gradient_descent = my_gradient_descent(y, X);b_linear_system = linsolve(X'*X, X'*y);or in Matlab, the b_linear_system = X \\ y;","Display_name":"Matthew Gunn","Creater_id":97925,"Start_date":"2016-08-09 20:37:51","Question_id":229102}
{"_id":{"$oid":"5837a580a05283111e4d558d"},"Last_activity":"2016-08-12 11:43:40","Creator_reputation":7559,"Question_score":1,"Answer_content":"When might you want the mean?Examples from finance:Bond returns:The median bond return will generally be a few percentage points.The mean bond return might be low or high depending on the default rate and recovery in default. The median will ignore all this!Good luck explaining to your investors, \"I know our fund is down 40% this year because almost half are bonds went bust with no recovery, but our median bond returned 1%!\"Venture capital returns:Same thing in reverse. The median VC or angel investment is a bust, and all the return comes from a few winners! (Side note/warning: estimates of venture capital or private equity returns are highly problematic... be careful!)When forming a diversified portfolio, deciding what to invest in and how much, the mean and covariance of returns are likely to factor prominently into your optimization problem. ","Display_name":"Matthew Gunn","Creater_id":97925,"Start_date":"2016-08-12 10:44:53","Question_id":14210}
{"_id":{"$oid":"5837a580a05283111e4d558e"},"Last_activity":"2011-08-16 02:17:01","Creator_reputation":26229,"Question_score":23,"Answer_content":"When a value is garbage for us we call it \"outliar\" and want analysis be robust to it (and prefer median); when that same value is attractive we call it \"extreme\" and want analysis be sensitive to it (and prefer mean). Dialectics...Mean reacts equally to a shift of value irrespective to where in the distribution the shift takes place. For example, in 1 2 3 4 5 you may increase any value by 2 - the increase of mean will be the same. Median's reaction is less \"consistent\": add 2 to data points 4 or 5, and median won't increase; but add 2 to point 2 - so that the shift is over the median, and the median changes dramatically (greatly than mean will change).Mean is always exactly located. Median is not; for example, in set 1 2 3 4 any value between 2 and 3 can be called median. Thus, analyses based on medians are not always unique solution.Mean is a locus of minimal sum-of-squared-deviations. Many optimization tasks based on linear algebra (including famous OLS regression) minimize this squared error and therefore imply concept of mean. Median a locus of minimal sum-of-absolute-deviations. Optimization techniques to minimize such error are non-linear and are more complex / poorly known.","Display_name":"ttnphns","Creater_id":3277,"Start_date":"2011-08-13 01:25:49","Question_id":14210}
{"_id":{"$oid":"5837a580a05283111e4d558f"},"Last_activity":"2011-08-15 10:49:13","Creator_reputation":14825,"Question_score":4,"Answer_content":"\"It is a known that median is resistant to outliers. If that is the case, when and why would we use the mean in the first place?\"In cases one knows there are no outliers, for example when one knows the data-generating process (for example in mathematical statistics). One should point out the trivial, that, these two quantities (mean and median) are actually not measuring the same thing and that most users ask for the former when what they really ought to be interested in the latter (this point is well illustrated by the median-based Wilcoxon tests which are more readily interpreted than the t-tests).Then, there are the cases where for some happenstance reason or another, some regulation imposes the use of he mean.","Display_name":"user603","Creater_id":603,"Start_date":"2011-08-15 09:59:01","Question_id":14210}
{"_id":{"$oid":"5837a580a05283111e4d5590"},"Last_activity":"2011-08-15 07:24:16","Creator_reputation":16279,"Question_score":16,"Answer_content":"There are a lot of answers to this question.  Here's one that you probably won't see elsewhere so I'm including it here because I believe it's pertinent to the topic.  People often believe that because the median is considered a robust measure with respect to outliers that it's also robust to most everything.  In fact, it's also considered robust to bias in skewed distributions.  These two robust properties of the median are often taught together.  One might note that underlying skewed distributions also tend to generate small samples that look like they have outliers and conventional wisdom is that one use medians in such situations.#function to generate random values from a skewed distributionrexg \u0026lt;- function (n, m, sig, tau) {    rexp(n, rate = 1/tau) + rnorm(n, mean = m, sd = sig)    }(just a demonstration that this is skewed and the basic shape)hist(rexg(1e4, 0, 1, 1))Now, let's see what happens if we sample from this distribution various sample sizes and calculate median and mean to see what the differences between them are.#generate values with various n'sN \u0026lt;- 1e4ns \u0026lt;- 2:30y \u0026lt;- sapply(ns, function(x) mean(apply(matrix(rexg(x*N, 0, 1, 1), ncol = N), 2, median)))plot(ns,y, type = 'l', ylim = c(0.85, 1.03), col = 'red') y \u0026lt;- sapply(ns, function(x) mean(colMeans(matrix(rexg(x*N, 0, 1, 1), ncol = N))))lines(ns,y)As can be seen from the above plot the median (in red) is much more sensitive to the n than the mean.  This is contrary to some conventional wisdom regarding using medians with low ns, especially if the distribution might be skewed.  And, it reinforces the point that the mean is a known value while the median is sensitive to other properties, one if which being the n. This analysis is similar to Miller, J. (1988). A warning about median reaction time. Journal of Experimental Psychology: Human Perception and Performance, 14(3):539–543.REVISIONUpon thinking about the skew issue I considered that the impact on the median might just be because in small samples you have a greater probability that the median is in the tail of the distribution, whereas the mean will almost always be weighted by values closer to the mode.  Therefore, perhaps if one was just sampling with a probability of outliers then maybe the same results would occur.So I thought about situations where outliers may occur and experimenters may attempt to eliminate them.If outliers happened consistently, such as one in every single sampling of data, then medians are robust against the effect of this outlier and the conventional story about the use of medians holds.But that's not usually how things go.  One might find an outlier in very few cells of an experiment and decide to use median instead of mean in this case.  Again, the median is more robust but it's actual impact is relatively small because there are very few outliers.  This would definitely be a more common case then the one above but the effect of using a median would probably be so small that it wouldn't matter much.Perhaps more commonly outliers might be a random component of the data.  For example, the true mean and standard deviation of the population may be about 0 but there's a percentage of the time we sample from an outlier population where the mean is 3.  Consider the following simulation, where just such a population is sampled varying the sample size.#generate n samples N times with an outp probability of an outlier.rout \u0026lt;- function (n, N, outp) {    outPos \u0026lt;- sample(0:1,n*N, replace = TRUE, prob = c(1-outp,outp))    numOutliers \u0026lt;- sum(outPos)    y \u0026lt;- matrix( rnorm(N*n), ncol = N )    y[which(outPos==1)] \u0026lt;- rnorm(numOutliers, 4)    return(y)    }outp \u0026lt;- 0.1N \u0026lt;- 1e4ns \u0026lt;- 3:30yMed \u0026lt;- sapply(ns, function(x) mean(apply(rout(x,N,outp), 2, median)))var(yMed)yM \u0026lt;- sapply(ns, function(x) mean(colMeans(rout(x,N,outp))))var(yM)plot(ns,yMed, type = 'l', ylim = range(c(yMed,yM)), ylab = 'Y', xlab = 'n', col = 'red') lines(ns,yM)The median is in red and mean in black. This is a similar finding to that of a skewed distribution.  In a relatively practical example of the use of medians to avoid the effects of outliers one can come up with situations where the estimate is affected by n much more when the median is used than when the mean is used.","Display_name":"John","Creater_id":601,"Start_date":"2011-08-13 02:00:34","Question_id":14210}
{"_id":{"$oid":"5837a580a05283111e4d5591"},"Last_activity":"2011-08-14 08:24:46","Creator_reputation":119,"Question_score":2,"Answer_content":"If the concern is over the presence of outliers, there are some straight-forward ways to check your data.Outliers, almost by definition, come into our data when something changes either in the process generating the data or in the process collecting the data. i.e. the data ceases to be homogeneous. If your data is not homogeneous then neither the mean nor the median make much sense, since you are trying to estimate the central tendency of two separate data sets that have been mixed together.The best method to ensure homogeneity is to examine the data-generating and -collection processes to ensure that all of your data is coming from a single set of processes. Nothing beats a little brain-power, here.As a secondary check, you can turn to one of several statistical tests: chi-squared, Dixon's Q-test, Grubb's test or the control chart / process behavior chart (typically X-bar R or XmR). My experience is that, when your data can be ordered as it was collected, the process behavior charts are better at detecting outliers than the outlier tests. This use for the charts may be somewhat controversial, but I believe it is entirely consistent with Shewhart's original intent and it is a use that is explicitly advocated by Donald Wheeler. Whether you use the outliers tests or the process behavior charts, remember that a detected \"outlier\" is merely signalling potential non-homogeneity that needs to be further examined. It rarely makes sense to throw out data points if you don't have some explanation for why they were outliers.If you are using R, the outliers package provides the outliers tests, and for process behavior charts there is the qcc, IQCC and qAnalyst. I have a personal preference for the usage and output of the qcc package.","Display_name":"Tom","Creater_id":145,"Start_date":"2011-08-14 08:24:46","Question_id":14210}
{"_id":{"$oid":"5837a580a05283111e4d5592"},"Last_activity":"2011-08-14 05:56:16","Creator_reputation":57712,"Question_score":28,"Answer_content":"Lots of great answers already, but, taking a step back and getting a little more basic, I'd say it's because the answer you get depends on the question you ask. The mean and median answer different questions - sometimes one is appropriate, sometimes the other.It's simple to say that the median should be used when there are outliers, or for skewed distributions, or whatever. But that's not always the case. Take income - nearly always reported with median, and usually that's right. But if you are looking at the spending power of a whole community, it may not be right. And in some cases, even the mode might be best (esp. if the data are grouped).","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2011-08-14 05:56:16","Question_id":14210}
{"_id":{"$oid":"5837a580a05283111e4d5593"},"Last_activity":"2011-08-13 09:46:31","Creator_reputation":39311,"Question_score":87,"Answer_content":"In a sense, the mean is used because it is sensitive to the data.  If the distribution happens to be symmetric and the tails are about like the normal distribution, the mean is a very efficient summary of central tendency.  The median, while being robust and well-defined for any continuous distribution, is only  as efficient as the mean if the data happened to come from a normal distribution.  It is this relative inefficiency of the median that keeps us from using it even more than we do.  The relative inefficiency translates into a minor absolute inefficiency as the sample size gets large, so for large  we can be more guilt-free about using the median.It is interesting to note that for a measure of variation (spread, dispersion), there is a very robust estimator that is 0.98 as efficient as the standard deviation, namely Gini's mean difference.  This is the mean absolute difference between any two observations. [You have to multiply the sample standard deviation by a constant to estimate the same quantity estimated by Gini's mean difference.]   An efficient measure of central tendency is the Hodges-Lehmann estimator, i.e., the median of all pairwise means.  We would use it more if its interpretation were simpler.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2011-08-13 09:46:31","Question_id":14210}
{"_id":{"$oid":"5837a580a05283111e4d5594"},"Last_activity":"2011-08-13 04:31:51","Creator_reputation":860,"Question_score":11,"Answer_content":"From the mean it's easy to calculate the sum over all items, e.g. if you know the average income of the population and the size of the population, you can immediately calculate the total income of the entire population.The mean is straightforward to calculate in O(n) time complexity. Calculating the median in linear time is possible but requires more thought. The obvious solution requiring sorting has worse (O(n log n)) time complexity.And I speculate that there is another reason for the mean being more popular than the median:The mean is taught to more persons at school and it's probably taught before teaching the median","Display_name":"Andre Holzner","Creater_id":961,"Start_date":"2011-08-13 04:31:51","Question_id":14210}
{"_id":{"$oid":"5837a580a05283111e4d55a1"},"Last_activity":"2010-11-19 16:09:37","Creator_reputation":14468,"Question_score":3,"Answer_content":"Whether you have a reasonable chance of obtaining (i.e. power to obtain) reliable conclusions depends on how big the effects are you wish to be able to detect. With such small numbers they'll have to be very large. Clearly having fewer treatments and more replications per treatment will give you at least a bit more power, or equivalently the aiblity to detect somewhat smaller effects with the same power.To put some rough numbers on that, let's ignore the soil types for simplicity (including them will make things more gloomy) and do some standard power calculations two-sample for 2-sample t-tests. If you compare one treatment vs control with 10 in each group (i.e. 20 in total) you'll have 80% power to detect a difference between treatment and control of 1.25 standard deviations (SDs). With two treatments + control, 6 in each group (18 in total), you have 80% power to detect a difference of 1.4 SDs between both treatments combined and control, or 1.6 SDs between either treatment by itself and control (or between the two treatments). It may well be sensible use a log-transform (or perhaps some other transform) your data prior to analysis, in which case the SDs are the SDs of the transformed variables.In the social sciences, an effect of around 0.8 SDs or over would often be considered \"large\", and designing a study to detect to have decent power only to detect a bigger effect than this might be politely described as \"optimistic\". But remember that the SD here is the SD of the residual, unexplained variation. You can reduce this by either (1) making your experimental units more uniform or (2) explaining more of the variation by other means. The lower the uncontrolled variability the higher the power you'll have to detect effects due to the factors open to experimental manipulation. You say \"variability in field measurements can be as high as 25% within one group\". But this is a laboratory experiment; is there a reason the variability need be this high in the lab? Can you homogenise your soil before you start the experiment? I guess this may destroy the soil structure though?Can you take baseline measurements before the treatments are applied? Using these to explain some of the inate variability between units by either analysing change since baseline or (better) adding them to the model as covariates (.e. ANCOVA) may help a lot.Sorry I haven't mentioned G*Power 3 but i've never heard of it and from a quick look the link you gave it looks considerably more sophisticated, and therefore complicated, than is necessary here.","Display_name":"onestop","Creater_id":449,"Start_date":"2010-11-19 15:16:04","Question_id":4729}
{"_id":{"$oid":"5837a580a05283111e4d55ae"},"Last_activity":"2016-08-12 10:13:56","Creator_reputation":66,"Question_score":0,"Answer_content":"Carry out a multiple regression with a logit transformed dependent variable. This may be able to identify what factors are associated with stunting, but there are limitations to how you can use and interpret the output.The typical way to analyse this data would be to use multiple regression with the percentage of children with Vitamin A deficiency and the percentage of children that recently had diarrhoea both as predictor variables, and to use the percentage of stunted children as the response variable.Because classical regression assumes that the errors of the response variable are normally distributed, convert the percentage of stunted children to a decimal (57% is 0.57) and then apply a logit transformation ( ln (p / ( 1- p ) ), where p is your proportion. (A logit transformation is very similar to a probit transformation.)There is no need to transform the predictor variables on the basis that percentages are not normally distributed, only the response variable.Regression, and other ANOVA based techniques, have a whole range of assumptions that must be met if the interpretation of the statistical outputs is to lead reliable conclusions. A nice guide to preliminary data exploration is given here http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full, and an explanation of regression diagnostics is given here http://www.nature.com/nmeth/journal/v13/n5/full/nmeth.3854.html. Both of these should help you identify whether the results of the multiple regression are useful.With your data, you will not be able to explicitly test whether the relationships between Vitamin A deficiency, diarrhoea and stunting differ among the provinces. Of course, if there are one or two outliers then this could be taken as evidence that there are different relationships in these provinces compared to all the others.If you do find a significant effect of either factor and want to use the regression model, then it only applies to the prediction of the proportion of stunting within a state, not to the individual children within that state. It is not clear how the results could be used to predict a child's risk of stunting given their nutritional and health history.The multiple regression will treat each of the risk factors as being independent and additive in the effect on the risk of stunting, but the risks may not be additive. Again, the presence of outliers in the regression may indicate that this is occurring.With the example you have presented, there is a strong risk that both your predictors are correlated which, firstly, decreases the likelihood that you will detect the effects of the predictors, and more importantly, will lead to biased estimates of the regression coefficients.Regression techniques also assume that the predictors are measured without error. I assume this is not the case here. The use of classical (Model I) regression techniques on data where all variables are subject to measurement error is that, again, regression coefficients are biased. Some more information in this problem is given here https://www.ine.pt/revstat/pdf/rs100104.pdf.Sokal and Rohlf (Biometry, Third Edition, Chapters 14, 15 and 16) explore the conceptual confusion between correlation (which your problem is, strictly speaking) and regression. Despite this, multiple regression is probably a sensible starting point and may give some useful indications of factors associated with stunting.","Display_name":"Groovy_Worm","Creater_id":124314,"Start_date":"2016-08-12 10:13:56","Question_id":226326}
{"_id":{"$oid":"5837a580a05283111e4d55bb"},"Last_activity":"2015-04-18 10:02:08","Creator_reputation":17449,"Question_score":1,"Answer_content":"The reference category is the combination of first levels of the factors in the model:\u0026gt; with(Arrests, levels(colour))[1] \"Black\" \"White\"\u0026gt; with(Arrests, levels(year))[1] \"1997\" \"1998\" \"1999\" \"2000\" \"2001\" \"2002\"So the Intercept is for colourBlack:year1997 and the default contrasts specify differences in means with is class and the other combinations of factors involved in your model specification, hence colourWhite reflects the difference in  for the combination colourWhite:year1997. you can think of this really as colourBlack:year1997 + colourWhiteas colourWhite represents the difference in 1997 for colour White.The other interaction terms in the model are the additional differences for colour White in the other years, whilst the year main effects are the differences in  between the reference year and the other years for colourBlack`.Looking at the model matrix can often help in deciphering these things:\u0026gt; head(model.matrix(~ colour * year + colour * age, data = Arrests))  (Intercept) colourWhite year1998 year1999 year2000 year2001 year2002 age1           1           1        0        0        0        0        1  212           1           0        0        1        0        0        0  173           1           1        0        0        1        0        0  244           1           0        0        0        1        0        0  465           1           0        0        1        0        0        0  276           1           0        1        0        0        0        0  16  colourWhite:year1998 colourWhite:year1999 colourWhite:year20001                    0                    0                    02                    0                    0                    03                    0                    0                    14                    0                    0                    05                    0                    0                    06                    0                    0                    0  colourWhite:year2001 colourWhite:year2002 colourWhite:age1                    0                    1              212                    0                    0               03                    0                    0              244                    0                    0               05                    0                    0               06                    0                    0               0The look at the first few rows of the data to see how the dummy variables indicate the various groupings indicated by the combinations of factors and their interactions.","Display_name":"Gavin Simpson","Creater_id":1390,"Start_date":"2015-04-18 10:02:08","Question_id":147083}
{"_id":{"$oid":"5837a580a05283111e4d55c8"},"Last_activity":"2016-08-12 09:59:56","Creator_reputation":3619,"Question_score":0,"Answer_content":"In answer to the part of your question about combining -values, yes someone has written a function to do it - me. Search CRAN for the metap package. There are other methods for combining -values included in it. Having said that I would regard combining -values as a last resort for situations where you cannot do either a full overall analysis or at least a meta-analysis of summary statistics.For more information about combining see this discussion which will lead you to this answer","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-12 02:40:53","Question_id":229433}
{"_id":{"$oid":"5837a580a05283111e4d55d5"},"Last_activity":"2016-08-12 09:36:13","Creator_reputation":4441,"Question_score":2,"Answer_content":"First please check Matthew Drury's answer here for a similar question.In general, model complexity can be defined as a function of number of free parameters: the more free parameters a model has, the more complex the model is. In addition, if a model has many parameters but we put many restrictions to them, so they are not that \"free\", it would also give us a \"simpler\" model. That is the key idea behind regularization.There are different definitions of model complexity. AIC and BIC are widely used. In the link you provided, the notations are not standardized or widely used in statistics / machine learning fields but with some customization of author's preference. As mentioned in the document,  is number of leaves in a boosting tree and  is score for each leaves. So intuitively, the more leaves we have, the more free parameters we have, and the large the weights are the more complex the model is (which is similar to ridge regression)To conclude, as mentioned in the original link, this is one way of defining the complexity (not a \"standard way\"), and the it penalize number of leaves and the weights of the leaves using L2 norm. is a threshold for the gain.  if the gain is smaller than , we would do better not to add that branch.  is a regularization parameter.The larger the  and  are the more regularization on the model / simpler the model is.check this link and search gamma and lambda.  gamma [default=0]  minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be.    lambda [default=1]  L2 regularization term on weights, increase this value will make model more conservative.","Display_name":"hxd1011","Creater_id":113777,"Start_date":"2016-08-12 08:43:20","Question_id":229557}
{"_id":{"$oid":"5837a580a05283111e4d55e2"},"Last_activity":"2016-08-02 07:56:23","Creator_reputation":152613,"Question_score":4,"Answer_content":"If you have two variables,  and  that are bivariate normal, then  the correlation of two corresponding lognormals is a simple function of the correlation of the corresponding normals and their standard deviations. For example, mpiktas' answer to this question gives the result. Now  and  are bivariate normal with the same variances and correlation as  and , so  and  must have the same correlation as  and  do.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-02 07:56:23","Question_id":226844}
{"_id":{"$oid":"5837a580a05283111e4d55ef"},"Last_activity":"2016-08-12 09:31:58","Creator_reputation":8337,"Question_score":0,"Answer_content":"You can use the defaults. Of course, it's possible to overfit to the test set by trying lots of different hyperparameter values and seeing what performance on the test set they lead to, but if all you're trying is two options, the default values and tuned values, and the default values themselves weren't set with the test set somehow, this kind of overfitting is not a substantial danger.Note that when tuning hyperparameters worsens test-set performance but improves training-set performance, that's a hint that the tuning procedure is overfitting on the training set. Using regularization or switching to a simpler model may help.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-12 09:31:58","Question_id":229445}
{"_id":{"$oid":"5837a580a05283111e4d55fb"},"Last_activity":"2016-08-12 09:19:11","Creator_reputation":4884,"Question_score":6,"Answer_content":"Exchangeability doesn't apply to a distribution, but a sequence of random variables. From wikipedia:  Formally, an exchangeable sequence of random variables is a finite or infinite sequence  of random variables such that for any finite permutation  of the indices , [...] the joint probability distribution of the permuted sequence     is the same as the joint probability distribution of the original sequence.A sequence  of draws from a hypergeometric distribution has mass , which you can see is invariant to permutation, and thus exchangeable.What I think you might be curious about is whether the sequence of successes summarized by a hypergeometric is exchangeable. It is: The PMF depends only on , the count of successes, and not their order.For examples of sequences that are non-exchangeable, any sequence  in which each  depends solely on the prior random variable  will do. (Meaning, )Say, , with :import numpy as npfrom scipy.stats import normX = np.array([0,1,2,3])X_permuted = np.random.permutation(X)X_diff = np.diff(X)X_permuted_diff = np.diff(X_permuted)assert norm.pdf(X_diff).prod() == norm.pdf(X_diff_perm).prod()You can also view this example as  with , which provides a—helpful, I hope—counterexample: If you take the differences first, they're exchangeable.# This won't throw an errorX_diff_permuted = np.random.permutation(X_diff)assert norm.pdf(X_diff).prod() == norm.pdf(X_diff_permuted).prod()","Display_name":"Sean Easter","Creater_id":28462,"Start_date":"2016-08-12 06:58:52","Question_id":229528}
{"_id":{"$oid":"5837a580a05283111e4d55fc"},"Last_activity":"2016-08-12 06:15:00","Creator_reputation":26,"Question_score":2,"Answer_content":"I am not sure I completely understand your question. The term exchangeability refers to a sequence of random variables. A sequence of random variables is exchangeable when is invariant to random permutations. A sequence of random variable is exchangeable as long as the random variables are i.i.d.. Therefore if you have lets say some samples for any distribution that are independent one another then you have an exchangeable sequence of random variables.Does this answer your question?","Display_name":"Akis","Creater_id":106636,"Start_date":"2016-08-12 06:15:00","Question_id":229528}
{"_id":{"$oid":"5837a580a05283111e4d5609"},"Last_activity":"2016-08-12 08:17:29","Creator_reputation":5797,"Question_score":8,"Answer_content":"Given that the inverse exponential distribution has , you have stumbled upon the fact that the mean of the inverse exponential is .  And therefore, the variance of the inverse exponential is undefined.If  is inverse exponentially distributed,  exists and is finite for , and  for .","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-12 07:50:34","Question_id":229543}
{"_id":{"$oid":"5837a580a05283111e4d560a"},"Last_activity":"2016-08-12 08:00:53","Creator_reputation":3354,"Question_score":0,"Answer_content":"After a quick simulation (in R), it seems that the mean does not exist :n\u0026lt;-1000rates \u0026lt;- c(1,0.5,2,10)par(mfrow = c(2,2))for(rate in rates){  plot(cumsum(1/rexp(n, rate))/seq(1,n),type='l',main = paste0(\"Rate = \",rate),       xlab = \"Sample size\", ylab = \"Empirical Mean\")}For the sake of comparison, here is what happens with a genuine exponential random variable.","Display_name":"RUser4512","Creater_id":73794,"Start_date":"2016-08-12 07:50:21","Question_id":229543}
{"_id":{"$oid":"5837a580a05283111e4d560b"},"Last_activity":"2016-08-12 07:51:57","Creator_reputation":104,"Question_score":3,"Answer_content":"I'll show the calculation for the mean of an Exponential distribution so it will recall you the approach. Then, I'll go for the inverse Exponential with the same approach.Given Integrating by part (ignore the  in front of the integral for the moment),Multiply by the  in front of the integral,Evaluate for  and ,Which is a known results.For , the same logic apply.The main difference is that for an integration by parts,andso it doesn't help us for . I think the integral is undefined here. Wolfram alpha tell me it doesn't converge.http://www.wolframalpha.com/input/?i=integrate+from+0+to+infinity+(1%2Fx)+exp(-x)+dxSo the mean doesn't exist for the inverse Exponential, or, equivalently, for the inverse Gamma with . The reason is similar for the variance and .","Display_name":"\u0026#201;tienne Vanasse","Creater_id":126899,"Start_date":"2016-08-12 07:51:57","Question_id":229543}
{"_id":{"$oid":"5837a580a05283111e4d5618"},"Last_activity":"2016-08-12 08:51:59","Creator_reputation":11,"Question_score":0,"Answer_content":"There is a classic paper (Wolpert, 1996) that discusses no-free-lunch theorem mentioned above. The paper can be found here. But according to the paper and most practitioners,  \"there are [rarely]a priori distinctions between learning algorithms.\" Note: I replaced \"no\" with \"rarely\".ReferenceWolpert, D. H. (1996). The lack of a priori distinctions between learning algorithms. Neural computation, 8(7), 1341-1390.","Display_name":"Scott Worland","Creater_id":114321,"Start_date":"2016-08-12 08:29:02","Question_id":87135}
{"_id":{"$oid":"5837a580a05283111e4d5619"},"Last_activity":"2014-02-19 06:01:19","Creator_reputation":15579,"Question_score":1,"Answer_content":"  are there any rules of the format \"IF feature X has property Z THEN do Y\"?Yes, there are such rules. Or rather, if x then is is sensible to try y and z and avoid w.However, what is sensible and what is not depends onyour application (influences e.g. expected complexity of the problem)the size of the data set: how many rows, how many columns, how many independent casesthe type of data / what kind of measurement. E.g. gene microarray data and vibrational spectroscopy data often have comparable size, but the different nature of the data suggests different regularization approaches.and in practice also on your experience in applying different methods.Without more specific information I think that is about as much as we can say.If you want to have a general answer to the general problem, I recommend the Elements of Statistical Learning for a start.","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2014-02-19 06:01:19","Question_id":87135}
{"_id":{"$oid":"5837a580a05283111e4d5625"},"Last_activity":"2016-08-12 08:42:10","Creator_reputation":147548,"Question_score":20,"Answer_content":"Easy: sample from a Uniform distribution and recode from binary to ternary, interpreting each \"1\" as a \"2\".  (This is the inverse probability transform approach: it does indeed invert the CDF!)Here is an R implementation, written in a way that ought to port readily to almost any computing environment.binary.to.ternary \u0026lt;- function(x) {  y \u0026lt;- 0  x \u0026lt;- round(2^52 * x)  for (i in 1:52) {    y \u0026lt;- y + 2*(x %% 2)    y \u0026lt;- y/3    x \u0026lt;- floor(x/2)  }  y}n \u0026lt;- 1000x \u0026lt;- runif(n)y \u0026lt;- binary.to.ternary(x)plot(ecdf(y), pch=\".\")","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-12 08:28:48","Question_id":229556}
{"_id":{"$oid":"5837a580a05283111e4d5634"},"Last_activity":"2016-08-12 08:27:01","Creator_reputation":7975,"Question_score":1,"Answer_content":"  Why does the accuracy suddenly spike down randomly into the mid 90 percents, then shoot back up?It depends on your parameter update strategy, your parameter update strategy's parameters (e.g., learning rate), your network, and your data set. E.g. see the spike of Rmsprop even though the network had almost converged:(Images credit: Alec Radford.)As long as you have the same spike on your validation set, it shouldn't be an issue. If you wish to further investigate, you may want to look at the gradients, and whether the spikes happen on some particular samples.  How can I tell if I am \"overfitting\" my data set?See http://datascience.stackexchange.com/a/627/843.Since it looks like the accuracy on the test set is pretty much 1, I wouldn't worry about overfitting.   Is my breakdown of training vs. testing data optimal? (i.e. I chose to use 5% of my images to test and 95% of my images to train. Is there a better combination?)The test set should reflect as best as possible the actual samples on which your system will be used in production.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-12 08:27:01","Question_id":229301}
{"_id":{"$oid":"5837a580a05283111e4d5640"},"Last_activity":"2016-08-12 08:20:06","Creator_reputation":147548,"Question_score":5,"Answer_content":"Use a robust non-parametric fit, such as Loess, to smooth the points.  A boxplot of the residuals will separately indicate all those that are \"far outliers.\"  The code below computes those far outliers using a standard (robust) method very similar to the one popularized by John Tukey.  Assuming the robust fit captures the main trend of the points, up to 25% of the data can be high outliers and another 25% low outliers and still be detected.  An advantage of Loess is that it can be \"tuned\" to local parts of the plot or to follow the global trend; so if it seems to be affected by clumps of outliers, make it smooth more agressively (by modifying the f parameter in this implementation).The Loess fit is drawn as a black curve.  The automatically identified outliers have been overplotted in red.How well does it work?  Let's make the random errors fifty times greater and try again:That's about its limit: if we increase the random errors still further, the \"noise\" around the main body of points will grow to absorb the outliers one by one.  Experiment by changing the sd parameter in the calculation of y.## Generate sample data.# Comment out `set.seed` to vary the data on subsequent runs.#set.seed(17)x \u0026lt;- seq(250, 950, length.out=100)y \u0026lt;- 120*(1 - exp((x-2000)/2000))i \u0026lt;- runif(length(x)) \u0026lt; 1/8y[i] \u0026lt;- y[i] + 20 * ceiling(rnorm(sum(i)))y[!i] \u0026lt;- y[!i] + rnorm(sum(!i), sd=0.1)    # `sd=5` for the second figure## Plot the data.#par(mfrow=c(1,1))plot(x,y, ylim=c(0, max(y)), col=\"#404040\", xlab=\"\", ylab=\"\")## Fit the data with a robust method and plot the fit.#model \u0026lt;- lowess(y ~ x)lines(model, lwd=2)## Identify the extreme residuals and plot them.#residuals \u0026lt;- y - model$yhinges \u0026lt;- quantile(residuals, c(1/4, 3/4))step \u0026lt;- diff(hinges)fences \u0026lt;- 2.5*step*c(-1,1) + hingesfar.out \u0026lt;- residuals \u0026lt; fences[1] | residuals \u0026gt; fences[2]points(x[far.out], y[far.out], pch=19, col=\"Red\")","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-12 08:11:11","Question_id":229537}
{"_id":{"$oid":"5837a580a05283111e4d564f"},"Last_activity":"2016-08-12 07:54:09","Creator_reputation":371,"Question_score":1,"Answer_content":"Jokingly saying, you should ask Preacher and Hayes what they have to say :-) but afaik their steps are just a little bit better than Baron and Kenny, which happened to be heavily criticized and has been deemed outdated by many (eg. Is Baron and Kenny method for mediation now outdated? )Preacher actually did a paper about partial mediation (but not only). Here is some interesting excerpts:  Importantly, we also extend past work on this topic by questioning the  emphasis on the significance of c' after including the proposed  mediator, as well as the use of terms such as ‘full’ versus ‘partial’  mediation.Or this:  Abandon the emphasis on the significance of c and c'Or this one:  These findings underscore the importance of avoiding the terms ‘full’  or ‘partial’ when describing mediation.Rucker, D.D., Preacher, K.J., Tormala, Z.L., Petty, R.E., (2011)., Mediation Analysis in Social Psychology: CurrentPractices and New Recommendations, Social and Personality Psychology Compass 5/6, 359–371, DOI: 10.1111/j.1751-9004.2011.00355.x However, I'd like to draw your attention to another paper about whole this mediation story. It's titled 'Mediation myth' and it covers almost all aspects of today's mediation analysis. Nicely written, but with quite nasty (for us) conclusions:  The observation that ab is significant would be taken by many  researchers as evidence for mediation.  But whether results of  significance testing in mediation analysis generally have any  meaningful interpretation is a critical question, one that is rarely  considered in most mediation studies.Rex B. Kline (2015). The Mediation Myth, Basic and Applied SocialPsychology, 37:4, 202-213, DOI: 10.1080/01973533.2015.1049349 ","Display_name":"Lil","Creater_id":48279,"Start_date":"2016-08-12 07:54:09","Question_id":229136}
{"_id":{"$oid":"5837a580a05283111e4d565e"},"Last_activity":"2016-08-12 07:38:57","Creator_reputation":5445,"Question_score":2,"Answer_content":"Imputation using within subject means isn't a great idea because it will result in biased (too small) standard errors and possibly biased estimates.Assuming that the data are missing at random, a much better idea is to use multiple imputation. The mice package in R has the capability to impute continuous variables in a mixed efects framework with a single random effect (grouping variable) - just specify 2l.norm as the grouping variable. For example, suppose our analysis model is\u0026gt; require(mice)\u0026gt; require(lme4)\u0026gt; m0 \u0026lt;- lmer(teachpop~sex+texp+popular + (1|school), data=popmis)\u0026gt; confint(m0)                  2.5 %     97.5 %.sig01       0.44905533 0.62574295.sigma       0.54368549 0.59259188(Intercept)  2.03118933 2.67864796sex         -0.07108881 0.09183821texp         0.03024598 0.06505065popular      0.22257646 0.32572600Due to missingness in the predictor popular this model may be biased. So we will use multiple imputation:\u0026gt; ini \u0026lt;- mice(popmis, maxit=0)\u0026gt; (pred \u0026lt;- ini$pred)         pupil school popular sex texp const teachpoppupil        0      0       0   0    0     0        0school       0      0       0   0    0     0        0popular      1      1       0   1    1     0        1sex          0      0       0   0    0     0        0texp         0      0       0   0    0     0        0const        0      0       0   0    0     0        0teachpop     0      0       0   0    0     0        0This is the default predictor matrix for the imputation model. Only popular has missing values, and we are going to impute them using a mixed model where school is the grouping factor, and the other variables are fixed effects. To do this, we use -2 to tell mice that school is the grouping variable, and 2 for the fixed effects:\u0026gt; pred[\"popular\",] \u0026lt;- c(0, -2, 0, 2, 2, 2, 0)\u0026gt; (pred)So now we have:         pupil school popular sex texp const teachpoppupil        0      0       0   0    0     0        0school       0      0       0   0    0     0        0popular      0     -2       0   2    2     2        0sex          0      0       0   0    0     0        0texp         0      0       0   0    0     0        0const        0      0       0   0    0     0        0teachpop     0      0       0   0    0     0        0We have set up the predictor matrix, so we can now create 10 multiply imputed datasets using the 2l.norm method to impute values for popular\u0026gt; imp \u0026lt;- mice(popmis, meth = c(\"\",\"\",\"2l.norm\",\"\",\"\",\"\",\"\"), pred = pred, maxit=10, m = 10)Now we run the mixed model on each of the imputed datasets:\u0026gt; fit \u0026lt;- with(imp, lmer(teachpop~sex+texp+popular + (1|school)))...and pool the results:\u0026gt; summary(pool(fit))                   est          se         t        df     Pr(\u0026gt;|t|)      lo 95      hi 95 nmis(Intercept) 2.73951576 0.165053863 16.597708 1991.5874 0.000000e+00 2.41581941 3.06321211   NAsex         0.08620420 0.031042794  2.776947  915.1865 5.599307e-03 0.02528087 0.14712753    0texp        0.05682495 0.009713717  5.849970 1991.4452 5.733929e-09 0.03777484 0.07587506    0popular     0.16696926 0.018760706  8.899945 1980.9159 0.000000e+00 0.13017647 0.20376205  848 ","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-12 07:38:57","Question_id":229410}
{"_id":{"$oid":"5837a580a05283111e4d566a"},"Last_activity":"2016-08-12 07:34:38","Creator_reputation":12982,"Question_score":0,"Answer_content":"  How can I make this guess?As pointed out in other answers, sometimes you know what the distribution must be due to the nature of the data generating process. Consider Generalized Extreme Value Distribution as described in Wikipedia:   By the extreme value theorem the GEV distribution is the only possible limit distribution of properly normalized maxima of a sequence of independent and identically distributed random variables.Of course, this is an asymptotic result, but you may count on it for sufficiently large samples.Other times you may just have a rough idea and would not know exactly. However, this may suffice in the framework of quasi maximum likelihood estimation (QMLE). QMLE allows consistently estimating model parameters and doing inference when the assumed distribution does not match the true distribution. Even though it does not work universally (not all distributions can be assumed in place of other distributions), it can still be pretty useful.(I have been trying to get an intuitive explanation of why and how QMLE works by asking about the Idea and intuition behind quasi maximum likelihood estimation (QMLE) here before.)","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-11 23:28:01","Question_id":229396}
{"_id":{"$oid":"5837a580a05283111e4d566b"},"Last_activity":"2016-08-11 16:16:42","Creator_reputation":19131,"Question_score":1,"Answer_content":"To apply MLE you need to assume a distribution. So, yes, you need to have a distribution in mind, usually. The standard intro texts use Gaussian. For instance, they'd show you how Gaussian distribution leads to MLE in linear model to the same estimators as in least squares regression.Gaussian distribution with independence (random sample) assumption is a popular choice. However, other distributions are used when they're more suitable for a problem. Often, you don't have to \"guess\" the distribution, but already know what family it belongs to. Maybe you know it must be Poisson, for instance. In this case you plug it into MLE equations and derive the appropriate likelihood function to estimate the parameter of the distribution","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-08-11 12:16:18","Question_id":229396}
{"_id":{"$oid":"5837a580a05283111e4d566c"},"Last_activity":"2016-08-11 16:15:56","Creator_reputation":5797,"Question_score":4,"Answer_content":"To apply parametric MLE, you need to specify a parametric distribution.  For non-parametric MLE, you do not specify a parametric distribution.The most popular of the non-parametric MLE approaches is called Empirical Likelihood https://en.wikipedia.org/wiki/Empirical_likelihood (not much of a write up on that page).  The classic book in the field is \"Empirical Likelihood\" by Art B, Owen https://www.amazon.com/Empirical-Likelihood-Art-B-Owen/dp/1584880716 . The freely accessible paper \"Empirical Likelihood\", Art B, Owen, Annals of Statistics 1990, Vol. 18, pp. 90-120 https://projecteuclid.org/download/pdf_1/euclid.aos/1176347494 will give you a pretty good idea of the field.  Freely available slides by Owen are at http://statweb.stanford.edu/~owen/pubtalks/DASprott.pdf .Basically, Empirical Likelihood makes use of the empirical distribution of the data, as the basis for forming an empirical likelihood. This empirical likelihood can be maximized, subject to various constraints, sometimes in closed form, but often requiring numerical constrained nonlinear optimization methods.  It can be used as the basis for computing non-parametric likelihood ratio tests and confidence regions (not necessarily ellipsoidal or symmetric).There are relationships between empirical likelihood and bootstrapping, and indeed, the two can be combined.If you don't have a solid rationale for use of a particular parametric distribution, you're generally better off using a non-parametric method, such as empirical likelihood.  The downside may be that the computations are more computationally intensive, and the confidence regions which result do not look like those most people have come to expect based on, for instance, Normal distribution assumptions.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-11 12:49:01","Question_id":229396}
{"_id":{"$oid":"5837a580a05283111e4d566d"},"Last_activity":"2016-08-11 16:09:01","Creator_reputation":17414,"Question_score":0,"Answer_content":"In general, no you cannot use MLE to find which family of distributions might provide a good parametric model for an outcome. That's not to say that there aren't some exploratory techniques that could shed some light on possibilities. But, as we know from statistics, using the same data as a hypothesis generating and hypothesis confirming tool will lead to increased false positive errors.Ideally a family of distributions is chosen before the data are collected. You can often think about the data generating mechanism and/or draw parallels between what other researchers have used and discussed. For instance, Poisson variables come from independent exponential interarrival times, and 3 parameter Weibull models can flexibly describe time-to-event curves. You can also rely on the fact that predictions and inference coming from similar probability models tends to be quite similar, for instance, inference from the t-test tends to be quite similar to the z-test even in moderately small samples.Another thing to consider is that Tukey was quoted as having said, \"Build your model as big as a house!\" within the limits of the data themselves, making oversimplified assumptions tends to be unnecessary when more flexible nested parametric models are available. For instance, instead of exponential time-to-event models, you could consider Weibull as a bigger class, or 3 parameter Weibull as an even bigger class of models. For counting processes, negative binomial models are basically two parameter Poisson models. You can even consider mixtures or empirical likelihood as ways of describing densities with a minimal number of assumptions.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-11 16:09:01","Question_id":229396}
{"_id":{"$oid":"5837a580a05283111e4d567a"},"Last_activity":"2016-08-12 07:28:47","Creator_reputation":1,"Question_score":-1,"Answer_content":"If we start with an infinitely-large ordered set and we pick two elements from the set, we know that one (and only one) of the following situations can be correct:a \u0026lt; ba = ba \u003e bWe can state:P( a = b ) = 0 (assuming selection from an infinite set of values)P( a \u0026lt; b ) = P ( a \u003e b ) (because a and b are independently selected)Therefore P( a \u0026lt; b ) = 0.5.  Your example refer to independent selection from the set of all real numbers in the range (0, 1), so this result applies.","Display_name":"Owen Garrett","Creater_id":127496,"Start_date":"2016-08-12 07:28:47","Question_id":229499}
{"_id":{"$oid":"5837a580a05283111e4d567b"},"Last_activity":"2016-08-12 05:24:33","Creator_reputation":30471,"Question_score":4,"Answer_content":"For independent continuous  random variables we have (taking into account that their density is constant and equal to )P(a\u0026lt;b) =P(a\\leq b)= \\int_0^1 \\int_0^b da\\,db = \\int_0^1(b-0)\\, db =\\int_0^1b\\, db = \\frac 12 b^2 \\big|^1_0 = \\frac 12\\cdot1^2 - \\frac 12\\cdot 0^2 = \\frac 12","Display_name":"Alecos Papadopoulos","Creater_id":28746,"Start_date":"2016-08-12 05:24:33","Question_id":229499}
{"_id":{"$oid":"5837a580a05283111e4d567c"},"Last_activity":"2016-08-12 05:05:51","Creator_reputation":5797,"Question_score":7,"Answer_content":"If  and  are independent and identically distributed as , then . It is also true that , because In fact, if  and  are independent and identically distributed from any continuous distribution on the real numbers, then , , and ","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-12 05:05:51","Question_id":229499}
{"_id":{"$oid":"5837a580a05283111e4d568b"},"Last_activity":"2016-08-12 07:08:59","Creator_reputation":246,"Question_score":5,"Answer_content":"1) There is nothing wrong with negative AIC values. AIC values are arbitrary; even an AIC of 0 isn't directly interpretable. 2) There is nothing wrong with large differences in AIC values between models. Indeed, this is often what you are looking for with AIC to begin with. Without more details of what exactly your models are it is difficult to be more specific. If you are really concerned about wildly shifting AIC values between models, you may want to take a look at the assumptions of your models themselves. Maybe you have variables with very different ranges and should consider standardization, maybe you have misspecified the functional relationship between outcome and predictors, etc.3) Why can't you compute the Akaike weight? The weight is just the value exp(-0.5*AIC) averaged across all pairwise model comparisons (where AIC is just the difference between two AIC values).EDIT: In light of your comment on the OP, you say the weight is going to infinity. My answer above actually contains an error, the weight is not that value averaged across all pairwise model comparisons. The weight is the average of those values where AIC is the difference between each model and the SMALLEST AIC in the group of models of interest. So, if: Then:\u0026gt; AIC \u0026lt;- c(-105885.1, -105121.2, -109740.6, -117007, -105858.8, -108601.9, -108856.9)\u0026gt; delta \u0026lt;- x-min(x)\u0026gt; denominator \u0026lt;- sum(exp(-0.5*delta))\u0026gt; weights \u0026lt;- exp(-0.5*delta)/denominator\u0026gt; weights[1] 0 0 0 1 0 0 0The weights aren't being driven towards infinity, but towards 0. Even so, this doesn't seem to be a huge issue (any more so than having large pairwise AIC differences, anyway). Weights are just a way of comparing relative likelihoods between models; if the differences in likelihood (and thus AIC) between models are large, the weights will trend towards 0 and 1. In any case, the weights just give you the same interpretation as choosing the smallest AIC. Honestly, weights would only seem to be useful to begin with in the case where the differences between AIC values for all the models are relatively small, anyway.","Display_name":"Ryan Simmons","Creater_id":40443,"Start_date":"2016-08-12 06:41:50","Question_id":229533}
{"_id":{"$oid":"5837a580a05283111e4d5698"},"Last_activity":"2016-08-12 07:01:35","Creator_reputation":26,"Question_score":1,"Answer_content":"To answer my own question (community wiki, feel free to adjust), after giving it some more thought and researching a bit:    Q: Is there a difference in the way the test statistics are calculated?A: No. I suppose @Glen_b was hinting in that direction. T-statistic remains the same, as do the DF.   Q: Is the latter approach wrong?A: No, the former is wrong. As can be read here (http://www.biostathandbook.com/pairedttest.html): The paired t–test assumes that the differences between pairs are normally distributed. Performing a t-test without checking the normality of the difference is wrong, regardless of the normality of X or Z (the constituents of the difference).   Q: In the proposed should situation should I always favour option (1) over option (2)?A: Either options work but normality of the difference needs to be verified in either case. You might as well do the one-sample t-test because you need to calculate the difference anyhow. ","Display_name":"Jef Van Alsenoy","Creater_id":123928,"Start_date":"2016-08-12 07:01:35","Question_id":229491}
{"_id":{"$oid":"5837a580a05283111e4d56a5"},"Last_activity":"2016-08-11 13:54:46","Creator_reputation":8893,"Question_score":11,"Answer_content":"In general, I am  unsure that the spectral norm is the most widely used. For example the Frobenius norm is used for to approximate solution on non-negative matrix factorisation or correlation/covariance matrix regularisation. I think that part of this question stems from the terminology misdemeanour some people do (myself included) when referring to the Frobenius norm as the Euclidean matrix norm. We should not because actually the  matrix norm (ie. the spectral norm) is the one that is induced to matrices when using the  vector norm. The Frobenius norm is that is element-wise: , while the  matrix norm () is based on singular values so it is therefore more \"univeral\". (for luck of a better term?)The  matrix norm is a Euclidean-type norm since it is induced by the Euclidean vector norm, where . It therefore an induced norm for matrices because it is induced by a vector norm, the  vector norm in this case.Probably MATLAB  aims to provide the  norm by default when using the command norm; as a consequence it provides the Euclidean vector norm but also the  matrix norm, ie. the spectral matrix norm (rather than the wrongly quoted \"Frobenius/Euclidean matrix norm\"). Finally let me note that what is the default norm is a matter of opinion to some extend: For example J.E. Gentle's \"Matrix Algebra - Theory, Computations, and Applications in Statistics\" literally has a chapter (3.9.2) named: \"The Frobenius Norm - The “Usual” Norm\"; so clearly the spectral norm is not the default norm for all parties considered! :) As commented by @amoeba, different communities might have different terminology conventions. It goes without saying that I think Gentle's book is an invaluable resource on the matter of Lin. Algebra application in Statistics and I would prompt you to look it further!","Display_name":"usεr11852","Creater_id":11852,"Start_date":"2016-08-11 08:10:34","Question_id":229354}
{"_id":{"$oid":"5837a580a05283111e4d56a6"},"Last_activity":"2016-08-11 13:35:30","Creator_reputation":3398,"Question_score":5,"Answer_content":"The answer to this depends on the field you're in. If you're a mathematician, then all norms in finite dimensions are equivalent: for any two norms  and , there exist constants , which depend only on dimension  (and a,b) such that:C_1\\|x\\|_b\\leq \\|x\\|_a\\leq C_2\\|x\\|_b.This implies that norms in finite dimensions are quite boring and there is essentially no difference between them except in how they scale. This usually means that you can choose the most convenient norm for the problem you're trying to solve. Usually you want to answer questions like \"is this operator or procedure bounded\" or \"does this numerical process converge.\" With boundedness, you only usually care that something is finite. With convergence, by sacrificing the rate at which you have convergence, you can opt to use a more convenient norm. For example, in numerical linear algebra, the Frobenius norm is sometimes preferred because it's a lot easier to calculate than the euclidean norm, and also that it naturally connects with a wider class of Hilbert Schmidt operators. Also, like the Euclidean norm, it's submultiplictive: , unlike say, the max norm, so it allows you to easily talk about operator multiplication in whatever space you're working in. People tend to really like both the  norm and the Frobenius norm because they have natural relations to both the eigenvalues and singular values of matrices, along with being submultiplictive. For practical purposes, the differences between norms become more pronounced because we live in a world of dimensions and it usually matters how big a certain quantity is, and how it's measured. Those constants  above are not exactly tight, so it becomes important just how much more or less a certain norm  is compared to . ","Display_name":"Alex R.","Creater_id":61092,"Start_date":"2016-08-11 11:25:42","Question_id":229354}
{"_id":{"$oid":"5837a580a05283111e4d56a7"},"Last_activity":"2016-08-11 12:00:38","Creator_reputation":21588,"Question_score":6,"Answer_content":"A part of the answer may be related to numeric computing.When you solve the systemAx=bin finite precision, you don't get the exact answer to that problem. You can some degree of approximation  due to the constraints of finite arithmetics, so that , in some suitable sense. What is it that your solution represents, then? Well, it may well be an exact solution to some other system like\\tilde A \\tilde x = \\tilde bSo for  to have utility, the tilde-system must be close to the original system:\\tilde A \\approx A, \\quad \\tilde b \\approx bIf your algorithm of solving the original system satisfies that property, then it is referred to as backward stable. Now, the accurate analysis of how big the discrepancies ,  are eventually leads to errors on bounds which are expressed as , . For some analyses, the  norm (max column sum) is the easiest one to push through, for others, the  norm (max row sum) is the easiest to push through (for components of the solution in the linear system case, for instance), and for yet others, the  spectral norm is the most appropriate one (induced by the traditional  vector norm, as pointed out in another answer). For the work horse of statistical computing in symmetric p.s.d. matrix inversion, Cholesky decomposition (trivia: the first sound is a [x] as in Greek letter \"chi\", not [tʃ] as in \"chase\"), the most convenient norm to keep track of the error bounds is the  norm... although the Frobenius norm also pops up in some results e.g. on partitioned matrix inversion.","Display_name":"StasK","Creater_id":5739,"Start_date":"2016-08-11 12:00:38","Question_id":229354}
{"_id":{"$oid":"5837a580a05283111e4d56b3"},"Last_activity":"2016-08-12 06:38:50","Creator_reputation":6087,"Question_score":4,"Answer_content":"Indeed when the univariate Markov chain central limit theorem holds, the multivariate CLT also holds. In fact, the the univariate CLT is a consequence of the multivariate CLT via the Cramer-Wold Theorem. Look at Page 7-9 here.However, the Theorem 4.4 you indicated has the hidden line of \"satisfying enough regularity conditions\". These regularity conditions are difficult to verify but do hold in general. These conditions are that of the rate of convergence of the Markov chain to its stationary distribution. This paper summarizes most of the research done in this field. For the Metropolis-Hasting MCMC (which is reversible), you need that the chain be geometrically ergodic with a finite 2nd moment.If these conditions hold true, the the multivariate CLT takes the following form: If , and  and  is geometrically ergodic, then there exists a  positive definite matrix,  such that,\\sqrt{t} \\left[\\dfrac{1}{t}\\sum_{i=1}^t{ \\phi(X_i)} - \\int_\\mathbb{X} \\phi(x) \\pi(x) \\right] \\to N_n(0, \\Sigma), where if , then the  entry of  is \\Sigma_{ij} = Cov_{\\pi}(Y^{(i)}_1, Y^{(j)}_1) + 2 \\sum_{k=1}^{\\infty} Cov_{\\pi}(Y^{(i)}_1, Y^{(j)}_{1+k}). ","Display_name":"Greenparker","Creater_id":31978,"Start_date":"2015-10-18 10:14:59","Question_id":177317}
{"_id":{"$oid":"5837a580a05283111e4d56c0"},"Last_activity":"2016-08-12 06:38:28","Creator_reputation":11400,"Question_score":0,"Answer_content":"You do seem to have found an error in the source you cite, but you have to be careful in general. Note this sentence from that source:  Previous use has been split into two components (according to whatever contrasts were specified for this variable).Exactly what is being displayed in the output of a model for a multi-level categorical variable depends on how the contrasts for that variable were coded, which isn't shown in the excerpts you quote in your question. In this case it seems that the R default treatment contrast was used. In that case, you are correct that the coefficient (and statistical tests) for each level's coefficient is relative to the reference level for that variable.Several other types of contrasts are possible, however, as explained for example on this page. One might be able to devise a contrast that provides, for each of 2 levels, a comparison of that level to the average of the other levels, as the source claims for this example. It just doesn't appear that such a contrast was actually used in this particular case.","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-08-12 06:38:28","Question_id":229456}
{"_id":{"$oid":"5837a580a05283111e4d56d3"},"Last_activity":"2016-08-12 05:43:00","Creator_reputation":57712,"Question_score":3,"Answer_content":"  If one wants to see how much variance there is in the responses that  various subjects give to a single rating measure, why are measures any  more sophisticated than a mere box plot, or error bar with SEMs  needed?If that's what you want to look at, then you don't need inter-rater reliability. In fact, you don't even need multiple raters. Measures of inter-rater reliability tell you something completely different; namely, they are about inter-rater association.  Box plots tell you nothing about this.  You can have perfect inter-rater reliability with big dispersion in each rater; you can have low IRR with small dispersion in each rater, or any other combination. The answer to your question in the middle paragraph is \"No, that's not it\". The answer to your question in the last paragraph is also \"No, that's not it\". EDIT: It appears I misunderstood. The question was about a boxplot of the differnces between raters. That makes more sense. However, it's still quite different from a measure of interrater reliability, just as a (regular) boxplot is not the same as the standard deviation; one is a number, the other a graph.  ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-12 04:59:32","Question_id":229506}
{"_id":{"$oid":"5837a580a05283111e4d56df"},"Last_activity":"2016-08-10 05:49:52","Creator_reputation":12308,"Question_score":2,"Answer_content":"library(lme4)library(agridat)data(john.alpha)# modelsmodel1 \u0026lt;- lmer(yield ~ 1 + rep + (1|gen) + (1|rep:block), john.alpha)model2 \u0026lt;- lmer(yield ~ 1 + rep + gen + (1|rep:block), john.alpha)As @ZheyuanLi says, you \"just apply a linear transform matrix\".  The shortcut is to use the lsmeans package, which does this for you:library(lsmeans)vcov(lsmeans(model1,specs=\"rep\"))vcov(lsmeans(model2,specs=\"rep\"))vcov(lsmeans(model2,specs=\"gen\"))","Display_name":"Ben Bolker","Creater_id":2126,"Start_date":"2016-08-10 05:49:52","Question_id":229525}
{"_id":{"$oid":"5837a580a05283111e4d56ec"},"Last_activity":"2015-11-17 21:13:00","Creator_reputation":463,"Question_score":1,"Answer_content":"Before beginning any sort of statistical analysis, I would strongly consider transforming your data from diversity indices into effective species counts.  Indexes such as Simpson's and Shannon's are highly nonlinear (e.g., a doubling of the index value does not equal a doubling of species diversity).  This could present a substantial challenge to interpreting your results. For example, a Simpson's index with a standard deviation of 0.1 represents substantially more variation in actual diversity when it the mean is near 0 than when it is near 1.  Additionally, the arithmetic mean of multiple diversity indices will not provide ecologically meaningful results. Using effective species counts (which are explained simply and thoroughly in the above link) linearizes these measurements, which will make the subsequent analyses substantially easier to interpret. ","Display_name":"C.R. Peterson","Creater_id":95049,"Start_date":"2015-11-17 21:13:00","Question_id":146188}
{"_id":{"$oid":"5837a580a05283111e4d56ed"},"Last_activity":"2015-04-13 14:55:16","Creator_reputation":1034,"Question_score":0,"Answer_content":"From the description, a t-test is the way to go. To rephrase what you said, you have two sites, and multiple observations at each site. This is the classic example of a two-sample independent T-Test (wiki).The gist of it is you need to not only see how far apart the means are, you need to know how variable the data is. That way you can say the difference in means is unlikely to be attributable to random sampling variance. The t-test tells you exactly that.You can do a t test in excel as fast as any of the stats programs. You just need two columns of SDI values, one for each site. Then you use the function t.test(), select your first column for the first parameter, the second column for your second parameter, use a 2-tailed test (wiki) for the third parameter (enter the number 2), and use the second option for the last parameter (homoskedastic).If you use R, the function is also t.test(), but there are multiple ways you can set up your data. Here is a tutorial. Basically, you could have two columns, one for each site like Excel, or one column with all your SDI values, and another column that categorizes your data (i.e. tells you whether that row was a sample from Site A or Site B).There are a lot of other considerations, for instance your samples from your sites ideally would have similar standard deviations. If they are very different, you can use option 3 for the last parameter in Excel. I won't give a full step-by-step in R, but there are plenty of ways to test for differences in standard deviation and correct for it. However, basic t-tests assuming homogeneity of variance (i.e. what I described above) should get you going.","Display_name":"le_andrew","Creater_id":73192,"Start_date":"2015-04-13 14:55:16","Question_id":146188}
{"_id":{"$oid":"5837a580a05283111e4d56fa"},"Last_activity":"2016-08-12 05:11:42","Creator_reputation":151,"Question_score":1,"Answer_content":"There was a similar question at:http://math.stackexchange.com/questions/513342/expected-squared-prediction-error/623473#623473I've posted the same answer below for convenience: By Bayes' Theorem  we have:Rearranging gives:Using definition of  we get:Using definition of  we get:Or an even shorter notation:","Display_name":"matt","Creater_id":30545,"Start_date":"2013-12-31 11:22:53","Question_id":21086}
{"_id":{"$oid":"5837a580a05283111e4d56fb"},"Last_activity":"2012-01-14 00:05:28","Creator_reputation":24512,"Question_score":5,"Answer_content":"(1) You're pointing out some rather ambiguous notation sometimes used sloppily with expectations. In my experience, whenever the extra parentheses are suppressed, as in , this indicates:  is a constant.  is that constant squared, not the expectation of the random variable . When that latter expectation is desired,  is written to reduce the ambiguity. In other cases, when parentheses can be harmlessly suppressed, as in , then they are. So, yes, E_{X}E_{Y|X}(g(X,Y))=E_{X}\\big(E_{Y|X}(g(X,Y))\\big).(2) This is a special case of the law of total expectation. By the definition of expectation, it can also be seen since it is equivalent to saying \\int_{D} g(x,y)p_{xy}(x,y)dxdy = \\int_{D_{x}} p_{x}(x) \\left(\\int_{D_{y}}g(x,y)p_{y|x}(y)dy\\right)dx where  is the joint density of ,  is the marginal density of , and  is the conditional density of  given .  denote the support of , respectively and . After noting that p_{xy}(x,y) = p_{y|x}(y)p_{x}(x),  the identity becomes pretty clear. (3) In the integral identity I wrote above, this refers to re-writing the bivariate integral (after you've replaced  with ) as separate integrals over  and , as on the right hand side. (4) Review the properties of expectations and conditional/joint densities. Any intermediate text on statistical theory would have this information (e.g. Statistical Inference by Casella and Berger). You may also want to review some of the basic calculus associated with these topics (since expectations are defined as integrals). ","Display_name":"Macro","Creater_id":4856,"Start_date":"2012-01-14 00:05:28","Question_id":21086}
{"_id":{"$oid":"5837a580a05283111e4d5708"},"Last_activity":"2016-08-12 05:09:28","Creator_reputation":308,"Question_score":0,"Answer_content":"stackexchange is possibly not the best place to answer this question which is more an \"open\" discussion question rather than something with a \"closed\" solution.My 50 cents:In reality, modelling the distribution of race results means modelling  where  represents the set of competitors (contestants). I think that modelling the distribution of  where  is the time is possibly more \"stable\". I suspect that the distribution of  varies a lot and flattens out (tends to become a uniform distribution) as the \"gap\" of the distributions of  across contestants declines or the \"overlap\" of the distributions increases (if normal you can check out this). I suspect that, as the competition progresses towards the finals, the overlap increases as the contestants become faster and more consistent.Not sure about literature about  but Monte Carlo simulations could do the job.","Display_name":"IcannotFixThis","Creater_id":26474,"Start_date":"2016-08-12 05:09:28","Question_id":229490}
{"_id":{"$oid":"5837a580a05283111e4d5714"},"Last_activity":"2016-08-12 05:08:27","Creator_reputation":11,"Question_score":0,"Answer_content":"The covariance matrix does not have such a nice expression in general:https://en.wikipedia.org/wiki/Covariance_matrix#DefinitionNote also that there are no unique bivariate versions of the distributions you mentioned (exponential, chi-squared ...).You might also be interested in looking at Copulas:https://en.wikipedia.org/wiki/Copula_(probability_theory)The bivariate Gaussian copula contains a parameter that quantifies the dependence between the marginals.","Display_name":"Palumbo","Creater_id":127478,"Start_date":"2016-08-12 05:08:27","Question_id":229451}
{"_id":{"$oid":"5837a580a05283111e4d5720"},"Last_activity":"2016-08-12 05:03:57","Creator_reputation":11,"Question_score":1,"Answer_content":"The Laplace distribution is also related to median linear regression models. For a model:y_i=x_i^T\\beta + \\epsilon_i,where  are iid Laplace with location  and scale , the maximum likelihood estimators of  coincide with the median regression estimators\\hat{\\beta} =\\text{argmin} \\sum_i\\vert y_i-x_i^t\\beta\\vert.See: https://en.wikipedia.org/wiki/Quantile_regressionThe half-Cauchy prior is very popular in Bayesian hierarchical models:  Nicholas G. Polson and James G. Scott (2012). On the Half-Cauchy Prior for a Global Scale Parameter. Bayesian Analysis.","Display_name":"Palumbo","Creater_id":127478,"Start_date":"2016-08-12 05:03:57","Question_id":229474}
{"_id":{"$oid":"5837a580a05283111e4d5721"},"Last_activity":"2016-08-12 00:59:15","Creator_reputation":25400,"Question_score":4,"Answer_content":"One example is using them as robust priors for regression parameters, where Laplace prior corresponds to LASSO (Tibshirani, 1996) , but -distribution, or Cauchy are other alternatives (Gelman et al, 2008).Moreover, you can have L1 regularized regression with Laplace errors (i.e. minimizing absolute error).Another example: Laplace noise is used in currently trendy field of differential-privacy.Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 267-288.Gelman, A., Jakulin, A., Pittau, G.M., and Su, Y.-S. (2008). A weakly informative default prior distribution for logistic and other regression models. The Annals of Applied Statistics, 2(4), 1360-1383.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-12 00:59:15","Question_id":229474}
{"_id":{"$oid":"5837a580a05283111e4d572e"},"Last_activity":"2016-08-12 05:03:42","Creator_reputation":6,"Question_score":0,"Answer_content":"Depends on where you NULLs are coming from.If NULL means that no previous contact was made, it makes sense to assign it a value like 1000, which assumes that in the three years that you have been tracking, no contact was made. You can decide this value as suits your case.In Case NULL means data is unavailable, I'll suggest replacing NULLs with the average 'previous_contact' value.","Display_name":"absk","Creater_id":117080,"Start_date":"2016-08-12 05:03:42","Question_id":228622}
{"_id":{"$oid":"5837a580a05283111e4d572f"},"Last_activity":"2016-08-07 05:48:49","Creator_reputation":3619,"Question_score":0,"Answer_content":"Code the missing values as zero and construct a new predictor which is one if the value is missing and zero otherwise. Then make sure you always include them both together in the model and test them together.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-07 05:48:49","Question_id":228622}
{"_id":{"$oid":"5837a580a05283111e4d573e"},"Last_activity":"2016-08-12 04:59:11","Creator_reputation":51,"Question_score":0,"Answer_content":"Besides all the answers above: It is possible to calculate an exact chi2 permutation test for 2x2 and rxc tables.Instead of comparing our observed value of the chi-square statistic to an asymptotic chi-square distribution we need to compare it to the exact permutation distribution. We need to permute our data in all possible ways keeping the row and column margins constant. For each permuted dataed set we caluclated the chi2 statistics . We then compare our observed chi2 with the (sorted) chi2 statistics The ranking of the real test statistic among the permuted chi2 teststatistics gives a p-value. ","Display_name":"Stats_Monkey","Creater_id":53159,"Start_date":"2016-08-11 08:11:21","Question_id":211710}
{"_id":{"$oid":"5837a580a05283111e4d573f"},"Last_activity":"2016-05-10 07:33:19","Creator_reputation":2442,"Question_score":7,"Answer_content":"Almost any approach that does some form of model selection and then does further analyses as if no model selection had previously happened typically has poor proporties. Unless there are compelling theoretical arguments backed up by evidence from e.g. extensive simulation studies for realistic sample sizes and feature versus sample size ratios to show that this is an exception, it is likely that such an approach will have unsatisfactory properties. I am not aware of any such positive evidence for this approach, but perhaps someone else is. Given that there are reasonable alternatives that achieve all desired goals (e.g. the elastic net), it this approach is hard to justify using such a suspect ad-hoc approach instead.","Display_name":"Bj\u0026#246;rn","Creater_id":86652,"Start_date":"2016-05-10 07:33:19","Question_id":211710}
{"_id":{"$oid":"5837a580a05283111e4d574c"},"Last_activity":"2016-08-12 04:46:23","Creator_reputation":3354,"Question_score":3,"Answer_content":"When you represent all the scatter plots, you only want to plot the pairs of different variables. And:\\binom{p}{2}=\\frac{p(p-1)}2As @NickCox stressed, the plot of  versus  contains the same points as that of  versus  (up to exchange of axes). ","Display_name":"RUser4512","Creater_id":73794,"Start_date":"2016-08-12 03:11:17","Question_id":229494}
{"_id":{"$oid":"5837a580a05283111e4d574d"},"Last_activity":"2016-08-12 03:12:04","Creator_reputation":103,"Question_score":0,"Answer_content":"When you have p variables and would like to have scatter plots for 2 variables each then number of combinations come upto p combination 2(Pc2 # P subscript C subscript 2) by expanding the formula you get p(p-1)/2sorry i don't know to add subscripts here so i wrote subscript","Display_name":"alily","Creater_id":90391,"Start_date":"2016-08-12 03:12:04","Question_id":229494}
{"_id":{"$oid":"5837a580a05283111e4d575a"},"Last_activity":"2016-08-12 04:34:21","Creator_reputation":550,"Question_score":2,"Answer_content":"No, I don't see why this is a bad idea. It seems to me that it is a natural (and interesting) extension to draw samples from a CDF.However, I believe that the acceptance should be  \\min\\left(  \\frac{F(Y+\\varepsilon) - F(Y)}{F(x^{(t)}+\\varepsilon) - F(x^{(t)})} , 1 \\right) because by definition \\lim_{\\varepsilon\\rightarrow 0}\\frac{F(x + \\varepsilon) - F(x)}{\\varepsilon} = PDF(x)Nevertheless, it is the first time I see this. With a strong case for sampling from a non-trivial CDF, this could become an interesting publication.","Display_name":"J. C. Leit\u0026#227;o","Creater_id":12100,"Start_date":"2016-08-12 04:28:22","Question_id":225389}
{"_id":{"$oid":"5837a580a05283111e4d576b"},"Last_activity":"2016-08-03 10:33:42","Creator_reputation":338,"Question_score":1,"Answer_content":"Not really, no. Take a look at this picture:In this case, we have two dimensions. Let's say we reduce it to 1 (just ). What will happen is that all the points will be projected onto  so many points (an infinite number, in fact) will map to the same point on . The same will happen with any dimensionality reduction via SVD. ","Display_name":"roundsquare","Creater_id":122754,"Start_date":"2016-08-03 10:33:42","Question_id":227104}
{"_id":{"$oid":"5837a580a05283111e4d5778"},"Last_activity":"2016-08-12 03:59:25","Creator_reputation":28606,"Question_score":3,"Answer_content":"\"By whatever sensible criteria\" is itself a sensible if casual remark, and the nub of the matter. Here my criterion, or rather attitude, is that we usually have broad ideas about a relationship but need to be sensitive to what the data can tell us. That implies above all not throwing away information unless we are certain that it is really noise. We also need to be mindful that other predictors lurk, even if they are not within our dataset. Age is in practice often already categorised when reported in completed years. Even if exceptionally you know people's birthdays and have fine reporting of age it's not at all obvious that age should be further categorised for investigating its role as a predictor. Income is well known to be typically skewed, so much so that working with log income is almost a default procedure in various fields. For exploration I would tend to look at geometric mean or median of income for each reported age. The geometric mean naturally corresponds to the mean of the logarithms. I doubt that the relationship between age and income is \"not continuous\". There might be kinks in the curve corresponding to key stages in an education system, e.g. those who stay the full course in high school and leave at 18, those who do a first degree and leave at 21 or 22, and so forth. If you see these effects, some kind of spline approach with key ages as specified knots may be appropriate. In practice, we all know that people can move faster or slower through any system, people can go \"back to school\" in later life, etc., all of which would fuzz out any effects of key ages. But binning ages will at best muddy any underlying relationship. It's throwing away information. In some fields, there is a widespread habit of quantile binning, e.g. using deciles to divide firms into the least successful 10%, and so on, up to the most successful 10%. I don't fully understand the motive for this as compared with smoothing approaches, but I note that with a variable like age, it is usually impossible to get exactly or even roughly equal frequencies, given ties in the data. NOTE: Age within a year (i.e. time of year when someone was born) is sometimes argued to have an effect on people's careers. In many countries schooling starts once a child is old enough but only once a year, so with a new school year  those children who passed a certain birthday within the last 12 months start school. Then children in the same class vary with age, with some almost a year older than others (and physically bigger and stronger, perhaps more confident, etc.). So even the fine detail could have an effect, at least on average. You might need a very large dataset to detect this effect, but that's not to say it doesn't exist.  ","Display_name":"Nick Cox","Creater_id":22047,"Start_date":"2016-08-12 01:38:34","Question_id":229476}
{"_id":{"$oid":"5837a580a05283111e4d578b"},"Last_activity":"2016-08-12 03:34:36","Creator_reputation":3354,"Question_score":5,"Answer_content":"If  and  are uncorrelated,  and  certainly are correlated. Therefore, you cannot write:Instead:Which gives, after factorization :","Display_name":"RUser4512","Creater_id":73794,"Start_date":"2016-08-12 03:34:36","Question_id":229502}
{"_id":{"$oid":"5837a580a05283111e4d5798"},"Last_activity":"2016-08-12 03:25:13","Creator_reputation":129,"Question_score":0,"Answer_content":"In the next version of mlr that will come out on monday, there will be many multilabel algorithms available in R. For learning how to use it, you can use the tutorial: http://mlr-org.github.io/mlr-tutorial/devel/html/multilabel/index.htmlYou can install the development version, where all multilabel algorithms learner are already available now with following code:install.packages(\"devtools\")devtools::install_github(\"mlr-org/mlr\") If you have more questions you can ask me, I am a developer of mlr. ","Display_name":"PhilippPro","Creater_id":96979,"Start_date":"2016-08-12 03:25:13","Question_id":161552}
{"_id":{"$oid":"5837a580a05283111e4d5799"},"Last_activity":"2015-07-16 13:43:33","Creator_reputation":19594,"Question_score":1,"Answer_content":"Predict each label independently.Because objects may have more than one label.Thus, this isn't really a multi-class problem, because the classes aren't disjoint.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2015-07-16 13:43:33","Question_id":161552}
{"_id":{"$oid":"5837a580a05283111e4d579a"},"Last_activity":"2015-07-15 00:56:13","Creator_reputation":971,"Question_score":0,"Answer_content":"I would suggest MP Boost link. It is based on AdaBoost.MH and effectively does pivot sample selection in each iteration.In my application I needed a multi label classification capability. Unfortunately it was not directly applicable since there is no easy way to forbid/allow certain combinations of labels.The software is implemented in C++ and requires little dependencies.","Display_name":"xeon","Creater_id":27961,"Start_date":"2015-07-15 00:56:13","Question_id":161552}
{"_id":{"$oid":"5837a580a05283111e4d57a7"},"Last_activity":"2016-08-12 02:32:05","Creator_reputation":12982,"Question_score":0,"Answer_content":"Autocorrelation-step-ahead forecast errors for  are autocorrelated by construction, because the forecasts cover overlapping time periods. This holds even for \"optimal\" forecasts. Meanwhile, 1-step-ahead forecast errors need not be autocorrelated, at least for \"optimal\" forecasts. However, they still might be autocorrelated for \"suboptimal\" forecasts. E.g. if the forecast for  is just the actual value as of  for some , you effectively get the same overlap and thus autocorrelation in forecast errors as with -step-ahead forecasts. Of course, this need not be a sensible forecast, but it serves as an example.Using autocorrelation-robust standard errors for 1-step-ahead forecasts in the Diebold-Mariano test is thus a conservative, \"safe\" choice.HeteroskedasticityRegarding heteroskedasticity, neither Diebold \u0026amp; Mariano (1995) nor Diebold (2015) mentions it, so probably there is nothing special about it in the context of the Diebold-Mariano test. If there is reason to believe that the forecast error distribution had a varying variance in your sample, you could use the heteroskedasticity-robust standard errors.References:Diebold, Francis X., and Roberto S. Mariano. \"Comparing Predictive Accuracy.\" Journal of Business \u0026amp; Economic Statistics 13.3 (1995): 253-263.Diebold, Francis X. \"Comparing predictive accuracy, twenty years later: A personal perspective on the use and abuse of Diebold–Mariano tests.\" Journal of Business \u0026amp; Economic Statistics 33.1 (2015): 1-1.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-12 02:32:05","Question_id":229483}
{"_id":{"$oid":"5837a580a05283111e4d57b4"},"Last_activity":"2016-08-12 02:18:56","Creator_reputation":152613,"Question_score":3,"Answer_content":"Statistical significance doesn't mean what you think it does.Firstly, it is not any guarantee that it's \"not due to chance\". If  is true the probability of observing a statistic at least as extreme as the one you had is low, but clearly possible (how low depends on your choice of significance level).When a test statistic is unusual (in the sense of being in a part of the distribution under the null - generally a tail - that's most highly consistent with the alternative)  we're left with two competing explanations. Either the null is true and something quite rare happened the null isn't true (and so there's no need to invoke a rare fluke as an explanation)If we reject  what we're really saying is that the difference was large enough that we could reasonably conclude it wasn't exactly 0.  But isn’t saying that a difference is real is also implying that the difference is large enough to be real?This is the fallacy of equivocation -- you used the word \"real\" in two different senses. Let's avoid the word \"real\" and see what you're saying:  But isn’t saying that we can detect that the difference is not zero also implying that the difference is large enough to be important?The answer to that question is hopefully now clear - no.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-12 02:18:56","Question_id":229466}
{"_id":{"$oid":"5837a580a05283111e4d57b5"},"Last_activity":"2016-08-12 01:37:39","Creator_reputation":46,"Question_score":3,"Answer_content":"I suppose the gist of this has already been answered in the comments, and I understand why the commentators have (perhaps impatiently) referred to previous answers.  But as a newbie I have a little more patience, so let me elaborate a little.A. Statistical significance1. A significant result does not indicate that the result is real. It simply indicates that if the null hypothesis were true (if there were no effect in the population, or to put it another way, if the two groups were drawn from the same population in terms of the thing you were measuring), you would have seen a result this impressive 5% of the time or less.2.In more detail, suppose the null hypothesis were true.  You take samples (let's say 20 people each) from the same population and measure them on some attribute, e.g. fine motor skills.  Because everyone is different, the means in the two samples would not be the same as each other (or as the mean in the population).  This is called sampling error. (I know that other people will read this, so I will acknowledge that sampling error has a slightly more exact definition which I don't think matters here.)3.Statistical tests (certainly for the difference between two means) use various clever maths to work out what would happen if we repeated the study time and time again, and hence what would be an unusual size of the difference between the means.4.\"Unusual\" is defined in terms of how often a result that impressive (in the case of a difference in means, that big or larger) would happen if the null hypothesis were true.  This is called the \"alpha level\" and is usually set at 5%, which is what I have presumed in the above explanation.5. Even then, 5% is not that impressive.  If you did 20 statistical tests where there was no effect and with a 5% alpha level, you would expect 1 of them to be significant.  (That doesn't mean that exactly one will be significant; the chance of none of them being significant is a slightly more complicated calculation which I don't think we need to bother with.)6. Even then, this is the percentage chance of getting the result if the null hypothesis were true. Contrary to what you might think, that still does not make it the chance that the null hypothesis is untrue.7.  I heartily recommend Cohen's classic 1994 paper \"The earth is round (p\u0026lt;.05)\" to take the issue further.  I think Cohen was even more fed up than some of the people commenting on this question.B. Is the result \"large\" enough?1. The chance of getting a significant result, if the effect is real, is called \"power\".  This depends on various things, including the size of the effect and the sample size.2. If you have a large sample size, you could easily get a significant result even if the effect is very small.  For example if I test a new headache pill against aspirin, and people taking the new headache pill get 1% fewer headaches, I might get a quite correctly statistically significant result.  But this is not a difference that patients are likely to notice.  It would be statistically significant, but of clinical or practical significance.","Display_name":"Mike","Creater_id":127452,"Start_date":"2016-08-12 01:37:39","Question_id":229466}
{"_id":{"$oid":"5837a580a05283111e4d57c4"},"Last_activity":"2016-08-12 02:11:46","Creator_reputation":152613,"Question_score":1,"Answer_content":"It may be a conflict-of-terminology issue. Cross-product is not an unambiguous term (e.g. see here which includes a number of different meanings - yet doesn't seem to include the intended sense). They don't mean the vector-cross-product .In the sense they meant it,  is a cross product. This is common in statse.g. see here or here or hereYou also often see  called a cross-product*. Consequently   might be called either a \"mean cross product\" (in the first sense) or say a \"normalized cross product\" (in the second sense). * Compare with say the R function crossprod which means .","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-12 02:06:41","Question_id":229478}
{"_id":{"$oid":"5837a580a05283111e4d57d7"},"Last_activity":"2016-08-12 00:18:43","Creator_reputation":71,"Question_score":7,"Answer_content":"No.For any i.i.d.  and  the distribution of their difference is invariant under sign-change, , and thus symmetric around zero, something  is not.","Display_name":"J. Virta","Creater_id":107445,"Start_date":"2016-08-12 00:18:43","Question_id":229335}
{"_id":{"$oid":"5837a580a05283111e4d57d8"},"Last_activity":"2016-08-11 05:43:44","Creator_reputation":5797,"Question_score":17,"Answer_content":"No.If  is ever (with positive probability) , then , so it can't be . If  and  are iid,  can not be guaranteed (i.e., with probability ) to not be  unless  and  are both the same constants with probability 1. In such case  will equal  with probability . Therefore, there exists no iid  and  such that  is .","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-11 05:43:44","Question_id":229335}
{"_id":{"$oid":"5837a580a05283111e4d57e7"},"Last_activity":"2012-11-30 20:22:57","Creator_reputation":321,"Question_score":2,"Answer_content":"The weights  are not a function of . So when computing derivatives, you should treat them as a constant.In particular, the partials w.r.t  looks like:\\frac{\\partial{L}}{\\partial{w_{k}}} = \\sum_{i=1}^{n}r_i y_{i}x_{ik}g(-y_{i}z_{i})\\frac{\\partial^{2}{L}}{\\partial{w_{j}}\\partial{w_{k}}} = -\\sum_{i=1}^{n}r_i x_{ij}x_{ik}g(y_{i}z_{i})g(-y_{i}z_{i})If it helps you conceptually, you can think about problems with weighted samples as equivalent to unweighted problems, but where some particular observation  appears  times rather than only once.","Display_name":"Dapz","Creater_id":17214,"Start_date":"2012-11-30 20:22:57","Question_id":44776}
{"_id":{"$oid":"5837a580a05283111e4d57f4"},"Last_activity":"2016-08-11 21:45:11","Creator_reputation":131,"Question_score":0,"Answer_content":"One must be a bit careful in curve-fitting. One of my professors once commented during a presentation (by someone else) that had a high-order curve fit: \"With five free parameters, one can curve-fit an elephant. With eight, one can fit a running elephant.\"Somewhere between a least squares line (or the L1 or L_inf equivalent) and a full Lagrange interpolation polynomial should lie a useful curve. Sometimes the form or even some of the parameters in a curve fit will be obvious from the conditions of the problem. This decision ends up being what I like to call a \"physics\" or \"engineering\" or \"real world\" decision, not so much a mathematical decision. How well a curve fits the data can be given a mathematical description; the meaning of such a fit cannot.A method I have used in such cases is to separate the data into two parts; use one part to fit a curve; then use the left over so see how good the fit is. Another way that may be better is to bootstrap the system. With N data points (assuming just an X and a Y for each point), one draws (with replacement) N points from the data set and the computes the proposed fit. This is done some number of times. When a sample of fits is obtained, the variance of this sample is an indication of how well the fit fits the data. (fit fits?) A smaller variance can indicate that one type of fit is better than another but not whether such a fit is meaningful.","Display_name":"ttw","Creater_id":127436,"Start_date":"2016-08-11 21:45:11","Question_id":56767}
{"_id":{"$oid":"5837a580a05283111e4d57f5"},"Last_activity":"2013-04-21 10:47:09","Creator_reputation":57712,"Question_score":5,"Answer_content":"There is not enough information here for a definitive answer, but two possibilities are:1) Adding polynomial terms of X (e.g. )2) Using some sort of spline curve (e.g. restricted cubic splines) of x.The choice would depend on a few things:1) How complex is the curve? Very complex relationships are better served by splines.2) How \"explicable\" does the model have to be? It is generally easier for people to understand polynomial terms (at least, if you don't go beyond ) than splines.I would use a log transform only if the log transform makes substantive sense. Such a transform changes additive operations to multiplicative ones. That is, if the IV is now log(x), then it implies that (say) doubling x will have a uniform effect on y, rather than (say) adding 10 to x having a uniform effect on y. Log transforms often make sense for amounts, in particular monetary amounts. For example, it seems reasonable that doubling income ought to have a constant effect. One case where splines have been well used (as an example) is the relationship between age and height over the human lifespan: Height rises quickly, then slows, then speeds up, then gradually slows to a plateau and then (much later) declines slowly. That would be very hard to model with a polynomial!","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2013-04-21 10:47:09","Question_id":56767}
{"_id":{"$oid":"5837a580a05283111e4d5804"},"Last_activity":"2015-05-04 21:48:28","Creator_reputation":1956,"Question_score":3,"Answer_content":"A/B testing seems to be computer geeks terminology, but the idea is of course same. You have an control version of web-page and changed one and you test if difference between some user action rate is statistically significant between versions of pages.  A/B testing tests single feature combination differences when multivariate testing tests different combinations of their interactions. ","Display_name":"Analyst","Creater_id":28732,"Start_date":"2015-05-04 21:48:28","Question_id":149774}
{"_id":{"$oid":"5837a580a05283111e4d5813"},"Last_activity":"2016-08-11 20:27:39","Creator_reputation":1068,"Question_score":1,"Answer_content":"Cohen's kappa coefficient is a chance-adjusted index of agreement. It estimates the likelihood of chance agreement using Bayes' Rule: it assumes that the likelihood of raters randomly assigning an item to the same category is based on the product of raters' individual distributions for each category. In your case, the \"raters\" are the trusted labels and the classification predictions. By using downsampling, you are artificially modifying one of the rater's (i.e., the trusted labels') distributions. Unfortunately, this undermines the logic of the coefficient. So I would base my decision on other metrics (AUC or just accuracy) and use random or no downsampling.","Display_name":"Jeffrey Girard","Creater_id":111380,"Start_date":"2016-08-11 20:27:39","Question_id":228406}
{"_id":{"$oid":"5837a580a05283111e4d581f"},"Last_activity":"2016-08-11 19:02:12","Creator_reputation":26,"Question_score":2,"Answer_content":"I was trying to prove the same and struggled a bit but finally got to a solution. I know this was asked long back but there might be other people interested.Cov (\\bar{Y}, \\hat{\\beta_1}) = E[(\\bar{Y} - E(\\bar{Y}))(\\hat{\\beta}_1 - \\beta_1)]\\\\= E[(\\beta_0 + \\bar{X}\\beta_1 + \\bar{\\epsilon} - \\beta_0 - \\bar{X} \\beta_1) (\\hat{\\beta}_1 - \\beta_1)]\\\\= E(\\bar{\\epsilon} (\\hat{\\beta}_1 - \\beta_1)]\\\\= E(\\bar{\\epsilon} \\hat{\\beta}_1) - \\beta_1 E(\\bar{\\epsilon})\\\\= E(\\bar{\\epsilon} \\hat{\\beta}_1) Now we know that  \\hat{\\beta}_1 = \\sum_{i=1}^n \\dfrac{(X_i - \\bar{X})Y_i}{\\sum(X_i - \\bar{X})^2} which we can write as  \\hat{\\beta}_1 = \\sum_{i=1}^n c_i Y_i for convenience. Now consider, Cov(\\bar{\\epsilon}, \\hat{\\beta}_1) = E(\\bar{\\epsilon} \\hat{\\beta}_1)\\\\= Cov(\\sum c_i \\epsilon_i , \\bar{\\epsilon})\\\\= \\sum_{i=1}^n \\sum_{j=1}^n c_i\\times \\dfrac{1}{n} Cov(\\epsilon_i, \\epsilon_j)  \\\\ \\text{by independence we get,}\\\\= \\sum_{i=1}^n \\dfrac{c_i}{n} \\sigma^2\\\\= \\dfrac{\\sigma^2}{n}\\sum_i c_i  = 0This is becausec_i = \\dfrac{(X_i - \\bar{X})}{\\sum_{j=1}^n (X_j - \\bar{X})^2}\\\\\\sum_{i=1}^n c_i = \\dfrac{1}{\\sum_{j=1}^n (X_j - \\bar{X})^2} \\sum_{i=1}^n (X_i - \\bar{X})\\\\=\\dfrac{1}{\\sum_{j=1}^n (X_j - \\bar{X})^2} (n\\bar{X} - n\\bar{X}) = 0 ","Display_name":"CommTask","Creater_id":111092,"Start_date":"2016-08-11 09:49:38","Question_id":92964}
{"_id":{"$oid":"5837a580a05283111e4d5820"},"Last_activity":"2016-02-15 01:51:31","Creator_reputation":39,"Question_score":0,"Answer_content":"{\\rm Cov}\\bigg[\\frac 1 n \\sum y_i,\\ \\frac{\\sum(x_i - \\bar x)y_i}{\\sum(x_i-\\bar x)^2}\\bigg] = \\frac 1 n \\times \\frac{\\sum(x_i-\\bar x)\\sigma^2}{\\sum(x_i - \\bar x)^2} = 1 it does not equal zero this way I believe","Display_name":"user2350622","Creater_id":43273,"Start_date":"2014-04-20 20:07:46","Question_id":92964}
{"_id":{"$oid":"5837a580a05283111e4d582d"},"Last_activity":"2011-11-03 20:57:42","Creator_reputation":186,"Question_score":1,"Answer_content":"As it turns out, the code above does work.  If you remove the hash tags in the middle portion of the code, you have a simulation-based logit estimator (producing intercepts, slopes, the number of iterations until converging on the true value, and an output saying \"Done\").  The estimator adjusts the standard deviation after a given number of correct predictions.  The variable \"ti\" provides the number of iterations the logit estimator runs until the estimator converges on the true value.  To manipulate the shock (standard deviation), adjust the value of the variable \"f\".  Two examples are below:nb0\u0026lt;-b0 + rnorm(1,0,1*f)nb1\u0026lt;- b1+ rnorm(1,0,1*f)nb0\u0026lt;-b0 + rnorm(1,0,1/f)nb1\u0026lt;- b1+ rnorm(1,0,1/f)To further clarify, below is code that plots the estimated slopes and intercepts.  You'll notice that the estimates will be less accurate at the beginning, but will then converge toward the true value as shock decreases after a given number of iterations plot(1:n,bin1,ylim=c(-10,0), ylab=\"\", main=\"intercepts\") plot(1:n,bin2,ylim=c(-2,2), ylab=\"\", main=\"slopes\")Enjoy!","Display_name":"Captain Murphy","Creater_id":7188,"Start_date":"2011-11-03 20:57:42","Question_id":17864}
{"_id":{"$oid":"5837a580a05283111e4d583a"},"Last_activity":"2016-08-11 18:03:41","Creator_reputation":147548,"Question_score":14,"Answer_content":"Statistics, by Freedman, Pisani, \u0026amp; Purves, originated from a popular and successful course taught at U.C. Berkeley.  I have used it as an intro stats text for undergraduates, have borrowed some of its ideas when teaching graduate stats courses, and have given away many copies to colleagues and clients.  There are many reasons for its popularity:Its narrative and its problems are driven by real case studies and actual data of obvious importance, rather than the made-up drivel found in so many texts.  These are truly interesting and memorable, including the Salk polio vaccine trials, the 1936 Literary Digest poll debacle, the Berkeley graduate student discrimination lawsuit (hinging on Simpson's Paradox), Fisher's criticism of Mendel's pea results, and much more.It has extensive problems at three levels: at the end of each chapter subsection (of which there are hundreds), at the end of each chapter (over 30), and at the ends of major groups of chapters (about 4, I recall).  These problems require minimal or no mathematics: they focus on potential misunderstandings that the authors, in their extensive experience, have found to arise among students.It focuses on statistical ideas and reasoning rather than mathematics.It uses (almost) no mathematical formulas.  Quantitative relationships are usually expressed graphically and in words.  (They are so clearly conveyed that when I first read this book, as a math graduate student entirely ignorant of statistics, I was able to reproduce all the underlying mathematical theory with no trouble.)It covers most of the traditional material, including the Binomial and Normal distributions, confidence intervals, z tests, t tests, chi squared tests, regression, and the minimum amount of probability and combinatorics needed to understand these.Some potential drawbacks would include:No treatment of Bayesian statistics.  This will make this book outmoded within a decade.No treatment of ANOVA (psychology students might miss this the most).No discussion of computing.I believe the latter two are not critical: a good instructor can easily supply the ANOVA material and can teach as much or little computing as they might wish.  Whether the omission of Bayesian statistics is important will depend on the instructor's tastes and aims.Finally, I should note that although the mathematical demands are as small as one could possibly imagine, my pre- and post-testing of students indicates that people who come to the book with a disposition and habit of thinking quantitatively still get much more out of it than those who do not.  Most of my students performed badly on pretests of mathematical knowledge (90% got failing grades), but those who also performed badly on pretests of critical thinking (Shane Frederick's Cognitive Reflection Test) exhibited markedly less improvement during the semester than others did.  The pre and post tests both included the full 40-item CAOS test of fundamental concepts any introductory college-level stats course ought to include.  The students in this class have consistently exhibited twice as much improvement as that reported in the CAOS literature; the students with poor cognitive reflection scores improved only an average amount (or failed to complete the course).  I haven't the data to assign causes to this extra improvement, but suspect the textbook deserves at least some of the credit.","Display_name":"whuber","Creater_id":919,"Start_date":"2013-01-23 09:41:27","Question_id":48347}
{"_id":{"$oid":"5837a580a05283111e4d583b"},"Last_activity":"2016-08-11 16:29:35","Creator_reputation":108,"Question_score":5,"Answer_content":"I read Freedman (almost the entire book) and OpenIntro Statistics (more than a third). Both of these books are quite good.I eventually found the book that came close to what I was looking for: Learning Statistics with R: A tutorial for psychology students and other beginners by Daniel Navarro. It is freely available online (legally) and you can also order a print version for about US $30 (see the book page for details).The main pros of this book are:R implementations embedded in text as topics are introduced. R has built-in functions for most of the methodsexplained in the book. Where R doesn't have a built-in, the authorhas written his own function for it and made it available on CRANunder his lsr library, so your learning is quite complete. Ipersonally found this to be the biggest plus point of this book.The book is more comprehensive than Freedman and OpenIntro. Alongwith the basics, it covers topics like Shapiro-Wilk test, Wilcoxon test,Spearman correlation, trimmed means and a chapter on Bayesian statistics, to name a few.The motivation behind each topic is explained clearly. There is also a good amount of history behind the topics, so you get to appreciate how a method was arrived at.The book was written iteratively with feedback from readers and I believe   the author is still improving upon the book.The only drawback is that the hard copy version is large and heavy!","Display_name":"arun","Creater_id":108868,"Start_date":"2016-05-01 08:37:27","Question_id":48347}
{"_id":{"$oid":"5837a580a05283111e4d583c"},"Last_activity":"2016-05-01 19:46:16","Creator_reputation":2040,"Question_score":1,"Answer_content":"I have been a TA, observer, or student in a lot of courses involving quantitative methods for psychology, with SPSS as the main program. In all cases it has seemed to me that students have gravitated towards Field (2013), irrespective of whether the course coordinator has mentioned this book or not. In numerous cases students have ignored a recommended textbook and read Field's textbook instead. I'm not properly competent to assess the rigour of the explanations in the book, and nor am I aware of any research on learning outcomes. However, I can say that this book is comprehensive, cheap (where I'm from anyway), and popular with students. The author's writing style relies a great deal on personal anecdotes, which will grate with some readers. However, I've found that at least as many students enjoy it. I seemed to run into a lot of typos and other issues in the early editions, but by the fourth edition most of these seem to be weeded out. So, Field (2013) is my recommendation, since:  I've seen psychology students engage with it and enjoy reading it.Even if you recommend another book, it's quite likely that some students will use Field (2013) anyway. This can then create administration issues within your course.The book is popular and the author is still relatively young, so it's likely that there will be further editions and improvements.If you later decide that you want to use R instead, you can transition pretty seamlessly to Field, Miles, and Field (2012), which uses most of the same examples. @jeremy-miles is a frequent contributor to this site.Field, A. (2013). Discovering statistics using IBM SPSS statistics. Sage.Field, A., Miles, J., \u0026amp; Field, Z (2012). Discovering Statistics Using R. Sage.","Display_name":"user1205901","Creater_id":9162,"Start_date":"2016-05-01 19:46:16","Question_id":48347}
{"_id":{"$oid":"5837a580a05283111e4d583d"},"Last_activity":"2014-09-15 10:40:37","Creator_reputation":1,"Question_score":3,"Answer_content":"Check out the introductory statistics book, Making Sense of Data through Statistics: An Introduction (2014) by Dorit Nevo.  It is written in an extremely accessible manner and is meant for undergraduate or graduate students in business and in the social sciences.  The textbook makes use of examples meaningful to today’s students and is accompanied with Excel worksheets providing hands-on experience that reinforces the statistical concepts and techniques covered.  Instructors  are provided with supplementary teaching materials, including PPT lecture slides for each chapter, a Solutions Manual for all Unit Exercises and End-of-Chapter Practice sets, and a Test Bank.  The book is sold in digital format only (.pdf), allowing for the very reasonable price of $19.95.  Educators may register for free access to the book and teaching materials by signing up at the Legerity Digital Press Educator Preview portal. ","Display_name":"Jana Williams","Creater_id":55841,"Start_date":"2014-09-15 10:40:37","Question_id":48347}
{"_id":{"$oid":"5837a580a05283111e4d583e"},"Last_activity":"2014-02-07 17:52:43","Creator_reputation":16,"Question_score":4,"Answer_content":"Statistics Unplugged is a great book for introductory statistics. The author first introduces the logic of the statistical test and later gives the mathematical formula. This approach helps in digesting the new concepts. There are several examples throughout the book which are presented in the form of a problem required to be solved rather than a hypothetical statement and mathematical steps.","Display_name":"umair","Creater_id":39718,"Start_date":"2014-02-07 17:22:27","Question_id":48347}
{"_id":{"$oid":"5837a580a05283111e4d583f"},"Last_activity":"2013-01-23 09:24:16","Creator_reputation":57712,"Question_score":3,"Answer_content":"How about The Statistical Sleuth by Ramsey and Schafer?I think this book gets at some important points without either a) Too much math or b) dumbing things down.I would suggest that an intro stats course for psychology and other social science types should emphasize how not to go wrong too much. A survey of methods would also be a good thing for undergrads to get. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2013-01-23 08:36:20","Question_id":48347}
{"_id":{"$oid":"5837a580a05283111e4d5840"},"Last_activity":"2013-01-23 08:40:38","Creator_reputation":529,"Question_score":2,"Answer_content":"Here is a list of books. Puzzles/riddles are a great way to instil an interest in what mathematics/statistics can do. Real life examples help too.","Display_name":"broccoli","Creater_id":13516,"Start_date":"2013-01-23 08:40:38","Question_id":48347}
{"_id":{"$oid":"5837a580a05283111e4d5841"},"Last_activity":"2013-01-23 08:03:44","Creator_reputation":21588,"Question_score":4,"Answer_content":"Thom Baguley, an outgoing editor of The British Journal of Mathematical and Statistical Psychology, published Serious Stats book that you could find useful. It relies on R rather than SPSS, though.I am suspicious of the books that are in their 7th edition. In my teaching experience, it means that the sections and problems were reshuffled so that the students would have to buy the latest edition to generate the cash flow for the publisher and royalties for the authors keep up with the course. Few serious, research level monographs have undergone a second edition by their authors, and any higher number is obviously an outlier. (Kendall's Library of Statistics is a notable exception, but I cannot really think of any other book that I know that would be in its third edition.)In my very strong opinion, Excel is a good tool for statistical analysis only when used by a Ph.D. statistician. Teaching undergraduate statistics with it will likely have disastrous consequences, and teaches little statistics as compared to using a modern package like R or Stata. Just try to produce a standardized residual vs. leverage regression plot in Excel, and compare it to one-liners in these packages. Stat majors would need to know the theory, so they would need to build these plots from scratch, but still using a statistical package rather than copy/paste the formulae around in Excel. Non-major undergrads need to get the feel for data analysis, and Excel obscures it, at best.","Display_name":"StasK","Creater_id":5739,"Start_date":"2013-01-23 07:57:12","Question_id":48347}
{"_id":{"$oid":"5837a580a05283111e4d5850"},"Last_activity":"2016-08-11 17:14:44","Creator_reputation":1,"Question_score":0,"Answer_content":"For reproducibility you should set the random seed for your code.import randomrandom.seed(2016)Random seed accepts any hashable object. You will most commonly see a number in there. It is an optional argument, if you don't pass it it will use the system time as seed.Another way to achieve reproducible results is to set the random_state=int as argument of Classifier in your code.You can use any number of hidden layers. In order to set the function for each layer, define the first argument of Layer(). That's 'Softmax' in your code.It is recommended to use a pipeline for the construction of your network. You should also normalize your data. In the below code you can see an example:from sknn.mlp import Classifier, Layerfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import MinMaxScalerpipeline = Pipeline([        ('min/max scaler', MinMaxScaler(feature_range=(0.0, 1.0))), #normalization        ('neural network', Classifier(layers=[Layer(\"Softmax\")], n_iter=25))])Then you callpipeline.fit(X_train, y_train)","Display_name":"Bernardo Dor\u0026#233;","Creater_id":111137,"Start_date":"2016-08-11 17:14:44","Question_id":202399}
{"_id":{"$oid":"5837a580a05283111e4d5863"},"Last_activity":"2016-08-11 15:29:50","Creator_reputation":221,"Question_score":4,"Answer_content":"Since we're using MAP we are trying to maximize the probability of the parameters given the data.   P(W|x,y) = \\frac {P(x,y|W) P(W)} {P(x,y)} can be ignored since it's fixed for our data. So we are trying to maximize .  Let's look at .If each element of  is drawn independently from a unit Gaussian, the probability of the matrix isP(W) = \\frac {1} {\\sqrt{2 \\pi}} \\prod_{ij} exp\\Big( -\\frac {w_{ij}^2} 2 \\Big)so  is-\\frac 1 2 \\sum_{ij}{w_{ij}}^2plus some constant terms. The log-likelihood is then  log(P(x,y|W)) - \\frac 1 2 \\sum_{ij} {{w_{ij}}^2}Since we are thinking in terms of losses we negate the log-likelihood and minimize. Now the first term is cross-entropy and the second term is .If you have a (reasonable in some sense that I don't know how to define) more or less arbitrary penalty function you can obtain a density for it by integrating over its domain and then normalizing to obtain a density. In this case it's just easy to see that it's a Gaussian.This does not mean that the elements of  actually are sampled from a Gaussian. What it means is that you believe that's what  looks like before you have any evidence to the contrary. In other words the prior on the elements of  is a Gaussian.","Display_name":"testuser","Creater_id":27028,"Start_date":"2016-08-11 13:03:41","Question_id":229415}
{"_id":{"$oid":"5837a580a05283111e4d5870"},"Last_activity":"2016-08-11 15:14:33","Creator_reputation":3768,"Question_score":0,"Answer_content":"You may want to look the Wikipedia entry \"Random Matrix\" . Among other things It provides pointers to the  work of Marcenko-Pastur which may be of interest if I understand your question correctly.","Display_name":"F. Tusell","Creater_id":892,"Start_date":"2016-08-11 15:14:33","Question_id":229404}
{"_id":{"$oid":"5837a580a05283111e4d587d"},"Last_activity":"2016-08-11 14:49:10","Creator_reputation":31,"Question_score":3,"Answer_content":"You said your integer-valued variables have a fixed range, which makes things relatively easy. You will need to encode them as several features rather than one.If they are categorical (there is no inherent natural order within them), you can use one-hot encoding (i.e. the \"fifth option\" is represented as (0,0,0,0,1,0,0)), this is the common practice.If they are ordinal (discrete, but with an order), using one-hot is possible, but it's even better to use (1,1,1,1,1,0,0). The closest reference I'm aware of is \"A neural network approach to ordinal regression\".PS: If they can assume arbitrarily high values, it depends on the problem and goals at hand. For example, they can be represented as one variable with an accordingly adjusted cost function.","Display_name":"root","Creater_id":86620,"Start_date":"2015-08-22 14:36:35","Question_id":156971}
{"_id":{"$oid":"5837a580a05283111e4d588b"},"Last_activity":"2016-08-11 13:45:18","Creator_reputation":1838,"Question_score":0,"Answer_content":"What does it mean for the residuals to follow a Student's t distribution?The meaning depends on the degrees of freedom, , of the Student's-t Distribution. For background information the Wikipedia entries on the Cauchy, Student's-, and normal distributions are useful reading. I have not seen anything theoretically quantify the regression errors for the Student's- distribution from ordinary least squares (OLS) regression. For maximum likelihood iff degrees of freedom \u003e 2 one can use MatLab, also see ref. Cross-Validated. It is easy enough to show what the effects are using simulations.For 1 degree of freedom, the Student's-t becomes a Cauchy distribution and OLS regression is not useful, because the mean of a Cauchy distribution is undefined (therefore unstable when calculated). See the first image, Monte Carlo simulation of 10000 Cauchy residuals for sequential equidistant x-axis points, below with regression equation  where the truth is  When the degrees of freedom are greater than 1 up to and including 2, the mean is defined but the standard deviation (second moment) is not defined, and then the OLS regression is more accurate but not exact, see next image with , and regression equation .  As  increases the regression accuracy does as well. At , the third moment, skewness is defined. For  kurtosis is defined, and so on for higher moments and degrees of freedom.To get some idea of what this means in practice, I made sequential models for increasing  on the -axis using two sets of random numbers, which coincidentally approach a slope of one and intercept of zero from opposite directions. First the intercepts:Next the slopes:As  the Student's- distribution becomes a normal distribution. Thank-you for a very interesting question that I was wondering about myself. ","Display_name":"Carl","Creater_id":99274,"Start_date":"2016-07-07 15:57:19","Question_id":213873}
{"_id":{"$oid":"5837a580a05283111e4d588c"},"Last_activity":"2016-08-04 09:34:01","Creator_reputation":12982,"Question_score":0,"Answer_content":"If the residuals appear to be asymmetric judging by the normal Q-Q plot, it surprises me that they no longer appear asymmetric judging by the Student's  Q-Q plot, because both the Normal and the Student's  distributions are symmetric.Aside of that, you can say you have fit your SARIMA model using quasi maximum likelihood (QML) rather than maximum likelihood (ML), as the realized error distribution does not match the assumed error distribution. Since both Normal and Student's  distributions are in the same exponential family, the QML coefficient estimators are consistent. Just the standard errors should be adjusted to reflect the mismatch in distributions.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-05-22 02:21:22","Question_id":213873}
{"_id":{"$oid":"5837a580a05283111e4d5899"},"Last_activity":"2016-08-11 13:36:45","Creator_reputation":16279,"Question_score":1,"Answer_content":"If I take apart, \"p-value is the probability that an effect is due to chance,\" it seems to be implying that the effect is caused by chance. But every effect is partially caused by chance. In a statistics lesson where one is explaining the need to try to see through random variability this is a pretty magical and overreaching statement. It imbues p-values with powers they do not have.If you define chance in a specific case to be the null hypothesis then you're stating that the p-value yields the probability that the observed effect is caused by the null hypothesis. That seems awfully close to the correct statement but claiming that a condition on probability is the cause of that probability is again overreaching. The correct statement, that the p-value is the probability of the effect given the null hypothesis is true, does not ascribe cause to the null effect. The causes are various including the true effect, the variability around the effect, and random chance. The p-value doesn't measure the probability of any of those.","Display_name":"John","Creater_id":601,"Start_date":"2016-08-11 13:36:45","Question_id":16939}
{"_id":{"$oid":"5837a580a05283111e4d589a"},"Last_activity":"2012-12-20 02:55:37","Creator_reputation":2289,"Question_score":5,"Answer_content":"Referring directly to the question: Where is the harm?In my opinion, the answer to this question lies in the converse of the statement, \"A p-value is the probability that the findings are due to random chance.\" If one believes this, then one also probably believes the following: \"[1-(p-value)] is the probability that the findings are NOT due to random chance.\"The harm then lies in the second statement, because, given the way most people's brains work, this statement grossly overestimates how confident we should be in the specific values of an estimated parameter. ","Display_name":"D L Dahly","Creater_id":16049,"Start_date":"2012-12-20 02:55:37","Question_id":16939}
{"_id":{"$oid":"5837a580a05283111e4d589b"},"Last_activity":"2012-12-14 05:00:04","Creator_reputation":13461,"Question_score":3,"Answer_content":"OK another, slightly different take on this:A first basic problem is the phrase \"due to [random] chance\".  The idea of unspecified 'chance' comes naturally to students but it is hazardous for thinking clearly about uncertainty and catastrophic for doing sensible statistics.  With something like a sequence of coin flips it is easy to assume that 'chance' is described by the Binomial setup with a probability of 0.5.  There is a certain naturalness to it for sure, but from a statistical point of view it's not more natural than assuming 0.6 or something else. And for other less 'obvious' examples, e.g. involving real parameters it's utterly unhelpful to think about what 'chance' would look like.  With respect to the question, the key idea is understanding what sort of 'chance' is described by H0, i.e. what actual likelihood/DGP H0 names.  Once that concept is in place, students finally stop talking about things happening 'by chance', and start asking what H0 actually is.  (They also figure out that things can be consistent with a rather wide variety of Hs so they get a head start on confidence intervals, via inverted tests). The second problem is that if you're on the way to Fisher's definition of p-values, you should (imho) always explain it first in terms of the data's consistency with H0 because the point of p is to see that, not to interpret the tail area as some sort of 'chance' activity, (or frankly to interpret it at all).  This is purely a matter of rhetorical emphasis, obviously, but it seems to help.In short, the harm is that this way of describing things will not generalise to any non-trivial model they might subsequently try to think about.  At worst it may just add to sense of mystery that the study of statistics already generates in the sorts of people such bowdlerised descriptions are aimed at.","Display_name":"conjugateprior","Creater_id":1739,"Start_date":"2012-12-13 14:20:19","Question_id":16939}
{"_id":{"$oid":"5837a580a05283111e4d589c"},"Last_activity":"2012-12-13 12:31:02","Creator_reputation":15579,"Question_score":11,"Answer_content":"I'll add a late answer from the (ex) student perspective: IMHO the harm cannot be separated from its being wrong. This type of wrong \"didactic approximations/shortcut\" can create a lot of confusion for students who realize that they cannot logically understand the statement, but assuming that what is taught to them is right they do not realize that they are not able to understand it because it is not right. This does not affect students who just memorize rules presented to them. But it  requires students who learn by understanding to be good enough to  arrive at the correct solution by themselves and be good enough so they can be sure they are right and conclude that they are taught bullshit (for some allegedly didactic reason). I'm not saying that there aren't valid didactic shortcuts. But IMHO when such a shortcut is taken, this should be mentioned (e.g. as \"for the ease of the argument, we assume/approximate that ...\").In this particular case, however, I think it is too misleading to be of any use. ","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2012-12-13 12:20:39","Question_id":16939}
{"_id":{"$oid":"5837a580a05283111e4d589d"},"Last_activity":"2011-10-13 09:46:10","Creator_reputation":33216,"Question_score":5,"Answer_content":"Here is a simple example that I use:  Suppose our null hypothesis is that we are flipping a 2-headed coin (so prob(heads) = 1).  Now we flip the coin one time and get heads, the p-values for this is 1, so does that mean that we have a 100% chance of having a 2-headed coin?The tricky thing is that if we had flipped a tails then the p-value would have been 0 and the probability of having a 2-headed coin would have been 0, so they match in this case, but not the above.  The p-value of 1 above just means that what we have observed is perfectly consistent with the hypothesis of a 2-headed coin, but it does not prove that the coin is 2-headed.Further, if we are doing frequentist statistics then the null hypothesis is either True or False (we just don't know which) and making (frequentist) probability statements about the null hypothesis is meaningless.  If you want to talk about the probability of the hypothesis, then do proper Bayesian statistics, use the Bayesian definition of probability, start with a prior and calculate the posterior probability that the hypothesis is true.  Just don't confuse a p-value with a Bayesian posterior.","Display_name":"Greg Snow","Creater_id":4505,"Start_date":"2011-10-13 09:46:10","Question_id":16939}
{"_id":{"$oid":"5837a580a05283111e4d589e"},"Last_activity":"2011-10-12 23:28:10","Creator_reputation":8274,"Question_score":21,"Answer_content":"I have a different interpretation of the meaning of the wrong statement than @Karl does. I think that it is a statement about the data, rather than about the null. I understand it as asking for the probability of getting your estimate due to chance. I don't know what that means---it's not a well-specified claim. But I do understand what is likely meant by the probability of getting my estimate by chance given that the true estimate is equal to a particular value. For example, I can understand what it means to get a very large difference in average heights between men and women given that their average heights are actually the same. That's well specified. And that is what the p-value gives. What's missing in the wrong statement is the condition that the null is true.Now, we might object that this isn't statement perfect (the chance of getting an exact value for an estimator is 0, for example). But it's far better than the way that most would interpret a p-value.The key point that I say over and over again when I teach hypothesis testing is \"Step one is to assume that the null hypothesis is true. Everything is calculated given this assumption.\" If people remember that, that's pretty good.","Display_name":"Charlie","Creater_id":401,"Start_date":"2011-10-12 23:28:10","Question_id":16939}
{"_id":{"$oid":"5837a580a05283111e4d589f"},"Last_activity":"2011-10-12 21:02:08","Creator_reputation":5187,"Question_score":13,"Answer_content":"I've seen this interpretation a lot (perhaps more often than the correct one).  I interpret \"their findings are due to [random] chance\" as \" is true\", and so really what they are saying is  [which actually should be ; say, \"given what we have seen (the data), what is the probability that only chance is operating?\"]  This can be a meaningful statement (if you are willing to assign priors and do Bayes), but it is not the p-value.   can be quite different than the p-value, and so to interpret a p-value in that way can be seriously misleading.The simplest illustration: say the prior,  is quite small, but one has rather little data, and so the p-value is largish (say, 0.3), but the posterior, , would still be quite small.  [But maybe this example isn't so interesting.]","Display_name":"Karl","Creater_id":5862,"Start_date":"2011-10-12 20:12:31","Question_id":16939}
{"_id":{"$oid":"5837a580a05283111e4d58b2"},"Last_activity":"2015-11-11 00:12:05","Creator_reputation":11,"Question_score":1,"Answer_content":"Straight and simple: if the scales are similar use cov-PCA, if not, use corr-PCA; otherwise, you better have a defense for not. If in doubt, use an F-test for the equality of the variances (ANOVA). If it fails the F-test, use corr; otherwise, use cov.","Display_name":"Bear Leg","Creater_id":94734,"Start_date":"2015-11-10 22:16:31","Question_id":53}
{"_id":{"$oid":"5837a580a05283111e4d58b3"},"Last_activity":"2015-10-01 05:15:46","Creator_reputation":19131,"Question_score":4,"Answer_content":"A common answer is to suggest that covariance is used when variables are on the same scale, and correlation when their scales are different. However, this is only true when scale of the variables isn't a factor. Otherwise, why would anyone ever do covariance PCA? It would be safer to alwasy perform correlation PCA. Imagine that your variables have different units of measure, such as meters and kilograms. It shouldn't matter whether you use meters or centimeters in this case, so you could argue that correlation matrix should be used.Consider now population of people in different states. The units of measure are the same - counts (number) of people. Now, the scales could be different: DC has 600K and CA - 38M people. Should we use correlation matrix here? It depends. In some applications we do want to adjust for the size of the state. Using the covariance matrix is one way for building factors that account for the size of the state.Hence, my answer is to use covariance matrix when variance of the original variable is important, and use correlation when it is not.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2015-09-30 06:54:30","Question_id":53}
{"_id":{"$oid":"5837a580a05283111e4d58b4"},"Last_activity":"2015-09-30 05:58:27","Creator_reputation":7991,"Question_score":75,"Answer_content":"You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales.Using the correlation matrix standardises the data. In general they give different results. Especially when the scales are different.As an example, take a look at this R heptathlon data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (run 800m) are around 120.library(HSAUR)heptathlon[,-8]      # look at heptathlon data (excluding 'score' variable)This outputs:                   hurdles highjump  shot run200m longjump javelin run800mJoyner-Kersee (USA)   12.69     1.86 15.80   22.56     7.27   45.66  128.51John (GDR)            12.85     1.80 16.23   23.65     6.71   42.56  126.12Behmer (GDR)          13.20     1.83 14.20   23.10     6.68   44.54  124.20Sablovskaite (URS)    13.61     1.80 15.23   23.92     6.25   42.78  132.24Choubenkova (URS)     13.51     1.74 14.76   23.93     6.32   47.46  127.90...Now let's do PCA on covariance and on correlation:# scale=T bases the PCA on the correlation matrixhep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)hep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)biplot(hep.PC.cov)biplot(hep.PC.cor)  Notice that PCA on covariance is dominated by run800m and javelin: PC1 is almost equal to run800m (and explains  of the variance) and PC2 is almost equal to javelin (together they explain ). PCA on correlation is much more informative and reveals some structure in the data and relationships between variables (but note that the explained variances drop to  and ).Notice also that the outlying individuals (in this data set) are outliers regardless of whether the covariance or correlation matrix is used.","Display_name":"csgillespie","Creater_id":8,"Start_date":"2010-07-19 12:54:38","Question_id":53}
{"_id":{"$oid":"5837a580a05283111e4d58b5"},"Last_activity":"2015-09-29 04:01:45","Creator_reputation":241,"Question_score":24,"Answer_content":"If you have variables with widely varying scales, that is, caloric intake per day, gene expression, ELISA/Luminex in units of ug/dl, ng/dl, based on several orders of magnitude of protein expression, then use correlation as an input to PCA.  However, if all of your data are based on e.g. gene expression from the same platform with similar range and scale, or you are working with log equity asset returns, then using correlation will throw out a tremendous amount of information. You actually don't need to think about the difference of using the correlation matrix  or covariance matrix  as an input to PCA, but rather, look at the diagonal values of  and .  You may observe a variance of  for one variable, and  on another -- which are on the diagonal of .  But when looking at the correlations, the diagonal contains all ones, so the variance of each variable is essentially changed to  as you use the  matrix.In summary, use the correlation matrix  when within-variable range and scale widely differs, and use the covariance matrix  to preserve variance if the range and scale of variables is similar or in the same units of measure.","Display_name":"lep","Creater_id":8906,"Start_date":"2012-02-01 16:13:09","Question_id":53}
{"_id":{"$oid":"5837a580a05283111e4d58b6"},"Last_activity":"2013-06-26 08:28:14","Creator_reputation":335,"Question_score":-4,"Answer_content":"The arguments based on scale (for variables expressed in the same physical units) seem rather weak. Imagine a set of (dimensionless) variables whose standard deviations vary between 0.001 and 0.1. Compared to a standardized value of 1, these both seem to be 'small' and comparable levels of fluctuations. However, when you express them in decibel, this gives a range of -60 dB against -10 and 0 dB, respectively. Then this would probably then be classified as a 'large range' -- especially if you would include a standard deviation close to 0, i.e., minus infinity dB.My suggestion would be to do BOTH a correlation- and covariance-based PCA. If the two give the same (or very similar, whatever this may mean) PCs, then you can be reassured you've got an answer that is meaningul. If they give widely different PCs don't use PCA, because two different answers to one problem is not sensible way to solve questions.","Display_name":"Lucozade","Creater_id":27330,"Start_date":"2013-06-26 08:28:14","Question_id":53}
{"_id":{"$oid":"5837a580a05283111e4d58b7"},"Last_activity":"2010-07-20 05:47:39","Creator_reputation":796,"Question_score":39,"Answer_content":"Bernard Flury, in his excellent book introducing multivariate analysis, described this as an anti-property of principal components.   It's actually worse than choosing between correlation or covariance.   If you changed the units (e.g. US style gallons, inches etc. and EU style litres, centimetres) you will get substantively different projections of the data.The argument against automatically using correlation matrices is that it is quite a brutal way of standardising your data.   The problem with automatically using the covariance matrix, which is very apparent with that heptathalon data, is that the variables with the highest variance will dominate the first principal component (the variance maximising property).So the \"best\" method to use is based on a subjective choice, careful thought and some experience.","Display_name":"user211","Creater_id":211,"Start_date":"2010-07-20 05:47:39","Question_id":53}
{"_id":{"$oid":"5837a580a05283111e4d58c6"},"Last_activity":"2016-08-11 11:40:38","Creator_reputation":1,"Question_score":0,"Answer_content":"So there is a student Joe.  Student Joe has a deal with his mom that he will get a good allowance as long as he keeps a grade of a B or better.  Joe is graded based on tests and quizzes.  His scores are as follows:Quizzes:8:10  - 80%9:10 – 90%9:10 – 70%10:10 – 100%Tests:50:100 – 50%70:100 – 70%85:100 – 85%When Joe’s mom asks how hes doing in school he says, “Good, I have an 81.4% average!!”.  His Mom says “Great, here is your allowance”.  A month later Joe’s mom gets his report card and sees a 70.8%.  Furious, Joe’s mom gets a meeting with Joe and the teacher.  Joe explains he was averaging his test percents and Joe’s teacher shows him how the “weighted average”.  Coincidentally Joes teacher also lowers Joe’s grade to a D.","Display_name":"Mat","Creater_id":127392,"Start_date":"2016-08-11 11:40:38","Question_id":203420}
{"_id":{"$oid":"5837a580a05283111e4d58c7"},"Last_activity":"2016-03-25 02:46:41","Creator_reputation":25400,"Question_score":2,"Answer_content":"What you are calling the \"weighted average\" is the only proper way to calculate the percentage of satisfied customers in both shops. Taking average of percentages (what you call \"unweighted\" average) will give you useless results if your samples differ in size.Imagine extreme case: you have two shops A and B, in shop A there was one customer and he was not satisfied and in shop B there was 100 customers and 90 of them were satisfied -- would you conclude that 45% of customers of both shops were satisfied? Obviously not!Smaller sample is much less reliable, so it should not be included in the final estimate with the same weight as the larger one. Speaking more formally, estimate from the smaller sample has a greater error.  Two shops: A and B, in A there are two customers, one happy, in B,  there were 100 and 75 were happy. So they have 50% and 75% happy  customers, so the average is 62.5%. Using your example from the comment:Standard error for the estimate from the first shop is , while for the second one . So the possible deviation from the true proportion of satisfied customers in shop A is much larger. If the samples differ that much in their reliability, you shouldn't give them equal trust and weight them equally when taking their average.You can easily convince yourself why using pooled mean is much better idea that using the raw one by conducting a simple simulation study. I simulate two shops, with  and  customers, both having the same proportion of satisfied customers. If you compare the results obtained using raw mean (\"unweighted\") and the pooled (\"weighted\") estimates, you'll see that when using the raw mean the variability of difference between the estimate and the true value is much greater. Saying this in simple English: using raw mean you are in greater risk of obtaining the wrong estimate, than in the case of pooled mean.For much more advanced methods of pooling different probabilities you can check Combining probabilities/information from different sources thread, or this one Combining two estimates .","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-03-24 04:01:40","Question_id":203420}
{"_id":{"$oid":"5837a580a05283111e4d58d4"},"Last_activity":"2016-08-11 12:28:25","Creator_reputation":3398,"Question_score":2,"Answer_content":"I'm surprised no one has mentioned the really essential property that Variance is additive for independent random variables:\\mbox{Var}(a_1X_1+\\cdots+a_nX_n)=\\sum_{i=1}^na_i^2\\mbox{Var}(X_i),and the equally nice linearity properties that covariance shares. This becomes completely intractable without the square inside the expectation in the definition of variance.As well for Central limit theorem it is variance, and not L1 that gives rise to the CLT. Specifically L1 gives rise to the strong law of large numbers, but not the fluctuations therein.","Display_name":"Alex R.","Creater_id":61092,"Start_date":"2016-08-09 15:51:58","Question_id":229059}
{"_id":{"$oid":"5837a580a05283111e4d58d5"},"Last_activity":"2016-08-09 15:48:25","Creator_reputation":7559,"Question_score":1,"Answer_content":"In some sense, a related and deeper question is why do people tend to use the  norm instead of the  norm or indeed other norms? In the two-dimensional Euclidean vector space, why do people tend to use  (i.e.  norm) as a measure of distance instead of  (i.e.  norm)?How the  and  norm are related to standard deviation and mean absolute deviation respectively:Let  be a mean zero random variable and  be a probability measure.The standard deviation is simply the  norm: \\left( \\int |x|^2 \\;dP \\right) ^\\frac{1}{2} And the mean absolute deviation is simply the  norm. \\left( \\int |x| \\; dP \\right)^ \\ ","Display_name":"Matthew Gunn","Creater_id":97925,"Start_date":"2016-08-09 15:36:46","Question_id":229059}
{"_id":{"$oid":"5837a580a05283111e4d58d6"},"Last_activity":"2016-08-09 13:23:37","Creator_reputation":19131,"Question_score":2,"Answer_content":"You may come up with infinite number of dispersion measures. It's a lost cause to compare the variance to each and everyone of them.There are two features of variance that are attractive to me. First, it's a smooth function. For instance, the mean absolute deviation is not.Second, it's one of the central moments:\\mu_k=\\sum_ip_i(x_i-\\mu_1)^kHere  is a variance.Being a moment is important, for it defines the distribution when combined with all other moments. Other measures of dispersion as stand-alone metrics.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-08-09 13:20:40","Question_id":229059}
{"_id":{"$oid":"5837a580a05283111e4d58e5"},"Last_activity":"2016-08-11 11:48:39","Creator_reputation":261,"Question_score":0,"Answer_content":"It shows that one of your basic assumptions is wrong.  However, in practice it depends on how sensitive the parameter you are analyzing is with respect to the underlying assumptions.  If you are looking at a strong effect (say, male wage compared to female wage), then whatever you do, the effect is there and about the correct size.  One the other hand, if the effect is weakly identified, your results may end up being extremely sensitive to the distributional (and other) assumptions.Note also that on big data you can pretty much reject every parametric assumption.","Display_name":"Ott Toomet","Creater_id":94588,"Start_date":"2016-08-11 11:48:39","Question_id":229037}
{"_id":{"$oid":"5837a581a05283111e4d59a2"},"Last_activity":"2016-08-08 14:22:07","Creator_reputation":3388,"Question_score":2,"Answer_content":"Training error is used in estimating model parameters. Think about linear regression: if our model is  we estimate  by minimizing  over . This is just minimizing the training loss. Why does this make sense here? It's because we are considering models (i.e. different values of ) that all have the same complexity, so we just pick the one with the smallest loss (or in this case largest likelihood if we're assuming normal errors).Once we're comparing models with different complexities we need to consider out-of-sample performance. Suppose in linear regression we are debating adding a quadratic term  to the model . The model  is a special case of , which has greater complexity, so it is no longer fair to compare the losses. If you're familiar with AIC and BIC, when two models have the same complexity we are just comparing likelihoods (i.e. losses), but when the dimensions of the models (i.e. complexities) differ, we then need to also take this into account. Now let's think about machine learning methods like SVM: once we've fixed our tuning parameters, we are considering a bunch of models with the same flexibility so we can just choose the one that fits the data best. It is only when we are comparing models with different complexities (i.e. when we tune the cost and kernel parameters, and maybe choose the kernel) that we need to consider out of sample performance.As far as how to \"use\" it, I don't really pay attention to the training error per se, but it can appear in certain calculations that I do pay attention to. For example, in linear regression the sum of the squared residuals, a value that is quite useful, is just the total training loss if we're using squared loss. ","Display_name":"Chaconne","Creater_id":30005,"Start_date":"2016-08-08 14:16:18","Question_id":228856}
{"_id":{"$oid":"5837a581a05283111e4d59b3"},"Last_activity":"2016-08-08 14:09:07","Creator_reputation":180,"Question_score":2,"Answer_content":"Check out these papers:\"Scoring functions for learning Bayesian networks\" by Alexandra M. Carvalho, http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf\"Comparison of Score Metrics for Bayesian Network learning\" by Shulin Yang and Kuo-Chu Chang, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.511\u0026amp;rep=rep1\u0026amp;type=pdf\"Metrics for evaluating performance and uncertainty of Bayesian network models\" by Bruce G. Marcot, http://www.fs.fed.us/pnw/pubs/journals/pnw_2012_marcot002.pdf?","Display_name":"Brian O\u0026#39;Donnell","Creater_id":108167,"Start_date":"2016-08-08 14:02:06","Question_id":228846}
{"_id":{"$oid":"5837a581a05283111e4d59c4"},"Last_activity":"2016-08-08 13:20:37","Creator_reputation":41,"Question_score":1,"Answer_content":"The linear discriminant function by definition is a linear combination of the original input space . You do classification on the projected data  which is 1 dimensional (you project the data on a line).","Display_name":"ori06","Creater_id":94103,"Start_date":"2016-08-08 13:20:37","Question_id":206661}
{"_id":{"$oid":"5837a581a05283111e4d59d1"},"Last_activity":"2016-08-08 13:18:04","Creator_reputation":1130,"Question_score":0,"Answer_content":"How could this work, in principle? There may be some procedures that, while not always indicative of REI like the ones you've identified, are procedures that REIs do more (or less!) frequently than other OBGYNs. The ML model will pick up on those subtle differences, especially in combination, and come up with a classifier.In a world where subspecialities are negatively correlated, you could train a multi-class classifier and use both positive and negative data--that is, if you know someone has subspecialty X, then that decreases the probability that they're an REI. But if subspecialties aren't particularly common, the value from this negative data will be relatively low. (One problem you'll also run into is that your model will be spending effort on differentiating the other classes that you don't necessarily care about.)I don't think you're going to get much value out of comparing OBGYNs to non-OBGYNs, if your main goal is separating out REIs from other OBGYNs. (I'm assuming that you'll be provided with specialty whenever you'd want to actually use this model.) Especially if you're using a decision-tree based method, it'll likely just split out on specialty and then throw away all non-OBGYN data, which makes it seem wasteful to include it in the first place.The two main contenders I see are comparing known REIs to possible REIs, and comparing known REIs to known Xs (where X is any sub-speciality other than REI). For both comparisons, you're going to want to exclude the procedures you used to determine subspecialty membership from the dataset, which should bring the subtler procedure differences to the fore.","Display_name":"Matthew Graves","Creater_id":91926,"Start_date":"2016-08-08 13:18:04","Question_id":228853}
{"_id":{"$oid":"5837a581a05283111e4d59de"},"Last_activity":"2016-08-08 12:45:18","Creator_reputation":1130,"Question_score":0,"Answer_content":"The first thing that comes to mind as potentially useful is work on link relevance in search. That is, if you Google a term, Google has a vast ocean of links that it could give you in response, but has to decide which ones it thinks are best.Typical simple systems to approach this provide a score for each option, sort the list of scores, and then return the top n. It sounds like you could do a similar thing--if you have finish times for every horse, just try to regress on the continuous finish time variable for all horses, and then determine whether the predicted finish time is in the top three.(If you can predict a distribution of finish times, then you can compute a probability it will be in the top three, instead of a flat yes/no, which will be a major improvement. Interaction effects between horses also seem potentially important.)If you only have categorical data to begin with--which horse finished in what place for each race--it seems like you can still build a regression model, but a weaker one which doesn't take into account the magnitude of differences between finishing times. (If you have data from enough races, this should be alright, though you own't be able to distinguish between mean differences and variability differences except indirectly.)","Display_name":"Matthew Graves","Creater_id":91926,"Start_date":"2016-08-08 12:45:18","Question_id":228849}
{"_id":{"$oid":"5837a581a05283111e4d59ea"},"Last_activity":"2016-08-08 12:40:24","Creator_reputation":9063,"Question_score":2,"Answer_content":"This is indeed a peculiar result. Let me try to explain what is going on here. If you look at category A, you see estimates of 0 and 20 and since the sampling variances are the same, we can just take their mean to get the fixed effect estimate, which is 10. So in study 1, the estimate is -10 below that and in study 2 it is +10 above that. Analogously, within category B, the estimates are 5 and 25 for a mean of 15 and again the estimate in study 1 is -10 below that and in study it is +10 above that. So roughly, the random effect is -10 for study 1 and +10 for study 2. This isn't 100% right, but close enough to understand what is happening here.Now in category C, you only have that one estimate of 10, which comes from study 1. Since the random effect for study 1 is -10, the fixed effect estimate for category C must be roughly 20, so that we end up with the estimate of 10 we actually see. That is why the model estimate is getting pushed up. So, this is not due to some kind of extrapolation of what the estimate would/should have been in study 2, but a simple consequence of having added a random effect at the study level (with such a peculiar dataset).We can also understand the estimated variance for the study random effects this way. If the random effects are -10 and +10 (and so their mean is 0), then their variance is , which is almost that value of  we get from the model. Note that I am dividing by  here, since you are using REML estimation, and I am ignoring the sampling variances, so the model estimate isn't quite the same as that 200 -- but close enough. This will get you even closer:rma.mv(d, .0001, random = ~ 1 | Study, mods = ~ factor(Category) - 1, data=TheData, control=list(optimizer=\"optim\"))(the syntax above is a shortcut for constraining all of the sampling variance to .0001; I had to switch optimizers though, because nlminb was having difficulties).Note that in practice, one should not just have random effects for study, but also at the estimate level. Please take a look at:http://www.metafor-project.org/doku.php/analyses:konstantopoulos2011where I discuss the three-level model at length. So one should use:TheData$id \u0026lt;- 1:5rma.mv(d, .0001, random = ~ 1 | Study/id, mods = ~ factor(Category) - 1, data=TheData)This doesn't really get you around the issue observed above (the estimate level variance component is estimated to be 0, so nothing is changed), but it is a common mistake to forget this, so it is worth repeating.Interestingly, if we just stick to random effects at the estimate level, then things make more intuitive sense:rma.mv(d, .0001, random = ~ 1 | id, mods = ~ factor(Category) - 1, data=TheData)So, I don't think the solution is to switch to a model without any random effects, but to understand what is going on and to possibly adjust the random effects structure.","Display_name":"Wolfgang","Creater_id":1934,"Start_date":"2016-08-08 12:40:24","Question_id":226859}
{"_id":{"$oid":"5837a581a05283111e4d59f7"},"Last_activity":"2016-08-08 12:36:19","Creator_reputation":4124,"Question_score":2,"Answer_content":"One consequence of such an orthogonal or \"balanced\" design: The estimated coefficients of the two variables will equal those of the two corresponding simple linear regressions.Check for instance the following example in R:  # Generate two uncorrelated variables by PCA. The variables are shifted by two#   to avoid side effects from being centeredpc \u0026lt;- princomp(iris[c(\"Sepal.Length\", \"Sepal.Width\")], cor = TRUE)Petal.Length ~ pc)# The multiple regression result(Intercept)     pcComp.1     pcComp.2        7.903       -1.447       -0.625# Now only the first guy...lm(irisPetal.Length ~ pc[, 2])(Intercept)      pc[, 2]        5.008       -0.625 Thus, the ceteris paribus interpretation (\"... holding the other variable fixed\") makes perfect sense in such scenario. The higher the variables are correlated (e.g. age at diagnosis and time between diagnosis and study entry in a clinical trial), the stranger such interpretation usually is.","Display_name":"Michael M","Creater_id":30351,"Start_date":"2016-08-08 12:29:53","Question_id":228843}
{"_id":{"$oid":"5837a581a05283111e4d5a04"},"Last_activity":"2016-08-08 12:33:40","Creator_reputation":15542,"Question_score":1,"Answer_content":"This is a bit of hack using the logit transform. I probably have the degrees of freedom slightly wrong:qui logit foreign i.mpg_cat if expen == 0scalar v = e(N)-e(k)qui margins, by(mpg_cat) postqui levelsof mpg_catforeach v in `r(levels)' {    local ME : label mpg_cat `v'    di \"`ME' 95% CI: [\" %9.7f ///        invlogit(ln(_b[`v'.mpg_cat]/(1-_b[`v'.mpg_cat]))-invttail(scalar(v),0.025)*_se[`v'.mpg_cat]/(_b[`v'.mpg_cat]*(1-_b[`v'.mpg_cat]))) ///        \", \" ///        invlogit(ln(_b[`v'.mpg_cat]/(1-_b[`v'.mpg_cat]))+invttail(scalar(v),0.025)*_se[`v'.mpg_cat]/(_b[`v'.mpg_cat]*(1-_b[`v'.mpg_cat]))) ///        \"]\"}It produces less ridiculous CIs:Low mpg 95% CI: [0.0352280, .29967771]High mpg 95% CI: [0.2269050, .60227194]Compare that to your output from ci:[.0235275, .2915869][.2112548, .6133465]","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-15 19:08:05","Question_id":223507}
{"_id":{"$oid":"5837a581a05283111e4d5a05"},"Last_activity":"2016-07-13 03:16:41","Creator_reputation":11915,"Question_score":4,"Answer_content":"margins is a very general purpose command. This is great in that you can use the same command for many situations, but often you can get a bit more out of a special purpose program. What margins does is use the delta method to approximate the standard error, which assumes that the sampling distribution for our proportion is normal, i.e. can take values from -infinity to +infinity. That is where the impossible bounds for the confidence interval come from. ci uses techniques designed specifically for proportions, so they will conform to the 0-1 bounds.","Display_name":"Maarten Buis","Creater_id":23853,"Start_date":"2016-07-13 03:16:41","Question_id":223507}
{"_id":{"$oid":"5837a581a05283111e4d5a14"},"Last_activity":"2016-08-08 12:17:47","Creator_reputation":15542,"Question_score":1,"Answer_content":"On (1), take a look at these 2-3 papers that use a data-driven approach:Chay, K. Y., P. J. McEwan, and M. Urquiola (2005). The central role of noise in evaluating interventions that use test scores to rank schools. American Economic Review 95(4), 1237– 1258Bertrand, M., R. Hanna, and S. Mullainathan (2010). Affirmative action in education: Evidence from engineering college admissions in India. Journal of Public Economics 94 (1), 16–29.They essentially run a series of regressions of treatment on a dummy that equals 1 after each possible cutoff point and choose the one cut-off that gives the highest  of the regression.My co-author Matt Backus and his student Sida Peng have a working paper on Identification and Estimation of Discontinuities that uses some machine learning methods to do this, but there is no public draft yet.On (2), see this question about fuzzy RD as IV, a kind of Wald estimator.","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-27 23:36:47","Question_id":225818}
{"_id":{"$oid":"5837a581a05283111e4d5a22"},"Last_activity":"2016-08-08 11:37:08","Creator_reputation":17414,"Question_score":4,"Answer_content":"Perhaps you haven't fully grasped the definition yet. The requirements for orthogonal designs are that the blocking is orthogonal and the treatment is orthogonal. This simply means that crossproducts total to zero, whether the blocking or treatment is continuous, pseudo-continuous, polytomous, or binary. As @whuber correctly points out, statisticians often call dot products cross products, and furthermore often assume blocking and treatment factors have mean 0. So any blocking factor or treatment factor \"crossed\" with any other will come out to 0.Efficiency.Absolutely. We would expect that cross products between any two columns of the design matrix will total out to zero.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-08 08:18:28","Question_id":228797}
{"_id":{"$oid":"5837a581a05283111e4d5a2e"},"Last_activity":"2016-08-08 11:33:45","Creator_reputation":1130,"Question_score":0,"Answer_content":"ARIMA and AR models can apply to both stationary processes and non-stationary processes. Note that the definition of a stationary process discusses the joint probability distribution. So they can't have a 'trend' in the traditional sense (trending up or trending down), but do have a trend of returning to the mean value. (If they didn't, it wouldn't be possible to satisfy the stationarity relationship.)While they can't have global structure, they can have local structure, because what is preserved is the joint probability distribution. Imagine an 'oscillating' series where the next observation can be predicted to be positive or negative with high probability; that can still be stationary so long as the oscillation relationship doesn't change over time and the oscillation is damped.","Display_name":"Matthew Graves","Creater_id":91926,"Start_date":"2016-08-08 11:33:45","Question_id":228790}
{"_id":{"$oid":"5837a581a05283111e4d5a2f"},"Last_activity":"2016-08-08 08:15:57","Creator_reputation":1,"Question_score":-1,"Answer_content":"The base condition to use an AR, MA or ARMA models is that your data should be stationary. If your data is not stationary, to make it stationary, we need to differentiate the data that's what 'I' stands for in AR-I-MA model.To know the trend component in your data, you need to decompose the time-series data.Please run the below R-code to understand better.library(forecast)births \u0026lt;- scan(\"http://robjhyndman.com/tsdldata/data/nybirths.dat\")#Converting data into time-series databirthstimeseries \u0026lt;- ts(births, frequency=12, start=c(1946,1))plot.ts(birthstimeseries)#decomposing the time-series data to get seasonal,trend,observed and random componentsplot(decompose(birthstimeseries))#building a arima modelar.model1 \u0026lt;- auto.arima(birthstimeseries)ar.forecast1 \u0026lt;- forecast.Arima(ar.model1,h = 12)plot.forecast(ar.forecast1)Sources:https://www.youtube.com/watch?v=Aw77aMLj9uM\u0026amp;index=8\u0026amp;list=PLUgZaFoyJafhB73-1JUTRT0y5u_5fjFCRhttp://a-little-book-of-r-for-timeseries.readthedocs.io/en/latest/src/timeseries.html","Display_name":"Praneeth","Creater_id":127001,"Start_date":"2016-08-08 08:15:57","Question_id":228790}
{"_id":{"$oid":"5837a581a05283111e4d5a3b"},"Last_activity":"2016-08-08 11:26:12","Creator_reputation":1183,"Question_score":0,"Answer_content":"The bivariate normalized scatterplot shows no relationship between Y and X1, X2 or X3, but there is something for X4.  The CCF shows no spikes except for X4. Only X1 needed prewhiteningThe first 3 observations need to be dropped as there are missing values in the data.We ran your data through Autobox(a software I am involed with) and it found a contemporaneous relationship plus a lag of 1 and 2 for X4 plus two time trends starting at period 1 and then another starting at period 47 plus 3 outliers and an AR1.Your early data looks very different from newer data.  I used the last 57 observations and things look very different.  The scatterplots SCREAM correlation using the newer data.  Here is the CCF using the prewhitened X1 series which shows a mild relationship.  When X3 gets dropped the significance becomes a lot clearer between the X's and the Y.","Display_name":"Tom Reilly","Creater_id":3411,"Start_date":"2016-08-08 10:06:28","Question_id":222727}
{"_id":{"$oid":"5837a581a05283111e4d5a4a"},"Last_activity":"2016-08-08 11:14:38","Creator_reputation":1130,"Question_score":3,"Answer_content":"Why would the intuitive method work?Think of the table as a collection of pairs; that is, instead of the traditional North-West-East-South cross of a bridge table, we look at it like a table with two rows:North-SouthWest-EastIf we condition on North being the senior partner of one couple, then there's a 1/3rd chance that South will be the junior partner of that couple, which then forces West and East to be a couple, and a 2/3rd chance that South will be a member of the other couple, and then the the last set is also definitely not a couple.When we extend from  to , we just add a row to the table:Northwest-SoutheastNorth-SouthWest-EastIf we set Northwest as always being the senior partner of the first couple, then there's clearly a  chance that there's a paired couple and we can stop, and a  chance that there isn't, and we can continue, with a smaller problem.Note that the smaller problem is a different one, though, which is 'coincidentally' the same. Instead of having four people and two couples going into the problem, we must have one couple and two singles, and the chance that the couple is paired up is  (for the same reasons as before).This gives us a recursive approach; we can talk about a problem with two parameters, , where  refers to the number of people and  refers to the number of couples. So  gives us  (that is to say, four couples with 8 people gives us a  chance of failure when assigning the first pair, and then the chance of failure for 2 couples and 6 people in the case where we survive), and then for  we need to expand out four cases:Both of the next couple are single: One was single, the other was in a couple: Both were in different couples:  (Note that , for obvious reasons.)Both were in the same couple:  (This is a loss condition)If you go through and do all the math, I think you end up with  for the 8 person case, which is not . (It's higher because of the chance that we totally break up the couples early on.)I'm not aware of an immediate trick that allows you to just use a combinatoric formula to get an answer in closed form, but it seems likely that there could be one. [edit: See whuber's answer for the solution.]","Display_name":"Matthew Graves","Creater_id":91926,"Start_date":"2016-08-05 11:04:27","Question_id":228467}
{"_id":{"$oid":"5837a581a05283111e4d5a4b"},"Last_activity":"2016-08-08 07:11:35","Creator_reputation":147568,"Question_score":5,"Answer_content":"AnalysisLet's guess and then systematically improve the guess until it's correct.Begin by guessing the answer is .  Of course that's wrong.  To see how wrong, label one partner in each pair \"Red\" and the other \"Blue\".  From the perspective of any Red individual, there is a  chance that their (Blue) partner will sit across from them.  Because there are  red individuals, let's subtract  from that initial guess.But wait--that's still not quite right, because all pairs of couples have been double-counted.  If one couple is seated opposite, there remain  couples,  places, and from any Red individual's point of view, the chance that they are part of a second couple is .  Therefore we need to re-add .But now we have undercounted contributions to the result from triples of couples, which we need to correct.  And so it goes, until finally we have accommodated all  couples in the formula.  (This, of course, is just the Principle of Inclusion-Exclusion in action.)  The resulting formula is\\sum_{i=0}^n (-1)^i \\binom{n}{i} \\frac{1}{(2n-1)(2n-3)\\ldots (2n-2i+1)} = {_1}F_1\\left(-n, -n+\\frac{1}{2}, -\\frac{1}{2}\\right).\\tag{1}ComputationFor positive integers , the Kummer confluent hypergeometric function  is a polynomial of degree  in .  From the Kummer Transformation{_1}F_1\\left(-n, -n+\\frac{1}{2}, -\\frac{1}{2}\\right) = e^{-1/2}\\ {_1}F_1\\left(\\frac{1}{2}, -n+\\frac{1}{2}, \\frac{1}{2}\\right)it is straightforward to deduce that the limiting value of the probability as  grows large is .  The convergence is slow: you have to multiply  by  to attain an additional decimal digit.  Nevertheless, accurate (double-precision) values can quickly be computed for any  by noting that the terms in the left hand sum of  grow more slowly than powers of .  Thus, by the time  reaches , the new values will be essentially zero compared to  (and in fact a closer analysis suggests that stopping the summation by  will work).  This formula will break down for  greater than 10,000,000 in certain computing environments due to imprecision in the log Gamma function.  The problem arises from cancellation in the differences arising when computing terms in the series.  An excellent approximation to those differences when  is sufficiently large can be found in terms of , where  is the derivative of  (the digamma function).  That is implemented in the code below, at a slight cost in computation time.ImplementationThe following R code computes about 20,000 double-precision values per second. f \u0026lt;- function(n) {  h \u0026lt;- function(n) {    ifelse(n \u0026lt; 1e6, lfactorial(n) - lfactorial(n-1/2), digamma(n+3/4)/2)  }  m \u0026lt;- min(n, 46)  k \u0026lt;- 0:m  x \u0026lt;- exp(h(n) - h(n-k) - lfactorial(k) - k*log(2)) * (-1)^k  sum(x)}As an example, let's track how closely log(f(n)) comes to its limiting value of  for large .  As claimed above, each factor of  in  adds one decimal place of limiting accuracy.  Let's therefore look at the  decimal place in the logarithm of the ratio of  to , for whole powers of  from  through :\u0026gt; round(sapply(1:14, function(n) 10^n * (log(f(10^n)) + 1/2)), 3)[1] -0.255 -0.251 -0.250 ... -0.250 -0.249 -0.249 -0.400(Seven values have been omitted from the middle, all equal to -0.250.)  The constant pattern is clear.  At the end, with , it starts to break down, indicating loss of precision.  Improving on this would likely require high-precision arithmetic.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-08 06:32:47","Question_id":228467}
{"_id":{"$oid":"5837a581a05283111e4d5a58"},"Last_activity":"2016-08-08 10:48:51","Creator_reputation":46,"Question_score":0,"Answer_content":"If you are saying that the spending falls into different groups or categories (e.g. different seasons) then lumping them all together breaks the assumption of independence of observations.  E.g. if the observations for spring constitute one group of observations, and the observations for Christmas constitute a different group, the relationship between spend and outcome might be quite different in the different groups.  This doesn't necessarily just give you some kind of average between the groups.  To see how badly this can go wrong, read up on Simpson's paradox, e.g. http://www.theregister.co.uk/2014/05/28/theorums_3_simpson/.","Display_name":"MikeG","Creater_id":125367,"Start_date":"2016-08-08 10:48:51","Question_id":228681}
{"_id":{"$oid":"5837a581a05283111e4d5a59"},"Last_activity":"2016-08-07 13:34:53","Creator_reputation":3645,"Question_score":1,"Answer_content":"Using ad spend (or any marketing instrument) in a model predicting unit sales is pretty standard. To your very good point, it is almost always endogenous, e.g., ad spend is frequently a percentage of sales revenue as in the A/S ratio. The statistical solutions are to create and model HAC residuals for the DV and then run the usual tests for endogeneity, e.g., Durbin-Wu-Hausman or simply correlate the model residuals with the predictors (Wooldridge's test). If the tests suggest that endogeneity is still a problem, then the next step would be to run a 2SLS to further control for it.That said, these solutions are far from perfect as endogeneity can be very difficult, if not impossible, to eliminate.Lots of references out there about it, e.g., Wooldridge's Econometric Analysis of Cross Section and Panel Data or Hanssen and Parsen's Market Response Models: Econometric and Time Series Analysis.","Display_name":"DJohnson","Creater_id":82102,"Start_date":"2016-08-07 13:34:53","Question_id":228681}
{"_id":{"$oid":"5837a581a05283111e4d5a66"},"Last_activity":"2016-08-08 10:36:10","Creator_reputation":275,"Question_score":0,"Answer_content":"Conceptually,  is the difference between an individual 's actual wage and your model's prediction for 's wage. For example, let's assume that in the data, individual  has  years of education and a wage of . Then the model prediction for 's wage (given the coefficients you listed) is . But we observed that 's actual wage is . Then . This is just a concrete example of the formula in @roundsquare's comment.The  are illustrated with fake data in the plot below. (The R code to create the plot is just below the plot.) We fit the model . In the plot, the points are the data, the line is the regression fit (where the function is ) and the red vertical lines are the  (the difference between the regression line and the actual data), which are known as the residuals. The regression line is the particular line that minimizes  (the sum of the squared residuals).library(ggplot2)# Fake dataset.seed(491)x = runif(50, 0, 100)y = 2*x + 5 + rnorm(50,0,20)# Combine x, y into a data framedat = data.frame(x,y)# Fit regression modelm1 = lm(y ~ x, data=dat)# Get predictions for regression modeldat\\epsilon_iy - predict(m1). ","Display_name":"eipi10","Creater_id":3162,"Start_date":"2016-08-06 11:23:15","Question_id":228567}
{"_id":{"$oid":"5837a581a05283111e4d5a73"},"Last_activity":"2016-08-08 10:34:07","Creator_reputation":46,"Question_score":0,"Answer_content":"I think this really ought to be a comment, but I don't have enough reputation (yet) to leave a comment.  Two thoughts occur to me.(a) Are you sure you did a correct moderator analysis and entered everything correctly? (Sorry if that sounds patronising, but I don't know you.)  See http://quantpsy.org/sobel/sobel.htm for a well recognised website.(b) Perhaps your AB correlation was parametric, but your moderator analysis used a nonparametric process such as resampling?  If so, have a good look at whether your variables meet parametric assumptions, and be consistent in whether you use parametric or nonparametric tests.","Display_name":"MikeG","Creater_id":125367,"Start_date":"2016-08-08 10:34:07","Question_id":228744}
{"_id":{"$oid":"5837a581a05283111e4d5a84"},"Last_activity":"2016-08-08 10:25:45","Creator_reputation":31,"Question_score":3,"Answer_content":"The standard errors of the model coefficients are the square roots of the diagonal entries of the covariance matrix. Consider the following: Design matrix:, where  is the value of the th predictor for the th observations.(NOTE: This assumes a model with an intercept.), where  represents the predicted probability of class membership for observation .The covariance matrix can be written as: This can be implemented with the following code:import numpy as npfrom sklearn import linear_model# Initiate logistic regression objectlogit = linear_model.LogisticRegression()# Fit model. Let X_train = matrix of predictors, y_train = matrix of variable.# NOTE: Do not include a column for the intercept when fitting the model.resLogit = logit.fit(X_train, y_train)# Calculate matrix of predicted class probabilities. # Check resLogit.classes_ to make sure that sklearn ordered your classes as expectedpredProbs = np.matrix(resLogit.predict_proba(X_train))# Design matrix -- add column of 1's at the beginning of your X_train matrixX_design = np.hstack((np.ones(shape = X_train.shape[0],1)), X))# Initiate matrix of 0's, fill diagonal with each predicted observation's varianceV = np.matrix(np.zeros(shape = (X_design.shape[0], X_design.shape[0])))np.fill_diagonal(V, np.multiply(predProbs[:,0], predProbs[:,1]).A1)# Covariance matrixcovLogit = np.linalg.inv(X_design.T * V * X_design)print \"Covariance matrix: \", covLogit# Standard errorsprint \"Standard errors: \", np.sqrt(np.diag(covLogit))# Wald statistic (coefficient / s.e.) ^ 2logitParams = np.insert(resLogit.coef_, 0, resLogit.intercept_)print \"Wald statistics: \", (logitParams / np.sqrt(np.diag(covLogit))) ** 2All that being said, statsmodels will probably be a better package to use if you want access to a LOT of \"out-the-box\" diagnostics. ","Display_name":"j_sack","Creater_id":127007,"Start_date":"2016-08-08 10:16:41","Question_id":89484}
{"_id":{"$oid":"5837a581a05283111e4d5a85"},"Last_activity":"2014-03-10 19:44:09","Creator_reputation":560,"Question_score":4,"Answer_content":"If you're interested in doing inference, then you'll probably want to have a look at statsmodels. Standard errors and common statistical tests are available. Here's a logistic regression example.","Display_name":"jseabold","Creater_id":6828,"Start_date":"2014-03-10 19:44:09","Question_id":89484}
{"_id":{"$oid":"5837a581a05283111e4d5a86"},"Last_activity":"2014-03-10 13:21:55","Creator_reputation":2770,"Question_score":7,"Answer_content":"Does your software give you a parameter covariance (or variance-covariance) matrix?  If so, the standard errors are the square root of the diagonal of that matrix.  You probably want to consult a textbook (or google for university lecture notes) for how to get the  matrix for linear and generalized linear models.","Display_name":"generic_user","Creater_id":17359,"Start_date":"2014-03-10 13:21:55","Question_id":89484}
{"_id":{"$oid":"5837a581a05283111e4d5a93"},"Last_activity":"2016-08-08 07:45:31","Creator_reputation":8000,"Question_score":0,"Answer_content":"What you call \"cross-validation error\" should be the \"validation error\", i.e. the error computed on the validation set, which is a set of labeled samples that appear in neither the training set (otherwise the early stop might not be that early, and you'll overfit) nor the test set (forbidden).","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-08 07:45:31","Question_id":228693}
{"_id":{"$oid":"5837a581a05283111e4d5a9f"},"Last_activity":"2016-08-08 10:07:29","Creator_reputation":3295,"Question_score":0,"Answer_content":"From this document:   \"2.6 Execution time    For a given set of input data, the following can increase the speed of the forward pass:    (i) decreasing degree (because there are fewer combinations of terms to consider),    (ii) decreasing nk (because there are fewer forward pass terms),    (iii) increasing minspan (because fewer knots need to be considered),    (iv) decreasing fast.k (because there are fewer potential parents to consider at each forward step),    (v) increasing thresh (faster if there are fewer forward pass terms).    The backward pass is normally much faster than the forward pass, unless method = \"exhaustive\". Reducing prune reduces exhaustive search time. One strategy is to first build a large model and then adjust pruning parameters such as prune using update.earth.\"(Note that train uses the strategy with update.earth when train's method = \"earth\" but not when method = \"fda\" so technical reasons related to how MARS is bundled into FDA.)Otherwise, it depends on your data size, number and types of predictors etc. The document linked above does have formulas for memory usage and that might be an acceptable proxy. Max","Display_name":"topepo","Creater_id":3468,"Start_date":"2016-08-08 10:07:29","Question_id":225275}
{"_id":{"$oid":"5837a581a05283111e4d5aae"},"Last_activity":"2016-08-08 09:51:30","Creator_reputation":3639,"Question_score":1,"Answer_content":"I have no idea how you do this in SPSS but you need to have a single column for your outcome with two columns for your predictors: one for treatment and one for time. You would also need a column to identify the person so you can specify the repeated measures nature of your data-set. It way well be that SPSS would let you do the analysis with the data-set in the format that you have but you would need to ask on an SPSS-specific forum.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-08 09:51:30","Question_id":228549}
{"_id":{"$oid":"5837a581a05283111e4d5abb"},"Last_activity":"2014-04-12 14:54:38","Creator_reputation":1457,"Question_score":2,"Answer_content":"Since you have multiple dependent and independent variables, a multivariate analysis would be one way to proceed. A multivariate analysis will attempt to model the relationship between your dependent and independent variables, and as an outcome you will be able to test if those factors are significant in your model. This is useful if you want to assess the significance of the factors within such a model, but if you are interested in knowing the significance of the relationship between the covariates and one response you can run a regression the way you describe.Suppose though, that you want to construct a model for both responses simultaneously, and assess the significance of the factors in  model.Then you can use multivariate analysis of covariance (MANCOVA). MANCOVA will provide you with the contribution to the variance in the responses made by each factor, as well as their significance. Note that MANCOVA will produce both type I, II, and III sums of squares (SS). Which one is appropriate depends on the balance of your data. See here for more information on the types of SS.http://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/If you are using R, you can determine the statistical significance of your factors by performing multivariate regression and using this as input in the manova function. The code would go something like:#fit a multivariate regression model and then test the type I SS using MANCOVA.fit = lm(formula = cbind(Abundance, Richness) ~ Temp_1 + Rain_1 + Sunlight_1 + Temp_2 + Rain_2 + Sunlight_2 + Temp_3 + Rain_3 + Sunlight_3 + Temp_4 + Rain_4 + Sunlight_4, data = yourData)summary(manova(fit), test=\"Hotelling-Lawley\")A more thorough overview of how to perform such an analysis is provided here:http://www.uni-kiel.de/psychologie/rexrepos/posts/multRegression.html Note that separate regressions return the same slopes as multivariate regression, and also not that different tests besides the \"Hotelling-Lawley\" are possible for the MANCOVA test of type I SS, and that you can also test type II SS.As you pointed out, PCA is another multivariate data analysis method. It may be interesting for performing exploratory data analysis, though you will not obtain the same sort of significance testing discussed above. If you perform PCA on your data, a bi-plot may be a good way to investigate interesting relationships. SAS provides some rather clear discussion interpreting the biplot:http://support.sas.com/documentation/cdl/en/imlsug/62558/HTML/default/viewer.htm#ugmultpca_sect2.htm","Display_name":"Deathkill14","Creater_id":17672,"Start_date":"2014-04-12 14:54:38","Question_id":90568}
{"_id":{"$oid":"5837a581a05283111e4d5ac8"},"Last_activity":"2016-08-08 09:27:43","Creator_reputation":12762,"Question_score":3,"Answer_content":"Looking at your reference, here's a representative use of the phrase geometric probability  Let  and let P be the geometrical probability on . Find the distribution function of the random variable    {definition of a piecewise function}    Is the distribution of X continuous (if so, find the relevant density)?So it looks to me like they are using it to mean \"the probability measure created by normalizing the area (or length or volume) measure\".  For example, using the  in the above problem, the area (which is really length here) measure of an interval is \\mu((a, b)) = b - a but this makes the entire  have measure , so we need to normalize, which creates the probability measure P((a, b)) = \\frac{1}{3} \\mu((a, b)) = \\frac{1}{3} (b - a) now , as we need to do probability.I do not think this is a standard term, I have not heard it used this way before.  It does make some sense though.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-08-08 09:27:43","Question_id":228815}
{"_id":{"$oid":"5837a581a05283111e4d5ad5"},"Last_activity":"2016-08-08 09:19:43","Creator_reputation":46,"Question_score":0,"Answer_content":"(Amended answer following discussion)The problem arises from the weighting, not just of the genders (which I understand is representative of your customers) but it is an artefact of the A/B split, which is different between the genders.  If you re-do the calculation, but both males and females have equal numbers in each condition, you have the following.Uplift for females 40% (as you calculated)Uplift for males 33% (as you calculated)Unweighted mean uplift (equal numbers of males and females) 38%Weighted mean uplift (males:females 3:2) 37%; between the two figures, but closer to the male figure because there are more males.Weighted mean uplift (males:females 2:1) 36%; still between the two figures, but even closer to the male figure because there are even more males.See screenshots below; let me know if you need the formulas.","Display_name":"MikeG","Creater_id":125367,"Start_date":"2016-08-02 23:18:15","Question_id":226994}
{"_id":{"$oid":"5837a581a05283111e4d5ae2"},"Last_activity":"2016-08-08 09:17:33","Creator_reputation":1130,"Question_score":2,"Answer_content":"One thing that's useful for analysis of time series like this is to explicitly break out the frequency dependent features, rather than trusting that your model will discover them. Do a wavelet transform and you'll quickly get both enough data that deep learning is sensible and you'll expose a number of useful features for the model.I am not yet aware of papers investigating what sort of masks are most useful for CNNs on wavelet data, but I haven't looked. You might be able to find some prior work.","Display_name":"Matthew Graves","Creater_id":91926,"Start_date":"2016-08-08 09:17:33","Question_id":228780}
{"_id":{"$oid":"5837a581a05283111e4d5af3"},"Last_activity":"2016-08-08 08:56:44","Creator_reputation":7692,"Question_score":3,"Answer_content":"Following up on Glen_b's answer, and in an attempt to dumb it down a bit more, the following illustrations shows how the bivariate or joint pdf of  and , both independent and standard uniform distributions , is the basis to obtaining the pdf of the variable , defined as the sum of .For every value of , the probability of getting a quantile lower than , i.e. the distribution or cdf of  will be given by the integral from  to the line  on the right on the  axis, and from  to  on the  axis. Luckily, the vast majority of the  plane corresponds to a joint pdf of .The idea is that by obtaining the cdf, i.e. , of  we can arrive at the cdf, i.e. , by differentiating:In general, the pdf of the sum of two independent variables  and  can be proven to correspond to the convolution of  and  at , i.e. .Algebraically, the joint density can be expressed by two indicator variables: Hence, can reach  inviting the following partition: If , and noting that  if , and  otherwise,resulting inwhereas if , and given that we are integrating over , and the integrand is  only when  (rearranged: ):explaining the triangular appearance of the  of .","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-08-06 09:38:16","Question_id":195433}
{"_id":{"$oid":"5837a581a05283111e4d5af4"},"Last_activity":"2016-02-14 09:27:57","Creator_reputation":152683,"Question_score":7,"Answer_content":"If you don't write down the support, you may not see what's going on -- but as soon as you do, it's a lot clearer.  I am not able to understand the difference between the joint density function and density function for a random variable Z = x1 + x2 where x1, x2 are uniform rvs in [0,1].Note that  for  and  elsewhere; similarly for .The joint density is bivariate - the density is a surface.  I think joint density in this case is f(x1,x2) = 1 So, assuming independence, the joint density will be:   on the unit square and  elsewhere.(At least, \"bivariate uniform under independence\")  Likewise the density function of z is defined as convolution of x1 and x2 It is the convolution if they're independent, yes.The sum of a pair of quantities is a single quantity -- the sum of a pair of random variables is a univariate random variable. The density function of the sum of independent variables goes from the sum of the smallest values of each variable to the sum of the largest values of each variable. Consequently the sum of a pair of independent variates each on  will lie in the interval  (i.e. on ).The shape of the density for the sum  (as you'll find if you perform the convolution) is symmetric and triangular, though it's also obvious from direct inspection of a picture of the joint density:The blue arrows show all the density at a fixed ; this is evaluated at each point along the red line. You can see the amount of density at each point increases linearly until the peak at 1, then decreases linearly again.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-02-14 04:02:50","Question_id":195433}
{"_id":{"$oid":"5837a581a05283111e4d5b03"},"Last_activity":"2016-08-08 08:52:30","Creator_reputation":151,"Question_score":1,"Answer_content":"1- You can choose the minimum based on the CP values or you can use \"which.min\".2- I have no idea about it.3- I do not think that you can plot lambda on the x-axis.4- Computing the MSE of any regression model on some test set:\\frac{1}{N}\\sum\\limits_{i=1}^N(\\hat{y}_i-y_i),where  are the predicted values and  are the observed values.Thus, you can write this formula in R program manually.mean(predicted(y_i)-observed(y_i))^2I hope that my answers help you.","Display_name":"jeza","Creater_id":120788,"Start_date":"2016-08-07 16:04:39","Question_id":228568}
{"_id":{"$oid":"5837a581a05283111e4d5b10"},"Last_activity":"2016-08-08 08:38:27","Creator_reputation":252,"Question_score":0,"Answer_content":"Generally, you need to operationalize the QUALITY of your measurement. Then you will have to systematically evaluate different scenarios. I will give an example. First, you will need to define the quality of the measurement depending on the application such as 1) little noise and 2) sensitive to unusual behaviour. Then you can use test signals to evaluate the answer of your metrics. As you did not provide any information about your measurements, I will use a simple example. Lets say you want to measure temperature, and you have the choice of either a Resistance temperature detector, infrared sensors or thermometers. First, to estimate the influence of noise, you will try to create an experimental environment with constant temperature (impossible, but you can try to minimize any changes as far as it goes). Then you can rank all three methods regarding their variance throughout a time series of 5 minutes. Then you can create several scenarios to estimate the sensitivity of the measurement, where you will (in this example) use a heat source to identify the sensitivity of the measurment. Again, you can use statistic evaluation such as the difference between the peak and the mean value as a measurement for sensitivy. ","Display_name":"Nikolas Rieble","Creater_id":124547,"Start_date":"2016-08-08 08:38:27","Question_id":228382}
{"_id":{"$oid":"5837a581a05283111e4d5b1d"},"Last_activity":"2016-08-07 16:43:20","Creator_reputation":8337,"Question_score":0,"Answer_content":"Fit the model to all of your training data, then use it to make predictions for the unlabeled cases. Nothing more specific can be said than that without knowing what sort of model you're using, what programming language you're using, etc.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-07 16:43:20","Question_id":228699}
{"_id":{"$oid":"5837a581a05283111e4d5b2a"},"Last_activity":"2016-08-08 08:21:07","Creator_reputation":252,"Question_score":1,"Answer_content":"The split of training and test sets is used to evaluate your algorithms such as classifiers regarding their accuracy. When you want to apply your algorithm to real data, you should then use a labelled data to train it - after systematically evaluating parameters of classifiers using the split data. Shuffeling the test and training sets is the concept of crossvalidation. Crossvalidation is such a helpful technique that it became the name of this forum. There are many sources in the internet providing detailled explanations of crossvalidation.If you have any concrete questions to crossvalidation, feel free to ask. Some questions in this forum deal with details of crossvalidation: Choice of K in K-fold cross-validationFurther, I think your question partly is answered here: Training with the full dataset after cross-validation?","Display_name":"Nikolas Rieble","Creater_id":124547,"Start_date":"2016-08-08 08:21:07","Question_id":228801}
{"_id":{"$oid":"5837a581a05283111e4d5b37"},"Last_activity":"2016-08-08 07:05:07","Creator_reputation":206,"Question_score":2,"Answer_content":"No, the power of a given statistical test is a measure of how large chance you have of getting a significant result with a certain data set given a certain effect size. If you have a guess of what the effect size is, this can then be used to calculate how many observations you should make in order to have a reasonable chance of getting a useful result.That 100 out of 1000 gives you a more accurate representation of the population than 100 out of 100000 isn't that strange. Let's assume that you have a population consisting of  number of data points, where each data point can have a value between  and  (where ).Now, if you sample  number of data points, your sample mean will naturally be equal to the population mean. If you, on the other hand, sample  data points, your sample estimate can theoretically not be more off than from the population mean (that is, in the scenario where you happen to have  data points with the value , which you've sampled, and one data point with the value , which you haven't sampled).More generally, if you draw a sample of size  (where , you can theoretically not be more off thanNow, as  goes from  towards  (that is, as the sample size increases), we can see that the weight of  increases while the weight of  decreases, meaning that the whole expression decreases since . (Remember that the two weights,  and , always has the sum 1 when put together.) This means that as the sample size gets bigger, the theoretical possibility off how much the sample mean can differ from the population mean shrinks. ","Display_name":"Speldosa","Creater_id":3812,"Start_date":"2015-05-07 16:59:27","Question_id":151336}
{"_id":{"$oid":"5837a581a05283111e4d5b46"},"Last_activity":"2016-08-08 08:04:43","Creator_reputation":1456,"Question_score":4,"Answer_content":"Here is an attempt:Consider  such that  and with     \\mathcal{M}_X(t) = \\left(1-2 \\, t\\right)^{-\\alpha/2}   \\mathcal{M}_Y(t) = \\left(1-2 \\, t\\right)^{-\\beta/2}\\mathcal{M}_Z(t) = M_X(t)M_Y(-t) = \\left(1-2 \\, t\\right)^{-\\alpha/2}\\left(1+2 \\, t\\right)^{-\\beta/2} = (1-4t^2)^{-\\beta/2}\\mathcal{M}_Z(t) = (1-2t)^{-n/2}(1+2t)^{-1/2} = (1-4t^2)^{-1/2} (1-2t)^{-(n-1)/2}I am not sure if it can be reduced to a fathomable MGF.","Display_name":"rightskewed","Creater_id":11668,"Start_date":"2016-08-08 08:04:43","Question_id":228558}
{"_id":{"$oid":"5837a581a05283111e4d5b53"},"Last_activity":"2016-08-05 03:49:25","Creator_reputation":57732,"Question_score":1,"Answer_content":"There is no single best test of the assumption of normality of residuals in a regression and many researchers recommend using graphical methods and an intuitive approach. There was a great quote which I am not quite remembering about how testing the assumptions after performing a regression was like sending a rowboat to test an ocean liner or something like that.  The tests will give a p value and, like all p values, it will be dependent on sample size. But the problems that violations of the assumptions cause are not dependent on sample size; they are dependent on the grossness of the violation of the assumptions.One approach is to just always use robust methods (e.g. quantile regression or robust regression); another is to use both robust methods and \"regular\" ones and then compare results to see if there are big differences. EDIT: Run a robust regression and get the predicted values; run an OLS reg. and get the predicted values. Plot the two sets against each other and against actual values. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-07-29 04:11:56","Question_id":226245}
{"_id":{"$oid":"5837a581a05283111e4d5b5f"},"Last_activity":"2016-08-08 07:49:30","Creator_reputation":3388,"Question_score":1,"Answer_content":"As the comments have pointed out, I'm going to assume that your data matrix  is normalized so that  actually is the correlation matrix. whuber pointed out that the diagonal of  is necessarily 1 and all other elements are at most 1 in absolute value, so it cannot be that off-diagonal elements are larger than the diagonal ones. I'm instead going to answer what happens when there are off-diagonal elements that are large in absolute value. Note that  is always positive semi-definite, and is positive definite if and only if  is full rank. This means that we can answer your question about the rank of  by looking at how \"far\" from being positive definite  is. This exactly answers the question of how far from full-rank  is. See here for a proof that . If two columns of  are perfectly correlated then we'll have an off-diagonal 1 in  (well, 2 1's by symmetry) and  has rank of at most  now (assuming ), because two columns are collinear. So we can relate the number of exact 1's and -1's in  to the number of collinear pairs of columns, but this doesn't tell us about larger sets of columns of  that are collinear. We could easily have three columns of  being perfectly collinear without any pair being collinear. To assess that we should look at the eigenvalues of .By the spectral theorem we have  where  is an orthonormal matrix of eigenvectors and  is a diagonal matrix containing the eigenvalues , ordered from largest to smallest.  is PD iff all ; by  being PSD we know all . This means that the rank of  is equal to the number of non-zero eigenvalues  has. What's nice about this is that looking for 1's and -1's in  will tell us if pairs of columns are collinear, but looking at the eigenvalues will tell us if any arbitrary combinations of columns are collinear and that includes the pairs that the correlations would detect. So, in a nutshell, you can look at the off-diagonal elements of  to learn about the rank of , but its eigenvalues will be far more informative.As a final comment, if you actually compute the eigenvalues of some observed matrix , you likely won't find any eigenvalues exactly equal to 0 due to numerical rounding. Generally though it's pretty obvious which ones should be exactly 0. If you use R there's a function zapsmall which is often used for this.","Display_name":"Chaconne","Creater_id":30005,"Start_date":"2016-08-08 07:38:42","Question_id":228788}
{"_id":{"$oid":"5837a581a05283111e4d5b6c"},"Last_activity":"2016-08-08 07:27:48","Creator_reputation":17864,"Question_score":3,"Answer_content":"There are a couple of obvious cases where random forests will struggle:Sparsity - When the data are very sparse, it's very plausible that for some node, the bootstrapped sample and the random subset of features will collaborate to produce an invariant feature space. There's no productive split to be had, so it's unlikely that the children of this node will be at all helpful. XGBoost can do better in this context.Data are not axis-aligned - Suppose that there is a diagonal decision boundary in the space of two features,  and . Even if this is the only relevant dimension to your data, it will take an ordinary random forest model many splits to describe that diagonal boundary. This is because each split is oriented perpendicular to the axis of either  or . (This should be intuitive because an ordinary random forest model is making splits of the form .) Rotation forest, which performs a PCA projection on the subset of features selected for each split, can be used to overcome this: the projections into an orthogonal basis will, in principle, reduce the influence of the axis-aligned property because the splits will no longer be axis-aligned in the original basis. This image provides another example of how axis-aligned splits influence random forest decisions. The decision boundary is a circle at the origin, but note that this particular random forest model draws a box to approximate the circle. There are a number of things one could do to improve this boundary; the simplest include gathering more data and building more trees.Random forests basically only work on \"flat\" data, i.e. there is not a strong, qualitatively important relationship among the features. These structures are typically not well-approximated by many rectangular partitions. I'm not sure how to best summarize my meaning here short of giving examples, but if your data live in a time series, or are a series of images, or live on a graph, or have some other obvious structure, the random forest will have a very hard time recognizing that. I have no doubt that researchers have developed variations on the method to attempt to accommodate these situations, but a vanilla random forest won't necessarily pick up on these structures in a helpful way. The good news is that you typically know when this is the case, i.e. you know you have images, a time-series or a graph to work with, so you can immediately apply a method more appropriate to that type of data.","Display_name":"Sycorax","Creater_id":22311,"Start_date":"2016-08-08 06:33:21","Question_id":112148}
{"_id":{"$oid":"5837a581a05283111e4d5b6d"},"Last_activity":"2016-08-08 04:18:49","Creator_reputation":4186,"Question_score":9,"Answer_content":"Sharp corners.  Exactness.They use diffusion methods.  They fit lumpy things well.  They do not fit elaborate and highly detailed things well when the sample size is low.  I would imagine that they do not do well on multivariate time-series data - when something over here depends on that one thing over there a distance.Gradient boosted forests might fit or over-fit, but can get substantially lower error for the same data.  \"Leathermen\" do not exist.  There are no \"silver bullets\".  There are toolboxes.  Know your tools, and take good care of them so they can take care of you.  Be wary of \"when you are a hammer, then every problem looks like a nail\" especially when you do not have a dense library in your toolbox.  Until you know the problem well, it is easy to imagine anything might solve it, or your favorite tool might solve it.  Wisdom suggests getting deep in understanding the problem, and being very familiar with your tools.Added:If you have enough compute resources or time margin to use something else.  The RF is not only fast to train, but fast to execute.  A very deep boosted structure is less of that.  You have to have the overhead to support that.","Display_name":"EngrStudent","Creater_id":22452,"Start_date":"2014-08-16 14:39:24","Question_id":112148}
{"_id":{"$oid":"5837a581a05283111e4d5b6e"},"Last_activity":"2014-08-16 14:13:13","Creator_reputation":64,"Question_score":3,"Answer_content":"This is the first time I actually answer a question, so do not pin me down on it .. but I do think I can answer your question:If you are indeed only interested in model performance and not in thing like interpretability random forest are indeed often a very good learning algorithm, but do perform slightly worse in the following cases:1.) When the dimensionality (number of features) is very high with respect to the number of training samples, in those cases a regularized linear regression or SVM would be better.2.) In the case there are higher order representations/convolutional structures in the data, like e.g. in computer vision problems. In those computer vision cases a convolutional neural network will outperform a random forest (In general if there is knowledge one can incorporate into the learning that is a better thing).That being said random forest are a very good starting point. One of the person I admire for his Machine Learning skills always starts with learning a random forest and a regularized linear regressor.However, if you want the best possible performance I believe nowadays neural networks aka. Deep Learning is looking like a very attractive approach. More and more winners on data-challenge websites like Kaggle use Deep Learning models for the competition. Another pro of neural networks is that they can handle very large numbers of samples (\u003e10^6 one can train them using stochastic gradient descend, feeding bits of data at a time).Personally I find this a very attractive pro for Deep Learning.","Display_name":"user3025665","Creater_id":54024,"Start_date":"2014-08-16 14:13:13","Question_id":112148}
{"_id":{"$oid":"5837a582a05283111e4d5b7b"},"Last_activity":"2016-08-05 08:09:21","Creator_reputation":324,"Question_score":9,"Answer_content":"All data can be found here.  Each value in the table represents the probability that given a 25-person sample from that location and birth year, 5 of them will share a name.Method: I used the Binomial PDF on on each name to find the probability that any given 25-person class would have 5 people who shared a name:n = class sizek = 5,6,...,n p_i = (# of name[i]'s) / (total # of kids)P_n(5+\\ kids\\ share\\ name) = \\sum_{\\forall\\ names}\\sum_{k=5}^n{n \\choose k}p_i^k(1-p_i)^{n-k} For example, if there are 4,000,000 total kids, and 21,393 Emily's, then the probability that there are 5 Emily's in any given class with 25 students is Binomial(25, 5, 0.0053) = 0.0000002.  Summing over all names does not give an exact answer, because by the Inclusion/Exclusion Principle, we must also account for the possibility of having multiple groups of 5 people who share names.  However, since these probabilities are for all practical purposes nearly zero, I've assumed them to be negligible, and thus .Update: As many people pointed out, there is considerable variance over time, and between states.  So I ran the same program, on a STATE BY STATE basis, and over time.  Here are the results (nation-wide probability is red, individual states are black):Interestingly, Vermont (my home state) has been consistently one of the most likely places for this to happen for the past several decades. ","Display_name":"David C","Creater_id":96030,"Start_date":"2016-08-03 18:48:31","Question_id":227139}
{"_id":{"$oid":"5837a582a05283111e4d5b7c"},"Last_activity":"2016-08-04 08:25:13","Creator_reputation":101,"Question_score":0,"Answer_content":"please see the following Python-script for Python2.Answer is inspired by David C's answer.My final answer would be, the probability of finding at least five Jacobs in one class, with Jacob being the most probable name according to the data from https://www.ssa.gov/oact/babynames/limits.html \"National Data\" from 2006.The probability is calculated according to a binomial distribution with Jacob-Probability being the probability of success. import pandas as pdfrom scipy.stats import binomdata = pd.read_csv(r\"yob2006.txt\", header=None, names=[\"Name\", \"Sex\", \"Count\"])# count of children in the dataset:sumCount = data.Count.sum()# do calculation for every name:for i, row in data.iterrows():    # relative counts of each name being interpreted as probabily of occurrence    data.loc[i, \"probability\"] = data.loc[i, \"Count\"]/float(sumCount)    # Probabilites being five or more children with that name in a class of size n=25,50 or 100    data.loc[i, \"atleast5_class25\"] = 1 - binom.cdf(4,25,data.loc[i, \"probability\"])    data.loc[i, \"atleast5_class50\"] = 1 - binom.cdf(4,50,data.loc[i, \"probability\"])    data.loc[i, \"atleast5_class100\"] = 1 - binom.cdf(4,100,data.loc[i, \"probability\"])maxP25 = data[\"atleast5_class25\"].max()maxP50 = data[\"atleast5_class50\"].max()maxP100 = data[\"atleast5_class100\"].max()print (\"\"\"Max. probability for at least five kids with same name out of 25: {:.2} for name {}\"\"\"   .format(maxP25, data.loc[data.atleast5_class25==maxP25,\"Name\"].values[0]))printprint (\"\"\"Max. probability for at least five kids with same name out of 50: {:.2} for name {}, of course.\"\"\"   .format(maxP50, data.loc[data.atleast5_class50==maxP50,\"Name\"].values[0]))printprint (\"\"\"Max. probability for at least five kids with same name out of 100: {:.2} for name {}, of course.\"\"\"   .format(maxP100, data.loc[data.atleast5_class100==maxP100,\"Name\"].values[0]))Max. probability for at least five kids with same name out of 25: 4.7e-07 for name JacobMax. probability for at least fivekids with same name out of 50: 1.6e-05 for name Jacob, of course.Max. probability for at least five kids with same name out of 100: 0.00045 for name Jacob, of course.By a factor of 10 same result as David C's. Thanks.(My answer does not sum all the names, should may be discussed)","Display_name":"feinmann","Creater_id":78825,"Start_date":"2016-08-04 08:18:28","Question_id":227139}
{"_id":{"$oid":"5837a582a05283111e4d5b89"},"Last_activity":"2016-08-08 06:43:10","Creator_reputation":1557,"Question_score":1,"Answer_content":"In terms of normalizing the probability mass function, it should be a relatively simple fix.Define your current probability mass function . Suppose that this sum of all probabilities  equals some value , where  in this case is greater than 1. Note that when , this is not a valid probability function.Define a new probability function  for all values . Because you are normalizing by the sum of probabilities , the new probability function  will now sum to 1 and, provided it satisfies all other Kolmogorov Axioms, will be a valid probability function.(What I described here will actually be applicable for any value of , so  need not be greater than 1 in order for this to work.)As for the missing/lost values, that will take some additional treatment. If you are comfortable proceeding without that lost information and assuming that, for example, the proportion of 1s you see across the three datasets is an accurate portrayal of the true proportion of 1s you expect to see in the aggregated dataset, then this normalization technique is all you need to do. (Note that this should be true for the proportion of 1s, 2s, and so on.)If you do not believe these values to be missing completely at random (and I strongly doubt that they are MCAR), then you may need to look into survival analysis or missing data imputation to estimate or impute these values before estimating the p.m.f. If you proceed with a p.m.f. ignoring the missingness or the censoring mechanism, then your p.m.f. will likely be an inaccurate representation of the true distribution of the variable of interest. (It follows that if your p.m.f. is an inaccurate representation of the variable of interest, then your c.d.f. will also be an inaccurate representation.)","Display_name":"Matt Brems","Creater_id":92737,"Start_date":"2016-08-08 06:43:10","Question_id":228778}
{"_id":{"$oid":"5837a582a05283111e4d5b96"},"Last_activity":"2016-08-08 06:31:50","Creator_reputation":275,"Question_score":0,"Answer_content":"Dummy variables are artificial variables created to represent attributes, usually with two or more distinct categories (in your case 13). At first, if your model is correctly specified the firm, industry, and macro factors will capture all the relevant variation. The macro effect will be controlled by the macro factors and your dummies will be not significant (redundant). But what if your macro factors are incomplete (wrong model specification), some dummies will show significance. Then, in your context is a matter of trusting in your model specification to consider or not the dummies. I will choose dummies for years where there is information that a macro effect is not being capture by the other variables. ","Display_name":"Robert","Creater_id":77852,"Start_date":"2016-08-08 06:31:50","Question_id":228768}
{"_id":{"$oid":"5837a582a05283111e4d5ba3"},"Last_activity":"2016-08-08 06:13:36","Creator_reputation":755,"Question_score":3,"Answer_content":"Let .  Conditional on the number of occurences , the arrival times  are known to have the same distribution as the order statstics of  iid unif random variables.  Hence, the likelihood becomes\\begin{align}L(\\lambda,t) \u0026amp;= P(N=n) f(t_1,t_2,\\dots,t_N|N=n) \\\\\u0026amp;= \\frac{e^{-\\lambda t}(\\lambda t)^n}{n!}\\frac{n!}{t^n} \\\\\u0026amp;= e^{-\\lambda t}\\lambda^n.\\end{align}for  and zero elsewhere.  This is maximised for  and .  These MLEs don't exist if there are no occurrences , however.  Conditional on , again using the fact that  can be viewed as an order statistic (the maximum) of  iid unif random variables, .  Hence, the estimator  is unbiased for  conditional on  and hence also conditional on .   A reasonable frequentist estimator of  might be  but this does not have finite expectation when  so assessing its bias is even more troublesome.  Bayesian inference using independent, non-informative scale priors on  and  on the other hand leads to a posterior f(\\lambda,t|t_1,\\dots,t_N) \\propto e^{-\\lambda t}\\lambda^{n-1}t^{-1}.for .  Integrating out , the marginal posterior of  becomesf(t|t_1,\\dots,t_N) = \\frac{n t_n^n}{t^{n+1}}, t\u0026gt;t_n,and the posterior mean .  A -credible interval for  is given by .The marginal posterior of ,\\begin{align}f(\\lambda|t_1,\\dots,t_N) \u0026amp;\\propto \\int_{t_\\text{max}}^\\infty e^{-\\lambda t}\\lambda^{n-1}t^{-1} dt \\\\\u0026amp;= \\lambda^{n-1}\\Gamma(0,\\lambda t_n)\\end{align}where  is the incomplete gamma function.","Display_name":"Jarle Tufto","Creater_id":77222,"Start_date":"2016-08-08 04:56:06","Question_id":228661}
{"_id":{"$oid":"5837a582a05283111e4d5bb0"},"Last_activity":"2016-08-08 06:12:03","Creator_reputation":19131,"Question_score":5,"Answer_content":"I'll give you the exemplary case of how p-values should be used and reported. It's a very recent report on the search of a mysterious particle on Large Hadron Collider(LHC) in CERN.A few months ago there was a lot of excited chatter in high energy physics circles about a possibility that a large particle was detected on LHC. Remember this was after Higgs boson discovery. Here's the excerpt from the paper \"Search for resonances decaying to photon pairs in 3.2 fb−1 of p pcollisions at √s = 13 TeV with the ATLAS detector\" by The ATLAS Collaboration Dec 15 2015 and my comments follow:What they're saying here is that the event counts exceed what the Standard Model predicts. The Figure below from the paper shows the p-values of excess events as a function of a mass of a particle. You see how p-value dives around 750 GeV. So, they're saying that there's a possibility that a new particle is detected with a mass equal to 750 Giga eV. The p-values on the figure are calculated as \"local\". The global p-values are much higher. That's not important for our conversation though. What's important is that p-values are not yet \"low enough\" for physicists to declare a find, but \"low enough\" to get excited. So, they're planning to keep counting, and hoping that that p-values will further decrease.Zoom a few months forward to Aug 2016, Chicago, a conference on HEP. There was a new report presented \"Search for resonant production of high mass photon pairs using 12.9 fb−1 of proton-proton collisions at √ s = 13 TeV and combined interpretation of searches at 8 and 13 TeV\" by The CMS Collaboration this time. Here's the excerpts with my comments again:So, the guys continued collecting events, and now that blip of excess events at 750 GeV is gone. The figure below from the paper shows p-values, and you can see how p-value increased compared to the first report. So, they sadly conclude that no particle is detected at 750 GeV.I think this is how p-values are supposed to be used. They totally make a sense, and they clearly work. I think the reason is that frequentist approaches are inherently natural in physics. There's nothing subjective about particle scattering. You collect a a sample large enough and you get a clear signal if it's there.If you're really into how exactly p-values are calculated here, read this paper: \"Asymptotic formulae for likelihood-based tests of new physics\" by  Cowan et al","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-06-07 11:38:54","Question_id":201146}
{"_id":{"$oid":"5837a582a05283111e4d5bb1"},"Last_activity":"2016-06-07 11:25:24","Creator_reputation":29,"Question_score":1,"Answer_content":"I can think of example in which p-values are useful, in Experimental High Energy Physics. See Fig.1 This plot is taken from this paper: Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHCIn this Fig, the p-value is shown versus the mass of an hypothetical particle. The null hypothesis denotes the compatibility of the observation with a continuous background. The large () deviation at m GeV was the first evidence and discovery of a new particle.This earned François Englert, Peter Higgs the Nobel Prize in Physics in 2013.","Display_name":"Nicolas Gutierrez","Creater_id":119128,"Start_date":"2016-06-07 09:15:17","Question_id":201146}
{"_id":{"$oid":"5837a582a05283111e4d5bb2"},"Last_activity":"2016-06-07 08:59:48","Creator_reputation":21,"Question_score":2,"Answer_content":"Error rates control is similar to quality control in production. A robot in a production line has a rule for deciding that a part is defective which guarantees not to exceed a specified rate of defective parts that go through undetected. Similarly, an agency that makes decisions for drug approval based on \"honest\" P-values has a way to keep the rate of false rejections at a controlled level, by definition via the frequentist long-run construction of tests. Here, \"honest\" means absence of uncontrolled biases, hidden selections, etc.However, neither the robot, nor the agency have a personal stake in any particular drug or a part that goes through the assembly conveyor. In science, on the other hand, we, as individual investigators care most about the particular hypothesis we study, rather than about the proportion of spurious claims in our favorite journal we submit to. Neither the P-value magnitude nor the bounds of a confidence interval (CI) refer directly to our question about the credibility of what we report. When we construct the CI bounds, we should be saying that the only meaning of the two numbers is that if other scientists do the same kind of CI computation in their studies, the 95% or whatever coverage will be maintained over various studies as a whole.In this light, I find it ironic that P-values are being \"banned\" by journals, considering that in the thick of replicability crisis they are of more value to journal editors than to researchers submitting their papers, as a practical way of keeping the rate of spurious findings reported by a journal at bay, in the long run. P-values are good at filtering, or as IJ Good wrote, they are good for protecting statistician's rear end, but not so much the rear end of the client.P.S. I'm a huge fan of Benjamini and Hochberg's idea of taking the unconditional expectation across studies with multiple tests. Under the global \"null\", the \"frequentist\" FDR is still controlled - studies with one or more rejections pop up in a journal at a controlled rate, although, in this case, any study where some rejections have been actually made has the proportion of false rejections that is equal to one.","Display_name":"D.Z.","Creater_id":119190,"Start_date":"2016-06-07 08:54:33","Question_id":201146}
{"_id":{"$oid":"5837a582a05283111e4d5bb3"},"Last_activity":"2016-04-07 04:25:32","Creator_reputation":139,"Question_score":4,"Answer_content":"Forgive my sarcasm, but one obvious good example of the utility of p-values is in getting published. I had one experimenter approach me for producing a p-value... he had introduced a transgene in a single plant to improve growth. From that single plant he produced multiple clones and chose the largest clone, an example where the entire population is enumerated. His question, the reviewer wants to see a p-value that this clone is the largest. I mentioned that there is not any need for statistics in this case as he had the entire population at hand, but to no avail. More seriously, in my humble opinion, from an academic perspective i find these discussion interesting and stimulating, just like the frequentist vs Bayesian debates from a few years ago. It brings out the differing perspectives of the best minds in this field and illuminates the many assumptions/pitfalls associated with the methodology thats not generally readily accesible. In practice, I think that rather than arguing about the best approach and replacing one flawed yardstick with another, as has been suggested before elsewhere, for me it is rather a revelation of an underlying systemic problem and the focus should be on trying to find optimal solutions. For instance, one could present situations where p-values and CI complement each other and circumstance wherein one is more reliable than the other. In the grand scheme of things, I understand that all inferential tools have their own shortcomings which need to be understood in any application so as to not stymie progress towards the ultimate goal.. the deeper understanding of the system of study.","Display_name":"ashokragavendran","Creater_id":109477,"Start_date":"2016-04-07 04:25:32","Question_id":201146}
{"_id":{"$oid":"5837a582a05283111e4d5bb4"},"Last_activity":"2016-03-29 10:55:40","Creator_reputation":2486,"Question_score":1,"Answer_content":"I agree with Matt that p-values are useful when the null hypothesis is true. The simplest example I can think of is testing a random number generator. If the generator is working correctly, you can use any appropriate sample size of realizations and when testing the fit over many samples, the p-values should have a uniform distribution. If they do, this is good evidence for a correct implementation. If they don't, you know you have made an error somewhere. Other similar situations occur when you know a statistic or random variable should have a certain distribution (again, the most obvious context is simulation). If the p-values are uniform, you have found support for a valid implementation. If not, you know you have a problem somewhere in your code. ","Display_name":"soakley","Creater_id":24073,"Start_date":"2016-03-29 10:55:40","Question_id":201146}
{"_id":{"$oid":"5837a582a05283111e4d5bb5"},"Last_activity":"2016-03-25 07:35:39","Creator_reputation":30005,"Question_score":40,"Answer_content":"  With large samples, significance tests pounce on tiny, unimportant departures from the null hypothesis.The logic here is that if somebody reports highly significant , then from this number alone we cannot say if the effect is large and important or irrelevantly tiny (as can happen with large ). I find this argument strange and cannot connect to it at all, because I have never seen a study that would report a -value without reporting [some equivalent of] effect size. Studies that I read would e.g. say (and usually show on a figure) that group A had such and such mean, group B had such and such mean and they were significantly different with such and such -value. I can obviously judge for myself if the difference between A and B is large or small.Update: @RobinEkman pointed me to several highly-cited studies by Ziliak \u0026amp; McCloskey (1996, 2004) who observed that the majority of the economics papers trumpet \"statistical significance\" of some effects without paying much attention to the effect size and its \"practical significance\" (which, Z\u0026amp;MS argue, can often be minuscule). This is clearly bad practice. Still, as @MatteoS explained, the effect sizes (regression estimates) are always reported.  Almost no null hypotheses are true in the real world, so performing a significance test on them is absurd and bizarre.This concern is also often voiced, but here again I cannot really connect to it. It is important to realize that researchers do not increase their  ad infinitum. In the branch of neuroscience that I am familiar with, people will do experiments with  or maybe , say, rats. If there is no effect to be seen then the conclusion is that the effect is not large enough to be interesting. Nobody I know would go on breeding, training, recording, and sacrificing  rats to show that there is some statistically significant but tiny effect. And whereas it might be true that almost no real effects are exactly zero, it is certainly true that many many real effects are small enough to be detected with reasonable sample sizes that reasonable researchers are actually using, exercising their good judgment. Himself, Norm Matloff suggests to use confidence intervals instead of -values because they show the effect size. Confidence intervals are good, but notice one disadvantage of a confidence interval as compared to the -value: confidence interval is reported for one particular coverage value, e.g. . Seeing a  confidence interval does not tell me how broad a  confidence interval would be. But one single -value can be compared with any  and different readers can have different alphas in mind.In other words, I think that for somebody who likes to use confidence intervals, a -value is a useful and meaningful additional statistic to report.I would like to give a long quote about the practical usefulness of -values from my favorite blogger Scott Alexander; he is not a statistician (he is a psychiatrist) but has lots of experience with reading psychological/medical literature and scrutinizing the statistics therein. The quote is from his blog post on the fake chocolate study which I highly recommend. Emphasis mine.  [...] But suppose we're not allowed to do -values. All I do is tell you \"Yeah, there was a study with fifteen people that found chocolate helped with insulin resistance\" and you laugh in my face. Effect size is supposed to help with that. But suppose I tell you \"There was a study with fifteen people that found chocolate helped with insulin resistance. The effect size was .\" I don't have any intuition at all for whether or not that's consistent with random noise. Do you? Okay, then they say we’re supposed to report confidence intervals. The effect size was , with  confidence interval of . Okay. So I check the lower bound of the confidence interval, I see it’s different from zero. But now I’m not transcending the -value. I’m just using the p-value by doing a sort of kludgy calculation of it myself – “ confidence interval does not include zero” is the same as “-value is less than ”.    (Imagine that, although I know the  confidence interval doesn’t include zero, I start wondering if the  confidence interval does. If only there were some statistic that would give me this information!)    But wouldn’t getting rid of -values prevent “-hacking”? Maybe, but it would just give way to “d-hacking”. You don’t think you could test for twenty different metabolic parameters and only report the one with the highest effect size? The only difference would be that p-hacking is completely transparent – if you do twenty tests and report a  of , I know you’re an idiot – but d-hacking would be inscrutable. If you do twenty tests and report that one of them got a , is that impressive? [...]    But wouldn’t switching from -values to effect sizes prevent people from making a big deal about tiny effects that are nevertheless statistically significant? Yes, but sometimes we want to make a big deal about tiny effects that are nevertheless statistically significant! Suppose that Coca-Cola is testing a new product additive, and finds in large epidemiological studies that it causes one extra death per hundred thousand people per year. That’s an effect size of approximately zero, but it might still be statistically significant. And since about a billion people worldwide drink Coke each year, that’s a ten thousand deaths. If Coke said “Nope, effect size too small, not worth thinking about”, they would kill almost two milli-Hitlers worth of people.For some further discussion of various alternatives to -values (including Bayesian ones), see my answer in ASA discusses limitations of -values - what are the alternatives?","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-03-11 08:22:25","Question_id":201146}
{"_id":{"$oid":"5837a582a05283111e4d5bb6"},"Last_activity":"2016-03-12 12:30:34","Creator_reputation":7090,"Question_score":26,"Answer_content":"I take great offense at the following two ideas:    With large samples, significance tests pounce on tiny, unimportant departures from the null hypothesis.  Almost no null hypotheses are true in the real world, so performing a significance test on them is absurd and bizarre.  It is such a strawman argument about p-values. The very foundational problem that motivated the development of statistics comes from seeing a trend and wanting to know whether what we see is by chance, or representative of a systematic trend. With that in mind, it is true that we, as statisticians, do not typically believe that a null-hypothesis is true (i.e. , where  is the mean difference in some measurement between two groups). However, with two sided tests, we don't know which alternative hypothesis is true! In a two sided test, we may be willing to say that we are 100% sure that  before seeing the data. But we do not know whether  or . So if we run our experiment and conclude that , we have rejected  (as Matloff might say; useless conclusion) but more importantly, we have also rejected  (I say; useful conclusion). As @amoeba pointed out, this also applies to one sided test that have the potential to be two sided, such as testing whether a drug has a positive effect. It's true that this doesn't tell you the magnitude of the effect. But it does tell you the direction of the effect. So let's not put the cart before the horse; before I start drawing conclusions about the magnitude of the effect, I want to be confident I've got the direction of the effect correct! Similarly, the argument that \"p-values pounce on tiny, unimportant effects\" seems quite flawed to me. If you think of a p-value as a measure of how much the data supports the direction of your conclusion, then of course you want it to pick up small effects when the sample size is large enough. To say this means they are not useful is very strange to me: are these fields of research that have suffered from p-values the same ones that have so much data they have no need to assess the reliability of their estimates? Similarly, if your issues is really that p-values \"pounce on tiny effect sizes\", then you can simply test the hypotheses  and  (assuming you believe 1 to be the minimal important effect size). This is done often in clinical trials. To further illustrate this, suppose we just looked at confidence intervals and discarded p-values. What is the first thing you would check in the confidence interval? Whether the effect was strictly positive (or negative) before taking the results too seriously. As such, even without p-values, we would informally be doing hypothesis testing. Finally, in regards to the OP/Matloff's request, \"Give a convincing argument of p-values being significantly better\", I think question is a little awkward. I say this because depending on your view, it automatically answers itself (\"give me one concrete example where testing a hypothesis is better than not testing them\"). However, a special case that I think is almost undeniable is that of RNAseq data. In this case, we are typically looking at the expression level of RNA in two different groups (i.e. diseased, controls) and trying to find genes that are differentially expressed in the two groups. In this case, the effect size itself is not even really meaningful. This is because the expression levels of different genes vary so wildly that for some genes, having 2x higher expression doesn't mean anything, while on other tightly regulated genes, 1.2x higher expression is fatal. So the actual magnitude of the effect size is actually somewhat uninteresting when first comparing the groups. But you really, really want to know if the expression of the gene changes between the groups and direction of the change! Furthermore, it's much more difficult to address the issues of multiple comparisons (for which you may be doing 20,000 of them in a single run) with confidence intervals than it is with p-values. ","Display_name":"Cliff AB","Creater_id":76981,"Start_date":"2016-03-11 15:36:19","Question_id":201146}
{"_id":{"$oid":"5837a582a05283111e4d5bb7"},"Last_activity":"2016-03-12 00:20:51","Creator_reputation":1487,"Question_score":2,"Answer_content":"The other explanations are all fine, I just wanted to try and give a brief and direct answer to the question that popped into my head.Checking Covariate Imbalance in Randomized ExperimentsYour second claim (about unrealistic null hypotheses) is not true when we are checking covariate balance in randomized experiments where we know the randomization was done properly. In this case, we know that the null hypothesis is true. If we get a significant difference between treatment and control group on some covariate - after controlling for multiple comparisons, of course - then that tells us that we got a \"bad draw\" in the randomization and we maybe shouldn't trust the causal estimate as much. This is because we might think that our treatment effect estimates from this particular \"bad draw\" randomization are further away from the true treatment effects than estimates obtained from a \"good draw.\"I think this is a perfect use of p-values. It uses the definition of p-value: the probability of getting a value as or more extreme given the null hypothesis. If the result is highly unlikely, then we did in fact get a \"bad draw.\"Balance tables/statistics are also common when using observational data to try and make causal inferences (e.g., matching, natural experiments). Although in these cases balance tables are far from sufficient to justify a \"causal\" label to the estimates.","Display_name":"Matt","Creater_id":44764,"Start_date":"2016-03-11 23:01:52","Question_id":201146}
{"_id":{"$oid":"5837a582a05283111e4d5bc4"},"Last_activity":"2016-08-08 05:27:26","Creator_reputation":27820,"Question_score":2,"Answer_content":"From the look of the code, it implies that the whole model fitting procedure is being cross-validated for each model, which is indeed the right way to go about it.  Cross-validation is best viewed as a method for evaluating the performance of a procedure for fitting a model, not the fitted model itself.  So if you perform feature selection, then that needs to be performed independently in each fold of the cross-validation, if you tune hyper-parameters, then they need to be tuned independently in each fold of the cross-validation.  It is important to remember however that the cross-validation score of the best model will be an optimistically biased estimate of the performance of the final system as it has been directly optimised to choose the best model.","Display_name":"Dikran Marsupial","Creater_id":887,"Start_date":"2016-08-08 05:27:26","Question_id":228774}
{"_id":{"$oid":"5837a582a05283111e4d5bd4"},"Last_activity":"2016-08-08 04:20:29","Creator_reputation":4186,"Question_score":1,"Answer_content":"Can you not make MatLab call the WEKA and pass data off to it?For very high dimensional data I find the work of Eugene Tuv to be very useful.  Random forests of Gradient Boosted Trees can whittle 100k rows and 100k columns to something useful in a very short time.http://www.stanford.edu/class/ee392m/Lecture3Tuv.pdfhttp://www.journalogy.net/Publication/6491785/feature-selection-with-ensembles-artificial-variables-and-redundancyelimination","Display_name":"EngrStudent","Creater_id":22452,"Start_date":"2013-03-24 22:18:42","Question_id":53042}
{"_id":{"$oid":"5837a582a05283111e4d5bd5"},"Last_activity":"2014-03-29 13:01:50","Creator_reputation":464,"Question_score":0,"Answer_content":"That's a lot of features to input into a model.  Have you thought about redundancy of information within the features?  I would suggest dimensional reduction using e.g. PCA or non-linear manifold learning (diffusion maps, Laplacian eigenmaps, locally preserving projections, Sammon mapping, or even SOM, etc.).  Would not some of your features also be noisy with little informativeness?","Display_name":"wrtsvkrfm","Creater_id":32398,"Start_date":"2014-03-29 13:01:50","Question_id":53042}
{"_id":{"$oid":"5837a582a05283111e4d5be2"},"Last_activity":"2016-08-08 04:18:11","Creator_reputation":56,"Question_score":0,"Answer_content":"The purpose of factor analysis is evaluating the relationship between observed variables.Exploratory factor analysis is usually used to find the underline structure of the observed variables and identifying the latent structs.PCA is mathematical tool used for finding such underline structure between variables. PCA is subjected to scaling and actual relation between variables(such as correlation).I wouldnt linger on filtering variables pre factor analysis, mainly because the math behind most such process(PCA specifically) handles it well.","Display_name":"DaFanat","Creater_id":123020,"Start_date":"2016-08-08 04:18:11","Question_id":228766}
{"_id":{"$oid":"5837a582a05283111e4d5bef"},"Last_activity":"2016-08-08 04:03:03","Creator_reputation":68,"Question_score":0,"Answer_content":"I was able to track down the correct formula which is essentially the same as above except 1.00 in the denominator is replaced with 3.29, thus:for a factor correlation metric (standardized) from a logit-link discrimination parameter, where  is the discrimination parameter.","Display_name":"Jhaltiga68","Creater_id":101183,"Start_date":"2016-08-08 00:20:29","Question_id":228629}
{"_id":{"$oid":"5837a582a05283111e4d5bfc"},"Last_activity":"2016-05-10 01:04:07","Creator_reputation":20442,"Question_score":2,"Answer_content":"I added the variance-decomposition tag to your question. Here is part of its tag wiki: One common method is to add regressors to the model one by one and record the increase in  as each regressor is added. Since this value depends on the regressors already in the model, one needs to do this for every possible order in which regressors can enter the model, and then average over orders. This is feasible for small models but becomes computationally prohibitive for large models, since the number of possible orders is  for  predictors.Grömping (2007, The American Statistician) gives an overview and pointers to literature in the context of assessing variable importance.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2016-05-10 01:04:07","Question_id":60872}
{"_id":{"$oid":"5837a582a05283111e4d5bfd"},"Last_activity":"2013-07-02 08:39:42","Creator_reputation":27883,"Question_score":6,"Answer_content":"In addition to John's answer, you may wish to obtain the squared semi-partial correlations for each predictor. Uncorrelated predictors: If the predictors are orthogonal (i.e., uncorrelated), then the squared semi-partial correlations will be the same as the squared zero-order correlations. Correlated predictors: If the predictors are correlated, then the squared semi-partial correlation will represent the unique variance explained by a given predictor. In this case, the sum of squared semi-partial correlations will be less than . This remaining explained variance will represent variance explained by more than one variable.If you are looking for an R function there is spcor() in the ppcor package.You might also want to consider the broader topic of evaluating variable importance in multiple regression (e.g., see this page about the relaimpo package).","Display_name":"Jeromy Anglim","Creater_id":183,"Start_date":"2013-07-02 08:39:42","Question_id":60872}
{"_id":{"$oid":"5837a582a05283111e4d5bfe"},"Last_activity":"2013-06-04 10:26:56","Creator_reputation":16279,"Question_score":4,"Answer_content":"You can just get the two separate correlations and square them or run two separate models and get the R^2.  They will only sum up if the predictors are orthogonal.","Display_name":"John","Creater_id":601,"Start_date":"2013-06-04 10:26:56","Question_id":60872}
{"_id":{"$oid":"5837a582a05283111e4d5c0f"},"Last_activity":"2016-08-08 02:34:47","Creator_reputation":8893,"Question_score":2,"Answer_content":"The most straightforward way to solve a constrained regression problem is simply to re-express it as an optimization problem. Indeed most routines solving constrained least squares  pose them as quadratic programming tasks (rather than linear) but this is a bit of an overkill for the problem you pose, a standard introductory reference would be Practical Optimization by Gill, Murray, and Wright, see Sect. 4.7 on Methods for Sum of Squares, after that Chapt. 5 then deals with Linear constraints. For this simple a simple optimization routine like R's optim is adequate. You simply define your cost function myRSS:myRSS \u0026lt;- function(X,y,beta){ return( sum( (y - X%*%beta)^2 ) ) }And then your pass it to your favourite solver (here optim's L-BGFS-B routine):bfgsOptimConst = optim(myRSS, par = c(1,1), X=newMatX, y= newVecY,                    method = 'L-BFGS-B',                    lower = c(c_low,m_low), uppper = c(c_high, m_high))Just notice that you need the matrix X and the vectors y and beta to be of the correct dimensions (,  and  respectively for your problem at hand), you also need to provide a feasible solution as a starting point  for your solution parameters. (I used the  but this was just for illustration.)","Display_name":"usεr11852","Creater_id":11852,"Start_date":"2016-08-08 02:34:47","Question_id":228750}
{"_id":{"$oid":"5837a582a05283111e4d5c1e"},"Last_activity":"2016-08-08 02:12:56","Creator_reputation":388,"Question_score":1,"Answer_content":"Yes, you are reading the diagram correctly. Regarding your question in the comments, \"if darker color area (dark-red in my example) means a and b has higher correlation, or not?\": Darker color area does not means a higher correlation. It just means that more data are observed in this area.To make this last point more clear, I generate two independent and uncorrelated variables x and y. As the are uncorrelated, the dark area cannot show correlation. (PS: I think axis labels are a must have for statistical graphs, so a added them (and a title))library(MASS)set.seed(123)x = rnorm(1000, mean = 10)y = rnorm(1000, mean = 0)density \u0026lt;- kde2d(x, y, n = 100)filled.contour(density,            color.palette = colorRampPalette(c('white', 'blue', 'yellow', 'red', 'darkred')),           xlab = \"x\", ylab = \"y\", main = \"contour plot of x and y\")","Display_name":"Qaswed","Creater_id":112892,"Start_date":"2016-08-08 02:05:43","Question_id":228528}
{"_id":{"$oid":"5837a582a05283111e4d5c29"},"Last_activity":"2016-08-08 02:01:32","Creator_reputation":8893,"Question_score":2,"Answer_content":"Yes, : is the interaction operator. For example it is specifically mentioned as such in the lme4: Mixed-effects modeling with R book draft by D. Bates (see Sect. 2.2.1.1 on Nested Factors)It effectively says that instead of having an intercept varying among schools and among pupils within schools ((1|schools)+(1|school:pupil)), you have intercept variations only among pupils within schools. It is a rather restrictive model and I think is hard to justify because you would expect at least some minimal effect to be due to school as a factor.The  interpretation would be that you are having an intercept varying among schools, among pupils and among pupils within schools. I strongly suspect this would be an overdetermined model. It could potentially work if you have multiple individual students who attended multiple schools in your sample but even then it would be probably computationally intractable. Conceptually, you would have to say that you are able to differentiate between the variation due to school, pupil and school:pupil; which I think is a bit of a tall order for not exceptionally large and well-behaved samples.","Display_name":"usεr11852","Creater_id":11852,"Start_date":"2016-08-07 14:56:31","Question_id":228574}
{"_id":{"$oid":"5837a582a05283111e4d5c38"},"Last_activity":"2016-08-08 01:19:36","Creator_reputation":388,"Question_score":0,"Answer_content":"You can use the predict function. Try:set.seed(123)x \u0026lt;- 1:10y \u0026lt;- -2 + 3 * x + rnorm(10)our_data \u0026lt;- data.frame(y = y, x = x)our_model \u0026lt;- lm(y ~ x, data = our_data)predict(our_model, newdata =  data.frame(x = 20))","Display_name":"Qaswed","Creater_id":112892,"Start_date":"2016-08-08 01:19:36","Question_id":228729}
{"_id":{"$oid":"5837a582a05283111e4d5c39"},"Last_activity":"2016-08-07 22:17:10","Creator_reputation":8337,"Question_score":0,"Answer_content":"Once your model and its parameters are fixed, there's only one way to do it: plug in the covariate values of the point you want to extrapolate at.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-07 22:17:10","Question_id":228729}
{"_id":{"$oid":"5837a582a05283111e4d5c48"},"Last_activity":"2016-08-07 22:03:34","Creator_reputation":56,"Question_score":1,"Answer_content":"Whichever way you go, you will still need to justify on what basis you (or an algorithm) set the number of clusters.There is an R package called optClust assesses a wide range of clustering algorithms and a set range of number of clusters.Here's a usage example from the documentation:library(optClust)## Obtain Datasetdata(arabid)## Analysis of Count Data using Internal and Stability Validation Measurescount1 \u0026lt;- optCluster(arabid, 2:4, clMethods = \"all\", countData = TRUE)summary(count1)This utilises the clValid and RankAggreg packages (among others). The clValid package has a several validation measures which each clustering algorithm and number of clusters is assessed against. The RankAggreg package then is used to \"combine these multiple rank lists together in some way to form asingle list that best represents the original rankings\" (Sekula, 2010, p 29) - giving a ranked list of algorithm and number of clusters. There is also the NbClust package that \"provide[s] an exhaustive list of validity indices [(30 in total)] to estimate the number of clusters in a data set\". Suitable for k-means or HAC.Example from documentation:data \u0026lt;- iris[, -5]diss_matrix \u0026lt;- dist(data, method = \"euclidean\", diag = FALSE)NbClust(data, diss = diss_matrix, distance = \"NULL\", min.nc = 2, max.nc = 10, method = \"complete\", index = \"alllong\")Coming back to my original point, both of these options have default parameters that may not suit your research question / dataset.You can have a look at the answers to this question for explanations of different methods for determining the number of clusters. There are also many other posts on the topic.","Display_name":"Simon","Creater_id":67446,"Start_date":"2016-08-07 22:03:34","Question_id":228718}
{"_id":{"$oid":"5837a582a05283111e4d5c57"},"Last_activity":"2016-08-07 21:42:20","Creator_reputation":8000,"Question_score":2,"Answer_content":"Addition and concatenation are special cases of  multiplication, where the weights are equal to 0 or 1. As a result, one can view using addition and concatenation as assumptions of what the network should be doing.E.g., in https://arxiv.org/abs/1606.03475, figure 1, we used concatenation to create the token emdeddings  from the characters as we want to motivate the higher layers to consider the information from both the forward character-based RNN and the backward character-based RNN.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-07 20:56:15","Question_id":226816}
{"_id":{"$oid":"5837a582a05283111e4d5c64"},"Last_activity":"2012-08-09 01:55:01","Creator_reputation":331,"Question_score":0,"Answer_content":"Lars and Glmnet give different solutions for the Lasso problem, as they use slightly different objective functions and different standartisations of the data. You can find details code for reproduction in the related question Why do Lars and Glmnet give different solutions for the Lasso problem?","Display_name":"Andre","Creater_id":13075,"Start_date":"2012-08-09 01:55:01","Question_id":7057}
{"_id":{"$oid":"5837a582a05283111e4d5c65"},"Last_activity":"2011-04-12 13:08:59","Creator_reputation":576,"Question_score":12,"Answer_content":"In my experience, LARS is faster for small problems, very sparse problems, or very 'wide' problems (much much more features than samples). Indeed, its computational cost is limited by the number of features selected, if you don't compute the full regularization path. On the other hand, for big problems, glmnet (coordinate descent optimization) is faster. Amongst other things, coordinate descent has a good data access pattern (memory-friendly) and it can benefit from redundancy in the data on very large datasets, as it converges with partial fits. In particular, it does not suffer from heavily correlated datasets. The conclusion that we (the core developers of the scikit-learn) have come too is that, if you do not have strong a priori knowledge of your data, you should rather use glmnet (or coordinate descent optimization, to talk about an algorithm rather than an implementation).Interesting benchmarks may be compared in Julien Mairal's thesis:http://www.di.ens.fr/~mairal/resources/pdf/phd_thesis.pdfSection 1.4, in particular 1.4.5 (page 22)Julien comes to slightly different conclusions, although his analysis of the problem is similar. I suspect this is because he was very much interested in very wide problems.","Display_name":"Gael Varoquaux","Creater_id":1265,"Start_date":"2011-04-12 12:43:01","Question_id":7057}
{"_id":{"$oid":"5837a582a05283111e4d5c66"},"Last_activity":"2011-02-10 13:30:18","Creator_reputation":3016,"Question_score":0,"Answer_content":"LASSO is non-unique in the case where multiple features have perfect collinearity.  Here's a simple thought experiment to prove it.  Let's say you have three random vectors , , .  You're trying to predict  from , .  Now assume  =  = .  An optimal LASSO solution would be , , where  is the effect of LASSO penalty.  However, also optimal would be , .","Display_name":"dsimcha","Creater_id":1347,"Start_date":"2011-02-10 12:19:24","Question_id":7057}
{"_id":{"$oid":"5837a582a05283111e4d5c75"},"Last_activity":"2016-08-07 17:41:00","Creator_reputation":6813,"Question_score":2,"Answer_content":"Hint: The confidence interval width and the p-value are related. For instance, if the p-value is bigger than 0.05, the 95% confidence interval (which is 1-0.05, not a coincidence) will include the null hypothesis value (120). If the p-value is smaller than 0.05, the 95% CI will exclude 120.Now here is an example, statement I: If we use a 96% confidence interval, any p-value smaller than 0.04 (1-.96) will lead to rejection of null in favor of concluding the mean is different from 120. Now, you actual p-value is 0.021, which is smaller than 0.04. Aka, the null hypothesis should be rejected. We also know if the null is to be rejected, the value to be compared against (120) should be outside of the confidence interval. So, statement I is wrong.","Display_name":"Penguin_Knight","Creater_id":13047,"Start_date":"2016-08-07 17:41:00","Question_id":228709}
{"_id":{"$oid":"5837a582a05283111e4d5c81"},"Last_activity":"2016-08-07 17:51:04","Creator_reputation":101,"Question_score":1,"Answer_content":"I'm curious, what makes you think it is not valid? I see no problem with transforming variables if you can justify the reason for the transformation. Making n transformations to an independent variable is equivalent to making 1 if the function is. For example, if  and  then .","Display_name":"dmanuge","Creater_id":43067,"Start_date":"2016-08-05 21:13:37","Question_id":228533}
{"_id":{"$oid":"5837a582a05283111e4d5c90"},"Last_activity":"2016-08-07 17:40:11","Creator_reputation":87,"Question_score":0,"Answer_content":"ok.. thank you.Let say i want to simulate the data based on this data set:y\u0026lt;-c(18.73,14.52,17.43,14.54,13.44,24.39,13.34,22.71,12.68,19.32,30.16,27.09,25.40,26.05,33.49,35.62,26.07,36.78,34.95,43.67)x1\u0026lt;-c(610,950,720,840,980,530,680,540,890,730,670,770,880,1000,760,590,910,650,810,500)x2\u0026lt;-c(1,1,3,2,1,1,3,3,2,2,1,3,3,2,2,2,3,3,1,2)fit\u0026lt;-lm(y~x1+x2)summary(fit)then i get the output:Call:lm(formula = y ~ x1 + x2)Residuals:     Min       1Q   Median       3Q      Max -13.2805  -7.5169  -0.9231   7.2556  12.8209 Coefficients:            Estimate Std. Error t value Pr(\u003e|t|)(Intercept) 42.85352   11.33229   3.782  0.00149 **x1          -0.02534    0.01293  -1.960  0.06662 . x2           0.33188    2.41657   0.137  0.89238Signif. codes:  0 ‘’ 0.001 ‘’ 0.01 ‘’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 8.679 on 17 degrees of freedomMultiple R-squared:  0.1869,    Adjusted R-squared:  0.09127 F-statistic: 1.954 on 2 and 17 DF,  p-value: 0.1722My question is how to simulate a new data that mimic the original data above?","Display_name":"Nor Hisham Haron","Creater_id":13614,"Start_date":"2016-08-07 17:40:11","Question_id":59062}
{"_id":{"$oid":"5837a582a05283111e4d5c91"},"Last_activity":"2016-07-15 04:11:44","Creator_reputation":1096,"Question_score":1,"Answer_content":"Here is another code to generate multiple linear regression with errors follow normal distribution:sim.regression\u0026lt;-function(n.obs=10,coefficients=runif(10,-5,5),s.deviation=.1){  n.var=length(coefficients)    M=matrix(0,ncol=n.var,nrow=n.obs)  beta=as.matrix(coefficients)  for (i in 1:n.var){    M[,i]=rnorm(n.obs,0,1)  }  y=M %*% beta + rnorm(n.obs,0,s.deviation)  return (list(x=M,y=y,coeff=coefficients))}","Display_name":"TPArrow","Creater_id":46139,"Start_date":"2014-09-26 05:00:46","Question_id":59062}
{"_id":{"$oid":"5837a582a05283111e4d5c92"},"Last_activity":"2015-11-02 16:00:15","Creator_reputation":152683,"Question_score":14,"Answer_content":"If you don't have them already, start by setting up some predictors, , , ...Choose the population ('true') coefficients of your predictors, the 's, including , the intercept.Choose the error variance,  or equivalently its square root, generate the error term, , as an independent random normal vector, with mean 0 and variance  Let then you can regress the  on your 'se.g.x1 \u0026lt;- 11:30x2 \u0026lt;- runif(20,5,95)x3 \u0026lt;- rbinom(20,1,.5)b0 \u0026lt;- 17b1 \u0026lt;- 0.5b2 \u0026lt;- 0.037b3 \u0026lt;- -5.2sigma \u0026lt;- 1.4eps \u0026lt;- rnorm(x1,0,sigma)y \u0026lt;- b0 + b1*x1  + b2*x2  + b3*x3 + epsproduces a single simulation of  from the model. Then running summary(lm(y~x1+x2+x3))givesCall:lm(formula = y ~ x1 + x2 + x3)Residuals:    Min      1Q  Median      3Q     Max -2.6967 -0.4970  0.1152  0.7536  1.6511 Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept) 16.28141    1.32102  12.325 1.40e-09 ***x1           0.55939    0.04850  11.533 3.65e-09 ***x2           0.01715    0.01578   1.087    0.293    x3          -4.91783    0.66547  -7.390 1.53e-06 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 1.241 on 16 degrees of freedomMultiple R-squared:  0.9343,    Adjusted R-squared:  0.9219 F-statistic: 75.79 on 3 and 16 DF,  p-value: 1.131e-09You can simplify this procedure in several ways, but I figured spelling it out would help to begin with.If you want to simulate a new random  but with the same population coefficients, just rerun the last two lines of the procedure above (generate a new random eps and y), corresponding to steps 3 and 4 of the algorithm.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2013-05-14 20:32:44","Question_id":59062}
{"_id":{"$oid":"5837a582a05283111e4d5c9f"},"Last_activity":"2016-08-07 17:35:16","Creator_reputation":11,"Question_score":0,"Answer_content":"where can i find intel on formulas and how to perform, MdAPE, MdRAE and RMSE in R ? i found a meta study detailing the performance of error measurement techniques on wiki (year 1992) http://faculty.weatherhead.case.edu/Fred-Collop/researchArticles/ErrorMeasures.pdf(chap 8 for results, chap 3 for definition of perf. criterias)","Display_name":"Po Stulat","Creater_id":74736,"Start_date":"2016-08-07 13:51:34","Question_id":228556}
{"_id":{"$oid":"5837a582a05283111e4d5cac"},"Last_activity":"2016-08-07 17:13:56","Creator_reputation":30005,"Question_score":6,"Answer_content":"The effect of  in the ridge regression estimator is that it \"inflates\" singular values  of  via terms like  . Specifically, if SVD of the design matrix is , then \\hat\\beta_\\mathrm{ridge} = V^\\top \\frac{S}{S^2+\\lambda I} U y.This is explained multiple times on our website, see e.g. @whuber's detailed exposition here: The proof of shrinking coefficients using ridge regression through \u0026quot;spectral decomposition\u0026quot;.This suggests that selecting  much larger than  will shrink everything very strongly. I suspect that \\lambda=\\|X\\|_2^2=\\sum s_i^2 will be too big for all practical purposes.I usually normalize my lambdas by the squared Frobenius norm of  and have a cross-validation grid that goes from  to  (on a log scale).Having said that, no value of lambda can be seen as truly \"maximum\", in contrast to the lasso case. Imagine that predictors are exactly orthogonal to the response, i.e. that the true . Any finite value of  for any finite value of sample size  will yield  and hence could benefit from stronger shrinkage.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-08-03 07:43:38","Question_id":227071}
{"_id":{"$oid":"5837a582a05283111e4d5cb9"},"Last_activity":"2016-08-07 17:11:57","Creator_reputation":8337,"Question_score":0,"Answer_content":"I'm very confused by your description, but here are some remarks that will hopefully be helpful.If you only apply exactly one predicate to each ball, as in the example in your first paragraph, then you have no information about how the predicates are related to each other. You can estimate the univariate distribution of each predicate's values across all the balls, but not the bivariate distribution, which is what you seem interested in.  At at even simpler level, if we get A_(true) = 100, B_(true) = 100, chances are 100% that the underlying population is homogeneous (i.e. of a single type).I disagree. It seems quite possible, in your scenario, for any number of the 100 balls not tested with A to be false on A, and similarly for B.  But how does one compute the confidence of this statement?Careful with your terminology. \"Confidence\" is a term from frequentist statistics, where we don't attach probabilities to hypotheses about populations.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-07 17:11:57","Question_id":228652}
{"_id":{"$oid":"5837a582a05283111e4d5cc8"},"Last_activity":"2016-08-07 09:38:01","Creator_reputation":293,"Question_score":0,"Answer_content":"You would probably have to pack recursive structures into finite-dimensional real vectors and there have been such attempts. The finite precision limits goes as far as the recursion can go.The limitation of feedforward neural networks is restricted to finite input and output spaces, so recurrent may be more suitable for this task as in theory can process arbitrarily long strings of numbers, but it has much more practical difficulties than feedforward network.These kind of methods are still needs to be researched.Source: SAS FAQQuote:  There have been attempts to pack recursive structures into finite-dimensional real vectors (Blair, 1997; Pollack, 1990; Chalmers, 1990; Chrisman, 1991; Plate, 1994; Hammerton, 1998). Obviously, finite precision limits how far the recursion can go (Hadley, 1999). The practicality of such methods is open to debate.","Display_name":"kenorb","Creater_id":12989,"Start_date":"2016-08-06 12:50:23","Question_id":228598}
{"_id":{"$oid":"5837a582a05283111e4d5cd5"},"Last_activity":"2016-08-07 14:22:38","Creator_reputation":6097,"Question_score":1,"Answer_content":"Yes, just choose those  draws that are more than  (but still divide by ). The reason this works mathematically is you can write the bounded below integral as an unbounded integral with an indicator function.  \\int_{\\bar{x}}^{\\infty} g(x) f(x) dx = \\int_{-\\infty}^{\\infty} g(x) I_{\\{x \u0026gt; \\bar{x} \\}} f(x) dx \\approx \\dfrac{1}{R}\\sum_{r = 1}^{R} g(x_r) I_{\\{x_r \u0026gt; \\bar{x} \\}}.So if the draw from  is more than , you evaluate , otherwise, you throw it away.","Display_name":"Greenparker","Creater_id":31978,"Start_date":"2016-08-07 14:16:42","Question_id":228687}
{"_id":{"$oid":"5837a582a05283111e4d5ce8"},"Last_activity":"2016-08-07 12:25:13","Creator_reputation":11,"Question_score":1,"Answer_content":"Have you trained some network with the setting you have mentioned? I have not yet, but if we think in some function like  or , these are simple functions, however if you want to approximate them, complex values should be supported by the learned transformation , i.e. in order to approximate functions like the mentioned ones, the network's architecture must be endowed with complex valued basis funtions 1. Otherwise, the imaginary part of the image of  could be lost. It is seemed to the case of approximating an injective function, where more than a  corresponds to a , e.g. . Then try to approximate . I hope this helps.Complex valued neural networks","Display_name":"Nacho","Creater_id":126924,"Start_date":"2016-08-07 12:25:13","Question_id":182112}
{"_id":{"$oid":"5837a582a05283111e4d5cf5"},"Last_activity":"2016-08-07 11:48:08","Creator_reputation":5218,"Question_score":0,"Answer_content":"I tried a Sankey-style chart with simulated data. It's really a line chart with a translucent line for each of 1000 employees, applying some incremental offsets to reduce overlap. There is some appeal to seeing all the data, but I don't think it will be useful at answering your questions. It could, though, answer questions like, how did those Level 9 employees get there?And you might be able to recognize patterns in different populations.To answer your more analytical questions, I would go with simple charts targeted toward each question (or related set of questions). Here's a chart showing the average tenure at each level, with the shaded region showing some bounds of interest.A nice feature of this chart (not shown) is that you can reasonably overlay a small number of these for comparing different groups. For instance, you could have a line for each employee rating group.ScriptThe above mock-ups were made in JMP using its scripting language, JSL. Included in case it's a useful template even if you don't have JMP.Here is the script to create the simulated data. The initial table has one row per employee and one column for each time period.ne = 1000;nt = 15;nl = 9;dt1 = new table(\"emp\",    New Column(\"ID\", \"Nominal\", Formula(Row())),    New Column(\"probs\", \"Expression\", Formula(J(nl-1, 1, Random Uniform(0.05, 0.4))|/0)),    New Column(\"t0\", Formula(Random Category(.5, 1, .3, 2, .1, 3, .1, 4, 0.05, 5, 0.025, 6))),    Add Rows(ne));for (i = 1, i \u0026lt;= nt, i++,    s = \"\\[dt1 \u0026lt;\u0026lt; New Column(\"tAAA\", Formula(:tBBB + if (random uniform() \u0026lt; probs[Row()][:tBBB], 1, 0)));]\\\";    substitute into(s, \"AAA\", char(i), \"BBB\", char(i-1));    Eval(parse(s)););Next, convert the table to tall form, stacking all the time periods into one column.dt2 = dt1 \u0026lt;\u0026lt; Stack(            columns(:t0,:t1,:t2,:t3,:t4,:t5,:t6,:t7,:t8,:t9,:t10,:t11,:t12,:t13,:t14,:t15),            Source Label Column( \"tc\" ),            Stacked Data Column( \"Level\" )        );dt2 \u0026lt;\u0026lt; New Column(\"Time\", Formula(Num(Substr(tc, 2))));dt2 \u0026lt;\u0026lt; New Column(\"Row\", Formula( Row()));dt2 \u0026lt;\u0026lt; New Column(\"rank\", Formula( Col Rank( :Row, :Level, :Time )));dt2 \u0026lt;\u0026lt; New Column(\"levelj\", Formula( :level+:rank/colmax(:rank)));The formula columns add an offset to the level column. Here's the graph itself. Graph Builder( Size( 692, 428 ), Show Control Panel( 0 ), Show Legend( 0 ),    Variables( X( :levelj ), Y( :Time ), Overlay( :ID ) ),    Elements( Line( X, Y, Legend( 9 ), Row order( 1 ) ) ));I removed the color and transparency customizations for brevity.","Display_name":"xan","Creater_id":1191,"Start_date":"2016-08-07 09:23:29","Question_id":228381}
{"_id":{"$oid":"5837a582a05283111e4d5d0a"},"Last_activity":"2016-08-05 15:07:57","Creator_reputation":11400,"Question_score":2,"Answer_content":"Either a fully parametric survival model (e.g., survreg in R) or a Cox* proportional hazards semi-parametric regression (e.g., coxph in R) is fully capable of handling continuous variables as predictors. If you have a continuous predictor this is the preferred approach over breaking the continuous predictor into categories. You may need a transformation of the continuous predictor to meet the linearity and proportional hazards assumptions, but that is not really different from any regression. Using the predict function for a coxph or survreg fit in R is no trickier for a continuous predictor than for a categorical predictor (although there can be a learning curve if you are trying survival predictions for the first time).*For clarity in terminology, a Cox proportional hazards regression model of survival is semi-parametric, in that there is are no parameters characterizing the baseline hazard but the coefficients are parameters whose values are determined by the regression.A fully parametric survival model specifies a particular functional form for the hazard or survival function along with parameters describing the influence of predictors on survival. A truly non-parametric model (log-rank test) simply looks for differences between survival curves without trying to estimate parameters of either the baseline hazard or of the effects of the predictor variable. To avoid confusion, these last two types of models are perhaps best not called \"Cox\" models (even though the log-rank test is sometimes called the Mantel-Cox test), reserving that name for the semi-parametric regression case.","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-08-05 15:07:57","Question_id":228493}
{"_id":{"$oid":"5837a582a05283111e4d5d19"},"Last_activity":"2016-08-07 08:23:27","Creator_reputation":3639,"Question_score":1,"Answer_content":"If you split the control group you end up with estimates which are not independent. If there is only one study out of many this is probably not an issue but if it occurs often in your data-set then you need to fit a multi-level model with study as a random effect. This can be done in R using metafor There is an example here.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-07 08:23:27","Question_id":227142}
{"_id":{"$oid":"5837a582a05283111e4d5d1a"},"Last_activity":"2016-08-04 11:45:03","Creator_reputation":523,"Question_score":0,"Answer_content":"While a network meta-analysis is one way to go. The simpler way is to use the recommendations of the Cochrane Handbook. If you're down the route of splitting the comparator group then divide the 'N' by half. That way you give half the weight of the comparator to each comparison.I made a mock meta-analysis below whereby study 2 is a three arm study.","Display_name":"abousetta","Creater_id":24137,"Start_date":"2016-08-04 11:45:03","Question_id":227142}
{"_id":{"$oid":"5837a582a05283111e4d5d1b"},"Last_activity":"2016-08-03 12:57:54","Creator_reputation":730,"Question_score":1,"Answer_content":"Definitely you need to go beyond the traditional paradigm of pairwise meta-analysis, and instead conduct a network meta-analysis (in this case an adjusted indirect comparison given the star-shaped evidence network).You can find ample references on this, such as:a recent question on CrossValidated;a comprehensive website; the Cochrane toolkit; a recent book;an upcoming book. You can easily do network meta-analysis with R, Stata, or WinBUGS. I recommend you to use R, for instance the netmeta package.","Display_name":"Giuseppe Biondi-Zoccai","Creater_id":107799,"Start_date":"2016-08-03 12:57:54","Question_id":227142}
{"_id":{"$oid":"5837a582a05283111e4d5d28"},"Last_activity":"2016-08-07 07:33:12","Creator_reputation":48,"Question_score":1,"Answer_content":"There are two major methods for evaluating a Hidden Markov Model (HMM). 1- Likelihood of test data:In this approach, one should keep some test data and compute the likelihood of the test sequences by using the forward algorithm.2- Predicting parts of data given other parts:A meaningful prediction task depends on the application. For example, you may be interested in predicting the future. In this case, you can use forward algorithm to track the state of the hmm (i.e. compute P(X_t = i | o_1,o_2,o_t,pi) where X_t is the state at time t and o is the observation, and use that to predict the expected observation at some future time t+k. Do that for all t and compute the mean error.By the way a very useful reference for learning about forward-backward algorithm would be This note. Some detailed instructions both about HMM and other PGM models can be found in Koller's book of Probabilistic Graphical Models.","Display_name":"Reza_Research","Creater_id":126608,"Start_date":"2016-08-07 07:33:12","Question_id":228277}
{"_id":{"$oid":"5837a582a05283111e4d5d34"},"Last_activity":"2016-08-07 07:23:29","Creator_reputation":8000,"Question_score":0,"Answer_content":"https://arxiv.org/abs/1606.03475 (De-identification of Patient Notes with Recurrent Neural Networks) uses a neural network with a \"label sequence optimization layer\" as the top layer to do some sequence tagging, which could be seen as a \"deep learning\" equivalent to CRF.See Section 2.2.4 Label sequence optimization layer:  The label sequence optimization layer takes the sequence of  probability vectors  from the label prediction layer  as input, and outputs a sequence of labels , where  is  the label assigned to the token .    The simplest strategy to select the label  would be to choose  the label that has the highest probability in , i.e.  . However, this greedy approach  fails to take into account the dependencies between subsequent labels.  For example, it may be more likely to have a token with the PHI type  STATE followed by a token with the PHI type ZIP than any other PHI  type. Even though the label prediction layer has the capacity to  capture such dependencies to a certain degree, it may be preferable to  allow the model to directly learn these dependencies in the last layer  of the model.    One way to model such dependencies is to incorporate a matrix  that  contains the transition probabilities between two subsequent labels.   is the probability that a token with label  is followed by  a token with the label . The score of a label sequence  is  defined as the sum of the probabilities of individual labels and the  transition probabilities:  s(y_{1:n}) = { \\sum_{i=1}^{n}\u0026gt; \\mathbf{a}_{i}[y_{i}]+  \\sum_{i=2}^{n} T [y_{i-1},y_{i}} ].  These  scores can be turned into probabilities of the label sequences by  taking a softmax function over all possible label sequences.  During  the training phase, the objective is to maximize the log probability  of the gold label sequence. In the testing phase, given an input  sequence of tokens, the corresponding sequence of predicted labels is  chosen as the one that maximizes the score.The network:","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-07 07:23:29","Question_id":227185}
{"_id":{"$oid":"5837a582a05283111e4d5d45"},"Last_activity":"2016-08-07 06:06:03","Creator_reputation":5797,"Question_score":3,"Answer_content":"normpdf(X,0,2) returns the probability density function at all values of X (be it a scalar, vector, or matrix) of a Normal random variable having mean = 0 and standard deviation = 2. It does not return sample (simulated) values of such a Normal random variable. For that, either A) use normrnd  orB) generate uniform random variables and use them as arguments for the inverse Normal cdf.So here is what I think you wanted to do:\u0026gt;\u0026gt; % Method A\u0026gt;\u0026gt; % Generates 1e6 Normal random numbers with mean 0, standard deviation 2\u0026gt;\u0026gt; y = normrnd(0,2,1e6,1); \u0026gt;\u0026gt; disp(std(y))    1.9994or\u0026gt;\u0026gt; % Method B\u0026gt;\u0026gt; % Generate 1e6 U[0,1} r.v.s, apply inverse normal CDF, and multiply by standard deviation\u0026gt;\u0026gt; u = rand(1e6,1);\u0026gt;\u0026gt; y = 2*norminv(u);\u0026gt;\u0026gt; disp(std(y))    1.9991Edit: Here is a plot of what you generated, which of course is the pdf:\u0026gt;\u0026gt; x=-5:.01:5;\u0026gt;\u0026gt; plot(x,normpdf(x,0,2))","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-07 05:47:40","Question_id":228644}
{"_id":{"$oid":"5837a582a05283111e4d5d52"},"Last_activity":"2015-07-09 05:52:02","Creator_reputation":152683,"Question_score":1,"Answer_content":"The original Mann-Whitney test assumes continuous distributions.If there are many observations and few ties, a normal approximation with a correctly-calculated variance is sufficient.To deal with heavy ties, the Mann-Whitney test needs to properly deal with the effect those ties have on the distribution of ranks under the null; in some cases the effect can be substantial.What happens in that situation varies from package to package - some packages don't handle heavy ties well. The p-value from the t-test may well be suspect.I'd be inclined to perform a permutation test on the actual set of ranks, or if primary interest focuses on testing for a difference in means, perhaps a permutation test based on the means; this way I don't have to rely on either the t-statistic having a distribution close to the t-distribution under the null, nor on the Mann-Whitney correctly dealing with heavy ties.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2015-07-09 05:52:02","Question_id":160633}
{"_id":{"$oid":"5837a582a05283111e4d5d53"},"Last_activity":"2015-07-09 04:29:06","Creator_reputation":57732,"Question_score":0,"Answer_content":"The Mann Whitney test and the t-test ask different questions, so they give different answers. The t-test is a test of difference in means, the MW is a test of difference in distributions.  In addition (as you noted), the t-test makes assumptions about the distributions that the MW does not. However, if the vast majority of your values are equal to exactly 300, then I think you might want some other method, but I am not sure what. Clearly, your data is not continuous (or values would not be exactly 300) and the t-test definitely assumes continuous variables.  In addition, all those values will have the same rank in the MW test (which may be what you mean by \"ignoring medians\").  I think you need to think about what exactly you want to test between these two distributions. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2015-07-09 04:29:06","Question_id":160633}
{"_id":{"$oid":"5837a582a05283111e4d5d64"},"Last_activity":"2016-01-28 10:07:50","Creator_reputation":301,"Question_score":1,"Answer_content":"Absolutely, see my example in - http://sanealytics.com/2015/03/10/matrix-factorization/You can substitute for your favorite optimization method.","Display_name":"ignorant","Creater_id":27866,"Start_date":"2016-01-28 10:07:50","Question_id":192833}
{"_id":{"$oid":"5837a582a05283111e4d5d65"},"Last_activity":"2016-01-27 19:28:33","Creator_reputation":241,"Question_score":1,"Answer_content":"L-BFGS, Conjugate gradient and SGD can solve unconstrained optimization problems.You can find a low-rank approximation with any of those methods.","Display_name":"Felipe Cruz","Creater_id":99870,"Start_date":"2016-01-27 19:28:33","Question_id":192833}
{"_id":{"$oid":"5837a582a05283111e4d5d71"},"Last_activity":"2016-08-01 18:13:09","Creator_reputation":71,"Question_score":0,"Answer_content":"Lets take the issue in hand step by step. First training and test data creation. For this you need to do stratified sampling just to get proper representation of both behaviors in both parts. You can do so in data.table by:train_ind \u0026lt;- data[, sample(.I, round(0.66*.N), FALSE),by=\"behavior\"]$V1Now to treat imbalance you have many ways in RF:Put higher penalty of misclassification for rarer classes.Increase penalty even more with sample weight.Due to presence of first two options in neat way I like Python scikit-learn RF more. However if you want to do it in R then I would suggest you to use caret and use appropriate RF version where these options are available (choose from http://topepo.github.io/caret/Random_Forest.html).I have dealt with even more skewed class in my project and based on my experience I like xgboost (gbm is there but its slow) much more to deal with such problem.","Display_name":"abhiieor","Creater_id":30830,"Start_date":"2016-08-01 18:13:09","Question_id":228636}
{"_id":{"$oid":"5837a582a05283111e4d5d7c"},"Last_activity":"2016-08-07 03:36:37","Creator_reputation":1,"Question_score":0,"Answer_content":"May I point you to a recent paper which may help you to address this issue? You will find it athttp://www.jneurosci.org/content/32/37/12651.full.pdf+htmlAll the analyses outlined in that paper are readily implemented in R.Best regards, and good luck with your analysesKarl Schilling","Display_name":"Karl Schilling","Creater_id":126889,"Start_date":"2016-08-07 03:36:37","Question_id":211990}
{"_id":{"$oid":"5837a582a05283111e4d5d89"},"Last_activity":"2016-08-07 03:27:08","Creator_reputation":36,"Question_score":2,"Answer_content":"A standard reference on this topic is:  A tutorial on particle filtering and smoothing: Fifteen years later. Arnaud Doucet, Adam M JohansenIn particular:So, as you can see, SMC = SIS + resampling,  also known as SIR or SIS/R. This definition is also in the paragraph immediately after equation (1) in the paper your cited. The resampling step, as discussed in section 2.4 of the paper you cited, circumvents the need for calculating the importance distribution in SIS (the equation (9) you refer to), which is impossible to compute is most cases.","Display_name":"Goro","Creater_id":126852,"Start_date":"2016-08-06 15:03:31","Question_id":228569}
{"_id":{"$oid":"5837a582a05283111e4d5d9a"},"Last_activity":"2016-08-07 02:32:22","Creator_reputation":26249,"Question_score":6,"Answer_content":"As I've noted in the comment to your question, discriminant analysis is a composite procedure with two distinct stages - dimensionality reduction (supervised) and classification stage. At dimensionality reduction we extract discriminant functions which replace the original explanatory variables. Then we classify (typically by Bayes' approach) observations to the classes using those functions.Some people tend to fail to recognize this clear-cut two-stage nature of LDA simply because they have got acquainted themselves only with LDA with 2 classes (called Fisher's discriminant analysis). In such analysis, only one discriminant function exists and classification is straightforward, and so everything can be explained in a textbook in a single \"pass\" without inviting concepts of space reduction and Bayes classification.LDA is closely related to MANOVA. The latter is a \"surface and broad\" side of the (multivariate) linear model while the \"depth and focused\" picture of it is canonical correlation analysis (CCA). The thing is that the correlation between two multivariate sets of variables is not uni-dimensional and is explained by a few pairs of \"latent\" variables called canonical variates.As a dimensionality reduction, LDA is theoretically a CCA with two sets of variables, one set being the correlated \"explanatory\" interval variables and the other set being the  dummy (or other contrast coded) variables representing the  groups, the classes of observations.In CCA, we consider the two correlated variable sets X and Y as equal in rights. Therefore we extract canonical variates from both sides, and they form pairs: variate 1 from set X and variate 1 from set Y with canonical correlation between them maximal; then variate 2 from set X and variate 2 from set Y with a smaller canonical correlation, etc. In LDA, we usually are not interested numerically in canonical variates from the class set side; we however take interest in the canonical variates from the explanatory set side. Those are called canonical discriminant functions or discriminants.The discriminants are what correlate maximally with the \"lines\" of separateness among the groups. Discriminant 1 explains the major portion of separateness; discriminant 2 picks some of the separeteness left unexplained due to the orthogonality to the previous separateness; descriminat 3 explains yet some remnant of separateness orthogonal to the previous two, etc. In LDA with  input variables (dimensions) and  classes the possible number of discriminants (reduced dimensions) is  and when the assumptions of LDA hold this number of them completely discriminate between classes and are able to fully classify the data to the classes (see).To repeate, this is actually CCA in its nature. LDA with 3+ classes is even called \"canonical LDA\". Despite that CCA and LDA are typically implemented algorithmically somewhat differently, in views of program efficiency, they are \"same\" enough so that it is possible to recalculate results (coefficients etc.) obtained in one procedure onto those obtained in the other. Most of the LDA specificity lies in the domain of coding the categorical variables representing groups. This is that same dilemma which is observed in (M)ANOVA. Different coding schemes lead to different ways of interpretation of the coefficients.Since LDA (as dimensionality reduction) can be understood as a particular case of CCA, you definitely have to explore this answer comparing CCA with PCA and regression. The main point there is that CCA is, in a sense, closer to regression than to PCA because CCA is a supervised technique (a latent linear combination is drawn out to correlate with something external) and PCA is not (a latent linear combination is drawn to summarize the internal). These are two branches of dimensionality reduction.When it comes to math you might find that while the variances of the principal components correspond to the eigenvalues of the data cloud (the covariance matrix between the variables), the variances of the discriminants are not so clearly related to those eigenvalues that are produced in LDA. The reason is that in LDA, eigenvalues do not summarize the shape of the data cloud; rather, they pertain to the abstract quantity of the ratio of the between-class to the within-class variation in the cloud.So, principal components maximize variance and discriminants maximize class separation; a simple case where a PC fails to discriminate between classes well enough but a discriminant can is these pictures. When drawn as lines in the original feature space discriminants do not usually appear orthogonal (being uncorrelated, nevertheless), but PCs do.Footnote for meticulous. How, in their results, LDA is exactly related to CCA. To repeat: if you do LDA with p variables and k classes and you do CCA with Set1 as those p variables and Set2 as k-1 indicator dummy variables representing groups (actually, not necessarily indicator variables - other types of contrast variables, such as deviation or Helmert - will do), then the results are equivalent in regards to the canonical variates extracted for Set1 - they directly correspond to the discriminant functions extracted in the LDA. What is the exact relationship, though?Algebra and terminology of LDA is explained here, and algebra and terminology of CCA is explained here. Canonical correlations will be the same. But what about coefficients and \"latents\"'s values (scores)? Consider a th discriminant and correspondent (th) canonical variate. For them,\"Pooled within class variance\" is the weighted average of the group variances with weight = n-1 in a group. In discriminant, this quantity is  (read in LDA algebra link), and so the coefficient of proportionality to switch onto CCA results from LDA results is simply \\sqrt {\\text {pooled within class variance in the variate}}. But because the canonical variate is standardized in the whole sample, this coefficient is equal to the  (which is standardized within groups). So, just divide the LDA results (coefficients and scores) by the discriminant's  to get the CCA results.The difference between CCA and LDA is due to that LDA \"knows\" that there are classes (groups): you directly indicate the groups to compute the within and between scatter matrices. That makes it both the computations faster and results more convenient for subsequent classification by discriminants. CCA, on the other hand, isn't aware of classes and process the data as if they all were continuous variables - which is more general but a slower way of computation. But the results are equivalent, and I've shown how.So far it was implied that the k-1 dummies are entered CCA the typical way, i.e. centered (like the variables of Set1). One might ask, is it possible to enter all k dummies and do not center them (to escape singularity)? Yes, it is possible, albeit probably less convenient. There will appear a zero-eigenvalue additional canonical variate, coefficients for it should be thrown away. Other results remain valid. Except the dfs to test the significance of canonical correlations. Df for the 1st correlation will be p*k which is wrong and the true df, as in LDA, is p*(k-1).","Display_name":"ttnphns","Creater_id":3277,"Start_date":"2015-08-31 04:46:33","Question_id":169436}
{"_id":{"$oid":"5837a582a05283111e4d5da9"},"Last_activity":"2016-08-07 01:04:03","Creator_reputation":19604,"Question_score":1,"Answer_content":"Since the scores are bounded by 1, I would consider going from 0.1 to 0.2 a 1-(0.8/0.9) about 11% improvement only, whereas 0.5 to 0.6 is 20% improvement on this scale (20% reduction in \"error\" from the optimum).However, I would avoid the use of \"significant\" unless you can relate this to statistical testing!Beware that Silhouette etc. should not be used to compare different methods because they have a bias. Silhouette is a heuristic tool to see whether you have chosen parameters reasonably, and can thus be okay to use when having to choose e.g. k in k-means. But an argument like \"k-means is better than HAC because we had a significantly higher silhouette score on all our data sets\" will likely be nonsense, because of bias towards one clustering algorithm or another.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-08-07 01:04:03","Question_id":223574}
{"_id":{"$oid":"5837a582a05283111e4d5db6"},"Last_activity":"2016-08-07 00:53:29","Creator_reputation":19604,"Question_score":0,"Answer_content":"There exist semi-supdrvised clustering (also referred to as constrained clustering) algorithms.There you can specify \"must link\" and \"cannot link\" constraints for known objects. The algorithm then searches a solution that retains as much of these constraints as possible, clustering the remainder of the data.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-08-07 00:53:29","Question_id":225583}
{"_id":{"$oid":"5837a582a05283111e4d5dc5"},"Last_activity":"2016-08-07 00:18:47","Creator_reputation":1456,"Question_score":10,"Answer_content":" Thus  is almost surely constant. A better description for such random variables is that it follows a degenerate distribution.","Display_name":"rightskewed","Creater_id":11668,"Start_date":"2016-08-07 00:18:47","Question_id":228624}
{"_id":{"$oid":"5837a582a05283111e4d5dda"},"Last_activity":"2016-08-06 20:20:03","Creator_reputation":152683,"Question_score":7,"Answer_content":"If we have a variable  and multiply it by , then .Assume that we're dealing with independent continuous uniform on  and  respectively (with )(This assumption is not restrictive since we can obtain the general case from this easily.)Then the joint density is .Since the bivariate density is constant where it's non-zero, we can just draw it \"looking from above\" by marking the boundary of that non-zero region.... and so by elementary geometric argument (along the lines of (i) recognize that density increases linearly as the sum,  goes from  to , stays constant until  and then decreases linearly to , and (ii) that the height in the middle section must be  to get unit area, then (iii) the equations of the three non-zero sections follow immediately by inspection), the density of the convolution is [While formal integration will obviously work, it's somewhat quicker - for me at least - to proceed by something like the above reasoning, where one simply draws the density and then writes the result down immediately.]The general case:Imagine instead we had independent  and . Then the above density would simply be shifted right by .","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-05 05:18:16","Question_id":228421}
{"_id":{"$oid":"5837a582a05283111e4d5ddb"},"Last_activity":"2016-08-05 04:04:18","Creator_reputation":61,"Question_score":6,"Answer_content":"Define  and , find their distribution, and you are back to the problem you know how to solve:  (convolution).","Display_name":"Tea","Creater_id":126732,"Start_date":"2016-08-05 04:04:18","Question_id":228421}
{"_id":{"$oid":"5837a582a05283111e4d5de8"},"Last_activity":"2016-08-06 17:10:30","Creator_reputation":1165,"Question_score":1,"Answer_content":"Since  is bi-variate normal, . For , the mean you derived is for special case that , specifically, \\operatorname{E}[Y]=\\int_0^{\\infty}xf_{X_1}(x)\\,dx=\\frac{\\sigma_{11}}{\\sqrt{2\\pi}}. Moreover, we can compute that \\operatorname{E}[Y^2]=\\int_0^{\\infty}x^2f_{X_1}(x)\\,dx=\\frac{\\sigma_{11}^2}{2}. So \\operatorname{Var}(Y)=\\frac{\\sigma_{11}^2}{2}\\left(1-\\frac{1}{\\pi}\\right). This can also be derived from variance of half-normal distribution, note first , then  since whenever one r.v. is greater than  the other is , such that \\operatorname{Cov}(X_1^+,X_1^-)=-\\operatorname{E}[X_1^+]\\operatorname{E}[X_1^-]=-\\frac{\\sigma_{11}^2}{2\\pi}; finally, notice that  by symmetry. It follows then \\sigma_{11}^2\\left(1-\\frac{2}{\\pi}\\right)=\\operatorname{Var}(|X_1|)=2\\operatorname{Var}(X_1^+)-\\frac{\\sigma_{11}^2}{\\pi}, which yields \\operatorname{Var}(X_1^+)=\\frac{\\sigma_{11}^2}{2}\\left(1-\\frac{1}{\\pi}\\right)For more general , this entry about folded normal distribution may help.","Display_name":"Francis","Creater_id":29905,"Start_date":"2016-08-06 17:10:30","Question_id":228611}
{"_id":{"$oid":"5837a582a05283111e4d5df5"},"Last_activity":"2016-08-06 17:51:03","Creator_reputation":604,"Question_score":1,"Answer_content":"Matrix factorization using SVD decomposes the input matrix into three parts:The left singular vectors . The first column of this matrix specifies on which axis the rows of the input matrix vary the most. In your case, the first column tells you which words vary together the most. The singular values . These are scalings. These are relative to each other. If the first value of  is twice as big as the second it means that the first singular vector (in  and ) explain twice as much variation as the seconds singular vector.The right singular vectors . The first row of this matrix specifies on which axis the columns of the input matrix vary the most. In your case, the first row tells you which documents vary together the most. When words or documents vary together it indicates that they are similar. For example, if the word doctor occurs more often in a document, the word nurse and hospital also occur more. This is shown by the first scaled left singular vector, the first column of .You can validate this result by looking at the input data. Notice that when nurse does occur, hospital also occurs and when it does not occur, hospital also does not occur. ","Display_name":"Pieter","Creater_id":93550,"Start_date":"2016-08-06 17:51:03","Question_id":108156}
{"_id":{"$oid":"5837a582a05283111e4d5e04"},"Last_activity":"2016-08-05 16:16:47","Creator_reputation":30005,"Question_score":1,"Answer_content":"PCA cannot perform \"subsampling\" or selecting \"most representative observations\". It is not selecting anything, it is constructing linear combinations with maximum variance. In your case, PCA will be constructing linear combinations of observations, which has unclear meaning and hardly predictable consequences for your algorithm.You should be better off with random subsampling.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-08-05 16:16:47","Question_id":228428}
{"_id":{"$oid":"5837a582a05283111e4d5e11"},"Last_activity":"2016-08-06 16:20:40","Creator_reputation":30005,"Question_score":2,"Answer_content":"These are two names for the same thing.Linear discriminant analysis (LDA) is called a lot of different names. I have seencanonical discriminant analysiscanonical linear discriminant analysisdescriptive discriminant analysis (see What is \u0026quot;Descriptive Discriminant Analysis\u0026quot;?)Fisher's discriminant analysisand possibly some others. I suspect different names might be used in different applied fields. In machine learning, \"linear discriminant analysis\" is by far the most standard term and \"LDA\" is a standard abbreviation.The reason for the term \"canonical\" is probably that LDA can be understood as a special case of canonical correlation analysis (CCA). Specifically, the \"dimensionality reduction part\" of LDA is equivalent to doing CCA between the data matrix  and the group indicator matrix . The indicator matrix  is a matrix with  rows and  columns with  if -th data point belongs to class  and zero otherwise. [Footnote: this  should not be centered.]This fact is not at all obvious and has a proof, which this margin is too narrow to contain.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-08-04 17:50:58","Question_id":226854}
{"_id":{"$oid":"5837a582a05283111e4d5e1e"},"Last_activity":"2016-08-06 16:14:30","Creator_reputation":572,"Question_score":1,"Answer_content":"Let  be a sequence of i.i.d. random variables. The (Strong) Law of Large Numbers states that:If  and \\frac{1}{n}\\sum_{i = 1}^nX_i \\overset{a.s}{\\longrightarrow}\\muIf  \\limsup\\frac{1}{n}\\Big|\\sum_{i = 1}^nX_i\\Big| =+\\inftyNote that no assumption is made about the existence or finiteness of the variance of each  (see chapter 6 of this book for the proofs).Now for your question about the case of stationary sequences this paper gives a reasonably simple answer for the convergence in probability to the mean and this lecture note takes a not so simple (at least for me) take on the subject.","Display_name":"Mur1lo","Creater_id":120428,"Start_date":"2016-08-06 16:14:30","Question_id":220773}
{"_id":{"$oid":"5837a582a05283111e4d5e2b"},"Last_activity":"2016-08-06 15:38:47","Creator_reputation":379,"Question_score":2,"Answer_content":"ImbalanceBy far the cleanest* way to (50%-50%) balance your training set is to use observation weights. Say 1% of cases is fraudulent, 99% is not, then give each non-fraudulent case weight 1, and each fraudulent transaction weight 99. This is equivalent to adding 98 copies of each fraudulent cases back into the training set.For example, in R's rpart or lm use argument weights in randomForest set classwt = c(0.5, 0.5). In Python's sklearn.tree.DecisionTreeClassifier set class_weight = \"balanced\".Doing so will make the classifier \"work harder\" on the minority outcome class (fraud), as misclassifying it now carries a highly increased cost.*I write \"cleanest\" because (randomly) oversampling creates unnecessary sampling noise as certain cases are sampled more or less than 99 times. Weighting, instead, treats all cases equally.Decision TreesGiven that you are using a classification tree, you could force the node check == yes to split, so that you do not have to classify all cases that end up in that node as fraudulent (of course I'm speculating -- I don't know your data).Lastly, classification trees naturally output probabilities. Say a new case falls into an end-node (a leaf) that consists of 72% fraudulent cases and 28% non-frauds, then the estimated probability that the new case is fraudulent equals 72%.","Display_name":"Jim","Creater_id":67042,"Start_date":"2016-08-06 14:57:21","Question_id":228458}
{"_id":{"$oid":"5837a582a05283111e4d5e2c"},"Last_activity":"2016-08-05 14:54:34","Creator_reputation":304,"Question_score":4,"Answer_content":"In addition to @Matthew Drury's answer, you could train different models for different transaction methods (cheques, non-cheques). This way the features of people using cheques would be highlighted and the column will also remain in the data. See if the tool you are using allows grouping when fitting models. This could save additional work. ","Display_name":"SpeedBirdNine","Creater_id":56105,"Start_date":"2016-08-05 14:54:34","Question_id":228458}
{"_id":{"$oid":"5837a582a05283111e4d5e2d"},"Last_activity":"2016-08-05 09:52:10","Creator_reputation":12762,"Question_score":15,"Answer_content":"It sounds like what you have is a powerfully predictive variable, and there is no reason to remove it.What you have to watch out in situations like this is what is called leakage.  Leakage is when you have a predictor that is just some version of your response in disguise.For example, suppose that you have a system at your company that, when fraud is detected, first switched the account into \"investigation\" status, and then when the investigation is complete, cancels it due to fraud.  The \"investigation status\" will look like a very powerful variable, but it is caused by the response (fraud).  If you went to implement your model, attempting to detect fraud, then the \"investigation status\" variable would be useless, as if an account is in investigation status, you already know its fraudulent.You can see why this is called leakage, the response has \"leaked into\" the predictors.So, think carefully about whether this could be the case with your account status, but I suspect not.  In that case, you just have a really good predictive variable.  Most people trying to fraud have chosen checks and most people that have chosen checks and a third of people who have chosen checks are frauds so whenever my model tends to classify as a fraud any observation with check as the payment type.You shouldn't evaluate your model by classifying records as fraud or non fraud.  Instead, you should get your model to assign probabilities of fraud to each evaluation record, and work directly with those probabilities.  If you use this context, then your issue here goes away, as you will simply observe that using a check gives a high probability of fraud, which does not mean that all check users are fraudulent.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-08-05 08:18:55","Question_id":228458}
{"_id":{"$oid":"5837a582a05283111e4d5e3a"},"Last_activity":"2016-08-06 12:15:39","Creator_reputation":573,"Question_score":2,"Answer_content":"MLP: uses dot products (between inputs and weights) and sigmoidal activation functions (or other monotonic functions such as ReLU) and training is usually done through backpropagation for all layers (which can be as many as you want). This type of neural network is used in deep learning with the help of many techniques (such as dropout or batch normalization);RBF: uses Euclidean distances (between inputs and weights, which can be viewed as centers) and (usually) Gaussian activation functions (which could be multivariate), which makes neurons more locally sensitive. Thus, RBF neurons have maximum activation when the center/weights are equal to the inputs (look at the image below). Due to this property, RBF neural networks are good for novelty detection (if each neuron is centered on a training example, inputs far away from all neurons constitute novel patterns) but not so good at extrapolation. Also, RBFs may use backpropagation for learning, or hybrid approaches with unsupervised learning in the hidden layer (they usually have just 1 hidden layer). Finally, RBFs make it easier to grow new neurons during training.","Display_name":"rcpinto","Creater_id":49196,"Start_date":"2016-08-06 12:15:39","Question_id":228595}
{"_id":{"$oid":"5837a582a05283111e4d5e4b"},"Last_activity":"2016-08-06 11:53:07","Creator_reputation":3705,"Question_score":2,"Answer_content":"Using a variable as a regular regressor vs. as an offset is mainly a question of estimating its coefficient vs. knowing (or fixing) it to be 1. As a pragmatic solution I would look at the estimation results of WholeModel. If the coefficient of TotalWords is close to 1 you might also treat it as an offset.More importantly, it might make sense to use log(TotalWords) rather than TotalWords (possibly similarly for AvgFrequency) so that if you use it as an offset you could really interpret it as modeling the ratio NumWordsFound/TotalWords. Again, you could also decide whether or not to do this based on the model fit.","Display_name":"Achim Zeileis","Creater_id":60261,"Start_date":"2016-08-06 11:53:07","Question_id":228573}
{"_id":{"$oid":"5837a582a05283111e4d5e60"},"Last_activity":"2016-08-06 11:02:25","Creator_reputation":1865,"Question_score":3,"Answer_content":"From Wikipedia's Loss function: \"In mathematical optimization, statistics, decision theory and machine learning, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (sometimes called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized.\"On this site we have a good explanation for what AIC is. That is, ASSUMING the residuals are Gaussian, AIC is log-likelihood is given by: \",  being the covariance structure of your model,  being the number of points in your datasets,  the mean response and obviously  being your dependent variable.More specifically AIC is calculated to be equal to , where  is the number of fixed effect in your model and  your likelihood function [1]. It practically compares trade-off between variance() and bias () in your modelling assumptions. As such in your case it would compare two different log-likelihood structures when it came to the bias term. That is because when you calculate your log-likelihood practically you look at two terms:A fit term, denoted by  and a complexity penalization term, denoted by .\"Although the fit term is not called a loss function per se, it is a loss function, because log likelihood is optimized to yield an AIC value, that is, a step left out in the explanation above, and which is probably not fully characterized for general residuals that do not agree with the given Gaussian assumption. That answers your question, yes, it is a loss function. What you didn't ask is whether AIC is good for anything.","Display_name":"Carl","Creater_id":99274,"Start_date":"2016-08-05 14:37:27","Question_id":228503}
{"_id":{"$oid":"5837a582a05283111e4d5e6f"},"Last_activity":"2016-08-06 10:40:32","Creator_reputation":189,"Question_score":0,"Answer_content":"Yes, do apply exp(*).Some food for thought: the change of scale is not linear. Prediction errors that seem reasonable in the log-scale often became large in the natural scale (after you apply logarithms), and the predictions tend to be biased. Check the plots of time \u0026amp; (original TS), time \u0026amp; exp( fitted values in the log scale) if possible.","Display_name":"VictorZurkowski","Creater_id":53211,"Start_date":"2016-08-06 10:40:32","Question_id":228576}
{"_id":{"$oid":"5837a582a05283111e4d5e7c"},"Last_activity":"2016-08-06 08:25:19","Creator_reputation":27820,"Question_score":6,"Answer_content":"As @lacerbi suggests a kernel function (or covariance function in a Gaussian Process setting) is essentially a similarity metric, so that the value of the kernel is high if the two input vectors are considered \"similar\" according to the needs of the application and lower if they are dissimilar.  However not all similarity metrics are valid kernel functions.  To be a valid kernel, the function must be interpretable as computing an inner product in some transformed feature space, i.e.  where  is a function that maps the input vectors into the feature space.So why must the kernel be interpretable as an inner product in some feature space?  The reason is that it is much easier to devise theoretical bounds on generalisation performance for linear models (such as logistic regression) than it is for non-linear models (such as a neural network).  Most linear models can be written so that the input vectors only appear in the form of inner products.  This means that we can build a non-linear model by constructing a linear model in the kernel feature space.  This is a fixed transformation of the data, so all of the theoretical performance bounds for the linear model automatically apply to the new kernel non-linear model*.An important point that is difficult to grasp at first is that we tend not to think of a feature space that would be good for our particular application and then design a kernel giving rise to that feature space.  In general we come up with a good similarity metric and then see if it is a kernel (the test is straightforward, if any matrix of pairwise evaluations of the kernel function at points in general position is positive definite, then it is a valid kernel). Of course if you tune the kernel parameters to optimise generalisation performance, e.g. by minimising the cross-validation error, then it is no longer a fixed transformation, but one that has been learned from the data and much of the beautiful theory has just been invalidated.  So in practice, while the the design of kernel methods has a lot of reassuring theory behind them, the bounds themselves generally don't apply to practical applications - but it is still reassuring as there are sound principles underpinning the model.","Display_name":"Dikran Marsupial","Creater_id":887,"Start_date":"2016-08-06 07:21:04","Question_id":228552}
{"_id":{"$oid":"5837a582a05283111e4d5e7d"},"Last_activity":"2016-08-06 07:41:22","Creator_reputation":1747,"Question_score":8,"Answer_content":"In loose terms, a kernel or covariance function  specifies the statistical relationship between two points  in your input space; that is, how markedly a change in the value of the Gaussian Process (GP) at  correlates with a change in the GP at . In some sense, you can think of  as defining a similarity between inputs (*).Typical kernels might simply depend on the Euclidean distance (or linear transformations thereof) between points, but the fun starts when you realize that you can do much, much more.As David Duvenaud puts it:  Kernels can be defined over all types of data structures: Text,  images, matrices, and even kernels. Coming up with a kernel on a new  type of data used to be an easy way to get a NIPS paper.For an easy overview of kernels for GPs, I warmly recommend his Kernel Cookbook and references therein.(*) As @Dikran Marsupial notes, beware that the converse is not true; not all similarity metrics are valid kernels (see his answer).","Display_name":"lacerbi","Creater_id":80479,"Start_date":"2016-08-06 06:08:20","Question_id":228552}
{"_id":{"$oid":"5837a582a05283111e4d5e8c"},"Last_activity":"2016-08-06 09:58:37","Creator_reputation":26249,"Question_score":11,"Answer_content":"I take it that the question is about LDA and linear (not logistic) regression.There is a considerable and meaningful relation between linear regression and linear discriminant analysis. In case the DV consisting just of 2 groups the two analyses are actually identical. Despite that computations are different and the results - regression and discriminant coefficients - are not the same, they are exactly proportional to each other.Now for the more-than-two-groups situation. First, let us state that LDA (its extraction, not classification stage) is equivalent (linearly related results) to canonical correlation analysis if you turn the grouping DV into a set of dummy variables (with one redundant of them droped out) and do canonical analysis with sets \"IVs\" and \"dummies\". Canonical variates on the side of \"IVs\" set that you obtain are what LDA calls \"discriminant functions\" or \"discriminants\".So, then how canonical analysis is related to linear regression? Canonical analysis is in essence a MANOVA (in the sence \"Multivariate Multiple linear regression\" or \"Multivariate general linear model\") deepened into latent structure of relationships between the DVs and the IVs. These two variations are decomposed in their inter-relations into latent \"canonical variates\". Let us take the simplest example, Y vs X1 X2 X3. Maximization of correlation between the two sides is linear regression (if you predict Y by Xs) or - which is the same thing - is MANOVA (if you predict Xs by Y). The correlation is unidimensional (with magnitude R^2 = Pillai's trace) because the lesser set, Y, consists just of one variable. Now let's take these two sets: Y1 Y2 vs X1 x2 x3. The correlation being maximized here is 2-dimensional because the lesser set contains 2 variables. The first and stronger latent dimension of the correlation is called the 1st canonical correlation, and the remaining part, orthogonal to it, the 2nd canonical correlation. So, MANOVA (or linear regression) just asks what are partial roles (the coefficients) of variables in the whole 2-dimensional correlation of sets; while canonical analysis just goes below to ask what are partial roles of variables in the 1st correlational dimension, and in the 2nd.Thus, canonical correlation analysis is multivariate linear regression deepened into latent structure of relationship between the DVs and IVs. Discriminant analysis is a particular case of canonical correlation analysis (see exactly how). So, here was the answer about the relation of LDA to linear regression in a general case of more-than-two-groups.Note that my answer does not at all see LDA as classification technique. I was discussing LDA only as extraction-of-latents technique. Classification is the second and stand-alone stage of LDA (I described it here). @Michael Chernick was focusing on it in his answers.","Display_name":"ttnphns","Creater_id":3277,"Start_date":"2012-06-30 23:04:02","Question_id":31459}
{"_id":{"$oid":"5837a582a05283111e4d5e8d"},"Last_activity":"2015-09-03 05:20:38","Creator_reputation":30005,"Question_score":2,"Answer_content":"The purpose of this answer is to explain the exact mathematical relationship between linear discriminant analysis (LDA) and multivariate linear regression (MLR). It will turn out that the correct framework is provided by reduced rank regression (RRR).We will show that LDA is equivalent to RRR of the whitened class indicator matrix on the data matrix.NotationLet  be the  matrix with data points  in rows and variables in columns. Each point belongs to one of the  classes, or groups. Point  belongs to class number .Let  be the  indicator matrix encoding group membership as follows:  if  belongs to class , and  otherwise. There are  data points in class ; of course .We assume that the data are centered and so the global mean is equal to zero, . Let  be the mean of class .LDAThe total scatter matrix  can be decomposed into the sum of between-class and within-class scatter matrices defined as follows:\\begin{align}\\C_b \u0026amp;= \\sum_j n_j \\bmu_j \\bmu_j^\\top \\\\\\C_w \u0026amp;= \\sum(\\x_i - \\bmu_{g(i)})(\\x_i - \\bmu_{g(i)})^\\top.\\end{align}One can verify that . LDA searches for discriminant axes that have maximal between-group variance and minimal within-group variance of the projection. Specifically, first discriminant axis is the unit vector  maximizing , and the first  discriminant axes stacked together into a matrix  should maximize the trace \\DeclareMathOperator{\\tr}{tr} L_\\mathrm{LDA}=\\tr\\left(\\W^\\top \\C_b \\W (\\W^\\top \\C_w \\W)^{-1}\\right).Assuming that  is full rank, LDA solution  is the matrix of eigenvectors of  (ordered by the eigenvalues in the decreasing order).This was the usual story. Now let us make two important observations.First, within-class scatter matrix can be replaced by the total scatter matrix (ultimately because maximizing  is equivalent to maximizing ), and indeed, it is easy to see that  has the same eigenvectors.Second, the between-class scatter matrix can be expressed via the group membership matrix defined above. Indeed,  is the matrix of group sums. To get the matrix of group means, it should be multiplied by a diagonal matrix with  on the diagonal; it's given by . Hence, the matrix of group means is  (sapienti will notice that it's a regression formula). To get  we need to take its scatter matrix, weighted by the same diagonal matrix, obtaining \\C_b = \\X^\\top \\G (\\G^\\top \\G)^{-1}\\G^\\top \\X.  If all  are identical and equal to  (\"balanced dataset\"), then this expression simplifies to .We can define normalized indicator matrix  as having  where  has . Then for both, balanced and unbalanced datasets, the expression is simply . Note that  is, up to a constant factor, the whitened indicator matrix: .RegressionFor simplicity, we will start with the case of a balanced dataset. Consider linear regression of  on . It finds  minimizing . Reduced rank regression does the same under the constraint that  should be of the given rank . If so, then  can be written as  with both  and  having  columns. One can show that the rank two solution can be obtained from the rank solution by keeping the first column and adding an extra column, etc.To establish the connection between LDA and linear regression, we will prove that  coincides with .The proof is straightforward. For the given , optimal  can be found via regression: . Plugging this into the loss function, we get \\| \\G - \\X \\D (\\D^\\top \\X^\\top \\X \\D)^{-1} \\D^\\top \\X^\\top \\G\\|^2, which can be written as trace using the identity . After easy manipulations we get that the regression is equivalent to maximizing (!) the following scary trace: \\tr\\left(\\D^\\top \\X^\\top \\G \\G^\\top \\X \\D (\\D^\\top \\X^\\top \\X \\D)^{-1}\\right), which is actually nothing else than \\ldots = \\tr\\left(\\D^\\top \\C_b \\D (\\D^\\top \\C \\D)^{-1}\\right)/m \\sim L_\\mathrm{LDA}.This finishes the proof. For unbalanced datasets we need to replace  with .One can similarly show that adding ridge regularization to the reduced rank regression is equivalent to the regularized LDA.Relationship between LDA, CCA, and RRRIn his answer, @ttnphns made a connection to canonical correlation analysis (CCA). Indeed, LDA can be shown to be equivalent to CCA between  and . In addition, CCA between any  and  can be written as RRR predicting whitened  from . The rest follows from this. BibliographyIt is hard to say who deserves the credit for what is presented above.There is a recent conference paper by Cai et al. (2013) On The Equivalent of Low-Rank Regressions and Linear Discriminant Analysis Based Regressions that presents exactly the same proof as above but creates the impression that they invented this approach. This is definitely not the case. Torre wrote a detailed treatment of how most of the common linear multivariate methods can be seen as reduced rank regression, see A Least-Squares Framework for Component Analysis, 2009, and a later book chapter A unification of component analysis methods, 2013; he presents the same argument but does not give any references either. This material is also covered in the textbook Modern Multivariate Statistical Techniques (2008) by Izenman, who introduced RRR back in 1975.The relationship between LDA and CCA apparently goes back to Bartlett, 1938, Further aspects of the theory of multiple regression -- that's the reference I often encounter (but did not verify). The relationship between CCA and RRR is described in the Izenman, 1975, Reduced-rank regression for the multivariate linear model. So all of these ideas have been around for a while.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2015-09-01 01:36:37","Question_id":31459}
{"_id":{"$oid":"5837a582a05283111e4d5e8e"},"Last_activity":"2015-09-01 03:45:04","Creator_reputation":25897,"Question_score":5,"Answer_content":"Here is a reference to one of Efron's papers: The Efficiency of Logistic Regression Compared to Normal Discriminant Analysis, 1975.Another relevant paper is Ng \u0026amp; Jordan, 2001, On Discriminative vs. Generative classifierers: A comparison of logistic regression and naive Bayes. And here is an abstract of a comment on it by Xue \u0026amp; Titterington, 2008, that mentions O'Neill's papers related to his PhD dissertation:  Comparison of generative and discriminative classifiers is an  ever-lasting topic. As an important contribution to this topic, based  on their theoretical and empirical comparisons between the naïve Bayes  classifier and linear logistic regression, Ng and Jordan (NIPS  841---848, 2001) claimed that there exist two distinct regimes of  performance between the generative and discriminative classifiers with  regard to the training-set size. In this paper, our empirical and  simulation studies, as a complement of their work, however, suggest  that the existence of the two distinct regimes may not be so reliable.  In addition, for real world datasets, so far there is no theoretically  correct, general criterion for choosing between the discriminative and  the generative approaches to classification of an observation  into  a class ; the choice depends on the relative confidence we have in  the correctness of the specification of either  or   for the data. This can be to some extent a demonstration of why Efron  (J Am Stat Assoc 70(352):892---898, 1975) and O'Neill (J Am Stat Assoc  75(369):154---160, 1980) prefer normal-based linear discriminant  analysis (LDA) when no model mis-specification occurs but other  empirical studies may prefer linear logistic regression instead.  Furthermore, we suggest that pairing of either LDA assuming a common  diagonal covariance matrix (LDA) or the naïve Bayes classifier and  linear logistic regression may not be perfect, and hence it may not be  reliable for any claim that was derived from the comparison between  LDA or the naïve Bayes classifier and linear logistic regression to be  generalised to all generative and discriminative classifiers.There are a lot of other references on this that you can find online. ","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-06-30 22:47:01","Question_id":31459}
{"_id":{"$oid":"5837a582a05283111e4d5e8f"},"Last_activity":"2012-06-30 22:28:49","Creator_reputation":25897,"Question_score":0,"Answer_content":"Linear regression and linear discriminant analysis are very different.  Linear regression relates a dependent variable to a set of independent predictor variables.  The idea is to find a function linear in the parameters that best fits the data.  It does not even have to be linear in the covariates.  Linear discriminant analysis on the other hand is a procedure for classifying objects into categories. For the two-class problem it seeks to find the best separating hyperplane for dividing the groups into two catgories.  Here best means that it minimizes a loss function that is a linear combination of the error rates.  For three or more groups it finds the best set of hyperplanes (k-1 for the k class problem).  In discriminant analysis the hypoerplanes are linear in the feature variables.The main similarity between the two is term linear in the titles.","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-06-30 20:46:08","Question_id":31459}
{"_id":{"$oid":"5837a582a05283111e4d5e9c"},"Last_activity":"2016-08-06 09:54:21","Creator_reputation":108,"Question_score":1,"Answer_content":"As of July 2016, the package PRROC works great for computing both ROC AUC and PR AUC.Assuming you already have a vector of probabilities (called probs) computed with your model and the true class labels are in your data frame as dflabel == 1]bg \u0026lt;- probs[df$label == 0]# ROC Curve    roc \u0026lt;- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)plot(roc)# PR Curvepr \u0026lt;- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)plot(pr)PS: The only disconcerting thing is you use scores.class0 = fg when fg is computed for label 1 and not 0.Here are the example ROC and PR curves with the areas under them:The bars on the right are the threshold probabilities at which a point on the curve is obtained.Note that for a random classifier ROC AUC will be close to 0.5 but PR ROC will be close to 0.","Display_name":"arun","Creater_id":108868,"Start_date":"2016-08-02 16:15:20","Question_id":10501}
{"_id":{"$oid":"5837a582a05283111e4d5e9d"},"Last_activity":"2011-09-05 08:37:11","Creator_reputation":277,"Question_score":0,"Answer_content":"ROCR, pROC - are really nice!","Display_name":"Vladimir Chupakhin","Creater_id":3345,"Start_date":"2011-09-05 08:37:11","Question_id":10501}
{"_id":{"$oid":"5837a582a05283111e4d5e9e"},"Last_activity":"2011-08-31 02:29:44","Creator_reputation":21,"Question_score":2,"Answer_content":"Once you've got a precision recall curve from qpPrecisionRecall, e.g.:pr \u0026lt;- qpPrecisionRecall(measurements, goldstandard)you can calculate its AUC by doing this:f \u0026lt;- approxfun(pr[, 1:2])auc \u0026lt;- integrate(f, 0, 1)$valuethe help page of qpPrecisionRecall gives you details on what data structure expects in its arguments.","Display_name":"robertc","Creater_id":6089,"Start_date":"2011-08-31 02:29:44","Question_id":10501}
{"_id":{"$oid":"5837a582a05283111e4d5e9f"},"Last_activity":"2011-05-08 01:17:15","Creator_reputation":37824,"Question_score":2,"Answer_content":"A little googling returns one bioc package, qpgraph (qpPrecisionRecall), and a cran one, minet (auc.pr). I have no experience with them, though. Both have been devised to deal with biological networks.","Display_name":"chl","Creater_id":930,"Start_date":"2011-05-08 01:17:15","Question_id":10501}
{"_id":{"$oid":"5837a582a05283111e4d5eac"},"Last_activity":"2016-08-05 08:56:29","Creator_reputation":61,"Question_score":4,"Answer_content":"I think I asked a dumb question. The proof is right. The missing step in the proof is as follows. \\begin{align}\u0026amp;~~~~\\sum_{k=1}^{\\infty} P\\{\\frac{\\log^{+}|\\xi_0|}{\\log\\zeta}\u0026gt;k\\}\\\\\u0026amp;\\leq \\sum_{k=1}^{\\infty} \\int_{k-1}^k P\\{\\frac{\\log^{+}|\\xi_0|}{\\log\\zeta}\u0026gt;s \\}ds\\\\\u0026amp;=\\mathrm{E} \\log^{+} |\\xi_0|/\\log \\zeta, \\end{align}","Display_name":"JRBNB","Creater_id":126677,"Start_date":"2016-08-05 08:50:55","Question_id":228348}
{"_id":{"$oid":"5837a582a05283111e4d5ead"},"Last_activity":"2016-08-04 15:55:25","Creator_reputation":1,"Question_score":0,"Answer_content":"Your argument does not invalidate their argument. Basically, they say , where , while you are saying, , where . Both arguments are compatible and your argument does not invalidate theirs. ","Display_name":"Kevin Levrone","Creater_id":126678,"Start_date":"2016-08-04 15:55:25","Question_id":228348}
{"_id":{"$oid":"5837a582a05283111e4d5eba"},"Last_activity":"2016-08-06 09:53:06","Creator_reputation":26249,"Question_score":15,"Answer_content":"Here is a short tale about Linear Discriminant Analysis (LDA) as a reply to the question.When we have one variable and  groups (classes) to discriminate by it, this is ANOVA. The discrimination power of the variable is , or .When we have  variables, this is MANOVA. If the variables are uncorrelated neither in total sample nor within groups, then the above discrimination power, , is computed analogously and could be written as , where  is the pooled within-group scatter matrix (i.e. the sum of  x SSCP matrices of the variables, centered about the respective groups' centroid);  is the between-group scatter matrix , where  is the scatter matrix for the whole data (SSCP matrix of the variables centered about the grand centroid. (A \"scatter matrix\" is just a covariance matrix without devidedness by sample_size-1.)When there is some correlation between the variables - and usually there is - the above  is expressed by  which is not a scalar anymore but a matrix. This simply due to that there are  discriminative variables hidden behind this \"overall\" discrimination and partly sharing it.Now, we may want to submerge in MANOVA and decompose  into new and mutually orthogonal latent variables (their number is ) called discriminant functions or discriminants - the 1st being the strongest discriminator, the 2nd being next behind, etc. Just like we do it in Pricipal component analysis. We replace original correlated variables by uncorrelated discriminants without loss of discriminative power. Because each next discriminant is weaker and weaker we may accept a small subset of first  discriminants without great loss of discriminative power (again, similar to how we use PCA). This is the essense of LDA as of dimensionality reduction technique (LDA is also a Bayes' classification technique, but this is an entirely separate topic).LDA thus resembles PCA. PCA decomposes \"correlatedness\", LDA decomposes \"separatedness\". In LDA, because the above matrix expressing \"separatedness\" isn't symmetric, a by-pass algebraic trick is used to find its eigenvalues and eigenvectors. Eigenvalue of each discriminant function (a latent variable) is its discriminative power  I was saying about in the first paragraph.  Also, it is worth mentioning that discriminants, albeit uncorrelated, are not geometrically orthogonal as axes drawn in the original variable space.Some potentially related topics that you might want to read:LDA is MANOVA \"deepened\" into analysing latent structure and is a particular case of Canonical correlation analysis (exact equivalence between them as such).How LDA classifies objects and what are Fisher's coefficients. (I link only to my own answers currently, as I remember them, but there is many good and better answers from other people on this site as well). LDA extraction phase computations are as follows. Eigenvalues () of  are the same as of symmetric matrix , where  is the Cholesky root of : an upper-triangular matrix whereby . As for the eigenvectors of , they are given by , where  are the eigenvectors of the above matrix .Canonical correlations corresponding to the eigenvalues are . Whereas eigenvalue of a discriminant is  of the ANOVA of that discriminant, canonical correlation squared is  (T = total sum-of-squares) of that ANOVA.If you normalize columns of eigenvectors  then these values can be seen as the direction cosines of the rotation of axes-variables into axes-discriminants; so with their help one can plot discriminants as axes on the scatterplot defined by the original variables (the eigenvectors, as axes in that variables' space, are not orthogonal).The unstandardized discriminant coefficients or weights are simply the scaled eigenvectors . These are the coefficients of linear prediction of discriminants by the centered original variables. The values of discriminant functions themselves (discriminant scores) are , where  is the centered original variables (input multivariate data with each column centered). Discriminants are uncorrelated. And when computed by the just above formula they also have the property that their pooled within-class covariance matrix is the identity matrix.Optional constant terms accompanying the unstandardized coefficients and allowing to un-center the discriminants if the input variables had nonzero means are , where  is the diagonal matrix of the p variables' means and  is the sum across the variables.In standardized discriminant coefficients, the contribution of variables into a discriminant is adjusted to the fact that variables have different variances and might be measured in different units;  (where diag(Sw) is diagonal matrix with the diagonal of ). Despite being \"standardized\", these coefficients may occasionally exceed 1 (so don't be confused). If the input variables were z-standardized within each class separately, standardized coefficients = unstandardized ones. Coefficients may be used to interpret discriminants.Pooled within-group correlations (\"structure matrix\") between variables and discriminants are given by . Correlations are insensitive to collinearity problems and constitute an alternative (to the coefficients) guidance in assessment of variables' contributions, and in interpreting discriminants.See the complete output of the extraction phase of the discriminant analysis of iris data here.Read this nice later answer which explains a bit more formally and detailed the same things as I did here.This question deals with the issue of standardizing data before doing LDA.","Display_name":"ttnphns","Creater_id":3277,"Start_date":"2013-01-30 02:39:30","Question_id":48786}
{"_id":{"$oid":"5837a582a05283111e4d5ebb"},"Last_activity":"2013-10-09 09:34:16","Creator_reputation":29,"Question_score":1,"Answer_content":"While B/W is an index of discrimination, it is not the most direct measure of discrimination, which is accuracy.  When an observation is predicted to be a member of class 0, and is a member of class 0, a point is awarded.  If predicted class 1 and actual class 1, a point is awarded.  But, if predicted 0 and actual 1, or visa versa, no points.For any sample, and data geometry, there is a statistical paradigm that explicitly finds a model that maximizes classification accuracy--the number of points--that it is possible to get in the application.  All models are developed using an exact, non-parametric methodology.Here is a link to the seminal introduction to the Optimal Data Analysis (ODA) paradigm--where \"Optimal\" means \"classify with maximum-accuracy\" (there is a review of the book, which comes with software and is available in many libraries, on this page): http://www.apa.org/pubs/books/4316000.aspxHere is a link to a free article which introduces early development of the paradigm: http://optimalprediction.com/files/pdf/V1A2.pdf","Display_name":"user31256","Creater_id":31256,"Start_date":"2013-10-09 09:34:16","Question_id":48786}
{"_id":{"$oid":"5837a582a05283111e4d5eca"},"Last_activity":"2016-08-06 09:47:55","Creator_reputation":11400,"Question_score":0,"Answer_content":"In this question and in two other recent questions (here and here) you are interested in the \"slope\" that represents the relation of a continuous variable, NE, to survival in a Cox proportional hazards model. You are particularly interested in whether the relation of NE to survival differs between sexes; furthermore, individuals might belong to either of 2 Groups, G1 and G2. I'll assume that linearity with respect to NEand the proportional hazards assumption are both verified. I'll ignore the random-effects term* that you included in other related questions.Much depends on how you want to treat the group membership issue. In the example of this question,** where there is little evidence for differences in outcome with respect to Group G1 versus G2, preliminary data exploration and subject-matter knowledge might suggest that you simply remove the Group variable from your model. Tests of the interaction term SexM:NE would then provide the answer to your question about sex differences with respect to the relation of NE to survival.If you wish to maintain the breakdown by Group, you might consider using the anova wrapper for the Cox model output in R to provide a single test of the SexM:NE coefficient, combining information from both Groups. The anova function performs a hierarchical test of coefficients in the order of entry into the model, using the equivalent of Type I sums of squares. If the data are reasonably balanced among combinations of covariates (for survival models, in terms of event numbers) then this may provide a useful test provided that you are clear about what it examines. For example, if you specified the following model:Surv(time,status)~ NE + Sex + Group + NE:Sex + NE:Group + Sex:Group + NE:Sex:Groupthen anova would first associate as much as possible with NE, then with Sex, then with Group, and then test whether the NE:Sex interaction significantly explained any residual. Note that this is a different way of evaluating the results of the model from the treatment-contrasts summaries provided by print or summary that you display above, even though it is based on the same model.If you have a particular interest in the 3-way interaction among NE, Sex and Group then you will need to examine contrasts of the coefficients, similar to what you propose but with an important difference in implementation. The test you propose is equivalent to a Wald test. Examining whether there was a difference between males and females with respect to the value of the NE coefficient in Group G2, you would test whether  (MG2-FG2 in your question) is different from 0. As the coefficient estimates are correlated, you need the variance of a sum of correlated variables, which in this example is: \\mathop{\\rm var}(\\beta_{SexM:NE})+\\mathop{\\rm var}(\\beta_{SexM:GroupG2:NE}) + 2\\mathop{\\rm cov}(\\beta_{SexM:NE},\\beta_{SexM:GroupG2:NE})Your proposed Z-test (based on the square root of the variance) ignores the covariance of the estimates of the coefficients, which is necessary and is provided by the corresponding off-diagonal element of the variance-covariance matrix, which you can get from the vcov function applied to your model. If you use the rms package, then there is a contrast function for that package's cph Cox models that allows tests of arbitrary contrasts, including bootstrap non-parametric tests.One warning on this approach, however: Terry Therneau, responsible for much of the survival analysis infrastructure in R, has warned in this vignette that the apparent similarity of Cox regression models to linear regression models does not necessarily extend to tests on contrasts of coefficients. Examine those arguments carefully as you proceed.*In another question you note that you are also evaluating a \"random effect\" that only has 2 levels. Treating a variable with so few levels as a random effect can be considered inadvisable. **The model results in this question differ from those in other questions you have asked recently on the same general matter. It helps to specify the seed for the random-number generator, e.g., with set.seed() in R, before you generate your random data, to have reproducible \"random\" data for demonstration.","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-08-04 08:26:03","Question_id":228257}
{"_id":{"$oid":"5837a582a05283111e4d5ed7"},"Last_activity":"2016-08-06 08:24:49","Creator_reputation":26249,"Question_score":2,"Answer_content":"If you don't have the original casewise data but know the correlations (and hopefully the variances and the sample size) you may simply generate random data having those correlations and analyze that dataset as usual by the canonical correlations program that take in raw data. This way, every output will be correct except the computation of canonical variates' values - for this would need the true data you don't have.But anyway, if you want to program canonical correlation analysis (CCA) youself, here is a step-by-step algorithm for you. You may use any language having basic linear algebra matrix functions.Let  be correlations (or covariances) in Set1 of  variables.  be correlations (or covariances) in Set2 of  variables.  be  correlations (or covariances) between the sets.Make  the diagonal matrix containing standard deviations in Set1; likewise  the diagonal matrix with standard deviations in Set2. If you don't know the variances (such as when you know only the correlations) assume that they all = 1. Then, unstandardized canonical coefficients will be equal to the standardized ones.Doing analysis on covariance matrices is equivalent to analyzing centered variables, while doing analysis on correlation matrices is equivalent to analyzing z-standardized variables.Find , the Cholesky root of : an upper-triangular matrix whereby . (Please note that in the Wikipedia they show it transposed, as \"L\", lower-triangular.) Likewise, find , the Cholesky root of .Compute :, if ; or, if .Do singular-value decomposition of , whereby .Canonical correlations  where  stand on the diagonal of . How to test them for significance - see here.Compute standardized canonical coefficients  (for Set1) and  (for Set2): and  (first  columns of ), if ; or (first  columns of ) and , if .Standardized coefficients correspond to the decompositions of the -matrices as when they were correlation matrices, even if actually the matrices were covariance. Hence \"standardized\" label.Compute unstandardized canonical coefficients  (for Set1) and  (for Set2): and .When the three input -matrices are correlations, not covariances, and the two  diagonals are comprised of ones - which corresponds to the analysis of z-standardized variables - then standardized and unstandardized coefficients are same. Some CCA programs just don't display unstandardized coefficients at all - mostly the programs which base the CCA analysis only on correlations; these programs may omit label \"standardized\" when they output the (standardized) coefficients.Compute canonical loadings  (for Set1) and  (for Set2): and  .Mean squares in columns of  are the proportion-of-variance in Set1 explained by its own canonical variates. Likewise, analogously in .Compute canonical cross-loadings  (for Set1) and  (for Set2): and  .Mean squares in columns of  are the proportion-of-variance in Set1 explained by the opposite set's canonical variates. Likewise, analogously in .Compute canonical variates scores (if you have casewise data at hand):Variates extracted from Set1  and variates extracted from Set2 , where  and  are the (centered) variables of Set1 and Set2.The variates are produced standardized (mean = 0, st. dev. = 1). Pearson correlation between variates  and  is the canonical correlation . For visual explanation of the idea of canonical correlations please look in here.","Display_name":"ttnphns","Creater_id":3277,"Start_date":"2013-11-21 17:32:30","Question_id":77287}
{"_id":{"$oid":"5837a582a05283111e4d5ee4"},"Last_activity":"2016-08-06 08:16:17","Creator_reputation":8000,"Question_score":4,"Answer_content":"Garten et al. (1) compared word vectors obtained by adding input word vectors with output word vectors, vs. word vectors obtained by concatenating input word vectors with output word vectors. In their experiments, concatenating yield significantly better results:(1) Garten, J., Sagae, K., Ustun, V., \u0026amp; Dehghani, M. (2015, June). Combining Distributed Vector Representations for Words. In Proceedings of NAACL-HLT (pp. 95-101).","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2015-10-30 17:53:59","Question_id":177667}
{"_id":{"$oid":"5837a582a05283111e4d5ef1"},"Last_activity":"2016-08-06 07:13:57","Creator_reputation":25350,"Question_score":1,"Answer_content":"The simple answer is: when you need the point estimate. For example, you are making sales forecast that would be used for ordering and allocating certain number of goods in the warehouse. In such case you cannot say that the order should follow some distribution because you need some numeric value for the order. Of course, since you have the posterior distribution you can make interval forecast and quantify the risk associated with the forecast, so it is actually better to have the distribution, but still you may need the point estimate.The second reason is that MAP estimation is much more computationally efficient then, say, MCMC sampling, so using MAP may be more convenient in many cases.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-06 06:44:26","Question_id":228560}
{"_id":{"$oid":"5837a582a05283111e4d5efe"},"Last_activity":"2012-01-23 17:26:59","Creator_reputation":1084,"Question_score":5,"Answer_content":"The formula for Cronbach's alpha is:\r\\alpha =\\frac{K}{K-1} ( 1-\\frac{\\sum_{i=1}^{K}\\sigma^2_{Y_i}}{\\sigma^2_X})\rHere, K is the number of different items you administered to each subject. Sometimes items are different questionnaire items designed to measure the same underlying construct. In your case, it sounds like each item is a separate run of the experiment. In order to calculate Cronbach's alpha, you need to put your data in \"wide format\" (as Michelle mentioned). This means that each of the 200 measurements needs to have its own column/variable. So your columns would be SubjectID, Answer1, Answer2, ... , Answer 200.I'm not sure where your expected answer column comes in. Cronbach's alpha is used to test the consistency of answers to each other, not to some true value, because the true value being measured is latent (i.e., unknown). I suppose you could calculate the correlation between the mean of the answers and the expected answer to see how well people did. Or calculate the times they answered exactly the way you expected and call that their score. As has been suggested in the comments, it doesn't seem to make sense to calculate alpha on the raw scores. If they are only meaningful in relation to the expected answer (i.e., as indicative of how well a subject does on this particular test), you need to use adjusted scores that actually quantify how well a subject did. If an answer of 50 with an expected answer of 25 doesn't mean the same thing as an answer of 50 with an expected answer of 45, then calculating alpha on the raw scores is meaningless. To calculate Cronbach's alpha using R, read the CSV file into a dataframe, reformat into wide format, then run cronbach.alpha on only the answer columns (assuming your columns for subject and values are called SubjectID and Score):x.long \u0026lt;- read.csv(file=\"myfile.csv\")library(reshape2)x.wide \u0026lt;- dcast(x.long, SubjectID ~ Score)library(ltm)cronbach.alpha(x.wide[,-1]) # remove SubjectID in first column","Display_name":"Anne Z.","Creater_id":8676,"Start_date":"2012-01-22 17:26:01","Question_id":21515}
{"_id":{"$oid":"5837a582a05283111e4d5eff"},"Last_activity":"2012-01-23 13:43:22","Creator_reputation":2752,"Question_score":3,"Answer_content":"Update with proviso: it is impossible to answer the question without knowing the detail of what occurred in the experiment, as the risk of providing incorrect advice is very high. Depending on what actually happened in the experiment, it is possible that both of my suggestions below are incorrect. This update is from my perspective that \"software is stupid\" - so long as the input is numbers (in most cases) you'll get an answer on any analysis you do, the problem being that the analysis may not have been appropriate, and therefore the analysis \"answer\" is wrong. This assumes that the desired ultimate outcome is to provide an answer that is appropriate to the method and data, and therefore may recommend a solution that is different to the one asked in the original question.Information on Cronbach's alpha is here and any standard statistics package will calculate this for you. You will need to use the individual trial data to do the calculation so K=200. Assuming you're going to be using R or similar, you'll need the data in wide format. If you believe that you may have training or acclimatisation effects in your data, you may wish to consider a split half reliability alternative, for which you will still need all the individual data points. The point of doing this is to tell you whether V is an appropriate summary statistic, given your data.Is there a reason why you think that something like a mean and standard deviation won't be informative? Internal reliability is normally performed when you don't administer an identical stimulus repeatedly to a person (e.g. when they answer a questionnaire, because a questionnaire doesn't consist of the same question repeated a number of times). If you're presenting them with the same stimulus repeatedly, I'm not sure why you would wish to use a reliability measure. I'm now not convinced that a mean and SD is the right approach either, however depending on what the experiment actually did with the subjects an analysis based on the difference (maybe relative rather than absolute given the response range) between the expected answer and the subject's answer could be a way forward.","Display_name":"Michelle","Creater_id":8605,"Start_date":"2012-01-22 16:26:34","Question_id":21515}
{"_id":{"$oid":"5837a583a05283111e4d5f0e"},"Last_activity":"2016-08-05 08:10:41","Creator_reputation":56,"Question_score":2,"Answer_content":"If you have two models  and  for a sample , then, as long as the models are sensible, you can employ AIC to compare them. Of course, this does not mean that AIC will select the model that is closest to the truth, among the competitors, since AIC is based on asymptotic results. In an extreme scenario, suppose that you want to compare two models, one with 1 single parameter, and another one with 100 parameters, and the sample size is . Then, it is expected to observe a very low precision in the estimation of the model with 100 parameters, while in the model with 1 parameter it is likely that the parameter is accurately estimated. This is one of the arguments against using AIC for comparing models for which the likelihood estimators have very different convergence rates. This may happen even in models with the same number of parameters.Yes, you can use AIC to compare two models where you transformed the response variable in one of them as long as the model still makes sense. However, this is not always the case. If you have a linear model y_i = x_i^T\\beta + e_i,where , this implies that the variable  can take any real value. Consequently, a log transformation makes no sense from a theoretical perspective, even if the sample only contains positive values.This is known as stepwise AIC variable selection. Already implemented in the R command stepAIC().Again, as long as it makes sense to model the data with that sort of models.  Some interesting discussion on the use of AIC can be found here:AIC MYTHS AND MISUNDERSTANDINGS ","Display_name":"Pokemon","Creater_id":126745,"Start_date":"2016-08-05 08:10:41","Question_id":228436}
{"_id":{"$oid":"5837a583a05283111e4d5f1b"},"Last_activity":"2016-08-06 05:03:31","Creator_reputation":13002,"Question_score":1,"Answer_content":"  Is it possible to have GARCH effects without ARCH effects present?See \"Does GARCH(,0) make sense at all?\" (note that the notation there swaps the ARCH and the GARCH orders). It might be technically possible to specify and even estimate such a model (that may depend on the software), but the patterns generated by the model may not make sense in your application.  Also the ARCH-LM test affirms the finding, but with a very high lag order (see picture 1).By looking at the table you may notice that the -value drops significantly at lag 3. Since the ARCH-LM test considers all lags up to the maximum lag together, I would guess that there is an ARCH effect at lag three. And this happens to be the case:  Also, I found an ARCH effect at lag t = 3 instead of t = 1.Regarding   But how do I make that clear in the notation? You could say \"restricted GARCH(3,1) model\" (admittedly, this is not specific enough) or \"GARCH(3,1) with ARCH(1) and ARCH(2) terms restricted to zero\" or just write the corresponding formula \\sigma_t^2 = \\omega + \\alpha_3 \\varepsilon_{t-3}^2 + \\beta_1 \\sigma_{t-1}^2. ","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-03-06 13:06:40","Question_id":200249}
{"_id":{"$oid":"5837a583a05283111e4d5f28"},"Last_activity":"2015-08-12 03:39:27","Creator_reputation":24981,"Question_score":2,"Answer_content":"There is such thing as best linear unbiased prediction (BLUP). It is similar and actualy closely related to Gauss-Markov theorem about best linear unbiased estimator. If your forecasts are BLUP, then the MSE is smallest for the underlying model. If the disturbances are not correlated and homoscedastic, then the lowest MSE comes from OLS, if disturbances are correlated, then the lowest MSE comes from GLS.","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2015-08-12 03:39:27","Question_id":137085}
{"_id":{"$oid":"5837a583a05283111e4d5f29"},"Last_activity":"2015-05-06 13:51:49","Creator_reputation":78,"Question_score":1,"Answer_content":"Actually I have also been spending a lot of time being concentrated on this issue.MSE, specifically in sample, is by definition least in OLS, which many times I simulated and got the results that MSE from OLS is less than MSE from GLS, however the differences between two were not large.Meanwhile, I could also check that S.E of GLS is a bit less than S.E of OLS as in textbook. In a view of population a coefficient of GLS is unbiased as OLS and most efficient, which would give us least MSE in a population. So it shows greater MSE than MSE from OLS only in a current sample data. p.s : Do you get an another insight on this issue? ","Display_name":"kurtkim","Creater_id":70877,"Start_date":"2015-05-06 13:51:49","Question_id":137085}
{"_id":{"$oid":"5837a583a05283111e4d5f36"},"Last_activity":"2016-08-06 04:32:23","Creator_reputation":9087,"Question_score":7,"Answer_content":"The logLik() function provides the evaluation of the log-likelihood by substituting the ML estimates of the parameters for the values of the unknown parameters. Now, the maximum likelihood estimates of the regression parameters (the 's in ) coincide with the least-squares estimates, but the ML estimate of  is , whereas you are using , that is the square root of the unbiased estimate of .\u0026gt;  n \u0026lt;- 5\u0026gt;  x \u0026lt;- 1:n\u0026gt;  set.seed(1)\u0026gt;  y \u0026lt;- 10 + 2*x + rnorm(n, 0, 2)\u0026gt;  modlm \u0026lt;- lm(y ~ x)\u0026gt;  sigma \u0026lt;- summary(modlm)$sigma\u0026gt; \u0026gt;  # value of the likelihood with the \"classical\" sigma hat\u0026gt;  sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma)))[1] -9.192832\u0026gt; \u0026gt;  # value of the likelihood with the ML sigma hat\u0026gt;  sigma.ML \u0026lt;- sigma*sqrt((n-dim(model.matrix(modlm))[2])/n) \u0026gt;  sum(log(dnorm(x = y, mean = predict(modlm), sd = sigma.ML)))[1] -8.915768\u0026gt;  logLik(modlm)'log Lik.' -8.915768 (df=3)","Display_name":"St\u0026#233;phane Laurent","Creater_id":8402,"Start_date":"2013-10-19 11:45:53","Question_id":73196}
{"_id":{"$oid":"5837a583a05283111e4d5f43"},"Last_activity":"2016-08-06 04:18:31","Creator_reputation":152683,"Question_score":5,"Answer_content":"Some hints to help you gain some insightMake up or generate some data consistent with the conditions in the question. Try  and  (choosing some values for , ). Where do the lines pass relative to the first point?Now start as above but try placing  ,  at say 1,2,3,4,5,6,7,8,9 respectively. Where do the lines go?Now place ,  at say 1,2,3,4,5,6,7,8,99 respectively. Where do the lines go?What is special/interesting about the fitted values for the two lines at ? (If it's not clear try some other values for .)Can you prove this is the case more generally?","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-05 09:31:56","Question_id":228468}
{"_id":{"$oid":"5837a583a05283111e4d5f50"},"Last_activity":"2014-10-28 13:54:33","Creator_reputation":6503,"Question_score":2,"Answer_content":"Looking at the documentation and the code of arima, I conclude that the following linear model with ARIMA errors is fitted when exogenous regressors are included:\\begin{eqnarray}\\begin{array}{l}(y_t-\\mu-X_t^{'}\\vec{\\beta})\u0026amp;=\u0026amp;\\phi_1(y_{t-1}-\\mu-X_{t-1}^{'}\\vec{\\beta})+...+\\phi_p(y_{t-p}-\\mu-X_{t-p}^{'}\\vec{\\beta}) \\\\ \u0026amp;+\u0026amp;\\varepsilon_t+\\theta_1\\varepsilon_{t-1}+...+\\theta_q\\varepsilon_{t-q} \\,.\\end{array}\\end{eqnarray} is a row vector containing the values of the external regressors at time  and  is a column vector containing the coefficients related to those regressors.Thus, there is no  term and the mean  is not removed from the exogenous regressors.","Display_name":"javlacalle","Creater_id":48766,"Start_date":"2014-10-28 13:38:12","Question_id":121749}
{"_id":{"$oid":"5837a583a05283111e4d5f5f"},"Last_activity":"2014-10-15 13:43:04","Creator_reputation":21598,"Question_score":7,"Answer_content":"Bootstrap inference for the extremes of a distribution is generally dubious. When bootstrapping n-out-of-n the minimum or maximum in the sample of size , you have  chance that you will reproduce your sample extreme observation, and likewise approximately  chance to reproduce your second extreme observation, and so on. You get a deterministic distribution that has little to do with the shape of the underlying distribution at the tail. Moreover, the bootstrap cannot give you anything below your sample minimum, even when the distribution has the support below this value (as would be the case with most continuous distributions like say normal).The solutions are complicated and rely on the combinations of asymptotics from extreme value theory and subsampling fewer than n observations (actually, way fewer, the rate should converge to zero as ).","Display_name":"StasK","Creater_id":5739,"Start_date":"2014-10-15 09:27:42","Question_id":119748}
{"_id":{"$oid":"5837a583a05283111e4d5f60"},"Last_activity":"2014-10-15 07:42:33","Creator_reputation":11400,"Question_score":0,"Answer_content":"Although much depends on the intended use of your point estimate of the 1st percentile and its confidence interval, I don't see any reason why you can't try to apply bootstrapping to this problem. You are likely to have very wide confidence intervals, and you might want to consider if there's any reason why your sample might have systematically missed low-percentile members of the population.Your sense that bias needs to be taken into account is correct, but that is not the only difficulty in estimating confidence intervals in bootstrapping. These difficulties and different ways to approach the problem are explained in a 1996 Statistical Science paper by DiCiccio and Efron, freely available at http://projecteuclid.org/download/pdf_1/euclid.ss/1032280214.In practice, you should try using the boot package in R. You use the boot function to generate your bootstrap samples, then apply the boot.ci function to obtain confidence interval estimates based on any of several alternative approaches.","Display_name":"EdM","Creater_id":28500,"Start_date":"2014-10-15 07:42:33","Question_id":119748}
{"_id":{"$oid":"5837a583a05283111e4d5f6d"},"Last_activity":"2016-08-06 03:09:34","Creator_reputation":13002,"Question_score":0,"Answer_content":"GARCH(,0) can generate the following patterns of the conditional variance : If  and ,  explodes exponentially (a bit slower). If  and ,  explodes exponentially (a bit faster). If  and ,  is constant and equals the unconditional variance , .This is the case where the GARCH(,0) model is redundant.If  and ,  grows linearly at the speed of .  If  and ,  converges to zero.If  and ,  converges to .Whether any of the patterns makes sense depends on the applications.Nevertheless, there seems to be nothing wrong with the model definition per se, except perhaps for case 3. where the conditional variance equation is redundant.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-06 03:09:34","Question_id":113294}
{"_id":{"$oid":"5837a583a05283111e4d5f6e"},"Last_activity":"2016-03-06 13:29:12","Creator_reputation":1154,"Question_score":0,"Answer_content":"Why bother with GARCH(1,0)? The  term is easier to estimate than the  term (i.e. you can estimate ARCH() with OLS) anyway.Nevertheless, my understanding of the way MLE GARCH programs work is they will set the initial GARCH variance equal to either the sample variance or the expected value (that you derive for this case). Without any ARCH terms, the sample variance version would converge to the long-term one (depending on the size of ). I don't think there would be any change for the expected variance version. So, I'm not sure if you could say it is homoskedastic no matter what (it depends on how you choose the initial variance), but it likely would converge quickly to the expected value for common values of .","Display_name":"John","Creater_id":11623,"Start_date":"2014-08-26 08:04:43","Question_id":113294}
{"_id":{"$oid":"5837a583a05283111e4d5f6f"},"Last_activity":"2014-08-26 08:49:38","Creator_reputation":1143,"Question_score":0,"Answer_content":"If GARCH(p, 0) is \"redundant\", then why did the authors of GARCH put together such a queer model statement? Apparently, the GARCH part is \"redundant\" only when both p and q are equal to zero. That's a common sense explanation. The problem is you are confusing the conditional and unconditional variances. Any GARCH(p, q) process is a stationary process with a constant unconditional variance that can be computed similarly to how you got the limit for . The value of that limit answers a question: if one selects  at random, what is its variance going to be? Conditional variance answers a question: if one knows the entire process history upto , inclusive, then what is the variance of  going to be? Apparently, given the relationship: \\sigma^2_t = \\omega + \\delta \\sigma^2_{t-1}   you will have a much better estimate for the variance of  in the second case.  On the other hand, if it were a “regular” ARMA(p,q) process, the answer would be exactly the same in both cases.","Display_name":"James","Creater_id":54099,"Start_date":"2014-08-26 08:49:38","Question_id":113294}
{"_id":{"$oid":"5837a583a05283111e4d5f80"},"Last_activity":"2016-08-05 23:20:47","Creator_reputation":121,"Question_score":0,"Answer_content":"For these kind of questions, it is possible to use Laplace Smoothing. In general Laplace Smoothing can be written as: \\text{If } y \\in \\begin{Bmatrix} 1,2,...,k\\end{Bmatrix} \\text{then,}\\\\P(y=j)=\\frac{\\sum_{i=1}^{m} L\\begin{Bmatrix} y^{i}=j \\end{Bmatrix} + 1}{m+k}Here  is the likelihood.So in this case the emission probability values (  ) can be re-written as:b_i(o) = \\frac{\\Count(i \\to o) + 1}{\\Count(i) + n}where  is the number of tags available after the system is trained.","Display_name":"Ramesh-X","Creater_id":115826,"Start_date":"2016-08-05 23:20:47","Question_id":212961}
{"_id":{"$oid":"5837a583a05283111e4d5f8f"},"Last_activity":"2015-05-06 01:53:18","Creator_reputation":372,"Question_score":1,"Answer_content":"Finally... @BenBoker was right with predict and plogis. What I am exactly looking for is the predicted values for model terms (i.e. plogis(predict(fit, type = \"terms\")), however, I'm not sure how to get predicted values for model terms from merMod objects. predict.merMod has no type = \"terms\" option.","Display_name":"Daniel","Creater_id":54740,"Start_date":"2015-05-06 01:53:18","Question_id":123703}
{"_id":{"$oid":"5837a583a05283111e4d5f9c"},"Last_activity":"2016-08-05 20:33:50","Creator_reputation":152683,"Question_score":6,"Answer_content":"Note (proceeding somewhat loosely and assuming the necessary derivatives exist and so on for it all to work) that  (e.g. consider a first order Taylor expansion) and so the rate of change . Indeed in the limit as  becomes small this will become the result.So you're effectively saying \"why is  at  smaller than elsewhere?\".The answer is that the function  is flat when you're at the top of the hill (any mode of a continuously differentiable density):The normal has only one mode. The derivative is  there, and non-zero everywhere else. Let's look at a plot of :Clearly it's bigger everywhere than at the mode (= mean = 0).This function (the absolute value of the derivative of the density) doesn't have any particular name I am aware of.The absolute derivative of any other normal will follow the same pattern about its mode.  Obviously, all normal distributions have it, but do all unimodal continuous finite distributions?No. First we have to get away from saying mean/mode. In general the mean and mode aren't in the same place. The mean may well be situated in a place where the derivative is large. So lets focus on the mode.It's perfectly possible to have a unimodal density where the derivative is 0 not at a mode.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-04 22:49:01","Question_id":228384}
{"_id":{"$oid":"5837a583a05283111e4d5fab"},"Last_activity":"2016-08-05 16:20:18","Creator_reputation":11,"Question_score":1,"Answer_content":"I have successfully implemented DTW in 'C' as applied to dynamic signature verification. I used a test data base of Chinese and Dutch signatures to verify EER and got very impressive results. It is currently implemented as a demo on an iPad. My algorithm was hand-coded from several published descriptions. I will share the code if there is a way to get it to you. One thing that also contributed to success was 'normalizing' the input data. This made it a lot easier when comparing disparate data using different sample rates.","Display_name":"user6009837","Creater_id":126787,"Start_date":"2016-08-05 15:43:56","Question_id":109343}
{"_id":{"$oid":"5837a583a05283111e4d5fac"},"Last_activity":"2015-02-27 18:26:39","Creator_reputation":4927,"Question_score":3,"Answer_content":"As far as I understand, by irregular time series you mean unevenly spaced time series, also referred to as irregularly sampled time series. Since I am curious about time series in general, I have performed a brief research on the topic of your (and now mine) interest. The results follow.Despite high popularity of dynamic time warping (DTW) approach in time series analysis, clustering and classification, irregular time series present some challenges to direct application of DTW to such data type (for example, see this paper and this paper). Based on my relatively brief research efforts, it is not totally clear to me, whether it is impossible to apply DTW directly, as some research suggests otherwise (also see this paper/chapter). For more comprehensiveness, I also would like to mention an IMHO excellent and relevant to the topic dissertation on irregular time series.Nevertheless, it seems that this topic is mostly covered by the following two research streams:proposing and evaluating approaches, alternative to DTW, such as model-based ones (see this paper and this paper);proposing and evaluating modified DTW approaches, such as cDTW, EDR, ERP, TWED, envelope transforms, CDTW (continuous DTW - do not confuse with cDTW - constrained DTW!) and others variants (for example, see this paper). An overview of the above-mentioned approaches and results of some empirical comparisons can be found in this paper.Finally, I would like to touch on the subject of open source software, available for research or system implementation, focused on DTW and supporting some of the above-mentioned algorithms for irregular time series. Such software include Python/NumPy-based cDTW module project as well as GPU-focused CUDA-based CUDA-DTW project. For R enthusiasts, a comprehensive Dynamic Time Warp project also should be mentioned (corresponding package dtw is available on CRAN). Even though it might not support many DTW algorithms for irregular time series at the moment (though I think it supports cDTW), I think it is just a matter of time until this project will offer more comprehensive support for DTW algorithms, focused on such type of data. I hope that you have enjoyed reading my answer as much as I have enjoyed researching the topic and writing this post.","Display_name":"Aleksandr Blekh","Creater_id":31372,"Start_date":"2015-01-17 00:55:08","Question_id":109343}
{"_id":{"$oid":"5837a583a05283111e4d5fad"},"Last_activity":"2014-08-03 18:01:19","Creator_reputation":1,"Question_score":0,"Answer_content":"TSdist has a function that determines the distance through dtw. It accepts irregular zoo time series","Display_name":"tiagovrtr","Creater_id":46498,"Start_date":"2014-08-03 18:01:19","Question_id":109343}
{"_id":{"$oid":"5837a583a05283111e4d5fae"},"Last_activity":"2014-07-25 10:18:03","Creator_reputation":189,"Question_score":-1,"Answer_content":"I am only just getting into DTW myself and have not personally used the packages referred to below, but I hope the following may help you.The Cran.R Project, in particular: •   \"ts\" is the basic class for regularly spaced time series using numeric time stamps. •   The \"zoo\" package provides infrastructure for regularly AND IRREGULARLY spaced time series using arbitrary classes for the time stamps.  It is designed to be as consistent as possible with \"ts\". •   zoo: S3 Infrastructure for Regular and Irregular Time Series (Z's ordered observations)References:http://cran.r-project.org/web/views/TimeSeries.html, andhttp://cran.r-project.org/web/packages/zoo/index.htmlBest wishes.","Display_name":"TonyMorland","Creater_id":52243,"Start_date":"2014-07-25 10:18:03","Question_id":109343}
{"_id":{"$oid":"5837a583a05283111e4d5fbb"},"Last_activity":"2016-08-05 16:06:00","Creator_reputation":1,"Question_score":-5,"Answer_content":"From the reference I give below:   is explained as, =    where, 1)  is always between 0 and 100%:2) 0% indicates that the model explains none of the variability of the response data around its mean.3) 100% indicates that the model explains all the variability of the response data around its mean. the \"variation divided by the total variation.\" Also from another reference:\"...The coefficient of determination, , is useful because it gives the proportion of the variance (fluctuation) of one variable that is predictable from the other variable. It is a measure that allows us to determine how certain one can be in making predictions from a certain model/graph.\"  In http://mathbits.com/MathBits/TISection/Statistics2/correlation.htmSince expansion and contraction was covered by the other answer, I just want to make some comments on of aspects that effect the .  An important part of , is the selection of the function used to fit data.  You could have functions that would have have the same expansion or contraction and they can have different  . With the results you mention and the  given I would be incline to try a polynomial (in you independent variable) to see how it fits.  This function that contains multiple terms that are powers of x that would fit best, which would mean you can try a polynomial fit.  In the most general case of the polynomial, you would use spline regression to find an fit. The  review is the first step in analyzing data.  Note: \"Pearson Product-Moment Correlation\" (which can be found on the Internet) discusses using  to determine the \"strength of the correlation.\"The general reference on regression (and also \"over fitting\") I mention above a few times on how to interpret the correlation coefficient is \"Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?\" is http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit.     Lastly, just want to make a general comment on function selection vs model.  A model is development from the principles and laws of the field of study you are working in.  In many case a model is developed even before data is collected (e.g. Theoretical Physics -- I have done this many times).  On the other hand just selecting functions to try to \"fit\" data from experiments is not classified as a model. You are just looking for the best fit to the data (again as a first step) -- then others could study your data and develop/derive a model.   ","Display_name":"jimmeh","Creater_id":99610,"Start_date":"2016-08-03 18:37:42","Question_id":228184}
{"_id":{"$oid":"5837a583a05283111e4d5fbc"},"Last_activity":"2016-08-03 20:14:54","Creator_reputation":518,"Question_score":6,"Answer_content":"The total sum of squares  will be altered by  transformation. The total variation available to be explained in the three cases () will be different. Specifically, if  tends to be substantially larger than , you'll compress the variation by logging it and similarly expand the variation by squaring it (if  is positive but tends to be much smaller than  then the log transform will stretch it and the square will compress it). That stretching/compression may tend to explain the changes in your . ","Display_name":"VCG","Creater_id":124896,"Start_date":"2016-08-03 18:35:45","Question_id":228184}
{"_id":{"$oid":"5837a583a05283111e4d5fcc"},"Last_activity":"2016-08-05 05:28:57","Creator_reputation":5797,"Question_score":6,"Answer_content":"\\mathbb{E}[ \\text{sign}(X_1) \\text{sign}(X_2)] = 1 * (P(X_1 \\ge 0,X_2 \\ge 0) +  P(X_1 \\le 0,X_2 \\le 0)) - (P(X_1 \\ge 0,X_2 \\le 0) + P(X_1 \\le 0,X_2 \\ge 0)) which in turn = 2P(X_1 \\ge 0,X_2 \\ge 0) - 2P(X_1 \\ge 0,X_2 \\le 0) by symmetry.Plugging in the Bivariate Normal density, this evaluates (integrates) to . The details of performing the integration are left to you.Edit: Have changed what was  to  to match edit of question.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-02 17:21:11","Question_id":226940}
{"_id":{"$oid":"5837a583a05283111e4d5fcd"},"Last_activity":"2016-08-04 20:40:20","Creator_reputation":1165,"Question_score":8,"Answer_content":"For convenience let's call  as  and , respectively. There are only  possible combinations of : , and at least one of the  being . Now, since we are looking for , ignoring the states of  will not affect the result. Hence, \\begin{align*}E[S_1S_2]\u0026amp;=1\\times P(S_1=1,S_2=1)+1\\times P(S_1=-1,S_2=-1)\\\\\u0026amp;\\quad+(-1)\\times P(S_1=1,S_2=-1)+(-1)\\times P(S_1=-1,S_2=1)\\\\\u0026amp;=1\\times P(X_1\u0026gt;0,X_2\u0026gt;0)+1\\times P(X_1\u0026lt;0,X_2\u0026lt;0)\\\\\u0026amp;\\quad+(-1)\\times P(X_1\u0026gt;0,X_2\u0026lt;0)+(-1)\\times P(X_1\u0026lt;0,X_2\u0026gt;0).\\end{align*}Further, one can show that P(X_1\u0026gt;0,X_2\u0026gt;0)=P(X_1\u0026lt;0,X_2\u0026lt;0)=\\frac{1}{4}+\\frac{1}{2\\pi}\\sin^{-1}(\\rho), and P(X_1\u0026gt;0,X_2\u0026lt;0)=P(X_1\u0026lt;0,X_2\u0026gt;0)=\\frac{1}{2\\pi}\\cos^{-1}(\\rho). So \\begin{align*}E[S_1S_2]\u0026amp;=\\frac{1}{2}+\\frac{1}{\\pi}\\sin^{-1}(\\rho)-\\frac{1}{\\pi}\\left(\\frac{\\pi}{2}-\\sin^{-1}(\\rho)\\right)\\\\\u0026amp;=\\frac{2}{\\pi}\\sin^{-1}(\\rho).\\end{align*}","Display_name":"Francis","Creater_id":29905,"Start_date":"2016-08-02 17:34:03","Question_id":226940}
{"_id":{"$oid":"5837a583a05283111e4d5fde"},"Last_activity":"2016-08-05 14:00:03","Creator_reputation":4884,"Question_score":1,"Answer_content":"Supervised latent Dirichlet allocation seems the most natural fit. In this model, the representation of each document is augmented with some sort of response variable  (see illustration from the paper below). (In your case, a categorical variable denoting subject, or a one-hot vector equivalent.) Because both the observed words and the response variable depend on the topics, this will produce different topic distributions from standard LDA. The paper's experiments demonstrated that this was more effective for prediction than standard LDA followed by regression:  We compared sLDA to linear regression on the  from unsupervised LDA. This is the regression equivalent of using LDA topics as classification features [4]. Figure 2 (L) illustrates that sLDA provides  improved predictions on both data sets. Moreover, this improvement does not come at the cost of document model quality. The per-word hold-out likelihood comparison in Figure 2 (R) shows that  sLDA fits the document data as well or better than LDA.Of course, this requires labelled documents.","Display_name":"Sean Easter","Creater_id":28462,"Start_date":"2016-08-05 14:00:03","Question_id":184592}
{"_id":{"$oid":"5837a583a05283111e4d5fec"},"Last_activity":"2016-08-05 12:50:13","Creator_reputation":4884,"Question_score":4,"Answer_content":"Since the hypergeometric is discrete with support over a set of non-negative integers, , where  is the CDF of the hypergeometric distribution. The formula for the CDF is given in the same article you link to. I'm not sure I'd call it nice, but it's widely implemented if you're looking for a ready solution.","Display_name":"Sean Easter","Creater_id":28462,"Start_date":"2016-08-05 12:50:13","Question_id":228497}
{"_id":{"$oid":"5837a583a05283111e4d5ff9"},"Last_activity":"2016-08-05 12:46:29","Creator_reputation":5797,"Question_score":1,"Answer_content":"The problem is not as you stated (reflecting the beginning of your solution process). That's where your error is.Per the problem statement, the total amount added to the storage bin for the week is the sum of 20 i.i.d. N(1.5,0.0625) random variables, which is N(30,20 * 0.0625).  Your calculation is based on making one draw from a N(1.5,0.0625) random variable, call it , say, and then using .  That is very different than the sum of 20 i.i.d. random variables having the same distribution as A.  You made the corresponding error on the removal term.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2016-08-05 12:46:29","Question_id":228489}
{"_id":{"$oid":"5837a583a05283111e4d6006"},"Last_activity":"2014-07-02 19:08:36","Creator_reputation":5522,"Question_score":6,"Answer_content":"This is an issue with lm.wrapper = function(formula,...)  lm(formula=formula,...)x=(1:27)wrapper(ts.input~x,weights=ts.weights)produces the exact same error. If you read the source code to tslm, you'll find that lm is called in more or less the sam way.I found here that ..1 means the first argument included in ..., which in this case is the only argument, weights. It suggests that this kind of error could be caused because argument being passed doesn't exist in the environment from which the function is being called. You can see this behavior with wrapper(ts.input~x,weights=foo) if you don't have any object called foo.Running a traceback on wrapper(ts.input~x,weights=ts.weights) reveals:8 eval(expr, envir, enclos) 7 eval(extras, data, env) 6 model.frame.default(formula = formula, weights = ..1, drop.unused.levels = TRUE) 5 stats::model.frame(formula = formula, weights = ..1, drop.unused.levels = TRUE) 4 eval(expr, envir, enclos) 3 eval(mf, parent.frame()) 2 lm(formula = formula, ...) 1 wrapper(ts.input ~ x, weights = ts.weights) And in the source code of lm,mf \u0026lt;- match.call(expand.dots = FALSE)m \u0026lt;- match(c(\"formula\", \"data\", \"subset\", \"weights\", \"na.action\",     \"offset\"), names(mf), 0L)mf \u0026lt;- mf[c(1L, m)]mf$drop.unused.levels \u0026lt;- TRUEmf[[1L]] \u0026lt;- quote(stats::model.frame)mf \u0026lt;- eval(mf, parent.frame())if (method == \"model.frame\")     return(mf)which suggests that the call to model.frame is picking up on the ... in a strange way.So I decided to try it without ...:wrapper = function(formula,weights)  lm(formula=formula,weights=weights)x=(1:27)wrapper(ts.input~x,weights=ts.weights)which produced Error in model.frame.default(formula = formula, weights = weights, drop.unused.levels = TRUE) : invalid type (closure) for variable '(weights)' A closure, of course, is a function in R. Which can mean only one thing... weights is already a function in the global environment. Indeed, ?weights reveals that it's ironically the extractor function for model weights. It's a no-brainer that it gets search priority over local variables. So I changed the argument names (since formula is also a function):wrapper = function(fm,ws){  print(ws)  lm(formula=fm,weights=ws)}x=(1:27)wrapper(fm=ts.input~x,ws=ts.weights)now produces [1] 2.260324e-05 6.385091e-05 1.803683e-04 5.094997e-04 1.439136e-03 4.064300e-03 [7] 1.147260e-02 3.234087e-02 9.082267e-02 2.523791e-01 6.815483e-01 1.712317e+00[13] 3.685455e+00 6.224593e+00 8.232410e+00 9.293615e+00 9.737984e+00 9.905650e+00[19] 9.966395e+00 9.988078e+00 9.995776e+00 9.998504e+00 9.999471e+00 9.999813e+00[25] 9.999934e+00 9.999977e+00 9.999992e+00 Error in eval(expr, envir, enclos) : object 'ws' not found And if you run traceback you still get the same issue with model.frame. So I'm completely baffled. My only conclusion is that a) it has nothing to do with tslm or time series analysis, and b) the problem lies somewhere in the way arguments are passed around inside lm.That probably doesn't help you at all, but hopefully at least someone can come along and explain what's going on here. My provisional answer to your actual question of how to use weights in tslm is that you can't.","Display_name":"ssdecontrol","Creater_id":36229,"Start_date":"2014-07-02 19:08:36","Question_id":105605}
{"_id":{"$oid":"5837a583a05283111e4d6015"},"Last_activity":"2016-08-05 12:21:16","Creator_reputation":379,"Question_score":0,"Answer_content":"Eye-balling. The fit does not look too bad. There is a clear seasonal pattern that your model seems to be picking up.Residuals. What you should always look at are the fit and the forecast residuals: fit residuals, , and forecast residuals, . ( Where  is the prediction residual of the forecast at time  for time . )Some ways of looking at these residuals: Is there a seasonal pattern in the residuals? If so, find out its period and add this into back into the model through the exogenous Fourier regressors.Is there remaining auto-correlation in the residuals? Yes: add AR or MA terms, or change model along different avenues. No: you're good.Look at histogram of residuals: do they look normal? Is their mean equal to zero? If yes: you're fine. If no: continue modelling.Without more info, that's all I can tell you.p.s. Unfortunately the words error and residual are often used loosely and interchangeably. Idem: prediction and forecast. Residuals are observed and are estimates of error, a random variable. Forecast always refers to a prediction into the future.p.p.s. It is good practice to report the Mean Squared Prediction Error [MSPE] instead of the Sum of Squared (Prediction) Residuals [SSR]. ","Display_name":"Jim","Creater_id":67042,"Start_date":"2016-08-05 09:11:21","Question_id":228364}
{"_id":{"$oid":"5837a583a05283111e4d6022"},"Last_activity":"2016-08-05 12:12:55","Creator_reputation":19604,"Question_score":0,"Answer_content":"Cosine is equivalent to squared Euclidean distance on normalized data.So yes, you can use k-means with cosine (see \"spherical k-means\"). Or you just scale your data to unit length, and use regular k-means.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-08-05 12:12:55","Question_id":228419}
{"_id":{"$oid":"5837a583a05283111e4d602f"},"Last_activity":"2016-08-05 11:38:35","Creator_reputation":111,"Question_score":1,"Answer_content":"I ran into the same issue with some data I was using in my research- while I don't quite understand what leads to this mathematically/computationally, hopefully my answer and the code below helps:Two comments on your problem:The raw CSV file includes data fields which have not been de-meaned or scaled. Normalizing the data is a helpful step that is important for some types of processing. This can be accomplished with the sklearn StandardScaler() class.The l1-regularized covariance implementation seems to be sensitive to instabilities when the empirical covariance matrix has a broad eigenvalue range. Your initial data has eigenvalues in the range of [0, 3e6].After normalizing your input data, the eigenvalues of your empirical covariance matrix still span a relatively large range of about [0-8]. Shrinking this using the sklearn.covariance.shrunk_covariance() function can bring it into a more computationally acceptable range (from what I've read, [0,1] is ideal but slgihtly larger ranges also appear to work).If anyone knows what's going on mathematically/computationally that causes this error, and what the caveats of shrinking the covariance matrix are in terms of the interpretation of the output, I'd love to hear your comments and improvements. However, the code below appears to both work for the problem presented by @rano, and the errors that I've run into with my research (~10k samples of data in the energy market).import numpy as npimport pandas as pdfrom sklearn import covariance, preprocessingmyData  = pd.read_csv('Data/weight_comp_simple_prop.df.train.csv')X = myData.values.astype('float64')myScaler = preprocessing.StandardScaler()X = myScaler.fit_transform(X)emp_cov = covariance.empirical_covariance(X)shrunk_cov = covariance.shrunk_covariance(emp_cov, shrinkage=0.8) # Set shrinkage closer to 1 for poorly-conditioned dataalphaRange = 10.0 ** np.arange(-8,0) # 1e-7 to 1e-1 by order of magnitudefor alpha in alphaRange:    try:         graphCov = covariance.graph_lasso(shrunk_cov, alpha)        print(\"Calculated graph-lasso covariance matrix for alpha=%s\"%alpha)    except FloatingPointError:        print(\"Failed at alpha=%s\"%alpha)","Display_name":"emunsing","Creater_id":126767,"Start_date":"2016-08-05 11:38:35","Question_id":172911}
{"_id":{"$oid":"5837a583a05283111e4d603e"},"Last_activity":"2016-08-05 11:06:20","Creator_reputation":24981,"Question_score":25,"Answer_content":"The correlation measures linear relationship. In informal context relationship means something stable.  When we calculate the sample correlation for stationary variables and increase the number of available data points this sample correlation tends to true correlation. It can be shown that for prices, which usually are random walks, the sample correlation tends to random variable. This means that no matter how much data we have, the result will always be different.Note I tried expressing mathematical intuition without the mathematics. From mathematical point of view the explanation is very clear: Sample moments of stationary processes converge in probability to constants. Sample moments of random walks converge to integrals of brownian motion which are random variables. Since relationship is usually expressed as a number and not a random variable, the reason for not calculating the correlation for non-stationary variables becomes evident.Update Since we are interested in correlation between two variables assume first that they come from stationary process . Stationarity implies that  and  do not  depend on . So correlationcorr(X_t,Y_t)=\\frac{cov(X_t,Y_t)}{\\sqrt{DX_tDY_t}}also does not depend on , since all the quantities in the formula come from matrix , which does not depend on . So the calculation of sample correlation \\hat{\\rho}=\\frac{\\frac{1}{T}\\sum_{t=1}^T(X_t-\\bar{X})(Y_t-\\bar{Y})}{\\sqrt{\\frac{1}{T^2}\\sum_{t=1}^T(X_t-\\bar{X})^2\\sum_{t=1}^T(Y_t-\\bar{Y})^2}}makes sense, since we may have reasonable hope that sample correlation will estimate . It turns out that this hope is not unfounded, since for stationary processes satisfying certain conditions we have that , as  in probability. Furthermore  in distribution, so we can test the hypotheses about . Now suppose that  is not stationary. Then  may depend on . So when we observe a sample of size  we potentialy need to estimate  different correlations . This is of course infeasible, so in best case scenario we only can estimate some functional of  such as mean or variance. But the result may not have sensible interpretation.Now let us examine what happens with correlation of probably most studied non-stationary process random walk. We call process  a random walk if , where  is a stationary process. For simplicity assume that . Then \\begin{align}corr(X_tY_t)=\\frac{EX_tY_t}{\\sqrt{DX_tDY_t}}=\\frac{E\\sum_{s=1}^tU_t\\sum_{s=1}^tV_t}{\\sqrt{D\\sum_{s=1}^tU_tD\\sum_{s=1}^tV_t}}\\end{align}To simplify matters further, assume that  is a white noise. This means that all correlations  are zero for . Note that this does not restrict  to zero. Then \\begin{align}corr(X_t,Y_t)=\\frac{tEU_tV_t}{\\sqrt{t^2DU_tDV_t}}=corr(U_0,V_0).\\end{align}So far so good, though the process is not stationary, correlation makes sense, although we had to make same restrictive assumptions. Now to see what happens to sample correlation we will need to use the following fact about random walks, called functional central limit theorem:\\begin{align}\\frac{1}{\\sqrt{T}}Z_{[Ts]}=\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{[Ts]}C_t\\to (cov(C_0))^{-1/2}W_s,\\end{align}in distribution, where  and  is bivariate Brownian motion (two-dimensional Wiener process). For convenience introduce definition .Again for simplicity let us define sample correlation as\\begin{align}\\hat{\\rho}=\\frac{\\frac{1}{T}\\sum_{t=1}^TX_{t}Y_t}{\\sqrt{\\frac{1}{T}\\sum_{t=1}^TX_t^2\\frac{1}{T}\\sum_{t=1}^TY_t^2}}\\end{align}Let us start with the variances. We have\\begin{align}E\\frac{1}{T}\\sum_{t=1}^TX_t^2=\\frac{1}{T}E\\sum_{t=1}^T\\left(\\sum_{s=1}^tU_t\\right)^2=\\frac{1}{T}\\sum_{t=1}^Tt\\sigma_U^2=\\sigma_U\\frac{T+1}{2}.\\end{align}This goes to infinity as  increases, so we hit the first problem, sample variance does not converge. On the other hand  continuous mapping theorem in conjunction with functional central limit theorem gives us\\begin{align}\\frac{1}{T^2}\\sum_{t=1}^TX_t^2=\\sum_{t=1}^T\\frac{1}{T}\\left(\\frac{1}{\\sqrt{T}}\\sum_{s=1}^tU_t\\right)^2\\to \\int_0^1M_{1s}^2ds\\end{align}where convergence is convergence in distribution, as .Similarly we get\\begin{align}\\frac{1}{T^2}\\sum_{t=1}^TY_t^2\\to \\int_0^1M_{2s}^2ds\\end{align}and\\begin{align}\\frac{1}{T^2}\\sum_{t=1}^TX_tY_t\\to \\int_0^1M_{1s}M_{2s}ds\\end{align}So finally  for sample correlation of our random walk we get\\begin{align}\\hat{\\rho}\\to \\frac{\\int_0^1M_{1s}M_{2s}ds}{\\sqrt{\\int_0^1M_{1s}^2ds\\int_0^1M_{2s}^2ds}}\\end{align}in distribution as .  So although correlation is well defined, sample correlation does not converge towards it, as in stationary process case. Instead it converges to a certain random variable. ","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2011-02-18 08:46:08","Question_id":7376}
{"_id":{"$oid":"5837a583a05283111e4d603f"},"Last_activity":"2011-02-18 12:18:50","Creator_reputation":147568,"Question_score":10,"Answer_content":"  ...is the computation of correlation whose data is non-stationary even a valid statistical calculation?Let  be a discrete random walk.  Pick a positive number .  Define the processes  and  by ,  if , and otherwise ; and .  In other words,  starts out identical to  but every time  rises above , it switches signs (otherwise emulating  in all respects).(In this figure (for )  is blue and  is red.  There are four switches in sign.)In effect, over short periods of time  tends to be either perfectly correlated with  or perfectly anticorrelated with it; however, using a correlation function to describe the relationship between  and  wouldn't be useful (a word that perhaps more aptly captures the problem than \"unreliable\" or \"nonsense\").Mathematica code to produce the figure:With[{h=5},pv[{p_, v_}, w_] := With[{q=If[v \u0026gt; h, -p, p]}, {q, q w}];w = Accumulate[RandomInteger[{-1,1}, 25 h^2]];{p,v} = FoldList[pv, {1,0}, w] // Transpose;ListPlot[{w,v}, Joined-\u0026gt;True]]","Display_name":"whuber","Creater_id":919,"Start_date":"2011-02-18 12:18:50","Question_id":7376}
{"_id":{"$oid":"5837a583a05283111e4d604c"},"Last_activity":"2016-08-03 04:26:21","Creator_reputation":5189,"Question_score":6,"Answer_content":"If the s are independent,  is the CDF of the maximum of , since\\begin{equation}\\mathbb{P}(\\max ( X_1,X_2,\\ldots,X_n) \\leq x) = \\mathbb{P}(X_1, \\leq x, X_2\\leq x \\ldots, X_n \\leq x) \\\\ = \\mathbb{P}(X_1 \\leq x)\\,\\mathbb{P}(X_2 \\leq x)\\,\\ldots\\,\\mathbb{P}(X_n \\leq x) = F_1(x)\\,F_2(x)\\,\\ldots\\,F_n(x).\\end{equation}Thus, a realization from the distribution  may be generated by simulating independent realizations from all s   and taking the maximum.  100 independent random numbers may be generated by repeating this 100 times. More efficient (using less random variables) sampling may be attainable in special cases (such as if  itself has an easy-to-sample-from form), but assuming only the premises of the question, I don't see how there could be any better method than taking the maximum.","Display_name":"Juho Kokkala","Creater_id":24669,"Start_date":"2016-08-03 01:12:05","Question_id":227008}
{"_id":{"$oid":"5837a583a05283111e4d605b"},"Last_activity":"2016-08-05 10:33:20","Creator_reputation":1130,"Question_score":0,"Answer_content":"Consider the following case (where  is a normal distribution, and  is 1 if  is true and 0 if  is false):And suppose we have . It should be trivial to establish that  is normally distributed.If we regress  on  and  jointly, what will the resulting models and residuals look like? What about individually?","Display_name":"Matthew Graves","Creater_id":91926,"Start_date":"2016-08-05 09:28:55","Question_id":228338}
{"_id":{"$oid":"5837a583a05283111e4d605c"},"Last_activity":"2016-08-05 09:03:52","Creator_reputation":56,"Question_score":2,"Answer_content":"(One step back first) Typically, the assumptions underlying a linear regression model y_i = x_i^T\\beta + e_i,\\,\\,\\, i=1,\\dots,nare:The errors  are i.i.d. with Normal distribution with mean zero and variance .The covariates are either a sequence of deterministic vectors or they come from a joint distribution such that for large enough  the matrix  is positive definite, where  is the design matrix., the covariates and the errors are independent.Of course, there are all sorts of generalizations of these assumptions (e.g. heteroscedasticity).Suppose that you remove some covariates and keep  covariates, then  are not necessarily normal since , and consequently nothing guarantees the normality of the residuals under the smaller model.In practice, if you fit a model, and the residuals look normal, this does not imply that under a smaller model the residuals will also look normal. Have a look at the following example in R for instance:# Simulated datans = 1000 # sample sizeX = cbind(1,rgamma(ns,5,5),rgamma(ns,5,5,)) # design matrixe = rnorm(ns,0,0.5) # errorsbeta = c(1,2,3) # true regression parameters y = X%*%beta + e  # simulating the responses # fitting the modellmr = lm(y~-1+X) # residualsres = lmrresiduals# histogram and normality test: not normal looking and failing the testhist(resz)shapiro.test(resz)","Display_name":"Pokemon","Creater_id":126745,"Start_date":"2016-08-05 09:03:52","Question_id":228338}
{"_id":{"$oid":"5837a583a05283111e4d605d"},"Last_activity":"2016-08-05 07:50:05","Creator_reputation":33,"Question_score":0,"Answer_content":"No, because every time you build a model, you must test the normal distribution and the zero mean of the residuals. So, in your case, if you drop a variable of the first model you must re-test the normal distribution of the residuals (and the zero mean of course).","Display_name":"Enzo D\u0026#39;Innocenzo","Creater_id":124731,"Start_date":"2016-08-05 07:26:26","Question_id":228338}
{"_id":{"$oid":"5837a583a05283111e4d606a"},"Last_activity":"2016-08-05 03:16:53","Creator_reputation":181,"Question_score":1,"Answer_content":"In order to better understand the problem I think it is worth explaining the main idea of how order-preserving cryptography works. Imagine that we have column vector  than we want to encrypt. The encryption function is   and it is monotonically increasing, and potentially with varying slope. The cipher text of  is , and has the following propertiesif  then if  then  (this is usually prevented)if   does not give information about  , and The second property can be prevented by adding noise to . For example if  is an integer between . Then, , where  is vector a of random integer between  . This transformation preserves the ordering in , but removes the identification of attributes with the same value.Given that I think you should stick to tree based classifiers such as random forest. ","Display_name":"PolinLnd","Creater_id":94645,"Start_date":"2016-08-05 03:16:53","Question_id":226699}
{"_id":{"$oid":"5837a583a05283111e4d606b"},"Last_activity":"2016-08-03 08:31:19","Creator_reputation":58,"Question_score":0,"Answer_content":"I don't think you should use order to predict the binary outcome since if a model trained on your data is applied to totally unseen data which was collected in a slightly different way (not sure how that data was collected) then it's going to perform very badly. Moreover, such \"golden features\" based on data ordering are sometimes a result of data leaks and you do not want to train your model on such data for the reason given above.","Display_name":"slazien","Creater_id":121401,"Start_date":"2016-08-03 08:31:19","Question_id":226699}
{"_id":{"$oid":"5837a583a05283111e4d607a"},"Last_activity":"2016-08-05 09:29:14","Creator_reputation":338,"Question_score":0,"Answer_content":"Yes, you could run into the problem of over-fitting. Any time you are tuning parameters in hopes of matching some known output (and perhaps in other situations as well) you run the risk over over-fitting. I would suggest holding out some data, using something like cross-validation to tune your parameters, and then seeing how it works on the held out data. If the result is good, re-train it on the entire data set with those parameters and use that going forward.Note that, as @Prerit mentioned, its not clear if over-fitting will occur in this case. Nevertheless, there is no harm in trying to avoid it.","Display_name":"roundsquare","Creater_id":122754,"Start_date":"2016-08-05 09:29:14","Question_id":228282}
{"_id":{"$oid":"5837a583a05283111e4d607b"},"Last_activity":"2016-08-04 09:28:50","Creator_reputation":4,"Question_score":-2,"Answer_content":"I think it should be fine to run the algorithm multiple times to find the best parameters. Clustering does depend on the initial centroid selection, so you might want to randomize the behavior and then take the average of the metrics. That should avoid overfitting.","Display_name":"Prerit","Creater_id":126628,"Start_date":"2016-08-04 09:20:44","Question_id":228282}
{"_id":{"$oid":"5837a583a05283111e4d6089"},"Last_activity":"2016-08-05 01:22:24","Creator_reputation":1165,"Question_score":1,"Answer_content":" in this setup has random part, e.g. with additive error . So for fixed , new response  to the predictor  needs not to be the same as the corresponding training response , hence the expectation . \"Biased downward\" just means that  is on average less than the true prediction error.","Display_name":"Francis","Creater_id":29905,"Start_date":"2016-08-05 01:22:24","Question_id":228394}
{"_id":{"$oid":"5837a583a05283111e4d6095"},"Last_activity":"2016-08-05 08:49:14","Creator_reputation":169,"Question_score":0,"Answer_content":"The description of the data would suggest that you have time-to-event data and that analysis with Cox's model could be employed to help resolve some of the distributional issues you describe. The issue here is not so much identifying a perfect distribution for your data as deciding what type of data you actually have. Based on the ability to perform 2500 repetitions of the experiment, please comment on whether your data are generated from a deterministic or stochastic process, which are less well-suited to statistical inference. In order of preference, the negative binomial, Poisson or normal distributions are candidate distributions given your data. To use the normal distribution, consider a data transformation to reduce the skew. For each distribution, fit statistics are available to test the fit to the respective distribution. Analysis of ranks can also be used, as you suggest in the original post. Note that ranks are being compared and not medians. For this sample size, non-parametric analysis would like perform as well as methods based on the normal distribution. The skew is not important for nonparametric analysis, as all values are ranked and analyzed.Very generally, I would recommend a regression technique given the need to evaluate multiple independent variables. I would generally prefer a time-to-event analysis given the natural time to convergence you describe as the dependent variable. Cox's model or a parametric Weibull model (fit statistics available for both) may allow you to nicely evaluate your parameters using one test instead of multiple tests for values of each parameter, suggested in a previous version of the post.","Display_name":"Todd","Creater_id":64263,"Start_date":"2016-08-05 08:49:14","Question_id":227079}
{"_id":{"$oid":"5837a583a05283111e4d60a4"},"Last_activity":"2015-10-04 13:43:53","Creator_reputation":234,"Question_score":1,"Answer_content":"What are your variables? I would usually go with AIC. In this case, it shows that the most unrealistic specification is the most appropriate for your data. However, I think you should decided what to choose by first testing for unit roots and then only use either the 3rd (i.e.Linear trend, intercept and no trend) or the 4th (i.e.Linear trend, intercept and trend) specification above. If your data is trend stationary and some are unit root, I would recommend considering the 4th specification.","Display_name":"mr.rox","Creater_id":23970,"Start_date":"2015-10-04 13:43:53","Question_id":172296}
{"_id":{"$oid":"5837a583a05283111e4d60b1"},"Last_activity":"2016-08-05 08:24:05","Creator_reputation":1300,"Question_score":0,"Answer_content":"if  then the expression simplifies alot (the 2nd expression becomes impossible).","Display_name":"MikeP","Creater_id":36115,"Start_date":"2016-08-05 08:24:05","Question_id":228433}
{"_id":{"$oid":"5837a583a05283111e4d60c1"},"Last_activity":"2016-08-05 08:19:02","Creator_reputation":4907,"Question_score":4,"Answer_content":"In a forecasting problem (i.e., when you need to forecast  given  , with the use of a learning set  ), the rule of the thumb (to be done before any complex modelling) areClimatology ( forecast by the mean observed value over the learning set, possibly by removing obvious periodic patterns) Persistence ( forecast by the last observed value: ). What I often do now as a last simple benchmark / rule of the thumb is using randomForest(~, data=learningSet) in R software. It gives you (with 2 lines of code in R) a first idea of what can be achieved without any modelling.","Display_name":"robin girard","Creater_id":223,"Start_date":"2010-11-02 06:02:40","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60c2"},"Last_activity":"2016-08-05 08:18:43","Creator_reputation":28606,"Question_score":2,"Answer_content":"Despite increasingly larger datasets and more powerful software, over-fitting models is a major danger to researchers, especially those who have not yet been burned by over-fitting. Over-fitting means that you have fitted something more complicated than your data and the state of the art. Like love or beauty, it is hard to define, let alone to define formally, but easier to recognise. A minimal rule of thumb is 10 data points for every parameter estimated for anything like classical regression, and watch out for the consequences if you ignore it. For other analyses, you usually need much more to do a good job, particularly if there are rare categories in the data. Even if you can fit a model easily, you should worry constantly about what it means and how far it is reproducible with even a very similar dataset. ","Display_name":"Nick Cox","Creater_id":22047,"Start_date":"2016-08-05 08:00:13","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60c3"},"Last_activity":"2016-08-05 08:09:13","Creator_reputation":28606,"Question_score":0,"Answer_content":"There are no criteria to choose information criteria. Once someone says something like \"The ?IC indicates this, but it is known often to give the wrong results\" (where ? is any letter you like), you know that you will have also to think about the model and particularly whether it makes scientific or practical sense. No algebra can tell you that. ","Display_name":"Nick Cox","Creater_id":22047,"Start_date":"2016-08-05 08:09:13","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60c4"},"Last_activity":"2016-08-05 08:04:59","Creator_reputation":28606,"Question_score":0,"Answer_content":"If the model won't converge easily and quickly, it could be the fault of the software. It is, however, much more common that your data are not suitable for the model or the model is not suitable for the data. It could be hard to tell which, and empiricists and theorists can have different views. But subject-matter thinking, really looking at the data, and constantly thinking about the interpretation of the model help as much as anything can. Above all else, try a simpler model if a complicated one won't converge. There is no gain in forcing convergence or in declaring victory and taking results after many iterations but before your model really has converged. At best you fool yourself if you do that. ","Display_name":"Nick Cox","Creater_id":22047,"Start_date":"2016-08-05 08:04:59","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60c5"},"Last_activity":"2013-06-17 11:33:36","Creator_reputation":11870,"Question_score":4,"Answer_content":"In instrumental variables regression always check the joint significance of your instruments. The Staiger-Stock rule of thumb says that an F-statistic of less than 10 is worrisome and indicates that your instruments might be weak, i.e. they are not sufficiently correlated with the endogenous variable. However, this does not automatically imply that an F above 10 guarantees strong instruments. Staiger and Stock (1997) have shown that instrumental variables techniques like 2SLS can be badly biased in \"small\" samples if the instruments are only weakly correlated with the endogenous variable. Their example was the study by Angrist and Krueger (1991) who had more than 300,000 observations - a disturbing fact about the notion of \"small\" samples.","Display_name":"Andy","Creater_id":26338,"Start_date":"2013-06-17 11:33:36","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60c6"},"Last_activity":"2013-06-17 11:04:25","Creator_reputation":17903,"Question_score":28,"Answer_content":"There is no free lunchA large part of statistical failures is created by clicking a big shiny button called \"Calculate significance\" without taking into account its burden of hidden assumptions. RepeatEven if a single call to a random generator is involved, one may have luck or bad luck and so jump to the wrong conclusions.","Display_name":"mbq","Creater_id":88,"Start_date":"2010-09-16 05:08:07","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60c7"},"Last_activity":"2013-06-17 11:03:39","Creator_reputation":20442,"Question_score":43,"Answer_content":"Keep your analysis reproducible. A reviewer or your boss or someone else will eventually ask you how exactly you arrived at your result - probably six months or more after you did the analysis. You will not remember how you cleaned the data, what analysis you did, why you chose the specific model you used... And reconstructing all this is a pain.Corollary: use a scripting language of some kind, put comments in your analysis scripts, and keep them. What you use (R, SAS, Stata, whatever) is less important than having a completely reproducible script. Reject environments in which this is impossible or awkward. ","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2010-09-18 11:15:49","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60c8"},"Last_activity":"2011-04-10 07:26:46","Creator_reputation":914,"Question_score":6,"Answer_content":"Think hard about the underlying data generating process. If the model you want to use doesn't reflect the DGP, you need to find a new model.","Display_name":"Jason Morgan","Creater_id":3265,"Start_date":"2011-04-10 07:26:46","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60c9"},"Last_activity":"2011-01-16 06:48:53","Creator_reputation":15787,"Question_score":13,"Answer_content":"Always ask yourself \"what do these results mean and how will they be used?\"Usually the purpose of using statistics is to assist in making decisions under uncertainty.  So it is important to have at the front of your mind \"What decisions will be made as a result of this analysis and how will this analysis influence these decisions?\" (e.g. publish an article, recommend a new method be used, provide $X in funding to Y, get more data, report an estimated quantity as E, etc.etc.....)If you don't feel that there is any decision to be made, then one wonders why you are doing the analysis in the first place (as it is quite expensive to do analysis).  I think of statistics as a \"nuisance\" in that it is a means to an end, rather than an end itself.  In my view we only quantify uncertainty so that we can use this to make decisions which account for this uncertainty in a precise way.I think this is one reason why keeping things simple is a good policy in general, because it is usually much easier to relate a simple solution to the real world (and hence to the environment in which the decision is being made) than the complex solution.  It is also usually easier to understand the limitations of the simple answer.  You then move to the more complex solutions when you understand the limitations of the simple solution, and how the complex one addresses them.","Display_name":"probabilityislogic","Creater_id":2392,"Start_date":"2011-01-16 06:48:53","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60ca"},"Last_activity":"2010-09-19 03:13:15","Creator_reputation":27883,"Question_score":25,"Answer_content":"If you're deciding between two ways of analysing your data, try it both ways and see if it  makes a difference.This is useful in many contexts:To transform or not transformNon-parametric or parameteric testSpearman's or Pearson's correlationPCA or factor analysisWhether to use the arithmetic mean or a robust estimate of the meanWhether to include a covariate or notWhether to use list-wise deletion, pair-wise deletion, imputation, or some other method of missing values replacementThis shouldn't absolve one from thinking through the issue, but it at least gives a sense of the degree to which substantive findings are robust to the choice.","Display_name":"Jeromy Anglim","Creater_id":183,"Start_date":"2010-09-17 02:40:03","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60cb"},"Last_activity":"2010-09-18 14:07:17","Creator_reputation":20442,"Question_score":21,"Answer_content":"One rule per answer ;-)Talk to the statistician before conducting the study. If possible, before applying for the grant. Help him/her understand the problem you are studying, get his/her input on how to analyze the data you are about to collect and think about what that means for your study design and data requirements. Perhaps the stats guy/gal suggests doing a hierarchical model to account for who diagnosed the patients - then you need to track who diagnosed whom. Sounds trivial, but it's far better to think about this before you collect data (and fail to collect something crucial) than afterwards.On a related note: do a power analysis before starting. Nothing is as frustrating as not having budgeted for a sufficiently large sample size. In thinking about what effect size you are expecting, remember publication bias - the effect size you are going to find will probably be smaller than what you expected given the (biased) literature.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2010-09-18 14:07:17","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60cc"},"Last_activity":"2010-09-18 12:49:37","Creator_reputation":4062,"Question_score":6,"Answer_content":"For histograms, a good rule of thumb for number of bins in a histogram:square root of the number of data points","Display_name":"doug","Creater_id":438,"Start_date":"2010-09-18 12:49:37","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60cd"},"Last_activity":"2010-09-16 15:39:16","Creator_reputation":null,"Question_score":15,"Answer_content":"Use software that shows the chain of programming logic from the raw data through to the final analyses/results. Avoid software like Excel where one user can make an undetectable error in one cell, that only manual checking  will pick up.","Display_name":null,"Creater_id":null,"Start_date":"2010-09-16 15:39:16","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60ce"},"Last_activity":"2010-09-16 15:36:18","Creator_reputation":2607,"Question_score":9,"Answer_content":"For data organization/management, ensure that when you generate new variables in the dataset (for example, calculating body mass index from height and weight), the original variables are never deleted. A non-destructive approach is best from a reproducibility perspective. You never know when you might mis-enter a command and subsequently need to redo your variable generation. Without the original variables, you will lose a lot of time!","Display_name":"pmgjones","Creater_id":561,"Start_date":"2010-09-16 15:36:18","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60cf"},"Last_activity":"2010-09-16 14:32:16","Creator_reputation":6409,"Question_score":14,"Answer_content":"Question your data. In the modern era of cheap RAM, we often work on large amounts of data. One 'fat-finger' error or 'lost decimal place' can easily dominate an analysis. Without some basic sanity checking, (or plotting the data, as suggested by others here) one can waste a lot of time. This also suggests using some basic techniques for 'robustness' to outliers.","Display_name":"shabbychef","Creater_id":795,"Start_date":"2010-09-16 14:32:16","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60d0"},"Last_activity":"2010-09-16 06:13:10","Creator_reputation":26624,"Question_score":22,"Answer_content":"One thing I tell my students is to produce an appropriate graph for every p-value. e.g., a scatterplot if they test correlation, side-by-side boxplots if they do a one-way ANOVA, etc.","Display_name":"Rob Hyndman","Creater_id":159,"Start_date":"2010-09-16 06:13:10","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60d1"},"Last_activity":"2010-09-16 05:57:26","Creator_reputation":14468,"Question_score":50,"Answer_content":"Don't forget to do some basic data checking before you start the analysis. In particular, look at a scatter plot of every variable you intend to analyse against ID number, date / time of data collection or similar. The eye can often pick up patterns that reveal problems when summary statistics don't show anything unusual. And if you're going to use a log or other transformation for analysis, also use it for the plot.","Display_name":"onestop","Creater_id":449,"Start_date":"2010-09-16 05:57:26","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60d2"},"Last_activity":"2010-09-16 05:22:52","Creator_reputation":4898,"Question_score":19,"Answer_content":"There can be a long list but to mention a few: (in no specific order)P-value is NOT probability. Specifically, it is not the probability of committing Type I error. Similarly, CIs have no probabilistic interpretation for the given data. They are applicable for repeated experiments.Problem related to variance dominate bias most the time in practice, so a biased estimate with small variance is better than an unbiased estimate with large variance (most of the time).Model fitting is an iterative process. Before analyzing the data understand the source of data and possible models that fit or dont fit the description. Also, try model any design issues in your model. Use the visualization tools, look at the data (for possible abnormalities, obvious trends etc etc to understand the data) before analyzing it. Use the visualization methods (if possible) to see how the model fits to that data.Last but not the least, use statistical software for what they are made for (to make your task of computation easier), they are not a substitute for human thinking.","Display_name":"suncoolsu","Creater_id":1307,"Start_date":"2010-09-16 05:15:45","Question_id":2715}
{"_id":{"$oid":"5837a583a05283111e4d60df"},"Last_activity":"2016-08-04 15:25:07","Creator_reputation":6,"Question_score":0,"Answer_content":"I think the answer of the question means that the linear regression can't give zero claassification error on the test or the validation set, not the training set.  The original purpose of this question is to see if you understand the concept of overfitting, I guess.","Display_name":"MeiCheng Shih","Creater_id":125558,"Start_date":"2016-08-04 15:25:07","Question_id":228344}
{"_id":{"$oid":"5837a583a05283111e4d60ec"},"Last_activity":"2016-08-05 07:58:43","Creator_reputation":3639,"Question_score":1,"Answer_content":"If you remove the intercept you get an estimate for each level. If you then subtract out the mean from k you should get estimates relative to zero (the overall mean). You may need to remove the offset from the formula and specify offset = ..., not sure. Try both ways if necessary.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-05 07:58:43","Question_id":228362}
{"_id":{"$oid":"5837a583a05283111e4d60f8"},"Last_activity":"2015-11-28 15:31:11","Creator_reputation":8893,"Question_score":1,"Answer_content":"In my comments I already mentioned two threads (here and here) in SE that presented some interesting discussion points regarding FunkSVD. The only place I have found a decent FunkSVD implementation that could potentially work with R is LensKit's, Matrix Factorization CF module. Let me point out that this will not be straightforward as you will need to use a JAVA API to connect from R to Lenskit.Given you problem maybe checking the R package recommenderlab might also be beneficial. Let me note that in many cases recommender systems essentially give out an informed nearest-neighbour suggestion so for your particular problem you might be able to re-express it as clustering/classification task rather than a manifold-learning one.","Display_name":"usεr11852","Creater_id":11852,"Start_date":"2015-11-28 15:31:11","Question_id":183477}
{"_id":{"$oid":"5837a583a05283111e4d6107"},"Last_activity":"2016-08-05 07:10:09","Creator_reputation":4441,"Question_score":2,"Answer_content":"All the answers are great and I have similar simulations with Matt to give you another example to show why complex model with regularization is usually better than simple model.I made a analogy to have intuitive explanation.Case 1 you only have a high school student with limited knowledge (a simple model without regularization)Case 2 you have a graduate student but restrict him/her to only use high school knowledge to solve problems. (complex model with regularization)If two persons are solving the same problem, usually the graduate students would work better solution, because the experience and insights about the knowledge.Figure 1 is showing 4 fittings to the same data. 4 fittings are line, parabola, 3rd order model and 5th order model. You can observe the 5th order model may have overfitting problem.On the other hand, in the second experiment, we will use 5th order model with different level of regularization. Compare last one with the second order model. (two models are highlighted) you will find the last one is similar (roughly have the same model complexity) to parabola, but slightly more flexible to the data well.","Display_name":"hxd1011","Creater_id":113777,"Start_date":"2016-08-05 06:52:22","Question_id":226553}
{"_id":{"$oid":"5837a583a05283111e4d6108"},"Last_activity":"2016-08-04 20:10:45","Creator_reputation":12762,"Question_score":41,"Answer_content":"I recently made a little in browser app that you can use to play with these ideas: Scatterplot Smoothers (*).Here's some data I made up, with a low degree polynomial fitIt's clear that the quadratic polynomial is just not flexible enough to give a good fit to the data.  We have regions of very high bias, between  and  all the data is below the fit, and after  all the data is above the curve.To rid ourselves of bias, we can increase the degree of the curve to three, but the problem remains, the cubic curve is still too rigidSo we continue to increase the degree, but now we incur the opposite problemThis curve tracks the data too closely, and has a tendency to fly off in directions not so well borne out by general patterns in the data.  This is where regularization comes in.  With the same degree curve (ten) and some well chosen regularizationWe get a really nice fit!It's worth a little focus on one aspect of well chosen above.  When you are fitting polynomials to data you have a discrete set of choices for degree.  If a degree three curve is underfit and a degree four curve is overfit, you have nowhere to go in the middle.  Regularization solves this problem, as it gives you a continuous range of complexity parameters to play with.  how do you claim \"We get a really nice fit!\". For me they all look the same, namely, inconclusive. Which rational are you using to decide what is a nice and a bad fit?Fair point.The assumption I'm making here is that a well fit model should have no discernable pattern in the residuals.  Now, I'm not plotting the residuals, so you have to do a little bit of work when looking at the pictures, but you should be able to use your imagination.In the first picture, with the quadratic curve fit to the data, I can see the following pattern in the residualsFrom 0.0 to 0.3 they are about evenly placed above and below the curve.From 0.3 to about 0.55 all the data points are above the curve.From 0.55 to about 0.85 all the data points are below the curve.From 0.85 on, they are all above the curve again.I'd refer to these behaviours as local bias, there are regions where the curve is not well approximating the conditional mean of the data.Compare this to the last fit, with the cubic spline.  I can't pick out any regions by eye where the fit does not look like it's running precisely through the center of mass of the data points.  This is generally (though imprecisely) what I mean by a good fit.  Final Note:  Take all this as illustration.  In practice, I do not recommend using polynomial basis expansions for any degree higher than .  Thier problems are well discussed elsewhere, but, for example:Their behaviour at the boundaries of your data can be very chaotic,even with regularization. They are not local in any sense. Changing your data in one place can significantly affect the fit in a very different place.I instead, in a situation like you describe, reccomend using natural cubic splines along with regularization, which give the best compromise between flexibility and stability.  You can see for yourself by fitting some splines in the app.(*) I believe this only works in chrome and firefox due to my use of some modern javascript features (and overall lazyness to fix it in safari and ie).  The source code is here, if you are interested.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-07-31 09:34:54","Question_id":226553}
{"_id":{"$oid":"5837a583a05283111e4d6109"},"Last_activity":"2016-07-31 11:23:19","Creator_reputation":141,"Question_score":4,"Answer_content":"For polynomials even small changes in coefficients can make a difference for the higher exponents. regularization ( least squares ) usually encourages many small coefficients but none exactly 0 and therefore the higher order monomials are able to make a difference.","Display_name":"mathreadler","Creater_id":125155,"Start_date":"2016-07-31 11:23:19","Question_id":226553}
{"_id":{"$oid":"5837a583a05283111e4d610a"},"Last_activity":"2016-07-31 08:04:08","Creator_reputation":8337,"Question_score":4,"Answer_content":"No, it isn't the same. Compare, for example, a second-order polynomial without regularization to a fourth-order polynomial with it. The latter can posit big coefficients for the third and fourth powers so long as this seems to increase predictive accuracy, according to whatever procedure is used to choose the penalty size for the regularization procedure (probably cross-validation). This shows that one of the benefits of regularization is that it allows you to automatically adjust model complexity to strike a balance between overfitting and underfitting.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-31 08:04:08","Question_id":226553}
{"_id":{"$oid":"5837a583a05283111e4d6119"},"Last_activity":"2016-08-05 06:22:49","Creator_reputation":143,"Question_score":0,"Answer_content":"Because it is based on a Second Order Approximation of the Squared Residual Function.Hence it requires \"Squared Residual Function\".The method will fit any model you can make which has the same form.","Display_name":"Royi","Creater_id":6244,"Start_date":"2016-08-05 06:22:49","Question_id":177395}
{"_id":{"$oid":"5837a583a05283111e4d6126"},"Last_activity":"2016-08-05 06:08:18","Creator_reputation":6097,"Question_score":4,"Answer_content":"First, there are more Markov chain Monte Carlo (MCMC) samplers aside from the Metropolis-Hastings (MH), but I will focus on MH.It is not that a Markov chain is needed, it is that using an MH algorithm the samples obtained themselves form a Markov chain that converge to the stationary distribution indicated in the accept-reject criterion. There are a couple of differences between rejection sampling and MH. Lets say your distribution from which you desire samples is .In MH if your current sample is , you draw a sample  from a proposal distribution , and find the following accept reject ratio\\min \\left( 1,\\dfrac{\\pi(y) \\,q(x \\mid y)}{\\pi(x) \\, q(y \\mid x) } \\right). Notice that the accept-reject ratio itself is a function of the current step . Even if your proposal is not dependent on your current step (like in Independent MH), your ratio will still be\\min \\left( 1,\\dfrac{\\pi(y) \\,q(x)}{\\pi(x) \\, q(y) } \\right), which again depends on the current step. Thus the probability with which you accept or reject depends on the current step, making the samples that you obtain a Markov chain.If the proposed step is rejected in MH, then the next step is the same as the current step. So again, the next step depends on the previous step, and thus we have a Markov chain.The stationary distribution that the MH algorithm will converge to is the one that takes s position in the ratio. If you replace it with any other distribution, the samples you get will converge to that distribution. Whether the Markov chain converges or not is something practioners don't have to worry about because this has already been proven for the MH algorithm.Finally, there is no need to calculate the transition matrix because there is a clear path of updating the Markov chain without a transition matrix. Also, for when the state space infinite (like the real line), then we can no longer deal with the transition matrix and have to study the transition \"kernel\". But this is not required since the way the Markov chain updates itself is only theMH-ratio algorithm, and no other information is needed.","Display_name":"Greenparker","Creater_id":31978,"Start_date":"2016-08-05 06:08:18","Question_id":228437}
{"_id":{"$oid":"5837a583a05283111e4d6127"},"Last_activity":"2016-08-05 06:06:52","Creator_reputation":5522,"Question_score":2,"Answer_content":"This is just confusion with the terminology. It's not that you need to \"use\" a Markov chain to draw Metropolis-Hastings samples, it's that the sequence of Metropolis-Hastings draws is a Markov chain. This is an innate part of the algorithm design.\"Markov chain Monte Carlo\" is just a qualifier indicating that the draws from this Monte Carlo algorithm produce a Markov chain.","Display_name":"ssdecontrol","Creater_id":36229,"Start_date":"2016-08-05 06:06:52","Question_id":228437}
{"_id":{"$oid":"5837a583a05283111e4d6128"},"Last_activity":"2016-08-05 06:01:38","Creator_reputation":152683,"Question_score":3,"Answer_content":"MCMC in general and Metropolis-Hastings in particular is quite distinct from rejection sampling.Note that rejection sampling is independent from one generated value to the next -- it doesn't matter what value you just generated, the distribution of the next one doesn't consider it. MCMC involves a series of moves. At one step you are at some value and then conditional on that value you have some way of possibly moving to a (possibly) new value next step.In addition, you left out of your description what happens when you reject.In rejection sampling you simply fail to have a value. You generate again.In Metropolis-Hastings, you fail to accept the move, so you retain the previous value.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-05 06:01:38","Question_id":228437}
{"_id":{"$oid":"5837a583a05283111e4d6135"},"Last_activity":"2015-01-20 13:09:17","Creator_reputation":141,"Question_score":2,"Answer_content":"Your error message \"weights must be like glm weights for generalized case\" is saying that if you choose to use Gamm() with a generalized case (which means: using a non-Gaussian probability distribution such as Gamma) then the weights argument should be specified as it would be for GlmmPQL(). The explanation is that GAMM is essentially a wrapper function and depending on how it is used it may utilize the functions nlme() or GlmmPQL(). If you specify a Gaussian distribution for your model then GAMM makes a call directly to nlme() by default. With this default method the weights argument specifies a gls variance structure, because that is what nlme() does (read ?nlme and the weights argument). If you switch to a generalized distribution (eg, gamma, beta, poisson) GAMM calls to GlmmPQL(), which does have a weights argument but it entirely different (read ?GlmmPQL and the weights argument). Thus, as far as I know you cannot access gls weights through gamm with a non-Gaussian distribution. If I am mistaken, please somebody correct this. ","Display_name":"Ira S","Creater_id":61510,"Start_date":"2015-01-20 13:02:48","Question_id":118592}
{"_id":{"$oid":"5837a583a05283111e4d6142"},"Last_activity":"2016-08-05 05:44:31","Creator_reputation":3639,"Question_score":1,"Answer_content":"Fisher's method involves converting each -value into a  and then summing them. Since there is a simple formula for doing this he may, given the technology of his time, have preferred this to what you suggest in your first question. There is not a fundamental difference between them though.The whole problem is quite poorly specified. The null is that all the s are drawn from a uniform distribution on (0, 1). The alternative is either that (a) at least one of them is not (b) none of them is. For more details consult@article{birnbaum54,   author = {Birnbaum, A},   title = {Combining independent tests of significance},   journal = {Journal of the American Statistical Association},   year = {1954},   volume = {49},   pages = {559--574},   keywords = {meta-analysis, significance values}}and@misc{cousins08,   author = {Cousins, R D},   title = {Annotated bibliography of some papers on combining      significances or --values},   year = {2008},   note = {arXiv:0705.2209},   keywords = {meta-analysis, significance values}}","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-05 05:44:31","Question_id":228407}
{"_id":{"$oid":"5837a583a05283111e4d614f"},"Last_activity":"2016-08-05 05:28:15","Creator_reputation":36,"Question_score":2,"Answer_content":"One of the causes of the packages producing different results is the way nodesize is implemented internally. In randomForest, the value appears to be a strict lower bound.  In randomForestSRC, while we (unfortunately) don't document the subtlety, we will not attempt to split a node without at least 2 * nodesize replicates in a node.  But when we do, it can result in one daughter \u0026lt; nodesize, and the other daughter \u003e= nodesize. What we can say is that \"on average\" our terminal nodes across the forest will be of size = nodesize.  The result is that we can grow slightly better trees than RF with the \"same\" setting. If you set nodesize = 1 to avoid this issue, and accommodate for Monte Carlo effects by growing multiple forests with multiple simulations you will find that the MSE for both packages are coincident.","Display_name":"Udaya Kogalur","Creater_id":98007,"Start_date":"2016-08-05 05:28:15","Question_id":190911}
{"_id":{"$oid":"5837a583a05283111e4d6150"},"Last_activity":"2016-01-17 10:23:46","Creator_reputation":12270,"Question_score":1,"Answer_content":"I like randomForestSRC a lot. It has some really nice plots and diagnostics.There are a lot of choices of how to implement the algorithm. For example, looking at the help pages, rfsrc has a splitrule, where \"The default rule is weighted mean-squared error splitting mse\". How does randomForest do it? They each can control the size of trees, but do it in two different ways: one by specifying the maximum number of leaves, the other by specifying the maximum depth. There are dozens of such choices, some of which are exposed as parameters (the two I mentioned), but many which are not.So I can't tell you exactly why they produce different results, but a random forest is not a closed-form like, say, OLS, nor is it an optimization (MLE) procedure. It's more algorithmic in nature so there are no mathematical reasons that would force them to agree.Why do you ask? Are you simply looking for an explanation? Your question about forcing the same answer makes me think you're doing something like speed benchmarking and want to compare speeds for reaching exactly the same answer. Or something that might better be explicit.EDIT: OK, per your comment, I'd recommend changing your question to be about the paradox and documenting what you did and your results.My guess is, to the extent that an RV actually helps results, it's because adding an RV might dilute the effect of RF's preferring to split continuous or categorical variables with lots of levels. If so, trying RVs with randomForestSRC with split of zero (usual, deterministic) and non-zero (random splits) might illustrate this.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-01-15 17:53:05","Question_id":190911}
{"_id":{"$oid":"5837a583a05283111e4d615d"},"Last_activity":"2016-08-05 05:21:15","Creator_reputation":1511,"Question_score":0,"Answer_content":"You got the bound \\frac 1n\\mathbb E\\left[\\max_{1\\leqslant i\\leqslant n}\\left|X_i\\right|^\\alpha  \\right]\\leqslant \\frac{\\lambda^\\alpha}{n}+\\mathbb E\\left[\\left|X_1\\right|^\\alpha I\\left(\\left|X_1\\right| \\gt \\lambda\\right)\\right],which is valid for any  and . Taking the  as  goes to infinity, we derive that for each , \\tag{*}  \\limsup_{n\\to +\\infty}\\frac 1n\\mathbb E\\left[\\max_{1\\leqslant i\\leqslant n}\\left|X_i\\right|^\\alpha  \\right]\\leqslant \\mathbb E\\left[\\left|X_1\\right|^\\alpha I\\left(\\left|X_1\\right| \\gt \\lambda\\right)\\right].Now, by monotone convergence, we have   and since  is valid for any positive , we get the wanted result. ","Display_name":"Davide Giraudo","Creater_id":14675,"Start_date":"2016-08-05 05:21:15","Question_id":181189}
{"_id":{"$oid":"5837a583a05283111e4d616a"},"Last_activity":"2016-08-05 05:13:41","Creator_reputation":1511,"Question_score":1,"Answer_content":"Define . We have to prove that \\mathbb P\\left\\{Y\\geqslant 1\\right\\}\\leqslant \\mathbb E\\left[Y^4\\right].This can be done by integrating the pointwise inequality  \\mathbf 1\\left\\{Y\\geqslant 1\\right\\}\\leqslant Y^4,where  denotes the indicator function. ","Display_name":"Davide Giraudo","Creater_id":14675,"Start_date":"2016-08-05 05:13:41","Question_id":188827}
{"_id":{"$oid":"5837a583a05283111e4d6177"},"Last_activity":"2016-08-05 05:08:37","Creator_reputation":1511,"Question_score":2,"Answer_content":"This can be solved directly using von Bahr and Esseen's inequality. Here is an alternative approach, by truncation.Let  be arbitrary but fixed and define X'_i:= X_i\\mathbf 1\\left\\{\\left|X_i\\right|\\leqslant R\\right\\}-\\mathbb E\\left[X_i\\mathbf 1\\left\\{\\left|X_i\\right|\\leqslant R\\right\\}\\right]\\mbox{ and }  X''_i:= X_i\\mathbf 1\\left\\{\\left|X_i\\right|\\gt R\\right\\}-\\mathbb E\\left[X_i\\mathbf 1\\left\\{\\left|X_i\\right|\\gt R\\right\\}\\right].In this way, we have  for each . Therefore,\\left|\\overline{X_n}-\\mu\\right|\\leqslant \\frac 1n\\left|\\sum_{i=1}^nX'_i  \\right|+\\frac 1n\\left|\\sum_{i=1}^nX''_i  \\right|,and using the elementary inequality , we get \\mathbb E\\left[ \\left|\\overline{X_n}-\\mu\\right|^p\\right]\\leqslant \\frac{2^{p-1}}n\\mathbb E\\left[ \\left|\\sum_{i=1}^nX'_i  \\right|^p\\right]+\\frac{2^{p-1}  } n\\mathbb E\\left[\\left|\\sum_{i=1}^nX''_i  \\right|^p\\right].Notice that by Jensen's inequality applied to the function  , \\mathbb E\\left[ \\left|\\sum_{i=1}^nX'_i  \\right|^p\\right]\\leqslant \\left(\\mathbb E\\left[ \\left|\\sum_{i=1}^nX'_i  \\right|^2\\right]\\right)^{2/p}.  Now the remaining tasks are the following:Show that \\lim_{n\\to +\\infty}\\frac 1n\\left(\\mathbb E\\left[ \\left|\\sum_{i=1}^nX'_i  \\right|^2\\right]\\right)^{2/p}=0.Use Minkowski's inequality to bound by a constant independent of  and  times .Conclude.   ","Display_name":"Davide Giraudo","Creater_id":14675,"Start_date":"2016-08-05 03:06:54","Question_id":226998}
{"_id":{"$oid":"5837a583a05283111e4d6184"},"Last_activity":"2016-08-05 05:03:52","Creator_reputation":825,"Question_score":3,"Answer_content":"In case of difference between levels of categorical predictors that you suspect that you suspect too big, you can try Fused Lasso algorithm.In stead of finding the MLE estimates, they propose an algorithm to minimise the following amount:\\sum_{i=1}^N(y_i - x_i^T\\beta)^2 + \\lambda_N^{(1)} \\sum_{j=1}^p\\mid \\beta_j \\mid + \\lambda_N^{(1)} \\sum_{j=2}^p \\mid \\beta_j - \\beta_{j-1}\\midInterpretation:When you chose , this is equal to the MLE approach. When  you have two pernalizations terms. The first term will shrink the coefficient of bad predictors towards 0 . The second term is what you are interested in, it pernalizes the difference between the coefficients of the categorical predictors. Big value of  results in small difference across levels of categorical predictors.","Display_name":"Metariat","Creater_id":78313,"Start_date":"2016-08-05 04:52:48","Question_id":225857}
{"_id":{"$oid":"5837a583a05283111e4d6190"},"Last_activity":"2016-08-05 05:02:34","Creator_reputation":77,"Question_score":0,"Answer_content":"That's right, t-tests are pretty robust to violations of normality, but so for violations of homogeneity of variance. I'd go for a dependent t-test and then access normality by inspecting a qq-plot and looking at boxplots of the data, instead of conducting any of those commonly used normality tests, like Levene's","Display_name":"Diogo B Provete","Creater_id":103642,"Start_date":"2016-08-05 05:02:34","Question_id":225921}
{"_id":{"$oid":"5837a583a05283111e4d619d"},"Last_activity":"2016-08-05 04:32:51","Creator_reputation":3038,"Question_score":3,"Answer_content":"A confidence interval is an interval-estimate for some true value of a parameter. Let us (as an example) start with e.g. a confidence interval for the mean of a normal distribution and then move on to ROC and AUC so that one sees the analogy. Assume that you have a random normal variable . Where  is the unknown population mean and, to keep it simple, let us assume that  is known.  We now draw a sample of size  from the distribution of X, i.e. we get a sample .  The goal is to have an idea about the unknown  using the sample drawn.  It is well known that the arithemetic average  is an unbiased (point) estimator for (the unknown)  and that  is a  confidence interval for (the unknown) .  If we draw another sample  from the distribtion of  then, in the same way we will find another confidence interval for the (unknown)  as . So each time we draw a sample of size  from the distribution of , we find a confidence interval for the (unknown)  and all these intervals will be different.  The fact that it is a  confidence interval means that, if we draw an 'infinite' number of samples of size  from the distribution of , and for each of these samples we compute the  confidence interval, then  of all these intervals (one interval for each sample) will contain the unknown .  (so sometimes , namely  of the intervals, such an interval will not contain the unknown , so sometimes you have bad luck.)The same holds for the AUC, when you compute the AUC, you compute it from a sample, in other words what you compute is an estimate for the true unknown AUC. Similarly you can, for the sample that you have, compute a confidence interval for the true but unknown AUC.  If you were able to draw an infinite number of samples, and for each sample obtained compute the confidence interval for the true AUC, then  of these computed intervals would contain the true but unknown AUC.  Note that the interval is random, because it is computed from a random sample.  The true AUC is not random, it is some unknown property of your population.Unfortunately you can not draw an infinite number of samples, most of the time you have only one sample, so you will have to do it with one interval, but you are rather confident ( of the so computed intervals will contain the true unknown AUC) that this interval will contain the true AUC.  And yes, if the lower border of the interval is higer than 0.5 then you can be rather confident that your model is not the random model, but, as above, you may also have had bad luck with the sample. ","Display_name":"fcop","Creater_id":83346,"Start_date":"2015-08-14 07:01:03","Question_id":165033}
{"_id":{"$oid":"5837a583a05283111e4d619e"},"Last_activity":"2015-08-06 16:18:27","Creator_reputation":7425,"Question_score":1,"Answer_content":"Probably the best interpretation would be in terms of the so-called  statistic, which turns out to equal the area under the ROC curve.  That is, if you are trying to predict some response  (which is often binary) using a score , then the  statistic is defined as , where  and  are independent copies of  and .You would then be  confident that the \"true\" value of this conditional probability lies within the specified interval.  This would allow you to somewhat more formally reject the claim that your model is no better than random if the lower bound is above .","Display_name":"dsaxton","Creater_id":78861,"Start_date":"2015-08-06 13:47:10","Question_id":165033}
{"_id":{"$oid":"5837a583a05283111e4d61ab"},"Last_activity":"2016-08-05 04:24:34","Creator_reputation":15589,"Question_score":0,"Answer_content":"RandomForestRegressor man page explains that you get  oob_prediction_s for the training cases as well.You can then calculate e.g. MSE as((y - rfr.oob_prediction_)**2).sum() / y.size","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2016-08-05 04:24:34","Question_id":228303}
{"_id":{"$oid":"5837a583a05283111e4d61ac"},"Last_activity":"2016-08-04 11:29:22","Creator_reputation":7,"Question_score":-1,"Answer_content":"You should be able to use OOB. In random forest bagging, we build each tree on part of data. Then we check r^2 in the remaining data not used to build data. So u should be able to use OOB. ","Display_name":"user125524","Creater_id":125524,"Start_date":"2016-08-04 11:29:22","Question_id":228303}
{"_id":{"$oid":"5837a583a05283111e4d61b9"},"Last_activity":"2016-08-05 04:07:44","Creator_reputation":57732,"Question_score":2,"Answer_content":"As @Nick Cox points out in a comment, if you want your predicted values to always be positive, you don't want linear regression. If the dependent variable is a count (and maybe even if it is not) you could use Poisson regression or negative binomial regression. If it is bounded, you can transform it to 0-1 and then use beta regression. There are other options too.Or it might be that you want to transform your dependent variable. If your DV is never negative then you can take the log.  Then the predicted values on the raw score would never be negative. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-08-05 04:07:44","Question_id":228409}
{"_id":{"$oid":"5837a583a05283111e4d61c5"},"Last_activity":"2016-08-05 03:36:35","Creator_reputation":15589,"Question_score":0,"Answer_content":"First of all, while I'd usually agree that hold-out is not making efficient use of the available samples and the typical set-up is prone to the same mistakes as cross validation, repeated set validation / repeated hold-out is a resampling technique that I think is well suitable for your learning curve calculation. This way, you can reflect what is going on inside the data set you have covering the variation due to different splits (but not fully the variation you'd have to expect with new data set of size ). This way you get the fine-grained control over training set size of hold out together with resampling properties close to k-fold.However, here's a caveat for the informed decision: in case you are talking about small sample size classification, the usual figures of merit (sensitivity, specificity, overall accuracy etc.) are subject to very high testing variance. This testing variance is limited by the number of actual independent cases you have in the denominator of the calculation and can easily be so large that you cannot sensibly use such measured learning curves (keep in mind, \"use\" typically means extrapolation).See our paper for details: Beleites, C. and Neugebauer, U. and Bocklitz, T. and Krafft, C. and Popp, J.: Sample size planning for classification models. Anal Chim Acta, 2013, 760, 25-33.DOI: 10.1016/j.aca.2012.11.007accepted manuscript on arXiv: 1211.1323","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2016-08-05 03:36:35","Question_id":226982}
{"_id":{"$oid":"5837a583a05283111e4d61d5"},"Last_activity":"2016-08-05 03:09:13","Creator_reputation":33,"Question_score":0,"Answer_content":"You can obtain an estimate of  for non linear regression by calculate the square of the correlation value between the fitted values and the real values of the response variable.","Display_name":"Enzo D\u0026#39;Innocenzo","Creater_id":124731,"Start_date":"2016-08-05 03:09:13","Question_id":228391}
{"_id":{"$oid":"5837a583a05283111e4d61e8"},"Last_activity":"2016-08-05 02:48:12","Creator_reputation":879,"Question_score":5,"Answer_content":"I think you should definitely look into more metrics than just AUC and accuracy.Accuracy (together with sensitivity and specificity) is a very simple but biased metric which forces you to look at the absolute prediction result and does not open for assertion of class probabilities or ranking. It also does not take the population into account which invites to misinterpretation as a model giving a 95% accuracy on a population with 95% chance of being correct at random isn't really a good model, even if the accuracy is high. AUC is a good metric for asserting model accuracy that is independent of population class probabilities. It will, however not tell you anything about how good the probability estimates actually are. You could get a high AUC but still have very skewed probability estimates. This metric more discriminating than accuracy and will definitely give you better models when used in combination with some proper scoring rule, e.g Brier score as mentioned in another post. You can get a more formal proof here, although this paper is quite theoretical: AUC: a Statistically Consistent and more Discriminating Measure than AccuracyThere are however a bunch of good metrics available.Loss Functions for Binary Class Probability Estimationand Classiﬁcation: Structure and Applications is a good paper investigaing proper scoring rules such as the Brier score. Another interesting paper with metrics for assertion of model performance is Evaluation: from precision, recall and F-measure to ROC, informedness, markedness \u0026amp; correlation taking up other good performance metrics such as informedness. To summarize I would recommend looking at AUC/Gini and Brier score to assert you model performance, but depending on the goal with your model other metrics might suit your problem better.","Display_name":"while","Creater_id":17481,"Start_date":"2013-09-04 01:13:51","Question_id":58756}
{"_id":{"$oid":"5837a583a05283111e4d61e9"},"Last_activity":"2013-05-11 19:46:56","Creator_reputation":39321,"Question_score":8,"Answer_content":"Proportion classified correctly is an improper scoring rule, i.e., it is optimized by a bogus model.  I would use the quadratic proper scoring rule known as the Brier score, or the concordance probability (area under ROC curve in the binary  case).  Random forest works better than SVM in your case.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2013-05-11 19:46:56","Question_id":58756}
{"_id":{"$oid":"5837a583a05283111e4d61f6"},"Last_activity":"2016-08-05 02:44:36","Creator_reputation":879,"Question_score":1,"Answer_content":"Do you need to impute NA's?First I would ask if you really need to impute the missing values? If you intend to use the imputed set to train another model you might as well just add NA as a level. In my experience this is really the simplest solution when you have NA's in a categorical variable. Especially when NA's actually do mean something, which is quite common. But even if it does not it is easy, especially for random forests, to ignore that level if it is not predictive. This will add NA as a level in the factor.datasetvarWithNAs)Dummy encoding large categorical featuresRegarding the problem with too many levels it seems to be the factor w 1601 levels that is your main problem. This is really a lot of levels and it is hard to give you any direct usage tips as little is stated about the variable. What you always can do in the case of too many levels is to transform the variable into many boolean (true, false) variables.I'll give you an example.dataset \u0026lt;- data.frame(x1 = sample(c('a','b','c'), 10, replace=T))#     x1# 1   c# 2   b# 3   a# 4   a# 5   b# 6   c# 7   a# 8   a# 9   b# 10  cYou could use the caret package to create dummy variables for your factor levels. library(caret)dummyObj \u0026lt;- dummyVars(~x1, dataset)dummyset \u0026lt;- predict(dummyObj, dataset)     x1.a x1.b x1.c# 1     0    0    1# 2     0    1    0# 3     1    0    0# 4     1    0    0# 5     0    1    0# 6     0    0    1# 7     1    0    0# 8     1    0    0# 9     0    1    0# 10    0    0    1In your case it will make your feature vector quite a lot wider but it is actually what is done internally in a lot of, especially linear, models before training (although not in RF which is why you get this problem). If you look at eg. the glm package it transforms the dataset into dummy variables using the model.matrix function which does the same but adds an intercept term. Removing this intercept term will give you the same answer. And as model.matrix exists in the stats package you don't need to install anything.model.matrix(~ x1 - 1, dataset) # -1 removes the intercept#    x1a x1b x1c# 1    0   0   1# 2    0   1   0# 3    1   0   0# 4    1   0   0# 5    0   1   0# 6    0   0   1# 7    1   0   0# 8    1   0   0# 9    0   1   0# 10   0   0   1If you find that your dataset get too many features now you should resort to the options Michael M gave in his answer to reduce the feature space. Chances are you have levels that never occur or several that are very similar in meaning and can be combined etc. Of course it is tedious to do this manually when you have so many levels. ","Display_name":"while","Creater_id":17481,"Start_date":"2016-08-05 00:29:25","Question_id":226514}
{"_id":{"$oid":"5837a583a05283111e4d61f7"},"Last_activity":"2016-08-05 00:41:34","Creator_reputation":4124,"Question_score":1,"Answer_content":"The magic word is feature construction. Statistics and machine learning almost always take some effort to prepare data before running an algorithm. Some \"tricks\" with respect to your particular problem:try to reduce the number of factor levels without losing too much information (e.g. combine rare levels)Convert ordered input factors to numeric (tree-based methods work much faster then).A possibility is also to manually dummy code an input factor which greatly reduces computational effort.Some further hints:If the response variable is a factor, then a random forest does classification, not regression. You can try out faster algoritms like multinomial logistic regression to fill in missings.","Display_name":"Michael M","Creater_id":30351,"Start_date":"2016-07-31 00:37:20","Question_id":226514}
{"_id":{"$oid":"5837a583a05283111e4d61f8"},"Last_activity":"2016-08-02 06:58:53","Creator_reputation":5445,"Question_score":1,"Answer_content":"You could use random hot deck imputation. Roughly, this is a method where missing values are replaced with values from an observation with \"similar\" values in the non-missing variables. For each missing value, the algorithm generates a pool of similar observations (\"donors\") and randomly chooses from them. The bigger the pools of donors and the bigger the number of datasets being imputed, the better. For example we might have data as follows var1  var2  var3  var41 B     Z     U     5.12 \u0026lt;NA\u0026gt;  Z     U     5.03 B     Z     U     4.94 A     W     U     5.25 B     W     U     4.86 C     U     T     6.27 B     C     T     5.28 B     T     T     6.19 B     S     T     6.0Here we have one observation with a missing value in var1. The algorithm would identify observations 1 and 3 as donors since they both have the same values for the two other categorical variables and similar values for the numeric variable. Hence, B would be chosen as the imputed values. If instead row 1 was1 A     Z     U     5.1..then the algorithm would randomly choose either A or B as the imputed value.The hot.deck package for R should be able to handle your situation. The only problem I see is that, since you only have ~45000 observations, it is clear that there are many combinations of the categorical variable levels that do not occur (since, if every combination occured, there would have to be a minimum of  observations. So, for the observations with missing data, if the combinations of the categorical variables are not repeated at least  times elsewhere in the data, and you are imputing  datasets, if  there will be insufficient donors to draw imputations from. If this occurs then the software will generate warnings. ","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-02 06:43:54","Question_id":226514}
{"_id":{"$oid":"5837a583a05283111e4d6205"},"Last_activity":"2016-08-05 02:41:23","Creator_reputation":164,"Question_score":0,"Answer_content":"The cost function gives you a metric for how \"good\" your predictor is: the lower the cost, the better your predictor.So your target while training a predictor is to minimise the cost function (ideally, you would want it to be zero).This is what machine learning algorithms do: optimise the parameters of the predictor in order to minimise the cost function. If you didn't have a handy cost function like the (squared) sum of errors, you would not know how well your predictor is doing.To give a simple example: assuming you are training a predictor , first with the vector of coefficients  and then with , getting as result the following error vectors:Just by looking at the error vectors, how would you decide whether parameters  or  are better?In contrast, if you calculate a cost function on each vector (for instance, the squared sum of errors), you obtain Like this, it is immediately obvious that the error vector  is indicative of a higher error, since the cost function is higher. So you would know that using the coefficients  for  is the better choice.","Display_name":"Cristina","Creater_id":10040,"Start_date":"2016-08-04 04:00:43","Question_id":228213}
{"_id":{"$oid":"5837a583a05283111e4d6211"},"Last_activity":"2016-08-05 02:40:50","Creator_reputation":152683,"Question_score":41,"Answer_content":"You seem to assume in your question that the concept of the normal distribution was around before the distribution was identified, and people tried to figure out what it was. It's not clear to me how that would work. [Edit: there is at least one sense it which we might consider there being a \"search for a distribution\" but it's not \"a search for a distribution that describes lots and lots of phenomena\"]This is not the case; the distribution was known about before it was called the normal distribution.   how would you prove to such a person that the probability density function of all normally distributed data has a bell shapeThe normal distribution function is the thing that has what is usually called a \"bell shape\" -- all normal distributions have the same \"shape\" (in the sense that they only differ in scale and location). Data can look more or less \"bell-shaped\" in distribution but that doesn't make it normal. Lots of non-normal distributions look similarly \"bell-shaped\". The actual population distributions that data are drawn from are likely never actually normal, though it's sometimes quite a reasonable approximation.This is typically true of almost all the distributions we apply to things in the real world -- they're models, not facts about the world. [As an example, if we make certain assumptions (those for a Poisson process), we can derive the Poisson distribution -- a widely used distribution. But are those assumptions ever exactly satisfied? Generally the best we can say (in the right situations) is that they're very nearly true.]  what do we actually consider normally distributed data? Data that follows the probability pattern of a normal distribution, or something else? Yes, to actually be normally distributed, the population the sample was drawn from would have to have a distribution that has the exact functional form of a normal distribution. As a result, any finite population cannot be normal.  Variables that necessarily bounded cannot be normal (for example, times taken for particular tasks, lengths of particular things cannot be negative, so they cannot actually be normally distributed).  it would perhaps be more intuitive that the probability function of normally distributed data has a shape of an isosceles triangleI don't see why this is necessarily more intuitive. It's certainly simpler.When first developing models for error distributions (specifically for astronomy in the early period), mathematicians considered a variety of shapes in relation to error distributions (including at one early point a triangular distribution), but in much of this work it was mathematics (rather than intuition) that was used. Laplace looked at double exponential and normal distributions (among several others), for example. Similarly Gauss used mathematics to derive it at around the same time, but in relation to a different set of considerations than Laplace did. In the narrow sense that Laplace and Gauss were considering \"distributions of errors\", we could regard there as being a \"search for a distribution\", at least for a time. Both postulated some properties for a distribution of errors they considered important (Laplace considered a sequence of somewhat different criteria over time) led to different distributions.  Basically my question is why does the normal distribution probability density function has a bell shape and not any other? The functional form of the thing that is called the normal density function gives it that shape. Consider the standard normal (for simplicity; every other normal has the same shape, differing only in scale and location):f_Z(z) = k \\cdot e^{-\\frac12 z^2};\\;-\\infty\u0026lt;z\u0026lt;\\infty(where  is simply a constant chosen to make the total area 1)this defines the value of the density at every value of , so it completely describes the shape of the density. That mathematical object is the thing we attach the label \"normal distribution\" to. There's nothing special about the name; it's just a label we attach to the distribution. It's had many names (and is still called different things by different people). While some people have regarded the normal distribution as somehow \"usual\" it's really only in particular sets of situations that you even tend to see it as an approximation.The discovery of the distribution is usually credited to de Moivre (as an approximation to the binomial). He in effect derived the functional form when trying to approximate binomial coefficients (/binomial probabilities) to approximate otherwise tedious calculations but - while he does effectively derive the form of the normal distribution - he doesn't seem to have thought about his approximation as a probability distribution, though some authors do suggest that he did. A certain amount of interpretation is required so there's scope for differences in that interpretation.Gauss and Laplace did work on it in the early 1800s; Gauss wrote about it in 1809 (in connection with it being the distribution for which the mean is the MLE of the center) and Laplace in 1810, as an approximation to the distribution of sums of symmetric random variables. A decade later Laplace gives an early form of central limit theorem, for discrete and for continuous variables. Early names for the distribution include the law of error, the law of frequency of errors, and it was also named after both Laplace and Gauss, sometimes jointly.The term \"normal\" was used to describe the distribution independently by three different authors in the 1870s (Peirce, Lexis and Galton), the first in 1873 and the other two in 1877. This is more than sixty years after the work by Gauss and Laplace and more than twice that since de Moivre's approximation. Galton's use of it was probably most influential but he used the term \"normal\" in relation to it only once in that 1877 work (mostly calling it \"the law of deviation\").However, in the 1880s Galton used the adjective \"normal\" in relation to the distribution numerous times (e.g. as the \"normal curve\" in 1889), and he in turn had a lot of influence on later statisticians in the UK (especially Karl Pearson). He didn't say why he used the term \"normal\" in this way, but presumably meant it in the sense of \"typical\" or \"usual\".The first explicit use of the phrase \"normal distribution\" appears to be by Karl Pearson; he certainly uses it in 1894, though he claims to have used it long before (a claim I would view with great caution).References:Miller, Jeff\"Earliest Known Uses of Some of the Words of Mathematics:\"Normal distribution (Entry by John Aldrich)http://jeff560.tripod.com/n.htmlStahl, Saul (2006),\"The Evolution of the Normal Distribution\",Mathematics Magazine, Vol. 79, No. 2 (April), pp 96-113https://www.maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdfNormal distribution, (2016, August 1).In Wikipedia, The Free Encyclopedia.Retrieved 12:02, August 3, 2016, fromhttps://en.wikipedia.org/w/index.php?title=Normal_distribution\u0026amp;oldid=732559095#HistoryHald, A (2007),\"De Moivre’s Normal Approximation to the Binomial, 1733, and Its Generalization\",In: A History of Parametric Statistical Inference from Bernoulli to Fisher, 1713–1935; pp 17-24[You may note substantial discrepancies between these sources in relation to their account of de Moivre]","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-03 04:57:08","Question_id":227034}
{"_id":{"$oid":"5837a583a05283111e4d6212"},"Last_activity":"2016-08-04 14:00:21","Creator_reputation":31,"Question_score":3,"Answer_content":"The Normal Distribution (aka \"Gaussian Distribution\") has a firm mathematical foundation.  The Central Limit Theorem says that if you have a finite set of n independent and identically distributed random variables having a specific mean and variance, and you take the average of those random variables, the distribution of the result will converge to a Gaussian Distribution as n goes to infinity.  There is no guesswork here, since the mathematical derivation leads to this specific distribution function and no other.To put this into more tangible terms, consider a single random variable, such as flipping a fair coin (2 equally possible outcomes).  The odds of getting a particular outcome is 1/2 for heads and 1/2 for tails.If you increase the number of coins and keep track of the total number of heads obtained with each trial, then you will get a Binomial Distribution, which has a roughly bell shape. Just graph with the number of heads along the x-axis, and the number of times you flipped that many heads along the y-axis.The more coins you use, and the more times you flip the coins, the closer the graph will come to looking like a Gaussian bell curve.  That's what the Central Limit Theorem asserts.The amazing thing is that the theorem does not depend on how the random variables are actually distributed, just so long as each of the random variables has the same distribution.  One key idea in the theorem is that you are adding or averaging the random variables.  Another key concept is that the theorem is describing the mathematical limit as the number of random variables becomes larger and larger.  The more variables you use, the closer the distribution will approach a Normal Distribution.I recommend you take a class in Mathematical Statistics if you want to see how mathematicians determined that the Normal Distribution is actually the mathematically correct function for the bell curve.","Display_name":"user126665","Creater_id":126665,"Start_date":"2016-08-04 14:00:21","Question_id":227034}
{"_id":{"$oid":"5837a583a05283111e4d6213"},"Last_activity":"2016-08-04 09:50:16","Creator_reputation":19131,"Question_score":14,"Answer_content":"\"The Evolution of the Normal Distribution\" by SAUL STAHL is the best source of information to answer pretty much all the questions in your post. I'll recite a few points for your convenience only, because you'll find the detailed discussion inside the paper.  This is probably an amateur questionNo, it's an interesting question to anyone who uses statistics, because this is not covered in detail anywhere in standard courses.  Basically what bugs me is that for someone it would perhaps be more intuitive that the probability function of normally distributed data has a shape of an isosceles triangle rather than a bell curve, and how would you prove to such a person that the probability density function of all normally distributed data has a bell shape? Look at this picture from the paper. It shows the error curves that Simpson came up with before Gaussian (Normal) was discovered to analyze experimental data. So, your intuition is spot on.  By experiment? Yes, that's why they were called \"error curves\". The experiment was astronomical measurements. Astronomers struggled with measurement errors for centuries.   Or by some mathematical derivation?Again, YES! Long story short: the analysis of errors in astronomical data led Gauss to his (aka Normal)  distribution. These are the assumptions he used:By the way, Laplace used a few different approaches, and also came up with his distribution too while working with astronomical data:As to why normal distribution shows in experiment as measurement errors, here's a typical \"hand-wavy\" explanation physicist are used to give (a quote from Gerhard Bohm, Günter Zech, Introduction to Statistics and DataAnalysis for Physicists p.85):  Many experimental signals follow to a very good approximation a normal  distribution. This is due to the fact that they consist of the sum of  many contributions and a consequence of the central limit theorem.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-08-04 08:03:39","Question_id":227034}
{"_id":{"$oid":"5837a583a05283111e4d6214"},"Last_activity":"2016-08-04 07:04:36","Creator_reputation":81,"Question_score":8,"Answer_content":"The \"normal\" distribution is defined to be that particular distribution.The question is why would we expect this particular distribution to be common in nature, and why is it so often used as an approximation even when the real data does not exactly follow that distribution? (Real data is often found to have a \"fat tail\", i.e. values far from the mean are much more common than the normal distribution would predict).To put it another way, what is special about the normal distribution?The normal has a lot of \"nice\" statistical properties, (see e.g. https://en.wikipedia.org/wiki/Central_limit_theorem), but the most relevant IMO is the fact that is the \"maximum entropy\" function for any distribution with a given mean and variance. https://en.wikipedia.org/wiki/Maximum_entropy_probability_distributionTo express this in ordinary language, if you are given only the mean (central point) and variance (width) of a distribution, and you assume nothing else whatsoever about it, you will be forced to draw a normal distribution. Anything else requires additional information (in the sense of Shannon information theory), for example skewness, to determine it.The principle of maximum entropy was introduced by E.T. Jaynes as a way of determining reasonable priors in Bayesian inference, and I think he was the first to draw attention to this property.See this for further discussion: http://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/documents/tutorials/Gaussian-distribution.pdf","Display_name":"gareth","Creater_id":106175,"Start_date":"2016-08-03 07:00:39","Question_id":227034}
{"_id":{"$oid":"5837a585a05283111e4d6325"},"Last_activity":"2016-08-04 04:35:13","Creator_reputation":3354,"Question_score":1,"Answer_content":"  But in common sense, KPCA should be better than PCA.It is very hard to state anything related to the performance of dimension reduction techniques. The main concern is that it is hard to define a metric to compare different dimension reduction techniques.Usually, a visual examination is the best thing to do when comparing projections in low dimensional spaces (see per example What\u0026#39;s wrong with t-SNE vs PCA for dimensional reduction using R?)","Display_name":"RUser4512","Creater_id":73794,"Start_date":"2016-08-04 03:37:36","Question_id":192673}
{"_id":{"$oid":"5837a585a05283111e4d633a"},"Last_activity":"2016-08-04 04:05:44","Creator_reputation":152693,"Question_score":2,"Answer_content":"If the  -values are a random sample from the marginal distribution for , then - because  - you'll have a sample from the joint distribution and so the  values will in turn be a sample from their marginal.If the  -values are just some arbitrary collection of X-values (as your question implies), then you won't have a random sample from the marginal distribution for , so you won't have a sample from the joint distribution and so the  values won't in turn be a sample from their marginal.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-04 04:05:44","Question_id":228193}
{"_id":{"$oid":"5837a585a05283111e4d6347"},"Last_activity":"2016-08-04 04:01:20","Creator_reputation":131,"Question_score":0,"Answer_content":"Many time we wish to know whether there is a non-zero chance that we will be stuck in some state of my chain. Such states are called 'aperiodic' states. They are easy to quantify: if for any , I don't get , I claim that such state is aperiodic ( being the probability of staying in state ). A state which is not aperiodic is periodic. But the terminology is bit unfortunate because by period we usually mean a fixed value after which a system repeat itself. In Markov chain we don't have luxury of 'always a fixed value' for period. We might return to a periodic state in any  number of steps. We run the chain and note down the numbers and take the gcd of these numbers. Probably this definition have some advantage in proving other theorem; I've no clue about it. If you don't want to simulate the chain the note down the number, you can note down the power  to which transition matrix  is raised such that  and take gcd of those numbers.","Display_name":"Dilawar","Creater_id":54851,"Start_date":"2016-08-04 04:01:20","Question_id":48838}
{"_id":{"$oid":"5837a585a05283111e4d6348"},"Last_activity":"2013-01-30 02:56:30","Creator_reputation":6391,"Question_score":12,"Answer_content":"First of all, your definition is not entirely correct. Here is the correct definition from wikipedia, as suggested by Cyan.Periodicity (source: wikipedia)A state i has period k if any return to state i must occur in multiples of k time steps. Formally, the period of a state is defined ask = (where \"gcd\" is the greatest common divisor). Note that even though a state has period k, it may not be possible to reach the state in k steps. For example, suppose it is possible to return to the state in {6, 8, 10, 12, ...} time steps; k would be 2, even though 2 does not appear in this list.If k = 1, then the state is said to be aperiodic: returns to state i can occur at irregular times. In other words, a state i is aperiodic if there exists n such that for all n' ≥ n,Otherwise (k \u003e 1), the state is said to be periodic with period k. A Markov chain is aperiodic if every state is aperiodic.My ExplanationThe term periodicity describes whether something (an event, or here: the visit of a particular state) is happening at a regular time interval. Here time is measured in the number of states you visit.First Example:Now imagine that the clock represents a markov chain and every hour mark a state, so we got 12 states. Every state is visted by the hour hand every 12 hours (states) with probability=1, so the greatest common divisor is also 12.So every (hour-)state is periodic with period 12. Second example:Imagine a graph describing a sequence of coin tosses, starting at state  and state  and  representing the outcome of the last coin toss.The transition probability is 0.5 for every pair of states (i,j), except  -\u003e  and  -\u003e  where it is 0.Now imagine you are in state . The number of states you have to visit before you visit  again could be 1,2,3 etc.. It will happen, so the probability is greater 0, but it is not exactly predictable when. So the greatest common divisior of all possible number of visits which could occur before you visit  again is 1. This means that  is aperiodic.The same applies for . Since it does not apply for , the whole graph is not aperiodic. If we remove , it would be.","Display_name":"steffen","Creater_id":264,"Start_date":"2013-01-30 02:56:30","Question_id":48838}
{"_id":{"$oid":"5837a585a05283111e4d6357"},"Last_activity":"2016-08-04 03:40:36","Creator_reputation":3038,"Question_score":0,"Answer_content":"If  are your independent variables then the output of the logistic regression, let's call it , is the predicted ''conditional'' probability , i.e. the predicted value of the probability conditional on that value of . So if you have a very large number of subjects with the same value for  then it is expected that a fraction  of these will be 'successes' (or 1).","Display_name":"fcop","Creater_id":83346,"Start_date":"2016-08-04 03:40:36","Question_id":227009}
{"_id":{"$oid":"5837a585a05283111e4d6358"},"Last_activity":"2016-08-04 02:39:33","Creator_reputation":25495,"Question_score":4,"Answer_content":"It simply is probability, you can call it \"predicted\" as suggested by others.I see from the discussion that you disagree with such name, so let me proove you that this is probability.First, recall that if  is a Bernoulli distributed random variable parametrized by , then . Second, take an intercept-only logistic regression model, such model will calculate mean of your predicted  variable. This would be the same as if you calculated it simply taking . This mean would converge to expected value as , i.e. to . In fact, sample mean is a maximum likelihood estimator of  for Bernoulli distributed random variable. In case of more complicated logistic regression model you predict conditional means, i.e. conditional probabilities.Check also Why isn\u0026#39;t Logistic Regression called Logistic Classification?If this still does not convince you, below you can see simple R example showing exactly that case:set.seed(123)p1 \u0026lt;- 0.75Y1 \u0026lt;- sample(0:1, 500, replace = TRUE, prob = c(1-p1, p1))fit1 \u0026lt;- glm(Y1~1, family = \"binomial\")p1## [1] 0.75fitted(fit1)[1] # only the first one since all predictions are the same##     1 ## 0.762mean(Y1)## [1] 0.762q \u0026lt;- 0.3p2 \u0026lt;- c(0.4, 0.7)X \u0026lt;- sample(0:1, 500, replace = TRUE, prob = c(1-q, q))Y2 \u0026lt;- numeric(500)Y2[X==0] \u0026lt;- sample(0:1, sum(X==0), replace = TRUE, prob = c(1-p2[1], p2[1]))Y2[X==1] \u0026lt;- sample(0:1, sum(X==1), replace = TRUE, prob = c(1-p2[2], p2[2]))fit2 \u0026lt;- glm(Y2~X, family = \"binomial\")# predicted probabilities vs the true onestable( ifelse(X==0, p2[1], p2[2]), round(fitted(fit2), 3)) ##      ##      0.359 0.658##  0.4   348     0##  0.7     0   152# empirical conditional probabilities (conditional means)tapply( Y2, X, mean )##         0         1 ## 0.3591954 0.6578947 ","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-04 01:41:25","Question_id":227009}
{"_id":{"$oid":"5837a585a05283111e4d6359"},"Last_activity":"2016-08-03 01:45:32","Creator_reputation":11,"Question_score":1,"Answer_content":"Hand-Waving: The logistic regression is some sort of squeezing a linear regression (fitting a line through you samples) into the range [0,1].Since you have now some numerical value in the [0,1] range you can actually cal it (TADAM!) probability!More Rigorous: What you get is  or in other words the probability of some sample  to be labeled as  (or whatever two classes that you have) depending on -the data, or more specifically the independent measures of the features represented by .So to sum it up,  is the probability of  being .If it is only about semantics, you can always drop likelihood inside.  ","Display_name":"elkbrs","Creater_id":124385,"Start_date":"2016-08-03 01:45:32","Question_id":227009}
{"_id":{"$oid":"5837a585a05283111e4d6366"},"Last_activity":"2016-08-04 03:35:49","Creator_reputation":178,"Question_score":1,"Answer_content":"Let's start with the second question first: The fact that  is unbiased can be seen as follows. First, in the paper it is also stated that the samples  of the latent variables are independent. Thus, we have \\begin{align}E_{Q(h|x)}(\\hat{I})  \u0026amp; = E_{Q(h|x)}\\left( \\frac{1}{K} \\sum_{i=1,\\ldots,K} \\frac{P(x,h)}{Q(h|x)}\\right)\\\\ \u0026amp; =  \\frac{1}{K} \\sum_{i=1,\\ldots,K}       E_{Q(h|x)}\\left(  \\frac{P(x,h)}{Q(h|x)}\\right)\\\\ \u0026amp; =  \\frac{1}{K} \\sum_{i=1,\\ldots,K}       \\int  \\frac{P(x,h)}{Q(h|x)} \\cdot Q(h|x)\\,\\text{d}h\\\\ \u0026amp;= P(x)\\end{align}The reason why this expectation is taken over  rather than  is that we actually want to estimate the function . By taking the expectation , we would integrate out  and obtain an expression that only depends on  rather than .From this we can now easily obtain the answer to your first question. As  is concave, the inequality follows from Jensen's inequality which states that for concave functions  we have . The equality follows from the proof above.","Display_name":"Igor","Creater_id":18530,"Start_date":"2016-08-04 00:43:58","Question_id":228205}
{"_id":{"$oid":"5837a585a05283111e4d6378"},"Last_activity":"2016-08-04 02:49:55","Creator_reputation":730,"Question_score":1,"Answer_content":"Propensity score calculation and subsequent paired analysis is possible in several ways. There are already some overlapping Q\u0026amp;A in CV that you might wish to look at:Propensity Score Matching in R with Multiple TreatmentsSoftware that matches 6 groups by propensity score? Comparingtwo or more treatments with inverse probablity of treatmentweightingMy advice would be to use the twang R package.","Display_name":"Giuseppe Biondi-Zoccai","Creater_id":107799,"Start_date":"2016-08-04 02:49:55","Question_id":227013}
{"_id":{"$oid":"5837a585a05283111e4d6379"},"Last_activity":"2016-08-03 09:30:47","Creator_reputation":7,"Question_score":0,"Answer_content":"I have mostly used PSM for 2 class problems. We predict the probability of the treatment. And then compare effect of treatment vs control in same decile of our probability scores. Customers in same decile typically are similar and so comparable. So you can repeat the PSM twice. Control each time being people who have taken medicine on time. Though i am not sure if doing the test will lead to higher type 1 errors like in multiple t tests. For PSM in r for 2 variables there is a package matchit but have not used it. ","Display_name":"user125524","Creater_id":125524,"Start_date":"2016-08-03 09:30:47","Question_id":227013}
{"_id":{"$oid":"5837a585a05283111e4d6388"},"Last_activity":"2016-08-04 02:37:47","Creator_reputation":111,"Question_score":1,"Answer_content":"Please be skeptical of the following, as it is just something I thought up just now.One way could be to create a simpler model for all the data, excluding the regions of missing data.  The predictions from this model could be ensembled with a more complex model for the 90% of the data for which all attributes are present. Another way, would be to create a model that could learn whether extra attributes were available or not by including an extra boolean attribute that would denote this--I'm not sure if it would work in practice, but a neural network would at least theoretically be able to approximate such a model.  For other models, eg. models based on decision trees, missing data can potentially be handled intrinsically (by setting the missing data to NA or some other value than can be thresholded to signify missing).A third way, would be creating meta data, by eg. predicting the missing values in a two step classifier.If the problem arises from you trying to do multiclass classification, perhaps consider simplifying the problem to several binary classifications.Also, consider whether the fact that data is missing is potentially useful information.  For example, maybe the missing data is due to different data gathering techniques that could perturb the results, in which case it might be important information to know if the data is missing or not.","Display_name":"Asagen","Creater_id":84445,"Start_date":"2016-08-04 02:37:47","Question_id":228215}
{"_id":{"$oid":"5837a585a05283111e4d6397"},"Last_activity":"2016-08-03 22:06:57","Creator_reputation":8357,"Question_score":2,"Answer_content":"You have neither experimental nor longitudinal data. Hence, you have nothing with which to identify any causal relationships.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-03 22:06:57","Question_id":228191}
{"_id":{"$oid":"5837a585a05283111e4d63aa"},"Last_activity":"2016-08-03 23:34:05","Creator_reputation":993,"Question_score":1,"Answer_content":"SummaryI share my thoughts in Details section. I think they are useful in identifying what we really want to achieve.I think that the main problem here is that you haven't defined what a rank similarity means. Therefore, no one knows which method of measuring the difference between the ranks is better.Effectively, this leaves us to ambiguously choose a method based on guesses.What I really suggest is to first define a mathematical optimization objective. Only then we will be sure whether we really know what we want.Unless we do that, really don't know what we want. We might almost know what we want, but almost knowing  knowing.My text in Details essentially is a step towards reaching a mathematical definition of ranks similarity. Once we nail this, we can confidently move forward to choose the best method of measuring such similarity.DetailsBased on one of yur comments:\"The objective is to see if the two groups rankings differ\", Peter Flom.To answer this while strictly interpreting the objective: The ranks are different if, any item , there exists  such that , where  is the rank of of item  by group  and  is the rank of the same item but by group .Else, the ranks are not different.But I don't think that you really want that strict interpretation. Therefore, I think what you really meant to say is:How different are the ranks of groups  and ?One solution here is simply to measure the minimum edit distance. I.e. what are the minimum number of edits that need to be performed on the ranked list of group  such that it becomes identical to that of group . An edit could be defined as swapping two items, and costs costs  points depending how many hops are needed. So if item  needs to be swapped with item  (in order to achieve identical ranks between those of groups  and ), then the cost for this edit is .But is this method suitable? To answer this, let's look at it a bit deeper:It's not normalized. If we say that the distance between ranks of groups  is , while the distance between the ranks of groups  is , it doesn't necessarily mean that  are more similar each other than  are to each other (it could also possibly mean that  were ranking a much larger set of items). It assumes that the cost of each edit is linear with respect to number of hops. Is this true for our application domain? Could it be that a logistic relationship is more suitable? Or an exponential one?It assumes that all items are equally important. E.g. disagreement in ranking item (say)  is treated identically to the disagreement in ranking item (say) . Is this true in your domain? For example, if we are ranking books, is disagreeing on ranking of a famous book such as a TAOCP one, equally important to disagreeing on the ranking of a terrible book such as TAOUP?Once we address the points above, and reach a suitable measure of similarity between two ranks, we will then need to ask more interesting questions, such as:What is the probability of observing such differences, or more extreme differences, if the difference between the groups  and  was only due to random chance?","Display_name":"caveman","Creater_id":100507,"Start_date":"2016-08-03 23:23:51","Question_id":51295}
{"_id":{"$oid":"5837a585a05283111e4d63ab"},"Last_activity":"2016-08-01 18:02:09","Creator_reputation":646,"Question_score":1,"Answer_content":"Warning: it's a great question and I don't know the answer, so this is really more of a \"what I would do if I had to\":In this problem there are lots of degrees of freedom and lots of comparisons one can do, but with limited data it's really a matter of aggregating data efficiently. If you don't know what test to run, you can always \"invent\" one using permutations:First we define two functions:Voting function: how to score the rankings so we can combine all the rankings of a single group. For example, you could assign 1 point to the top ranked item, and 0 to all others. You'd be losing a lot of information though, so maybe it's better to use something like: top ranked item gets 1 point, second ranked 2 points, etc. Comparison function: How to compare two aggregated scores between two groups. Since both will be a vector, taking a suitable norm of the difference would work.Now do the following:First compute a test statistic by computing the average score using the voting function for each item across the two groups, this should lead to two vectors of size 25.Then compare the two outcomes using the comparison function, this will be your test statistic.The problem is that we don't know the distribution of the test statistic under the null that both groups are the same. But if they are the same, we could randomly shuffle observations between groups. Thus, we can combine the data of two groups, shuffle/permute them, pick the first  (number of observations in original group A) observations for group A and the rest for group B. Now compute the test statistic for this sample using the preceding two steps.Repeat the process around 1000 times, and now use the permutation test statistics as empirical null distribution. This will allow you to compute a p-value, and don't forget to make a nice histogram and draw a line for your test statistic like so: Now of course it is all about choosing the right voting and comparison functions to get good power. That really depends on your goal and intuition, but I think my second suggestion for voting function and the  norm are good places to start. Note that these choices can and do make a big difference. The above plot was using the  norm and this is the same data with an  norm:But depending on the setting, I expect there can be a lot of intrinsic randomness and you'll need a fairly large sample size to have a catch-all method work. If you have prior knowledge about specific things you think might be different between the two groups (say specific items), then use that to tailor your two functions. (Of course, the usual do this before you run the test and don't cherry-pick designs till you get something significant applies)PS shoot me a message if you are interested in my (messy) code. It's a bit too long to add here but I'd be happy to upload it.","Display_name":"Sven","Creater_id":67254,"Start_date":"2016-08-01 17:53:17","Question_id":51295}
{"_id":{"$oid":"5837a585a05283111e4d63ac"},"Last_activity":"2016-08-01 07:01:23","Creator_reputation":121,"Question_score":2,"Answer_content":"This sounds like the 'Willcoxon signed-rank test' (wikipedia link). Assuming that the values of your ranks are from the same set (ie [1, 25]) then this is a paired-difference test (with the null-hypothesis being these pairs were picked randomly). NB this is a dis-similarity score!There are both R and Python implementations linked to in that wiki page. ","Display_name":"danodonovan","Creater_id":13446,"Start_date":"2016-08-01 06:51:02","Question_id":51295}
{"_id":{"$oid":"5837a585a05283111e4d63bd"},"Last_activity":"2016-08-03 22:45:36","Creator_reputation":5325,"Question_score":2,"Answer_content":"The cfa() function is a wrapper for lavaan, which (among other things) adds the arguments auto.fix.first = TRUEauto.var = TRUEChanging your model type to CFA changes a few cosmetic things, but not the basic model. Because the lavaan() command has not put these in, you need to add the arguments, or add the appropriate parameters to the model.lavaan(model = 'latent =~ 1*ind1 + ind2 + ind3 + ind4                latent ~~ latent                ind1 ~~ ind1                ind2 ~~ ind2                ind3 ~~ ind3                ind4 ~~ ind4',       data = foo,        model.type = \"cfa\")That seems like more work, but I prefer to (almost) always use the lavaan() function - the trouble with using cfa() is that it does some of the work for you, but you can forget what it has done, and what you want to do. You can therefore screw up by, say, fixing the variance of the latent to 1 AND fixing the first loading to 1 (because the second thing is done automatically, you might forget). But cfa() is much easier for simple models that you're not going to futz around with much. ","Display_name":"Jeremy Miles","Creater_id":17072,"Start_date":"2016-08-03 22:45:36","Question_id":227150}
{"_id":{"$oid":"5837a585a05283111e4d63cc"},"Last_activity":"2016-08-03 21:34:58","Creator_reputation":3038,"Question_score":4,"Answer_content":"A probability space is defined as a tripple , where  is the set of outcomes,  a -algebra on  and  a probability measure on . As you say, a random variable  is a map from  to  such that for any Borel set  in  it holds that .  Because of the latter propery, it holds that, for any Borel set , there exists an event  such that . Therefore we can measure any Borel set with a measure (depending on )  by defining  where  is the event supra. In other words, to ''measure'' B, look for its inverse image under , which (by definition of ) belongs to , and, as this inverse  is in , we can measure it with the probability measure , i.e. If  is the set of all Borel sets, then it can be shown that  is also a probability space. The map  is called a random variable, and the function  is called the distribution of . This formally defines a random variable and its distribution.  Examples are a normal random variable, Binomial random variables, etc. (see this link for detail on the Binomial random variable). Let us take the normal random variable, with mean  and standard deviation  as an example i.e. ). A (random) sample of size  are just  random outcomes  from the distribution of . These outcomes are random, so if we redo the random draws, we will find ''other'' values .  Therefore the sample average  is also ''random''. This sample average is a so-called ''test statistic'' and it is a special case of a random variable, namely a random variable that is derived from the normal random variable .  Let's see how these two are defined formally, and let's , for simplicity, say that the Borel sets are all the intervals in .  Then the normal random variable  has ,  is the set of all intervals (I simplified here, it should be a -algebra), and the measure  for an interval  is  where  is the density of a normal variable with mean  and standard deviation . Your test statistic  is another random variable, derived from , with the same , the same -algebra, but with another measure  where  and  is the density of a normal variable with mean  and standard deviation . So, to answer your question, a test statistic is a special case of a random variable. The test statistic is thus a random variable, related to the random variable from which the sample (used to compute the test statistic) was drawn.  I would not go that far as saying that a sample relates to a test statistic like an outcome relates to a random variable.  A test statistic is a random variable (cfr supra) however in my opinion the sample () is not the outcome of a test statistic (). I would say that the sample average is an outcome of the random variable . The sample itself is an outcome of another random variable with outcomes in the product probability space with outcome set  (and ''induced'' sigma-algebra and probability measures). ","Display_name":"fcop","Creater_id":83346,"Start_date":"2016-08-03 00:10:00","Question_id":226977}
{"_id":{"$oid":"5837a585a05283111e4d63d9"},"Last_activity":"2013-02-13 11:09:59","Creator_reputation":404,"Question_score":13,"Answer_content":"That sentence does not actually make sense and is clearly in error.Data cannot be statistically significant or insignificant. Only relationships between data, the product of statistical tests, can be spoken about in these terms.If the question is: Can we drop data from our analyses because the inclusion of that data means we cannot reject the null hypothesis? The answer is — obviously, I hope! — no. The message you've cited is a news report, not a scientific paper. Had it been a paper that was reviewed, it never would have gotten in.Probably, data was not included because there are substantive reasons to not include those data. Probably, as others have suggested, the excluded data was incomplete or collected using different or incomparable methods.","Display_name":"Benjamin Mako Hill","Creater_id":6575,"Start_date":"2011-09-30 08:34:54","Question_id":16268}
{"_id":{"$oid":"5837a585a05283111e4d63da"},"Last_activity":"2011-10-01 10:47:39","Creator_reputation":5187,"Question_score":6,"Answer_content":"In the report cited in whuber's comment, it says on page 104 [pg 114 in the pdf]:  The survey succeeded in activating the participation of approximately 8,900 doctoral candidates from more than 30 countries...Then, spanning pages 104-105, it says:  While conducting data cleaning procedures, the Eurodoc survey experts' team decided to run a power test analysis.  Based on the assumption of fully completed questionnaires which will result in a multi normal distribution, a power test for estimation of the confidence interval was used.  This was done to test the accuracy of the data.  It was decided to accept maximum a 6% error-level at a 95% confidence interval.  A loss of 16% of the sampling size resulted in a sample of 12 participating countries with 7,600 participants.So it's not really clear exactly why the 16% loss in the sample, but the assumption of incomplete responses is likely correct.  (And you can see why the reporter was confused.)","Display_name":"Karl","Creater_id":5862,"Start_date":"2011-10-01 10:47:39","Question_id":16268}
{"_id":{"$oid":"5837a585a05283111e4d63db"},"Last_activity":"2011-10-01 08:55:25","Creator_reputation":16289,"Question_score":2,"Answer_content":"No, but reporters can use technical jargon completely nonsensically.","Display_name":"John","Creater_id":601,"Start_date":"2011-09-30 02:58:23","Question_id":16268}
{"_id":{"$oid":"5837a585a05283111e4d63dc"},"Last_activity":"2011-09-30 02:39:36","Creator_reputation":7438,"Question_score":6,"Answer_content":"No.I suspect the reporter meant to say that the other individuals were left out because the surveys were incomplete or internally inconsistent.","Display_name":"Harvey Motulsky","Creater_id":25,"Start_date":"2011-09-30 02:39:36","Question_id":16268}
{"_id":{"$oid":"5837a585a05283111e4d63e9"},"Last_activity":"2016-08-03 19:39:59","Creator_reputation":1376,"Question_score":3,"Answer_content":"Let . It's always true by independence thatF_Z(z) = P(\\max(X,Y) \\le z) = P(X \\le z)P(Y \\le z). If , then . If  then .You get the density by taking the derivative. The density is . ThenE[\\max(X,Y)] = \\int_0^1 z^2dz + \\int_1^2 z/2 dz = \\frac{1}{3} + 1 - \\frac{1}{4}We can check this by taking the simulating data and taking the sample average with some R code. It should be close to our answer by the law of large numbers.num \u0026lt;- 1000x \u0026lt;- runif(num,0,1)y \u0026lt;- runif(num,0,2)df \u0026lt;- cbind(x,y)mean(apply(df, 1, max))  # [1] 1.08731","Display_name":"Taylor","Creater_id":8336,"Start_date":"2016-08-03 19:30:22","Question_id":228188}
{"_id":{"$oid":"5837a586a05283111e4d64aa"},"Last_activity":"2016-08-03 09:27:32","Creator_reputation":2271,"Question_score":1,"Answer_content":"This is the most intuitive article that I have seen so far:The Cramér-Rao Lower Bound on Variance: Adam and Eve’s “Uncertainty Principle” by Michael R. Powers, Journal of Risk Finance, Vol. 7, No. 3, 2006The bound is explained by an analogy of Adam and Eve in the Garden of Eden tossing a coin to see who gets to eat the fruit and they then ask themselves just how big a sample is necessary to achieve a certain level of accuracy in their estimate, and they then discover this bound...Nice story with a profound message about reality indeed.","Display_name":"vonjd","Creater_id":230,"Start_date":"2016-08-03 09:27:32","Question_id":10578}
{"_id":{"$oid":"5837a586a05283111e4d64ab"},"Last_activity":"2011-05-10 08:09:16","Creator_reputation":4054,"Question_score":23,"Answer_content":"Here I explain why the asymptotic variance of the maximum likelihood estimator is the Cramer-Rao lower bound.  Hopefully this will provide some insight as to the relevance of the Fisher information.Statistical inference proceeds with the use of a likelihood function  which you construct from the data.  The point estimate  is the value which maximizes . The estimator  is a random variable, but it helps to realize that the likelihood function  is a \"random curve\".Here we assume iid data drawn from a distribution , and we define the likelihood\r\\mathcal{L}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\log f(x_i|\\theta)\rThe parameter  has the property that it maximizes the value of the \"true\" likelihood, .  However, the \"observed\" likelihood function  which is constructed from the data is slightly \"off\" from the true likelihood.  Yet as you can imagine, as the sample size increases, the \"observed\" likelihood converges to the shape of the true likelihood curve.  The same applies to the derivative of the likelihood with respect to the parameter, the score function .  (Long story short, the Fisher information determines how quickly the observed score function converges to the shape of the true score function.)At a large sample size, we assume that our maximum likelihood estimate  is very close to .  We zoom into a small neighborhood around  and  so that the likelihood function is \"locally quadratic\".There,  is the point at which the score function  intersects the origin.  In this small region, we treat the score function as a line, one with slope  and  random intercept  at .  We know from the equation for a line thata(\\hat{\\theta} - \\theta)  + b = 0or\r\\hat{\\theta} = \\theta - b/a .\rFrom the consistency of the MLE estimator, we know that\r\\mathbb{E}(\\hat{\\theta}) = \\theta\rin the limit.Therefore, asymptotically\rnVar(\\hat{\\theta}) = nVar(b/a)\rIt turns out that the slope varies much less than the intercept, and asymptotically, we can treat the score function as having a constant slope in a small neighborhood around .  Thus we can write\rnVar(\\hat{\\theta}) = \\frac{1}{a^2}nVar(b)\rSo, what are the values of  and ?  It turns out that due to a marvelous mathematical coincidence, they are the very same quantity (modulo a minus sign), the Fisher information.-a = \\mathbb{E}\\left[-\\frac{\\partial^2 \\mathcal{L}}{\\partial \\theta^2}\\right] = I(\\theta)nVar(b) = nVar\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\theta}\\right] = I(\\theta)Thus,\rnVar(\\hat{\\theta}) = \\frac{1}{a^2}nVar(b) = (1/I(\\theta)^2)I(\\theta) = 1/I(\\theta)\rasymptotically: the Cramer-Rao lower bound.  (Showing that  is a the lower bound on the variance of an unbiased estimator is another matter.)","Display_name":"charles.y.zheng","Creater_id":3567,"Start_date":"2011-05-09 16:09:17","Question_id":10578}
{"_id":{"$oid":"5837a586a05283111e4d64ac"},"Last_activity":"2011-05-10 00:19:38","Creator_reputation":15827,"Question_score":11,"Answer_content":"One way that I understand the fisher information is by the following definition:I(\\theta)=\\int_{\\cal{X}} \\frac{\\partial^{2}f(x|\\theta)}{\\partial \\theta^{2}}dx-\\int_{\\cal{X}} f(x|\\theta)\\frac{\\partial^{2}}{\\partial \\theta^{2}}log[f(x|\\theta)]dxThe Fisher Information can be written this way whenever the density  is twice differentiable.  If the sample space  does not depend on the parameter , then we can use the Leibniz integral formula to show that the first term is zero (differentiate both sides of  twice and you get zero), and the second term is the \"standard\" definition.  I will take the case when the first term is zero.  The cases when it isn't zero aren't much use for understanding Fisher Information.Now when you do maximum likelihood estimation (insert \"regularity conditions\" here) you set\\frac{\\partial}{\\partial \\theta}log[f(x|\\theta)]=0And solve for .  So the second derivative says how quickly the gradient is changing, and in a sense \"how far\"  can depart from the MLE without making an appreciable change in the right hand side of the above equation.  Another way you can think of it is to imagine a \"mountain\" drawn on the paper - this is the log-likelihood function.  Solving the MLE equation above tells you where the peak of this mountain is located as a function of the random variable .  The second derivative tells you how steep the mountain is - which in a sense tells you how easy it is to find the peak of the mountain.  Fisher information comes from taking the expected steepness of the peak, and so it has a bit of a \"pre-data\" interpretation.One thing that I still find curious is that its how steep the log-likelihood is and not how steep some other monotonic function of the likelihood is (perhaps related to \"proper\" scoring functions in decision theory? or maybe to the consistency axioms of entropy?).The Fisher information also \"shows up\" in many asymptotic analysis due to what is known as the Laplace approximation.  This basically due to the fact that any function with a \"well-rounded\" single maximum raise to a higher and higher power goes into a Gaussian function  (similar to Central Limit Theorem, but slightly more general).  So when you have a large sample you are effectively in this position and you can write:f(data|\\theta)=\\exp(log[f(data|\\theta)])And when you taylor expand the log-likelihood about the MLE:f(data|\\theta)\\approx [f(data|\\theta)]_{\\theta=\\theta_{MLE}}\\exp\\left(-\\frac{1}{2}\\left[-\\frac{\\partial^{2}}{\\partial \\theta^{2}}log[f(data|\\theta)]\\right]_{\\theta=\\theta_{MLE}}(\\theta-\\theta_{MLE})^{2}\\right)and that second derivative of the log-likelihood shows up (but in \"observed\" instead of \"expected\" form).  What is usually done here is to make to further approximation:-\\frac{\\partial^{2}}{\\partial \\theta^{2}}log[f(data|\\theta)]=n\\left(-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial^{2}}{\\partial \\theta^{2}}log[f(x_{i}|\\theta)]\\right)\\approx nI(\\theta)Which amounts to the usually good approximation of replacing a sum by an integral, but this requires that the data be independent.  So for large independent samples (given ) you can see that the Fisher information is how variable the MLE is, for various values of the MLE.","Display_name":"probabilityislogic","Creater_id":2392,"Start_date":"2011-05-10 00:19:38","Question_id":10578}
{"_id":{"$oid":"5837a586a05283111e4d64b8"},"Last_activity":"2016-08-03 09:05:51","Creator_reputation":71,"Question_score":3,"Answer_content":"One possibility for approximating the ISE is by expanding the binomial:ISE = \\int p(x)^2dx - 2\\int p(x)q(x)dx + \\int q(x)^2dx.Assuming that you can simulate from  and , you can approximate this quantity as follows:Simulate  samples from , , and use the Monte Carlo approximation .Simulate  samples from , , and use the Monte Carlo approximation .The cross product is a bit more tricky but you can compare the estimators:2\\int p(x)q(x)dx \\approx \\frac{2}{N}\\sum_{j=1}^N p(y_j),2\\int p(x)q(x)dx \\approx \\frac{2}{N}\\sum_{j=1}^N q(x_j).If you cannot simulate from  and , then you may need use some deterministic numerical integration method such as quadrature. Another possibility is to use importance sampling integration, but you need to choose an appropriate importance density function  that you can simulate from as follows:\\int p(x)^2dx = \\int \\frac{p(x)^2}{r(x)}r(x)dx \\approx \\frac{1}{N}\\sum_{j=1}^N \\frac{p(z_j)^2}{r(z_j)},where . Analogously for the other quantities. You can also use this trick on .Anyway, it is also a good practice to compare these estimators with those obtained with quadrature methods, and my main message is that life is not easy, and neither multivariate numerical integration.","Display_name":"cestlavie","Creater_id":125522,"Start_date":"2016-08-03 09:05:51","Question_id":227093}
{"_id":{"$oid":"5837a586a05283111e4d64d1"},"Last_activity":"2016-08-03 07:44:01","Creator_reputation":535,"Question_score":3,"Answer_content":"In addition to calculating two reliability or alpha coefficients (as @mdewey rightly suggested), you may also have a look at  (hierarchical omega) and  (omega total), which are very useful for \"multidimensional\" item sets. The first one is a measure of variability due to a general \"g-factor\", and the second one is a measure of variability due to both a general and group factors (scale.1 and scale.2 would be the group factors in your example). Definitely look at Revelle's sources for his psych package (e.g., http://personality-project.org/r/psych/HowTo/R_for_omega.pdf and Chapter 7.2.5 of his book on psychometric theory.","Display_name":"hplieninger","Creater_id":27276,"Start_date":"2016-08-03 07:03:22","Question_id":226839}
{"_id":{"$oid":"5837a586a05283111e4d64d2"},"Last_activity":"2016-08-02 03:47:26","Creator_reputation":3724,"Question_score":3,"Answer_content":"Cronbach's alpha is a measure of internal consistency of a scale. It makes sense to do it for scale 1 separately and for scale 2 separately but there seems no point in doing it for the whole questionnaire when there are supposed to be separate sub-scales.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-02 03:47:26","Question_id":226839}
{"_id":{"$oid":"5837a586a05283111e4d64e0"},"Last_activity":"2016-08-03 07:29:18","Creator_reputation":3645,"Question_score":3,"Answer_content":"There are several possible workarounds. The simplest, although not necessarily the best, is to use poisson regression instead of logistic regression. The reasoning behind this is that the poisson model was \"designed\" for rare events. Gary King discusses case control, bias corrections and adjusting causal models for rare event data in the context of political outcomes. He has published several papers as well as a software tool (ReLogit) specifically for this purpose.  http://gking.harvard.edu/category/research-interests/methods/rare-eventsFinally, Paul Allison, founder of the training institute Statistical Horizons and one of the best writers on statistical issues out there, cites King's article in formulating his views on the subject. His opinion is that,  The problem is not specifically the rarity of events, but rather the  possibility of a small number of cases on the rarer of the two  outcomes.  If you have a sample size of 1000 but only 20 events, you  have a problem. If you have a sample size of 10,000 with 200 events,  you may be OK. If your sample has 100,000 cases with 2000 events,  you’re golden.He offers a course for dealing with these issues as well as a useful discussion here ... http://statisticalhorizons.com/logistic-regression-for-rare-events","Display_name":"DJohnson","Creater_id":82102,"Start_date":"2016-08-03 07:29:18","Question_id":227066}
{"_id":{"$oid":"5837a586a05283111e4d64ee"},"Last_activity":"2016-08-03 07:19:34","Creator_reputation":535,"Question_score":1,"Answer_content":"All three assumptions may be studied using CFA. If a unidimensional, -equivalent (all loadings equal) model with uncorrelated errors holds, then you have evidence that the assumptions hold. However, with only 63 people you may not have good power for a rigorous test.The most critical of the three assumptions is unidimensionality. To investigate that issue, it's often a good starting point to use exploratory FA and to compare Alpha to measures such as  or . For that, you may have a look at Revelle's sources for his psych package (e.g., http://personality-project.org/r/psych/HowTo/R_for_omega.pdf and Chapter 7.2.5 of his book on psychometric theory.","Display_name":"hplieninger","Creater_id":27276,"Start_date":"2016-08-03 07:19:34","Question_id":210150}
{"_id":{"$oid":"5837a586a05283111e4d64fd"},"Last_activity":"2016-08-03 06:39:29","Creator_reputation":56,"Question_score":4,"Answer_content":" E_Q\\left[\\frac{\\nabla_\\phi Q_\\phi(h|x)}{Q_\\phi(h|x)}\\right] = \\int\\frac{\\nabla_\\phi Q_\\phi(h|x)}{Q_\\phi(h|x)} Q_\\phi(h|x) = \\int{\\nabla_\\phi Q_\\phi(h|x)}Assuming that you can exchange the integral and the gradient operators (deep waters)  = \\nabla_\\phi\\int{ Q_\\phi(h|x)} = \\nabla_\\phi E_Q[1] = \\nabla_\\phi 1 =0Since the distribution  is chosen by the user, you can say that this result is satisfied by those  that allow exchanging the integral and the gradient.","Display_name":"Sale","Creater_id":125490,"Start_date":"2016-08-03 06:27:11","Question_id":227065}
{"_id":{"$oid":"5837a586a05283111e4d6509"},"Last_activity":"2016-08-03 04:11:09","Creator_reputation":56,"Question_score":0,"Answer_content":"The Gamma prior is not conjugate for the Gamma sampling model. See Conjugate prior for a Gamma distributionThe following R code shows how to sample from the posterior of a Gamma model with uniform priors on  for the scale and shape parameters, you can play with other priors, using an adaptive MCMC sampler.set.seed(12345)# Simulated datadata = rgamma(100,shape = 2.5,scale=97)# Histogram and likelihood fithist(data)library(MASS)fitdistr(data,\"gamma\")# -log posteriormlogpost = function(par){loglik = sum(dgamma(data,shape=par[1],scale = par[2],log=T))logprior = -2*log(1000)return(-loglik - logprior)}# Library for the MCMC samplerlibrary(Rtwalk)# Initial pointinit = c(2.5,97)# Support of the posteriorSupport \u0026lt;- function(x) {    ((0 \u0026lt; x[1])\u0026amp;(x[2]\u0026gt;0))   }# Random initial points (see documentation of Rtwalk)X0 \u0026lt;- function(x) { init + runif(2,-0.1,0.1) }# Samplerinfo \u0026lt;- Runtwalk( dim=2,  Tr=105000,  Obj=mlogpost, Supp=Support, x0=X0(), xp0=X0()) #burn in and thinning the posterior samplesind = seq(5000,105000,by=10)sc = infooutput[,2][ind]# histograms of the posterior sampleshist(sc)hist(sh)# summary of the posterior samplessummary(sc)summary(sh)","Display_name":"Sale","Creater_id":125490,"Start_date":"2016-08-03 04:11:09","Question_id":227035}
{"_id":{"$oid":"5837a586a05283111e4d651e"},"Last_activity":"2016-08-03 05:53:54","Creator_reputation":3724,"Question_score":0,"Answer_content":"If the quantity of data you have is fixed then there is little you can do to improve your power to detect a difference. You could also use the Brown-Forsythe test which may have some slight advantage but really with five in your smaller group your prospects are not stunning.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-03 05:53:54","Question_id":227036}
{"_id":{"$oid":"5837a586a05283111e4d6529"},"Last_activity":"2015-09-30 13:24:55","Creator_reputation":12922,"Question_score":6,"Answer_content":"This methodology is described in the glmnet paper Regularization Paths for Generalized Linear Models via Coordinate Descent.  Although the methodology here is for the general case of both  and  regularization, it should apply to the LASSO (only ) as well.The solution for the maximum  is given in section 2.5.    When , we see from (5) that  will stay zero if . Hence That is, we observe that the update rule for beta forces all parameter estimates to zero for  as determined above.The determination of  and the number of grid points seems less principled.  In glmnet they set , and then choose a grid of  equally spaced points on the logarithmic scale.This works well in practice, in my extensive use of glmnet I have never found this grid to be too coarse.In the LASSO () only case things work better, as the LARS method provides a precise calculation for when the various predictors enter into the model.  A true LARS does not do a grid search over , instead producing an exact expression for the solution paths for the coefficients.Here is a detailed look at the exact calculation of the coefficient paths in the two predictor case.The case for non-linear models (i.e. logistic, poisson) is more difficult.  At a high level, first a quadratic approximation to the loss function is obtained at the initial parameters , and then the calculation above is used to determine .  A precise calculation of the parameter paths is not possible in these cases, even when only  regularization is provided, so a grid search is the only option.Sample weights also complicate the situation, the inner products must be replaced in appropriate places with weighted inner products.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2015-09-30 13:14:18","Question_id":174897}
{"_id":{"$oid":"5837a586a05283111e4d6538"},"Last_activity":"2016-08-03 05:41:32","Creator_reputation":6097,"Question_score":3,"Answer_content":"I am interpreting your question as much more general in that \"Is there any gain to using a reversible Markov chain over a non-reversible Markov chain?\". Here are two reasons I can think of off the top of my head:Standard errors: If the chain is reversible, then a Markov chain CLT can hold for geometrically ergodic Markov chains while assuming only a finite second moment. If the chain is not reversible, then you have to assume  for  finite moments. So if you are estimating the posterior mean, and have only two finite moments available, then only a nonreversible Markov chain might not allow analysis of standard errors. You can find more information here.Spectral Gap: Often analysis of the convergence rates of MCMC samplers is done by looking at the spectral gap of the Markov chain. For a reversible Markov chain, the second largest eigenvalue determines the mixing time, and there are many known bounds for this. Maybe see a review here. So if your Markov chain is reversible, it is likely easier to study its convergence rates. There is also some work that has been done for non-reversible Markov chains (see this), but the literature is not as rich. This is some more discussion on this in Mathoverflow.Overall, if you don't have a need to study the exact convergence rate for your sampler, and your distribution is well behaved enough that it has larger than 2 moments for most functions of interest, then there should be no reason to restrict yourself to just reversible Markov chains. This is part of the reason why fixed-scan Gibbs sampler are so often used; in practice nothing is lost.","Display_name":"Greenparker","Creater_id":31978,"Start_date":"2016-07-31 08:38:34","Question_id":226550}
{"_id":{"$oid":"5837a586a05283111e4d6545"},"Last_activity":"2016-08-03 05:31:09","Creator_reputation":15629,"Question_score":2,"Answer_content":"You really found a bug, which was immediately fixed by the maintainer (version 1.4.1 on CRAN - available already as .tar.gz, win binaries will take a bit longer):library (chemometrics)library (pls)train \u0026lt;- yarn [yarnNIR, trainNIR, center = colMeans (traindensity)y.hat \u0026lt;- x.centered %*% restrain])now we have:\u0026gt; cbind (y.hat, y.pls, y.hat - y.pls)              density       density110 53.09713 53.09713  0.000000e+0022  52.74115 52.74115 -8.526513e-1431  34.67483 34.67483 -8.526513e-1441  37.48693 37.48693 -7.105427e-1551  32.26085 32.26085 -3.552714e-1461  21.23447 21.23447 -6.394885e-1471  22.18097 22.18097 -4.973799e-14 So basically the same (up to \"numeric noise\") ","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2016-08-03 05:31:09","Question_id":226651}
{"_id":{"$oid":"5837a586a05283111e4d6552"},"Last_activity":"2016-08-03 04:56:36","Creator_reputation":15629,"Question_score":0,"Answer_content":"  how does 10 folds method determine the best parameter in the end? Strictly speaking: it doesn't. The best parameter is usually determined by comparing the performance (which may be measured by cross validation - or by other methods) of a number of models that were built with varying hyperparameters. The hyperparameter value that lead to the best performance is then selected. Remember that by such a selection, you \"use up\" the performance estimate: it enters the model training that way and that is the reason why you need another performance measurement of the final model that is independent also of this selection procedure.  here's an example: so I understand that each fold is compared against the other 8 for the best fit parameters by running it with the validation set. No. Typically all 10 folds are evaluated with the same parameter (s).   Let's say the first time param A is selected, the second time around another fold is selected, let's call it param B. No. The fold is not a parameter to select. Hyperparameter and fold (or more precisely: your whole performance measurement scheme) are independent of each other.   How does the method make a decision if param A or B is better?You can tabulate your performance measurements (loss function values, errors) as table of case against parameter value, e.g. parameter value:   A    Bloss:case 1             L1A  L1B    case 2             L2A  L2B                ...            case n             L1A  L1B                If you evaluate both hyperparameter values with the same cross validation splits (and of course for the same cases), you can set up the comparison as a paired test. For example, for each case you calculate the difference LiA - LiB and test whether that is different from zero. (This is just an example, there are other tests such as rank tests and not paired tests as well)","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2016-08-03 04:56:36","Question_id":226983}
{"_id":{"$oid":"5837a586a05283111e4d6553"},"Last_activity":"2016-08-02 18:51:49","Creator_reputation":338,"Question_score":1,"Answer_content":"Start with something simpler. Say you have a parameter that can many values and you want to see which value is best for that parameter. What would you do? You would loop through all the values for that parameter and try them out.How do you try them out? This is where k-fold cross validation comes in. (It need not be 10 fold by the way). You split the training data into 10 folds and loop through the folds. For each fold, you train the model on the other 9 folds and then evaluate the model on the held out fold.So, for each parameter value, you will have k (in your example, 10) values. You can average these 10 and this will give an evaluation of how that parameter value is.This pseudo code might be clearer (in this case, I assume you are trying to minimize the error):best_parameter_value \u0026lt;- Nothing best_parameter_score \u0026lt;- infinity for p in parameter_values:    folds \u0026lt;- split data into k folds    results_per_fold \u0026lt;- empty list    for f in folds:        model \u0026lt;- train model on the other k-1 folds with parameter p        result \u0026lt;- evaluate model on f and get the error        add result to results_per_fold    average \u0026lt;- mean of results_per_fold    if average \u0026lt; best_parameter_score:        best_parameter_value = p Return best_parameter_value","Display_name":"roundsquare","Creater_id":122754,"Start_date":"2016-08-02 18:51:49","Question_id":226983}
{"_id":{"$oid":"5837a586a05283111e4d6560"},"Last_activity":"2016-08-03 04:41:45","Creator_reputation":123,"Question_score":2,"Answer_content":"There are basicly three types of correlation testsa pearsons correlation is the most common. It assumes that the data is normally distributed.To figure out if your variables are normally or non-normally distributed you can use a shapiro-wilk test like this: shapiro.test(variable) info here: http://stackoverflow.com/questions/15427692/perform-a-shapiro-wilk-normality-test . If your p-value is below \u0026lt;= 0.05, then you would reject the NULL hypothesis that the samples came from a Normal distribution.If not then you cannot reject the null hypothesis that the data is normally distributed, e.i above 0.05. In other words you can savely (without violating assumptions) do a correlation like this: cor.test(variable1, variable1, method=\"pearson\")If the variables are non-normally distributed then you have to use a different correlation test, called \"spearmans\"cor.test(variable1, variable1, method=\"spearman\")A spearmans test works by ranking the data, such that the lowest value is assigened to 1, secondlowest to 2 and so on.If there are many values that are identical, it will give several tied ranks, if you have a lot of tied ranks or if you have a small data set then a method called \"kendall\" is more appropriate (or won't give biased estimates of the p-value).cor.test(variable1, variable1, method=\"kendall\")You could if you wanted to write it in code like this: df\u0026lt;-data.frame(x=rnorm(100),y=rnorm(100))sp_Cov1 \u0026lt;- shapiro.test(df[,1])sp_Cov2 \u0026lt;- shapiro.test(df[,2]) if(sp_Cov1[2] \u0026lt; 0.05 | sp_Cov2[2] \u0026lt; 0.05) {correlationToUse = 'kendall'} else {correlationToUse = 'pearson'}cortest_pvalue\u0026lt;-as.numeric(format((cor.test(df[,1],df[,2], method = correlationToUse)[3]),digits=5,scientific=FALSE))in this case the correlation would be method=\"pearson\" since it was generated from a normal distribution There is also a boot-strap correlation for non-parametric data with a low sample size, but i know nothing about it.I would recommend the book Discovering Statistics Using R - by Andy Fields for more info","Display_name":"user2673238","Creater_id":86330,"Start_date":"2015-08-19 06:19:29","Question_id":167872}
{"_id":{"$oid":"5837a586a05283111e4d656d"},"Last_activity":"2016-08-03 03:57:03","Creator_reputation":56,"Question_score":0,"Answer_content":"You can generalize any distribution  by adding parameters  through the probability integral transform:S(x;\\eta,\\theta)= P[F(x;\\eta);\\theta],where  is the initial cdf, and  is a distribution with parameter . If  is the uniform distribution, then . Popular choices of  are the Beta distribution, or the Kumaraswamy distribution.In terms of the density functions(x;\\eta,\\theta)= p[F(x;\\eta);\\theta]f(x;\\eta),where  is the pdf associated to . Ideally, when you add parameters to a distribution, you would like them to have separate roles, and to avoid adding redundant parameters that may make your model unidentifiable. See:  Ferreira, J. T. S., \u0026amp; Steel, M. F. (2006). A constructive representation of univariate skewed distributions. Journal of the American Statistical Association.","Display_name":"Sale","Creater_id":125490,"Start_date":"2016-08-03 03:47:35","Question_id":226449}
{"_id":{"$oid":"5837a586a05283111e4d657a"},"Last_activity":"2016-08-03 03:53:32","Creator_reputation":1,"Question_score":0,"Answer_content":"Evaluation of recommender systems can be done by splitting the entire dataset into Training and Test datasets. Metrics like precision, recall, ROC can be used for binary ratings and metrics like RMSE, MAE can be used for absolute ratings.These error metrics can be analyzed across split time dimensions, different user clusters to note if there are any seasonal pattern or built in clusters within the data ","Display_name":"user3126530","Creater_id":125484,"Start_date":"2016-08-03 03:34:53","Question_id":201968}
{"_id":{"$oid":"5837a586a05283111e4d657d"},"Last_activity":"2016-08-03 03:26:38","Creator_reputation":5445,"Question_score":2,"Answer_content":"AIC can't be used to to compare models fitted on different datasets, so comparing your model to that of the English model makes no sense whatsoever. So, there is no justification at all to abandon your model and favour the English one on this basis.I would also advise a little caution in the use of modification indices, which allow the analyst to obtain a better fitting model without thinking. However, it does appear from what you said that these residual error covariances are justified in your case, since you say that they are similar items.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-03 03:26:38","Question_id":227027}
{"_id":{"$oid":"5837a586a05283111e4d658a"},"Last_activity":"2016-08-03 03:28:54","Creator_reputation":51,"Question_score":1,"Answer_content":"Your sizes of  and  are wrong. They have to have the same size. But let's understand why: both of them represent the covariance among the different variables into your data, one in a single class and the other among all classes. Therefore, being them expressing covariance among all different variables, is clear that their size should be square and  where  is the dimensionality of your data. In your case is 500, hence you should have two matrices of .Here is a post where they discuss correct formulas to compute LDA, have a look and make sure you are exactly computing these matrices. Also, when you get a  MATRIX it should ring a bell that something is wrong, because it would be otherwise called 'a scalar' and not a matrix if its size would be that. Although it is formally correct having a matrix 1 by 1 in math, when it comes to scatter matrix this makes absolutely no sense.","Display_name":"Renthal","Creater_id":125326,"Start_date":"2016-08-02 08:28:29","Question_id":92170}
{"_id":{"$oid":"5837a586a05283111e4d659b"},"Last_activity":"2016-08-03 02:03:07","Creator_reputation":152738,"Question_score":5,"Answer_content":"Test statistics are random variables.So just like with any other  random variables -- as long as the  are independent -- their sum is distributed as  with df equal to the sum of the individual dfs.(I presume that you intend them to be distributed as chi-squared under the null.)","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-03 02:03:07","Question_id":227007}
{"_id":{"$oid":"5837a586a05283111e4d65a8"},"Last_activity":"2016-08-03 01:50:07","Creator_reputation":498,"Question_score":4,"Answer_content":"lmer is used to fit linear mixed-effect models, so it assumes that the residual error has a Gaussian distribution. If your dependent variable A is a binary outcome (e.g. a yes/no response), then the error distribution is binomial and not Gaussian. In this case you have to use glmer, which allow to fit a generalized linear mixed-effects model: these models include a link function that allows to predict response variables with non-Gaussian distributions. One example of link function that could work in your case is the logistic function, which takes an input with any value from negative to positive infinity and return an output that always takes values between zero and one, which is interpretable as the probability of the binary outcome (e.g. the probability of the subject responding 'yes'). About the repeated measurement design, both lmer and glmer can handle it equally well, you just have to set 'subject' as a grouping factor (in the random-effect part of the model) for the within-subject predictors. In this way you allow these predictors to have a fixed-effect (common to all subjects) and a subject-specific random effect, so that you can test statistically the effect common to all subjects, and treat the subject-specific variations as a nuisance term.For more details on how to proceed I would recommend this excellent book by Knoblauch and Maloney that dedicates a large section on the application of mixed-effects models (using R and the lme4 library) to the modelling of psychophysical data.","Display_name":"Matteo Lisi","Creater_id":61836,"Start_date":"2016-08-03 01:50:07","Question_id":226946}
{"_id":{"$oid":"5837a587a05283111e4d65b5"},"Last_activity":"2016-08-03 01:32:18","Creator_reputation":2186,"Question_score":0,"Answer_content":"Yes, samples not reflecting the relation between features and target variable sufficiently would be one explanation for always prediction the \"regular case\" - but there are other possible answers too. It could e.g. also that your model very strongly overfits, thereby only remembering exactly those occurrences to fire for that it saw in the training set - hence will never fire for any \"just similar\" occurrences (you can imagine this e.g. as \"island\" around a sample in your feature space, instead of the decision border you would expect).If your training error is better than your test error (so the model is able to find a reasonable separation between 0 and 1 samples during fitting data), and this is the case for multiple hyperparameters: instead of training your model on all training data, perform (repeated) cross validation on your training data to evaluate different hyperparameters and select a suitable final model, which you then test on the held-back test set. If you assumption about samples not sufficiently reflecting the feature-target relation is true you will see that the model is able to fit to the  partitions during, hence the internal training error of each CV model will be small, while the internal evaluation error of each CV model on the th partition will be higher. If this is the case you can try different hyperparameters now to see if you ran into a high variance before (use e.g. a  parameter grid), or e.g. try to get more features and/or transform your data to better show the feature-target relation for your model (= represent your information differently by introducing new features based on the existing ones).If you change the prevalence of your data, only do this for the training partition (you want to see how it performs on \"real life\", unchanged test data later). ","Display_name":"geekoverdose","Creater_id":112731,"Start_date":"2016-08-03 01:32:18","Question_id":227011}
{"_id":{"$oid":"5837a587a05283111e4d65c4"},"Last_activity":"2016-08-02 07:23:48","Creator_reputation":101,"Question_score":3,"Answer_content":"This is an interesting question because (so far as I know) there is no widely used formula for computing the variance in this situation. Some time ago, I did some simulations to examine the performance of different formulas to estimate the sampling variance of Cohen's d in case of a one-sample t-test.I was aware of three different formulas:The formula used in the Comprehensive Meta-analysis Software:(1/sqrt(ni))*sqrt(1+di^2/2)^2,with ni being the sample size per study and di the observed Cohen's d.Other people use the standard formula for the dependent samples t-test (e.g., Borenstein, 2009) with correlation between pre- and posttest (r) equal to 0.5:(1/ni)+di^2/(2*ni)Another formula I have seen is one that was used in a paper by Koenig et al. (2011). This formula is obtained by personal communication with B. Becker.(1/ni)+di^2/(2*ni*(ni-1)) I did a very small simulation study to examine the performance of these three formulas with sample sizes ranging from 10 to 500 and effect sizes in the population ranging from 0 to 0.8. The differences between the formulas were most observable for a population effect size of 0.8.Using the formula of the dependent samples t-test with r=0.5 yielded the least biased estimates. However, there may be other formulas with better properties. I am curious what other people think about this.Code: rm(list = ls()) # Clean workspacek \u0026lt;- 10000 # Number of studiesthetais \u0026lt;- c(0, 0.2, 0.5, 0.8) # Effect in population nis \u0026lt;- c(10,15,20,30,50,75,100,250,500) # Sample size in primary studysigma \u0026lt;- 1 # Standard deviation in population### Empty objects for storing resultsvi.ac \u0026lt;- vi.beck \u0026lt;- vi.comp \u0026lt;- vi.dep \u0026lt;- matrix(NA, nrow = length(nis),                                                 ncol = length(thetais),                                                 dimnames = list(nis, thetais))############################################for(thetai in thetais) {  for(ni in nis) {     ### Actual variance Cohen's d    sdi \u0026lt;- sqrt(sigma/(ni-1) * rchisq(k, df = ni-1))    mi \u0026lt;- rnorm(k, mean = thetai, sd = sigma/sqrt(ni))    di \u0026lt;- mi/sdi    vi.ac[as.character(ni),as.character(thetai)] \u0026lt;- var(di)    ############################################    ### Suggestion by Becker in Koenig et al.    vi \u0026lt;- (1/ni)+di^2/(2*ni*(ni-1))    vi.beck[as.character(ni),as.character(thetai)] \u0026lt;- mean(vi)    ############################################    ### Comprehensive meta-analysis software    vi \u0026lt;- (1/sqrt(ni))*sqrt(1+di^2/2)^2    vi.comp[as.character(ni),as.character(thetai)] \u0026lt;- mean(vi)    ############################################    ### Dependent sample t-test with r=0.5    vi \u0026lt;- (1/ni)+di^2/(2*ni)    vi.dep[as.character(ni),as.character(thetai)] \u0026lt;- mean(vi)  }}plot(x = nis, y = vi.ac[ ,1], type = \"l\", main = \"theta = 0\", ylab = \"Variance\")lines(x = nis, y = vi.beck[ ,1], type = \"l\", col = \"red\")lines(x = nis, y = vi.comp[ ,1], type = \"l\", col = \"blue\")lines(x = nis, y = vi.dep[ ,1], type = \"l\", col = \"green\")legend(\"topright\", legend = c(\"Actual variance\", \"Becker\", \"CMA\", \"Dep. samples\"),        col = c(\"black\", \"red\", \"blue\", \"green\"), lty = c(1,1,1,1))plot(x = nis, y = vi.ac[ ,2], type = \"l\", main = \"theta = 0.2\")lines(x = nis, y = vi.beck[ ,2], type = \"l\", col = \"red\")lines(x = nis, y = vi.comp[ ,2], type = \"l\", col = \"blue\")lines(x = nis, y = vi.dep[ ,2], type = \"l\", col = \"green\")legend(\"topright\", legend = c(\"Actual variance\", \"Becker\", \"CMA\", \"Dep. samples\"),        col = c(\"black\", \"red\", \"blue\", \"green\"), lty = c(1,1,1,1))plot(x = nis, y = vi.ac[ ,3], type = \"l\", main = \"theta = 0.5\")lines(x = nis, y = vi.beck[ ,3], type = \"l\", col = \"red\")lines(x = nis, y = vi.comp[ ,3], type = \"l\", col = \"blue\")lines(x = nis, y = vi.dep[ ,3], type = \"l\", col = \"green\")legend(\"topright\", legend = c(\"Actual variance\", \"Becker\", \"CMA\", \"Dep. samples\"),        col = c(\"black\", \"red\", \"blue\", \"green\"), lty = c(1,1,1,1))plot(x = nis, y = vi.ac[ ,4], type = \"l\", main = \"theta = 0.8\")lines(x = nis, y = vi.beck[ ,4], type = \"l\", col = \"red\")lines(x = nis, y = vi.comp[ ,4], type = \"l\", col = \"blue\")lines(x = nis, y = vi.dep[ ,4], type = \"l\", col = \"green\")legend(\"topright\", legend = c(\"Actual variance\", \"Becker\", \"CMA\", \"Dep. samples\"),        col = c(\"black\", \"red\", \"blue\", \"green\"), lty = c(1,1,1,1))data.frame(vi.ac[,1], vi.beck[,1], vi.comp[,1], vi.dep[,1])References:Borenstein, M. (2009). Effect sizes for continuous data. In H. Cooper, L. V. Hedges \u0026amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221-236). New York: Russell Sage Foundation.Koenig, A. M., Eagly, A. H., Mitchell, A. A., \u0026amp; Ristikari, T. (2011). Are leader stereotypes masculine? A meta-analysis of three research paradigms. Psychological Bulletin, 137, 4, 616-42.","Display_name":"User33","Creater_id":79539,"Start_date":"2016-08-02 07:23:48","Question_id":226836}
{"_id":{"$oid":"5837a587a05283111e4d65d0"},"Last_activity":"2016-08-01 23:13:42","Creator_reputation":937,"Question_score":0,"Answer_content":"If all students for example are tested on the same metric, then you should be able to do a t-test/anova to compare two classes (data is assumed to be normally distributed) . As n grows however you will run into the problem of multiple comparisons. Is there any known bound on n?","Display_name":"Arun Jose","Creater_id":30417,"Start_date":"2016-08-01 23:13:42","Question_id":226804}
{"_id":{"$oid":"5837a587a05283111e4d65dd"},"Last_activity":"2016-08-03 00:44:05","Creator_reputation":1,"Question_score":0,"Answer_content":"You need to distinguish samples and features. A sample is one input sequence you pass to the LSTM model, e.g., your t_1. t_1 is a sequence with any length, so it may be a sequence of 1,2,3,4 and your LSTM model will give an ouput 5. Each individual element '1','2','3','4' of the input sequence is called features.Batch_size is used when training the model. You may want to train the model (i.e., updating the weights and biases) one sample by one sample. It is very slow and has some disadvantages. So you want to train multiple samples at one time. For example, if the batch_size equals to 3, then you will train t_1, t_2, t_3 as one batch before updating the model weights, t_4,t_5,t_6 at the same time, and so on.","Display_name":"sth","Creater_id":125467,"Start_date":"2016-08-03 00:44:05","Question_id":223420}
{"_id":{"$oid":"5837a587a05283111e4d65ea"},"Last_activity":"2016-08-01 18:07:19","Creator_reputation":106,"Question_score":1,"Answer_content":"There are two things at play here.  First off, there are five other variables, many of which appear to have strong effect sizes.  Since these variables aren't completely independent, a positive correlation does not necessarily imply a positive coefficient.  Second, you should look at the average of the log farm sizes rather than the log of the average farm sizes if you want to see which group is bigger in the eyes of your regression model.  ","Display_name":"Wart","Creater_id":125187,"Start_date":"2016-08-01 18:07:19","Question_id":226698}
{"_id":{"$oid":"5837a587a05283111e4d65eb"},"Last_activity":"2016-08-01 16:24:20","Creator_reputation":447,"Question_score":1,"Answer_content":"As others said its hard to interpret effect of one variable with others included. A model comparison approach may help. Also, you might consider standardizing your predictors. You might find that farm size is relatively weak, albeit in the 'wrong' direction. As well, if you have high inter correlations between features you can sometimes get these sorts of counterintuitive parameter estimates. Examination of the correlation matrix may help with that. ","Display_name":"HEITZ","Creater_id":86794,"Start_date":"2016-08-01 16:24:20","Question_id":226698}
{"_id":{"$oid":"5837a587a05283111e4d65f7"},"Last_activity":"2016-08-02 23:30:37","Creator_reputation":46,"Question_score":1,"Answer_content":"I think there are two ways of looking at this.You could an F test as you suggested on the two columns.  If youwant to do this using SPSS you would need to re-format the data sothat it looked like between-subjects data, and look at the result ofLevene's test (which relates to the difference in variances, nottheir proportion as your formula suggests). But I think what wouldbe closer to what you have in mind is to go back to the originaldata on which you calculated the mean for each participant in eachcondition.  Use the same figures to calculate a variance for eachparticipant in each condition (or an SD, it wouldn't matter much),then do a within-subjects Anova on those variances.","Display_name":"MikeG","Creater_id":125367,"Start_date":"2016-08-02 23:30:37","Question_id":226973}
{"_id":{"$oid":"5837a587a05283111e4d6608"},"Last_activity":"2016-03-22 15:27:22","Creator_reputation":2650,"Question_score":2,"Answer_content":"When you conduct VAR all variables should be on the same scale or same variable transformation basis (or as close as possible).  It makes perfect sense that when you multiply your original variables by a 100, the IRF graph also reflects responses that are 100 times greater than in the original.  The revised graph proportionally has not changed the response (visually the graphs will look identical).  You are just using a different scale (i.e. 1 instead of 1% or something similar).An IRF indicates what is the impact of an upward unanticipated one-unit change in the \"impulse\" variable on the \"response\" variable over the next several periods (typically 10).IRFs do not have coefficients.  The original regressions as you specified them have the coefficients.  The IRFs has three main outputs: the expected level of the shock in a given period surrounded by a 95% Confidence Interval (a low estimate and a high estimate).  And, all those also generate the IRF graphs.       ","Display_name":"Sympa","Creater_id":1329,"Start_date":"2016-03-22 15:27:22","Question_id":203122}
{"_id":{"$oid":"5837a587a05283111e4d661e"},"Last_activity":"2016-08-01 12:28:23","Creator_reputation":17464,"Question_score":3,"Answer_content":"The short answer is yes. You will need to go about this with a simulation study. The only wrinkle to it is that you will need to make specific assumptions about the error distribution, the nature of the dependence, heterogeneity or whatever violation may occur when conducting a power analysis. This limits what you can say about the power of the tests in general. When you actually go about collecting data, the whole concept of a \"data generating mechanism\" goes out the window. But simulating a variety of scenarios is useful for explaining a test's possible limitations (or lack thereof).It is a fault of classical statistics that assumptions are taught so dogmatically. Statistical tests may be well applied when assumptions have been violated. As an analyst, your responsibility is to report the findings from these tests, and discuss the possible limitations that may arise. As a statistician conducting a power analysis, your responsibility is to anticipate a variety of scenarios where assumptions are violated and make recommendations based on prior subject matter knowledge to recommend a test that's most general (not necessarily most powerful).When you set up a simulation experiment to demonstrate statistical power for incorrectly applied tests, it is usually useful to report the absolute relative efficiency (ARE) for the \"right\" statistical test. For instance, if model misspecification is happening, or heteroscedasticity, or distributional violations occur, there is a correct test that may be applied based on the data generating mechanism that you have set up. An ARE of 1 shows the user that the \"incorrect\" test is just as good as the perfect one. Many statisticians and researchers would prefer a more \"general\" test that can be applied in many situations to a \"maximum power\" test that fails absolutely when assumptions are violated. This is the statistical notion of risk. For instance, a Cox proportional hazards model may have AREs as low as 0.3 to parametric survival models, but it's ability to accommodate a wide variety of baseline survival functions is what's preferred by researchers and statisticians.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-01 12:28:23","Question_id":226736}
{"_id":{"$oid":"5837a587a05283111e4d662b"},"Last_activity":"2016-08-02 15:47:34","Creator_reputation":111,"Question_score":1,"Answer_content":"Yes, you can certainly do this with scikit-learn/python and pandas. This tutorial demonstrates how to cluster spatial data with scikit-learn's DBSCAN using the haversine metric, and discusses the benefits over k-means that you touched on in your question. Also, this example demonstrates applying the technique from that tutorial to cluster a dataset of millions of GPS points which provides a clear proof of concept for what you're attempting to do.A couple tips from those tutorials: use the haversine metric with the ball-tree algorithm. And pass radian units into the DBSCAN fit method.","Display_name":"eos","Creater_id":125431,"Start_date":"2016-08-02 15:47:34","Question_id":218530}
{"_id":{"$oid":"5837a587a05283111e4d6638"},"Last_activity":"2016-08-02 14:18:42","Creator_reputation":133,"Question_score":0,"Answer_content":"Type of model: since you are including variables from a survey (given out using a sampling technique - simple random sampling, stratified sampling, cluster, etc) you may want to use methods for survey-weighted regression (See Complex Surveys, A guide to analysis using R). It would be a linear model if the response variable is continuous; logistic if dichotomous/binary; multinomial/ordinal if there are multiple levels of the response variable. Multiple regression is simply a multivariate (more than one independent variable) regression model.Most medical studies include demographic variables (gender, race/ethnicity, age, etc) and socioeconomic status if available. As for block design, I'm not very familiar with that terminology but it seems like it is somewhat similar to controlling for confounding by including demographic/socioeconomic variables and other variables we not be interested in but could be responsible for the \"trend\" in the data.I hope this helps a little","Display_name":"godspeed","Creater_id":103245,"Start_date":"2016-08-02 14:18:42","Question_id":226951}
{"_id":{"$oid":"5837a587a05283111e4d6649"},"Last_activity":"2016-08-02 14:53:06","Creator_reputation":15561,"Question_score":1,"Answer_content":"This is a common mistake.Here is the right way to do it, all in one step (towards the end). I also show your method with my first stage to keep it apples to apples, as well as OLS. Note that I also added own effects for X2 and X3, since that is generally good practice.set more offwebuse nlswork, clear gen tenureXage = c.tenure#c.agegen tenureXhours = c.tenure#c.hours/* OLS */xtreg ln_w i.not_smsa c.tenure##(c.age c.hours), femargins, dydx(tenure) at(age=30 hour=40)/* Your Method With My First Stage */xtreg tenure i.not_smsa i.union##(c.age c.hours), fepredict that, xbuxtreg ln_w i.not_smsa c.that##(c.age c.hours), femargins, dydx(that) at(age=30 hour=40)/* My Method */xtivreg ln_w i.not_smsa c.age c.hours (tenure tenureXage tenureXhours = i.union##(c.age c.hours)), fe firstnlcom ME: _b[tenure] + _b[tenureXage]*30 + _b[tenureXhours]*40The last line gives you the marginal effect of endogenous tenure for a thirty-year-old who works 40 hours a week. Note how different it is from your approach and from OLS.","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-08-02 14:09:29","Question_id":226943}
{"_id":{"$oid":"5837a587a05283111e4d665e"},"Last_activity":"2016-08-02 12:47:09","Creator_reputation":111,"Question_score":1,"Answer_content":"I think you did well to challenge the notion that p-values and AIC values alone can determine the viability of a model.  I'm also glad you chose to share it here.As you've demonstrated, there are various trade-offs being made as you consider various terms and possibly their interaction.  So one question to have in mind is the purpose of the model.  If you're commissioned to determine the effect of location on y, then you should keep location in the model regardless of how weak the p-value is.  A null result is itself significant information in that case.At first glance, it seems clear the D location implies a larger y.  But there is only a narrow range of x for which you have both D and N values for location.  Regenerating your model coefficients for this small interval will likely yield a much larger standard error.But maybe you don't care about location beyond its capacity for predicting y.  It was data you just happened to have and color coding it on your plot revealed an interesting pattern.  In this case you may be more interested in the predictability of the model than the interpretability of your favorite coefficient.  I suspect AIC values are more useful in this case.  I'm not familiar with AIC yet; but I suspect it may be penalizing the mixed term because there is only a small range in which you can change location for fixed x.  There is very little that location explains that x doesn't already explain.","Display_name":"pglezen","Creater_id":109413,"Start_date":"2016-08-02 12:47:09","Question_id":218667}
{"_id":{"$oid":"5837a587a05283111e4d665f"},"Last_activity":"2016-08-02 11:01:58","Creator_reputation":46,"Question_score":0,"Answer_content":"You must report both groups separately (or perhaps consider multi-level modelling).  To simply combine the groups violates one of the basic assumptions of regression (and most other inferential statistical techiques), independence of observations.  Or to put it another way, the grouping variable (location) is a hidden variable unless it is taken into account in your analysis.  In an extreme case, ignoring a grouping variable can lead to Simpson's paradox.  In this paradox, you can have two groups in both of which there is a positive correlation, but if you combine them you have a (false, incorrect) negative correlation.  (Or vice versa, of course.)  See http://www.theregister.co.uk/2014/05/28/theorums_3_simpson/.","Display_name":"MikeG","Creater_id":125367,"Start_date":"2016-08-02 11:01:58","Question_id":218667}
{"_id":{"$oid":"5837a587a05283111e4d6660"},"Last_activity":"2016-06-30 11:15:12","Creator_reputation":41,"Question_score":1,"Answer_content":"Did you try using both predictors without the interaction? So it would be:y ~ x + LocThe AIC might be better in the first model because location is important. But the interaction is not important, which is why the P-values are not significant. You would then interpret it as the effect of x after controlling for Loc.","Display_name":"AJ12","Creater_id":121606,"Start_date":"2016-06-30 11:15:12","Question_id":218667}
{"_id":{"$oid":"5837a587a05283111e4d666d"},"Last_activity":"2016-08-02 11:12:47","Creator_reputation":109,"Question_score":1,"Answer_content":"I assume that you have raised this question in context of frequency argument while modeling the time series using some tool, say R.The interpretation of frequency for time series packages is generally 'the number of observations in a series if you consider the natural time interval of measurement'. For example, if you measure value of some variable once in a month, and you have data for multiple years, you can use value of 12 for frequency.But things get tricky where there could be multiple levels of seasonality. For example, if you measure number of visitors to a web page every hour, there will be seasonality by hour as well as by day.Bottom line is, very hard to tell you one single number purely based on information that you have given. You can study Rob J. Hyndman's blog post \"Seasonal periods\" for more details.","Display_name":"hssay","Creater_id":26218,"Start_date":"2016-08-02 03:05:27","Question_id":226810}
{"_id":{"$oid":"5837a587a05283111e4d667a"},"Last_activity":"2016-08-02 12:27:26","Creator_reputation":166,"Question_score":3,"Answer_content":"A really nice way to keep your results organized is to use rmarkdown.  It interweaves human language and presentation with the r programming language.  So at the end you have a presentable document that runs and presents the results of r code along the way.It a combination of markdown and r code so that you both have runnable code and a structure to present the results of your code.  Markdown is simple syntax that allows you to build simple webpages (or other kinds of documents) from text.  Markdown is to html like latex is to pdf. So rmarkdown just adds the functionality of displaying the results of r code as well in the flow of the document.I usually don't write the final paper in rmarkdown but it is nice starting place for how I want to present the experimental section or if I want to highlight certain results on my webpage.Here are some resources: rmarkdown cheat sheetofficial websitelong rmarkdown example","Display_name":"AWashburn","Creater_id":112626,"Start_date":"2016-08-02 12:27:26","Question_id":226937}
{"_id":{"$oid":"5837a587a05283111e4d6687"},"Last_activity":"2016-08-02 12:12:39","Creator_reputation":136,"Question_score":0,"Answer_content":"If there was no random error for the angle measurements then a linear regression model would be the way to go.  For example, in R:\u0026gt; n \u0026lt;- 50\u0026gt; alpha \u0026lt;- runif(n, -pi/4, pi/4)\u0026gt; tau \u0026lt;- matrix(c(sin(alpha), cos(alpha)), nrow=n, ncol=2,+     dimnames=list(NULL, c(\"sin(alpha)\", \"cos(alpha)\")))\u0026gt; x \u0026lt;- c(1, 2)\u0026gt; y \u0026lt;- (tau %*% x) + rnorm(n)\u0026gt; angle_dat \u0026lt;- data.frame(tau, y)\u0026gt; lm(y ~ 0 + sin(alpha) + cos(alpha), angle_dat)Call:lm(formula = y ~ 0 + sin(alpha) + cos(alpha), data = angle_dat)Coefficients:sin(alpha)  cos(alpha)      0.8036      1.9271However, since you have random error in the observed values of the  things are far more complicated.  This type of model is known as a measurement error model.  A good introduction / survey of these types of models can be found at Measurement Error in Epidemiological Studies.  I'd imagine that you can find some direction there or by following the many references at the end of the document.","Display_name":"dpritch","Creater_id":93528,"Start_date":"2016-08-01 21:02:06","Question_id":226546}
{"_id":{"$oid":"5837a587a05283111e4d6694"},"Last_activity":"2016-08-02 12:10:49","Creator_reputation":46,"Question_score":1,"Answer_content":"As a previous reply mentioned, yes it is and the technical description is at SPSS's support page: https://www-304.ibm.com/support/docview.wss?uid=swg21477269This is a useful statistic for those who understand it.  Suppose we investigate whether 78 employees' promotion (yes/no) is related to their performance ranking in the previous year (1-4, 1=low), as follows:Ranking 1: Not promoted 17, Promoted 2, Total 19. Ranking 2: Not promoted 16, Promoted 4, Total 20.Ranking 3: Not promoted 14, Promoted 6, Total 20.Ranking 4: Not promoted 10, Promoted 9, Total 19.SPSS shows a significant linear-by-linear association (p=.008) showing that there is a significant association between the ranking and being promoted.  Some useful details of how this works are:1.  The test relates to the odds.  Odds are used for their statistical properties, and are not quite the same as probabilities.  For ranking 1, the odds of being promoted are 2:17, as opposed to the probability which is 2:19.2.  Then, the test is on the odds ratios; e.g. if you move from rank 1 to rank 2, the odds ratio is 4:16/2:17 = 0.250/0.118 = 2.12.  (The null hypothesis is that the odds ratio is 1, i.e. a change in ranks makes no difference to the odds.)3.  The procedure presumes that the odds ratios (in the population) are the same for all steps (i.e. if moving from rank 1 to rank 2 doubles the odds of promotion, moving from rank 2 to rank 3 would also double the odds of promotion).  That is why there is only 1 degree of freedom.  (This assumption is known as \"linearity in the logit\".)4.  The test is therefore conceptually the same (and gives a similar answer) to doing logistic regression with just one covariate. (In logistic regression, \"covariate\" means a variable like this one).  In this case the covariate would be ranking, and the DV would be promotion decision.","Display_name":"MikeG","Creater_id":125367,"Start_date":"2016-08-02 07:24:46","Question_id":184918}
{"_id":{"$oid":"5837a587a05283111e4d6695"},"Last_activity":"2015-12-04 04:52:01","Creator_reputation":74,"Question_score":4,"Answer_content":"It is. You might have a look at IBM's support page for SPSS,where it is stated in a technote on the Chi² test:'The Crosstabs procedure includes the Mantel-Haenszel test of trend among its chi-square test statistics. ... The MH test for trend will be printed in the \"Chi-Square Tests\" table and labelled \"Linear-by-Linear Association\".'see: https://www-304.ibm.com/support/docview.wss?uid=swg21477269","Display_name":"jf1","Creater_id":97050,"Start_date":"2015-12-03 18:33:21","Question_id":184918}
{"_id":{"$oid":"5837a587a05283111e4d66a2"},"Last_activity":"2016-08-02 12:04:42","Creator_reputation":3645,"Question_score":0,"Answer_content":"Structural zeros or voids are special cases in the analysis of contingency tables. These are vacancies in cell structure that, as noted by the OP, represent theoretically impossible combinations. If one treats the impossible cells as observed zero values, they distort any test of independence. Tables with these values have an incomplete factorial design requiring different treatment. This usually involves excluding or ignoring these cells when rolling up the chi-square values in a test of quasi-independence. Note that this is the same model as in the complete table -- just with certain cells excluded.Good discussions of these issues abound in the contingency table modeling literature. My favorite citation for it is chapter 10 of Wickens Multiway Contingency Table Analysis for the Social Sciences.","Display_name":"DJohnson","Creater_id":82102,"Start_date":"2016-08-02 12:04:42","Question_id":226918}
{"_id":{"$oid":"5837a587a05283111e4d66a3"},"Last_activity":"2016-08-02 11:42:16","Creator_reputation":1557,"Question_score":0,"Answer_content":"If you want to execute a chi-square test, you must meet the assumptions which will include independence of observations and an expected count of at least 5 in each cell. Note that the observed count can be less than 5 as long as the expected count is at least 5. If the expected count in one or more cells are less than 5, then you will want to collapse cells - for example, collapse the age categories 18-23 and 23-28 into one 18-28 category or collapse the experience categories 5-7 and 7+ into one 5+ category. If you do not meet these assumptions and you still use a chi-square test, then you are not losing details from your data but you are using a test where all of the assumptions have not been met and your result (whether you reject or fail to reject) will be unreliable!If you do not want to lose the details there, it is possible to execute Fisher's exact test. Fisher's exact test will calculate an exact -value from your data rather than calculating an approximate -value that relies on the assumptions of the chi-square test being met. This exact -value will allow you to evaluate whether or not salary has an association with age or education or experience. It is important to note that Fisher's exact test, like a chi-squared test, will only check for associations between two variables and cannot check for associations among more than two variables.With respect to log-linear models, the Wikipedia page for log-linear models has the following suggestions: \"If both (a) the expected frequencies are greater than or equal to 5 for 80% or more of the categories and (b) all expected frequencies are greater than 1 [then using a log-linear model is appropriate.]... Suggested solutions [if either or both of these assumptions are violated] are: delete a variable, combine levels of one variable (e.g., put males and females together), or collect more data.\"Logistic regression would be inappropriate here, because the term \"logistic regression\" as it is most frequently used only applies to dependent variables that are binary, whereas salary (as you specified it) is a categorical outcome.I would either recommend using \"ordinal logistic regression\" to indicate that there are multiple ordered categories of salary you seek to predict or using linear regression and predicting salary directly (instead of multiple categories). If you have the raw salary data, then I strongly recommend using that as your dependent variable.","Display_name":"Matt Brems","Creater_id":92737,"Start_date":"2016-08-02 11:42:16","Question_id":226918}
{"_id":{"$oid":"5837a587a05283111e4d66b0"},"Last_activity":"2016-08-02 11:50:04","Creator_reputation":46,"Question_score":3,"Answer_content":"I think it's hard to beat Cohen's classic 1994 paper in the American Psychologist, \"The earth is round, (p\u0026lt;.05)\". You can always find it somewhere on Google, e.g. http://ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf.  He includes a brilliant example based on a medical screening test, and discusses the implications for interpreting p-values.","Display_name":"MikeG","Creater_id":125367,"Start_date":"2016-08-02 11:50:04","Question_id":131322}
{"_id":{"$oid":"5837a587a05283111e4d66bd"},"Last_activity":"2016-08-02 11:46:05","Creator_reputation":15561,"Question_score":3,"Answer_content":"I think SUTVA violations come in two flavors, which are not always distinct:\"spillovers/interferences\" that arise from contact across individuals in social, commodity, or physical space (independence flavor)dilution/concentration of treatment effects that stem from changes in the prevalence of treatment (what economists call general equilibrium effects or failure of the ceteris paribus assumption flavor)Consider a job training program that teaches a handful of people how to knit and sell their output on Etsy (a small program in a large market). If you have treated trainees that teach control group people how to crochet, or more knitting takes place when you treat groups of friends (knitting is often a social activity), you have an example of (1). Two real world examples of this are patients in early AIDS drug trials sharing their medication or irrigation/rain causing fertilizer runoff from treated to control plots. If you a have a mandatory job training program that teaches knitting and selling at a local farmer's market (large program in a small market), you might expect the prices of scarves and socks to plummet, with the pecuniary benefit of the knitting knowledge declining with the number of people treated. A real world example is the effect of charter schools on academic achievement, which might change if you had a large influx of public school students into the other sector, or a program that teaches farmers to all grow a particular type of crop. You can think of this dilution as either a dosage change or as a kind of treatment effect change.I think it is frequently hard to make these two fully distinct, and (2) often operates through (1)-type channel: the inrush of public school students is only problematic because of rival resource constraints or peer effects. However, (2) is more subtle than spillover/interference, so I think it does go \"beyond independence\" in some sense.I think (1) is often more harmful, since it undermines internal validity of an estimate, though we can sometimes redefine the unit of analysis to be the community within which individuals interact rather than the individuals themselves.I think of (2) as circumscribing the external validity, since when trials are small, we can think of the estimated partial equilibrium effects as a kind of bound on the general equilibrium effects that would be seen if the program was scaled up and prices and inputs or \"dosage\" change. This limits what you can claim, but if the costs of the small trial program already exceed the benefits, and we anticipate the benefits to decline if the program is scaled up, that is still useful information. Alternatively, SUTVA may only hold for some part of our data, and the analysis can proceed once the rest is discarded. This makes (2) less pernicious.Here's a slightly more rigorous way to think about this. We can write the treatment effect for person  as a function of the  indicator vector  that gives you treatment assignments in the remaining population: \\Delta_i(\\mathbf{t})=y^1_i(\\mathbf{t})-y^0_i(\\mathbf{t})We can think about how  varies as we change  in particular ways.Let , the  norm of the treatment assignment vector. This tells you how many people got treated in a particular treatment configuration. If  depends on where the ones are in , holding  fixed, you have SUTVA violation of type (1). This means it matters whether people \"connected\" to person  are treated or not, a kind of dependence.If  only changes with , but is the same for all pairs  and  where  you have a type 2 violation.If SUTVA is fully satisfied since the potential outcomes does not depend on how the treatment is rolled out.To summarize all this, there are two types of SUTVA violations which are not fully conceptually distinct, but have differing implications, which makes it useful to emphasize their differences.      ","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-06 15:51:23","Question_id":221939}
{"_id":{"$oid":"5837a587a05283111e4d66ce"},"Last_activity":"2016-08-02 11:32:21","Creator_reputation":46,"Question_score":0,"Answer_content":"Here's a simple suggestion.  I don't know whether it works for you and maybe I should have made it as a comment, but it seems you need more privileges to make a comment than to make a reply.If I understand correctly, the figures you are using are the amounts of storage you are using each month.  Probably these usualy increase, and you want to predict what the amount will be at some time in the future if trends continue.  Once you realise that your big change has happened (e.g. that 500 GB has been released) can you go back and change the previous months' figures (e.g. delete 500 GB from all of them)?  Basically what you would be doing is to adjust the previous months' figures to what they should have been, if you knew then what you know now.Of course I don't recommend this unless you make sure you can go back to the old figures.  But the forecasting you want to do sounds like it could even be done in Excel, in which case you can have as many versions as you want.","Display_name":"MikeG","Creater_id":125367,"Start_date":"2016-08-02 11:32:21","Question_id":195124}
{"_id":{"$oid":"5837a587a05283111e4d66df"},"Last_activity":"2016-08-02 11:06:46","Creator_reputation":26,"Question_score":1,"Answer_content":"I don't know if you still need the answer for this, but I'll try anyway.The ICC for a two level negative binomial model (Tseloni and Pease, 2003) can be easily calculated by:\\rho = \\frac{\\sigma_{j}^2}{\\sigma_{j}^2 + \\alpha}where  is the variance of between-group differences (level 2), and  is the variance at level 1, though the parameter reported by lme4 as the overdispersion parameter () is .So, in a three level model, there are two intra-class correlations that can be calculated: individuals within level-2 groups, and level-2 groups within level-3 groups.Using a hypothetical example, if we had pupils nested in classes () nested in schools (), the formulas for the ICC are:\\rho_{class} = \\frac{\\sigma_{j}^2 + \\sigma_{k}^2}{\\sigma_{j}^2 +\\sigma_k^2 + \\alpha}\\rho_{school} = \\frac{\\sigma_{k}^2}{\\sigma_{j}^2 +\\sigma_k^2 + \\alpha}where  is the between class variance,  is the between school variance, and  is the between pupil variance.  would be the correlation between two pupils in the same class, and  the correlation between two classes in the same school.Now, to calculate these ICCs using lme4 we need to access the specific estimates stored in the glmerMod object generated by glmer.nb.So assuming you have a three level model named mclass(m)   ## \"glmerMod\"### store the intercepts variance, which### frustratingly, is also called theta in lme4 models### and it's stored as the sd, hence the need to square itvar_k \u0026lt;- as.numeric(getME(m, \"theta\")[2]^2) # level 3 variancevar_j \u0026lt;- as.numeric(getME(m, \"theta\")[1]^2) # level 2 variance### store the alpha value (which lme4 stores as theta = 1/alpha)alpha \u0026lt;- 1/getME(m, \"glmer.nb.theta\")### ICC for level 2ICC_l2 \u0026lt;- (var_k + var_j)/(var_k + var_j + alpha) ### ICC for level 3ICC_l3 \u0026lt;- var_k/(var_k + var_j + alpha) ReferencesTseloni, A., \u0026amp; Pease, K. (2003). Repeat personal victimization. ‘Boosts’ or ‘flags’? British Journal of Criminology, 43(1), 196-212. doi:10.1093/bjc/43.1.19","Display_name":"prestevez","Creater_id":124790,"Start_date":"2016-07-27 17:49:31","Question_id":174071}
{"_id":{"$oid":"5837a587a05283111e4d66ec"},"Last_activity":"2015-04-14 23:46:29","Creator_reputation":24991,"Question_score":2,"Answer_content":"There is a good reason for that. Within model for probit regression suffers from incidental parameters problem. Within model for logit regression can be estimated, but requires quite strong assumptions. This is discussed in J. Wooldridge's \"Econometric analysis of cross-section and panel data\", chapter 15. If you look at the code for pglm, you can see that starting values are calculated with function starting.values. For family binomial the code calculates starting values only for model random and pooling, there is no variant for within. Hence the error. If you supply the starting values, the error is given in the function lnl.binomial. Looking at the code it is clear that model within is not supported.The author of package pglm could add explicit error message for the case of within model. I would advise you to write to him.","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2015-04-14 23:46:29","Question_id":146434}
{"_id":{"$oid":"5837a587a05283111e4d66f9"},"Last_activity":"2016-08-02 10:54:55","Creator_reputation":3832,"Question_score":6,"Answer_content":"If the vectors are orthogonal, you can just take the variance of the scalar projection of the data onto each vector. Say we have a data matrix  ( points x  dimensions), and a set of orthonormal column vectors . Assume the data are centered. The variance of the data along the direction of each vector  is given by .If there are as many vectors as original dimensions (), the sum of the variances of the projections will equal the sum of the variances along the original dimensions. But, if there are fewer vectors than original dimensions (), the sum of variances will generally be less than for PCA. One way to think of PCA is that it maximizes this very quantity (subject to the constraint that the vectors are orthogonal).You may also want to calculate  (the fraction of variance explained), which is often used to measure how well a given number of PCA dimensions represent the data. Let  represent the sum of the variances along each original dimension of the data. Then:R^2 = \\frac{1}{S}\\sum_{i=1}^{k} \\text{Var}(X v_i)This is just the ratio of the summed variances of the projections and the summed variances along the original dimensions.Another way to think about  is that it measures the goodness of fit if we try to reconstruct the data from the projections. It then takes the familiar form used for other models (e.g. regression). Say the th data point is a row vector . Store each of the basis vectors along the columns of matrix . The projection of the th data point onto all vectors in  is given by . When there are fewer vectors than original dimensions (), we can think of this as mapping the data linearly into a space with reduced dimensionality. We can approximately reconstruct the data point from the low dimensional representation by mapping back into the original data space: . The mean squared reconstruction error is the mean squared Euclidean distance between each original data point and its reconstruction:E = \\frac{1}{n} \\|x_{(i)} - \\hat{x}_{(i)}\\|^2The goodness of fit  is defined the same way as for other models (i.e. as one minus the fraction of unexplained variance). Given the mean squared error of the model () and the total variance of the modeled quantity (), . In the context of our data reconstruction, the mean squared error is  (the reconstruction error). The total variance is  (the sum of variances along each dimension of the data). So:R^2 = 1 - \\frac{E}{S} is also equal to the mean squared Euclidean distance from each data point to the mean of all data points, so we can also think of  as comparing the reconstruction error to that of the 'worst-case model' that always returns the mean as the reconstruction.The two expressions for  are equivalent. As above, if there are as many vectors as original dimensions () then  will be one. But, if ,  will generally be less than for PCA. Another way to think about PCA is that it minimizes the squared reconstruction error.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-08-02 10:54:55","Question_id":226905}
{"_id":{"$oid":"5837a587a05283111e4d6706"},"Last_activity":"2016-08-02 10:46:46","Creator_reputation":13091,"Question_score":0,"Answer_content":"The (regular) residuals are , i.e. the fitted values of .The standardized residuals are , i.e. the fitted values of .The model assumes that the standardized errors have a certain distribution (e.g. Normal, Student- or the like) with zero mean and unit variance. The likelihood function for the model is built using this assumption. If the assumption does not hold, the likelihood function is misspecified and the maximum likelihood estimator (MLE) might not have the desirable properties (although it still might work alright as quasi MLE in some cases). Therefore, you check the empirical counterpart of the standardized (rather than regular) errors which is the standardized residuals.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-02 10:40:50","Question_id":226883}
{"_id":{"$oid":"5837a587a05283111e4d6715"},"Last_activity":"2016-08-02 10:09:52","Creator_reputation":30065,"Question_score":5,"Answer_content":" can be interpreted as multivariate signal-to-noise ratio.The between-class scatter matrix  tells us how far from each other class means are located. The within-class scatter matrix  tells us how much variability is inside each class. If classes correspond to the \"signal\" and within-class variability can be seen as noise, then  can be interpreted as multivariate signal-to-noise ratio.If the classes are well separated, then signal-to-noise ratio should be \"large\" (). If they are completely overlapping, then the signal-to-noise ratio should be \"small\" (). The problem is that  is not a number but a matrix; so what does \"large\" and \"small\" really mean?There are several reasonable ways to quantify how \"big\"  is. One way is to add up its eigenvalues, i.e. to compute the trace. As @ttnphns mentioned, this is called Hotelling's trace and is used as one of the test statistics in MANOVA. So the interpretation is that it is one possible way to quantify the signal-to-noise ratio .In turn, the eigenvectors of  represent the directions in space along which the class discriminability is the highest. The eigenvector corresponding to the largest eigenvalue is the axis of the best class separation. In linear discriminant analysis (LDA) the eigenvectors of  are called \"discriminant axes\".Once the data are projected on the -th discriminant axis, the standard univariate signal-to-noise ratio defined as between-class sum of squares divided by within-class sum of squares, will equal . This explains the quote from Duda.Further reading:Algebra of LDA. Fisher discrimination power of a variable and Linear Discriminant AnalysisHow is MANOVA related to LDA?","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-02-29 17:21:59","Question_id":199182}
{"_id":{"$oid":"5837a587a05283111e4d6726"},"Last_activity":"2016-08-02 09:52:40","Creator_reputation":9989,"Question_score":3,"Answer_content":"So, by @whuber's suggestion:E(\\text{number of attempts for }N\\text{ pairs})=\\sum_{j=1}^NE(\\text{number of attempts for }j\\text{th pair})The first pair has, as mentioned in the question, a success probability of  in any given attempt, and follows a geometric distribution. The expectation therefore isE(\\text{number of attempts for }1\\text{st pair})=2N-1Likewise,E(\\text{number of attempts for }2\\text{nd pair})=2N-2-1and in general\\sum_{j=1}^NE(\\text{number of attempts for }j\\text{th pair})=\\sum_{j=1}^N2N-2(j-1)-1which indeed simplifies very nicely to\\sum_{j=1}^N2N-2(j-1)-1=2N^2+N-2N(N+1)/2=N^2","Display_name":"Christoph Hanck","Creater_id":67799,"Start_date":"2016-08-02 09:52:40","Question_id":226890}
{"_id":{"$oid":"5837a587a05283111e4d6733"},"Last_activity":"2016-08-02 09:20:48","Creator_reputation":8055,"Question_score":1,"Answer_content":"Some ideas when facing such a situation:it might happen if you have a very low number of labeled samplestry training the same neural network several times on the same train/test split, to make sure that the variability of the performance you observed isn't due to the random initialization of the weight, or random creation of the mini-batches.the discussion should mention that the results are heavily impacted by the train/test splittry spotting which samples result in a significant performance difference when included in the training set","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-02 09:20:48","Question_id":220485}
{"_id":{"$oid":"5837a587a05283111e4d6740"},"Last_activity":"2016-08-02 08:59:04","Creator_reputation":2186,"Question_score":1,"Answer_content":"With only having few samples in the training set it is easier for the model to find a fit/representation of the data with few error - so the resulting training error will usually be lower than with using more samples. In this state, the training samples under-represent the complexity of the real data, hence the resulting fit will usually not generalize well. In contrast, if you increase the amount of samples, finding a correct fit/representation of the training samples will usually become harder, thereby increasing the training error. But this also leads to a better generalization, as the representation becomes more realistic (thereby reducing the test error). If you try different training set sizes and compare the corresponding training and test error, you will usually end up with a curve similar to this one:In the figure, training error is purple and evaluation error (denoted CV) is pink.The same mechanism applies to CV and seems to be used for determining the  in the example you mentioned in your question. In case 50% of data are enough to achieve a suitable fit, 2-fold CV would be enough. In contrast, 90% training data would require a 10-fold CV instead. You would thereby see the training error cure flattening out the same way as in the figure above.How to obtain such a plot (slow, stable):Use different  and preserve both the corresponding  associated training and  test errors. Plot the average/median/etc. training and test error over .What I assume the authors suggested (much faster, but less stable):Do a simple train/test split of your data, fit your model and preserve the training and test error. At first, do point 1 with a small portion of data in your training partition, then increase the training partition continuously. Preserve all associated training and test errors and plot them over the training partition size. As soon as the curve flattens out (will usually not be easy to determine) you know how big the portion of the data to use for training your model will be, and can therefore derive the required amount of partitions (e.g. using 80% of data in the training partition would correspond to having a 5-fold CV). (PS: I'm sure that there are better figures than the one I just used, right here on CV, so anyone knowing such: feel free to replace it!)","Display_name":"geekoverdose","Creater_id":112731,"Start_date":"2016-08-02 08:34:49","Question_id":226894}
{"_id":{"$oid":"5837a587a05283111e4d674d"},"Last_activity":"2016-08-02 08:49:45","Creator_reputation":51,"Question_score":1,"Answer_content":"You are using a confusing terminology. Alotugh there are several way to express the same concept, note that one observation is one instance. One instance can have multiple characteristics/features/columns/variables/...Now for what it concerns computing the average of classes, the mathematical formula would be:\\begin{equation*}\\mu_i = \\frac{1}{N_i} \\sum_{x \\in \\omega_i} x\\end{equation*}for the class average and for the global average:\\begin{equation*}\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i = \\frac{1}{n} \\sum_{i=1}^C N_i \\mu_i\\end{equation*}Where  denotes a class label,  are the class sizes and  the total number of samples. In your JAVA related situation, if you have 10 classes but only 3 instances per class, expect poor results. Anyway, class average for class 1 would be computed as (if your columns are the features and the rows the observations) as: avg = [(a1+b1+c1/3),(a2+b2+c2/3),(a3+b3+c3/3)]And that would be a three dimensional vector, not a single scalar. As you are working with multi dimensional data you cannot expect that the mean is a single number only.","Display_name":"Renthal","Creater_id":125326,"Start_date":"2016-08-02 08:38:44","Question_id":91955}
{"_id":{"$oid":"5837a587a05283111e4d675c"},"Last_activity":"2016-08-02 08:19:54","Creator_reputation":51,"Question_score":0,"Answer_content":"LDA seeks to reduce dimensionality while preserving as much of the class discriminatory information as possible. Assume we have a set of -dimensional observations , belonging to  different classes. The goal of LDA is to find an linear transformation (projection) matrix  that converts the set of labelled observations  into another coordinate system  such that the class separability is maximized. The dataset is transformed into the new subspace as:\\begin{equation}Y = XL\\end{equation}The columns of the matrix  are a subset of the  largest (non-orthogonal) eigenvectors of the  squared matrix , obtained as:\\begin{equation}J = S_{W}^{-1} S_B\\end{equation}where  and  are the scatter matrices within-class and respectively between-classes. When it comes to dimension reduction in LDA, if some eigenvalues have a significantly bigger magnitude than others then we might be interested in keeping only those dimensions, since they contain more information about our data distribution. This becomes particularly interesting as  is the sum of  matrices of rank , and the mean vectors are constrained by  \\cite{c.radhakrishnarao1948}. Therefore,  will be of rank  or less, meaning that there are only  eigenvalues that will be non-zero (more info here). For this reason, even if the dimensionality  of the sub-space  can be arbitrarily chosen, it does not make any sense to keep more than  dimensions, as they will not carry any useful information.  In fact, in \\ac{lda} the smallest  dimensions have magnitude zero, and therefore the subspace  should have exactly  dimensions.","Display_name":"Renthal","Creater_id":125326,"Start_date":"2016-08-02 08:19:54","Question_id":167690}
{"_id":{"$oid":"5837a587a05283111e4d675d"},"Last_activity":"2015-08-18 09:09:27","Creator_reputation":15629,"Question_score":0,"Answer_content":"LDA project to (at most)  dimensions, so binary (2-class) LDA reduces to 1D (= onto line).10 classes would lead to a 9D projection (as long as X is at least 9D, of course).      does LDA map a full n-dimensional vector onto a scalar (a singe number)?  Not always, see above.For more details on what the projection step does, see e.g.  http://stats.stackexchange.com/a/87509/4598(Obviously, if you code your classes as numbers then the final class prediction will be a single number)","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2015-08-18 09:09:27","Question_id":167690}
{"_id":{"$oid":"5837a587a05283111e4d6768"},"Last_activity":"2016-08-02 08:12:17","Creator_reputation":5445,"Question_score":1,"Answer_content":"The key concept with repeated measures is that there is clustering. This is why mixed effects models are often used for repeated measures - because they specifically handle clustered data. These models take account of non-independence of data within each cluster. ie, observations in one cluster will be more similar to each other than to observations in another cluster.To fit a mixed effects model for repeated measures, you need to specify the grouping variable (that is, what defines the cluster), and fit random intercepts for it. In your case you have observations nested in transects which are in turn nested within habitat. However you only have 2 habitats, therefore it is not a good idea to specify this as a random effect. You can specify it as a fixed effect instead.If you also specify Times as a random intercept, you are saying that observations are also nested within each occasion of measurement. This may or may not be appropriate. If there is some reason that observations on different transects at the same measurement occasion should be more alike one another than observations on the same transects but at a different time, and you are not interested in fixed effect of measurement occasion, then you can include this term as a random effect in the model. So, the key point is that in your case you need to fit a model with transect as a random intercept. In R, a better starting model is:glmer(Abundance ~ Airtemp * Sunshine + Habitat + (1|transect) + (1|Times),           family = poisson, data = bees)","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-02 08:12:17","Question_id":226875}
{"_id":{"$oid":"5837a587a05283111e4d6774"},"Last_activity":"2016-08-02 08:08:32","Creator_reputation":147763,"Question_score":6,"Answer_content":"This question comes up a lot in various guises.  What is common to them is  How can I combine moment-based statistics that have been computed from disjoint subsets of my data?The simplest application concerns data that have been split into two groups.  You know the group sizes and the group means.  In terms of these four quantities alone, what is the overall mean of the data?Other applications generalize from means to variances, standard deviations, covariance matrices, skewnesses, and multivariate statistics; and might involve multiple subgroups of data.  Notice that many of these quantities are somewhat complicated combinations of moments: the standard deviation, for instance, is the square root of a quadratic combination of the first and second moments (mean and mean square).All such cases are easily handled by reducing the various moments to sums, because sums are obviously and easily combined: they are added.  Mathematically, it comes down to this: you have a batch of data  that have been separated into disjoint groups of sizes : .  Let's call the th group .  By definition, the th moment of any batch of data  is the average of th powers,\\mu_k(y) = \\left(y_1^k + y_2^k + \\cdots + y_j^k\\right)/j.Obviously  is the sum of the th powers.  Therefore, referring to our previous decomposition of data into  subgroups, we can break a sum of  powers into groups of sums, obtaining\\eqalign{n \\mu_k(X) \u0026amp;= \\left(x_1^k + x_2^k + \\cdots + x_n^k\\right) \\\\\u0026amp;= \\left(x_1^k + x_2^k + \\cdots + x_{j_1}^k\\right) + \\cdots + \\left(x_{j_1+\\cdots+j_{g-1}+1}^k + x_{j_1+\\cdots+j_{g-1}+2}^k + \\cdots + x_n^k\\right)\\\\\u0026amp;= j_1 \\mu_k(X_{(1)}) + j_2 \\mu_k(X_{(2)}) + \\cdots + j_g \\mu_k(X_{(g)}).}Dividing by  exhibits the th moment of the entire batch in terms of the th moments of its subgroups.In the present application, the entries in the covariance matrix are, of course, covariances, which are expressible in terms of multivariate second moments and first moments.  The key part of the calculation comes down to this: at each step you will have focused on two particular components of your multivariate data; let's call them  and .  The numbers you are looking at are in the form((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)),broken up as before into  groups.  For each group you know the average sum of products of the : this is the  multivariate moment, .  To combine these group values, you will multiply them by the group sizes, add up those results, and divide the total by .To apply this approach you need to think ahead: it is not possible to combine, say, covariances if you know only the covariances and the subgroup sizes: you also need to know the means of the subgroups (because means are involved in an essential way in all covariance formulas), or something algebraically reducible to the means.  You also might need to take some care concerning any constants that appear in the formulas; the chief trap for the unwary is to confuse a \"sample covariance\" (which involves a sum of products divided by ) with a \"population covariance\" (where the division is by ).  This does not introduce anything new; you just have to remember to multiply the sample covariance by  (or group covariance by ) to recover the sum, rather than by  (or ).Oh, yes: about the present question.  The formula given in the Wikipedia article is given in terms of group means (first moments) and the group sums of products.  As I described above, these will get combined by adding them and then adjusting the results with a division to obtain the covariances.  The final division by  is not shown.","Display_name":"whuber","Creater_id":919,"Start_date":"2013-03-11 10:44:14","Question_id":51622}
{"_id":{"$oid":"5837a587a05283111e4d678b"},"Last_activity":"2016-08-01 07:11:51","Creator_reputation":147763,"Question_score":9,"Answer_content":"This question has a unique answer: anything else will only be an approximation or will be based on an inferior hypothesis test.  The p-value is .  Its calculation is based on sampling without replacement from a population of .  The rest of this post provides the reasoning, which relies only on the definition of p-value and some straightforward combinatorial calculations.A probability model for your results can be described by a box with  tickets, one per person.  On each ticket is written whether that person went to the show.  Your random sample is like taking eight of those tickets out of the box (without replacing them).The statistical characteristics of this model which are yet unknown are completely determined by the number of people who went to this show.  Call this number .  The possible values of  are the whole numbers from zero through .Your null hypothesis, , is that five or more people went to the show: .  The alternative is that .To test this hypothesis, the only useful statistic is the count  of the people in your sample who went to the show.  (Counting those who did not go will give mathematically equivalent information, obviously.)  Evidently small values of  are evidence against  and large values are evidence for it.  In fact, if , you would be certain that  is true, because at least five people in your sample went.The p-value therefore is computed from the chance that  could have been less than or equal to the value you observed, which was .  This chance can easily be computed by breaking it into three mutually exclusive possibilities: means the entire sample of  tickets came from the  non-show-going tickets in the box.  There are  ways that could happen. means seven of the sample tickets came from the  non-show-going tickets (there are  ways for that to happen) and one came from the  show-going tickets: there are  ways for that to happen, independently of the choice of the other seven tickets.  The total number of such samples therefore is .An analogous argument shows there are  samples with  show-going tickets.Add these three values up and divide by the total number of possible (and equiprobable) samples, , to obtain the chances of  in terms of the unknown .  Although as it turns out we only need to perform this calculation for , here are the chances for some of the other values of  so you can appreciate the patterns:\\begin{array}{rr|cccccc} \u0026amp;\\theta \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; \\color{Red} 5 \u0026amp; \\color{Red} 6 \u0026amp; \\color{Red} \\cdots \u0026amp; \\color{Red} {14} \\\\ \u0026amp;\\text{Probability} \u0026amp; 1 \u0026amp; \\frac{271}{285} \u0026amp; \\frac{4103}{4845} \u0026amp; \\color{Red}{\\frac{682}{969}} \u0026amp; \\color{Red}{\\frac{176}{323}} \u0026amp;  \\color{Red}\\cdots \u0026amp; \\color{Red}{\\frac{7}{9690}} \\\\\u0026amp;\\text{(in decimals)}  \u0026amp; 1. \u0026amp; 0.951 \u0026amp; 0.847 \u0026amp; \\color{Red} {0.704} \u0026amp; \\color{Red} {0.545} \u0026amp; \\color{Red}\\cdots \u0026amp; \\color{Red} {0.001}\\\\\\end{array}(I started the table at  because you already observed two show-goers in your sample.  I ended it at  because you already observed six non-show-goers, leaving at most  show-goers.)When  is small (which it is under the alternative hypothesis, consisting only of the possibilities ), the chance that  is high.  But as  increases, the chance goes down.  Among the null hypothesis, which comprises the cases  (tabulated in red), the greatest chance occurs when , where it is .  This is the p-value.Let's interpret this conclusion to check that it makes sense.  The narrative might go like this:  I wish to test whether there are five or more show-going tickets in the box.  A small number of show-going tickets in my sample would be evidence against that.  I saw just two show-going tickets in the sample.  There actually is a situation--namely, where exactly five out of the twenty people went to the show--where the chance of observing two or fewer tickets in my sample is as great as .  This is very high, showing my sample is consistent with the null hypothesis.As a further check of this reasoning, consider a scenario in which your sample ought to have a low p-value. Suppose your null hypothesis were that more than half of the 20 people went to the show.  The corresponding set of possible values of  is .  The largest chance that  for any of those situations occurs when  and is only  ().  That's a pretty low p-value, allowing you to conclude it's likely there are fewer than  show-going tickets in the box.  Indeed, you have seen only two of them and there are just  tickets left in the box, so you would be confident there are fewer than nine show-going tickets among them.  Given that only one-quarter of the sample contains show-going tickets, that's a reasonable conclusion.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-07-24 14:38:27","Question_id":225203}
{"_id":{"$oid":"5837a587a05283111e4d678c"},"Last_activity":"2016-07-23 22:38:14","Creator_reputation":15561,"Question_score":2,"Answer_content":"I think this homework question is a bit unclear. The experiment seems to be handing out the flyer to a population of  people. You want to know what is the probability that  to  people attend as a result. Usually, experiments involve a control group that gets a placebo, but perhaps this is a new band that no one knows about, so the assumption that no one plans to attend in the absence of the treatment flyer is reasonable. Or maybe the organizers really liked this study. The second assumption is that the flyers cannot be shared, so treatment is not \"contagious\". This is less reasonable, but would complicate the problem too much to relax. In any case, if you think of an experiment as a procedure carried out to verify, refute, or validate a hypothesis, this fits the bill. The problem is that you don't know the take-up rate in the population. To learn it, you sample  receivers at random from the population of , and note that  went. The rate seems to be . You might even do a binomial test here and find that you cannot reject the null that , as in @E L M's answer.  The ultimate goal, however, is to to extrapolate from your sample to the population of 20, which is the experiment. The probability that  or more people attend when you hand out  flyers can be calculated by the binomial tail function, which gives your the probability of observing  or more successes in  trials when the probability of a success on one trial is . In Stata, this would be:. display binomialtail(20,5,1/4).5851585You can even do this from first principles by subtracting : di 1-[binomialp(20,0,1/4)+binomialp(20,1,1/4)+binomialp(20,2,1/4)+binomialp(20,3,1/4)+binomialp(20,4,1/4)].5851585You could also think of this as a one-sided binomial probability test:. bitesti 20 5 1/4        N   Observed k   Expected k   Assumed p   Observed p------------------------------------------------------------       20          5            5       0.25000      0.25000  Pr(k \u0026gt;= 5)           = 0.585158  (one-sided test)  Pr(k \u0026lt;= 5)           = 0.617173  (one-sided test)  Pr(k \u0026lt;= 5 or k \u0026gt;= 6) = 1.000000  (two-sided test)The first one-sided test gives you the same probability as the tail approach. It is also a p-value. Why?The p-value of a hypothesis test is the probability (calculated assuming  is true) of observing any outcome as extreme or more extreme than the observed outcome , with extreme meaning in the direction of the alternative hypothesis. You reject the null when the p-value is small, in favor of the alternative, because anything as extreme or more is unlikely if the null was true. You don't accept the null, however, the data can only be consistent with it. In R, this can be done with:\u0026gt; binom.test(5,20,1/4, alternative = \"greater\")        Exact binomial testdata:  5 and 20number of successes = 5, number of trials = 20, p-value = 0.5852alternative hypothesis: true probability of success is greater than 0.2595 percent confidence interval: 0.1040808 1.0000000sample estimates:probability of success                   0.25 ","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-23 19:47:32","Question_id":225203}
{"_id":{"$oid":"5837a587a05283111e4d679d"},"Last_activity":"2016-07-30 08:46:15","Creator_reputation":13091,"Question_score":2,"Answer_content":"Edited after comments  In order to calculate corrected standard errors for the regression coefficients, I intend to use generalized least squares with correction for the autocorrelation in the residuals.Note that generalized least squares (GLS) would affect not only the standard errors but also the point estimates. Anyhow, you could gain power by estimating regression with an explicitly specified error structure, e.g. regression with ARMA errors as can be done using functions stats::arima or forecast::auto.arima in R. There you use maximum likelihood estimation instead of GLS. See related blog posts by Francis X. Diebold \"The HAC Emperor has no Clothes\" and \"The HAC Emperor has no Clothes: Part 2\" where he encourages explicit error specification as a way to get better coefficient estimates and gain predictive power. Although he discusses the case of HAC there, I believe similar conclusions apply here, too.  But for the seasonality, I am not sure for which time series I should perform the differencing: If for raw original data (observations), or for the regression residuals.Since the problem arises due to a cyclic regressor, you could remove the deterministic component of the cyclic variable before including it in the model, or alternatively you could include some seasonal terms (dummies or Fourier terms) in the model.  For the data, I could superimpose each cycle and take the mean, thus removing season effects, but this would alter the data, and also would generalize bad for other kinds of data.I am a little confused here, but I will try addressing this nevertheless.With regards to the regressor, you can adjust using a model, and so altering data is not really a problem because you keep track of how you did it and you can recreate the original variable if you need to.Regarding generalization, if the cyclic behaviour is unique for this instance, keeping it untreated would not help. If, on the other hand, it is similar across this instance and the ones you want to generalize to, you would not lose by removing the deterministic component before running the regression but then using it to adjust the other cases similarly.A technical note: If you are doing a regression with ARMA errors, then it is the error that gets differenced. If the errors is some SARIMA process, regular treatment of SARIMA models applies (roughly speaking, you do not have to worry that it is a regression error rather than raw data).","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-28 12:46:58","Question_id":226120}
{"_id":{"$oid":"5837a587a05283111e4d67ac"},"Last_activity":"2016-08-02 05:53:23","Creator_reputation":56,"Question_score":0,"Answer_content":"Yes it is possible getting great classification results using one feature.Think of the null example - using the class itself as a feature for classification. In general, the more complex are the interactions between your different target classes, the more feature you will need.","Display_name":"DaFanat","Creater_id":123020,"Start_date":"2016-08-02 05:53:23","Question_id":163885}
{"_id":{"$oid":"5837a587a05283111e4d67ad"},"Last_activity":"2015-07-30 04:09:00","Creator_reputation":530,"Question_score":2,"Answer_content":"As ever, it depends on the dataset. Sometimes, one feature may be sufficient to build a highly-accurate classifier, but on most interesting, non-trivial problems, multiple features are needed. For example, in image processing one often needs a large number of weak features. Note that this is a separate issue from data collection, where you have some control over what to measure in the first place. Some expert knowledge at that early stage can save a lot of work on later feature selection/dimension reduction etc.","Display_name":"dcorney","Creater_id":81857,"Start_date":"2015-07-30 04:09:00","Question_id":163885}
{"_id":{"$oid":"5837a587a05283111e4d67ae"},"Last_activity":"2015-07-30 00:34:57","Creator_reputation":199,"Question_score":2,"Answer_content":"Collecting \"good\" features (e.g., such features that lead to maximum accuracy) in advance, is in general, not always possible. Therefore, we collect \"alot\" of features in the hope that \"good\" features are among them. After we collected alot of features we often use feature selection/extraction methods, in order to reduce them to a smaller set of features, that lead to promising results. However, this doesn't always work as there are many pitfalls. For example there are \"bad\" features that in combination can work much better as a single \"good\" feature. So we must take this also into account. Here you can find 7 techniques for data dimensionality reduction. Have a look on these...","Display_name":"Unhandled exception","Creater_id":78200,"Start_date":"2015-07-30 00:34:57","Question_id":163885}
{"_id":{"$oid":"5837a587a05283111e4d67bb"},"Last_activity":"2016-08-02 05:48:42","Creator_reputation":25585,"Question_score":0,"Answer_content":"The paper is about Bayesian estimation and  is a prior. Given your data and the priors you can estimate posterior probabilities. Posterior probabilities are calculated because the paper is about density estimation, so you use their method since you are interested in the density itself. If you were interested in something else, you could use the MCMC samples to estimate any quantities of interest, as you correctly noticed.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-02 05:48:42","Question_id":226832}
{"_id":{"$oid":"5837a587a05283111e4d67c8"},"Last_activity":"2016-08-02 05:37:17","Creator_reputation":388,"Question_score":1,"Answer_content":"If you shorten you code, it becomes:library(forecast)set.seed(1234)y \u0026lt;- ts(sort(rnorm(30)), start = 1978, frequency = 1) # annual datafcasts \u0026lt;- numeric(10)for (i in 1:10) {  fcasts[i] \u0026lt;- forecast(auto.arima(window(y, end = 1996 + i) ), h = 1)mean  fc \u0026lt;- window(fitted.Arima(Arima(y, model = auto.arima(window(y, end = 1996 + i)))), start = 1998)[i]}fcasts-fc # is identical (beside very small numerical differences of `e-14`).","Display_name":"Qaswed","Creater_id":112892,"Start_date":"2016-08-02 05:12:08","Question_id":226685}
{"_id":{"$oid":"5837a587a05283111e4d67c9"},"Last_activity":"2016-08-02 04:57:04","Creator_reputation":275,"Question_score":0,"Answer_content":"Apart from the difference between forecast and fitted pointed by @Billywob in the comments, your loop is using different models to forecast:fcasts \u0026lt;- numeric(10)modls\u0026lt;-list()for (i in 1:10) { # start rolling forecast  # start from 1997, every time one more year included  win.y \u0026lt;- window(y, end = 1996 + i)   fit \u0026lt;- auto.arima(win.y)  modls[[i]]\u0026lt;- names(fitmean}\u0026gt; modls[[1]][1] \"ar1\"[[2]][1] \"ar1\"[[3]][1] \"ar1\"[[4]][1] \"ar1\"[[5]][1] \"ar1\"[[6]][1] \"ar1\"[[7]][1] \"ar1\"[[8]][1] \"ar1\"[[9]][1] \"ar1\"   \"drift\"[[10]][1] \"ar1\"   \"drift\"and your fitted only uses one model.\u0026gt; names(fit$coef)[1] \"ar1\"","Display_name":"Robert","Creater_id":77852,"Start_date":"2016-08-02 04:57:04","Question_id":226685}
{"_id":{"$oid":"5837a587a05283111e4d67da"},"Last_activity":"2016-08-02 05:05:35","Creator_reputation":25585,"Question_score":5,"Answer_content":"Actually, it is much simpler. Assuming that you have -component mixture of gamma distributions the algorithm to draw sample of size  is as follows:  Repeat  times:   1. draw  from categorical distribution parametrized by vector ,   2. draw single value from -th gamma distribution parametrized by , .You can use similar algorithm to draw samples from mixture of any distributions. Notice that this follows exactly from the definition of mixture distribution:  mixture distribution is the probability distribution of a random  variable that is derived from a collection of other random variables  as follows: first, a random variable is selected by chance from the  collection according to given probabilities of selection, and then the  value of the selected random variable is realized.If you know R, this translates to the following example:# densitydmixgamma \u0026lt;- function(x, pi, alpha, beta) {  k \u0026lt;- length(pi)  n \u0026lt;- length(x)  rowSums(vapply(1:k, function(i) pi[i] * dgamma(x, alpha[i], beta[i]), numeric(n)))}# random generationrmixgamma \u0026lt;- function(n, pi, alpha, beta) {  k \u0026lt;- sample.int(length(pi), n, replace = TRUE, prob = pi)  rgamma(n, alpha[k], beta[k])}set.seed(123)pi \u0026lt;- c(4/10, 6/10)alpha \u0026lt;- c(20, 15)beta \u0026lt;- c(10, 25)hist(rmixgamma(1e5, pi, alpha, beta), 100, freq = FALSE)xx \u0026lt;- seq(0, 10, by = 0.001)lines(xx, dmixgamma(xx, pi, alpha, beta), col = \"red\")","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-08-02 02:57:16","Question_id":226834}
{"_id":{"$oid":"5837a587a05283111e4d67e7"},"Last_activity":"2016-08-02 04:38:09","Creator_reputation":111,"Question_score":1,"Answer_content":"Your name tells me you might not be that familiar with R (correct me if I'm wrong). You will first need to install the needed package, see this link for how to: https://www.r-bloggers.com/installing-r-packages/After installing, make sure you load the package by usinglibrary(lmtest)After you've loaded the needed package, you can use?petestto find the helpfile that corresponds to the function. Examples and applications of the function are found at the bottom of the helpfile.If after taking these steps you still can't open the helpfile, something has gone wrong with the loading of the library.","Display_name":"Rogier","Creater_id":79898,"Start_date":"2016-08-02 04:38:09","Question_id":226849}
{"_id":{"$oid":"5837a587a05283111e4d67f4"},"Last_activity":"2016-08-02 04:28:20","Creator_reputation":404,"Question_score":0,"Answer_content":"If  and , is typical ecdf plot proper for visualizing the data ? I feel It may be better to specify the intervals closed on the right to avoid misreading. Or may be we should draw the graph like a barplot !??? (I used waterfalls package in R)","Display_name":"cuttlefish44","Creater_id":119029,"Start_date":"2016-08-01 04:57:10","Question_id":225076}
{"_id":{"$oid":"5837a587a05283111e4d67f5"},"Last_activity":"2016-07-22 02:24:54","Creator_reputation":25585,"Question_score":5,"Answer_content":"In most cases your plot is presented with flipped axes, where on -axis we present some values and on -axis their cumulated empirical probabilities (see example below). So such plot illustrates empirical cumulative distribution function and I think that in most cases people call it simply empirical cumulative distribution function plot.Check also here for some examples and definitions Why is the empirical cumulative distribution of 1:1000 a straight line?","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-22 02:18:15","Question_id":225076}
{"_id":{"$oid":"5837a587a05283111e4d6806"},"Last_activity":"2016-08-02 02:20:15","Creator_reputation":109,"Question_score":2,"Answer_content":"Evaluating implicit feedback based recommendations is tricky. Here are couple of approaches that I'd recommend.Approach 1You can use modified precision and recall metrics. Overall the procedure would be as follows,Divide your data into train and test set by users.For a user in test set, given their history, get the top N recommendations using implicit feedback based model.Precision can be calculated using # of recommendations given by model which actually matched by what user had acted upon (for example read in case of articles).Recall can be calculated using # of user actions (articles read by user) that were captured by top N recommendations.You can calculate these for all users in test set and average them.Approach 2The approach is similar to approach 1, but rather than splitting train and test data by users, you use something called as \"leave one out\" strategy. We use a simple accuracy metric in this case. The steps to calculate the accuracy will be as follows,For each user (or a subset of users), hide one of the articles read/browsed and move it to test set (leave one out).Using user history, get top N recommendations for each user.Calculate the number of times the left out article was captured by the top N recommendations.Note that in both cases, the N in top N recommendations will become your hyper-parameter which can be tuned further. Hope this helps.","Display_name":"hssay","Creater_id":26218,"Start_date":"2016-08-02 02:20:15","Question_id":226825}
{"_id":{"$oid":"5837a587a05283111e4d6813"},"Last_activity":"2016-07-31 02:11:26","Creator_reputation":13091,"Question_score":3,"Answer_content":"Looking at individual autocorrelations may help in simple cases, but this way you could miss lags that are important only jointly but not individually. Alternatively, you may try the following:Select a large number of lags and estimate a penalized model (e.g. using LASSO, ridge or elastic net regularization). The penalization should diminish the impact of irrelevant lags and this way effectively do the selection. There would be some inconvenience in that cross validation is normally used for selecting penalty intensity, and cross validation is a bit tricky with time series. But this is still doable, no doubt about it.Try a number of different lag combinations and either(i) select the best of them according to an information criterion (AIC should do well in terms of forecasting as it is an efficient selector) or out-of-sample performance OR(ii) combine some or even all of them weighting the models based on their likelihood, information criteria or the like. Refer to model averaging and forecast combination literature for detailed recipes.(ii) would often do better than (i) in terms of forecasting, especially if you are selecting from a large number of alternatives.Another alternative is to leave the job to some automated procedure like the auto.arima function in \"forecast\" package in R. The algorithm for auto.arima is available in Hyndman \u0026amp; Khandakar (2008).References:Hyndman, Rob J., and Yeasmin Khandakar. \"Automatic Time Series Forecasting: The forecast Package for R.\" Journal of Statistical Software 27.i03 (2008).","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-30 12:31:10","Question_id":226468}
{"_id":{"$oid":"5837a587a05283111e4d681f"},"Last_activity":"2016-08-02 03:18:43","Creator_reputation":152738,"Question_score":4,"Answer_content":"No, regression (it's not just some quirk of lm) does not get the degrees of freedom wrong.  I would imagine that the observations from T3 are not relevant for this t-testYou would be incorrect. While you're right in thinking that observations from T3 don't participate in the comparison of means in the numerator of the t-statistic, the t-statistic has both a numerator and a denominator.Because of the assumption of constant variance in the regression model, all the residuals have information about the variance estimate used in the denominator, so the observations all participate in its estimate. The degrees of freedom for the t-test come from the degrees of freedom in the estimate of the error variance. As suggested above, for regression this is based on the model using all observations, and does not relate to the subset of observations that participate in the comparison in the numerator of a t-statistic.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-29 05:11:48","Question_id":226237}
{"_id":{"$oid":"5837a587a05283111e4d682e"},"Last_activity":"2016-08-02 02:47:46","Creator_reputation":87,"Question_score":0,"Answer_content":"For those interested, the answer to this problem requires the Ordered Inverse Transform, the complete answer can be found here: https://groups.google.com/forum/#!category-topic/stan-users/general/sgX2Edo8qiQ","Display_name":"Anton","Creater_id":95411,"Start_date":"2016-08-02 02:47:46","Question_id":226070}
{"_id":{"$oid":"5837a587a05283111e4d683e"},"Last_activity":"2016-08-02 02:11:25","Creator_reputation":755,"Question_score":2,"Answer_content":"Interchanging the order of integration and expectation you getE(I)=E\\int_0^L X(t) dt = \\int_0^L EX(t) dt = \\int_0^L \\mu dt = L\\mu and similarly, the second moment of  becomes\\begin{align}E(I^2)\u0026amp;=E\\left(\\int_0^LX(t)dt\\int_0^LX(u)du\\right) \\\\\u0026amp;= E \\int_0^L \\int_0^L X(t)X(u)du dt \\\\\u0026amp;= \\int_0^L \\int_0^L E[X(t)X(u)]du dt \\\\\u0026amp;= \\int_0^L \\int_0^L [\\operatorname{Cov}(X(t),X(u))+EX(t)EX(u)] du dt \\\\\u0026amp;= \\sigma^2 \\int_0^L \\int_0^L \\rho(t-u)dudt + L^2 \\mu^2.\\end{align}If the correlation function  is for example exponential the double intergral can be easily solved and the variance of  is then .","Display_name":"Jarle Tufto","Creater_id":77222,"Start_date":"2016-08-01 06:33:25","Question_id":226657}
{"_id":{"$oid":"5837a587a05283111e4d684b"},"Last_activity":"2016-08-02 01:56:30","Creator_reputation":71,"Question_score":0,"Answer_content":"I'm not entirely sure if this is an answer to your question, but maybe you'll find it useful.Maybe the author of the randomForest package would disagree with me, but I feel like the rfImpute() function is mostly used or called upon other imputation packages in their algorithms to impute many variables. If you only have one variable with missing data, then using this function as a stand alone may work. However, I think it is the case for most people that they have many variables with missing data in a datset that they'd like to impute. Enter the packages missForest and mice. If you use the R package missForest, you can impute your entire dataset (many variables of different types may be missing) with one command missForest(). If I recall correctly, this function draws on the rfImpute() function from the randomForest package. For some reason (maybe others can elaborate), when you use the missForest() function, the other variables that are used to predict a single variable can also have missingness. So I think using this function and package are a nice idea if you are hoping to only get one dataset out, after all variables have been imputed.The downside to using missForest() is that you only get one dataset, which does not allow you to take into account the uncertainty of your estimates (in your follow-on analytical models). So your analytical models will have incorrect confidence intervals if you just base the analysis on that one imputed dataset. If that doesn't matter to you, then I highly recommend this package and function, because it is very easy to use and specify your imputation model.However, if you do need to get appropriate confidence intervals and pooled estimates in your analytical models, then you should probably use multivariate imputation by chained equations (MICE) approaches to imputation. For this, you can use the mice package. There is recent functionality within this package that allows you to specify which variables you'd like to impute with a random forest algorithm, and which you would like to use the usual methods (e.g. pmm). When specifying your imputation model with the mice() function, under methods you would do something like meth \u0026lt;- c(\"rfcat\", \"rfcont\").missForest has a nice vignette you can look up in R. Here is a nice resource for how to set up your imputation models using mice:http://www.stefvanbuuren.nl/publications/MICE%20in%20R%20-%20Draft.pdf","Display_name":"RNB","Creater_id":93199,"Start_date":"2016-08-02 01:56:30","Question_id":226803}
{"_id":{"$oid":"5837a587a05283111e4d6858"},"Last_activity":"2016-08-02 01:41:58","Creator_reputation":3038,"Question_score":2,"Answer_content":"Let's play a game that you might solve by maximum likelihood:Assume that a someone is in a room, you can not see the person.  The person has a die and a coin and decides to try one of these.  If he trows the die and observes a '1' then he says you that he has '1' else he says you he has zero.  If he throws the coin then he says that he has '1' when it ends head up and '0' otherwise.He does not tell you whether he threw the coin or the die, you can not see it neither. The only thing that he tells you is whether he has one or zero. Assume that he says it is 1. What would you guess, did he throw the die or the coin ?What is the likelihood of this observation ?if the coin has been thrown, then the probability that you have a one, given that is was the coin that was tossed, is equal to 0.5 or 50%, or the likelihood of having a coin, given that he says it is '1', is 50%  (note that this is a value of the parameter of the coin)If the die has been thrown, then the probability that he says '1', given that the die was thrown is 1/6=16.666%.  Or the likelihood that it was a die given that he says '1' is 16.666% (note that this is a value of the parameter of the die).  If you have to take a decision based on the maximum likelihood principle then you decide for the one with the highest likelihood, or you decide that he has thrown a coin (because the likelihood of the coin given that he says '1' is 50%, which is higher than the likelihood that he threw a die given that he says 1 (16.6666%)).","Display_name":"fcop","Creater_id":83346,"Start_date":"2016-08-01 23:36:11","Question_id":226619}
{"_id":{"$oid":"5837a587a05283111e4d6859"},"Last_activity":"2016-08-01 21:12:43","Creator_reputation":388,"Question_score":1,"Answer_content":"Suppose there are three observations: 3, 4, 8 (and they are independent). In this example, each of the three numbers is an individual observation. The whole data set is represented by the three observations. Because, I think, a specific example makes it easier to understand... If we assume that the data (3, 4, and 8) came from a Poisson distribution with mean 5. The likelihoods of the observations (3, 4, and 8), respectively, are dpois(3,lambda=5), dpois(4,lambda=5), dpois(8,lambda=5).The joint likelihood of the whole data set isdpois(3,lambda=5)*dpois(4,lambda=5)*dpois(8,lambda=5)The example is consistent with the statement. dpois(3,lambda=5), dpois(4,lambda=5), and dpois(8,lambda=5) are the likelihood of an individual observation.","Display_name":"quibble","Creater_id":44163,"Start_date":"2016-08-01 21:12:43","Question_id":226619}
{"_id":{"$oid":"5837a587a05283111e4d685a"},"Last_activity":"2016-08-01 19:34:14","Creator_reputation":8367,"Question_score":2,"Answer_content":"The likelihood is what you get when you evaluate the appropriate probability density function (using the model and parameter values of interest) at the value of the observation.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-31 21:48:04","Question_id":226619}
{"_id":{"$oid":"5837a587a05283111e4d6869"},"Last_activity":"2016-08-02 01:20:10","Creator_reputation":131,"Question_score":0,"Answer_content":"Neyman Pearson's lemma states that if you fix  or the false alarm probability () you can calculate the threshold, , as a solution to the following equation:\\begin{equation}\\alpha = P_{FA}=\\int_{\\eta}^{\\infty} P_r (Z|H_0)d Z,\\end{equation}where . So to answer your question it is essential in many cases to determine the PDF of the test.Example: for detecting a DC level of value  in white Gaussian noise (WGN) the test will read\\begin{equation}\\Lambda(X)=\\frac{1}{N} \\sum_{N=0}^{N-1}x_n \u0026gt;  \\eta.\\end{equation}In this case the test  is Gaussian distributed witn variance  under both hypotheses and with zero mean under hypothesis  and with mean  under .Using this you can calculate the threshold for a given probability of false alarm using Neyman Pearson's Lemma.","Display_name":"Nir Regev","Creater_id":95551,"Start_date":"2016-08-02 01:20:10","Question_id":225335}
{"_id":{"$oid":"5837a587a05283111e4d686a"},"Last_activity":"2016-07-28 00:20:07","Creator_reputation":107,"Question_score":0,"Answer_content":"This question is resolved, because it contains a misunderstanding of  the term . This term is not meant to be a cdf for , but a probability for  fulfilling the constraint of . So there is no need to think of the likelihood ratio to be distributed in any way.","Display_name":"data_hope","Creater_id":43084,"Start_date":"2016-07-28 00:17:44","Question_id":225335}
{"_id":{"$oid":"5837a587a05283111e4d6877"},"Last_activity":"2016-08-02 01:20:01","Creator_reputation":26,"Question_score":1,"Answer_content":"I have a similar problem - posted here - and no certain answer still. What I did for the moment is simply gather a set of very similar Xs and check if there's a big variation for Y within those lines. Another kind of approach could be some a simulation: you use a single X from your dataset, but replicate the lines following the predictors systematic error (something like rnorm(...,0,0.3)). The confidence interval for slope may be something similar to the systematic error span.","Display_name":"Paolo Nadalutti","Creater_id":124864,"Start_date":"2016-08-02 01:20:01","Question_id":165046}
{"_id":{"$oid":"5837a587a05283111e4d6886"},"Last_activity":"2016-08-02 01:08:20","Creator_reputation":24991,"Question_score":5,"Answer_content":"The regression would not be spurious. If  and  thent=\\frac{1}{\\gamma_1}X_t-\\frac{\\gamma_0}{\\gamma_1}-\\frac{1}{\\gamma_1}v_tandY_t=\\delta_0-\\frac{\\delta_1\\gamma_0}{\\gamma_1}+\\frac{\\delta_1}{\\gamma_1}X_t+u_t-\\frac{\\delta_1}{\\gamma_1}v_tNow this is simply a regressionY_t=\\alpha_0+\\alpha_1X_t+\\varepsilon_tand it is possible to show that OLS estimates  and  are consistent and assymptoticaly normal with means  and  respectively, albeit with non-standard normalizing constants. The mathematical details can be found in this answer.The consistency can be illustrated by the following code:gend \u0026lt;- function(n) {     data.frame(x=1+2*1:n+rnorm(n),y=3+4*1:n+rnorm(n))}\u0026gt; set.seed(13)\u0026gt; coef(lm(y~x,data=gend(10)))(Intercept)           x   -1.291464    2.067586 \u0026gt; coef(lm(y~x,data=gend(100)))(Intercept)           x    1.396720    1.997408 \u0026gt; coef(lm(y~x,data=gend(1000)))(Intercept)           x   0.9864317   1.9999570 \u0026gt; coef(lm(y~x,data=gend(10000)))(Intercept)           x   0.9595726   2.0000065 Here I generated two trend stationary variables with , ,  and . As we see regression estimates approach the true values  and .","Display_name":"mpiktas","Creater_id":2116,"Start_date":"2013-10-03 03:59:57","Question_id":71650}
{"_id":{"$oid":"5837a587a05283111e4d6894"},"Last_activity":"2016-08-01 05:27:01","Creator_reputation":33,"Question_score":0,"Answer_content":"This can be a good start for the multivariate regression. First of all, you need a mathematical model like this:.Basically you have a mathematical equation were the  is your response variable and the  matrix has 2 columns, your explanatory variables, the regressors. The  is a vector that concern all the things that are not explained with the , the remainder; usually is an  process with  mean and  costant variance.Assume that you estimate the coefficients with the  and so you get th classic Gaussian Multivariate Model.Shortly, the  coefficients are the effects of the two  variables on the response . You can interpret this effect with the Ceteris Paribus: is the effect on the  by an increment of 1 unit of the , with  constant.The same for  but in the inverse order. is the intercept, the mean value of the  when all the  variables (and so their effects) are null.","Display_name":"Enzo D\u0026#39;Innocenzo","Creater_id":124731,"Start_date":"2016-08-01 05:27:01","Question_id":226638}
{"_id":{"$oid":"5837a587a05283111e4d68a1"},"Last_activity":"2016-08-02 00:32:31","Creator_reputation":11925,"Question_score":0,"Answer_content":"If you center a variable at the mean and then compute the square of that centered variable, then the centered variable and its square show no linear relation. There is still a strong non-linear relationship, but that is not a problem. ","Display_name":"Maarten Buis","Creater_id":23853,"Start_date":"2016-08-02 00:32:31","Question_id":226768}
{"_id":{"$oid":"5837a587a05283111e4d68b0"},"Last_activity":"2016-08-01 23:22:52","Creator_reputation":8367,"Question_score":2,"Answer_content":"Here's a hint. In order to make fewer than 1 goal, how many goals would she have to make? If you can answer that question, it's just a matter of understanding what the table says.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-01 23:22:52","Question_id":226778}
{"_id":{"$oid":"5837a587a05283111e4d68bd"},"Last_activity":"2016-08-01 23:19:36","Creator_reputation":3832,"Question_score":1,"Answer_content":"Each of your models is a joint distribution for the random variables . This means you can calculate the probability according to each model that the random variables take particular values: . I'll use the shorthand  to mean the same thing (uppercase letters are random variables, lowercase are particular values).Say the the true distribution is  and the model is .In your case, the formula for KL divergence is:D_{KL}(p \\parallel q)= \\sum_{a, b, c, d \\in \\{0, 1\\}}p(a, b, c, d)\\log \\frac{p(a, b, c, d)}{q(a, b, c, d)}The sum is taken over all possible values of . You have 4 variables with 2 possible values each, so there are 16 possible joint configurations: , , etc. Loop over the 16 possible configurations. Determine the probability of each configuration according to each model. Plug those values into the  expression above, taking the sum over all configurations.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-08-01 23:19:36","Question_id":226744}
{"_id":{"$oid":"5837a587a05283111e4d68ca"},"Last_activity":"2016-08-01 20:02:07","Creator_reputation":152738,"Question_score":1,"Answer_content":"With or without exact zeros a histogram of a very skew distribution can look like this. It has nothing to do with the spread, nor with the existence of zeros, but with how far above the bulk of the data the largest observation is.You're dealing with two different problems at once here --  a. The distribution is very skew. b. The default number of bins in R is a good deal too low for seeing the shape well in general and much too low when the distribution is skew or has heavy tails.The second problem is easy to deal with -- use far more bins.Obtaining a good display with very skewed data is not a single step process. It may take several attempts and some decision-making about how to best represent the informationIf the data were actually lognormal with a large  parameter (and so very skew), it can look just like your plot. Here I generated some data (in \"x\", with n=1800) which has a particular lognormal distribution:I made that plot have about twice as many bins that you got from the default, but it still didn't show any detail. How skewed is this? Well, the sample mean is about 8000 times as big as the median. Those large values dominate more than just the plot. (I made a second data set that's somewhat less skew, to showthe potential value of some of the options I suggest)The most obvious thing to do would be to plot on a log scale:Note that the axis labels are in original values not log-currency, just as you'd get with plot(...,log=\"x\") (in fact that's what I used to make this plot, after extracting the results of hist by putting it into a variable); I also added some detail on the x-axis but this isn't really necessary.If you have exact zeros this approach of plotting on a log-scale is obviously not suitable for them as-is, since you can't take log of exact 0's (which are impossible in a lognormal; clearly you don't a lognormal). How you might deal with them depends on what you're trying to do with the variable and how many there are.(What's the actual proportion of exact zeros? You didn't say)Anyway, here's the simplest step in the process of trying to find a suitable display:try a histogram with a lot of bins - at least a hundred. And plot in a bigger window.This can sometimes help a lot but if your data is pretty skew, it won't solve the problem:That may not work (it didn't for my most skewed sample there), so what else is there?Cut off the largest values and list them on the plot, or do two displays, one with the top end cut off and one showing the larger values (equivalently, on one display, show a complete scale break and plot the two parts on two different x-scales). Something a bit like this:This can often help a lot but if your data is really skewed, it won't solve the problem:This is about the best plot possible with that data -- you really can't get more detail on the right without losing it all on the left.cut off the exact zeros, show a count of them and plot on the log-scale (but with original currency-scale tick-labels) as in my second diagram aboveConsider a transformation that can manage zeros, such as cube-root, but still show currency on the axis. (This would involve writing some R-code, so it may be too hard for you at this point. I don't really suggest it in this case, since people are much more used to seeingfinancial variables like income either on the log-scale, or on the original currency-scale.)Since someone is bound to ask, here's how I did the log-scale plot (absent additional fiddling with axis ticks):res \u0026lt;- hist(log(x),n=30)with(res,plot(counts~exp(mids),type=\"h\",lwd=10,col=8,log=\"x\",lend=2,xlab=\"income\"))(You need lwd wide enough that the bars just touch. You need lend to make the cars have square ends. You can make a version of \"hist\" to do this but it takes more work.)","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-01 18:34:21","Question_id":226669}
{"_id":{"$oid":"5837a587a05283111e4d68d7"},"Last_activity":"2016-08-01 22:06:12","Creator_reputation":7702,"Question_score":0,"Answer_content":"Several tentative approximations:Random variables are not random. They are deterministic functions from the outcome to the real line, . So you run a random experiment (the experiment, say tossing a coin, is random in the sense that we don't have a formula to return the outcome a priori), and get an outcome; run it again, and get another outcome. Soon you have a sample, and you happen to be interested in a parameter, say the proportion of heads, : you are mapping something like  to the interval  to get an estimate of the parameter  based on your sample, using the simple formula, , a deterministic formula. You may label this estimate, .Confidence interval: From this point estimate, you can calculate the CI with some formula, such as, . Again deterministic, meaning (crazy nomenclature), a random variable... or two: one for the lower bound, and the other for the upper bound. So effectively you have unfolded the point estimate into two point estimates, based on some underling distributional assumptions (normal approximation), completely unrelated to the specific realization that your sample represents.This interval can contain  or not. Again, think about the point estimate - it can fall very far from the true parameter, , and affect the CI accordingly.But there is one saving grace, which is at the same time a painful yoga position: If you were to repeat this process time and time again, and get many  estimates with their respective confidence intervals, the true parameter  would be contained in  of them.The confidence interval does not tell you that with  probability the true proportion is contained between its bounds, which is mind boggling. It is, instead nothing more than \"an elaboration\" on the sample based on things like the CLT. As such it is \"random\" (wink, wink).If you want the probability that the parameter  is contained within an certain interval, you have to change party affiliation, and look up credible intervals under the apparently more satisfying Bayesian paradigm.","Display_name":"Antoni Parellada","Creater_id":67822,"Start_date":"2016-08-01 21:59:03","Question_id":226799}
{"_id":{"$oid":"5837a587a05283111e4d68d8"},"Last_activity":"2016-08-01 22:05:03","Creator_reputation":152738,"Question_score":2,"Answer_content":"  Why is the confidence interval considered random? You just stated why in your question! You quoted this:  \"A confidence interval is a random variable because x-bar (its center) is a random variable.\"(In this case, it's presumably an interval for the mean, but the reasoning carries over to other confidence intervals.)The sample mean is a statistic -- a quantity you calculate from the sample. Because random samples from some population are, well, random, things calculated from them are also going to be random.Consider: If you drew a second sample from the same population would you have the same observations?Would the sample mean be the same in both samples? Would the sample standard deviation be the same in both samples? The largest observation? The lower quartile?No, they vary from sample to sample; indeed they're also random.A confidence interval is also based on the random sample, so it, too, is a statistic and it, too, is random.  If it's truly random then why bother with confidence intervals at all?  Am I missing something here?Well presumably you'd like to use the data to calculate your interval.  After all, it's the thing we have that tells us something about the population we drew the sample from.If you're using the data - a random sample of your population - then useful quantities you calculate from it will also be random, including confidence intervals.Random doesn't mean \"ignores your data\" -- for example a sample mean tells us about our population mean, and our sample standard deviation can be used to help us work out how far the sample mean will tend to be from the population mean.In fact, we rely on the randomness - we exploit it to get the best possible use of information from our sample. Without random sampling, our intervals wouldn't necessarily tell us much of anything.[You might like to ponder whether there might be a way to get an interval for a population quantity that is simultaneously reasonably informative and not random.]","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-08-01 21:42:01","Question_id":226799}
{"_id":{"$oid":"5837a587a05283111e4d68e5"},"Last_activity":"2016-08-01 22:03:20","Creator_reputation":3832,"Question_score":1,"Answer_content":"You're correct that automatic differentiation (including theano's grad function) just uses the chain rule. The interesting point is that this is also how backpropagation works; it's just the chain rule. The standard backprop equations for computing the weight update direction amount to computing the gradient of the loss function with respect to the parameters.The traditional way of training a network with backprop looks like this:Forward pass. Compute the activations and loss function, given the inputs and parameters. These will be used for computing the gradient in the next step.Backward pass. Compute the gradient of the loss function with respect to the parameters.Update parameters by moving in the direction opposite the gradient, with some step size.So, traditional backprop training is just gradient descent. When you compute the gradient with theano, the result will be identical to what you'd get with the forward and backward passes using the backprop equations. Under the hood, theano is using reverse-mode automatic differentiation which, in the case of neural nets, is equivalent to the backprop equations (perhaps modulo some optimizations of the computational graph that theano might make to increase efficiency).To complete the backprop training implementation, you'd just have to add some code for updating the parameters, given the gradient. Of course, there are many fancier modifications of the traditional gradient descent update rule, which you could use to boost performance.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-08-01 22:03:20","Question_id":226754}
{"_id":{"$oid":"5837a587a05283111e4d68f2"},"Last_activity":"2016-08-01 21:18:18","Creator_reputation":334,"Question_score":0,"Answer_content":"As Wart has pointed out, the  test for homogeneity (i.e. independence) is a good way to proceed in this situation.  In R, this can be done with the command chisq.test(mytable) Documentation","Display_name":"David C","Creater_id":96030,"Start_date":"2016-08-01 21:09:49","Question_id":226789}
{"_id":{"$oid":"5837a587a05283111e4d68f3"},"Last_activity":"2016-08-01 20:44:34","Creator_reputation":106,"Question_score":1,"Answer_content":"First, note that ratios being different is equivalent to proportions being different.  So you need to test for different proportions.  Here is the null hypothesis -  - and the alternative hypothesis is simply that one proportion does not equal one of the others.  Let  the number of males in group i and  the number of people in group i.  Under the null hypothesis of one common probability, we can estimate the probability someone is male: .  Define  to be our expectation of what  should be under the null hypothesis: .  Given large sample sizes,  is approximately normally distributed.  If  is normally distributed, then the following test statistic is chi-squared distributed with 9 (number of groups minus 1) degrees of freedom: .  If the test statistic exceeds the critical value, the null hypothesis can be rejected.  In this example, you need a statistic of at least 16.919 to reject at .05 significance.See here - http://www.itl.nist.gov/div898/handbook/prc/section4/prc46.htm - for an example. ","Display_name":"Wart","Creater_id":125187,"Start_date":"2016-08-01 20:44:34","Question_id":226789}
{"_id":{"$oid":"5837a588a05283111e4d68ff"},"Last_activity":"2014-06-23 00:51:43","Creator_reputation":1,"Question_score":-1,"Answer_content":"% Calculate the frequency bands for the wavelet decomposition-------------if Nf == 0    Nf = log(fds/f)/log(2);     % number of decomposition levels    Nf = double(uint32(Nf));endD = freq_bands( f, fds, Nf );%Nf=no of level%fds=Fs/ds;%Fs=sampling freq%ds=decimation factor for downsapling=1 or greater%f=fund freq","Display_name":"Oladapo Ogidi","Creater_id":48819,"Start_date":"2014-06-23 00:15:38","Question_id":14505}
{"_id":{"$oid":"5837a588a05283111e4d690c"},"Last_activity":"2016-07-31 22:35:26","Creator_reputation":106,"Question_score":1,"Answer_content":"If  is a random process, e.g. ARIMA(p, 1, q), then you cannot know the .  The important thing to realize here is that the influence of innovations from , an integrated time series, on its future observations do not decay over time, but their effect on  does.  The  statistic you get will essentially be nonsense, and should be taken with a grain of salt.  Granger and Newbold pointed this out in their paper Spurious Regressions in Economics (http://wolfweb.unr.edu/homepage/zal/STAT758/Granger_Newbold_1974.pdf).  They note the following three things about regressing integrated time series on each other without first differencing:(i) Estimates of the regression coefficients are inefficient.(ii) Forecasts based on the regression equations are sub-optimal.(iii) The usual significance tests on the coefficients are invalid.   ","Display_name":"Wart","Creater_id":125187,"Start_date":"2016-07-31 22:35:26","Question_id":226616}
{"_id":{"$oid":"5837a588a05283111e4d691c"},"Last_activity":"2016-08-01 19:46:56","Creator_reputation":338,"Question_score":1,"Answer_content":"If you've normalized the variables, then you have made all the features matter the \"same\" amount. You can now make a feature matter less by scaling.For example, lets say you have two features so each point is . Now let's take 3 points, , , and . So  is equidistant from  and . If we now transform all the points by multiplying the second feature by 2 you get , , and  and  is now closer to  than . This, effectively, makes the first feature matter more.","Display_name":"roundsquare","Creater_id":122754,"Start_date":"2016-08-01 19:46:56","Question_id":226769}
{"_id":{"$oid":"5837a588a05283111e4d691d"},"Last_activity":"2016-08-01 18:13:01","Creator_reputation":106,"Question_score":1,"Answer_content":"There are a few ways to do this.  You want the standard deviation of the scaled feature to be proportional to the feature's importance.  If you have some prior knowledge about how important each feature is, you could choose these yourself.  It's more difficult if you want a systematic way of determining the feature importances.  There is one approach I know of, but it requires a target variable (which is generally not available if you're doing clustering).  You run the model using the data above fit to the target variable.  The feature importances from the model should be the standard deviation you use to scale these features.  Fit Scikit Learn models generally have a feature importances attribute.","Display_name":"Wart","Creater_id":125187,"Start_date":"2016-08-01 18:13:01","Question_id":226769}
{"_id":{"$oid":"5837a588a05283111e4d692a"},"Last_activity":"2016-08-01 17:46:29","Creator_reputation":106,"Question_score":1,"Answer_content":"Question #1:Setting  with the Agresti Coull method will yield the same posterior distribution as would the Jeffreys Prior.  Observe that when , , so  and .Question #2:In general, the prior should not change based on the sample size.  The prior should reflect all the information that is known before any information has been observed.  Whether a small or large amount of data will be observed, the prior knowledge remains the same.","Display_name":"Wart","Creater_id":125187,"Start_date":"2016-08-01 17:46:29","Question_id":226774}
{"_id":{"$oid":"5837a588a05283111e4d6936"},"Last_activity":"2016-08-01 19:36:30","Creator_reputation":80,"Question_score":0,"Answer_content":"I think I figured it out.  It's important to mention that the discussion in Faraway was in the context of IRWLS. First of all, we can use either the variance of the response or the variance function in our IRWLS implementation.  It just represents a scale change:  where  is just some constant.  So I think Faraway is actually using the Variance of Y.Second, Faraway was using R, which actually takes the sample proportion, , when it fits the model:  If a binomial glm model was specified by giving a two-column response, the weights returned by prior.weights are the total numbers of cases (factored by the supplied case weights) and the component y of the result is the proportion of successes.So even though , the response  has variance So until I'm told otherwise, I will assume that his writing  above was either an error or done to gloss over something.  It's the variance of the response.","Display_name":"RMurphy","Creater_id":116056,"Start_date":"2016-08-01 19:36:30","Question_id":219649}
{"_id":{"$oid":"5837a588a05283111e4d6937"},"Last_activity":"2016-06-19 14:00:14","Creator_reputation":1,"Question_score":0,"Answer_content":"There is nothing wrong in your derivation, however, the variance function for exponential family written in your form isV(\\mu)=a(\\phi)b^{''}(\\theta)Which is exactly .","Display_name":"Jincheng","Creater_id":120538,"Start_date":"2016-06-19 14:00:14","Question_id":219649}
{"_id":{"$oid":"5837a588a05283111e4d6944"},"Last_activity":"2016-08-01 19:33:33","Creator_reputation":8367,"Question_score":3,"Answer_content":"I agree with your thinking. If you want to maximize predictive accuracy, then in general, when trying to minimize training error in order to select model parameters, you should use the same metric for training error that you'll use for test error. (You might want to do something other than just minimize training error, as in regularization, but deliberately mistmatching training-error and test-error metrics is probably not a good way to do this.)So why do we see mismatches in practice? Partly it's because, as you indicate, estimating parameters according to the test-error metric may just be more difficult than using another metric. It may also be tradition and inertia. For example, in the case of logistic regression, people are used to fitting with MLE but evaluating predictions by discretizing the model outputs and using zero–one loss rather than using a proper scoring rule. Scoring rules are an obscure topic although logistic regression is a very popular technique.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-06-27 15:31:19","Question_id":220924}
{"_id":{"$oid":"5837a588a05283111e4d6951"},"Last_activity":"2016-08-01 15:10:08","Creator_reputation":147763,"Question_score":3,"Answer_content":"PreliminariesWrite\\mathcal{I}_p(\\epsilon) = \\int_0^\\infty p(x) \\log\\left(\\frac{p(x)}{(1+\\epsilon)p(x(1+\\epsilon))}\\right)\\, dx.The logarithms and the relationship between  and  suggest expressing both  and its argument as exponentials.  To that end, defineq(y) = \\log(p(e^y))for all real  for which the right hand side is defined and equal to  wherever .  Notice that the change of variables  entails  and (taking  to be the density of a distribution) that the Law of Total Probability can thereby be expressed as1 = \\int_0^\\infty p(x)dx = \\int_\\mathbb{R} e^{q(y)+y} dy.\\tag{1}Let us assume  when .  This rules out probability distributions  with infinitely many spikes in density near  or .  In particular, if the tails of  are eventually monotonic,  implies this assumption, showing it is not a severe one.To make working with the logarithms easier, also observe that1+\\epsilon = e^\\epsilon + O(\\epsilon^2).Because the following calculations will be performed up to multiples of , define\\delta = \\log(1+\\epsilon).We might as well replace  by , with  corresponding to  and positive  corresponding to positive .AnalysisOne obvious way in which the inequality can fail would be for the integral  to diverge for some .  This would happen if, for instance, there were to be any proper interval  of positive numbers, no matter how small, in which  were identically zero but  were not zero on the interval .  That would cause the integrand to be infinite with positive probability.Because the question is unspecific concerning the nature of , we could get bogged down in technical issues concerning how smooth  might be.  Let's avoid such issues, still hoping to gain some insight, by assuming that  everywhere has as many derivatives as we might care to use.  (Two will suffice if  is continuous.)  Because that guarantees  remains bounded on any bounded set, it implies that  is never zero when .Note that the question really concerns the behavior of  as  approaches zero from above. Since this integral is a continuous function of  in the interval , it attains some maximum  when  is restricted to any positive interval , enabling us to choose , because obviously c\\epsilon^2 = M_p(a) \\left(\\frac{\\epsilon}{a}\\right)^2 \\ge M_p(a) \\ge \\mathcal{I}_p(\\epsilon)makes the inequality work.  This is why we need only be concerned with the calculation modulo .SolutionUsing the changes of variable from  to , from  to , and  to , let's calculate  through second order in  (or ) in the hope of achieving a simplification.  To that end define\\mathcal{R}(y, \\delta) \\delta^2 = q(y+\\delta) - q(y) - \\delta q^\\prime(y)to be the order- remainder in the Taylor expansion of  around .\\eqalign{\\mathcal{I}_p(\\epsilon) \u0026amp;= \\int_\\mathbb{R}e^{q(y) + y} \\left(q(y) - q(y+\\delta) - \\delta\\right)\\, dy \\\\\u0026amp;=-\\int_\\mathbb{R}e^{q(y) + y} \\left(\\delta + \\delta q^\\prime(y) + \\mathcal{R}(y, \\delta) \\delta^2 \\right)\\, dy \\\\\u0026amp;= -\\delta\\int_\\mathbb{R}e^{q(y) + y} \\left(1+q^\\prime(y)\\right)\\, dy-\\delta^2\\int_\\mathbb{R}e^{q(y) + y}  \\mathcal{R}(y, \\delta)\\, dy.}Changing variables to  in the left hand integral shows it must vanish, as remarked in the assumption following .  Changing variables back to  in the right hand integral gives\\mathcal{I}_p(\\epsilon) = - \\delta^2 \\int_\\mathbb{R} p(x) \\mathcal{R}(\\log(x), \\delta)\\, dy = -\\delta^2 \\mathbb{E}_p\\left(\\mathcal{R}(\\log(x), \\delta)\\right).The inequality holds (under our various technical assumptions) if and only if  the coefficient of  on the right hand side is finite.InterpretationThis is a good point to stop, because it appears to uncover the essential issue:  is bounded by a quadratic function of  precisely when the quadratic error in the Taylor expansion of  doesn't explode (relative to the distribution) as  approaches .Let's check some of the cases mentioned in the question: the Exponential and Gamma distributions. (The Exponential is a special case of the Gamma.)  We never have to worry about scale parameters, because they merely change the units of measurement.  Only non-scale parameters matter.Here, because  for , q(y) = -e^y + k y - \\log\\Gamma(k+1).  The Taylor expansion around an arbitrary  is \\text{Constant} + (k-e^y)\\delta - \\frac{e^y}{2}\\delta^2 + \\cdots. Taylor's Theorem with Remainder implies  is dominated by  for sufficiently small .  Since the expectation of  is finite, the inequality holds for Gamma distributions.Similar calculations imply the inequality for Weibull distributions, Half-Normal distributions, Lognormal distributions, etc.  In fact, to obtain counterexamples we would need to violate at least one assumption, forcing us to look at distributions where  vanishes on some interval, or is not continuously twice differentiable, or has infinitely many modes.  These are easy tests to apply to any family of distributions commonly used in statistical modeling.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-08-01 15:10:08","Question_id":186167}
{"_id":{"$oid":"5837a588a05283111e4d695e"},"Last_activity":"2016-07-22 01:53:13","Creator_reputation":152738,"Question_score":2,"Answer_content":"The mean and variance are not actually finite in either the Gaussian nor the Laplace case.You can try to use a Taylor expansion as they did in the paper you mention, but for that to actually be correct, you need the Taylor series to converge.The problem arises because the denominator has non-zero density in the neighborhood of zero.If you bound the denominator away from zero (whereupon you no longer have the ratio of two Laplaces or of two normals) then it may be that the Taylor approach will work, but even then it would be an approximation, not an exact formula.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-22 01:53:13","Question_id":225033}
{"_id":{"$oid":"5837a588a05283111e4d696d"},"Last_activity":"2016-08-01 17:56:46","Creator_reputation":8055,"Question_score":2,"Answer_content":"One way to report the result would be to perform cross-validation, and report min, max, standard deviation and average. If you compare your results against some other method,  you can use some significance test such as approximate randomization. The performance analysis in the paper could try to explain why the train/test impacts the results more than expected.As a side note:many papers unfortunately only report one number (e.g., \"F1-score = 0.65\").in addition to the train/test split, different runs with different initialization might have quite some impact on the neural network's performance (e.g. see https://arxiv.org/abs/1603.03827 table 3)","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-08-01 17:56:46","Question_id":226670}
{"_id":{"$oid":"5837a588a05283111e4d6979"},"Last_activity":"2016-08-01 17:39:50","Creator_reputation":3398,"Question_score":1,"Answer_content":"You have a finite chain, in which case there is always a stationary distribution. In this case it looks like your chain is irreducible after removing vertex \"4\", so there's going to be a unique stationary distribution.","Display_name":"Alex R.","Creater_id":61092,"Start_date":"2016-08-01 12:36:20","Question_id":226740}
{"_id":{"$oid":"5837a588a05283111e4d6986"},"Last_activity":"2016-08-01 17:07:04","Creator_reputation":11,"Question_score":1,"Answer_content":"Just change the variable you are trying to predict to the difference in the dependent variable.As the other posts point out, the random forest will not know how to treat time variables that occur after the training set. Let's say your training set has data from Minute 1 to Minute 60. The random forest might make a rule that after forty minutes the dependent variable is 100. Even if there is a trend, if you get out to Minute 10000 in the test data, the same rule will be applied. If you predict the difference though, this can have the same effect of including a trend.As to whether RF's are decent forecasters, I have had MUCH greater luck with RF's than other econometric models like VAR, VECM, etc. but especially for short-term forecasts. Some other models do seem to work better on most data, however, such as well-tuned GBM models.","Display_name":"Anders Christiansen","Creater_id":125306,"Start_date":"2016-08-01 17:07:04","Question_id":175908}
{"_id":{"$oid":"5837a588a05283111e4d6987"},"Last_activity":"2015-10-07 19:52:58","Creator_reputation":385,"Question_score":2,"Answer_content":"RFs, of course, can identify and model a long-term trend in the data.  However, the issue becomes more complicated when you are trying to forecast out to never seen before values, as you often are trying to do with time-series data. For example, if see that activity increases linearly over a period between 1915 and 2015, you would expect it to continue to do so in the future.  RF, however, would not make that forecast.  It would forecast all future variables to have the same activity as 2015. from sklearn import ensembleimport numpy as npyears = np.arange(1916, 2016)#the final year in the training data set is 2015years = [[x] for x in years]print 'Final year is %s ' %years[-1][0]#say your ts goes up by 1 each year - a perfect linear trendts = np.arange(1,101)est = ensemble.RandomForestClassifier().fit(years,ts)print est.predict([[2013], [2014], [2015], [2016] , [2017], [2018]])The above script will print 2013, 2014, 2015, 2015, 2015, 2015.  Adding lag variables into the RF does not help in this regard.  So careful.  I'm not sure if adding trend data to your RF is gonna do what you think it will.","Display_name":"captain_ahab","Creater_id":44340,"Start_date":"2015-10-07 18:47:56","Question_id":175908}
{"_id":{"$oid":"5837a588a05283111e4d6993"},"Last_activity":"2016-07-29 07:08:23","Creator_reputation":3832,"Question_score":4,"Answer_content":"It isn't meaningful to run PCA on a univariate time series (or, more generally, a single vector). To run PCA on time series data, you'd need to have either a multivariate time series, or multiple univariate time series. There are ways to transform a univariate time series into a multivariate one (e.g. wavelet or time-frequency transforms, time delay embeddings, etc.). For example, the spectrogram of a univariate time series gives you the power at each frequency, for each moment in time.Say we have a multivariate time series with  dimensions/variables. Or, we might have a set of  univariate time series, where each time point has some common meaning across time series (e.g. time relative to some event). In both cases, there are  time points. There are a couple ways to run PCA:Consider each time point to be an observation. Dimensions correspond to variables of the multivariate time series, or to the different univariate time series. So, there are  points in a  dimensional space. In this case, eigenvectors correspond to instantaneous patterns across the dimensions/time series. At each moment in time, we represent the amplitude across dimensions/time series as a linear combination of these patterns. Consider each variable of the multivariate time series (or each univariate time series) to be an observation. Dimensions correspond to time points. So, there are  points in an -dimensional space. In this case, the eigenvectors correspond to temporal basis functions, and we're representing each time series as a linear combination of these basis functions.Given the above, it's apparent why PCA doesn't make sense for a single univariate time series. Either you have  observations and 1 dimension (in which case there's nothing for PCA to do), or you have a single observation with  dimensions (in which case the problem is completely underdetermined and all solutions are equivalent).","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-07-29 06:25:06","Question_id":226281}
{"_id":{"$oid":"5837a588a05283111e4d6994"},"Last_activity":"2016-07-29 06:47:18","Creator_reputation":19151,"Question_score":2,"Answer_content":"PCA on a single time series can be done, of course. The result will be one principal component, which will be equal to the original series. Hence, technically it'll work, but it'll be pointless: you'll get your input series in the output.Here's a MATLAB example. I got the PCA of a random series, then plotted the only principal component against the original series to show that it's the same thing. I also show you the differences between to series (adjusted for mean) is zero.x=randn(10,1);[~,score,~,~,~,mu]=pca(x);scatter(x,score);max(abs(x-score-mu))ans =   4.4409e-16","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-07-29 06:36:05","Question_id":226281}
{"_id":{"$oid":"5837a588a05283111e4d69a1"},"Last_activity":"2014-12-03 15:19:44","Creator_reputation":30065,"Question_score":9,"Answer_content":"Q1: What is the connection between PC time series and \"maximum variance\"?The data that they are analyzing are  data points for each of the  neurons, so one can think about that as  data points in the -dimensional space . It is \"a cloud of points\", so performing PCA amounts to finding directions of maximal variance, as you are well aware. I prefer to call these directions (which are eigenvectors of the covariance matrix) \"principal axes\", and the projections of the data onto these directions \"principal components\". When analyzing time series, the only addition to this picture is that the points are meaningfully ordered, or numbered (from  to ), as opposed to being simply an unordered collection of points. Which means that if we take firing rate of one single neuron (which is one coordinate in the ), then its values can be plotted as a function of time. Similarly, if we take one PC (which is a projection from  on some line), then it also has  values and can be plotted as a function of time. So if original features are time series, then PCs are also time series.I agree with @Nestor's interpretation above: each original feature can be then seen as a linear combination of PCs, and as PCs are uncorrelated between each other, one can think of them as basis functions that the original features are decomposed into. It's a little bit like Fourier analysis, but instead of taking fixed basis of sines and cosines, we are finding the \"most appropriate\" basis for this particular dataset, in a sense that first PC accounts for most variance, etc.\"Accounting for most variance\" here means that if you only take one basis function (time series) and try to approximate all your features with it, then the first PC will do the best job. So the basic intuition here is that the first PC is a basis function time series that fits all the available time series the best, etc.Why is this passage in Freeman et al. so confusing?Freeman et al. analyze the data matrix  with variables (i.e. neurons) in rows (!), not in columns. Note that they subtract row means, which makes sense as variables are usually centred prior to PCA. Then they perform SVD: \\hat {\\mathbf Y} = \\mathbf{USV}^\\top. Using the terminology I advocate above, columns of  are principal axes (directions in ) and columns of  are principal components (time series of length ).The sentence that you quoted from Freeman et al. is quite confusing indeed:  The principal components (the columns of ) are vectors of length , and the scores (the columns of ) are vectors of length  (number of voxels), describing the projection of each voxel on the direction given by the corresponding component, forming projections on the volume, i.e. whole-brain maps.First, columns of  are not PCs, but PCs scaled to unit norm. Second,  columns of  are NOT scores, because \"scores\" usually means PCs. Third, \"direction given by the corresponding component\" is a cryptic notion. I think that they flip the picture here and suggest to think about  points in -dimensional space, so that now each neuron is a data point (and not a variable). Conceptually it sounds like a huge change, but mathematically it makes almost no difference, with the only change being that principal axes and [unit-norm] principal components change places. In this case, my PCs from above (-long time series) will become principal axes, i.e. directions, and  can be thought as normalized projections on these directions (normalized scores?).I find this very confusing and so I suggest to ignore their choice of words, but only look at the formulas. From this point on I will keep using the terms as I like them, not how Freeman et al. use them.Q2: What are the state space trajectories?They take single-trial data and project it onto the first two principal axes, i.e. the first two columns of ). If you did it with the original data , you would get two first principal components back. Again, projection on one principal axis is one principal component, i.e. a -long time series. If you do it with some single-trial data , you again get two -long time series. In the movie, each single line corresponds to such projection: x-coordinate evolves according to PC1 and y-coordinate according to PC2. This is what is called \"state space\": PC1 plotted against PC2. Time goes by as the dot moves around.Each line in the movie is obtained with a different single trial .","Display_name":"amoeba","Creater_id":28666,"Start_date":"2014-11-26 08:33:56","Question_id":125462}
{"_id":{"$oid":"5837a588a05283111e4d69a2"},"Last_activity":"2014-11-26 09:06:01","Creator_reputation":2178,"Question_score":1,"Answer_content":"With respect to the first question. Consider the whole time series through a particular voxel to be a single draw from a multivariate distribution. We can now think of this as a multivariate vector much like any other that we might apply PCA to. The first  columns of  are then the eigen-timecourses which, when linearly combined provide the best approximation to the time course through a particular voxel for the duration  of a stimulus.So  is an  matrix and therefore  is  while  is .With respect to the second question. The equation given is We are given that  is a 2 or 3  matrix. (This involves a small sleight of hand in dropping rows/columns.) Two or three is picked as the dimensionality as this is what can be plotted in figure 6 of the paper. However   so I expect the separate traces (lines in fig 6) have been obtained by chopping  into the different segments corresponding to presentations of the stimulus. Each of these blocks can then be plotted in 2 or 3 dimensional space by considering each column as a point in that space and then drawing a line between the points defined by adjacent columns giving the trajectories.Following on from the above video 8 appears for each block to add each (column-)point sequentially, join it to the last point, and render this length  sequence as a video.I've not dealt with the colouring methodology before, and it would take a while before I was confident to comment on that aspect. I found the comment on similarity to Fig 4c confusing as the colouring is obtained there by per-voxel regression. Whereas in Fig 6 each trace is a whole-image artefact. Unless I'm put straight I think it's the direction of the stimulus during that time segment as per the comment in the Figure.","Display_name":"conjectures","Creater_id":16663,"Start_date":"2014-11-26 07:33:53","Question_id":125462}
{"_id":{"$oid":"5837a588a05283111e4d69af"},"Last_activity":"2016-08-01 16:35:34","Creator_reputation":30065,"Question_score":45,"Answer_content":"I will focus this answer on the specific question of what are the alternatives to -values. There are 21 discussion papers published along with the ASA statement (as Supplemental Materials): by Naomi Altman, Douglas Altman,Daniel J. Benjamin, Yoav Benjamini, Jim Berger, Don Berry, John Carlin, George Cobb, Andrew Gelman, Steve Goodman, Sander Greenland, John Ioannidis, Joseph Horowitz, ValenJohnson, Michael Lavine, Michael Lew, Rod Little, Deborah Mayo, Michele Millar, CharlesPoole, Ken Rothman, Stephen Senn, Dalene Stangl, Philip Stark and Steve Ziliak (some of them wrote together; I list all for future searches). These people probably cover all existing opinions about -values and statistical inference.I have looked through all 21 papers.Unfortunately, most of them do not discuss any real alternatives, even though the majority are about the limitations, misunderstandings, and various other problems with -values (for a defense of -values, see Benjamini, Mayo, and Senn). This already suggests that alternatives, if any, are not easy to find and/or to defend.So let us look at the list of \"other approaches\" given in the ASA statement itself (as quoted in your question):  [Other approaches] include methods that  emphasize estimation over testing, such as confidence, credibility, or prediction intervals;  Bayesian methods; alternative measures of evidence, such as likelihood ratios or Bayes Factors;  and other approaches such as decision-theoretic modeling and false discovery rates.Confidence intervalsConfidence intervals are a frequentist tool that goes hand-in-hand with -values; reporting a confidence interval (or some equivalent, e.g., mean  standard error of the mean) together with the -value is almost always a good idea.Some people (not among the ASA disputants) suggest that confidence intervals should replace the -values. See this thread (and my answer therein) about such a suggestion by Norm Matloff and about why one would still like to have the -values reported: What is a good, convincing example in which p-values are useful?Some other people (not among the ASA disputants either), however, argue that confidence intervals, being a frequentist tool, are as misguided as -values and should also be disposed of. See, e.g., Morey et al. 2015, The Fallacy of Placing Confidence in Confidence Intervals linked by @Tim here in the comments. This is a very old debate.Bayesian methods(I don't like how the ASA statement formulates the list. Credible intervals and Bayes factors are listed separately from \"Bayesian methods\", but they are obviously Bayesian tools. So I count them together here.)There is a huge and very opinionated literature on the Bayesian vs. frequentist debate. See, e.g., this recent thread for some thoughts: When (if ever) is a frequentist approach substantively better than a Bayesian? Bayesian analysis makes total sense if one has good informative priors, and everybody would be only happy to compute and report  instead of \u0026mdash;but alas, people usually do not have good priors. An experimenter records 20 rats doing something in one condition and 20 rats doing the same thing in another condition; the prediction is that the performance of the former rats will exceed the performance of the latter rats, but nobody would be willing or indeed able to state a clear prior over the performance differences. Die-hard Bayesians suggest to use Bayesian methods even if one does not have any informative priors. One recent example is Krushke, 2012, Bayesian estimation supersedes the -test, humbly abbreviated as BEST. The idea is to use a Bayesian model with weak uninformative priors to compute the posterior for the effect of interest (such as, e.g., a group difference). The practical difference with frequentist reasoning seems usually to be minor, and as far as I can see this approach remains unpopular. (On the other hand, this is exactly the approach that @FrankHarrell says is his favourite in the comments to his answer.) See What is an \u0026quot;uninformative prior\u0026quot;? Can we ever have one with truly no information? for the discussion of what is \"uninformative\" (answer: there is no such thing, hence the controversy).An alternative approach, going back to Harold Jeffreys, is based on Bayesian testing (as opposed to Bayesian estimation) and uses Bayes factors. One of the more eloquent and prolific proponents is Eric-Jan Wagenmakers, who has published a lot on this topic in recent years. Two features of this approach are worth emphasizing here. First, see Wetzels et al., 2012, A Default Bayesian Hypothesis Test for ANOVA Designs for an illustration of just how strongly the outcome of such a Bayesian test can depend on the specific choice of an uninformative prior. Second, once a particular type of prior is chosen (Wagenmakers advertises Jeffreys' so called \"default\" priors), resulting Bayes factors turn out to be quite consistent with the standard -values, see e.g. this figure from this preprint by Marsman \u0026amp; Wagenmakers: So while Wagenmakers et al. keep insisting that -values are deeply flawed and Bayes factors are the way to go, one cannot but wonder...For completeness, I mention that Wagenmakers 2007, A practical solution to the pervasiveproblems of -values suggested to use BIC as an approximation to Bayes factor to replace the -values. BIC does not depend on the prior and hence, despite its name, is not really Bayesian; I am not sure what to think about this proposal. It seems that more recently Wagenmakers is more in favour of Bayesian tests with uninformative Jeffreys' priors, see above.Worst-case bounds on Bayes factorsI am not sure what is the appropriate name for these approaches. Among the ASA disputants, this is explicitly suggested by Benjamin \u0026amp; Berger and by Valen Johnson (the only two papers that are all about suggesting a concrete alternative). I am not very familiar with these suggestions so take it with a grain of salt, but they appear quite similar to me.The ideas of Berger go back to the Berger \u0026amp; Sellke 1987 and there are a number of papers by Berger, Sellke, and collaborators up until last year elaborating on this work. The idea is that under a spike and slab prior where point null  hypothesis gets probability  and all other values of  get probability  spread symmetrically around , then the minimal posterior  over all priors is much higher than the -value. This is the basis of the (much contested) claim that -values \"overstate the evidence\" against the null. The suggestion is to use a lower bound on Bayes factor in favour of the null instead of the -value; under some broad assumptions this lower bound turns out to be given by , i.e., the -value is effectively multiplied by  which is a factor of around  to  for the common range of -values. This approach has been endorced by Steven Goodman too.Valen Johnson suggested something similar in his PNAS 2013 paper; his suggestion approximately boils down to multiplying -values by  which is around  to .For a brief critique of Johnson's paper, see Andrew Gelman's and @Xi'an's reply in PNAS. For the counter-argument to Berger \u0026amp; Sellke 1987, see Casella \u0026amp; Berger 1987 (different Berger!). Among the APA discussion papers, Stephen Senn argues explicitly against any of these approaches:  Error probabilities are not posterior probabilities. Certainly, there is much more to statistical analysis than -values but they should be left alone rather than being deformed in some way to become second class Bayesian posterior probabilities.See also references in Senn's paper, including the ones to Mayo's blog.ASA statement lists \"decision-theoretic modeling and false discovery rates\" as another alternative. I have no idea what they are talking about, and I was happy to see this stated in the discussion paper by Stark:  The \"other approaches\" section ignores the fact that the assumptions of  some of those methods are identical to those of -values. Indeed, some of  the methods use -values as input (e.g., the False Discovery Rate).I am highly skeptical that there is anything that can replace -values in actual scientific practice such that the problems that are often associated with -values (replication crisis, -hacking, etc.) would go away.To quote from Andrew Gelman's discussion paper:  In summary, I agree with most of the ASA’s statement on -values but I feel that the problems  are deeper, and that the solution is not to reform -values or to replace them with some other  statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty  and embracing of variation.And from Stephen Senn:  In short, the problem is less with -values per se but with making an idol of them. Substituting another false god will not help.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2016-03-14 06:33:11","Question_id":200500}
{"_id":{"$oid":"5837a588a05283111e4d69b0"},"Last_activity":"2016-04-01 04:28:07","Creator_reputation":3632,"Question_score":2,"Answer_content":"A Brilliant forecaster Scott Armstrong from Wharton published an article a almost 10 years ago titled Significance Tests Harm Progress in Forecasting in the international journal of forecasting a journal that he co-founded. Even though this is in forecasting, it could be generalized to any data analysis or decision making. In the article he states that:  \"tests of statistical significance harms scientific progress. Efforts  to find exceptions to this conclusion have, to date, turned up none.\"This is an excellent read for any one interested in antithetical view of significance testing and P values. The reason why I like this article is because Armstrong provides alternatives to significance testing which is succinct and could be easily understood especially for a non-statistician like me. This is much better in my opinion than the ASA article cited in the question: All of which I continue to embrace and ever since stopped using significance testing or looking at P values except when I do randomized experimental studies or quasi experiments. I must add randomized experiments are very rare in practice except in pharmaceutical industry/life sciences and in some fields in Engineering.","Display_name":"forecaster","Creater_id":29137,"Start_date":"2016-03-31 19:27:47","Question_id":200500}
{"_id":{"$oid":"5837a588a05283111e4d69b1"},"Last_activity":"2016-04-01 03:57:48","Creator_reputation":735,"Question_score":1,"Answer_content":"My choice would be to continue using p values, but simply adding confidence/credible intervals, and possibly for the primary outcomes prediction intervals. There is a very nice book by Douglas Altman (Statistics with Confidence, Wiley), and thanks to boostrap and MCMC approaches, you can always build reasonably robust intervals.","Display_name":"Giuseppe Biondi-Zoccai","Creater_id":107799,"Start_date":"2016-03-08 03:49:28","Question_id":200500}
{"_id":{"$oid":"5837a588a05283111e4d69b2"},"Last_activity":"2016-03-14 06:48:02","Creator_reputation":2214,"Question_score":16,"Answer_content":"Here is my two cents.I think that at some point, many applied scientists stated the following \"theorem\":  Theorem 1:  and most of the bad practices come from here. The -value and scientific inductionI used to work with people using statistics without really understanding it and here is some of the stuff I see:running many possible tests/reparametrisations (without looking once at the distribution of the data) until finding the \"good\" one: the one giving ;trying different preprocessing (e.g. in medical imaging) to get the data to analyse until getting the one giving ;reach  by applying one-tailed t-test in the positive direction for the data with positive effect and in the negative direction for the data with negative effect (!!).All that is done by well-versed, honest scientists  having no strong sensation of cheating. Why ? IMHO, because of Theorem 1. At a given moment, applied scientist may believe strongly in their hypothesis. I even suspect that they believe they known they are true and the fact is that in many situations they have seen data from years, have thought about them while working, walking, sleeping... and they are the best to say something about the answer to this question. The fact is, in their mind (sorry I think I look a bit arrogant here), by Theorem 1 if they hypothesis is true, the -value must be lower than  ; no matter what the amount of data is, how they are distributed, the alternative hypothesis, the size effect, the quality of the data acquisition. If the -value is not  and the hypothesis is true, then something is not correct: the preprocessing, the choice of test, the distribution, the acquisition protocol... so we change them... -value  is just the ultimate key of scientific induction.To this point, I agree with the two previous answers that confidence intervals or credible intervals make the statistical answer more proper to the discussion and to the interpretation. While -value is difficult to interpret (IMHO) and ends the discussion, interval estimates can serve a scientific induction illustrated by objective statistics but lead by expert arguments.The -value and the alternative hypothesisAnother consequence of Th.1 is that if -value then the alternative hypothesis is false. Again this is something that I encounter many times :try to compare (just because we have the data) a hypothesis of the type : take randomly 10 data-points for each of the two groups, compute the -value for . Find , notice in some part of the brain that there is no difference between the two groups.A main issue with the -value is that the alternative is never mentioned while I think in many cases this could help a lot. A typical example is point 4., where I proposed to my colleague to compute posterior ratio for  vs.  and get something like 3 (I know this figure is ridiculously low). The researcher asks me if it means that the probability that  is 3 times stronger than those . I answered that this is a way to interpret it and she finds this amazing and that she should look at more data and write a paper... My point is not that this \"3\" helps her to understand that there is something in the data (again 3 is clearly anedoctic) but that it underlines that she misinterprets the p-value as \"p-value\u003e0.05 means nothing interesting/equivalent groups\". So in my opinion, always at least discussing the alternative hypothesis (es!) is mandatory, allows to avoid simplification, gives element to debate.Another related case is when experts want to :test . For that they test and reject  then conclude  using the fact that the ML estimates are ordered.Mentioning the alternative hypothesis is the only solution to solve this case. So using posterior odds, Bayes factor or likelihood ratio conjointly with confidence/credible intervals seems to reduce the main involved issues.  The common misinterpretation of -value / confidence intervals is a relatively minor flaw (in practice)While I am a Bayesian enthusiast, I really think that the common misinterpretation of -value and CI (i.e. the -value is not the probability that the null hypothesis is false and the CI is not the interval that contains the parameter value with 95% chance) is not the main concern for this question (while I am sure this is a major point from a philosophical point of view). The Bayesian/Frequentist view have both pertinent answers to help practitioner in this \"crisis\".  My two cents conclusionUsing credible interval and Bayes factor or posterior odds is what I try to do in my practice with experts (but am also enthusiast in CI+likelihood ratio). I came to statistics a few years ago mainly by self-studying from the web (so many thanks to Cross Validated !) and so grew up with the numerous agitations around -values. I do not know if my practice is a good one but it is what I pragmatically find as a good compromise between being efficient and making my job properly.","Display_name":"peuhp","Creater_id":14346,"Start_date":"2016-03-09 00:57:04","Question_id":200500}
{"_id":{"$oid":"5837a588a05283111e4d69b3"},"Last_activity":"2016-03-11 10:05:16","Creator_reputation":39366,"Question_score":15,"Answer_content":"The only reasons I continue to use -values areMore software is available for frequentist methods than Bayesian methods.Currently, some Bayesian analyses take a long time to run.Bayesian methods require more thinking and more time investment.  I don't mind the thinking part but time is often short so we take shortcuts.The bootstrap is a highly flexible and useful everyday technique that is more connected to the frequentist world than to the Bayesian.-values, analogous to highly problematic sensitivity and specificity as accuracy measures, are highly deficient in my humble opinion.  The problem with all three of these measures is that they reverse the flow of time and information.  When you turn a question from \"what is the probability of getting evidence like this if the defendant is innocent\" to \"what is the probability of guilt of the defendant based on the evidence\", things become more coherent and less arbitrary.  Reasoning in reverse time makes you have to consider \"how did we get here?\" as opposed to \"what is the evidence now?\".  -values require consideration of what could have happened instead of what did happen.  What could have happened makes one have to do arbitrary multiplicity adjustments, even adjusting for data looks that might have made an impact but actually didn't.When -values are coupled with highly arbitrary decision thresholds, things get worse.  Thresholds almost always invite gaming.Except for Gaussian linear models and the exponential distribution, almost everything we do with frequentist inference is approximate (a good example is the binary logistic model which causes problems because its log likelihood function is very non-quadratic).  With Bayesian inference, everything is exact to within simulation error (and you can always do more simulations to get posterior probabilities/credible intervals).","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2016-03-11 10:05:16","Question_id":200500}
{"_id":{"$oid":"5837a588a05283111e4d69b4"},"Last_activity":"2016-03-08 08:30:19","Creator_reputation":3724,"Question_score":2,"Answer_content":"What is preferred and why must depend on the field of study. About 30 years ago articles started appearing in medical journals suggesting that -values should be replaced by estimates with confidence intervals. The basic reasoning was that -values just tell you the effect was there whereas the estimate with its confidence interval tells you how big it was and how precisely it has been estimated. The confidence interval is particularly important when the -value fails to reach the conventional level of significance because it enables the reader to tell whether this is likely due to there genuinely being no difference or the study being inadequate to find a clinically meaningful difference.Two references are:@article{langman86,   author = {Langman, M J S},   title = {Towards estimation and confidence intervals},   journal = {British Medical Journal},   year = {1986},   volume = {292},   pages = {716},   keywords = {confidence intervals}}@article{gardner86b,   author = {Gardner, M J and Altman, D G},   title = {Confidence intervals rather than {P} values: estimation      rather than hypothesis testing},   journal = {British Medical Journal},   year = {1986},   volume = {292},   pages = {746--750},   keywords = {confidence intervals}}","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-03-08 08:30:19","Question_id":200500}
{"_id":{"$oid":"5837a588a05283111e4d69c1"},"Last_activity":"2016-08-01 16:18:41","Creator_reputation":8367,"Question_score":0,"Answer_content":"You can build whatever models you like, but it's all meaningless because you have no idea what the data represents. Data analysis can only tell you anything if you already know where the data comes from. Trying to figure out where a mystery dataset came from is outside the realm of data analysis as it is usually understood.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-08-01 16:18:41","Question_id":226759}
{"_id":{"$oid":"5837a588a05283111e4d69d2"},"Last_activity":"2016-08-01 10:29:06","Creator_reputation":4134,"Question_score":1,"Answer_content":"No, there is no such threshold. Considerations like that will always depend on the setting and the role of the variable in the analysis.Some examples:You have a response variable in a model with 20% missing values. If you remove this column, there will be nothing to analyze. You would just remove the lines without response and mention that 20% rows without response had to be discarded.You want to compute the correlation between two key variables. Even if there are many missing values in the two columns, you cannot just remove a column.","Display_name":"Michael M","Creater_id":30351,"Start_date":"2016-08-01 10:29:06","Question_id":226721}
{"_id":{"$oid":"5837a588a05283111e4d69df"},"Last_activity":"2016-08-01 13:03:14","Creator_reputation":23,"Question_score":1,"Answer_content":"After I centered ANGLE and added ANGLE^2, I used the following model:dataANGLE)-30 #centered variabledataANGLE)^2fm2 \u0026lt;- clmm(CERTAINTY ~ ANGLE*COND +ANGLE1*COND +(1+ANGLE|SUB), data = data,Hess=TRUE)The output now shows a much lower correlation of random effects (0.040):     Cumulative Link Mixed Model fitted with the Laplace approximationformula: CERTAINTY ~ ANGLE * COND + ANGLE1 * COND + (1 + ANGLE | SUB)data:    data link  threshold nobs logLik   AIC     niter      max.grad cond.H  logit flexible  1178 -1460.26 2948.52 1542(9319) 1.68e-01 7.3e+05Random effects: Groups Name        Variance Std.Dev. Corr   SUB    (Intercept) 0.85102  0.9225                 ANGLE       0.02647  0.1627   0.040 Number of groups:  SUB 44 Coefficients:                   Estimate Std. Error z value Pr(\u0026gt;|z|)    ANGLE             -0.021042   0.036965  -0.569 0.569197    CONDvisual         0.259612   0.316720   0.820 0.412394    ANGLE1             0.023086   0.001605  14.385  \u0026lt; 2e-16 ***ANGLE:CONDvisual   0.013147   0.051909   0.253 0.800063    CONDvisual:ANGLE1 -0.006120   0.001837  -3.332 0.000862 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Threshold coefficients:    Estimate Std. Error z value1|2  -3.7424     0.3212 -11.6512|3  -2.5498     0.2595  -9.8253|4  -1.4684     0.2368  -6.2024|5  -0.3946     0.2289  -1.7245|6   0.9193     0.2307   3.9856|7   2.3037     0.2425   9.499Is my model now specified correctly?","Display_name":"alen hajnal","Creater_id":124210,"Start_date":"2016-08-01 13:03:14","Question_id":225193}
{"_id":{"$oid":"5837a588a05283111e4d69e0"},"Last_activity":"2016-07-23 11:37:22","Creator_reputation":5445,"Question_score":2,"Answer_content":"Neither model is \"correct\", statistically:  \"Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful.\"    Box, G. E. P.; Draper, N. R. (1987), Empirical Model-Building and Response Surfaces, John Wiley \u0026amp; Sons.The first problem I see with both models is that the correlation between the random effects is estimated as . This is an indication of numerical and/or identification problems.  You have specified trialB as a random coeffcient  which estimates a random slope for the variable, however you haven't specified it as a fixed effect. Normally a variable that is a random coefficient would also be a fixed effect and the random effect. The fixed effect would be the \"global\" effect for the variable (ie an overall slope) and the random slopes would give each subject their own slope as a deviation from the overall slope. By excluding it as a fixed effect you are essentially saying that the overall slope is zero (because random effects have a mean of zero). If you really want random slopes for trialB I would suggest adding it as a fixed effect as well and see if a more reasonable correlation is estimated. If not, then it would be better to remove it as a random effect.The second problem I see is that, in your OP, you say  there should be an effect of ANGLE such that certainty is minimal around 30 degreesSo I interpret that is meaning you expect a non-linear association between the response variable and ANGLE. However in your first model you not fitting any nonlinear terms for ANGLE. So I would suggest adding a quadratic term. It would be a good idea to centre ANGLE first to avoid collinearity between the linear and quadratic terms.The second model estimates a fixed effect for each level of ANGLE. This is another way to model non-linear change, though it is little harder to interpret if there are many levels. The fact that you see estimates that are not changing linearly  and in particular, that they are all negative with a minimum value for ANGLE130 indicates that you do have nonlinearity and in particular that the minimum is at ANGLE=30, as you hypothesized.So I would say that, while neither model is \"correct\", the second one is better than the first, though I would rather keep ANGLE numeric and introduce a quadratic term into the model, since it is easier to interpret and more parsimonious.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-23 06:26:27","Question_id":225193}
{"_id":{"$oid":"5837a588a05283111e4d69f5"},"Last_activity":"2016-08-01 12:43:21","Creator_reputation":12280,"Question_score":2,"Answer_content":"Yes, there are other ways to create weights. Have you considered things like:What percentage of all emails that Person A received were from Person B? What percentage of all emails that Person A received did they reply to? What percentage of the emails that Person B sent were to Person A? And what percentage of all emails that Person B sent were replied to?You correctly sense that one email from Person B to A that is then replied to by A shouldn't count as a high weighting (i.e. as a 100% response rate). So come up with weights that reflect your insight.What other pieces of information about Person A and Person B do you know? Is Person A three levels up the hierarchy from Person B, or are they their supervisor or a coworker? Are there standardized titles in the organization, and do some jobs tend to send more emails -- that require responses -- than others?You say Person B is new, while Person A is well-established. Could you factor in longevity? (Maybe emails sent last month, average number of emails received per month over the last three months, lifetime number of emails sent/received, or something like that?)Do you know -- or can you determine -- the nature of the email? For example, can you do a little text mining to determine if the email was about Vacation, or scheduling a meeting? Might attachment names/types, or message length tell you something? How about what time of the day the emails were sent, or how quickly they were replied to?The actual analysis you do in the end is another matter. What algorithm do you use? But the key part of most work is to make sure you have correct data, make sure you have as much data as possible, and then engineer features you will use in your algorithm. In the real world, you rarely feed found data directly into an algorithm.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-08-01 12:12:54","Question_id":187848}
{"_id":{"$oid":"5837a588a05283111e4d69f6"},"Last_activity":"2015-12-22 01:53:38","Creator_reputation":21,"Question_score":2,"Answer_content":"You do not need an alternative to graph theory.. just extend the complexity of your graph. At a minimum label your edges with relationship type. Can you establish a hierarchy within the graph using a \"reports to\" edge type.I like neo4j for smaller analysis like this.If you are actually looking for a full alternative, then consider set theory. Which translates well to SQL. Either way for what you seem to be after you will need a richer data structure.","Display_name":"user98865","Creater_id":98865,"Start_date":"2015-12-22 01:53:38","Question_id":187848}
{"_id":{"$oid":"5837a588a05283111e4d6a03"},"Last_activity":"2016-05-30 14:07:14","Creator_reputation":3645,"Question_score":4,"Answer_content":"Unless the assumptions of regression modeling have changed, there is no stipulation about the distributions of the variables in the model -- normal or otherwise. There are some technical assumptions about the behavior of the residuals from the model but even those are subject to interpretation in the \"art and practice\" of modeling. This CV thread ( What is a complete list of the usual assumptions for linear regression?  ) contains an excellent discussion of the various ins and outs of these assumptions. In particular, the comments between @AndyW and WHuber are illuminating. AndyW states, \"There is no cook book, nor should there be given the potential variety of situations that linear regression could encompass.\" Which Whuber challenges, noting that he's extending the discussion into the \"art and science\" realm. ","Display_name":"DJohnson","Creater_id":82102,"Start_date":"2016-05-30 14:07:14","Question_id":215452}
{"_id":{"$oid":"5837a588a05283111e4d6a0f"},"Last_activity":"2016-08-01 07:04:19","Creator_reputation":117,"Question_score":1,"Answer_content":"The second part of the ARIMA model (P,D,Q) corresponds to the seasonal component (12 indicates the number of periods per season). In particular, the seasonal component (0,0,1) indicates a spike at lag 12 in the ACF but no other significant spikes, and The PACF will show exponential decay in the seasonal lags; that is, at lags 12, 24, 36. See explanation here. ","Display_name":"Andres Azqueta","Creater_id":111750,"Start_date":"2016-08-01 07:04:19","Question_id":223297}
{"_id":{"$oid":"5837a588a05283111e4d6a26"},"Last_activity":"2016-08-01 11:36:09","Creator_reputation":17464,"Question_score":1,"Answer_content":"The ratio you are modeling is BMD at follow-up over BMD at baseline. If the two values are the same the ratio is 1. But ratios are not additive quantities, so you deal with their log transform. A ratio representing no-change on the log-scale is 0.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-08-01 11:36:09","Question_id":226605}
{"_id":{"$oid":"5837a588a05283111e4d6a33"},"Last_activity":"2016-08-01 11:34:14","Creator_reputation":101,"Question_score":0,"Answer_content":"Keep the 2010 dates - but make the values 'nan' .  If you have a timeseries Dataframe, then it's as simple as:newDf = old.set_value('2010', 'Total Sales', float('nan').if your data drop out isn't exactly 2010, you can replace 0s with nans:new = old.replace([0], float('nan'))This will cause a \"pen lift\" (if you can imagine an old pen plotter, nan caused a pen lift, therefore matlab, and consequently matplotlib, emulated that behavior).If you do this, you need to make sure your analysis routines can handle nans properly.  (particularly any time filtering, like MA, across the gap.Finally, I would strongly suggest moving this kind of question to StackOverflow, since it's more of a Pandas question than an analytical question.","Display_name":"Marc","Creater_id":77330,"Start_date":"2016-08-01 11:34:14","Question_id":221658}
{"_id":{"$oid":"5837a588a05283111e4d6a40"},"Last_activity":"2016-08-01 11:11:02","Creator_reputation":13091,"Question_score":1,"Answer_content":"The purpose of your modelling could be, e.g., (1) descriptive, (2) explanatory or (3) predictive.If (1), smoothing could be useful. You could elicit the slow moving trend and use that for data visualizations. You would see the relations between the slowly-moving trend components of the different series more clearly than using the original series. Of course, you would have to acknowledge that smoothing has taken place and that the relations you have elicited only hold for smoothed components, while the real variables are more erratic. If (2), directly using smoothed variables would mess up point estimates and their standard errors in your models. Therefore, you could not test hypotheses in a straightforward way. Time series decomposition (see below) could probably be helpful here.If (3), instead of smoothing you could try decomposing the time series in the slow-moving trend, seasonal and remainder components. Then you could try modelling and forecasting each of them separately, and then put these forecasts together to obtain a forecast of the original variable. On the other hand, pure smoothing could make you lose valuable information.Your case seems to be explanatory. If you are interested in the long-term relation between variables, you should probably use time series decomposition and interpret your findings accordingly. That is, you should not claim a relationship between the original variables but just between specific components. You should then also think whether that has a sensible subject-matter interpretation.Edit (after an edit of the question):Removing the noise from the independent variable by smoothing gives you higher  (as you note), but this is an artefact of smoothing, so it should be taken with a grain of salt. Once you have smoothed the independent variable, you should not be making direct inference w.r.t. the original variable. That is something to be careful about -- see my point (2) above. However, in your case it seems that smoothing could make sense as the dependent variable at time  does not depend on the regressor as of a precise time point in the past, but rather over a time interval. Thus you would explicitly define your regressor as a smooth version of the original variable and you would make inference with respect to this smoothed regressor. That could work.If you smooth the dependent variable, too, you will probably increase the  even more, but you will depart even further from direct interpretation, because again the change in  will be an artefact of smoothing.As an alternative, you could probably sample your data less frequently. Then you should see more signal relative to noise (as signal would accumulate between the infrequent sample points while noise would not), but you would still be able to interpret the results directly (unlike in the case of smoothing). However, this approach could immediately be criticized as throwing away data. There could probably be better alternatives. If the smoothed regressor makes sense on its own, you do not need to do this.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-30 10:07:03","Question_id":226391}
{"_id":{"$oid":"5837a588a05283111e4d6a4d"},"Last_activity":"2016-08-01 10:45:19","Creator_reputation":13091,"Question_score":1,"Answer_content":"This error message seems to indicate not that the number of variables exceeds the number of observations but rather that the exogenous variables have a different number of observations than the dependent variable. You should make sure that the variable lengths coincide, i.e. dim(xreg)[1]==length(y).  For example, if we have 38 observations, should we cut off the PCA matrix to 38 rows?You would use the same (sub-)sample for running the PCA and fitting the regression with ARIMA errors. Therefore, there should not be a discrepancy in variable lengths at this stage. Next, when you do -step ahead forecasting, you need to make sure to supply -long predicted vectors of the exogenous variables.  Another approach could be to use the PCs to predict all the regressors and then use these predictions in the xreg parameter.Obtaining PCs from regressors first and reconstructing regressors back from PCs does not seem to add value in this exercise. But probably I do not get your idea.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-08-01 10:45:19","Question_id":226587}
{"_id":{"$oid":"5837a588a05283111e4d6a5a"},"Last_activity":"2016-07-29 23:41:48","Creator_reputation":178,"Question_score":0,"Answer_content":"I am not fully aware of your system (and therefore do not understand why you would need random sampling here at all). However, as I understand it, one thing that seems not correct is assigning different weights to your randomly drawn samples. This is not necessary as the weighting is already implicitly considered by the fact that they were drawn from the normal distribution and therefore automatically more samples are placed around the mean. Your approach might not harm the computation of the mean but, if understood correctly, it most certainly harms the computation of the covariance.","Display_name":"Igor","Creater_id":18530,"Start_date":"2016-07-29 23:41:48","Question_id":226291}
{"_id":{"$oid":"5837a588a05283111e4d6a8b"},"Last_activity":"2016-08-01 02:07:02","Creator_reputation":118,"Question_score":0,"Answer_content":"I'm going to post an answer here as a proposal, adding a second parameter to the base distribution.I would define the model as follows:\\begin{align}x~|~\\mu_i\u0026amp;\\sim N(\\mu_i, \\sigma^2_i)\\\\\\mu_i,\\sigma^2_i~|~G\u0026amp;\\sim G\\\\G\u0026amp;\\sim\\text{DP}(\\alpha,G_0)\\\\\\end{align}where  is a distribution over  such that\\begin{align}G_0^\\mu\u0026amp;= N(0, 10)\\\\G_0^{\\sigma^2}\u0026amp;= \\text{Inv-Gamma}(3, 1)\\end{align}I think that this is fairly clear, but I would be interested to hear alternatives or comments.","Display_name":"zelanix","Creater_id":53985,"Start_date":"2016-07-29 13:31:49","Question_id":226280}
{"_id":{"$oid":"5837a588a05283111e4d6a98"},"Last_activity":"2016-08-01 02:05:50","Creator_reputation":178,"Question_score":1,"Answer_content":"The mathematical difference is simple (and you probably got that already). A mixture distribution has a density which is a weighted sum of other probability densities (often from the same class) whereas a convolution is a sum of random variables. The intuition for a mixture can be illustrated (in line with your example) as follows: Let's say you have  sensors each of which draws an independent measurement  (for ). Furthermore, let's say that you are only observing the measurement  of one of these sensors , i.e.  by choosing the sensor s randomly (from ) using a discrete uniform distribution. Then, the density of  given that  is known corresponds to . Now, as  is not known, we can consider all possible values for s and we obtain for the density a mixture distributionf_W(x) = P(s=1)\\cdot f_1(x) + \\ldots + P(s=k)\\cdot f_k(x)=\\frac{1}{k}\\sum_{i=1}^k f_i(x)In the sensor example you would have a convolution if you would take all measurements (assuming them to be independent) and sum them up,i.e., . This may happen as part of averaging the sensor measurements. Then, the resulting density isf_W(x) = f_{X_1+\\ldots+X_k}(x) = (f_1 * f_2 * \\ldots*f_k)(x)\\ ,where  denotes the convolution operation. Side note: For the actual averaging procedure, we would have to divide the sum by the number of sensors. That is  and f_W(x) = f_{k^{-1}(X_1+\\ldots+X_k)}(x) = k^{-1}(f_1 * f_2 * \\ldots*f_k)(k\\cdot x)\\ .","Display_name":"Igor","Creater_id":18530,"Start_date":"2016-07-31 15:41:40","Question_id":226592}
{"_id":{"$oid":"5837a588a05283111e4d6aa7"},"Last_activity":"2016-08-01 01:43:23","Creator_reputation":3724,"Question_score":0,"Answer_content":"The odds ratios you get are for a unit change in the explanatory variable. If you have standardised the variables the unit is now the standard deviation (presumably, you did not say how you standardised them) so the odds ratio is for an increase of one standard deviation in the explanatory variable. The problem with doing this as compared to using the original units is that your odds ratio cannot easily be compared with someone else's odds ratio if s/he had different standard deviations in their sample.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-08-01 01:43:23","Question_id":226568}
{"_id":{"$oid":"5837a588a05283111e4d6ab4"},"Last_activity":"2016-08-01 01:29:04","Creator_reputation":2186,"Question_score":1,"Answer_content":"There are many things you could change in the chain used in your prediction, in case you are not yet satisfied with your prediction performance. Options would be, for example:Get more data: useful if you have too less samples for your problem, thereby a possibly bad performance.Change preprocessing: useful if you e.g. were not able to reduce noise sufficiently that was recorded alongside data.Change feature derivation: this is usually on of the major steps. As this is largely domain and problem dependent it is hard to give general advice. In a nutshell: you could remove features (e.g. features that don't add useful information), reduce features (e.g. create new features that replace existing ones), add features (e.g. transform data to different representation that is beneficial for your model). A book that I personally like for this purpose is Max Kuhn and Kjell Johnson (2013): \"Applied Predictive Modeling.\" Springer New York, because it provides very practical view, explanation, and approach towards those problems IMHO. Besides this one there are many publications out there related to feature derivation (which as mentioned is largely domain specific when in comes to the details).Try different models and parametrizations: this is what you already mentioned in your question.One more thing: keep in mind that whichever approach you use, in the end need to proper evaluate it using e.g. repeated cross validation with a separate, held-back test set that is used only on the one, final model. Otherwise you can easily over-engineer your features to your problem and thereby overfit your problem.","Display_name":"geekoverdose","Creater_id":112731,"Start_date":"2016-08-01 01:29:04","Question_id":226360}
{"_id":{"$oid":"5837a588a05283111e4d6ac1"},"Last_activity":"2016-08-01 01:22:52","Creator_reputation":6,"Question_score":0,"Answer_content":"If you are talking about a constant bias, I think that what you intend to do is impossible. Zero correlation means that the slope of the regression line between the two datasets is zero; changing the offset (+-constant bias correction) cannot correct the slope.If you are talking about a slow-varying bias (i.e. trend, drift, smooth signal, etc.), you may want to investigate detrending techniques. First, you should guess what kind of trend you have, it can be a linear drift, a seasonal component (common in Earth science datasets), etc.. You can try moving average with different window sizes to reveal hidden trends; if you are familiar with signal processing you can take a look to wavelet decomposition.Hope that helps.","Display_name":"bmr","Creater_id":124132,"Start_date":"2016-08-01 01:22:52","Question_id":226321}
{"_id":{"$oid":"5837a588a05283111e4d6acd"},"Last_activity":"2016-08-01 01:19:37","Creator_reputation":2186,"Question_score":2,"Answer_content":"No, in the returned model caret does only provide finalModel as the determined best parametrization trained again on all training data without resampling or similar. Thereby, the final training is the same as if you would have trained this parametrization with trainControl(method='none').Therefore, what you can do: train those parametrizations you would like to get a test set performance by hand, using trainControl(method='none') and all training data. You could then apply all those models to your test set using predict(model, ...). But keep in mind that you should not compare multiple models based on only the test set performance. Update: caret provides a good explanation on how to compare multiple models with partitioning + resampling. This could boil down to something like:library(caret)set.seed(123456)training_indexes \u0026lt;- createDataPartition(y = irisknn \u0026lt;- train(training[,1:4], training[,5], method='knn', tuneGrid=expand.grid(k=1:5), trControl = trainControl(method = 'repeatedcv', 10, 20, savePredictions = T))modelsknnpred, reference = modelspredknn, newdata = testing[,1:4]), testing[,5])","Display_name":"geekoverdose","Creater_id":112731,"Start_date":"2016-08-01 00:44:55","Question_id":226569}
{"_id":{"$oid":"5837a588a05283111e4d6ada"},"Last_activity":"2016-08-01 00:58:54","Creator_reputation":5445,"Question_score":2,"Answer_content":"First of all, yes, you are correct in the way you are interpreting the fixed effects.However, note that we are only dealing here with fixed effects. Your model also has random effects, and in particular you have random coefficients for treatment which means that each subject has their own individual treatment effect. The calculations with the fixed effects therefore represent averages across all subjects.Although the findings are largely the same, I would present your findings based on the model with the interactions.  We can view the model with no interactions with this plot:While the model with interactions looks like this:Formally, you can do a likelihood ratio test using the anova() function to test which model is better (you will have to re-run your models using the REML=FALSE option because likelihood-based methods cannot be used to compare models with different fixed effects).I would focus on treatment 3 being associated with higher values of amp.sqrt at time 7 and further at time 8, and these differences are also statistically significant at the 5% level. Treatment 3 is associated with lower values at time 6 (although this difference is not statistically significant at the 5% level). Also, there is very little differences between treatments 1 and 2 at all time points (and these are also not statistically significant). Moreover there appears to be no time trend for treatments 1 and 2. You might be interested in a test of whether treatment 2 is different between years 6 and 8 since there is a small upward trend. Personally this looks negligible to me, compared with treatment 3 but if you wanted to test this you could use a post-hoc test such as Dunnett's.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-08-01 00:58:54","Question_id":226559}
{"_id":{"$oid":"5837a588a05283111e4d6ae7"},"Last_activity":"2016-08-01 00:17:08","Creator_reputation":43,"Question_score":0,"Answer_content":"Since it's been almost a week since I asked this I suppose I can answer for anyone coming by.I suspect you should go with (2). I believe this is true because normalization is based typically upon an average, or subtracting the mean and dividing by the standard deviation or some variation on that.This means the normalization process is dependent on the moments of the column you're normalizing. In the example, I don't believe that the moving average done on the normalized wait time would be accurate because we would lose information on the moments by normalizing. As such, you need to normalize each column after generating all the data for analysis, rather than normalizing the \"main column\" you're analyzing and then applying analysis.I wish someone with more experience would confirm this for me, but until someone does, I believe this is correct.","Display_name":"rec","Creater_id":124589,"Start_date":"2016-08-01 00:17:08","Question_id":225727}
{"_id":{"$oid":"5837a588a05283111e4d6af6"},"Last_activity":"2015-08-17 10:25:20","Creator_reputation":86,"Question_score":0,"Answer_content":"Building on the comments to the question ...Unless these PNR's are arbitrary alpha-numerics as user777 points out, this seems to be a problem best solved with a Regular Expression.  You mention that all 70 \"IsPNR\" tags are 1 so you don't have a negative case to train with either, also making machine learning a bad choice.  ","Display_name":"Bob Dillon","Creater_id":85506,"Start_date":"2015-08-17 10:25:20","Question_id":167495}
{"_id":{"$oid":"5837a588a05283111e4d6af7"},"Last_activity":"2015-08-17 09:45:16","Creator_reputation":17914,"Question_score":1,"Answer_content":"The feasibility of this task depends on what two kinds of strings you're comparing. If you're comparing PNR numbers (which are formed as arbitrary combinations of numbers and letters) to any arbitrary string, I think you'll be hard pressed to do better than flipping a coin. I've heard it asserted that the set of valid PNRs is smaller than the set of all alphanumeric arbitrary strings of the same length, but I haven't found any reference to that effect. In practice, it's probably true on an airline-by-airline basis that not all combinations are valid because airlines all have their own business practices (caveat: acquisitions and mergers may make this effect nonuniform within airlines). If airline information is known, then you could work on discovering which combinations are/are not invalid and why... Since PNRs are recycled over a long enough time interval, you can just check which patterns never occur in that interval and mark those as invalid.If you're comparing PNR numbers to natural language (like what a person might send in an e-mail), then you can make a few simple rules that will give you considerable leverage. Check the length of the word. If it's no the length of a PNR number, it's not a PNR number. Likewise if it contains non-PNR characters like @ or I$), and it is also an English word.","Display_name":"Sycorax","Creater_id":22311,"Start_date":"2015-08-17 09:45:16","Question_id":167495}
{"_id":{"$oid":"5837a588a05283111e4d6b03"},"Last_activity":"2016-07-31 22:11:16","Creator_reputation":12922,"Question_score":1,"Answer_content":"Setting aside for a moment whether your sampling procedure in fact samples from the Haar measure on , it samples from some distribution on .  To show that this measure is (proportional to) Haar measure, we need to show that it is left invariant.Now, there is a natural map  which takes an orthogonal matrix to its first column.  The fiber of this map (inverse image) over any point  in  is the collection of vector pairs  in  that together with  form an oriented basis.  The first vector  is clearly confined to the circle  of unit vectors orthogonal to , and once  is known, the identity of  is forced by the orientation condition.  So, each fiber of the projection mapping is an .Your sampling procedure can be summarized in this setting asFirst sample a point  from the uniform measure on the unit sphere .Then sample a point from the uniform measure on the fiber over , which is isometric to a unit circle.The uniform measures on  and  are induced from the standard Riemannian metrics from  (they are proportional to the volume measure).  So, if we can show that an orthogonal transformation maps  isometrically to itself, and the fiber over and point  isometrically onto the fiber over its image, then we are done.An Orthogonal Transformation Maps the Unit Sphere Isometrically to Itself:Let  be two unit vectors in , and let  be an orthogoal matrix.  Then \\begin{align} |v - w|^2 \u0026amp;= (v - w)^t (v - w) \\\\ \u0026amp;= (v - w)^t O^t O (v - w) \\\\ \u0026amp;= (Ov - Ow)^t (Ov - Ow) \\\\ \u0026amp;= |Ov - Ow|^2 \\end{align}So  is a self isometry of .An Orthogonal Transformation Maps the Fibers Isometrically:First, since orthogonal transformations preserve orthogonality between vectors, an orthogonal transformation  does in fact map the fiber over a vector  to the fiber over .  To show this mapping is isometric, we only need to repeat the calculation given above for the  case with two vectors orthogonal to .  Since the angle between these vectors is preserved, the arc length between them is as well, and hence one circle is mapped isometrically to the other.So, the metric induced from your sampling procedure is left invariant, and hence you are sampling from the Haar measure on .","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-07-31 22:05:04","Question_id":226598}
{"_id":{"$oid":"5837a588a05283111e4d6b12"},"Last_activity":"2016-07-31 21:43:50","Creator_reputation":8367,"Question_score":2,"Answer_content":"Experimental design comprises all activites relating to the active elicitation of data, and thus certainly applies to the use of advertising when its effect is being studied. Note that what scientists call an \"experiment\", or more fully a \"true experiment\", marketers often call an \"A/B test\".","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-31 21:43:50","Question_id":226511}
{"_id":{"$oid":"5837a588a05283111e4d6b1f"},"Last_activity":"2016-07-31 21:38:40","Creator_reputation":106,"Question_score":1,"Answer_content":"First, note that if the ratio of C to D is larger in corpus A, then so will be the proportion.  So you can do the standard hypothesis test for difference of proportions.  Let: p_A = {C_A\\over C_A + D_A}p_B = {C_B\\over C_B + D_B}p = {C\\over C + D}These are the proportions of words in each corpus as well as the total for both corpuses.Then set Z = {(p_A - p_B)\\over p(1 - p)({1\\over C_A + D_A} + {1\\over C_B + D_B})}If the number of words from categories C and D is large, then your test statistic should be approximately normal.  The null hypothesis is that Z = 0 and the two proportions are equal.  If you want a p-value of .05, you can reject the null hypothesis when Z \u003e 1.645.  ","Display_name":"Wart","Creater_id":125187,"Start_date":"2016-07-31 21:33:40","Question_id":226594}
{"_id":{"$oid":"5837a588a05283111e4d6b2c"},"Last_activity":"2016-07-31 20:46:17","Creator_reputation":16,"Question_score":0,"Answer_content":"if you look at the  parameter of your density, this is a location parameter and can be restated as follows:nowthe minimum is only an exponential random variable intensity  plus thennow because your variables can be decomposed as beforenow You can also use the change of variable y = x-mu, the integrals are simplified in terms of densities and expected values Exponential","Display_name":"JavierMtz","Creater_id":125181,"Start_date":"2016-07-31 20:46:17","Question_id":215540}
{"_id":{"$oid":"5837a588a05283111e4d6b2d"},"Last_activity":"2016-05-31 11:14:52","Creator_reputation":147773,"Question_score":5,"Answer_content":"As explained at The Number of Exponential Summands in a Fixed Interval is Poisson, each of the variables  is the waiting time for the first point to appear in a Poisson process of rate  starting at time . It is obvious from the characteristic properties of a Poisson process that a union of Poisson processes of rates  is just a Poisson process of rate , equal to  in this case.  Therefore the expected waiting time to the first point among them all must be .After the first time  is encountered, there remain samples from the  other processes.  Since they are all independent, and the chance of new points appearing is independent of any previous points that have appeared, it's the same situation all over again: the starting time is now  instead of  and  has decreased to , that's all.  Consequently the expected time to wait for the next point, which is , is .(And so it continues: the expectation of  is  for .)In this figure a realization of a Poisson process of rate  is shown for each of  independent variables indexed  through .  The union of the points is plotted with ticks at the bottom: they form a realization of a Poisson process of rate .  The first points in each process are highlighted with solid dots (and shown with longer ticks at the bottom): these form the sample ; all the hollow dots are superfluous but are shown to illustrate the concepts.  The expected time to wait for the very first point overall is  because the combined rate is .  After it is encountered, four processes remain.  Thus, their combined rate is  and the expected waiting time to the first point among them is .","Display_name":"whuber","Creater_id":919,"Start_date":"2016-05-31 11:14:52","Question_id":215540}
{"_id":{"$oid":"5837a588a05283111e4d6b3c"},"Last_activity":"2016-07-31 19:29:14","Creator_reputation":334,"Question_score":0,"Answer_content":"This simulation is a classic example of the Binomial Distribution.  That is, assuming each trial is independent and identical, then a purely random guesser should get  guesses correct, where  follows the Binomial Distribution with parameters  and .  Furthermore, according to the Binomial Distribution, the expected number of correct guesses for a random guesser will be:E(X) = np = 500 With a variance of:Var(X) = np(1-p) = 250The standard deviation is just the square root of the variance:StdDev(X) = \\sqrt{Var(X)} \\approx 15.8Finally, it looks like you want to identify someone who is not purely guessing (perhaps they have ESP?), and so using a 2 standard deviation threshold as per the Normal Distribution Approximation gives a p-value of approximately .025.  That is, if someone is able to guess more than500 + 2 * 15.8 = 531.6of the coin flips (), you can be fairly confident that they are not purely guessing.","Display_name":"David C","Creater_id":96030,"Start_date":"2016-07-31 19:17:40","Question_id":226600}
{"_id":{"$oid":"5837a588a05283111e4d6b4a"},"Last_activity":"2016-07-31 17:30:42","Creator_reputation":8055,"Question_score":1,"Answer_content":"In that context, ground truth and gold labels are synonymous. LSTMs don't require any additional supervision compared to other supervised machine learning models.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-07-31 17:30:42","Question_id":226543}
{"_id":{"$oid":"5837a588a05283111e4d6b56"},"Last_activity":"2016-07-31 16:55:02","Creator_reputation":3832,"Question_score":3,"Answer_content":"It sounds like you're interested in 'stochastic optimization', where the goal is to optimize a stochastic objective function (typically its expected value). Note that some people take this term to include methods for optimizing deterministic functions where the solver uses randomness (not what you want).These references may be useful:  Hannah (2014). Stochastic Optimization.    Fu et al. (2005). Simulation optimization: A review, new developments, and applications    Amaran et al. (2014). Simulation optimization: A review of algorithms and applicationsYou may also be interested in Bayesian optimization. In this setting, the objective function can be stochastic, and the goal is to choose parameters that optimize its expected value, given the parameters. As with some other stochastic optimization methods, the objective function can be a black box, meaning you have the ability to evaluate it, but may not have a closed form expression (e.g. it might depend on the result of a simulation or physical experiment). Evaluating it may be very expensive. Bayesian optimization treats evaluations of the objective function as observed data, and uses them to update a probabilistic model of the objective function (e.g. using Gaussian process regression). New evaluation points are chosen in a way that trades off between exploration (sampling from uncertain regions to get a better estimate of the objective function) and exploitation (sampling from regions that are predicted to increase/decrease the objective function).  Brochu et al. (2010). A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning.    Snoek et al. (2012). Practical Bayesian Optimization of Machine Learning Algorithms.If you do have a closed form expression for the objective function (as in your example), it would make more sense to try to exploit this known structure than to throw it away and treat the function as a black box. For example, in the best case you may be able to derive an expression for the expected value given the parameters, then use standard, deterministic methods to optimize it.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-07-31 16:24:23","Question_id":226554}
{"_id":{"$oid":"5837a588a05283111e4d6b57"},"Last_activity":"2016-07-31 14:33:41","Creator_reputation":3632,"Question_score":1,"Answer_content":"You are trying to optimize a stochastic objective function because of random noise component in the objective function. For a noisy function like yours, You could use an evolutionary optimization methods such as genetic algortithm to solve stochastic optimization problems. Please have a look at this mathworks website that demonstrates how to solve a problem like yours. I would add that in addition to using an evolutionary optimization which is able to find near optimal (it can also easily handle multiple optimal solutions). In order to find a more precise accurate solution, I would club evolutionary optimization with a local optimization. This is called hybrid optimization. The above steps can be easily demonstrated using R. I have specified both global optimization using GA and a hybrid optimization (global-local) optimization.set.seed(1234)obj.f \u0026lt;- function(x) {  fx \u0026lt;- sin(x+pi/2+0.5*rnorm(1))  return(fx)}#Global optimization onlyga.opt.global \u0026lt;- ga(type = \"real-valued\", fitness = obj.f, min = -2, max = 2)summary(ga.opt.global)plot(ga.opt.global)# Hybrid Optimziationga.opt.hybrid \u0026lt;- ga(type = \"real-valued\", fitness = obj.f, min = -2, max = 2,optim=TRUE,optimArgs = list(method = \"Nelder-Mead\"))summary(ga.opt.hybrid)plot(ga.opt.hybrid)Please Note, I'm not sure what your overall objective is, but I have tried to provide an answer from a pure optimization point of you. As it turns out as evident from your plot any values between -0.5 to 0 is optimal. IF you run the above optimization multiple times you will be able to find almost all the optimal values.\u0026gt; summary(ga.opt.global)+-----------------------------------+|         Genetic Algorithm         |+-----------------------------------+GA settings: Type                  =  real-valued Population size       =  50 Number of generations =  100 Elitism               =  2 Crossover probability =  0.8 Mutation probability  =  0.1 Search domain =     x1Min -2Max  2GA results: Iterations             = 100 Fitness function value = 1 Solution =             x1[1,] 0.4520799\u0026gt; summary(ga.opt.hybrid)+-----------------------------------+|         Genetic Algorithm         |+-----------------------------------+GA settings: Type                  =  real-valued Population size       =  50 Number of generations =  100 Elitism               =  2 Crossover probability =  0.8 Mutation probability  =  0.1 Search domain =     x1Min -2Max  2GA results: Iterations             = 100 Fitness function value = 0.9999994 Solution =              x1[1,] -0.1413327","Display_name":"forecaster","Creater_id":29137,"Start_date":"2016-07-31 14:28:39","Question_id":226554}
{"_id":{"$oid":"5837a588a05283111e4d6b64"},"Last_activity":"2016-07-31 16:46:59","Creator_reputation":8055,"Question_score":0,"Answer_content":"If you want to learn, implement from scratch. If you want to apply, just use some packages: the amount of tweaking the latter require depends on the package and your application.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-07-31 16:46:59","Question_id":215528}
{"_id":{"$oid":"5837a588a05283111e4d6b70"},"Last_activity":"2016-07-31 16:15:13","Creator_reputation":16,"Question_score":0,"Answer_content":"I contend that it's better to actually scale your data before splitting it into a train and test set for two reasons:1 - There is no danger of leakage as there is no fitting being done when scaling features.2 - This actually mimics what you might do when receiving a new set of data. You might have the X for a new set of data but not the y. There is no reason not to include the X from the new data with the old data when preprocessing. It some situations it might be the only logical thing to do.","Display_name":"adamwlev","Creater_id":99091,"Start_date":"2016-07-30 11:18:12","Question_id":121886}
{"_id":{"$oid":"5837a588a05283111e4d6b71"},"Last_activity":"2015-10-04 05:00:37","Creator_reputation":15629,"Question_score":1,"Answer_content":"Here's another chemometric application example where feature scaling would be disastrous:There are lots of classification (qualitative analysis) tasks of the form \"test whether some analyte (= substance of interest) content is below (or above) a given threshold (e.g. legal limit)\". In this case, the sensors to produce the input data for the classifier would be chosen to have signal =  f (analyte~concentration), preferrably with  being a steep and even linear function.In this situation, feature scaling would essentially erase all relevant information from the raw data.In general, some questions that help to decide whether scaling is a good idea:What does normalization do to your data wrt. solving the task at hand? Should that become easier or do you risk to delete important information?Does your algorithm/classifier react sensitively to the (numeric) scale of the data? (convergence) Is the algorithm/classifier heavily influenced by different scales of different features?If so, do your features share the same (or comparable) scales or even physical units?Does your classifier/algorithm/actual implementation perform its own normalisation?","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2015-10-04 05:00:37","Question_id":121886}
{"_id":{"$oid":"5837a588a05283111e4d6b72"},"Last_activity":"2014-10-29 03:05:40","Creator_reputation":296,"Question_score":8,"Answer_content":"In my view the question about scaling/not scaling the features in machine learning is a statement about the measurement units of your features. And it is related to the prior knowledge you have about the problem.Some of the algorithms, like Linear Discriminant Analysis and Naive Bayes do feature scaling by design and you would have no effect in performing one manually. Others, like knn can be gravely affected by it.So with knn type of classifier you have to measure the distances between pairs of samples. The distances will of course be influenced by the measurement units one uses. Imagine you are classifying population into males and females and you have a bunch of measurements including height. Now your classification result will be influenced by the measurements the height was reported in. If the height is measured in nanometers then it's likely that any k nearest neighbors will merely have similar measures of height. You have to scale.However as a contrast example imagine classifying something that has equal units of measurement recorded with noise. Like a photograph or microarray or some spectrum. in this case you already know a-priori that your features have equal units. If you were to scale them all you would amplify the effect of features that are constant across all samples, but were measured with noise. (Like a background of the photo). This again will have an influence on knn and might drastically reduce performance if your data had more noisy constant values compared to the ones that vary. Now any similarity between k nearest neighbors will get influenced by noise.So this is like with everything else in machine learning - use prior knowledge whenever possible and in the case of black-box features do both and cross-validate.","Display_name":"Karolis Koncevičius","Creater_id":18417,"Start_date":"2014-10-29 03:05:40","Question_id":121886}
{"_id":{"$oid":"5837a588a05283111e4d6b73"},"Last_activity":"2014-10-29 02:58:58","Creator_reputation":6271,"Question_score":4,"Answer_content":"You should normalize when the scale of a feature is irrelevant or misleading, and not normalize when the scale is meaningful.K-means considers Euclidean distance to be meaningful.  If a feature has a big scale compared to another, but the first feature truly represents greater diversity, then clustering in that dimension should be penalized.In regression, as long as you have a bias it does not matter if you normalize or not since you are discovering an affine map, and the composition of a scaling transformation and an affine map is still affine.When there are learning rates involved, e.g. when you're doing gradient descent, the input scale effectively scales the gradients, which might require some kind of second order method to stabilize per-parameter learning rates.  It's probably easier to normalize the inputs if it doesn't matter otherwise.","Display_name":"Neil G","Creater_id":858,"Start_date":"2014-10-29 02:58:58","Question_id":121886}
{"_id":{"$oid":"5837a588a05283111e4d6b74"},"Last_activity":"2014-10-29 02:49:06","Creator_reputation":138,"Question_score":3,"Answer_content":"There are several methods of normalization. In regards to regression, if you plan on normalizing the feature by a single factor then there is no need. The reason being that single factor normalization like dividing or multiplying by a constant already gets adjusted in the weights(i.e lets say the weight of a feature is 3, but if we normalize all the values of the feature by dividing by 2, then the new weight will be 6, so overall the effect is same). In contrast if you are planning to mean normalize, then there is a different story. Mean normalization is good when there is a huge variance in the feature values ( 1  70  300  4 ). Also if a single feature can have both a positive and negative effect, then it is good to mean normalize. This is because when you mean normalize a given set of positive values then the values below mean become negative while those above mean become positive.In regards to k-nearest neighbours, normalization should be performed all the times. This is because in KNN, the distance between points causes the clustering to happen. So if you are applying KNN on a problem with 2 features with the first feature ranging from 1-10 and the other ranging from 1-1000, then all the clusters will be generated based on the second feature as the difference between 1 to 10 is small as compared to 1-1000 and hence can all be clustered ito a single group","Display_name":"nar","Creater_id":59397,"Start_date":"2014-10-29 02:49:06","Question_id":121886}
{"_id":{"$oid":"5837a588a05283111e4d6b81"},"Last_activity":"2015-01-28 06:38:53","Creator_reputation":15629,"Question_score":3,"Answer_content":"I'd say in image analysis the cases to set aside are images not pixels. This would apply to cross validation as well as to hold out. Update: From what you explain about your application, you can and should indeed test on images that were not used for deriving the shape fitting algorithm (i.e. a new set of images). You can then count how many cars were correctly recognized, and how many false positives or false negatives you have.So this is generalization to unknown images.Testing left-out pixels: Not only are neighbour pixels not independent but also very basic properties of your image change. Imagine a kind of 4-fold cross validation scheme where you keep every 2nd column and every 2nd row for testing. This corresponds to an image at half the resolution compared to the original image. What you would test with this hold out pixels approach is generalization to (ruggedness agains) lower resolution images.I.e. this may make sense in certain situations, but you'll measure a completely different performance characteristic.","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2015-01-28 00:51:31","Question_id":135256}
{"_id":{"$oid":"5837a588a05283111e4d6b8e"},"Last_activity":"2015-01-19 16:39:54","Creator_reputation":10758,"Question_score":2,"Answer_content":"I'd start by doing a little \"feature engineering\".You know exactly what parts of the image carry semantic information: the positions of the hour, minute and (optionally) second hands. The rest of the image is irrelevant. You could manually mask it out, or you could subtract the average image from each one. If you have enough images (and consistent lighting), only the hands will remain. Now you just have to find the angle of each hand (to determine the time) and its length/width (to identify the hour/minute/second) hand. The Hough Transform (or one of its variants) is a standard technique for identifying lines in an image. Once you've got the two lines, it's trivial to convert their lengths and angles into a time.This seems like a reasonable starting point to which you could add progressively more sophisticated image processing/machine learning techniques to deal with e.g., variations in lighting or movement. You could feed Hough-like features into your model, for example.If you wanted to apply a less ad-hoc algorithm, I'd be tempted to think about it as a regression problem instead of a classification problem. Predicting a continuous value is going to be easier than performing a 720+ way classification (12 hours with 60 minutes each) and a prediction that misses the correct time by 1-2 minutes feels better than one which is off by 6 hours. As an added complication, time is circular on a watch: both 12:59 and 1:01 are reasonably close to 1:00.","Display_name":"Matt Krause","Creater_id":7250,"Start_date":"2015-01-19 16:39:54","Question_id":134097}
{"_id":{"$oid":"5837a588a05283111e4d6b9b"},"Last_activity":"2014-10-11 21:22:38","Creator_reputation":530,"Question_score":3,"Answer_content":"To deal with different image sizes, the standard technique is down-sampling the images to a fixed resolution (see the famous Krizhevsky ImageNet paper for a example). To achieve scale invariance, the most used technique is convolutional networks. In LeCun's website there are some nice examples of LeNet-5 dealing with variances in scale.","Display_name":"Saul Berardo","Creater_id":46039,"Start_date":"2014-10-11 21:22:38","Question_id":118654}
{"_id":{"$oid":"5837a588a05283111e4d6ba8"},"Last_activity":"2014-09-24 11:27:19","Creator_reputation":5842,"Question_score":1,"Answer_content":"Yes, they do. See the following links:http://people.idsia.ch/~juergen/nips2012.pdfhttp://blog.kaggle.com/2014/04/18/winning-the-galaxy-challenge-with-convnets/","Display_name":"bayerj","Creater_id":2860,"Start_date":"2014-09-24 11:27:19","Question_id":116639}
{"_id":{"$oid":"5837a588a05283111e4d6bb5"},"Last_activity":"2014-09-09 07:43:43","Creator_reputation":188,"Question_score":1,"Answer_content":"You have to use Gaussian visible units instead of binary visible units. This is described in the practical guide for training RBM by G. Hinton. ","Display_name":"Baptiste Wicht","Creater_id":49296,"Start_date":"2014-09-09 07:43:43","Question_id":113686}
{"_id":{"$oid":"5837a588a05283111e4d6bc2"},"Last_activity":"2014-05-08 07:46:21","Creator_reputation":19151,"Question_score":2,"Answer_content":"If you truly knew that your data generating process (DGP) is ARMA(0,0): , then, yes, it is, basically, a white noise.The problem is that you do not know the DGP, usually. So, all you can say is that you don't see the serial correlation, and thus conclude that it's a constant with a noise.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2014-05-08 07:46:21","Question_id":96930}
{"_id":{"$oid":"5837a588a05283111e4d6bcf"},"Last_activity":"2016-07-31 13:12:51","Creator_reputation":1350,"Question_score":2,"Answer_content":"Welcome to the site Mahdieh. It seems like your two problems are stemming from having selected an analytic approach (PCA/EFA) that is ill-suited to your goals of testing your theory of construct measurement. EFA, though a very popular analytic approach for the purpose of scale construction, is better thought of as a statistical tool for building a theory of construct measurement when you don't already have one. It's a gross oversimplification of the process, but in a nutshell, you plug your variables into an EFA with a  few details (e.g., how many factor to keep; rotation and estimation methods), and the EFA spits out the configuration of which variables load onto which factors. But in your case, you conducted interviews and carried out an extensive literature review, and in doing so, you have come into the process of scale-development with an already-developed theory of construct measurement in mind--you think you have a pretty good idea of how many factors you need, and which items go with which factors. Simply stated, you therefore don't need an analysis like EFA to give you a theory of construct measurement; you already have one, and you just need to test it to determine how plausible your theory of measurement is.To accomplish the goal of testing a theory of construct measurement, you would be much better off using a confirmatory factor analysis (CFA). Whereas in EFA, the statistical program tells you which items go with which factors, the process is reversed in CFA; you tell the program which items you think load onto which factors. Then, your statistical program will fit your implied model to your data, and, in addition to estimates of factor loadings/correlations between factors, will give you a variety of indexes quantifying how well your model fits the data. I've been deliberately avoiding describing technical details because...well, they are technical. Having proclaimed yourself as a novice, I don't think details will be especially helpful for you in an answer to your question. But what I will say is this: I think you ought to consider trying to familiarize yourself with the basics of CFA. I'd recommend Beaujean's (2014) text, which does a pretty good job covering the basics in a very accessible way. It also helps you learn how to conduct CFA using the lavaanpackage for R, which is among the most user-friendly (IMO) options out there for people new to CFA. ","Display_name":"jsakaluk","Creater_id":53456,"Start_date":"2016-07-31 13:12:51","Question_id":226487}
{"_id":{"$oid":"5837a588a05283111e4d6bdc"},"Last_activity":"2016-07-28 22:20:18","Creator_reputation":3832,"Question_score":7,"Answer_content":"How bridge regression and elastic net differ is a fascinating question, given their similar-looking penalties. Here's one possible approach. Suppose we solve the bridge regression problem. We can then ask how the elastic net solution would differ. Looking at the gradients of the two loss functions can tell us something about this.Bridge regressionSay  is a matrix containing values of the independent variable ( points x  dimensions),  is a vector containing values of the dependent variable, and  is the weight vector.The loss function penalizes the  norm of the weights, with magnitude :L_b(w)= \\| y - Xw\\|_2^2+ \\lambda_b \\|w\\|_q^qThe gradient of the loss function is:\\nabla_w L_b(w)= -2 X^T (y - Xw)+ \\lambda_b q |w|^{\\circ(q-1)} \\text{sgn}(w) denotes the Hadamard (i.e. element-wise) power, which gives a vector whose th element is .  is the sign function (applied to each element of ). The gradient may be undefined at zero for some values of .Elastic netThe loss function is:L_e(w)= \\|y - Xw\\|_2^2+ \\lambda_1 \\|w\\|_1+ \\lambda_2 \\|w\\|_2^2This penalizes the  norm of the weights with magnitude  and the  norm with magnitude . The elastic net paper calls minimizing this loss function the 'naive elastic net' because it doubly shrinks the weights. They describe an improved procedure where the weights are later rescaled to compensate for the double shrinkage, but I'm just going to analyze the naive version. That's a caveat to keep in mind.The gradient of the loss function is:\\nabla_w L_e(w)= -2 X^T (y - Xw)+ \\lambda_1 \\text{sgn}(w)+ 2 \\lambda_2 wThe gradient is undefined at zero when  because the absolute value in the  penalty isn't differentiable there.ApproachSay we select weights  that solve the bridge regression problem. This means the the bridge regression gradient is zero at this point:\\nabla_w L_b(w^*)= -2 X^T (y - Xw^*)+ \\lambda_b q |w^*|^{\\circ (q-1)} \\text{sgn}(w^*)= \\vec{0}Therefore:2 X^T (y - Xw^*)= \\lambda_b q |w^*|^{\\circ (q-1)} \\text{sgn}(w^*)We can substitute this into the elastic net gradient, to get an expression for the elastic net gradient at . Fortunately, it no longer depends directly on the data:\\nabla_w L_e(w^*)= \\lambda_1 \\text{sgn}(w^*)+ 2 \\lambda_2 w^*-\\lambda_b q |w^*|^{\\circ (q-1)} \\text{sgn}(w^*)Looking at the elastic net gradient at  tells us: Given that bridge regression has converged to weights , how would the elastic net want to change these weights?It gives us the local direction and magnitude of the desired change, because the gradient points in the direction of steepest ascent and the loss function will decrease as we move in the direction opposite to the gradient. The gradient might not point directly toward the elastic net solution. But, because the elastic net loss function is convex, the local direction/magnitude gives some information about how the elastic net solution will differ from the bridge regression solution.Case 1: Sanity check(). Bridge regression in this case is equivalent to ordinary least squares (OLS), because the penalty magnitude is zero. The elastic net is equivalent ridge regression, because only the  norm is penalized. The following plots show different bridge regression solutions and how the elastic net gradient behaves for each.Left plot: Elastic net gradient vs. bridge regression weight along each dimensionThe x axis represents one component of a set of weights  selected by bridge regression. The y axis represents the corresponding component of the elastic net gradient, evaluated at . Note that the weights are multidimensional, but we're just looking at the weights/gradient along a single dimension.Right plot: Elastic net changes to bridge regression weights (2d)Each point represents a set of 2d weights  selected by bridge regression. For each choice of , a vector is plotted pointing in the direction opposite the elastic net gradient, with magnitude proportional to that of the gradient. That is, the plotted vectors show how the elastic net wants to change the bridge regression solution.These plots show that, compared to bridge regression (OLS in this case), elastic net (ridge regression in this case) wants to shrink weights toward zero. The desired amount of shrinkage increases with the magnitude of the weights. If the weights are zero, the solutions are the same. The interpretation is that we want to move in the direction opposite to the gradient to reduce the loss function. For example, say bridge regression converged to a positive value for one of the weights. The elastic net gradient is positive at this point, so elastic net wants to decrease this weight. If using gradient descent, we'd take steps proportional in size to the gradient (of course, we can't technically use gradient descent to solve the elastic net because of the non-differentiability at zero, but subgradient descent would give numerically similar results).Case 2: Matching bridge \u0026amp; elastic net(). I chose the bridge penalty parameters to match the example from the question. I chose the elastic net parameters to give the best matching elastic net penalty. Here, best-matching means, given a particular distribution of weights, we find the elastic net penalty parameters that minimize the expected squared difference between the bridge and elastic net penalties:\\min_{\\lambda_1, \\lambda_2} \\enspaceE \\left [ (    \\lambda_1 \\|w\\|_1 + \\lambda_2 \\|w\\|_2^2    - \\lambda_b \\|w\\|_q^q)^2 \\right ]Here, I considered weights with all entries drawn i.i.d. from the uniform distribution on  (i.e. within a hypercube centered at the origin). The best-matching elastic net parameters were similar for 2 to 1000 dimensions. Although they don't appear to be sensitive to the dimensionality, the best-matching parameters do depend on the scale of the distribution.Penalty surfaceHere's a contour plot of the total penalty imposed by bridge regression () and best-matching elastic net () as a function of the weights (for the 2d case):Gradient behaviorWe can see the following:Let  be the chosen bridge regression weight along dimension .If , elastic net wants to shrink the weight toward zero.If , the bridge regression and elastic net solutions are the same. But, elastic net wants to move away if the weight differs even slightly.If , elastic net wants to grow the weight.If , the bridge regression and elastic net solutions are the same. Elastic net wants to move toward this point from nearby weights.If , elastic net wants to shrink the weight.The results are qualitatively similar if we change the the value of  and/or  and find the corresponding best . The points where the bridge and elastic net solutions coincide change slightly, but the behavior of the gradients are otherwise similar.Case 3: Mismatched bridge \u0026amp; elastic net. In this regime, bridge regression behaves similar to ridge regression. I found the best-matching , but then swapped them so that the elastic net behaves more like lasso ( penalty greater than  penalty).Relative to bridge regression, elastic net wants to shrink small weights toward zero and increase larger weights. There's a single set of weights in each quadrant where the bridge regression and elastic net solutions coincide, but elastic net wants to move away from this point if the weights differ even slightly.. In this regime, the bridge penalty is more similar to an  penalty (although bridge regression may not produce sparse solutions with , as mentioned in the elastic net paper). I found the best-matching , but then swapped them so that the elastic net behaves more like ridge regression ( penalty greater than  penalty).Relative to bridge regression, elastic net wants to grow small weights and shrink larger weights. There's a point in each quadrant where the bridge regression and elastic net solutions coincide, and elastic net wants to move toward these weights from neighboring points.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-07-28 02:15:06","Question_id":224531}
{"_id":{"$oid":"5837a588a05283111e4d6be9"},"Last_activity":"2015-12-27 12:10:01","Creator_reputation":21,"Question_score":2,"Answer_content":"Whether \"nearly significant\" makes sense or not depends on one's philosophy of statistical inference.  It's perfectly valid to consider the alpha level as a line in the sand, in which case one should only pay attention to whether  or .  For such an \"absolutist\", \"nearly significant\" makes no sense.  But it's also perfectly valid to think of p values as providing continuous measures of strength of support (not strength of effect, of course).  For such a \"continualist\", \"nearly significant\" is a sensible way to describe a result with a moderate p-value.  The problem arises when people mix these two philosophies - or worse, are not aware that both exist.  (By the way - people often assume these map cleanly onto Neyman/Pearson and Fisher, but they don't; hence my admittedly clumsy terms for them).  More detail about this in a blog post on this subject here: https://scientistseessquirrel.wordpress.com/2015/11/16/is-nearly-significant-ridiculous/","Display_name":"Stephen Heard","Creater_id":99158,"Start_date":"2015-12-27 09:46:45","Question_id":172928}
{"_id":{"$oid":"5837a588a05283111e4d6bea"},"Last_activity":"2015-09-20 23:17:51","Creator_reputation":51,"Question_score":5,"Answer_content":"The difference between two p-values itself typically is not significant. So, it doesn't matter whether your p-value is 0.05, 0.049, 0.051...With regards to p-values as  a measure of strength of association: A p-value is not directly a measure of strength of association. A p-value is the probability of finding as extreme or more extreme data as the data you have observed, given the parameter is hypothesized to be 0 (if one's interested in the null hypothesis -- see Nick Cox' comment). However, this is often not the quantity the researcher is interested in. Many researchers are rather interested in answering questions like \"what's the probability of the parameter to be greater than some chosen cut-off value?\" If this is what you're interested in, you need to incorporate additional prior information in your model.","Display_name":"RBirkelbach","Creater_id":45910,"Start_date":"2015-09-17 08:39:25","Question_id":172928}
{"_id":{"$oid":"5837a588a05283111e4d6beb"},"Last_activity":"2015-09-17 11:42:38","Creator_reputation":447,"Question_score":4,"Answer_content":"This slippery slope calls back to the Fisher vs Neyman/Pearson framework for null-hypothesis significance testing (NHST). On the one hand, one wants to make a quantitative assessment of just how unlikely a result is under the null hypothesis (e.g., effect sizes).  On the other hand, at the end of the day you want a discrete decision as to whether your results are, or are not, likely to have been due to chance alone. What we've ended up with is a kind of hybrid approach that isn't very satisfying.In most disciplines, the conventional p for significance is set at 0.05, but there is really no grounding for why this must be so.  When I review a paper, I have absolutely no problem with an author calling 0.06 significant, or even 0.07, provided that the methodology is sound, and the entire picture, including all analyses, figures, etc. tell a consistent and believable story.  Where you run into problems is when authors attempt to make a story out of trivial data with small effect sizes. Conversely, I might not fully 'believe' a test is practically meaningful even when it reaches conventional p \u0026lt; 0.05 significance. A colleague of mine once said: \"Your statistics should simply back up what is already apparent in your figures.\"That all said, I think Vasilev is correct.  Given the broken publication system, you pretty much have to include p values, and therefore you pretty much have to use the word 'significant' to be taken seriously, even if it requires adjectives like \"marginally\" (which I prefer). You can always fight it out in peer review, but you have to get there first.","Display_name":"HEITZ","Creater_id":86794,"Start_date":"2015-09-17 11:42:38","Question_id":172928}
{"_id":{"$oid":"5837a588a05283111e4d6bec"},"Last_activity":"2015-09-17 10:10:45","Creator_reputation":71,"Question_score":5,"Answer_content":"From my perspective, the issue boils down to what it actually means to carry out a significance test. Significance testing was devised as a means of making the decision of either to reject the null hypothesis or to fail to reject it. Fisher himself introduced the infamous 0.05 rule for making that (arbitrary) decision. Basically, the logic of significance testing is that the user has to specify an alpha level for rejecting the null hypothesis (conventionally 0.05) before collecting the data. After completing the significance test, the user rejects the null if the p value is smaller than the alpha level (or fails to reject it otherwise). The reason why you cannot declare an effect to be highly significant (say, at the 0.001 level) is because you cannot find stronger evidence than you set out to find. So, if you set your alpha level at 0.05 before the test, you can only find evidence at the 0.05 level, regardless of how small your p values is. In the same way, speaking of effects that are \"somewhat significant\" or \"approaching significance\" also doesn't make much sense because you chose this arbitrary criterion of 0.05. If you interpret the logic of significance testing very literally, anything bigger than 0.05 is not significant. I agree that terms like \"approaching significance\" are often used to enhance the prospects of publication. However, I do not think that authors can be blamed for that because the current publication culture in some sciences still heavily relies on the \"holy grail\" of 0.05. Some of these issues are discussed in:Gigerenzer, G. (2004). Mindless statistics. The Journal of Socio-Economics, 33(5), 587-606.Royall, R. (1997). Statistical evidence: a likelihood paradigm (Vol. 71). CRC press.","Display_name":"Martin R. Vasilev","Creater_id":89673,"Start_date":"2015-09-17 10:00:31","Question_id":172928}
{"_id":{"$oid":"5837a588a05283111e4d6bed"},"Last_activity":"2015-09-17 08:42:07","Creator_reputation":18720,"Question_score":12,"Answer_content":"If you want to allow \"significance\" to admit of degrees then fair enough (\"somewhat significant\", \"fairly significant\"), but avoid phrases that suggest you're still wedded to the idea of a threshold, such as \"nearly significant\", \"approaching significance\", or \"at the cusp of significance\" (my favourite from \"Still Not Significant\" on the blog Probable Error), if you don't want to appear desperate.","Display_name":"Scortchi","Creater_id":17230,"Start_date":"2015-09-17 08:42:07","Question_id":172928}
{"_id":{"$oid":"5837a588a05283111e4d6bfa"},"Last_activity":"2016-07-31 14:11:21","Creator_reputation":30075,"Question_score":8,"Answer_content":"Are smaller -values \"more convincing\"? Yes, of course they are.In the Fisher framework, -value is a quantification of the amount of evidence against the null hypothesis. The evidence can be more or less convincing; the smaller the -value, the more convincing it is. Note that in any given experiment with fixed sample size , the -value is monotonically related to the effect size, as @Scortchi nicely points out in his answer (+1). So smaller -values correspond to larger effect sizes; of course they are more convincing!In the Neyman-Pearson framework, the goal is to obtain a binary decision: either the evidence is \"significant\" or it is not. By choosing the threshold , we guarantee that we will not have more than  false positives. Note that different people can have different  in mind when looking at the same data; perhaps when I read a paper from a field that I am skeptical about, I would not personally consider as \"significant\" results with e.g.  even though the authors do call them significant. My personal  might be set to  or something. Obviously the lower the reported -value, the more skeptical readers it will be able to convince! Hence, again, lower -values are more convincing.The currently standard practice is to combine Fisher and Neyman-Pearson approaches: if , then the results are called \"significant\" and the -value is [exactly or approximately] reported and used as a measure of convincingness (by marking it with stars, using expressions as \"highly significant\", etc.); if  , then the results are called \"not significant\" and that's it. This is usually referred to as a \"hybrid approach\", and indeed it is hybrid. Some people argue that this hybrid is incoherent; I tend to disagree. Why would it be invalid to do two valid things at the same time?Further reading:Is the \u0026quot;hybrid\u0026quot; between Fisher and Neyman-Pearson approaches to statistical testing really an \u0026quot;incoherent mishmash\u0026quot;? -- my question about the \"hybrid\". It generated some discussion, but I am still not satisfied with any of the answers, and plan to get back to that thread at some point.Is it wrong to refer to results as being \u0026quot;highly significant\u0026quot;? -- see my yesterday's answer, which is essentially saying: it isn't wrong (but perhaps a bit sloppy).Why are lower p-values not more evidence against the null? Arguments from Johansson 2011 -- an example of an anti-Fisher paper arguing that -values do not provide evidence against the null; the top answer by @Momo does a good job in debunking the arguments. My answer to the title question is: But of course they are.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2015-02-18 08:21:46","Question_id":137702}
{"_id":{"$oid":"5837a588a05283111e4d6bfb"},"Last_activity":"2016-07-31 08:29:14","Creator_reputation":16289,"Question_score":1,"Answer_content":"The p-value cannot be a measure of surprise because it is only a measure of probability when the null is true. If the null is true then each possible value of p is equally likely. One cannot be surprised at any p-value prior to deciding to reject the null. Once one decides there is an effect then the p-value's meaning vanishes. One merely reports it as a link  in a relatively weak inductive chain to justify the rejection, or not, of the null. But if it was rejected it actually no longer has any meaning.","Display_name":"John","Creater_id":601,"Start_date":"2016-07-29 18:51:38","Question_id":137702}
{"_id":{"$oid":"5837a588a05283111e4d6bfc"},"Last_activity":"2015-02-18 09:22:35","Creator_reputation":406,"Question_score":1,"Answer_content":"Thank you for the comments and suggested readings. I've had some more time to ponder on this problem and I believe I've managed to isolate my main sources of confusion.Initially I thought there was a dichotomy between viewing the p-value as a measure of surprise versus stating that it's not an absolute measure. Now I realise these statements don't necessarily contradict each other. The former allows us to be more or less confident in the extremeness (unlikeness even?) of an observed effect, compared to other hypothetical results of the same experiment. Whereas the latter only tells us that what might be considered a convincing p-value in one experiment, might not be impressive at all in another one, e.g. if the sample sizes differ. The fact that some fields of science utilise a different baseline of strong p-values, could either be a reflection of the difference in common sample sizes (astronomy, clinical, psychological experiments) and/or an attempt to convey effect size in a p-value. But the latter is an incorrect conflation of the two.Significance is a yes/no question based on the alpha that was chosen prior to the experiment. A p-value can therefore not be more significant than another one, since they are either smaller or larger than the chosen significance level. On the other hand, a smaller p-value will be more convincing than a larger one (for a similar sample size/identical experiment, as mentioned in my first point).Confidence intervals inherently convey the effect size, making them a nice choice to guard against the issues mentioned above.","Display_name":"Zenit","Creater_id":62518,"Start_date":"2015-02-18 09:22:35","Question_id":137702}
{"_id":{"$oid":"5837a588a05283111e4d6bfd"},"Last_activity":"2015-02-18 03:18:05","Creator_reputation":18720,"Question_score":7,"Answer_content":"I don't know what's meant by smaller p-values being \"better\", or by us being \"more confident in\" them. But regarding p-values as a measure of how surprised we should be by the data, if we believed the null hypothesis, seems reasonable enough; the p-value is a monotonic function of the test statistic you've chosen to measure discrepancy with the null hypothesis in a direction you're interested in, calibrating it with respect to its properties under a relevant procedure of sampling from a population or random assignment of experimental treatments. \"Significance\" has become a technical term  to refer to p-values' being either above or below some specified value; thus even those with no interest in specifying significance levels \u0026amp; accepting or rejecting hypotheses tend to avoid phrases such as \"highly significant\"\u0026mdash;mere adherence to convention.Regarding the dependence of p-values on sample size \u0026amp; effect size, perhaps some confusion arises because e.g. it might seem that 474 heads out of 1000 tosses should be less surprising than 2 out of 10 to someone who thinks the coin is fair\u0026mdash;after all the sample proportion only deviates a little from 50% in the former case\u0026mdash;yet the p-values are about the same. But true or false don't admit of degrees; the p-value's doing what's asked of it: often confidence intervals for a parameter are really what's wanted to assess how precisely an effect's been measured, \u0026amp; the practical or theoretical importance of its estimated magnitude.","Display_name":"Scortchi","Creater_id":17230,"Start_date":"2015-02-16 05:30:30","Question_id":137702}
{"_id":{"$oid":"5837a588a05283111e4d6c0a"},"Last_activity":"2016-07-31 14:09:52","Creator_reputation":30075,"Question_score":9,"Answer_content":"I think there is not much wrong in saying that the results are \"highly significant\" (even though yes, it is a bit sloppy).It means that if you had set a much smaller significance level , you would still have judged the results as significant. Or, equivalently, if some of your readers have a much smaller  in mind, then they can still judge your results as significant.Note that the significance level  is in the eye of the beholder, whereas the -value is (with some caveats) a property of the data.Observing  is just not the same as observing , even though both might be called \"significant\" by standard conventions of your field (). Tiny -value means stronger evidence against the null (for those who like Fisher's framework of hypothesis testing); it means that the confidence interval around the effect size will exclude the null value with a larger margin (for those who prefer CIs to -values); it means that the posterior probability of the null will be smaller (for Bayesians with some prior); this is all equivalent and simply means that the findings are more convincing. See Are smaller p-values more convincing? for more discussion.The term \"highly significant\" is not precise and does not need to be. It is a subjective expert judgment, similar to observing a surprisingly large effect size and calling it \"huge\" (or perhaps simply \"very large\"). There is nothing wrong with using qualitative, subjective descriptions of your data, even in the scientific writing; provided of course, that the objective quantitative analysis is presented as well. See also some excellent comments above, +1 to @whuber, @Glen_b, and @COOLSerdash.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2015-02-16 16:03:17","Question_id":107640}
{"_id":{"$oid":"5837a588a05283111e4d6c0b"},"Last_activity":"2015-02-16 16:35:22","Creator_reputation":4134,"Question_score":3,"Answer_content":"A test is a tool for a black-white decision, i.e. it tries to answer a yes/no question like 'is there a true treatment effect?'. Often, especially if the data set is large, such question is quite a waste of resources. Why asking a binary question if it is possible to get an answer to a quantitative question like 'how large is the true treatment effect?' that implicitly answers also the yes/no question? So instead of answering an uninformative yes/no question with high certainty, we often recommend the use of confidence intervals that contains much more information.","Display_name":"Michael M","Creater_id":30351,"Start_date":"2014-07-11 12:00:23","Question_id":107640}
{"_id":{"$oid":"5837a588a05283111e4d6c0c"},"Last_activity":"2014-07-11 20:06:18","Creator_reputation":195,"Question_score":3,"Answer_content":"This is a common question.A similar question may be \"Why is p\u0026lt;=0.05 considered significant?\" (http://www.jerrydallal.com/LHSP/p05.htm)@Michael-Mayer gave one part of the answer: significance is only one part of the answer.  With enough data, usually some parameters will show up as \"significant\" (look up Bonferroni correction).  Multiple testing is a specific problem in genetics where large studies looking for significance are common and p-values \u0026lt;10-8 are often required (http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2621212/).Also, one issue with many analyses is that they were opportunistic and not pre-planned (i.e. \"If you torture the data enough, nature will always confess.\" - Ronald Coase).Generally, if an analysis is pre-planned (with a repeated-analysis correction for statistical power), it can be considered significant.  Often, repeated testing by multiple individuals or groups is the best way to confirm that something works (or not).  And repetition of results is most often the right test for significance.","Display_name":"Bill Denney","Creater_id":51954,"Start_date":"2014-07-11 20:06:18","Question_id":107640}
{"_id":{"$oid":"5837a589a05283111e4d6c1f"},"Last_activity":"2014-03-19 06:14:52","Creator_reputation":63,"Question_score":2,"Answer_content":"Copy-Paste of the answer from @Henrik:use afex::mixed as in mixed(acc ~ race + sex + emotion + sex:emotion + race:emotion + score +(1|subj), family=binomial, data=subset, method = \"LRT\"). To obtain p-values based on parametric bootstrap, you can use method = \"PB\" (but you will need to set the number of samples, see help). Also, you most likely need random slopes for your within-subject factors. Your random effects structure seems unreasonable! ","Display_name":"user42174","Creater_id":42174,"Start_date":"2014-03-19 06:14:52","Question_id":90511}
{"_id":{"$oid":"5837a589a05283111e4d6c2e"},"Last_activity":"2016-07-31 10:46:54","Creator_reputation":8055,"Question_score":0,"Answer_content":"The problem you are facing is called class imbalance. Here are some strategies to mitigate the issue:Oversampling: duplicating samples the label of which is the minority class (or equivalently increase the probability of drawing them when creating the minibatches). I believe this is the most common strategy.Undersampling: duplicating samples the label of which is the majority class.Changing the cost function to more heavily penalized mistakes done one the minority samplesDepending on your dataset, you could slightly perturb some of your minority samples to create new minority samples. For example, this is often done with images, which are easy to perturb (rotation, translation, etc.). It's often performed in some field such as computer vision.You might also want to have a look at anomaly detection.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-07-31 10:46:54","Question_id":226503}
{"_id":{"$oid":"5837a589a05283111e4d6c3b"},"Last_activity":"2016-07-31 10:24:55","Creator_reputation":111,"Question_score":0,"Answer_content":"There is a now a note in the package documentation that answers this:NoteIt can happen that some values of the LRT statistic in the reference distribution are negative. Whenthis happens one will see that the number of used samples (those where the LRT is positive) arereported (this number is smaller than the requested number of samples).In theory one can not have a negative value of the LRT statistic but in practice on can: We speculatethat the reason is as follows:  We simulate data under the small model and fit both the small andthe large model to the simulated data.  Therefore the large model represents - by definition - anoverfit;  the model has superfluous parameters in it.  Therefore the fit of the two models will forsome simulated datasets be very similar resulting in similar values of the log-likelihood.  There isno guarantee that the the log-likelihood for the large model in practice always will be larger thanfor the small (convergence problems and other numerical issues can play a role here).To look further into the problem, one can use thePBrefdist()function for simulating the refer-ence distribution (this reference distribution can be provided as input toPBmodcomp()). Inspectionsometimes reveals that while many values are negative, they are numerically very small. In this caseone may try to replace the negative values by a small positive value and then invokePBmodcomp()to get some idea about how strong influence there is on the resulting p-values.  (The p-values getsmaller this way compared to the case when only the originally positive values are used).","Display_name":"Andrew Olney","Creater_id":77273,"Start_date":"2016-07-31 10:24:55","Question_id":168632}
{"_id":{"$oid":"5837a589a05283111e4d6c48"},"Last_activity":"2016-07-31 10:15:14","Creator_reputation":8055,"Question_score":1,"Answer_content":"Empirically. People typically define a patience, i.e. the number of epochs to wait before early stop if no progress on the validation set. The patience is often set somewhere between 10 and 100 (10 or 20 is more common), but it really depends on your dataset and network. Example with patience = 10:","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-07-31 10:15:14","Question_id":226539}
{"_id":{"$oid":"5837a589a05283111e4d6c55"},"Last_activity":"2016-07-31 09:56:47","Creator_reputation":466,"Question_score":4,"Answer_content":"There is a new paper, A Signiﬁcance Test for the Lasso, including the inventor of LASSO as an author that reports results on this problem. This is a relatively new area of research, so the references in the paper cover a lot of what is known at this point. As for your second question, have you tried ? Often there is a value in this middle range that achieves a good compromise. This is called Elastic Net regularization. Since you are using cv.glmnet, you will probably want to cross-validate over a grid of  values. ","Display_name":"MichaelJ","Creater_id":17702,"Start_date":"2014-06-11 21:02:47","Question_id":45449}
{"_id":{"$oid":"5837a589a05283111e4d6c62"},"Last_activity":"2016-07-31 09:37:50","Creator_reputation":8367,"Question_score":0,"Answer_content":"No, significance testing of coefficients is not a good method of model selection. Better methods of model selection include AIC, Bayes factors, and cross-validated prediction error. What you should use depends on exactly what you want to do. Trying to find out whether crop type has a nonzero effect is not a sensible goal because  is nonzero, but also effectively 0 for all human purposes, so knowing that the effect is nonzero would be uninformative. If you want to estimate the effect of crop type, you could try searching for a predictively accurate model with cross-validation and then looking at the appropriate coefficients.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-31 09:37:50","Question_id":226516}
{"_id":{"$oid":"5837a589a05283111e4d6c71"},"Last_activity":"2016-07-31 04:08:20","Creator_reputation":5445,"Question_score":7,"Answer_content":"The meanings of the fixed effects change when you add an interaction, and often it makes no sense to interpret the main effects in the presence of an interactionWithout the interaction, the fixed effects can be interpreted on their own. In your first model, without the interaction,treatment3 is the mean difference in amp.sqrt between the treatment1 group and the treatment3 group with the other variables held constant.However, with the addition of the interaction treatment:time, treatment3 is now  the mean difference in amp.sqrt between the treatment1 group and the treatment3 group with the other variables held constant, but in particular with time held equal to it's reference level.In order to ascertain whether it makes any sense at all to interpret the main effects in the presence of the interaction it is important to understand the data. As mentioned above, the main effect for treatment3 now means the difference between the treatment3 and treatment1 group, when time is at it's reference level(-14.929). The interactions then give the additive differences for each combination of levels of the factors.So we can arrive at these interpretations of your output:For treatment1 at time6, we have amp.sqrt = 130.587For treatment2 at time6, we have amp.sqrt = 130.587 - 3.766For treatment3 at time6, we have amp.sqrt = 130.587 - 14.929For treatment1 at time7, we have amp.sqrt = 130.587 - 7.697For treatment2 at time7, we have amp.sqrt = 130.587 - 7.697 - 3.766 + 9.697For treatment3 at time7, we have amp.sqrt = 130.587 - 7.697 - 14.929 + 53.206For treatment1 at time8, we have amp.sqrt = 130.587 - 2.628For treatment2 at time8, we have amp.sqrt = 130.587 - 2.628 - 3.766 + 8.554For treatment3 at time8, we have amp.sqrt = 130.587 - 2.628 - 14.929 + 62.411If this still seems \"odd\" to you, then a simple plot may make help to make more sense of it:So to pull the discussion back to your question \"Why is there an odd output result when adding an interaction term\", I would say that there doesn't appear to be anything odd. The main effects just have a different interpretation, which is not particularly useful: So looking again at the treatment3        -14.929 estimate, this means that the response is 14.9 units lower in the treatment3 group, than the treatment1 group at time=6, as indicated on the plot.  Moreover, if we look at the output for the model without the interaction, there are positive estimates for the time and treatment variables. This is consistent with the above plot because we see from the plot that on average there is an increasing trend in the response with increasing time (consistent with the positive estimates in the no-interaction model for time). Also, on average the lines on the plot fortreatment1 and treatment2 are similar to each other (consistent with the fixed effect of treatment2 in the no-interaction model being small), while on average the line for treatment3 is much higher than that for the other treatments (consistent with the large fixed effect for treatment3 in the no-interaction model). ","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-31 03:02:03","Question_id":226505}
{"_id":{"$oid":"5837a589a05283111e4d6c7e"},"Last_activity":"2016-07-31 08:48:44","Creator_reputation":11910,"Question_score":37,"Answer_content":"What is a difference in differences estimatorDifference in differences (DiD) is a tool to estimate treatment effects comparing the pre- and post-treatment differences in the outcome of a treatment and a control group. In general, we are interested in estimating the effect of a treatment  (e.g. union status, medication, etc.) on an outcome  (e.g. wages, health, etc.) as inY_{it} = \\alpha_i + \\lambda_t + \\rho D_{it} + X'_{it}\\beta + \\epsilon_{it}where  are individual fixed effects (characteristics of individuals that do not change over time),  are time fixed effects,  are time-varying covariates like individuals' age, and  is an error term. Individuals and time are indexed by  and , respectively. If there is a correlation between the fixed effects and  then estimating this regression via OLS will be biased given that the fixed effects are not controlled for. This is the typical omitted variable bias.To see the effect of a treatment we would like to know the difference between a person in a world in which she received the treatment and one in which she does not. Of course, only one of these is ever observable in practice. Therefore we look for people with the same pre-treatment trends in the outcome. Suppose we have two periods  and two groups . Then, under the assumption that the trends in the treatment and control groups would have continued the same way as before in the absence of treatment, we can estimate the treatment effect as\\rho = (E[Y_{ist}|s=A,t=2] - E[Y_{ist}|s=A,t=1]) - (E[Y_{ist}|s=B,t=2] - E[Y_{ist}|s=B,t=1])Graphically this would look something like this:You can simply calculate these means by hand, i.e. obtain the mean outcome of group  in both periods and take their difference. Then obtain the mean outcome of group  in both periods and take their difference. Then take the difference in the differences and that's the treatment effect. However, it is more convenient to do this in a regression framework because this allows youto control for covariatesto obtain standard errors for the treatment effect to see if it is significantTo do this, you can follow either of two equivalent strategies. Generate a control group dummy  which is equal to one if a person is in group  and zero otherwise, generate a time dummy  which is equal to one of  and zero otherwise, and then regressY_{it} = \\beta_1 + \\beta_2 (\\text{treat}_i) + \\beta_3 (\\text{time}_t) + \\rho (\\text{treat}_i \\cdot \\text{time}_t) + \\epsilon_{it}Or you simply generate a dummy  which equals one if a person is in the treatment group AND the time period is the post-treatment period and is zero otherwise. Then you would regressY_{it} = \\beta_1 \\gamma_s + \\beta_2 \\lambda_t + \\rho T_{it} + \\epsilon_{it}where  is again a dummy for the control group and  are time dummies. The two regressions give you the same results for two periods and two groups. The second equation is more general though as it easily extends to multiple groups and time periods. In either case, this is how you can estimate the difference in differences parameter in a way such that you can include control variables (I left those out from the above equations to not clutter them up but you can simply include them) and obtain standard errors for inference.Why is the difference in differences estimator useful?As stated before, DiD is a method to estimate treatment effects with non-experimental data. That's the most useful feature. DiD is also a version of fixed effects estimation. Whereas the fixed effects model assumes , DiD makes a similar assumption but at the group level, . So the expected value of the outcome here is the sum of a group and a time effect. So what's the difference? For DiD you don't necessarily need panel data as long as your repeated cross sections are drawn from the same aggregate unit . This makes DiD applicable to a wider array of data than the standard fixed effects models that require panel data.Can we trust difference in differences?The most important assumption in DiD is the parallel trends assumption (see the figure above). Never trust a study that does not graphically show these trends! Papers in the 1990s might have gotten away with this but nowadays our understanding of DiD is much better. If there is no convincing graph that shows the parallel trends in the pre-treatment outcomes for the treatment and control groups, be cautious. If the parallel trends assumption holds and we can credibly rule out any other time-variant changes that may confound the treatment, then DiD is a trustworthy method.Another word of caution should be applied when it comes to the treatment of standard errors. With many years of data you need to adjust the standard errors for autocorrelation. In the past, this has been neglected but since Bertrand et al. (2004) \"How Much Should We Trust Differences-In-Differences Estimates?\" we know that this is an issue. In the paper they provide several remedies for dealing with autocorrelation. The easiest is to cluster on the individual panel identifier which allows for arbitrary correlation of the residuals among individual time series. This corrects for both autocorrelation and heteroscedasticity.For further references see these lecture notes by Waldinger and Pischke.","Display_name":"Andy","Creater_id":26338,"Start_date":"2014-11-24 06:27:25","Question_id":564}
{"_id":{"$oid":"5837a589a05283111e4d6c7f"},"Last_activity":"2016-02-13 20:07:54","Creator_reputation":116,"Question_score":1,"Answer_content":"It is a technique widely used in econometrics to examine the influence of any exogenous event in a time series. You pick two separate groups of data relating to before and after the event studied. A good reference to learn more is the book Introduction to Econometrics by Wooldridge.","Display_name":"Carlos Dutra","Creater_id":100832,"Start_date":"2016-02-13 20:07:54","Question_id":564}
{"_id":{"$oid":"5837a589a05283111e4d6c80"},"Last_activity":"2010-07-23 13:42:43","Creator_reputation":2065,"Question_score":5,"Answer_content":"Wikipedia has a decent entry on this subject, but why not just use linear regression allowing for interactions between your independent variables of interest? This seems more interpretable to me. Then you might read up on analysis of simple slopes (in the Cohen et al book free on Google Books) if your variables of interest are quantitative.","Display_name":"Stephen Turner","Creater_id":36,"Start_date":"2010-07-23 13:42:43","Question_id":564}
{"_id":{"$oid":"5837a589a05283111e4d6c8c"},"Last_activity":"2016-05-23 13:56:41","Creator_reputation":153,"Question_score":2,"Answer_content":"I see two questions here.1) What is the difference between weights and parms in rpart?If you look at the code, weights argument is passed to the model.frame object, so it should be applied towards each observation of your dataset, just like in lm.if (is.data.frame(model)) {    m \u0026lt;- model  ## \u0026lt;---- m is defined here    model \u0026lt;- FALSE}else {    indx \u0026lt;- match(c(\"formula\", \"data\", \"weights\", \"subset\"),         names(Call), nomatch = 0L)    if (indx[1] == 0L)         stop(\"a 'formula' argument is required\")    temp \u0026lt;- Call[c(1L, indx)]    tempy), where y indicates your response variable. For example, you might want to try something like the following:fit \u0026lt;- rpart(y ~ x1 + x2 + x3, data = data, parms = list(prior = c(0.000066, 1 - 0.000066)))","Display_name":"Boxuan","Creater_id":22012,"Start_date":"2016-05-23 13:56:41","Question_id":162372}
{"_id":{"$oid":"5837a589a05283111e4d6c9b"},"Last_activity":"2016-07-31 07:25:48","Creator_reputation":5445,"Question_score":1,"Answer_content":"  1) Is the z-value similar to the effect size?No, it is a Wald statistic to test the null hypothesis that the estimate is zero.  2) If not, how can I obtain the effect size for each variable?Since this is a generalized linear mixed model, you can't calculate effect sizes such as cohen's d, but since it is a logistic model with a logit link you can report odds ratios as effect sizes. The raw coefficients are on the log-odds scale, so to calculate the odds ratios, these are just exponentiated.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-31 07:25:48","Question_id":226545}
{"_id":{"$oid":"5837a589a05283111e4d6ca9"},"Last_activity":"2016-07-31 06:20:10","Creator_reputation":152738,"Question_score":0,"Answer_content":" is not \"the slope\" -- the model with the canonical link isn't linear on the scale of the response, so what would it be a \"slope\" of?There are a variety of kinds of residuals commonly associated with a GLM (deviance, Anscombe, Pearson, working). None of them are on the log scale.  The ones you're referring to there would be the working residuals (whereas if you had done residuals(mdl) you'd get deviance residuals by default).For a Poisson model, the working residuals are .[For a quasiPoisson model they'd presumably be scaled to adjust for the dispersion.]","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-31 06:20:10","Question_id":226535}
{"_id":{"$oid":"5837a589a05283111e4d6cb6"},"Last_activity":"2016-07-31 05:48:31","Creator_reputation":5445,"Question_score":1,"Answer_content":"While there is no universal definition, the most generally accepted view (for example see McNamee, 2003) is that for a variable to be a confounder it must be: (i) a cause, or a proxy of a cause, of the outcome; (ii) a cause, or a proxy of a cause, of the main exposure; and (iii) unaffected by the main exposure, thus not a mediator, i.e. not on the causal pathway from exposure to the outcome. Therefore, the answer to your question, is no, you should not treat them as confounders.However, this does not mean that you should not include them in a regression model. Whether or not to control for them crucially depends on the causal process(es) you assume (or is known) for the data. If the other variables are risk-factors that are not on the causal path between your main exposure and the outcome (otherwise known as competing exposures) you should include them, since this will increase the precision of the estimate for your main exposure, unless they are highly correlated with each other, in which case one or the other would suffice. However, if they are on the causal path (ie they are mediators) then you should not include them, as this could introduce bias due to the reversal paradox (of which Simpson's paradox, Lord's paradox and suppression are examples - see Tu, Gunnell and Gilthorpe, 2008 for more details on this).So, in your case, it seems that you can rule out that maternal short stature or low birth weight cause poor food hygiene, thus, they should not be treated as confounders. If maternal short stature and low birth weight are unaffected by poor food hygiene, then you can include them as competing exposures,  However, if poor food hygiene is a cause of maternal short stature or low birth weight, then you should not include them.References:McNamee R. Confounding and confounders. Occup Environ Med 2003; 60(3):227-234.Tu YK, Gunnell D, Gilthorpe MS. Simpson's Paradox, Lord's Paradox, and Suppression Effects are the same phenomenon - the reversal paradox. Emerg Themes Epidemiol 2008; 5:2.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-31 05:48:31","Question_id":226512}
{"_id":{"$oid":"5837a589a05283111e4d6cca"},"Last_activity":"2016-07-31 04:31:59","Creator_reputation":690,"Question_score":1,"Answer_content":"Your data is ordinal, yes. Self-reported ratings are ordinal in principle. However, you need to seek further literature to decide which tests you can apply. Yes, I know of the \"don't take the mean of ordinal data\" principle. However, this is the theory. In reality, it can turn out that you are better off treating your data as if it were interval. See for example Lewis' paper on usability ratings I cite below. That paper is specifically about usability ratings, and specifically about mean vs. median. You need to find literature which applies to your own case and decide. If there is no such literature, you can either treat the data as ordinal or as interval, but have to provide arguments for your choice, else your reviewers will likely (should!) raise an issue. Lewis, J. R. (1993). Multipoint scales: Mean and median differences and observed significance levels. International Journal of Human-Computer Interaction, 5(4), 383–392. http://doi.org/10.1080/10447319309526075 ","Display_name":"rumtscho","Creater_id":6867,"Start_date":"2016-07-31 04:31:59","Question_id":226533}
{"_id":{"$oid":"5837a589a05283111e4d6cd7"},"Last_activity":"2016-07-31 03:41:16","Creator_reputation":9083,"Question_score":3,"Answer_content":"The problem is that you are simulating data where the risks/probabilities and the group sizes are both relatively low. As a result, the sampling distributions of the log relative risk are not very well approximated by normal distributions and the sampling variances are poorly estimated. Take a look at:sum(d0 == 0 | d1 == 0)You will find that 0 events in at least one of the two groups are relatively common (in more than 15% of the cases). Things start to break down under the 'normal-normal' model under such conditions.Try making the risks not quite so low and the studies a bit larger:p0=0.18  # P(success) in control groupN = round( rchisq(.k, df=2) * 30 + 100 )Things start to look much better then.If you find yourself in a situation where both risks and sample sizes are low, you may have to switch to a 'binomial-normal' model. For relative risks, this is a bit more difficult, since you would have to fit a mixed-effects logistic regression model with a log link, which is a bit more tricky than the usual logit link (which is used to model odds ratios). For 'binomial-normal' models with a logit link, take a look at the rma.glmm() function. For example, if you are patient, you could try:rma.glmm(measure=\"OR\", ai=d1, bi=n1-d1, ci=d0, di=n0-d0, model=\"CM.EL\", verbose=TRUE)Note that you won't be estimating  then, since that applies to the (log) relative risks, not the (log) odds ratios. But when risks are low, then relative risks and odds ratios are similar, so you will get something not too far off.","Display_name":"Wolfgang","Creater_id":1934,"Start_date":"2016-07-31 03:41:16","Question_id":225311}
{"_id":{"$oid":"5837a589a05283111e4d6ce4"},"Last_activity":"2016-07-30 08:45:27","Creator_reputation":76390,"Question_score":10,"Answer_content":"The true parameter value simply is whatever it is.  It isn't clear what \"best case\" or \"worst case\" could mean.  You might be happy or sad about the actual value of the parameter, if you could magically find out its true value, but it is constant.  Both @PeterFlom and @R.Carlos have accurately explained what a confidence interval is.  Here is another way to think about it.  In your case, if you had chosen  (or ) as your null hypothesis and conducted your test at the  level, you would not have rejected the null, but if you had chosen  (or ) instead (or tested at the  level), you would have rejected the null.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-07-30 08:45:27","Question_id":226436}
{"_id":{"$oid":"5837a589a05283111e4d6ce5"},"Last_activity":"2016-07-30 06:19:48","Creator_reputation":61,"Question_score":6,"Answer_content":"As Peter said. Reapeating your experiment x times and calculating CIs everytime then 95% of these CIs will contain the real population parameter which you tried to estimate. The parameter is either in or not. There is no probability assigned of how close your estimate is to the population parameter. ","Display_name":"R. Carlos","Creater_id":125041,"Start_date":"2016-07-30 06:19:48","Question_id":226436}
{"_id":{"$oid":"5837a589a05283111e4d6ce6"},"Last_activity":"2016-07-30 05:41:43","Creator_reputation":57772,"Question_score":10,"Answer_content":"No.  That is not a correct statement; it might not even be a meaningful one.The Wikipedia page for confidence intervals is pretty good, I think.  In particular, this line:  When we say, \"we are 99% confident that the true value of the  parameter is in our confidence interval\", we express that 99% of the  hypothetically observed confidence intervals will hold the true value  of the parameter.is a good explanation of what CIs really are. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-07-30 05:41:43","Question_id":226436}
{"_id":{"$oid":"5837a589a05283111e4d6cf7"},"Last_activity":"2016-07-28 16:50:41","Creator_reputation":15561,"Question_score":4,"Answer_content":"There could all sorts of things going on, but without knowing more about the details of your model and actual commands and results, it will be hard to say more. Don't show us pseudo-code with generic y and x. No one but you can decipher what Xinstr. (Xinstr. * X1) means. At the very least, show us the actual Stata commands you typed. Also, from the parentheses arrangement in your question, it seems like you share the common misunderstanding that instruments map onto the endogenous variables one to one. That's not how IV works.Having said that, the first thing I would try is to make sure that you're comparing apples to apples. In the simple model, the IV and OLS coefficients on  are the marginal effects. In the interactions model, the marginal effects are more complicated and non-linear, so you need to take that into account when comparing. You can't just look at the coefficients.Here's an example:. webuse hsng2, clear(1980 Census housing data). ivregress 2sls rent c.pcturban (c.hsngval = faminc i.region)Instrumental variables (2SLS) regression          Number of obs   =         50                                                  Wald chi2(2)    =      90.76                                                  Prob \u0026gt; chi2     =     0.0000                                                  R-squared       =     0.5989                                                  Root MSE        =     22.166------------------------------------------------------------------------------        rent |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------     hsngval |   .0022398   .0003284     6.82   0.000     .0015961    .0028836    pcturban |    .081516   .2987652     0.27   0.785     -.504053     .667085       _cons |   120.7065   15.22839     7.93   0.000     90.85942    150.5536------------------------------------------------------------------------------Instrumented:  hsngvalInstruments:   pcturban faminc 2.region 3.region 4.region. ivregress 2sls rent c.pcturban (c.hsngval c.hsngval#c.pcturban = faminc i.region)Instrumental variables (2SLS) regression          Number of obs   =         50                                                  Wald chi2(3)    =      95.82                                                  Prob \u0026gt; chi2     =     0.0000                                                  R-squared       =     0.5886                                                  Root MSE        =     22.448--------------------------------------------------------------------------------------                rent |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]---------------------+----------------------------------------------------------------             hsngval |    .012628   .0038516     3.28   0.001     .0050791    .0201769                     |c.hsngval#c.pcturban |  -.0001453   .0000537    -2.71   0.007    -.0002505   -.0000401                     |            pcturban |   7.037653   2.587203     2.72   0.007     1.966828    12.10848               _cons |  -358.7519    177.772    -2.02   0.044    -707.1785   -10.32518--------------------------------------------------------------------------------------Instrumented:  hsngval c.hsngval#c.pcturbanInstruments:   pcturban faminc 2.region 3.region 4.region. margins, dydx(hsngval)Average marginal effects                        Number of obs     =         50Model VCE    : UnadjustedExpression   : Linear prediction, predict()dy/dx w.r.t. : hsngval------------------------------------------------------------------------------             |            Delta-method             |      dy/dx   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------     hsngval |   .0028993   .0004123     7.03   0.000     .0020912    .0037074------------------------------------------------------------------------------. regress rent c.pcturban c.hsngval      Source |       SS           df       MS      Number of obs   =        50-------------+----------------------------------   F(2, 47)        =     47.54       Model |  40983.5269         2  20491.7635   Prob \u0026gt; F        =    0.0000    Residual |  20259.5931        47  431.055172   R-squared       =    0.6692-------------+----------------------------------   Adj R-squared   =    0.6551       Total |    61243.12        49  1249.85959   Root MSE        =    20.762------------------------------------------------------------------------------        rent |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------    pcturban |   .5248216   .2490782     2.11   0.040     .0237408    1.025902     hsngval |   .0015205   .0002276     6.68   0.000     .0010627    .0019784       _cons |   125.9033   14.18537     8.88   0.000     97.36603    154.4406------------------------------------------------------------------------------. regress rent c.pcturban##c.hsngval      Source |       SS           df       MS      Number of obs   =        50-------------+----------------------------------   F(3, 46)        =     53.26       Model |  47553.1926         3  15851.0642   Prob \u0026gt; F        =    0.0000    Residual |  13689.9274        46  297.607117   R-squared       =    0.7765-------------+----------------------------------   Adj R-squared   =    0.7619       Total |    61243.12        49  1249.85959   Root MSE        =    17.251--------------------------------------------------------------------------------------                rent |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]---------------------+----------------------------------------------------------------            pcturban |   3.359486   .6378362     5.27   0.000     2.075588    4.643383             hsngval |   .0068502     .00115     5.96   0.000     .0045353     .009165                     |c.pcturban#c.hsngval |  -.0000666   .0000142    -4.70   0.000    -.0000951    -.000038                     |               _cons |  -97.85703    49.0617    -1.99   0.052    -196.6131    .8990436--------------------------------------------------------------------------------------. margins, dydx(hsngval)Average marginal effects                        Number of obs     =         50Model VCE    : OLSExpression   : Linear prediction, predict()dy/dx w.r.t. : hsngval------------------------------------------------------------------------------             |            Delta-method             |      dy/dx   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------     hsngval |   .0023936   .0002651     9.03   0.000     .0018599    .0029272------------------------------------------------------------------------------Note how in the IV spec with interaction, the coefficient on housing value is over 5.5 times larger than in the simple IV spec. The marginal effect (averaging over percent urban), however, is pretty similar.Finally, if you only have one instrument you probably want something like this:ivregress 2sls rent c.pcturban (c.hsngval c.hsngval#c.pcturban = c.faminc c.faminc#c.pcturban)margins, dydx(hsngval)A quadratic endogenous variable would be:ivregress 2sls rent c.pcturban (c.hsngval##c.hsngval = c.faminc##c.faminc)margins, dydx(hsngval)The example above did not work out as nicely with these, so I used two instruments.","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-27 14:18:10","Question_id":225947}
{"_id":{"$oid":"5837a589a05283111e4d6d03"},"Last_activity":"2016-07-31 02:40:34","Creator_reputation":198,"Question_score":2,"Answer_content":"It depends on what the research question is. Multivariate analyses help to answer the question \"Is there a main effect on any of these DVs?\". So, if there's no theory-driven prediction on any particular DV, it's important to run a MANOVA before running separate ANOVAs to help us interpret our finding. But if the 5 relationships are conceptually distinct, then just running the separate ANOVAs is fine. The problem, I guess, is that it can be difficult to work out if researchers are conjuring up conceptual distinctions post hoc. ","Display_name":"Jon","Creater_id":60644,"Start_date":"2016-07-31 02:40:34","Question_id":226470}
{"_id":{"$oid":"5837a589a05283111e4d6d10"},"Last_activity":"2016-07-31 02:32:46","Creator_reputation":25585,"Question_score":2,"Answer_content":"  Does this lack of  for lmer then mean that there's no way for me to  say how much variance is explained by my model?Even if you had valid , it wouldn't tell you much about how much variance was explained by your model. The whole idea of variance explained is often criticized. In general,  can be misleading. Moreover, the idea of  measuring explained variance applies only to linear regression with one predictor (or here), nonetheless, as described by Achen (1990) it still can be misleading at doing that.That being said, there are several approaches to calculating -like statistic for linear mixed models (see also here), so you can calculate it, but you have to remember that it does not tell you about \"variance explained\" and that it is not a perfect measure.Achen, C. H. (1990). What Does \"Explained Variance\" Explain?: Reply. Political Analysis 2 (1): 173–184.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-31 02:25:12","Question_id":226509}
{"_id":{"$oid":"5837a589a05283111e4d6d1d"},"Last_activity":"2016-07-31 02:31:23","Creator_reputation":198,"Question_score":0,"Answer_content":"Both hemisphere and latitude are fixed factors here, and tree diversity is your dependent variable. It sounds like you're running an ANOVA, but a multiple regression might be more appropriate. In this case, add a third factor, which is hemisphere multiplied by latitude: this is the interaction term. ","Display_name":"Jon","Creater_id":60644,"Start_date":"2016-07-31 02:31:23","Question_id":226498}
{"_id":{"$oid":"5837a589a05283111e4d6d2c"},"Last_activity":"2016-07-31 01:42:56","Creator_reputation":19753,"Question_score":0,"Answer_content":"On such small data, hierarchical clustering is your best choice.The bigger challenge is how to preprocess your data to account for different factors (such as group size) that you do not want to take into account.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-07-31 01:42:56","Question_id":222717}
{"_id":{"$oid":"5837a589a05283111e4d6d2d"},"Last_activity":"2016-07-08 01:39:03","Creator_reputation":233,"Question_score":1,"Answer_content":"In a similar problem with pharmacokinetics curves, I used the standard k-means algorithm. I used it in an automatic way for analyse several datasets, and it worked very well. Each curve is an observation and the variables are the values in each time point (50 variables in this plot)  ","Display_name":"Jesus Herranz Valera","Creater_id":15980,"Start_date":"2016-07-08 01:39:03","Question_id":222717}
{"_id":{"$oid":"5837a589a05283111e4d6d39"},"Last_activity":"2016-07-31 01:25:57","Creator_reputation":19753,"Question_score":0,"Answer_content":"ARI only works for labeled data. I.e. where you already know the \"true\" solution.So even if there was a rule such as \"ARI \u003e 0.8 is good\", it does not help solve any problem. Because you always know a solution that has an ARI of 1.0.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-07-31 01:25:57","Question_id":226345}
{"_id":{"$oid":"5837a589a05283111e4d6d46"},"Last_activity":"2016-04-05 15:08:40","Creator_reputation":76390,"Question_score":3,"Answer_content":"You are right. The situation calls for a paired t-test.  The grades for math and for biology are not independent because they come from the same people.  It is true that some people will be better at math than they are at biology and vice-versa, but you will have some people who score higher on both (due to being more intelligent generally, being better students, being more persistent and organized, etc.), and some who score worse on both.  Any time you can establish a correspondence between a score in one group and a score in another group, you should be using a paired t-test.  It doesn't necessarily have anything to do with before and after\u0026mdash;that is just a convenient example for Stats 101.  Here are some additional non-temporal examples where a paired t-test is appropriate:  Testing sets of identical twins where one twin is randomized to one group and the other sibling is assigned to the other group.  Testing the same person twice simultaneously, such as testing each arm with an allergy skin test and one arm at random is treated with an experimental salve.  To answer your second question, your independent variable is course content.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-04-05 15:08:40","Question_id":205702}
{"_id":{"$oid":"5837a589a05283111e4d6d47"},"Last_activity":"2016-04-05 14:46:57","Creator_reputation":66,"Question_score":2,"Answer_content":"Yes, a paired sample test would be appropriate for comparing the various subject results as long as you have the same students taking the various subject exams. Time is a common way to set up paired t-tests in textbooks (as you mentioned, something like pre-test versus post-test), but it isn't the only application for paired testing.","Display_name":"Chris K","Creater_id":41463,"Start_date":"2016-04-05 14:46:57","Question_id":205702}
{"_id":{"$oid":"5837a589a05283111e4d6d54"},"Last_activity":"2015-06-18 01:18:05","Creator_reputation":24562,"Question_score":48,"Answer_content":"You can prove it by explicitly calculating the conditional density by brute force, as in Procrastinator's link (+1) in the comments. But, there's also a theorem that says all conditional distributions of a multivariate normal distribution are normal. Therefore, all that's left is to calculate the mean vector and covariance matrix. I remember we derived this in a time series class in college by cleverly defining a third variable and using its properties to derive the result more simply than the brute force solution in the link (as long as you're comfortable with matrix algebra). I'm going from memory but it was something like this: Let  be the first partition and  the second. Now define  where . Now we can write \\begin{align*} {\\rm cov}({\\bf z}, {\\bf x}_2) \u0026amp;= {\\rm cov}( {\\bf x}_{1}, {\\bf x}_2 ) + {\\rm cov}({\\bf A}{\\bf x}_2, {\\bf x}_2) \\\\\u0026amp;= \\Sigma_{12} + {\\bf A} {\\rm var}({\\bf x}_2) \\\\\u0026amp;= \\Sigma_{12} - \\Sigma_{12} \\Sigma^{-1}_{22} \\Sigma_{22} \\\\\u0026amp;= 0\\end{align*}Therefore  and  are uncorrelated and, since they are jointly normal, they are independent. Now, clearly , therefore it follows that \\begin{align*}E({\\bf x}_1 | {\\bf x}_2) \u0026amp;= E( {\\bf z} - {\\bf A} {\\bf x}_2 | {\\bf x}_2) \\\\\u0026amp; = E({\\bf z}|{\\bf x}_2) -  E({\\bf A}{\\bf x}_2|{\\bf x}_2) \\\\\u0026amp; = E({\\bf z}) - {\\bf A}{\\bf x}_2 \\\\\u0026amp; = {\\boldsymbol \\mu}_1 + {\\bf A}  ({\\boldsymbol \\mu}_2 - {\\bf x}_2) \\\\\u0026amp; = {\\boldsymbol \\mu}_1 + \\Sigma_{12} \\Sigma^{-1}_{22} ({\\bf x}_2- {\\boldsymbol \\mu}_2)\\end{align*}which proves the first part. For the covariance matrix, note that \\begin{align*}{\\rm var}({\\bf x}_1|{\\bf x}_2) \u0026amp;= {\\rm var}({\\bf z} - {\\bf A} {\\bf x}_2 | {\\bf x}_2) \\\\\u0026amp;= {\\rm var}({\\bf z}|{\\bf x}_2) + {\\rm var}({\\bf A} {\\bf x}_2 | {\\bf x}_2) - {\\bf A}{\\rm cov}({\\bf z}, -{\\bf x}_2) - {\\rm cov}({\\bf z}, -{\\bf x}_2) {\\bf A}' \\\\\u0026amp;= {\\rm var}({\\bf z}|{\\bf x}_2) \\\\\u0026amp;= {\\rm var}({\\bf z})\\end{align*}Now we're almost done: \\begin{align*}{\\rm var}({\\bf x}_1|{\\bf x}_2) = {\\rm var}( {\\bf z} ) \u0026amp;= {\\rm var}( {\\bf x}_1 + {\\bf A} {\\bf x}_2 ) \\\\\u0026amp;= {\\rm var}( {\\bf x}_1 ) + {\\bf A} {\\rm var}( {\\bf x}_2 ) {\\bf A}'+ {\\bf A} {\\rm cov}({\\bf x}_1,{\\bf x}_2) + {\\rm cov}({\\bf x}_2,{\\bf x}_1) {\\bf A}' \\\\\u0026amp;= \\Sigma_{11} +\\Sigma_{12} \\Sigma^{-1}_{22} \\Sigma_{22}\\Sigma^{-1}_{22}\\Sigma_{21}- 2 \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} \\\\\u0026amp;= \\Sigma_{11} +\\Sigma_{12} \\Sigma^{-1}_{22}\\Sigma_{21}- 2 \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21} \\\\\u0026amp;= \\Sigma_{11} -\\Sigma_{12} \\Sigma^{-1}_{22}\\Sigma_{21}\\end{align*}which proves the second part. Note: For those not very familiar with the matrix algebra used here, this is an excellent resource. Edit: One property used here this is not in the matrix cookbook (good catch @FlyingPig) is property 6 on the wikipedia page about covariance matrices: which is that for two random vectors , {\\rm var}({\\bf x}+{\\bf y}) = {\\rm var}({\\bf x})+{\\rm var}({\\bf y}) + {\\rm cov}({\\bf x},{\\bf y}) + {\\rm cov}({\\bf y},{\\bf x}) For scalars, of course,  but for vectors they are different insofar as the matrices are arranged differently.","Display_name":"Macro","Creater_id":4856,"Start_date":"2012-06-16 16:29:22","Question_id":30588}
{"_id":{"$oid":"5837a589a05283111e4d6d60"},"Last_activity":"2016-07-30 01:14:52","Creator_reputation":5445,"Question_score":3,"Answer_content":"Yes this is a good model for your data and your research question. However, note that the syntax your wrote is for lmer from the lme4 package, not lme from the nlme package. lme4 is more recent is preferred to nlme unless you want to model covariance structures.The model will estimate the fixed effect of genotype while controlling for the random effect of each mouse (since measurements on the same mouse may be more alike one another than those on a different mouse).","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-30 01:14:52","Question_id":226371}
{"_id":{"$oid":"5837a589a05283111e4d6d71"},"Last_activity":"2016-07-30 22:27:52","Creator_reputation":2865,"Question_score":0,"Answer_content":"I think (1) and (2) are equal, with different interpretations of the  that we are summing over.For clarity I'll rewrite the formulas as p(X \\mid \\theta) = \\sum_\\mathbf{Z} p(X,Z \\mid \\theta) = \\sum_\\mathbf{Z} \\prod_N a_n^k \\qquad (1)p(X \\mid \\theta) = \\prod_N \\sum_\\mathbf{z} a_n^k \\qquad (2)a_n^k=\\sum_K z_n^k p(x_n\\mid\\mu_k, \\Sigma_k)\\pi_kAs you well noted, in (1) there are  different values in , well in (2) there are  different values in . Note that because  is a one-hot vector, the number of possible values for  is equal to the dimension of  (which is the other  that we are summing over in ).Expanding (2)\\prod_N \\sum_\\mathbf{z} a_n^k=\\prod_N(a_n^1+a_n^2+...+a_n^K)=(a_1^1a_2^1...a_N^1)+(a_1^2a_2^1...a_N^1)+...+(a_1^Ka_2^K...a_N^K)=\\sum_\\mathbf{Z} \\prod_N a_n^kIn fact as  being one-hot, we havep(X \\mid \\theta) = \\prod_N \\sum_\\mathbf{z} a_n^k=\\prod_N \\sum_\\mathbf{z} \\sum_K z_n^k p(x_n\\mid\\mu_k, \\Sigma_k)\\pi_k=\\prod_N\\sum_K\\pi_kp(x_n\\mid\\mu_k, \\Sigma_k)which is the original definition. So the above just shows that integrating over the joint probability equals the marginal probability, .It feels like there should be one more subscript or superscript of the  term in the original formula to denote which one-hot vector  it is.","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-07-30 07:27:18","Question_id":226404}
{"_id":{"$oid":"5837a589a05283111e4d6d7e"},"Last_activity":"2016-07-30 22:05:00","Creator_reputation":263,"Question_score":1,"Answer_content":"Part of the reason you're confused may be that you are considering the special case that all null hypotheses are true (i.e. m = m0). When all null hypotheses are true, the FWER and FDR are indeed the same. For m independent tests of true null hypotheses, FDR = FWER = 1-(1-alpha)^m.The difference comes when some null hypotheses are true and some null hypotheses are false. In that case, the FDR tells you the expected proportion of significant tests (not of all tests) that will be Type I errors. Computing the FDR is then not as simple, because it depends on the proportion of null hypotheses that are false and also on power (the probabilities of significance for the tests of the false null hypotheses).Neither FWER nor FDR can ever be greater than 1. The value of 1,000 that you computed is a different error rate called the per-family error rate: PFER = alpha*m.","Display_name":"Bonferroni","Creater_id":109785,"Start_date":"2016-07-30 12:08:50","Question_id":225688}
{"_id":{"$oid":"5837a589a05283111e4d6d8b"},"Last_activity":"2016-07-30 21:46:27","Creator_reputation":6,"Question_score":0,"Answer_content":"I believe I found the answer.  What I suspected is actually true.  \\begin{equation}  {\\bf I}(\\theta_1) \\approx {\\bf I}(\\theta_0)\\end{equation}Because it is assumed  is near, or \"contingent\" to .  And as  we are guaranteed the convergence of the MLE to these true values.  As such, this assumption is \"validated.\"  It is highly limiting, but makes sense why the distribution under the alternative hypothesis is rarely mentioned in literature.This also makes sense since the Wald, Rao, and likelihood tests are asymptotically equal up to a 1st order.  From 2nd order and up divergences happen.","Display_name":"pellis","Creater_id":124877,"Start_date":"2016-07-30 21:46:27","Question_id":226151}
{"_id":{"$oid":"5837a589a05283111e4d6d98"},"Last_activity":"2016-07-30 20:36:54","Creator_reputation":152738,"Question_score":6,"Answer_content":"This sort of \"problem\" occurs quite naturally and can look this way without actually indicating a problem. (There might be some problem, but a pattern similar to this doesn't necessarily indicate one.)It's a consequence of regression to the mean and arises directly out of fitting the conditional mean (i.e. it's exactly what you expect to see with regression).One thing that might throw off some answerers is that you have your plot \"backward\" to what most of us are used to -- with the random variable on the x axis rather than the y-axis.Here I have generated some data according to a regression model (with a normally distributed predictor and conditionally normal response) and fitted a model of the same form as the one that generated the data. Here's the corresponding plot to yours drawn the other way around:Looking at the slice between the blue lines, the red line (which is just the line with slope 1 and intercept 0) passes very near the mean of the  in that slice. That is, . You are asking if you should \"tweak\" your line to lay closer to the major axis of the roughly elliptical point cloud ... but that is not going to be the \"best fit\" line, and will tend to overpredict the mean for large  values and underpredict it for small y values. If the regression assumptions are reasonable, and assuming you actually want to predict , then there's nothing wrong here -- you see exactly what you should.A case where you might see something like this, and where it might be an issue:However, if your line at the edge of the cloud doesn't pass near the middle of small slices (vertical ones in my case) that might indicate that you have some underprediction (such as might occur if you're shrinking coefficients). That may or may not be a problem: shrinking coefficients toward zero is often quite useful; that will lead to bias but bias isn't the whole story of fitting.A small amount of bias toward zero (shrinkage) in the coefficients will produce a slightly \"shallower\" fit than the least squares line (on my plot; steeper on yours). That's not necessarily a problem at all.It's only if the bias is larger than you want it to be that there would be any need to act at all. Otherwise it could still be doing exactly what it should.So I don't see a problem here -- it looks to me like your model is doing what it should.For reference, here's the plot from the question flipped around:There's some hint that it's slightly biased toward 0 (which as mentioned, may not be a problem), and also perhaps a slight suggestion of a nonlinear relationship (which might potentially be a problem).","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-27 23:37:53","Question_id":225882}
{"_id":{"$oid":"5837a589a05283111e4d6d99"},"Last_activity":"2016-07-27 14:50:55","Creator_reputation":76390,"Question_score":3,"Answer_content":"I think most of the relevant information here has been provided by @EdM.  Let me add a few thoughts:  Regarding your plot, I would put the predicted values on the x-axis and the observed values on the y-axis.  That is the way scatterplots are more typically constructed and may help with interpretation.  In addition, I would make the plot square and force the plotting area to range over the same possible values (say,  to ) on both dimensions.  Lastly, I would plot a LOWESS fit as well as the one to one line.  These should make what is going on easier to see.  My guess is that your fitted model has too shallow a slope relative to the test data, but that the mean prediction is not biased (much).  That shouldn't happen often.  If your data were randomly split into train and test, they should be very similar, so the slopes wouldn't differ by much.  This is also true given that you seem to have a lot of data.  That should make estimates fairly stable.  For me, these facts raise some questions:  Was the split random, or was it by some pre-existing group structure (e.g., males vs. females)?  Did you do a lot of fitting to get your model (such that it was overfitted)?  More specifically, what was your modeling process?  Do you have a lot of variables or complex functions of variables in the original model (and/or relative to the amount of data)?  More information is needed here.  It is also possible that you are missing some curvature in the data.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-07-27 14:50:55","Question_id":225882}
{"_id":{"$oid":"5837a589a05283111e4d6d9a"},"Last_activity":"2016-07-27 08:31:50","Creator_reputation":11445,"Question_score":5,"Answer_content":"What these results tell you is something that is almost inevitable in this type of modeling: a model based on a training set will not fit a test set as well. The type of problem you face, with a difference in the slope of the relation between your linear predictor and the outcome variable between the training and test sets, seems to be one of calibration, as noted here in the context of logistic regression.Your suggestion to \"pivot\" the slope of the line is similar to the general idea of \"shrinking\" regression coefficients to improve predictive ability on new data, but you would be better off using established methods like those provided by the rms package in R. Note that these efforts necessarily entail making a bias-variance tradeoff in predictive modeling. If you are unfamilar with that tradeoff, you should read An Introduction to Statistical Learning or a similar general reference. Also, separate training and test sets might not be the most efficient way to use your data; developing the model on the entire data set and checking and adjusting calibration by bootstrap resampling may be better. Consider consulting Frank Harrell's course notes or his book for more detail.Please check that your axes are labeled correctly. Typically one expects predicted values to be over-optimistic, with a wider range of predicted values than of observed. Also, the range of your results suggests that you might be using proportions as your outcome variable. If so, then there might be an issue if you used standard linear regression to develop your model.","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-07-27 08:31:50","Question_id":225882}
{"_id":{"$oid":"5837a589a05283111e4d6da7"},"Last_activity":"2016-07-28 06:10:23","Creator_reputation":45,"Question_score":0,"Answer_content":"You may be interested in the process model by Bishara et al. (2010). I have some (old and undocumented !) implementation at http://github.com/simkovic/toolbox which may be helpful.In hindsight I would have preferred to implement the model in STAN or some other probabilistic inference engine rather than to use (as the github repo does) the Maximum likelihood method proposed by Bishara \u0026amp; Co.Bishara, A. J., Kruschke, J. K., Stout, J. C., Bechara, A., McCabe, D. P., \u0026amp; Busemeyer, J. R. (2010). Sequential learning models for the Wisconsin card sort task: Assessing processes in substance dependent individuals. Journal of mathematical psychology, 54(1), 5-13.","Display_name":"matus","Creater_id":30080,"Start_date":"2016-07-28 06:10:23","Question_id":226506}
{"_id":{"$oid":"5837a589a05283111e4d6da8"},"Last_activity":"2016-07-25 23:29:17","Creator_reputation":27913,"Question_score":2,"Answer_content":"You have not currently defined your hypothesis, but given what you describe, it is likely that you want to compare the two groups on speed and accuracy on the task switching task.The simplest approach would be to use an independent groups t-test to compare whether the means of the groups differ significantly on your outcome measures (i.e., speed, accuracy). Of course, all the usual issues about comparing two groups apply. In some case people would se a non-parametric test if they were particularly concerned about distributional assumptions. There are also more sophisticated approaches to jointly modelling reaction time and accuracy. Check out this question: How to analyze reaction times and accuracy together?","Display_name":"Jeromy Anglim","Creater_id":183,"Start_date":"2016-07-25 23:29:17","Question_id":226506}
{"_id":{"$oid":"5837a589a05283111e4d6db5"},"Last_activity":"2016-03-23 02:08:08","Creator_reputation":25585,"Question_score":1,"Answer_content":"You have also other choices that are commonly used in such cases, e.g. relative absolute error \\text{RAE} = \\frac{ \\sum^N_{i=1} | \\hat{\\theta}_i - \\theta_i | } {  \\sum^N_{i=1} | \\overline{\\theta} - \\theta_i | } root relative squared error \\text{RRSE} = \\sqrt{ \\frac{ \\sum^N_{i=1} \\left( \\hat{\\theta}_i - \\theta_i \\right)^2 } {  \\sum^N_{i=1} \\left( \\overline{\\theta} - \\theta_i \\right)^2 }} mean absolute percentage error \\text{MAPE} = \\frac{1}{N} \\sum^N_{i=1} \\left| \\frac{\\theta_i - \\hat{\\theta}_i}{\\theta_i} \\right| where  is true value,  is the forecast and  is a mean of  (see also https://www.otexts.org/fpp/2/5).","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-03-23 02:08:08","Question_id":116238}
{"_id":{"$oid":"5837a589a05283111e4d6db6"},"Last_activity":"2015-03-02 08:30:45","Creator_reputation":13,"Question_score":1,"Answer_content":"A possible way would be to normalize the RMSE with the standard deviation of : If this value is larger than 1, you'd obtain a better model by simply generating a random time series of the same mean and standard deviation as .","Display_name":"Fabzi","Creater_id":56077,"Start_date":"2015-03-02 08:30:45","Question_id":116238}
{"_id":{"$oid":"5837a589a05283111e4d6dc5"},"Last_activity":"2016-07-30 15:42:58","Creator_reputation":263,"Question_score":1,"Answer_content":"You have multiple treatment groups that are each compared to the control group, but not compared to each other. There's a method called the Dunnett procedure designed specifically for controlling the familywise error rate in this situation.","Display_name":"Bonferroni","Creater_id":109785,"Start_date":"2016-07-30 15:42:58","Question_id":124307}
{"_id":{"$oid":"5837a589a05283111e4d6dc6"},"Last_activity":"2014-11-17 10:54:43","Creator_reputation":1364,"Question_score":2,"Answer_content":"Yes, it is necessary to adjust for repeated testing to control for increasing probability of false positives. In terms of reporting, I think it is best to report both raw and adjusted p-values, specifying which correction was used (eg Bonferroni). But why did you do a series of t-tests instead of eg ANOVA with post-hoc comparisons?","Display_name":"katya","Creater_id":57390,"Start_date":"2014-11-17 10:54:43","Question_id":124307}
{"_id":{"$oid":"5837a589a05283111e4d6dd2"},"Last_activity":"2016-07-30 15:40:07","Creator_reputation":19753,"Question_score":0,"Answer_content":"Treat every object in the \"junk cluster\" to be a cluster of its own, and you should be able to compute silhouette.I don't know if the results are in any way helpful, because I believe a solution with less noise will be better.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-07-30 15:40:07","Question_id":226385}
{"_id":{"$oid":"5837a589a05283111e4d6ddf"},"Last_activity":"2016-07-30 14:51:30","Creator_reputation":1486,"Question_score":0,"Answer_content":"If I'm understanding your question, you want to pass to a function a unique vector for all permutations of the same value. I'm not sure if there is any standard way to do this in theano or in machine learning, but in Python I would just sort the vector before passing it to the function, since all permutations give the same list when ordered. Python has a sorted function and a sort method to sort lists in place.","Display_name":"Pere","Creater_id":123561,"Start_date":"2016-07-30 14:51:30","Question_id":226130}
{"_id":{"$oid":"5837a589a05283111e4d6dec"},"Last_activity":"2016-04-12 17:31:16","Creator_reputation":1093,"Question_score":1,"Answer_content":"For three raters and ordinal data, you can use a chance-adjusted agreement index with ordinal weights. Options include Conger's , Fleiss' , Bennett et al.'s , Krippendorff's , and Gwet's .These can be calculated in MATLAB using the mReliability functions. They can also be calculated in R using the irr package or the AgreeStat functions, sometimes with different names. ","Display_name":"Jeffrey Girard","Creater_id":111380,"Start_date":"2016-04-12 17:31:16","Question_id":48498}
{"_id":{"$oid":"5837a589a05283111e4d6dfd"},"Last_activity":"2016-01-12 13:51:26","Creator_reputation":13091,"Question_score":2,"Answer_content":"If your data is stationary, then yes, based on the information you provided it seems OK to use . If your data is nonstationary, you should follow Toda-Yamamoto procedure described very explicitly and clearly in Dave Giles' blog post. There are certain important points to pay attention to with respect to lag order selection under cointegrated data (see especially basic steps 5. and 8. of the Toda-Yamamoto procedure in the above source).","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-01-12 13:51:26","Question_id":190224}
{"_id":{"$oid":"5837a589a05283111e4d6e0a"},"Last_activity":"2016-07-30 13:04:04","Creator_reputation":14047,"Question_score":2,"Answer_content":"A good presentation of a Transfer Function (TF) is here Transfer function in forecasting models - interpretation and alternatively here http://en.wikipedia.org/wiki/Distributed_lag. Since we both have a  and one  for simplicity sake then I believe that one can form a TF with appropriate assumed lags and appropriate assumed differences of these two series that would match the assumed ECM, illustrating that the ECM is a particular constrained subset of a TF model. Perhaps some other readers (heavy econometricians) have already gone thought the proof/algebra but I will consider your positive suggestion in helping other readers.After a brief search on the web http://springschool.politics.ox.ac.uk/archive/2008/OxfordECM.pdf discussed how an ECM was a particular case of an ADL (Autoregressive Distributed Lag Model also known as a PDL). An ADL/PDL model is a particular case of a Transfer Function. This material from the above reference shows the equivalence of an ADL and ECM. Note that Transfer Functions are more general than ADL models as they allow explicit decay structure. My point is that the powerful model identification features available with Transfer Functions should be used rather than assuming a model because it fits the desire to have simple explanations such as Short Run/Long Run etc. The Transfer Function model/approach enables robustification by allowing the identification of an arbitrary ARIMA component and the detection of Gaussian Violations such as Pulses/Level Shifts/Seasonal Pulses (Seasonal Dummies) and Local Time Trends along with variance/parameter change augmentations.I would be interested in seeing examples of an ECM that were not functionally equivalent to an ADL model and couldn't be recast as a Transfer Function.","Display_name":"IrishStat","Creater_id":3382,"Start_date":"2015-01-25 15:29:11","Question_id":110757}
{"_id":{"$oid":"5837a589a05283111e4d6e0b"},"Last_activity":"2016-07-30 12:59:23","Creator_reputation":81,"Question_score":3,"Answer_content":"This boils down to maximum likelihood vs. methods of moments, and finite sample efficiency vs. computational expediency. Using a 'proper' AR(1) process and estimating the parameter  (and unknown variance ) via maximum likelihood (ML) gives the most efficient (lowest variance) estimates for a given amount of data.  The regression approach amounts to the Yule-Walker estimation method, which is the method of moments.  For a finite sample it isn't as efficient as ML, but for this case (i.e. an AR model) it has an asymptotic relative efficiency of 1.0 (i.e. with enough data it should give answers nearly as good as ML).  Plus, as a linear method it is computationally efficient and avoids any convergence issues of ML.I gleaned most of this from dim memories of a time series class and Peter Bartlett's lecture notes for Introduction to Time Series, lecture 12 in particular.Note that the above wisdom relates to traditional time series models, i.e. where there are no other variables under consideration.  For time series regression models, where there are various independent (i.e. explanatory) variables, see these other references:Achen, C. H. (2001). Why lagged dependent variables can supress the explanatory power of other independent variables. Annual Meeting of the Polictical Methodology Section of the American Politcal Science Association, 1–42. PDFNelson, C. R., \u0026amp; Kang, H. (1984). Pitfalls in the Use of Time as an Explanatory Variable in Regression. Journal of Business \u0026amp; Economic Statistics, 2(1), 73–82. doi:10.2307/1391356Keele, L., \u0026amp; Kelly, N. J. (2006). Dynamic models for dynamic theories: The ins and outs of lagged dependent variables. Political analysis, 14(2), 186-205.PDF(Thanks to Jake Westfall for the last one).The general take away seems to be \"it depends\".","Display_name":"Thomas Nichols","Creater_id":110985,"Start_date":"2016-04-04 12:33:07","Question_id":110757}
{"_id":{"$oid":"5837a589a05283111e4d6e0c"},"Last_activity":"2014-08-05 11:40:59","Creator_reputation":9475,"Question_score":6,"Answer_content":"There are many approaches to modeling integrated or nearly-integrated time series data. Many of the models make more specific assumptions than more general models forms, and so might be considered as special cases. de Boef and Keele (2008) do a nice job of spelling out various models and pointing out where they relate to one another. The single equation generalized error correction model (GECM; Banerjee, 1993) is a nice one because it is (a) agnostic with respect to the stationarity/non-stationarity of the independent variables, (b) can accommodate multiple dependent variables, random effects, multiple lags, etc, and (c) has more stable estimation properties than two-stage error correction models (de Boef, 2001).Of course the specifics of any given modeling choice will be particular to the researchers' needs, so your mileage may vary.Simple example of GECM:\\Delta{y_{ti}} = \\beta_{0} + \\beta_{\\text{c}}\\left(y_{t-1}-x_{t-1}\\right) + \\beta_{\\Delta{x}}\\Delta{x_{t}} + \\beta_{x}x_{t-1} + \\varepsilonWhere: is the change operator;instantaneous short run effects of  on  are given by ;lagged short run effects of  on  are given by ; andlong run equilibrium effects of  on  are given by .ReferencesBanerjee, A., Dolado, J. J., Galbraith, J. W., and Hendry, D. F. (1993). Co-integration, error correction, and the econometric analysis of non-stationary data. Oxford University Press, USA.De Boef, S. (2001). Modeling equilibrium relationships: Error correction models with strongly autoregressive data. Political Analysis, 9(1):78–94.De Boef, S. and Keele, L. (2008). Taking time seriously. American Journal of Political Science, 52(1):184–200.","Display_name":"Alexis","Creater_id":44269,"Start_date":"2014-08-05 11:32:05","Question_id":110757}
{"_id":{"$oid":"5837a589a05283111e4d6e19"},"Last_activity":"2015-03-20 14:39:57","Creator_reputation":13091,"Question_score":2,"Answer_content":"Regarding the first question, different equations of a VAR model need not have the same lag order. Each equation is meaningful by itself and can be treated separately (as regards estimation). If you find that one of the equations may benefit from including some more regressors, you may as well do that. Regarding the picture, I can understand why you have one full row in the lag 2 matrix, but why do you also have one full column? Based on what you have told, that seems unnecessary.Regarding lag 5, is it plausible that there could be an effect with lag 5? (This is a subject-matter question.) If yes, then consider including just lag 5; including all the lags in between 1 and 5 would not be a parsimonious solution. And you should care about parsimony since your sample is quite small. If lag 5 is quite implausible, maybe the significant autocorrelation at that lag is a false positive that is due to chance?Keep in mind that trying to fit the data very well may lead to overfitting. Using information criteria such as AIC or BIC could help decide between a few sensible candidate models. That means that you would deliberately accept ill-behaved model errors when including extra parameters is too costly due to increased estimation uncertainty. That should give some overall guidance as well as address the questions in the last paragraph.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2015-03-20 14:10:10","Question_id":142618}
{"_id":{"$oid":"5837a589a05283111e4d6e26"},"Last_activity":"2016-07-30 12:33:22","Creator_reputation":3645,"Question_score":1,"Answer_content":"Of course you understand that effect sizes are the preferred metric relative to p-values with large amounts of data in an analysis. The obvious reason for this being that significance is very sensitive to sample size -- with large data, everything is \"significant.\" Effect sizes help decide whether something matters. That said, there is a cult of significance among the technically semi-literate. This is one of the biggest problems with peer-review in publishing.A key question should be the choice of effect size measure. There are many as the term \"effect size\" does not have a single meaning and overlaps strongly with measures of feature relative importance. Ulrike Groemping's papers are one of the best sources for a review of this literature. Pairwise correlations are not an appropriate metric since they are not conditional.The many machine learning workarounds for modeling large numbers of features (e.g., random forests, bags of little jacknifes, etc.), once summarized, would generate the kind of information you seek to create such a plot. One barrier to this graphic will be the fact that most packages report significance only out to several decimal places, preferring to roll up smaller values with a \"\u0026lt;0.0001\" symbol.What would be interesting would be to see how the relationship between p-values and effect size changes as a function of the metric used.","Display_name":"DJohnson","Creater_id":82102,"Start_date":"2016-07-30 12:33:22","Question_id":226477}
{"_id":{"$oid":"5837a589a05283111e4d6e27"},"Last_activity":"2016-07-30 12:02:26","Creator_reputation":690,"Question_score":1,"Answer_content":"What measure of effect size are you using? People who report correlation coefficients tend to report their \"significance\", when they mean \"the significance level of the test that r =/= 0\". I suspect that this happens with other measures too, but have not worked with them sufficiently to be sure that it is common. If this is the case you are talking about, then of course there is a strong correlation 1. It is much \"easier\" to be certain that a correlation 2 of 0.8 is different from zero, than that a correlation 2 of 0.1 is different from zero. Note that this is somewhat of a side effect of limitations on the side of statistical knowledge and computational power available to some scienitifc traditions (such as the psychometry of the late 20th century). If you start doing different tests, their significance needs not be correlated to the strength itself. 1 denotes a correlation between effect size and statistical significance of the effect size, 2 denotes a correlation between your variables of interest, or a measurement of effect size. I had to use this unortodox notation to prevent confusion in the answer. ","Display_name":"rumtscho","Creater_id":6867,"Start_date":"2016-07-30 12:02:26","Question_id":226477}
{"_id":{"$oid":"5837a589a05283111e4d6e28"},"Last_activity":"2016-07-30 11:55:55","Creator_reputation":8367,"Question_score":1,"Answer_content":"Take a look at the formulae for your favorite hypothesis test and notice that the -value depends not only on the effect size, but also on the sample size(s). Furthermore, different tests use different notions of effect size. These two reasons are why no such plot as you have requested can be created for the general case.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-30 11:55:55","Question_id":226477}
{"_id":{"$oid":"5837a589a05283111e4d6e39"},"Last_activity":"2016-07-30 12:00:05","Creator_reputation":8367,"Question_score":2,"Answer_content":"There are a lot of issues here. The first two, and certainly not the least, are that higher-order polynomials will always fit at least as well as lower-order polynomials, and that data that lies perfectly on a parabola can still be significantly linearly correlated. These are some of many reasons that model fit alone is not a good model-selection criterion, and nor is correlation. Better methods of model selection include AIC, cross-validated prediction error, and Bayes factors. Which to use depends in part on what you want the model to do, because different purposes call for different models.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-30 11:49:45","Question_id":226474}
{"_id":{"$oid":"5837a589a05283111e4d6e46"},"Last_activity":"2016-07-30 11:36:25","Creator_reputation":25585,"Question_score":3,"Answer_content":"If you really wanted, then you could use one of multiple proposals for pseudo- for generalized linear models, since Poisson regression is a kind of generalized linear model. However, in general, even if  is popular, it is not the best measure and can be misleading.Instead, what you could do is:If you are comparing models, you could use multiple information criteria like AIC, or BIC, or likelihood-ratio tests.You could use cross-validation and if you are going to use your model for prediction, then you should consider it. By cross-validation we mean splitting the data into two parts, where one part is used for \"training\" your model, and the second part is used to make predictions. By this we test our model on the data that was \"not seen\" by it previously, so we can check how it could possibly behave with external data.In many cases very simple and very revealing thing to do is to plot distribution of your predicted variable and distribution of your predictions on two overlapping histograms or density plots. This may easily make you aware of what exactly does your model predict.Another thing to consider are posterior predictive checks (check also here). The idea is to simulate some random data using your model and then compare the distribution of simulated data, to the real data to check when and how they are similar to each other.Besides, I'd highly recommend to look at diagnostic plots (see also here, here, and here) to make sure if there are no problems with your model.Check also How to calculate goodness of fit in glm (R) andIf the model fits well, nothing can be done?For reading more, I'd highly recommend Data Analysis Using Regression and Multilevel/Hierarchical Models by Andrew Gelman and Jennifer Hill, or Regression Modeling Strategies by Frank E. Harrell.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-30 10:32:33","Question_id":225940}
{"_id":{"$oid":"5837a589a05283111e4d6e47"},"Last_activity":"2016-07-30 10:20:18","Creator_reputation":11445,"Question_score":2,"Answer_content":"You should be cautious in trying to find an -like measure for modeling of a discrete response, as a generalized linear model of a discrete response is based on maximum-likelihood estimates rather than least squares. In R, the summary of a glm model for discrete responses reports the deviance and the Akaike Information Criterion (AIC). This UCLA web page shows how to use a test on residual deviance to evaluate the overall goodness of fit of a Poisson regression. The AIC may also be useful in comparing models.There are several types of pseudo- that have been proposed to be comparable to the  values generated in least-squares analyses. These pose issues of which you should be aware, as explained nicely by this answer from @Gung and the links therein. As noted in that answer,  can be a slippery concept even in least squares.","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-07-30 10:20:18","Question_id":225940}
{"_id":{"$oid":"5837a589a05283111e4d6e58"},"Last_activity":"2016-07-29 16:27:55","Creator_reputation":134,"Question_score":0,"Answer_content":"It can be rewritten as:Of which - under the constraints you specify - the leftmost two terms together are always \u0026lt; 0, and the rightmost two terms together are always \u003e 0. Which means that as long as the two rightmost terms together are larger than the two leftmost terms, you're fine. Despite the fact that specific cases can be found for which this condition isn't met (as answered before, and proving the inequality as it is is false) there clearly are cases for which this constraint is met. For example . This will get you closer to finding that missing constraint which @Matthew Drury mentioned. ","Display_name":"Yuri Robbers","Creater_id":92928,"Start_date":"2016-07-29 15:49:54","Question_id":226364}
{"_id":{"$oid":"5837a589a05283111e4d6e59"},"Last_activity":"2016-07-29 14:55:50","Creator_reputation":12922,"Question_score":2,"Answer_content":"This is false.Take  and .  Then\\frac{2}{\\alpha^2} \\left( e^{\\alpha y} - e^{\\alpha x} \\right) +  e^{\\alpha x} \\left( x^2 - y^2 \\right) =  \\frac{2}{\\alpha^2} \\left( e^{-1} - 1\\right) + 1 \\left(0 - \\frac{1}{\\alpha^2}\\right)Which simplifies to \\frac{1}{\\alpha^2} \\left( 2 e^{-1} - 3 \\right) Which is negative\u0026gt;\u0026gt;\u0026gt; 2 * exp(-1) - 3-2.2642411176571153Perhaps you are missing another constraint?","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-07-29 14:32:56","Question_id":226364}
{"_id":{"$oid":"5837a589a05283111e4d6e68"},"Last_activity":"2016-07-30 08:36:58","Creator_reputation":11925,"Question_score":5,"Answer_content":"A pragmatic answer is that a meaningful mode is surprisingly hard to estimate in real data. Say we get all distinct point estimates in the bootstrap samples, then all estimated coefficients are the mode, which is probably not what we want. We could smooth the distribution, but then the number of modes is dependent on the smoothing parameter, etc. etc.","Display_name":"Maarten Buis","Creater_id":23853,"Start_date":"2016-07-30 08:36:58","Question_id":226434}
{"_id":{"$oid":"5837a589a05283111e4d6e69"},"Last_activity":"2016-07-30 05:47:28","Creator_reputation":57772,"Question_score":4,"Answer_content":"Others here can give you a technical answer as to exactly why we take the mean and not the mode. I expect it has to do with the fact that the mean is the expected value.However, your second sentence:  It's right-skewed, so the mean does not accurately summarize the  distribution.is not really correct, although I can certainly see where you could get that idea. The problem with the mean for skewed distributions is not that it is not an accurate summary but, rather, that for most purposes it is the wrong summary - that is, it is a good estimate of the wrong thing.The classic example is income.  In most countries, income is quite skewed so, if you want a one-number summary, the median corresponds better than the mean to what most people mean by \"summary\"; but for different purposes, the mean could be best, or the mode, or a trimmed mean, or even no measure at all. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-07-30 05:47:28","Question_id":226434}
{"_id":{"$oid":"5837a589a05283111e4d6e78"},"Last_activity":"2016-07-30 09:25:03","Creator_reputation":11,"Question_score":0,"Answer_content":"After some time, I am able to answer my own question.   library(\"RWeka\")  mybag= Bagging(T_apr  ~ ., data=train, control =  Weka_control(W = M5P))For further optimization options see:   WOW(Bagging)","Display_name":"JerryTheForester","Creater_id":111222,"Start_date":"2016-07-30 09:25:03","Question_id":222178}
{"_id":{"$oid":"5837a589a05283111e4d6e85"},"Last_activity":"2016-07-30 09:02:44","Creator_reputation":20452,"Question_score":1,"Answer_content":"Which frequency you should work with should mainly be governed by what you need the forecast for. If you only need daily forecasts, go with the daily data. (You won't get much more than a flat mean forecast with only four data points.) If you need sub-daily forecasts, calculate forecasts using that data.Then again, you may get better forecasts by working with other frequencies. For instance, calculating forecasts on higher frequencies (30 min), then aggregating the forecasts may yield better daily forecasts. One possible framework to integrate forecasts on multiple time granularities is the MAPA algorithm by Kourentzes, Petropoulos and Trapero.This is hard to answer. R happily fitted my toy data with four days of half-hourly observations: require(forecast) set.seed(1) foo \u0026lt;- ts(rnorm(4*48),frequency=48) auto.arima(foo)\"Not very accurate\" is again hard to parse. Some time series simply exhibit inherent variability that cannot be well forecasted. We can't really say more without more information. Do not judge possible forecasting accuracy by your in-sample fit. The in-sample fit can be very good through overfitting, although the out-of-sample accuracy is bad.I'd recommend looking at different ways of forecasting your data - ARIMA, Exponential Smoothing, a naive seasonal forecast, possibly MAPA. This will give you an idea as to how forecastable your data really are.And no forecasting post of mine would be complete without a recommendation for this great free online forecasting textbook.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2015-09-29 00:29:53","Question_id":174644}
{"_id":{"$oid":"5837a589a05283111e4d6e94"},"Last_activity":"2016-07-30 08:26:19","Creator_reputation":11443,"Question_score":12,"Answer_content":"Consider the following setup. We have a -dimensional parameter vector  that specifies the model completely and a maximum-likelihood estimator . The Fisher information in  is denoted . What is usually referred to as the Wald statistic is(\\hat{\\theta} - \\theta)^T I(\\hat{\\theta}) (\\hat{\\theta} - \\theta)where  is the Fisher information evaluated in the maximum-likelihood estimator. Under regularity conditions the Wald statistic follows asymptotically a -distribution with -degrees of freedom when  is the true parameter. The Wald statistic can be used to test a simple hypothesis  on the entire parameter vector. With  the inverse Fisher information the Wald test statistic of the hypothesis  is \\frac{(\\hat{\\theta}_1 - \\theta_{0,1})^2}{\\Sigma(\\hat{\\theta})_{ii}}.Its asymptotic distribution is a -distribution with 1 degrees of freedom.For the normal model where  is the vector of the mean and the variance parameters, the Wald test statistic of testing if  is \\frac{n(\\hat{\\mu} - \\mu_0)^2}{\\hat{\\sigma}^2}with  the sample size.Here  is the maximum-likelihood estimator of  (where you divide by ). The -test statistic is \\frac{\\sqrt{n}(\\hat{\\mu} - \\mu_0)}{s}where  is the unbiased estimator of the variance (where you divide by the ). The Wald test statistic is almost but not exactly equal to the square of the -test statistic, but they are asymptotically equivalent when . The squared -test statistic has an exact -distribution, which converges to the -distribution with 1 degrees of freedom for .The same story holds regarding the -test in one-way ANOVA. ","Display_name":"NRH","Creater_id":4376,"Start_date":"2013-05-30 10:24:19","Question_id":60438}
{"_id":{"$oid":"5837a589a05283111e4d6e95"},"Last_activity":"2013-05-30 14:03:54","Creator_reputation":33236,"Question_score":10,"Answer_content":"@NRH gave a good theoretical answer, here is one that intends to be simpler, more intuitive.There is the formal Wald test (described in the answer by NRH), but we also refer to tests that look at the difference between an estimated parameter and its hypothesized value relative to the variation estimated at the estimated parameter as a Wald style test.  So the t-test as we usually use it is a Wald Style test even if it is slightly different from the exact Wald test (a difference of  vs.  inside a square root).  We could even design a Wald style test based on an estimated median minus the hypothesized median divided by a function of the IQR, but I don't know what distribution it would follow, it would be better to use a bootstrap, permutation, or simulated distribution for this test rather than depending on chi-square asymptotics.  The F-test for ANOVA fits the general pattern as well, the numerator can be thought of as measuring the difference of the means from an overall mean and the denominator is a measure of the variation.Also note that if you Square a random variable that follows a t distribution then it will follow an F distribution with 1 df for the numerator and the denominator df will be those from the t distribution.  Also note that an F distribution with infinite denominator df is a chi-square distribution.  So that means that both the t-statistic (squared) and the F statistic are asymptotically chi-squared just like the Wald statistic.  We just use the more exact distribution in practice. ","Display_name":"Greg Snow","Creater_id":4505,"Start_date":"2013-05-30 14:03:54","Question_id":60438}
{"_id":{"$oid":"5837a589a05283111e4d6ea2"},"Last_activity":"2016-07-30 08:18:54","Creator_reputation":13091,"Question_score":4,"Answer_content":"If your regression-type model has serially correlated residuals, as a remedy you may include lags of the dependent variable as regressors, just as you mentioned. However, you might wish to preserve the original model for convenience of interpretation, direct representation of a theoretical model or other reasons. In such case, you have three options:Do what TPArrow suggested, i.e. keep the original model but allow the model errors to follow an AR process, and use penalized estimation. This way you get a penalized regression with AR errors.Keep the original model but allow the model errors to follow an ARMA (or more generally, SARIMA) process. This way you get a regression with ARMA errors. Leave the model specification intact and use heteroskedasticity and autocorrelation (HAC) robust standard errors.Let us examine the options in more detail:1.See the answer by TPArrow.2.The model can be estimated using functions arima (\"stats\" package)  or auto.arima (\"forecast\" package) in R. You enter the regressors via the argument xreg and select the autoregressive and moving-average lag orders either manually (with arima) or automatically (with auto.arima).Comparing 1. with 2., the question is whether not allowing for moving average components in the error process but using penalization OR allowing for moving average components but not using penalizationworks better for your particular example. I expect none of the two approaches to be uniformly better, so you could try both and see which gives better results. This could be evaluated, for example, by estimating the models on part of the original sample and examining their performance on the remaining part.3.Using HAC-robust standard errors may appear convenient but need not be the best option. Francis X. Diebold warns against that in his blog posts \"The HAC Emperor has no Clothes\" and \"The HAC Emperor has no Clothes: Part 2\" (and I am with him, if my voice counts):  Punting via kernel-HAC estimation is a bad idea in time series, for several reasons:    (1) Kernel-HAC is not likely to produce good  estimates [and that is important is not-so-large samples]. \u0026lt;...\u003e    (2) Kernel-HAC is not likely to produce good  inference [because] \u0026lt;...\u003e kernel-HAC standard errors may be unnecessarily unreliable in small samples, even if they're accurate asymptotically.    (3) Most crucially, kernel-HAC fails to capture invaluable predictive information. \u0026lt;...\u003e    The clearly preferable approach is traditional parametric disturbance heteroskedasticty / autocorrelation modeling, with GLS/ML estimation. Simply allow for ARMA(p,q)-GARCH(P,Q) disturbances (say), with p, q, P and Q selected by AIC (say). (In many applications something like AR(3)-GARCH(1,1) or ARMA(1,1)-GARCH(1,1) would be more than adequate.)(I encourage you to read the entire posts. They are quite short, very accessible and (last, but not the least) authored by a respected time series econometrician.)","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-30 06:00:50","Question_id":226279}
{"_id":{"$oid":"5837a589a05283111e4d6ea3"},"Last_activity":"2016-07-29 07:00:52","Creator_reputation":1106,"Question_score":2,"Answer_content":"The actual answer is to add  some orders of residuals as they are actually autocorrelated. Then, given you are in linear space, the problem reduces to,\\begin{align}\u0026amp; y_t=X_t\\beta+\\epsilon_t,\\\\\u0026amp; \\epsilon_t=\\theta_1 \\epsilon_{t-1}+\\ldots+\\theta_p \\epsilon_{t-p}+e_t\\end{align}where . Now a penalized likelihood will do the order selection. More preciesly you should estimate parameters under  penalized likelihood. Fortunetly it is already done, see DREGAR package in R click here.Update: From your sentence, \"i must add a certain number of lags of the response variable y\" adding lags of response leads to the different scenario than what you have listed in your equation. Both are implemented in DREGAR package.","Display_name":"TPArrow","Creater_id":46139,"Start_date":"2016-07-29 06:54:35","Question_id":226279}
{"_id":{"$oid":"5837a589a05283111e4d6eb0"},"Last_activity":"2016-07-30 08:09:56","Creator_reputation":2542,"Question_score":1,"Answer_content":"Bagging would help stabilize predictions, so it's worth a try, but you'll have to ensure models in the ensemble don't end all looking the same (high correlation between models is bad for bagging).So you could employ another concept, attribute bagging (otherwise called \"Random subspace method\") is useful in that scenario, I myself employed it.It's an ensemble method analogous to bagging, where you subset features instead of samples (Random Forests employ both at once, for example).Check this paper [1] that details the procedure and the properties.From personal experience, AB can work very well, even without bagging in  problems.[1] Bryll, R., Gutierrez-Osuna, R., \u0026amp; Quek, F. (2003). Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets. Pattern recognition, 36(6), 1291-1302. ","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-07-30 08:09:56","Question_id":179836}
{"_id":{"$oid":"5837a589a05283111e4d6ebf"},"Last_activity":"2016-07-30 07:37:39","Creator_reputation":2865,"Question_score":2,"Answer_content":"It seems to me this question is more about \"why do we keep the support vectors (because we only need know  and )\" than \"what are support vectors for\". AFAIK it's because SVMs are often used together with kernels. Without kernels, it is sufficient to store only the decision boundary , in such case the SVM will become a parametric method (instead of a nonparametric as quoted from the book).Kernels can be thought of as mapping the input to an implicit feature space, of which the dimensionality can be infinite (e.g. for RBF kernels). So storing the decision boundary in such high dimensional space would be inefficient (or impossible).If we decompose the decision boundary as a function of some support vectors (as shown in Daneel Olivaw's answer) then the storage would be independent of the dimensionality of the implicit feature space, so that it can work with any type of kernels.If fact if we don't use kernels, NOT to keep support vectors is more efficient in both time and space. Say the dimension of the data is  and the number of support vectors is , we need  space to store  vectors, and the time complexity for inference is also  because we need the inner product between the input vector and all the support vectors. Well if we only keep the parameters, the time and space complexity is both .","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-06-01 02:38:44","Question_id":215641}
{"_id":{"$oid":"5837a589a05283111e4d6ec0"},"Last_activity":"2016-06-01 02:14:23","Creator_reputation":110,"Question_score":4,"Answer_content":"You are right that the SVM decision function  depends only on  and , however it can be shown that  can be expressed as a sum of support vectors. You can consult Stanford's or MIT's course notes, pages 12 and 5 respectively, but basically it can be shown with optimization theory that the optimal weight vector  can be written in the following form:w^*=\\sum_{i=1}^{n}{\\alpha_i^*y_ix_i}  where  is your data   are the attributes,  the labels and  Lagrangian coefficients. Further, it can be shown that only a fraction of the 's are non zero; vectors  for which  are called support vectors, so that is why you need to store all of them and only them to make further predictions  check page 13 of Stanford's course; check also page 7 of MIT's course. So if  is the number of support vectors,  the support vectors and  their corresponding alphas  which are all strictly positive  we can write the optimal  as follows:w^*=\\sum_{i=1}^{N_{sv}}{\\alpha_i^{\\,sv\\,*}y_i^{\\,sv}x_i^{\\,sv}}  The optimal decision function is then:f_{SVM}^*(x) = \\sum_{i=1}^{N_{sv}}{\\alpha_i^{\\,sv\\,*}y_i^{\\,sv}(x_i^{\\,sv}\\cdot x)}+b^*","Display_name":"Daneel Olivaw","Creater_id":115034,"Start_date":"2016-06-01 01:18:47","Question_id":215641}
{"_id":{"$oid":"5837a589a05283111e4d6ecc"},"Last_activity":"2016-07-25 07:29:43","Creator_reputation":20452,"Question_score":2,"Answer_content":"Yes. You can either use an ARIMAX model, or a regression with ARIMA errors. Rob Hyndman explains the difference in his blog post \"The ARIMAX model muddle\". In R, you can use the forecast package to fit regressions with ARIMA errors, or the TSA package to fit an ARIMAX model. ","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2016-07-25 07:29:43","Question_id":225525}
{"_id":{"$oid":"5837a589a05283111e4d6ed9"},"Last_activity":"2016-07-30 07:30:13","Creator_reputation":13091,"Question_score":1,"Answer_content":"You can often tell the seasonal period by the nature of the data. If the process at hand is affected by weather, the period could be 1 year. If it is affected by the working week, the period could be 1 weak. If you don't really know, you could estimate the seasonal period(s) using spectral density as described in Rob J. Hyndman \"Measuring time series characteristics\". Note that the data can have multiple seasonalities, e.g. both yearly and weekly.If you are going to fit a SARIMA model later on, there is something to keep in mind. SARIMA models don't work well with very long period such as 365 days per year. For data with long seasonal periods, Rob J. Hyndman suggests in \"Forecasting with long seasonal periods\" to use Fourier terms in a regression with ARMA errors instead of a SARIMA model. That is also attractive as you may have as many seasonal periods as you like, while SARIMA allows for one (and TBATS allows for two, if I am not mistaken). Moreover, you could include one-offs like Easter straightforwardly besides the Fourier terms, while that could not be done in SARIMA.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-30 07:30:13","Question_id":225995}
{"_id":{"$oid":"5837a589a05283111e4d6eda"},"Last_activity":"2016-07-28 16:30:28","Creator_reputation":146,"Question_score":0,"Answer_content":"http://robjhyndman.com/hyndsight/dailydata/Rob Hyndman recommends using a period of 7 for data that is sampled daily. Also, are you familiar with the forecast{} package, and the auto.arima() function that is included in it? ","Display_name":"Matt Sandgren","Creater_id":121731,"Start_date":"2016-07-28 16:30:28","Question_id":225995}
{"_id":{"$oid":"5837a589a05283111e4d6ee7"},"Last_activity":"2016-07-30 07:15:50","Creator_reputation":15561,"Question_score":0,"Answer_content":"The FMM coefficients for class  are comparable to a weighted full sample regression where the weights are the latent class posterior probabilities of class  membership. The standard errors are more complicated. Intuitively, this weighting makes sense since you don't know for sure which class a particular observation belongs to, so you weight their contribution to the expected value for that group by your estimate of the probability of belonging to that same group.In the textbook fish sexing example, if you were sure that a fish was a dude fish, he should contribute nothing to the lady fish model. Most mixtures won't be that \"separable\" in practice and Cromwell's Rule will be obeyed by FMMs.  Here's an example with Stata using user-written fmm and fmmlc:. set more off        . webuse womenwk, clear        . fmm wage educ age married, mix(normal) comp(2)Fitting Normal regression model:Iteration 0:   log likelihood =  -4181.589  Iteration 1:   log likelihood =  -4181.586  Iteration 2:   log likelihood =  -4181.586  Fitting 2 component Normal model:Iteration 0:   log likelihood = -4181.7341  (not concave)Iteration 1:   log likelihood = -4181.5898  (not concave)Iteration 2:   log likelihood = -4181.4868  (not concave)Iteration 3:   log likelihood = -4180.4487  (not concave)Iteration 4:   log likelihood =  -4178.422  (not concave)Iteration 5:   log likelihood = -4177.1612  (not concave)Iteration 6:   log likelihood = -4176.2248  Iteration 7:   log likelihood = -4175.1488  (not concave)Iteration 8:   log likelihood = -4174.8259  Iteration 9:   log likelihood = -4174.5849  Iteration 10:  log likelihood = -4174.5798  Iteration 11:  log likelihood = -4174.5798  2 component Normal regression                   Number of obs     =      1,343                                                Wald chi2(6)      =     438.48Log likelihood = -4174.5798                     Prob \u0026gt; chi2       =     0.0000------------------------------------------------------------------------------        wage |      Coef.   Std. Err.      z    P\u0026gt;|z|     [95% Conf. Interval]-------------+----------------------------------------------------------------component1   |   education |   .9485605   .0916029    10.36   0.000     .7690222    1.128099         age |    .124996   .0333556     3.75   0.000     .0596202    .1903718     married |  -1.812724   1.407081    -1.29   0.198    -4.570552    .9451045       _cons |   6.278988   1.480055     4.24   0.000     3.378133    9.179843-------------+----------------------------------------------------------------component2   |   education |   .8014959   .1383142     5.79   0.000     .5304052    1.072587         age |   .2012236   .0590787     3.41   0.001     .0854314    .3170158     married |   3.499962   2.727121     1.28   0.199    -1.845097    8.845021       _cons |   5.704543   2.979757     1.91   0.056    -.1356727    11.54476-------------+---------------------------------------------------------------- /imlogitpi1 |    .869283   1.258804     0.69   0.490    -1.597928    3.336494   /lnsigma1 |    1.59161    .052347    30.40   0.000     1.489011    1.694208   /lnsigma2 |   1.626932   .0746651    21.79   0.000     1.480592    1.773273------------------------------------------------------------------------------      sigma1 |   4.911649   .2571102                      4.432711    5.442334      sigma2 |   5.088242   .3799143                      4.395545    5.890103         pi1 |   .7045965   .2620079                      .1682714    .9656598         pi2 |   .2954035   .2620079                      .0343402    .8317286------------------------------------------------------------------------------. qui fmmlc, savec savep     . /* Compare to FMM Coefficients */. reg wage educ age married [pw=_prob1_1](sum of wgt is   9.4627e+02)Linear regression                               Number of obs     =      1,343                                                F(3, 1339)        =     181.49                                                Prob \u0026gt; F          =     0.0000                                                R-squared         =     0.2765                                                Root MSE          =      4.919------------------------------------------------------------------------------             |               Robust        wage |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------   education |   .9485607   .0448757    21.14   0.000     .8605263    1.036595         age |    .124996   .0168575     7.41   0.000      .091926    .1580659     married |  -1.812725   .3192834    -5.68   0.000    -2.439075   -1.186375       _cons |   6.278987   .7822908     8.03   0.000     4.744338    7.813636------------------------------------------------------------------------------. reg wage educ age married [pw=_prob2_1]      (sum of wgt is   3.9673e+02)Linear regression                               Number of obs     =      1,343                                                F(3, 1339)        =     192.88                                                Prob \u0026gt; F          =     0.0000                                                R-squared         =     0.3906                                                Root MSE          =     5.0958------------------------------------------------------------------------------             |               Robust        wage |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------   education |   .8014961     .05736    13.97   0.000     .6889708    .9140213         age |   .2012236   .0253604     7.93   0.000     .1514731    .2509741     married |   3.499962   .3773495     9.28   0.000     2.759701    4.240222       _cons |    5.70454   1.093122     5.22   0.000     3.560122    7.848958------------------------------------------------------------------------------. /* This is what you want to do */. bys _class_1: reg wage educ age married---------------------------------------------------------------------------------------------------------------------------------------------\u0026gt; _class_1 = FMM Component 1      Source |       SS           df       MS      Number of obs   =     1,168-------------+----------------------------------   F(3, 1164)      =    184.82       Model |  11188.2566         3  3729.41888   Prob \u0026gt; F        =    0.0000    Residual |  23487.9617     1,164  20.1786612   R-squared       =    0.3226-------------+----------------------------------   Adj R-squared   =    0.3209       Total |  34676.2183     1,167  29.7139831   Root MSE        =    4.4921------------------------------------------------------------------------------        wage |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------   education |   .9643817   .0454831    21.20   0.000     .8751437     1.05362         age |   .1310328   .0173743     7.54   0.000     .0969444    .1651212     married |  -2.068989   .3082083    -6.71   0.000    -2.673695   -1.464283       _cons |   5.934805   .7905355     7.51   0.000     4.383772    7.485839---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u0026gt; _class_1 = FMM Component 2note: married omitted because of collinearity      Source |       SS           df       MS      Number of obs   =       175-------------+----------------------------------   F(2, 172)       =    131.14       Model |  2282.86052         2  1141.43026   Prob \u0026gt; F        =    0.0000    Residual |  1497.05137       172  8.70378703   R-squared       =    0.6039-------------+----------------------------------   Adj R-squared   =    0.5993       Total |  3779.91189       174  21.7236315   Root MSE        =    2.9502------------------------------------------------------------------------------        wage |      Coef.   Std. Err.      t    P\u0026gt;|t|     [95% Conf. Interval]-------------+----------------------------------------------------------------   education |    1.04502   .0716459    14.59   0.000     .9036013    1.186438         age |   .1466719   .0286646     5.12   0.000     .0900921    .2032516     married |          0  (omitted)       _cons |   12.33115   1.419097     8.69   0.000     9.530066    15.13224--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-29 19:40:03","Question_id":226388}
{"_id":{"$oid":"5837a589a05283111e4d6ef6"},"Last_activity":"2016-07-30 06:26:02","Creator_reputation":13091,"Question_score":4,"Answer_content":"If you want to do time series prediction and you have a sample of a univariate time series at hand, you would start by plotting it and familiarizing yourself with it. Then you would try to find patterns with the hope of extrapolating them into the future1. There could be, for example, seasonality -- in conditional mean or variance (or even higher-order moments);autocorrelation;autoregressive conditional heteroskedasticity.To discover these patterns, you could useseasonal plots (slice the time series into full periods, e.g. years, and plot those on top of each other); function seasonplot in \"forecast\" package in R;(partial) autocorrelation plots (ACF and PACF); functions acf and pacf in R;(partial) autocorrelation plots (ACF and PACF) on squared mean-adjusted data.Once you have identified some patterns, you could then start developing models.1Beware of outliers and nonstationary behaviour, e.g. long-lasting changes in level or variance of the series. When neglected, they could seriously affect your diagnostics of seasonality, autocorrelation and autoregressive conditional heteroskedasticity. Outliers and nonstationarities should be dealt with first.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-30 06:26:02","Question_id":226242}
{"_id":{"$oid":"5837a589a05283111e4d6ef7"},"Last_activity":"2016-07-29 04:31:21","Creator_reputation":21,"Question_score":1,"Answer_content":"I'm not an expert here, but I suggest some simple first steps:1. do some explorative analysis by plotting the data2. use some simple standard models, evaluate them (by plotting or cross-validation) and see which works for you","Display_name":"Tobi","Creater_id":124963,"Start_date":"2016-07-29 04:31:21","Question_id":226242}
{"_id":{"$oid":"5837a589a05283111e4d6f03"},"Last_activity":"2016-07-30 06:17:25","Creator_reputation":57772,"Question_score":1,"Answer_content":"I would argue that it makes no sense to say there is a \"linear relationship\" between a dichotomous response variable and an independent variable.  That's one reason we use logistic regression (or other transformations of the response). Also, perhaps it's just a sort of typo, but variables can't be linear.  Did you mean \"non-continuous\"?Finally, if you need a mixed model (to deal with dependent data) then any model which does not account for the dependency may be very misleading. ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-07-30 06:17:25","Question_id":226440}
{"_id":{"$oid":"5837a589a05283111e4d6f12"},"Last_activity":"2016-07-30 05:40:35","Creator_reputation":402,"Question_score":2,"Answer_content":"The equation given by Wikipedia connects cumulants to moments (generally).A proof of a formula connecting cumulants to central moments is found in A Recursive Formulation of the Old Problem of Obtaining Moments from Cumulants and Vice VersaLetting  be the cumulant-generating function, and  the moment-generating function. The relationship between the two is\\begin{equation}M(t)=\\exp{\\left[K(t)\\right]}\\end{equation}The proof follows by differentiation of this expression and noting that the th derivative can be written as\\begin{equation}D^n[M(t)]=\\sum_{i=0}^{n-1}\\binom{n-1}{i}D^{n-i}[K(t)]D^i[M(t)]\\end{equation}Where  denotes the th derivative. Now setting :\\begin{equation}\\theta_n=\\sum_{i=0}^{n-1}\\binom{n-1}{i}\\kappa_{n-i}\\theta_i\\\\\\theta_n=\\kappa_n+\\sum_{i=1}^{n-1}\\binom{n-1}{i}\\kappa_{n-i}\\theta_i\\\\\\end{equation}Rewriting yields:\\begin{equation}\\kappa_n = \\theta_n-\\sum_{i=1}^{n-1}\\binom{n-1}{i}\\kappa_{n-i}\\theta_i\\end{equation}In terms of the central moments and cumulants.","Display_name":"ltronneberg","Creater_id":36901,"Start_date":"2016-07-30 05:40:35","Question_id":226141}
{"_id":{"$oid":"5837a589a05283111e4d6f1f"},"Last_activity":"2016-07-30 05:27:10","Creator_reputation":13091,"Question_score":0,"Answer_content":"General remarksIf you want to assess all funds together, i.e. see if the seasonality is prevalent for all funds as a group, then follow Dimitriy's advice. If you would also like to inspect some funds separately, you could still use the regression you have. Just recall that by testing  funds one by one, you would end up rejecting the null hypothesis of no seasonality  times even when none of the funds were seasonal. Refer to the literature on multiple testing corrections then.Testing for autocorrelationDurbin-Watson test targets only first-order autocorrelation. For monthly returns on funds this could probably be sufficient; you could consult financial theory on whether higher-order autocorrelations could be expected. If checking higher-order autocorrelations were also of interest, you could use Breusch-Godfrey or Ljung-Box tests.Conditional heteroskedasticityYou could also see if the model residuals have autoregressive conditional heteroskedasticity by using ARCH-LM test (ibid.). By neglecting heteroskedasticity when present, you could lose some power of your tests.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-30 05:27:10","Question_id":226025}
{"_id":{"$oid":"5837a589a05283111e4d6f20"},"Last_activity":"2016-07-29 18:28:15","Creator_reputation":15561,"Question_score":1,"Answer_content":"I would start by performing one of the tests for unit roots (or stationarity) for panel data panel datasets.If the returns are stationary, I would fit a fixed effects linear regression model with a dummy for January. I would use heteroskedasticity-robust errors or perhaps cluster them by type of fund.Another approach would be to fit a fixed effects linear panel data model with an AR(1) disturbance or with panel-corrected standard errors.","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-29 18:28:15","Question_id":226025}
{"_id":{"$oid":"5837a589a05283111e4d6f2d"},"Last_activity":"2016-07-30 05:15:54","Creator_reputation":2865,"Question_score":0,"Answer_content":"fcn32 uses a stride of 32 because after pool5 the spatial resolution is 2^5=32 times smaller, similarly pool4 should use a stride of 2^4=16 and pool3 should use a stride of 2^3=8.That tensorflow model first uses stride 2 to upsample pool5 to the same size as pool4, then uses another stride 2 to upsample these two to be the same size as pool3, and finally upsamples these three all together with stride 8. So pool5 gets enlarged 2*2*8=32 times, pool4 gets enlarged 2*8=16 times and pool3 8 times, which is correct.The reason of doing it this way instead of using strides of 8 16 and 32 separately for each layer is to save the amount of computations. As in the end we are summing them together it's more efficient to sum before upsampling than after.The kernel size seems more of a design choice, though it is not mentioned in the paper directly, the paper provides a link to their Caffe implementation, which uses a kernel size of 4 for stride 2 layers.","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-07-30 03:09:31","Question_id":226047}
{"_id":{"$oid":"5837a589a05283111e4d6f3a"},"Last_activity":"2016-07-30 05:11:00","Creator_reputation":13091,"Question_score":2,"Answer_content":"I will only answer your first question. I will show that the \"finite sample adjustment\" is not really an adjustment and that the Ljung-Box statistic is only natural (more so than the Box-Pierce statistic). (For the second and third questions, you could consult Anderson (1942), which is unfortunately quite technical. Probably another user will offer a more intuitive answer.)Take an ARMA(,) model  \\phi(B) w_t = \\theta(B) a_t where  is the backshift (or lag) operator. Define the -th order autocorrelation of model errors (not residuals) as r_k := \\frac{ \\sum_{t=k+1}^n a_t a_{t-k} }{ \\sum_{t=1}^n a_t^2 } and collect the first  autocorrelations in one vector . Box \u0026amp; Pierce (1970) claim on p. 1510 that for large ,  has a multivariate normal distribution,  and  are uncorrelated for  and the variance of  is \\text{Var}(r_k) = \\frac{n-k}{n(n+2)}. Then it follows that the sum  \\sum_{k=1}^m \\frac{n(n+2)}{n-k} \\text{Var}(r_k) = n(n+2) \\sum_{k=1}^m \\frac{1}{n-k} \\text{Var}(r_k) is distributed as  for large  (because you get  distribution by summing up  squares of independent standard normal random variables). Up to this point we have an expression of the Ljung-Box (rather than Box-Pierce) test statistic. So apparently there is no \"finite sample correction\". What happens next is that Box \u0026amp; Pierce (1970) note that  \\text{Var}(r_k) \\approx \\frac{1}{n}  since  for large , and then also n \\sum_{k=1}^m \\text{Var}(r_k) \\sim \\chi_m^2. Here is where the Box-Pierce statistic (different from the exact statistic above) is introduced.This concerns the case where model errors are known, which is not what we encounter in practice. Therefore, Box \u0026amp; Pierce (1970) go on to examine the case with estimated residuals in place of the true model errors. After some elaboration on the pure autoregressive AR() case, they note on p. 1517 that when errors are unknown and are replaced by residuals, for large  it is sufficient to replace  with  ( since ) in the asymptotic distribution and the result will still hold: n \\sum_{k=1}^m \\text{Var}(\\hat r_k) \\sim \\chi_{m-p}^2 where  is the sample counterpart of .Further they show that the case of ARMA(,) in place of pure AR() does not change the essence, and so for a general ARMA(,) model one still has that n \\sum_{k=1}^m \\text{Var}(\\hat r_k) \\sim \\chi_{m-p-q}^2. In these last few expressions, the approximation  is used. It does not hurt in large samples, but apparently it causes trouble in small samples, which Ljung \u0026amp; Box (1987) note (citing a few studies). Therefore, they suggest dropping the approximation and going back to the original statistic.References:Anderson, Ronald L. \"Distribution of the serial correlation coefficient.\" The Annals of Mathematical Statistics 13.1 (1942): 1-13.Box, George EP, and David A. Pierce. \"Distribution of residual autocorrelations in autoregressive-integrated moving average time series models.\" Journal of the American statistical Association 65.332 (1970): 1509-1526.Ljung, Greta M., and George EP Box. \"On a measure of lack of fit in time series models.\" Biometrika 65.2 (1978): 297-303.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-30 04:46:01","Question_id":226334}
{"_id":{"$oid":"5837a589a05283111e4d6f47"},"Last_activity":"2016-07-30 05:10:53","Creator_reputation":39366,"Question_score":3,"Answer_content":"Forcing any continuous assessment to be classified using a cutoff/threshold is ultimately non-reproducible, arbitrary, and has very low precision.  Plus if you use the data to select the cutoff you will need to bootstrap the whole process to get a reasonable measure of variability/stability.Natura non facit saltus (Nature does not make jumps)-- Gottfried Wilhelm Leibniz","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2016-07-30 05:10:53","Question_id":226420}
{"_id":{"$oid":"5837a589a05283111e4d6f54"},"Last_activity":"2016-07-30 05:04:44","Creator_reputation":39366,"Question_score":4,"Answer_content":"The Harrell-Davis estimator was developed only for the case where there are no repeated measurements.  It may actually work with pooled multiple record per case data but the standard error it gives will be way off (it might possibly be corrected using the cluster bootstrap).  But I would seek a more general approach.The minimum sample size needed to estimate a reference range reliably, using any method, is probably in the hundreds.  And reference ranges are usually inconsistent with clinical decision making because they ignore the distribution of non-normal subjects.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2016-07-30 05:04:44","Question_id":226311}
{"_id":{"$oid":"5837a589a05283111e4d6f63"},"Last_activity":"2016-07-28 03:54:25","Creator_reputation":5445,"Question_score":2,"Answer_content":"The problem is likely due to the model matrix for either the count or zero inflated part (though it looks like they are the same from your code) being close to singular. One way forward is to inspect the model matrices and see if you can see if there is linear dependence between any of the rows or columns, or something else causing singularity. That could prove tricky to do, so before you try that, try simplifying the model. Does month really need to be a factor ? It doesn't seem so to me. In the count part of the model, the estimates for the main effects and the interactions with depth are all of a similar magnitude so month could be numeric.  With the zero inflated part, the estimates main effects of month indicate some nonlinearity, but the those for the interaction are similar, so I would again use month as numeric, and include a quadratic term (perhaps after centering to avoid collinearity between the linear and quadratic terms).","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-28 03:54:25","Question_id":226066}
{"_id":{"$oid":"5837a589a05283111e4d6f70"},"Last_activity":"2016-07-30 01:46:03","Creator_reputation":5445,"Question_score":3,"Answer_content":"Assuming that your outcome really is binary:You don't have enough areas to model it as a random effect so the 3rd model is not viable.However you can run model 1 and 2 on the same dataset with the addition of Area as a fixed effect.This will have the advantage of giving more statistical power than individual models for each area.glmer(Sites ~ group1 + group 2 + group3 + group4 +  Area + (1|Transect), family = binomial(link = \"logit\"), data = df_both_areas)The logit link has the benefit of providing easily interpretable results, so I would keep that.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-30 01:18:50","Question_id":226359}
{"_id":{"$oid":"5837a589a05283111e4d6f7d"},"Last_activity":"2016-07-30 00:31:28","Creator_reputation":3038,"Question_score":5,"Answer_content":"It might be good to read What follows if we fail to reject the null hypothesis? before the explanation below. Desirable properties: powerIn hypothesis testing, the goal is to find 'statistical evidence' for .  Thereby we can make type I errors, i.e. we reject  (and decide that there is evidence in favour of ) while  was true (i.e.  is false). So a type I error is 'finding false evidence' for .A type II error is made when  can not be rejected while it is false in reality, i.e. we ''accept '' and we 'miss' the evidence for .The probability of a type I error is denoted by , the choosen significance level.  The probability of a type II error is denoted as  and  is called the power of the test, it is the probability to find evidence in favour of  when  is true. In statitistical hypothesis testing the scientist fixes an upper threshold for the probability of a type I error and under that constraint tries to find a test with maximum power, given . The desirable properties of likelihood ratio tests have to do with power In a hypothesis test  versus  the null hypothesis and the alternative hypothesis are called ''simple'', i.e. the parameter is fixed to one value, just as well under  as under  (more precisely; the distributions are fully determined).  The Neyman-Pearson Lemma states that, for hypothesis tests with simple hypothesises, and for given type I error probability, a likelihood ratio test has the highest power.  Obviously, high power given  is a desirable property: power is a measure of 'how easy it is to find evidence for '. When the hypothesis is composite; like e.g.  versus  then the Neyman-Pearson lemma can not be applied because there are 'multiple values in '.  If one can find a test such that it is most powerfull for every value 'under ' then that test is said to be 'uniformly most powerfull' (UMP) (i.e. most powerfull for every value under ). There is a theorem by Karlin and Rubin that gives the necessary conditions for a likelihood ratio test to be uniformly most powerfull.  These conditions are fullfilled for many one-sided (univariate) tests.  So the desirable property of the likelihood ratio test lies in the fact that in several cases it has the highest power (although not in all cases). In most cases the existence of an UMP test can not be shown and in many cases (especially the multivariate) it can be shown that an UMP test does not exist. Nevertheless, in some of these cases likelihood ratio tests are applied because of their desirable properties (in the above context), because they are relatively easy to apply, and sometimes because no other tests can be defined. As an example, the one-sided test based on the standard normal distribution is UMP. Intuition behind the likelihood ratio test:If I want to test  versus  then we need an observation  derived from a sample.  Note that this is one single value.  We know that either  is true or  is true, so one can compute the probability of  when  is true (lets call it ) and also the probability of observing  when  is true (call it ). If  then we are inclined to believe that ''probably  is true''.  So if the ration  we have reasons to believe that  is more realistic than .  If  would be something like  then we might conclude that it could be due to chance, so to decide we need a test and thus the distribution of  which is ... a ratio of two likelihoods. I found this pdf on the internet.","Display_name":"fcop","Creater_id":83346,"Start_date":"2015-08-31 23:07:19","Question_id":169524}
{"_id":{"$oid":"5837a589a05283111e4d6f8a"},"Last_activity":"2016-07-30 00:30:40","Creator_reputation":3038,"Question_score":5,"Answer_content":"I assume that you understand that the margin is given by the equation  where the width of the margin is equal to  and one has to maximise the width .    Notice that the equation represents a hyperplane with normal vector .  Furthermore, if  is a normal vector, then  is also a normal vector (where  is a scalar).  So we can just as well write  or .  So by ''re-scaling\" the vector  with a factor  we can reduce the equation to .If you want to compute the distance of a point   to a hyperplane (see my answer to Getting distance of points from decision boundary with linear SVM?) than you have to compute .  (note that ). If I take  a point on the margin, then is must fullfill the equation of the margin, so  so for a point on the margin the distance is equal to EDIT: you added the picture and asked an additional question:For equation  we see that when  then  and for  we have , so the  axis is vertical and  horizontal (you see that when you look for these points on your graph).  The point  is on the red line (note that your  is vertical) ,  is the vector  and your equation is , so you have to compute  and divide this by the norm of  which is , so half the margin is EDIT: Added after the question in your commentsThe reason for 'eliminating' the  is technical: because of the fact that the normal vector is only known op to a constant, the problem has no unique solution. So either you have to fix the norm of w  or you have to fix the k. You could see both cases as choosing a different unit for measuring distances. ","Display_name":"fcop","Creater_id":83346,"Start_date":"2015-08-24 06:02:25","Question_id":168531}
{"_id":{"$oid":"5837a589a05283111e4d6f97"},"Last_activity":"2016-07-30 00:18:00","Creator_reputation":66,"Question_score":0,"Answer_content":"A standard approach would be to use a Kruskal-Wallis test to test for differences among the sectors, and then Dunns Test to see which pairs of sectors are different. This previous answer is relevant: Post-hoc tests after Kruskal-Wallis: Dunn\u0026#39;s test or Bonferroni corrected Mann-Whitney tests?","Display_name":"Groovy_Worm","Creater_id":124314,"Start_date":"2016-07-30 00:18:00","Question_id":226382}
{"_id":{"$oid":"5837a589a05283111e4d6fa4"},"Last_activity":"2016-07-29 23:26:00","Creator_reputation":3832,"Question_score":1,"Answer_content":"I'd recommend starting by looking at artificial neural nets where the units already take binary values. For example, check out Hopfield nets and the McCulloch–Pitts model (the original ANN). The Ising model may also be of interest. Although it was originally a model of ferromagnetism, it has connections to neural networks, and is of interest in neuroscience and physics.Although the units in these models take binary values, the connection strengths are continuous, so implementing them with digital logic would require something like replacing each unit with a local network of logic gates. This will only be an approximation because real numbers can't be truly represented using a finite sequence of discrete operations--almost all of them are uncomputable. Of course, this also applies to digital computers, which we use to approximate computational models (e.g. ANNs) involving real numbers.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-07-29 23:26:00","Question_id":226406}
{"_id":{"$oid":"5837a58aa05283111e4d6fb1"},"Last_activity":"2016-07-29 23:22:08","Creator_reputation":31,"Question_score":3,"Answer_content":"The Malley (2012) is available here: http://dx.doi.org/10.3414%2FME00-01-0052. A full reference is in the references part in the ranger documentation. In short, each tree predicts class probabilities and these probabilities are averaged for the forest prediction. For two classes, this is equivalent to a regression forest on a 0-1 coded response.In contrast, in randomForest with type=\"prob\" each tree predicts a class and probabilities are calculated from these classes. In the example here I tried to use the uniform distribution instead of the normal distribution to generate the probabilities, and here the other approach seems to perform better. I wonder if these probabilities are really the truth?By the way, the same results as in the randomForest example above can be achieved with ranger by using classification and manual probability computation (use predict.all=TRUE in prediction). ","Display_name":"mnwright","Creater_id":125052,"Start_date":"2016-07-29 23:22:08","Question_id":226109}
{"_id":{"$oid":"5837a58aa05283111e4d6fb2"},"Last_activity":"2016-07-28 13:58:03","Creator_reputation":2542,"Question_score":2,"Answer_content":"It's the proportion of votes of the trees in the ensemble.library(randomForest)rf = randomForest(Species~., data = iris, norm.votes = TRUE, proximity = TRUE)p1 = predict(rf, iris, type = \"prob\")p2 = predict(rf, iris, type = \"vote\", norm.votes = TRUE)identical(p1,p2)identical(p1,p2)#[1] TRUEAlternatively, if you multiply your probabilities by ntree, you get the same result, but now in counts instead of proportions.p1 = predict(rf, iris, type = \"prob\")p2 = predict(rf, iris, type = \"vote\", norm.votes = FALSE)identical(500*p1,p2)#[1] TRUE","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-07-28 13:58:03","Question_id":226109}
{"_id":{"$oid":"5837a58aa05283111e4d6fbe"},"Last_activity":"2016-07-29 22:53:16","Creator_reputation":3832,"Question_score":1,"Answer_content":"The mean of the distribution  is the point of aim. Say we know the covariance matrix . The hit location  has a bivariate normal distribution , although the following approach should generalize to other distributions and numbers of dimensions. The probability of hitting a particular target is the probability that the hit location falls within the bounds of the target. This is given by the integral of  over the target region. Say there are  targets and the region covered by the th target is . The probability of hitting the th target is:p(\\text{Hit}_i \\mid \\mu, C) = \\underset{x, y \\in R_i}{\\int \\int} p(x \\mid \\mu, C) dx dyAssuming the targets don't overlap, the probability of hitting any target is then the sum of the probabilities of hitting each individual target:p(\\text{Hit}_{any} \\mid \\mu, C) = \\sum_{i=1}^n p(\\text{Hit}_i \\mid \\mu, C)If the targets do overlap, you'll have to deal with it using the standard rules of probability, to avoid multiply-counting hits at locations that are part of multiple targets. One way to do this is to define a single ur-target, consisting of the union of all individual targets:R_{all} = \\bigcup_{i=1}^n R_iThe goal is to choose the point of aim  that maximizes the probability of hitting any target. Since optimization solvers are generally built to minimize functions, we can describe this goal equivalently as minimizing the negative probability:\\mu^* = \\underset{\\mu}{\\text{argmin }} -p(\\text{Hit}_{any} \\mid \\mu, C)In terms of programmatic approach, the first thing you need is a way to compute . For some distributions and target shapes, you may be able to derive a closed form expression, which will make everything much more efficient. But, if you can't, you can use numerical integration (e.g. the functions in the scipy.integrate module). It will be straightforward to define the integration bounds for circular targets. But, if targets were weirdly shaped, you could use rejection sampling and Monte Carlo integration.The second thing you need is a way to solve the optimization problem (e.g. check out the functions in the scipy.optimize module). This is where a closed form expression for  would help, since you'll have to repeatedly evaluate the objective function for different aim points. If you can derive an expression for the gradient, your computation will be even more efficient. Things will still work otherwise; the computation will just take longer.The objective function will probably have multiple local optima. For example, if there are two widely separated clusters of targets, there will be an aim point over each cluster that's better than the surrounding aim points, but one of these aim points may be better than the other. Convex optimization solvers only take local downhill steps. So, if you start near the worse of these two solutions, there's no way to get to the better solution. Because of this, you'll need to perform some form of global optimization. There are dedicated algorithms for this, but a quick and dirty solution would be to use a local optimization method with multiple starting points. For example, you could start from many random places, or try to pick starting points in a principled way (e.g. over the targets).","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-07-29 22:53:16","Question_id":226400}
{"_id":{"$oid":"5837a58aa05283111e4d6fcb"},"Last_activity":"2016-07-29 21:58:39","Creator_reputation":15561,"Question_score":0,"Answer_content":"Issues of statistical significance aside, that would be the interpretation as long as you had the level effects as well (say if your model also included a constant as well as experiment and gender coefficients).","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-29 21:58:39","Question_id":226366}
{"_id":{"$oid":"5837a58aa05283111e4d6fd8"},"Last_activity":"2016-07-27 20:56:13","Creator_reputation":51,"Question_score":0,"Answer_content":"I'm not familiar with plm, but I would assume that the between option runs your model on the group means such that you are only explaining between group variance in your outcome. That may be appropriate if that is the only variance of interest, but many people are interested in explaining both within and between outcome variance, and therefore use random effects models. You can always group mean center your within variables and add the group means of the predictors to your random effects model to get the best of both worlds!","Display_name":"Erik Ruzek","Creater_id":87305,"Start_date":"2016-07-27 20:56:13","Question_id":226006}
{"_id":{"$oid":"5837a58aa05283111e4d6feb"},"Last_activity":"2016-07-29 16:15:08","Creator_reputation":12922,"Question_score":2,"Answer_content":"  Intuitively it seems that low variance features are not useful and are just noise to the model.This is folklore that is false in an essential way.  There are two intuitive reasons to doubt it:The variance of a feature is not unitless, by re-expressing, say, a length in meters, millimeters, or feet, you change the variance.  Any well founded model should not care.The variance of a feature ignores the relationship between the feature and the response, which is the focus of supervised learning models.  While the predictor may have small variation, the relationship between the predictor and response may be very powerful within that range.With these points in mind, it's very easy to construct examples where a small variance feature dominates a large variance feature\u0026gt; set.seed(154)\u0026gt; x_1 \u0026lt;- rnorm(100, mean = 0, sd = .01)   # Low varaince\u0026gt; x_2 \u0026lt;- rnorm(100, mean = 0, sd =  100)  # High varaince\u0026gt; \u0026gt; y \u0026lt;- 100*x_1 + rnorm(100) \u0026gt; \u0026gt; lm(y ~ x_1 + x_2)Call:lm(formula = y ~ x_1 + x_2)Coefficients:(Intercept)          x_1          x_2   -0.2394094   95.5194309    0.0007392 Any sensible measurement of feature \"relevance\" must take into account the relationship between a predictor and the thing being predicted.  The internal structure of the predictors themselves can tell you only very little.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-07-29 16:03:36","Question_id":226381}
{"_id":{"$oid":"5837a58aa05283111e4d6ff8"},"Last_activity":"2016-07-29 15:04:56","Creator_reputation":338,"Question_score":0,"Answer_content":"The \"input space\" is just all the possible inputs. In this example, he is assuming that each dimension is binary so that means there are  possible inputs. A trillion examples would cover only  (i.e. ) of that input space.As for why its important to cover a large part of the input space, the short answer is that you need to see the behavior of what you are trying to learn in enough of the input space to build a good learner.As a toy example, if you have two points:  and  the best you can do to fit them is the line . But you can't be confident in that because you have no idea how the function acts between  and  (much less outside this space). This example might seem silly and obvious, but what is talking about is essentially a generalization of this. But the problem is even worse than that because the amount of data you need grows exponentially with the number of dimensions. This is the curse of dimensionality.","Display_name":"roundsquare","Creater_id":122754,"Start_date":"2016-07-29 14:57:31","Question_id":226369}
{"_id":{"$oid":"5837a58aa05283111e4d7005"},"Last_activity":"2016-07-29 15:19:34","Creator_reputation":17464,"Question_score":2,"Answer_content":"The only reason you would lose precision after adjusting for other factors in a randomized study is if 1) the other factors are correlated with randomization assignment. This means you did a bad job of randomizing, and further implies that the adjusted results are correct and the unadjusted results are confounded.2) The other factors are independent of both the outcome and the randomization assignment. This means that the tiny loss of degrees of freedom from adjusting for all of 2 parameters has led from borderline statistical significance (on one side of the 0.05 threshold) to borderline statistical significance (on the other side of the 0.05 threshold). If that happens, you should expand your mind and, instead of reported a p-value or statistical significance--a universally useless measure--report a 95% confidence interval.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-07-29 15:19:34","Question_id":226352}
{"_id":{"$oid":"5837a58aa05283111e4d7006"},"Last_activity":"2016-07-29 15:13:41","Creator_reputation":169,"Question_score":3,"Answer_content":"My sense is that your variables X1 and X2 are multicollinear with Z1 and Z2, respectively. Some or most of the information contained in X1 and X2 is contained in either of Z1 or Z2. Z1 cannot be zero if X1 indicates treatment. From the standpoint of a scientific approach, if your prospective hypothesis was framed in the setting of adjustment for either of Z1 or Z2, then I would suggest those variables not be removed after results of testing are obtained to obtain statistical significance. However, including those variables may have been ill-advised at the outset given what appears to be the intrinsic correlation between the independent variables.This is not so much a statistics question as a research design question. The statistics are merely shedding light on what seems to be a design flaw.Edit: As for Z1 and Z2 \"not accounting for much variance,\" they will not appear to account for much variance if most of the effect on the dependent variables is obtained from knowledge of either of the binary treatment variables.  The effect this has is increase in the standard deviation of the correlated variables's standard error. A formal analysis of this can be done with estimation of variance inflation factors for the beta-coefficients you obtain from the regression model.","Display_name":"Todd","Creater_id":64263,"Start_date":"2016-07-29 14:48:34","Question_id":226352}
{"_id":{"$oid":"5837a58aa05283111e4d7013"},"Last_activity":"2016-07-29 13:56:40","Creator_reputation":12343,"Question_score":1,"Answer_content":"Summarizing some of the above commentary:a lot of this discussion is summarized in the ?lme4::convergence help page, including source(system.file(\"utils\", \"allFit.R\", package=\"lme4\"))fm1.all \u0026lt;- allFit(fm1)which is doing the same thing as the Rpubs page referenced in the comments above.the difference between glmmADMB and lme4 is not really that one is converging and the other isn't, but that lme4 has more checks on the convergence -- and as hinted at in ?convergence, these checks give an unfortunately large number of false positives.I'm a little bit surprised that you're getting gradient-convergence warnings with max|grad| of 0.00103, since the gradient tolerance was changed to 0.002 in 2014; are you sure you're using the latest version?the reason for setting absolute rather than relative tolerances is that everything is scaled; the response is on the deviance scale (where absolute, not relative differences matter), and the predictors are scaled by their second derivatives (although that doesn't always work so well ...)glmer does indeed use bobyqa for the first (nAGQ=0) pass and Nelder-Mead for the second (nAGQ=1) pass ... I would indeed recommend using bobyqa throughout (control=glmerControl(method=\"bobyqa\")), the next release will probably make that switch. AD Model Builder (the underlying engine for glmmADMB uses a quasi-Newton method (Fournier et al 2012, DOI:10.1080/10556788.2011.597854 ), but very little documentation beyond the source code is available.if you want to do further cross-checking, the (experimental) glmmTMB package offers another alternative.the r-sig-mixed-models@r-project.org mailing list is an alternative venue for these questionsI think I'm a little hurt by @MarkL.Stone's \"Unfortunately, this doesn't sound like a top-notch piece of software\" ... have you ever tried to write and maintain a general-purpose GLMM-fitting package ... ?","Display_name":"Ben Bolker","Creater_id":2126,"Start_date":"2016-07-29 13:56:40","Question_id":226171}
{"_id":{"$oid":"5837a58aa05283111e4d7023"},"Last_activity":"2016-07-29 04:30:39","Creator_reputation":3832,"Question_score":3,"Answer_content":"The most important thing is that the inputs should carry information about the variable you want to predict. Use as much prior knowledge as you can to choose meaningful inputs, including consulting with domain experts who are familiar with your type of data. It may be the case that some inputs jointly carry relevant information, but alone they don't. It's not always possible to know ahead of time which features are meaningful, and many algorithms can tolerate some amount of 'noisy' inputs that don't carry relevant information. But, performance can break down as you add more of these. 'Feature selection' is a broad class of methods for identifying relevant features, but may involve either heavy assumptions or heavy computation. Some algorithms (including some neural nets) can identify relevant features as part of training.For neural nets (and other methods), the way you preprocess your data is important. Common preprocessing steps include centering the data and normalizing it (e.g. dividing by the standard deviation or range). One reason is to avoid saturating units in the network, which can slow or halt training. This can happen if inputs are too large. Another reason is to avoid creating long, narrow valleys in the loss function, which can happen if input dimensions have different scales, or if they're strongly correlated. Neural nets are often trained using gradient-based procedures like stochastic/minibatch gradient descent. On each step, they adjust the weights in a direction that reduces the loss function. If the loss function has a nice, symmetric bowl shape, the direction of steepest descent points toward the minimum, and stepping in this direction will give fast convergence. If the loss function has elongated, sloping canyons, the direction of steepest descent points toward the canyon floor, but not toward the actual minimum, so the network will zig-zag through weight space and training will be slower. Data can be preprocessed using PCA, to give uncorrelated inputs.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-07-29 04:30:39","Question_id":226218}
{"_id":{"$oid":"5837a58aa05283111e4d7030"},"Last_activity":"2016-07-29 12:54:00","Creator_reputation":346,"Question_score":3,"Answer_content":"In Econometrics a linear panel data model with the name fixed effects usually refers to estimating the within transformed model by OLS. What are the requirements for this model? Well, let us see. ModelWe assume a population model of the form:y_t = x_t \\beta +c+u_t,where  is ,  is ,  is ,   is  and  is . The within transformation subtracts the individual average from all periods:y_t-\\bar y=(x_t-\\bar x)\\beta +(u_t-\\bar u)\\\\\\tilde y_t = \\tilde x_t \\beta + \\tilde u_t,where  indicates within transformed variables and  disappears because it is time constant.Identification and consistencyIn order to achieve identification and estimate our parameters consistently we need two conditions fulfilled. The first is akin to the population orthogonality assumed in our pooled OLS. Basically we often assume:E(u_{it}|x_1,x_2,\\ldots,x_T,c_i)=0, ~ \\forall t \\in\\{ 1,...T\\}.This assumption is often called strict exogeneity. This implies . The second assumption is a rank condition saying:rank(E(\\tilde x_t ' \\tilde x_t^{\\,}))=KIn most cases this just tells us to remember to remove time constant observables. EfficiencyUnder the following condition, the estimator is also efficient:E(uu'|x,c)=\\sigma_u^2 I_TThis basically rules out serial correlation in the (non-transformed) errors. This is usually not the case in empirical work, so that is why a robust variance-covariance matrix is often computed:\\hat{Avar}(\\hat\\beta)=(\\tilde X' \\tilde X)^{-1}\\left(\\sum^N_{i=1}\\tilde x_i'\\hat{\\tilde u}_i\\hat{\\tilde u}'_i x_i\\right)(\\tilde X' \\tilde X)^{-1},where big X is 's stacked. Notice, we do not need normality of the 's or the 's.","Display_name":"pkofod","Creater_id":34944,"Start_date":"2014-01-23 11:43:16","Question_id":77332}
{"_id":{"$oid":"5837a58aa05283111e4d7031"},"Last_activity":"2013-11-22 09:54:01","Creator_reputation":2770,"Question_score":2,"Answer_content":"Those deviations from the normal look like they are largely driven by a few outliers.  See what happens when you trim those, and look closely at them to see whether they belong in your model in the first place.  They may have high leverage and thereby drive your estimates.  If you know why they are what they are, and have data on the mechanism, you should control for it, and possible interact it with your regressor of interest.Also, on a semi-related note, inference in standard OLS depends on an assumption that observations are independent and identically distributed.  The first is almost always violated in panel data, and the second is often violated in general, if you assume that heteroskedasticity is the norm, and that homoskedasticity is a special case.  To get around this, applied econometricians \"cluster their standard errors\", which corrects for both problems.  Google clustered standard error if you haven't heard of this before; you'll find lecture notes from universities on the subject.  It looks like you're using stata, on which this procedure is trivial to implement: xtreg y x, cluster(clustvar), where the cluster variable is usually the cross-sectional unit, but may be a higher-level unit into which cross-sectional units are aggregated.","Display_name":"generic_user","Creater_id":17359,"Start_date":"2013-11-22 06:08:48","Question_id":77332}
{"_id":{"$oid":"5837a58aa05283111e4d703e"},"Last_activity":"2016-07-29 12:40:44","Creator_reputation":12280,"Question_score":1,"Answer_content":"I'd add that if you directly use a State Space function, you're probably going to have to understand the several matrices that make up a model, and how they interact and work. It's much more like defining a program than defining an ARIMA model. If you're working with a dynamic State Space model, it gets even more complicated.If you use a software package that has a really, really nice State Space function, you may be able to avoid some of this, but the vast majority of such functions in R packages require you to jump into the details at some point.In my opinion, it's a lot like Bayesian statistics in general, the machinery of which takes more understanding, care, and feeding to use than more frequentist functions.In both cases, it's well worth the additional details/knowledge, but it could be a barrier to adoption.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-07-29 12:40:44","Question_id":78287}
{"_id":{"$oid":"5837a58aa05283111e4d703f"},"Last_activity":"2015-05-18 18:23:18","Creator_reputation":21,"Question_score":1,"Answer_content":"You can refer to the excellent book Bayesian forecasting and dynamic models (Harrison and West, 1997). The authors show that almost all traditional time series models are particular cases of the general dynamic model. They also emphasize the advantages. Perhaps one of the major advantages is the easiness with which you can integrate many state space models by simply augmenting the state vector. You can, for example, seamlessly integrate regressors, seasonal factors, and an autoregressive component in a single model.","Display_name":"Anselmo","Creater_id":71823,"Start_date":"2015-05-18 18:02:03","Question_id":78287}
{"_id":{"$oid":"5837a58aa05283111e4d7040"},"Last_activity":"2014-02-13 13:56:38","Creator_reputation":314,"Question_score":1,"Answer_content":"The Kalman Filter is the optimal linear quadratic estimator when the state dynamics and measurement errors follow the so-called linear Gaussian assumptions (http://wp.me/p491t5-PS). So, as long as you know your dynamics and measurement models and they follow the linear Gaussian assumptions, there is no better estimator in the class of linear quadratic estimators. However, the most common reasoners for \"failed\" Kalman Filter applications are:Imprecise/incorrect knowledge of the state dynamics and measurement models.Inaccurate initialization of the filter (providing an initial state estimate and covariance that is inconsistent with the true system state). This is easily overcome using a Weighted Least Squares (WLS) initialization procedure.Incorporating measurements that are statistical \"outliers\" with respect to the system dynamics model. This can cause the Kalman Gain to have negative elements, which can lead to a non positive semi-definite covariance matrix after update. This can be avoided using \"gating\" algorithms, such as ellipsoidal gating, to validate the measurement prior to updating the Kalman Filter with that measurement.These are some of the most common mistakes/issues I've seen working with the Kalman Filter. Otherwise, if the assumptions of your models are valid, the Kalman Filter is an optimal estimator.","Display_name":"concipiotech","Creater_id":39520,"Start_date":"2014-02-13 13:56:38","Question_id":78287}
{"_id":{"$oid":"5837a58aa05283111e4d7041"},"Last_activity":"2013-12-12 01:47:42","Creator_reputation":887,"Question_score":11,"Answer_content":"Here is some preliminary list of disadvantages I was able to extract from your comments. Criticism and additions are very welcome!Overall - compared to ARIMA, state-space models allow you to model more complex processes, have interpretable structure and easily handle data irregularities; but for this you pay with increased complexity of a model, harder calibration, less community knowledge.ARIMA is a universal approximator - you don't care what is the true model behind your data and you use universal ARIMA diagnostic and fitting tools to approximate this model. It is like a polynomial curve fitting - you don't care what is the true function, you always can approximate it with a polynomial of some degree. State-space models naturally require you to write-down some reasonable model for your process (which is good - you use your prior knowledge of your process to improve estimates). Of course, if you don't have any idea of your process, you always can use some universal state-space model also - e.g. represent ARIMA in a state-space form. But then ARIMA in its original form has more parsimonious formulation - without introducing unnecessary hidden states.Because there is such a great variety of state-space models formulations (much richer than class of ARIMA models), behavior of all these potential models is not well studied and if the model you formulated is complicated - it's hard to say how it will behave under different circumstances. Of course, if your state-space model is simple or composed of interpretable components, there is no such problem. But ARIMA is always the same well studied ARIMA so it should be easier to anticipate its behavior even if you use it to approximate some complex process.Because state-space allows you directly and exactly model complex/nonlinear models, then for these complex/nonlinear models you may have problems with stability of filtering/prediction (EKF/UKF divergence, particle filter degradation). You may also have problems with calibrating complicated-model's parameters - it's a computationally-hard optimization problem. ARIMA is simple, has less parameters (1 noise source instead of 2 noise sources, no hidden variables) so its calibration is simpler. For state-space there is less community knowledge and software in statistical community than for ARIMA.","Display_name":"Kochede","Creater_id":31774,"Start_date":"2013-12-07 03:53:27","Question_id":78287}
{"_id":{"$oid":"5837a58aa05283111e4d7042"},"Last_activity":"2013-12-12 00:30:39","Creator_reputation":887,"Question_score":3,"Answer_content":"Thanks @IrishStat for several very good questions in comments, the answer for your questions is too long to post as comment, so I post it as an answer (unfortunately, not to original question of the topic).Questions were: \"Does it clearly identify time trend changes and report the points in time where the trend changes ? Does it distinguish between parameter changes and error variance changes and report on this ? Does it detect and report on specific lead and lag effects around user specified predictors ? Can one specify the minimum number of values in a group before a level shift/local time trend is declared? Does it distinguish between the need for power transforms versus deterministic points in time where the error variance changes?\"Identify trend changes - yes, most naturally, you can make trend-slope one of state-variables and KF will continuously estimate current slope. You can then decide what slope-change is big enough for you. Alternatively, if slope is not  time-varying in your state-space model, you can test residuals during filtering in a standard way to see when there is some break of your model.Distinguish between parameters changes and error variance changes - yes, variance can be one of parameters(states), then which parameter most likely changed depends on a likelihood of your model and how particularly data have changed.Detect lead/lag relations - not sure about this, you certainly can include any lagged vars into a state-space model; for selection of lags, you can either test residuals of models with different lags included or, in a simple case, just use a cross-correlogram before formulating a model.Specify threshold number of observations to decide trend change - yes, as in 1) because filtering is done recursively, you can not only threshold slope change that is big enough for you, but also # of observations for confidence. But better - KF produces not only estimate of slope, but also confidence bands for this estimate, so you may decide that slope changed significantly when its confidence bound  passed some threshold.Distinguish between need for power-transform and need for bigger variance - not sure I understand correct, but I think you can test residuals during filtering to see if they are still normal with just bigger variance or they got some skew so that you need to change your model. Better - you may make it a binary switching state of your model, then KF will estimate it automatically based on likelihood. In this case model will be non-linear so you will need UKF to do filtering.","Display_name":"Kochede","Creater_id":31774,"Start_date":"2013-12-07 20:48:42","Question_id":78287}
{"_id":{"$oid":"5837a58aa05283111e4d704f"},"Last_activity":"2014-12-04 07:33:08","Creator_reputation":11,"Question_score":1,"Answer_content":"a)prevalence is the amount or the number of cases they have in a specified time(in other word the total number of cases divided by the number of individuals).example,300 players have participated in Ethiopia volleyball premier league in 2014 season,in that season 180 injuries were registered.Therefore,the prevalence of injuries are 180/300=0.6 .b)incidence is the frequency of cases in a specified period of time.example:-in the 2014 Ethiopian volleyball premier league players spend 12345hrs in one season during game and training,in that period 180 injuries were registered. therefore the incidence of injury will be 180/12345*1000= 14.58","Display_name":"ephrem","Creater_id":62088,"Start_date":"2014-12-04 07:33:08","Question_id":66894}
{"_id":{"$oid":"5837a58aa05283111e4d7050"},"Last_activity":"2013-08-08 23:03:16","Creator_reputation":14369,"Question_score":5,"Answer_content":"Incidence refers to new cases of a disease, while prevalence refers to all existing cases.The two are related (Prevalence = Incidence x Disease Duration), but aren't the same thing. For example, for an incurable disease like HIV, the incidence rate could be decreasing as prevention measures improve, but the period prevalence can still rise (due perhaps to increased survival rates).","Display_name":"Fomite","Creater_id":5836,"Start_date":"2013-08-08 23:03:16","Question_id":66894}
{"_id":{"$oid":"5837a58aa05283111e4d705f"},"Last_activity":"2016-07-29 12:22:08","Creator_reputation":536,"Question_score":1,"Answer_content":"You could try some of these approaches:A GLM consists of 3 components - the conditional distribution of the response variable given values of the independent variables, the linear predictor function, and the link function translating the linear predictor to the expected response variable. You've used the default family - Gaussian with default identity link function, this is the same as a multiple linear regression. Check if this is suitable for your target variable mass.loss.a.pct . Evaluate if other alternative family/link functions would be better suited to this dataset.You haven't mentioned the fitted model's null deviance as compared to the fit deviance, that's a good indicator to examine.The leverage plot looks somewhat worrying, it appears you have too many high leverage points which can result in a poor fit.Try using a validation set to see how well the model performs on held-out samples. Since this is a regression model, evaluate the root mean squared error (RMSE), which is the square root of the mean squared error.Since this dataset shows variables with interaction are significant to explain the response variable, you may be able to use regression trees. Try fitting an rpart model on this dataset.Specific answers to your queries are:Meaning of intercept: Yes, you're right, the intercept is supposed to give the expected value of the response when all other variables are 0. If this if too different from the mean you've observed when all variables are 0 then this might imply a poor fit. For this I would recommend checking the deviance and results over held-out validation set.You cant change the intercept, its supposed to balance the equation and the value is obtained by best fitting it to the data. In case the intercept is extremely odd, you could either identify and remove outliers with high leverage. Or, alternatively use regularization to diminish its effects - use glmnet for including regularization in fitting the model.Since these are nominal variables, GLM expands each of these into multiple dummy variables. So, the number of predictors grows to as many as the number of levels of the variable. With interaction effects, these multiply to an even greater number of variables. Your final model is mass.loss.a.pct ~ origin + species + origin:species + 1 . So the number of actual variables used by GLM = Number of unique levels of origin + number of unique levels of species + (number of levels of origins x number of levels of species). Think of each of these dummy variables along with non-dummy variables as a dimension, so if there are 118 dimensions, you're trying to fit a surface through this space. If for any of these dimensions you don't have sufficient values to fit this surface, these coefficients will show up as NA.Also, refer to this help page for guidance on GLM: http://stat.ethz.ch/R-manual/R-patched/library/stats/html/glm.html","Display_name":"Sandeep S. Sandhu","Creater_id":55831,"Start_date":"2016-07-29 12:22:08","Question_id":226337}
{"_id":{"$oid":"5837a58aa05283111e4d706d"},"Last_activity":"2016-07-29 11:53:19","Creator_reputation":12343,"Question_score":0,"Answer_content":"A couple of points:technically, you're not supposed to estimate  for each one of your models. Rather, the estimate of  is done for the most complex (full) model, then applied (without re-estimating it) to compute QAIC(c) for all of the other models. Among other things, the reduced models will always have higher (estimated) , since  is essentially a measure of residual variance. This is reflected in the example given in ?QAIC:budworm.lg \u0026lt;- glm(SF ~ sex*ldose, data = budworm, family = binomial)chat \u0026lt;- deviance(budworm.lg) / df.residual(budworm.lg)dredge(budworm.lg, rank = \"QAIC\", chat = chat)here budworm.lg is the full model; we calculate  from that and apply it uniformly (via an argument to dredge) to all the rest of the models. (Note that this example uses  rather than  as the estimator for  - both are reasonable approximations, there's much discussion of the properties of estimators of overdispersion elsewhere ..)you say that your data are binomial - I'm assuming , otherwise (i.e. for ungrouped  [Bernoulli]) responses, it's hard/not necessarily sensible to compute overdispersion at all).","Display_name":"Ben Bolker","Creater_id":2126,"Start_date":"2016-07-29 11:53:19","Question_id":226322}
{"_id":{"$oid":"5837a58aa05283111e4d707a"},"Last_activity":"2016-07-29 11:24:00","Creator_reputation":15561,"Question_score":1,"Answer_content":"This is edited a bit in response to revisions.It is strange to me that treated and after are dummies, yet you are treating them as continuous variables by using the c. prefix. I would have used the i. prefix. I will assume that is what you had intended below.This will not matter in simple models, but once you add interactions, Stata might choose a different city as the base in your specification (for reasons which elude me). This means the parameters will be different for that reason alone. Fixing the base with something like ib4.city and ib0.time will remedy this. I will add an example below.  Also, I might be inclined to cluster at the city level with this setup. Also, note that i.treated#c.time and c.treated#c.time are not equivalent. Here's example with the cars data:sysuse auto, clearreg price i.foreign#c.mpgreg price c.foreign#c.mpgThe first spec allows for two separate effects for mpg on price, one for domestic cars and one for foreign. The second allows for mpg to alter price for foreign cars only. Now to answer your main question. As a way to relax the parallel trends assumption, people will often include time dummies and a parametric time trend (linear or quadratic) for the treated only in the estimating specification. You are going further and making the time trend city-specific in your first command. There's a nice SJ paper by Mora and Reggio, where they discuss this approach (take a look at equations (3) and (4)). If I understand their notation, that corresponds to i.city#c.treated#c.t or i.city#1.treated#c.t with i.t. They also have a WP with a section on common types of specifications from the literature. All the linear/polynomial trends are for the treatment group only, not both.Your goal can be accomplished by i.city#c.treated#c.t since treating a binary variable as continous is equivalent to putting in a dummy. When you use i.city#i.treated#c.t, that is equivalent to putting in two trends for each city: one for treated and one for control observations. I don't think this is something you want. Personally, I think the clearest way to achieve the former goal is with i.city#1.treated#c.t. Here's an example with the Card \u0026amp; Krueger unemployment data, where I am using the fast food chain instead of city (Wendy's is the base category). This data only has two periods, so there's no separate time dummy. It's a bit confusing since X.chain#c.treated#c.t coefficient is the same parameter as X.chain#1.treated#c.t in the table, but it has different labels:. set more off. estimates clear. use http://fmwww.bc.edu/repec/bocode/c/CardKrueger1994.dta, clear(Dataset from Card\u0026amp;Krueger (1994)). drop if id==407(4 observations deleted). xtset id t       panel variable:  id (strongly balanced)        time variable:  t, 0 to 1                delta:  1 unit. gen chain = \"\"(816 missing values generated). foreach var of varlist  bk kfc roys wendys {  2. replace chain=\"`var'\" if `var'==1  3. }variable chain was str1 now str2(342 real changes made)variable chain was str2 now str3(158 real changes made)variable chain was str3 now str4(198 real changes made)variable chain was str4 now str6(118 real changes made). sencode chain, replace. eststo: qui xtreg fte i.treated##i.t i.chain#c.treated#c.t, fe cluster(id)(est1 stored). eststo: qui xtreg fte i.treated##i.t i.chain#1.treated#c.t, fe cluster(id)(est2 stored). eststo: qui xtreg fte i.treated##i.t i.chain#i.treated#c.t, fe cluster(id)(est3 stored). esttab *, noomitted drop(0.treated 0.t 0.treated#0.t) varwidth(25)-------------------------------------------------------------------------                                   (1)             (2)             (3)                                      fte             fte             fte   -------------------------------------------------------------------------1.t                             -2.523*         -2.523*         -2.577                                  (-2.02)         (-2.02)         (-1.04)   1.treated#1.t                    3.517           3.517           3.571                                   (1.63)          (1.63)          (1.17)   1.chain#c.treated#c.t            0.268                                                                   (0.14)                                   2.chain#c.treated#c.t           -0.225                                                                  (-0.12)                                   3.chain#c.treated#c.t           -2.439                                                                  (-1.28)                                   1.chain#1.treated#c.t                            0.268           0.268                                                   (0.14)          (0.14)   2.chain#1.treated#c.t                           -0.225          -0.225                                                  (-0.12)         (-0.12)   3.chain#1.treated#c.t                           -2.439          -2.439                                                  (-1.28)         (-1.28)   1.chain#0.treated#c.t                                           -0.791                                                                  (-0.23)   2.chain#0.treated#c.t                                            4.804                                                                   (1.87)   3.chain#0.treated#c.t                                           -1.291                                                                  (-0.41)   _cons                            17.69***        17.69***        17.69***                               (79.93)         (79.93)         (79.99)   -------------------------------------------------------------------------N                                  797             797             797   -------------------------------------------------------------------------t statistics in parentheses* p\u0026lt;0.05, ** p\u0026lt;0.01, *** p\u0026lt;0.001Fixing The Base:estimates cleareststo: xtreg fte i.treated##i.t ib4.chain#c.treated#c.t, fe cluster(id)eststo: xtreg fte c.treated##c.t ib4.chain#c.treated#c.t, fe cluster(id)eststo: xtreg fte c.treated##c.t i.chain#c.treated#c.t, fe cluster(id)esttab *, noomitted drop(0.treated 0.t 0.treated#0.t) varwidth(30)","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-23 22:26:32","Question_id":225312}
{"_id":{"$oid":"5837a58aa05283111e4d7087"},"Last_activity":"2016-07-29 11:20:30","Creator_reputation":21,"Question_score":0,"Answer_content":"EDIT: You can see a better discussion here: Understanding which features were most important for logistic regressionTL;DR: You can calculate and compare the marginal effects, but this is an interpretive process. It sounds to me like you're asking about the marginal effect of X on Y, and there's been a lot written about the relative weighting of marginal effects. In the case of an ordinary linear model you can usually just check the size of the  coefficient and interpret as a 1-unit change in  leading to a  change in .Calculating these effects for a logit is more difficult, but hardly impossible.See http://www.appstate.edu/~whiteheadjc/service/logit/intro.htm for a description of how to calculate these effects.A murkier question is how to weight the marginal effects given the different distribution types of  variables. For example, in your code, X1 is distributed uniformly, but X2 is not. When interpreting marginal effects, \"a one unit increase in \" now has different substantive meanings. Throttling up X1 from 0 to 1 means something different than for X2.One possible solution is to standardize your  by dividing by the standard deviations in your  data, but this still hides information about the distributions of your  and what kind of marginal increases are either likely or possible. (See http://gking.harvard.edu/files/mist.pdf for a deeper discussion)","Display_name":"AWP","Creater_id":79540,"Start_date":"2016-07-29 11:20:30","Question_id":226330}
{"_id":{"$oid":"5837a58aa05283111e4d7096"},"Last_activity":"2016-07-29 11:07:13","Creator_reputation":21,"Question_score":2,"Answer_content":"The time-stepped difference is indeed used in one form, the Allan Variance.http://www.allanstime.com/AllanVariance/","Display_name":"Lee J Rickard","Creater_id":125010,"Start_date":"2016-07-29 11:07:13","Question_id":225734}
{"_id":{"$oid":"5837a58aa05283111e4d7097"},"Last_activity":"2016-07-29 06:26:07","Creator_reputation":6097,"Question_score":27,"Answer_content":"The most obvious reason is that there is often no time sequence in the values. So if you jumble the data, it makes no difference in the information conveyed by the data. If we follow your method, then every time you jumble the data you get a different sample variance.The more theoretical answer is that sample variance estimates the true variance of a random variable. The true variance of a random variable  isE\\left[ (X - EX)^2 \\right]. Here  represents expectation or \"average value\". So the definition of the variance is the average squared distance between the variable from its average value. When you look at this definition, there is no \"time order\" here since there is no data. It is just an attribute of the random variable.When you collect iid data from this distribution, you have realizations . The best way to estimate the expectation is to take the sample averages. The key here is that we got iid data, and thus there is no ordering to the data. The sample  is the same as the sample  EDITSample variance measures a specific kind of dispersion for the sample, the one that measures the average distance from the mean. There are other kinds of dispersion like range of data, and Inter-Quantile range.Even if you sort your values in ascending order, that does not change the characteristics of the sample. The sample (data) you get are realizations from a variable. Calculating the sample variance is akin to understanding how much dispersion is in the variable. So for example, if you sample 20 people, and calculate their height, then those are 20 \"realizations\" from the random variable  height of people. Now the sample variance is supposed to measure the variability in the height of individuals in general. If you order the data  100, 110, 123, 124, \\dots,that does not change the information in the sample.Lets look at one more example. lets say you have 100 observations from a random variable  ordered in this way 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ... 100. Then the average subsequent distance is 1 units, so by your method the variance will be 1.The way to interpret \"variance\" or \"dispersion\" is to understand what range of values are likely for the data. In this case you will get a range of .99 unit, which of course does not represent the variation well.If instead of taking average you just sum the subsequent differences, then your variance will be 99. Of course that does not represent the variability in the sample, because 99 gives you the range of the data, not a sense of variability.","Display_name":"Greenparker","Creater_id":31978,"Start_date":"2016-07-26 10:05:48","Question_id":225734}
{"_id":{"$oid":"5837a58aa05283111e4d7098"},"Last_activity":"2016-07-27 09:59:12","Creator_reputation":147773,"Question_score":28,"Answer_content":"It is defined that way!Here's the algebra.  Let the values be .  Denote by  the empirical distribution function of these values (which means each  contributes a probability mass of  at the value ) and let  and  be independent random variables with distribution .  By virtue of basic properties of variance (namely, it is a quadratic form) as well as the definition of  and the fact  and  have the same mean,\\eqalign{\\operatorname{Var}(\\mathbf{x})\u0026amp;=\\operatorname{Var}(X) = \\frac{1}{2}\\left(\\operatorname{Var}(X) + \\operatorname{Var}(Y)\\right)=\\frac{1}{2}\\left(\\operatorname{Var}(X-Y)\\right)\\\\\u0026amp;=\\frac{1}{2}\\left(\\mathbb{E}((X-Y)^2) - \\mathbb{E}(X-Y)^2\\right)\\\\\u0026amp;=\\mathbb{E}\\left(\\frac{1}{2}(X-Y)^2\\right) - 0\\\\\u0026amp;=\\frac{1}{n^2}\\sum_{i,j}\\frac{1}{2}(x_i - x_j)^2.}This formula does not depend on the way  is ordered: it uses all possible pairs of components, comparing them using half their squared differences.  It can, however, be related to an average over all possible orderings (the group  of all  permutations of the indices ).  Namely,\\operatorname{Var}(\\mathbf{x})=\\frac{1}{n^2}\\sum_{i,j}\\frac{1}{2}(x_i - x_j)^2 = \\frac{1}{n!}\\sum_{\\sigma\\in\\mathfrak{S}(n)} \\frac{1}{n} \\sum_{i=1}^{n-1} \\frac{1}{2}(x_{\\sigma(i)} - x_{\\sigma(i+1)})^2.That inner summation takes the reordered values  and sums the (half) squared differences between all  successive pairs.  The division by  essentially averages these successive squared differences. It computes what is known as the lag-1 semivariance. The outer summation does this for all possible orderings.These two equivalent algebraic views of the standard variance formula give new insight into what the variance means.  The semivariance is an inverse measure of the serial covariance of a sequence: the covariance is high (and the numbers are positively correlated) when the semivariance is low, and conversely.  The variance of an unordered dataset, then, is a kind of average of all possible semivariances obtainable under arbitrary reorderings.","Display_name":"whuber","Creater_id":919,"Start_date":"2016-07-26 11:19:15","Question_id":225734}
{"_id":{"$oid":"5837a58aa05283111e4d7099"},"Last_activity":"2016-07-27 05:11:27","Creator_reputation":338,"Question_score":1,"Answer_content":"Lots of good answers here, but I'll add a few.The way it is defined now has proven useful. For example, normal distributions appear all the time in data and a normal distribution is defined by its mean and variance. Edit: as @whuber pointed out in a comment, there are various other ways specify a normal distribution. But none of them, as far as I'm aware, deal with pairs of points in sequence.Variance as normally defined gives you a measure of how spread out the data is. For example, lets say you have a lot of data points with a mean of zero but when you look at it, you see that the data is mostly either around -1 or around 1. Your variance would be about 1. However,  under your measure, you would get a total of zero. Which one is more useful? Well, it depends, but its not clear to me that a measure of zero for its \"variance\" would make sense.It lets you do other stuff. Just an example, in my stats class we saw a video about comparing pitchers (in baseball) over time. As I remember it, pitchers appeared to be getting worse since the proportion of pitches that were hit (or were home-runs) was going up. One reason is that batters were getting better. This made it hard to compare pitchers over time. However, they could use the z-score of the pitchers to compare them over time.Nonetheless, as @Pere said, your metric might prove itself very useful in the future.","Display_name":"roundsquare","Creater_id":122754,"Start_date":"2016-07-26 12:33:25","Question_id":225734}
{"_id":{"$oid":"5837a58aa05283111e4d709a"},"Last_activity":"2016-07-26 16:42:20","Creator_reputation":572,"Question_score":2,"Answer_content":"Although there are many good answers to this question I believe some important points where left behind and since this question came up with a really interesting point I would like to provide yet another point of view.Why isn't variance defined as the difference between every value following    each other instead of the difference to the average of the values?The first thing to have in mind is that the variance is a particular kind of parameter, and not a certain type of calculation. There is a rigorous mathematical definition of what a parameter is but for the time been we can think of then as mathematical operations on the distribution of a random variable. For example if  is a random variable with distribution function  then its mean , which is also a parameter, is:\\mu_X = \\int_{-\\infty}^{+\\infty}xdF_{X}(x)and the variance of , , is:\\sigma^2_X = \\int_{-\\infty}^{+\\infty}(x - \\mu_X)^2dF_{X}(x)The role of estimation in statistics is to provide, from a set of realizations of a r.v., a good approximation for the parameters of interest.What I wanted to show is that there is a big difference in the concepts of a parameters (the variance for this particular question) and the statistic we use to estimate it.Why isn't the variance calculated this way?So we want to estimate the variance of a random variable  from a set of independent realizations of it, lets say . The way you propose doing it is by computing the absolute value of successive differences, summing and taking the mean:\\psi(x) = \\frac{1}{n}\\sum_{i = 2}^{n}|x_i - x_{i-1}|and the usual statistic is:S^2(x) = \\frac{1}{n-1}\\sum_{i = i}^{n}(x_i - \\bar{x})^2,where  is the sample mean.When comparing two estimator of a parameter the usual criterion for the best one is that which has minimal mean square error (MSE), and a important property of MSE is that it can be decomposed in two components:MSE = estimator bias + estimator variance.Using this criterion the usual statistic, , has some advantages over the one you suggests. First it is a unbiased estimator of the variance but your statistic is not unbiased. One other important thing is that if we are working with the normal distribution then  is the best unbiased estimator of  in the sense that it has the smallest variance among all unbiased estimators and thus minimizes the MSE.When normality is assumed, as is the case in many applications,  is the natural choice when you want to estimate the variance.","Display_name":"Mur1lo","Creater_id":120428,"Start_date":"2016-07-26 15:25:01","Question_id":225734}
{"_id":{"$oid":"5837a58aa05283111e4d709b"},"Last_activity":"2016-07-26 14:14:28","Creator_reputation":2542,"Question_score":12,"Answer_content":"Just a complement to the other answers, variance can be computed as the squared difference between terms:\\begin{align}\u0026amp;\\text{Var}(X) = \\\\\u0026amp;\\frac{1}{2\\cdot n^2}\\sum_i^n\\sum_j^n \\left(x_i-x_j\\right)^2 = \\\\\u0026amp;\\frac{1}{2\\cdot n^2}\\sum_i^n\\sum_j^n \\left(x_i - \\overline x -x_j + \\overline x\\right)^2 = \\\\\u0026amp;\\frac{1}{2\\cdot n^2}\\sum_i^n\\sum_j^n \\left((x_i - \\overline x) -(x_j - \\overline x\\right))^2 = \\\\\u0026amp;\\frac{1}{n}\\sum_i^n \\left(x_i - \\overline x \\right)^2\\end{align}I think this is the closest to the OP proposition. Remember the variance is a measure of dispersion of every observation at once, not only between \"neighboring\" numbers in the set.UPDATEUsing your example: . We know the variance is .With your proposed method , so we know beforehand taking the differences between neighbors as variance doesn't add up. What I meant was taking every possible difference squared then summed:Var(X) = \\\\ = \\frac{(5-1)^2+(5-2)^2+(5-3)^2+(5-4)^2+(5-5)^2+(4-1)^2+(4-2)^2+(4-3)^2+(4-4)^2+(4-5)^2+(3-1)^2+(3-2)^2+(3-3)^2+(3-4)^2+(3-5)^2+(2-1)^2+(2-2)^2+(2-3)^2+(2-4)^2+(2-5)^2+(1-1)^2+(1-2)^2+(1-3)^2+(1-4)^2+(1-5)^2}{2 \\cdot 5^2} = \\\\=\\frac{16+9+4+1+9+4+1+1+4+1+1+4+1+1+4+9+1+4+9+16}{50} = \\\\=2","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-07-26 10:50:18","Question_id":225734}
{"_id":{"$oid":"5837a58aa05283111e4d709c"},"Last_activity":"2016-07-26 10:59:22","Creator_reputation":1486,"Question_score":6,"Answer_content":"Others have answered about the usefulness of variance defined as usual. Anyway, we just have two legitimate definitions of different things: the usual definition of variance, and your definition.Then, the main question is why the first one is called variance and not yours. That is just a matter of convention. Until 1918 you could have invented anything you want and called it \"variance\", but in 1918 Fisher used that name to what is still called variance, and if you want to define anything else you will need to find another name to name it.The other question is if the thing you defined might be useful for anything. Others have pointed its problems to be used as a measure of dispersion, but it's up to you to find applications for it. Maybe you find so useful applications that in a century your thing is more famous than variance.","Display_name":"Pere","Creater_id":123561,"Start_date":"2016-07-26 10:33:23","Question_id":225734}
{"_id":{"$oid":"5837a58aa05283111e4d709d"},"Last_activity":"2016-07-26 10:14:35","Creator_reputation":938,"Question_score":3,"Answer_content":"@GreenParker answer is more complete, but an intuitive example might be useful to illustrate the drawback to your approach. In your question, you seem to assume that the order in which realisations of a random variable appear matters. However, it is easy to think of examples in which it doesn't.Consider the example of the height of individuals in a population. The order in which individuals are measured is irrelevant to both the mean height in the population and the variance (how spread out those values are around the mean).Your method would seem odd applied to such a case. ","Display_name":"Antoine Vernet","Creater_id":6917,"Start_date":"2016-07-26 10:14:35","Question_id":225734}
{"_id":{"$oid":"5837a58aa05283111e4d70aa"},"Last_activity":"2016-07-29 10:41:35","Creator_reputation":3398,"Question_score":1,"Answer_content":"For any given question, you can split it into two situations: the student has seen the question and the student has not.\\begin{align*}P(\\mbox{correct})\u0026amp;=P(\\mbox{correct}|\\mbox{seen})P(\\mbox{seen})+P(\\mbox{correct}|\\mbox{not seen})P(\\mbox{not seen})\\\\\u0026amp;=1\\cdot (1/4)+(1/5)(3/4)\\\\\u0026amp;=2/5.\\end{align*}","Display_name":"Alex R.","Creater_id":61092,"Start_date":"2016-07-29 10:41:35","Question_id":226300}
{"_id":{"$oid":"5837a58aa05283111e4d70b7"},"Last_activity":"2016-07-29 10:32:49","Creator_reputation":336,"Question_score":0,"Answer_content":"I finally found a reference to the exact formula written above. It appears in this paper in Eq. 3.5. Hyvarinen also refers to this as a special case of structural equation modeling (SEM). However, he points out that this special case is the basis of the famous causal discovery framework LINGAM (with a few extra restrictions required). Hyvarinen lists a few other related works there. Here are three related things I mentioned before I found the reference above.Covariance estimationLet each sample, , be drawn iid from a multivariate normal distribution, . In that case, the distribution of each  conditioned on the other variables, , is just a normal distribution with mean, , and variance, , that can be constructed from  using standard identities. Use  to denote expectation values (and keep in mind that for simplicity and w.l.o.g. we considered random variables with zero mean). We will define  for compact notation. \\mathbb E [X_i |X_{j\\neq i} ] = \\sum_{j, k \\neq i}^n \\Sigma_{i,j}~ M_{j,k}^i ~X_k \\mathbb E [X_i^2 |X_{j\\neq i} ] =  \\mathbb E [ X_i^2 ] - \\sum_{j, k \\neq i}^n \\Sigma_{i,j} ~ M_{j,k}^i ~\\Sigma_{k,i} The matrix  is formed by first deleting row and column  of , and then taking the matrix inverse of the result.The first line tells us how to predict  from the other values and the second how uncertain that prediction is. Dual total correlation Considering instead a more general point of view, the uncertainty in a variable, , conditioned on the other observations, can be written as the conditional entropy, . The sum of these conditional entropies appears in the dual total correlation, which obeys various bounds, in particular with respect to the total correlation. Linear imputation One domain where this has been explicitly studied is for imputing missing values based on a linear model. This is implemented in Stata, where I see this book cited. I haven't had a chance to check this out yet. ","Display_name":"Greg Ver Steeg","Creater_id":88727,"Start_date":"2016-06-30 08:25:03","Question_id":221348}
{"_id":{"$oid":"5837a58aa05283111e4d70b8"},"Last_activity":"2016-07-02 14:37:47","Creator_reputation":81,"Question_score":1,"Answer_content":"Can we cast this problem as a sparse precision matrix learning problem? It assumes a Gaussian graphical model and learns its structure.  See for example, Ravikumar, P., Wainwright, M. J., Raskutti, G., \u0026amp; Yu, B. (2011). High-dimensional covariance estimation by minimizing ℓ1-penalized log-determinant divergence. Electronic Journal of Statistics, 5, 935-980.There are non-linear extensions of this idea using M-estimators by M. Wainwright. For example, see:Loh, P. L., \u0026amp; Wainwright, M. J. (2013). Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses. The Annals of Statistics, 41(6), 3022-3049. ","Display_name":"Taha","Creater_id":20492,"Start_date":"2016-07-02 14:37:47","Question_id":221348}
{"_id":{"$oid":"5837a58aa05283111e4d70c5"},"Last_activity":"2016-07-29 10:07:57","Creator_reputation":66,"Question_score":4,"Answer_content":"You can treat the sampling distribution of MEANS as being approximately normally distributed (Central Limit Theorem). This is of practical importance in hypothesis testing and fitting confidence intervals. The Central Limit Theorem does NOT apply to the measurements themselves. If your measurement variable is skewed, then it is skewed, no matter how many samples you have.","Display_name":"Groovy_Worm","Creater_id":124314,"Start_date":"2016-07-29 10:07:57","Question_id":226333}
{"_id":{"$oid":"5837a58aa05283111e4d70d2"},"Last_activity":"2016-07-28 22:22:53","Creator_reputation":2818,"Question_score":1,"Answer_content":"The whole thing is problematic, because you don't specify the support of the claimed probability density.  If  is a random variable with density f_X(x) = \\frac{\\alpha}{\\beta}e^{-x/\\beta} + c, then there is necessarily some relationship between the parameters  and the subset  for which  and \\int_{x \\in \\Omega} f_X(x) \\, dx = 0.  Note, for instance, that if  and  comprises a single continuous interval, the support is necessarily bounded below and above.  If we require , then this forces .  If , then again on this same interval, we would obtain \\int_{x=0}^\\infty f_X(x) \\, dx = \\alpha = 1, thus  is your usual exponential distribution with mean .The takeaway here is that the specification of any probability distribution is incomplete without explicitly stating its support.  You can write any density or mass function you like, but if you do not state the support, your distribution is meaningless.","Display_name":"heropup","Creater_id":36771,"Start_date":"2016-07-28 22:22:53","Question_id":226205}
{"_id":{"$oid":"5837a58aa05283111e4d70df"},"Last_activity":"2016-07-29 09:41:58","Creator_reputation":3398,"Question_score":1,"Answer_content":"tSNE is an embedding in lower dimensions (say 2,3) which tries to preserve distances, in this case distances being represented by probabilities. Note that you don't really care about the resulting orientation. You only care to see which clusters of points are close to each other and which are far away. If you look at the points upside-down your conclusion will be the same.Perhaps you could elaborate more on your question but, the result is rotation invariant so long as your distance function is rotationally invariant. Scale invariance is not completely guaranteed, unless you're normalizing your coordinates in a predefined way: there's a notion of perplexity in tSNE which is a hyperparameter that tunes the bandwidth of Gaussians used to define the above probabilities, and keeping this parameter fixed while changing scale will likely give you a different embedding.","Display_name":"Alex R.","Creater_id":61092,"Start_date":"2016-07-29 09:41:58","Question_id":226328}
{"_id":{"$oid":"5837a58aa05283111e4d70f0"},"Last_activity":"2016-07-29 09:19:17","Creator_reputation":1259,"Question_score":1,"Answer_content":"Since the reviewer only seems to be concerned about the two outcomes measured on the same subjects (and did not question the modeling procedure itself), I would simply use a sequential Bonferroni adjustment (a.k.a. Holm-Bonferroni method) to correct for it. Sort your -values in ascending orderRefer to them as  (i.e. , etc.)Than you adjust your -level and compare the -values against that new -levels, i.e. you test whether , where  is the number of statistical tests conducted, i.e. the number of -values calculated. You can stop when . Those  that fall below the sequentially adjusted -levels are now your significant tests which are adjusted for multiplicity (after the Holm-Bonferroni method). For example you conducted five tests () resulting in the following -values:The new -level you compare  against is:Since  you can move on to :Since  you can move on to :Since  you can stop.In this case, from initially four significant -values, you now only have two but those are adjusted for multiplicity (Note: Instead of adjusting the -levels, you can also adjust the -values and compare against your chosen -level (e.g. ). Then all you need to do is  instead).See also:Abdi, H. (2010). Holm’s sequential Bonferroni procedure. Encyclopedia of research design, 1.Peres-Neto, P. R. (1999). How many statistical tests are too many? The problem of conducting multiple ecological inferences revisited. Marine Ecology Progress Series, 176, 303-306.Alternatively, you could also argue that you don't want to adjust for multiplicity because of reason such as being concerned with making type II errors.See here:Feise, R. J. (2002). Do multiple outcome measures require p-value adjustment?. BMC Medical Research Methodology, 2(1), 1.Or maybe this one: Gelman, A., Hill, J., \u0026amp; Yajima, M. (2012). Why we (usually) don't have to worry about multiple comparisons. Journal of Research on Educational Effectiveness, 5(2), 189-211.","Display_name":"Stefan","Creater_id":32477,"Start_date":"2016-07-28 21:34:52","Question_id":225937}
{"_id":{"$oid":"5837a58aa05283111e4d70ff"},"Last_activity":"2016-07-29 08:43:34","Creator_reputation":304,"Question_score":1,"Answer_content":"The 2nd answer to a Google search for 4 parameter logistic r is this promising paper in which the authors have developed and implemented methods for analysis of assays such as ELISA in the R package drc.  Specifically, the authors have developed a function LL.4() which implements the 4 paramater logistic regression function, for use with the general dose response modeling function drm.Christian Ritz, Jens Streiberg.  Bioassay Analysis Using R.  Journal of Statistical Software, 2005, Vol. 12, No. 5.Ritz et al have published a new paper that covers improvements to the 'drc' R package.Dose Response Analysis using R (PLOS ONE, 2015)","Display_name":"learner","Creater_id":10530,"Start_date":"2013-06-07 04:30:11","Question_id":61144}
{"_id":{"$oid":"5837a58aa05283111e4d7100"},"Last_activity":"2013-12-05 14:59:45","Creator_reputation":3582,"Question_score":0,"Answer_content":"You can find the least-square estimate of the parameters using nonlinear regression.  Example:f=function(B,x)  (B[1]-B[4])/(1+(x/B[3])^B[2])+B[4]LS=function(B,y,x)  sum((y-f(B,x))^2)x=runif(100,0,5)B=c(1,5,2.5,5)y=f(B,x)plot(x,y)### Estimate should be very close to Bnlm(LS,c(1,1,1,1),x=x,y=y)","Display_name":"Glen","Creater_id":2310,"Start_date":"2013-12-05 14:59:45","Question_id":61144}
{"_id":{"$oid":"5837a58aa05283111e4d7101"},"Last_activity":"2013-12-05 14:04:46","Creator_reputation":1,"Question_score":0,"Answer_content":"  After days, I record my found here:    http://www.bioassay.dk/index-filer/start/DraftDrcManual.pdf gives me  the current manual of drc package in R. For example:    library(drc) model1 \u0026lt;- drm(SLOPE~DOSE, CURVE,  fct=LL.4(names=c(\"Slope\", \"Lower\", \"Upper\", \"ED50\")),data=spinach)  summary(model1) plot(model1)   If I wanna predict the dose from  observation.    model2 \u0026lt;- drm(DOSE~SLOPE, CURVE, fct=LL.4(names=c(\"Slope\", \"Lower\",  \"Upper\", \"ED50\")),data=spinach) predict(model2, newdata,  type=\"response\") newdata is a dataframe'predict' is not the best way to estimate the DOSE from SLOPE in this case, because you have to reverse them in your model2, which doesn't work in this example. If you want to estimate the DOSE from SLOPE, or 'Concentration' from 'OD' in case of an ELISA, just use the ED function of the 'drc' packageEXAMPLE:library(drc)model1 \u0026lt;- drm(SLOPE~DOSE, CURVE,               fct=LL.4(names=c(\"Slope\", \"Lower\", \"Upper\", \"ED50\")),data=spinach)plot(model1)# the ED function is used to give the EDx value. For example, the ED50 is the # DOSE value for the 50% responseED(model1,50)# check ?ED?ED# The result is a matrix, from which the Estimate values can be extracted using# the index (display=F is a good option also) ED(model1,50,display=F)[1:5]# type=\"absolute\" gives you the ability to use absolute values for the response, to # estimate the DOSEresponse\u0026lt;-0.5   #lets use 0.5 for the responseDOSEx\u0026lt;-ED(model1,response,type=\"absolute\",display=F)[1:5] # the estimated DOSEpoints(y=rep(response,5),x=DOSEx,col=\"blue\",pch=1:5)","Display_name":"RBA","Creater_id":35739,"Start_date":"2013-12-05 13:48:35","Question_id":61144}
{"_id":{"$oid":"5837a58aa05283111e4d7102"},"Last_activity":"2013-06-11 00:05:47","Creator_reputation":16,"Question_score":0,"Answer_content":"After days, I record my found here:http://www.bioassay.dk/index-filer/start/DraftDrcManual.pdf gives me the current manual of drc package in R. For example:library(drc)model1 \u0026lt;- drm(SLOPE~DOSE, CURVE, fct=LL.4(names=c(\"Slope\", \"Lower\", \"Upper\", \"ED50\")),data=spinach)summary(model1)plot(model1)If I wanna predict the dose from observation.model2 \u0026lt;- drm(DOSE~SLOPE, CURVE, fct=LL.4(names=c(\"Slope\", \"Lower\", \"Upper\", \"ED50\")),data=spinach)predict(model2, newdata, type=\"response\")newdata is a dataframeIf I wanna check the regression, R square is not good for nonlinear regressionRSD \u0026lt;- abs(sqrt(summary(model1)$\"resVar\") / mean(fitted(model1)))Thanks for Christian Ritz and my father's help.","Display_name":"kaji331","Creater_id":26342,"Start_date":"2013-06-11 00:05:47","Question_id":61144}
{"_id":{"$oid":"5837a58aa05283111e4d710f"},"Last_activity":"2016-07-29 08:28:58","Creator_reputation":1149,"Question_score":2,"Answer_content":"Of course, in SPSS, the missing value(s) 999 or whatever IS tagged as a special missing code and handled separately from other values.  It may be tabulated separately or excluded entirely.  A distinction is made from the result of things like zero division or log(0).","Display_name":"JKP","Creater_id":6768,"Start_date":"2016-07-29 08:28:58","Question_id":225175}
{"_id":{"$oid":"5837a58aa05283111e4d7110"},"Last_activity":"2016-07-22 14:29:34","Creator_reputation":147773,"Question_score":23,"Answer_content":"Such values are for databases.  Most databases long ago, and many today, allocated a fixed number of digits for integer-valued data.  A number like -999 is the smallest that can be stored in four characters, -9999 in five characters, and so on.(It should go without saying that--by definition--a numeric field cannot store alphanumeric characters such as \"NA\".  Some numeric code has to be used to represent missing or invalid data.)Why use the most negative number that can be stored to signify a missing value?  Because if you mistakenly treat it as a valid number, you want the results to be dramatically incorrect.  The further your codes for missing values get from being realistic, the safer you are, because hugely wrong input usually screws up the output.  (Robust statistical methods are notable exceptions!)How could such a mistake happen?  This occurs all the time when data are exchanged between systems.  A system that assumes -9999 represents a missing value will blithely output that value when you write the data out in most formats, such as CSV.  The system that reads that CSV file might not \"know\" (or not be \"told\") to treat such values as missing.Another reason is that good statistical data and computing platforms recognize many different kinds of missing values: NaNs, truly missing values, overflows, underflows, non-responses, etc, etc.  By devoting the most negative possible values (such as -9999, -9998, -9997, etc) to these, you make it easy to query out all missing values from any table or array.Yet another is that such values usually show up in graphical displays as extreme outliers.  Of all the values you could choose to stand out in a graphic, the most negative possible one stands the greatest chance of being far from your data.There are useful implications and generalizations:A good value to use for missing data in floating-point fields is the most negative valid number, equal approximately to  for double-precision floats.  (Imagine the effect that would have on any average!)  On the same principle, many old programs, which used single-precision floats, used somewhat arbitrary large numbers such as 1E+30 for missing values.Adopt  a standard rule of this type to make it easy to invent NoData codes in new circumstances (when you are designing your own database software).Design your software and systems to fail dramatically if they fail at all.  The worst bugs are those that are intermittent, random, or tiny, because they can go undetected and be difficult to hunt down.  ","Display_name":"whuber","Creater_id":919,"Start_date":"2016-07-22 14:29:34","Question_id":225175}
{"_id":{"$oid":"5837a58aa05283111e4d7111"},"Last_activity":"2016-07-22 13:00:41","Creator_reputation":25585,"Question_score":13,"Answer_content":"You can use anything to encode missing values. Some software, like R, use special values to encode missing data, but there are also software packages, e.g. SPSS, that do not have any special codes for missing data. In the second case you need to make arbitrary choice for such values. You can choose anything, but generally it is a good idea to choose some value that visibly differs from your data (e.g. your data are percentages in 0-100 range, so you choose 999 for encoding missing data, or your data is human age and you use negative values for missing observations). The idea behind it is that by doing so you should be able to notice if something went wrong and the numbers do not add up. The problem with such encoding is however that you actually can not notice the special encoding and end up with rubbish results. ","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-22 12:53:24","Question_id":225175}
{"_id":{"$oid":"5837a58aa05283111e4d7112"},"Last_activity":"2016-07-22 12:55:29","Creator_reputation":20452,"Question_score":61,"Answer_content":"This is a holdout from earlier times, when computer software stored numerical vectors as numerical vectors. No real number has the semantics \"I'm missing\". So when early statistical software had to differentiate between \"true\" numbers and missing values, they put in something that was \"obviously\" not a valid number, like -999 or -9999.Of course, that -999 or -9999 stood for a missing value is not \"obvious\" at all. Quite often, it can certainly be a valid value. Unless you explicitly check for such values, you can have all kinds of \"interesting\" errors in your analyses.Nowadays, numerical vectors that can contain missing values are internally represented as \"enriched\" numerical vectors, i.e., numerical vectors with additional information as to which values are missing. This of course is much better, because then missing values will be treated as such and not mistakenly treated as valid.Unfortunately, some software still uses such a convention, perhaps for compatibility. And some users have soaked up this convention through informal osmosis and enter -999 instead of NA even if their software supports cleanly entering missing values.Moral: don't encode missing values as -999.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2016-07-22 12:55:29","Question_id":225175}
{"_id":{"$oid":"5837a58aa05283111e4d7113"},"Last_activity":"2016-07-22 12:55:13","Creator_reputation":17464,"Question_score":2,"Answer_content":"Are there computed variables in the dataset? Or is this an analytic dataset that comes form merged / sorted data? Some software uses very large negative values to denote missing data. But other software creates missing values with NA or .. When they are discrepant, usually some post processing has led to disagreement. ","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-07-22 12:55:13","Question_id":225175}
{"_id":{"$oid":"5837a58aa05283111e4d7124"},"Last_activity":"2016-07-29 08:00:22","Creator_reputation":150,"Question_score":2,"Answer_content":"You can keep categorical as well as numeric variable together. Just make sure than you dont have any missing values in the dataset. If you are creating a categorical variable from a numeric that there will be information loss. Also do read this for more info.","Display_name":"Chirayu Chamoli","Creater_id":100552,"Start_date":"2016-05-23 06:36:46","Question_id":214031}
{"_id":{"$oid":"5837a58aa05283111e4d7131"},"Last_activity":"2016-07-29 07:56:04","Creator_reputation":26,"Question_score":0,"Answer_content":"I think when the assumption of normality is not met, you should do non-parametric tests of significance, if you had variables like rankings or categories. You could still continue with t-tests for the mean if you have a sample size above 30. ","Display_name":"user124526","Creater_id":124526,"Start_date":"2016-07-29 07:24:52","Question_id":226282}
{"_id":{"$oid":"5837a58aa05283111e4d7132"},"Last_activity":"2016-07-29 07:27:35","Creator_reputation":23,"Question_score":2,"Answer_content":"The t-test is quite robust to departures from the assumption of normality. Intuitively, the reason for this is that the T statistic is based on averages, which are asymptotically normal under mild conditions (Central limit theorem), and typically they converge fast to normality.Alternative nonparametric tests (for equality of distributions or means) include the Kolmogorov-Smirnov test, permutation tests, and Wilcoxon signed-rank test.","Display_name":"Dorian","Creater_id":124970,"Start_date":"2016-07-29 07:02:18","Question_id":226282}
{"_id":{"$oid":"5837a58aa05283111e4d7149"},"Last_activity":"2016-07-29 07:24:05","Creator_reputation":1143,"Question_score":1,"Answer_content":"P-values are computed as \"the likelihood of that result and all that are worse\" (in the sense of deviating from ). One way to see why that makes sense is that if the number of purchases is , then the probability of having exactly 229 purchases is fairly low, even though it's the most likely outcome. Further, if you apply your pmf/pdf approach to continuous statistics like Normal, then you will likelihood of zero every single time.","Display_name":"James","Creater_id":54099,"Start_date":"2016-07-29 07:24:05","Question_id":226298}
{"_id":{"$oid":"5837a58aa05283111e4d7158"},"Last_activity":"2016-07-29 06:51:21","Creator_reputation":19151,"Question_score":0,"Answer_content":"If you have two independent variables with different sampling frequencies then when referring to a sampling size you can usually use the higher frequency if both variables are important (in some sense) in the model. There's always a room for trickery, of course. For instance, you could add a variable with high frequency which doesn't have any meaningful impact, but you keep it so to prop up the sample size. Just be careful with this aspect, otherwise it's Ok to use higher frequency when referring to a sample size.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-07-29 06:51:21","Question_id":226288}
{"_id":{"$oid":"5837a58aa05283111e4d7167"},"Last_activity":"2016-07-29 06:48:39","Creator_reputation":21628,"Question_score":14,"Answer_content":"You can use any of the norms  (see Wikipedia on a variety of norms; note that the square-root of the sum of squared distances, , is called Frobenius norm, and is different from  norm, which is the square root of the largest eigenvalue of , although of course they would generate the same topology). The K-L distance between the two normal distributions with the same means (say zero) and the two specific covariance matrices is also available in Wikipedia as .Edit: if one of the matrices is a model-implied matrix, and the other is the sample covariance matrix, then of course you can form a likelihood ratio test between the two. My personal favorite collection of such tests for simple structures is given in Rencher (2002) Methods of Multivariate Analysis. More advanced cases are covered in covariance structure modeling, on which a reasonable starting point is Bollen (1989) Structural Equations with Latent Variables.","Display_name":"StasK","Creater_id":5739,"Start_date":"2011-08-22 20:18:09","Question_id":14673}
{"_id":{"$oid":"5837a58aa05283111e4d7168"},"Last_activity":"2016-07-29 06:11:29","Creator_reputation":14835,"Question_score":6,"Answer_content":"Denote  and   your matrices both of dimension .Cond number: where  () is the largest (smallest) eigenvalue of , where  is defined as:Edit: I edited out the second of the two proposals. I think I had misunderstood the question. The proposal based on condition numbers is used in robust statistics a lot to assess quality of fit. An old source I could find for it is:  Yohai, V.J. and Maronna, R.A. (1990). The Maximum Bias of Robust  Covariances. Communications in Statistics–Theory and Methods, 19,  3925–2933.I had originally included the Det ratio measure:    Det ratio:   where .  which would be the Bhattacharyya distance between two Gaussian distributions having the same location vector. I must have originally read the question as pertaining to a setting where the two covariances were coming from samples from populations assumed to have equal means.  ","Display_name":"user603","Creater_id":603,"Start_date":"2011-08-22 23:40:53","Question_id":14673}
{"_id":{"$oid":"5837a58aa05283111e4d7169"},"Last_activity":"2016-03-02 05:00:02","Creator_reputation":61,"Question_score":6,"Answer_content":"A measure introduced by Herdin (2005) Correlation Matrix Distance, a Meaningful Measurefor Evaluation of Non-Stationary MIMO Channels isd = 1 - \\frac{\\text{tr}(R_1 \\cdot R_2)}{\\|R_1\\| \\cdot \\|R_2\\|},where the norm is the Frobenius norm.","Display_name":"davidc","Creater_id":30885,"Start_date":"2014-05-29 07:49:52","Question_id":14673}
{"_id":{"$oid":"5837a58aa05283111e4d716a"},"Last_activity":"2012-08-08 18:07:28","Creator_reputation":31,"Question_score":3,"Answer_content":"The covariance matrix distance is used for tracking objects in Computer Vision. The currently used metric is described in the article: \"A metric for covariance matrices\", by Förstner and Moonen.","Display_name":"Andres Romero","Creater_id":11714,"Start_date":"2012-06-02 03:51:57","Question_id":14673}
{"_id":{"$oid":"5837a58aa05283111e4d7177"},"Last_activity":"2016-07-29 06:40:04","Creator_reputation":3832,"Question_score":4,"Answer_content":"In some cases yes; autodiff certainly makes life easier in many circumstances. But the EM algorithm may still be more appropriate in other cases. For example, consider fitting mixture models. At each step, the EM algorithm will always return valid parameters for the distribution. Although gradient-based optimization methods can be used, complicated constraints may be necessary. The mixture weights must be constrained to be positive and sum to one, covariance matrices must be constrained to be positive semidefinite, etc. The EM algorithm may have better convergence for some (but certainly not all) problems.The following posts may also of interest: 1, 2, 3.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-07-29 04:49:43","Question_id":226241}
{"_id":{"$oid":"5837a58aa05283111e4d7184"},"Last_activity":"2016-07-29 06:20:06","Creator_reputation":25585,"Question_score":1,"Answer_content":"Since Pearson correlation may be defined as r = \\frac{\\sum_{i=1}^n Z^{(X)}_i Z^{(Y)}_i}{n-1} where  and  are -scores of  and , then by converting your variables to -scores you are one step closer to calculating it! See also this answer, as it relates to your question: Can Spearman\u0026#39;s correlation be run on z scores?","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-29 06:10:53","Question_id":226283}
{"_id":{"$oid":"5837a58aa05283111e4d7191"},"Last_activity":"2016-07-29 05:58:05","Creator_reputation":2542,"Question_score":0,"Answer_content":"It's advisable to control either the Family-wise error rate (FWER) or the False discovery rate (FDR).FWER: estimate of the occurrence of at least one type-I error (lesser false positive rate, higher false negative rate)FDR: estimate of the rate of type-I errors (higher false positive rate, lesser false negative rate)The reason is simple, the more tests you do the more likely is to reject a null by chance. Check the article at Wikipedia.","Display_name":"Firebug","Creater_id":60613,"Start_date":"2016-07-29 05:58:05","Question_id":226275}
{"_id":{"$oid":"5837a58aa05283111e4d719e"},"Last_activity":"2016-07-29 05:48:57","Creator_reputation":23,"Question_score":0,"Answer_content":"It may well be the case that the available covariates/predictors simply do not explain the response variable. What you need to do, rather than just looking at credible intervals, is a formal Bayesian variable selection. There is an R package that can do this very quickly using some priors that were developed for variable selection:Package ‘mombf’If you prefer a simpler, rather informal, way of selecting variables, you can also use stepwise variable selection based on AIC or BIC (highly criticized) using the stepAIC() R command (BIC selection can be obtained after some small tweaks to this command).","Display_name":"Dorian","Creater_id":124970,"Start_date":"2016-07-29 05:16:23","Question_id":226197}
{"_id":{"$oid":"5837a58aa05283111e4d719f"},"Last_activity":"2016-07-28 17:39:22","Creator_reputation":8367,"Question_score":2,"Answer_content":"I presume that by \"significant from 0\", you mean \"significantly different from 0\". However, you haven't conducted any significance tests. You've created posterior intervals (and, I imagine, checked to see if they contain 0). Posterior intervals and significance tests are apples and oranges, so don't try to treat one as the other.Similarly, you sound like you are under the impression that if the posterior intervals for all the coefficients contain 0, then there must be something wrong with the model. This is not so.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-28 17:39:22","Question_id":226197}
{"_id":{"$oid":"5837a58aa05283111e4d71ac"},"Last_activity":"2016-07-29 05:25:27","Creator_reputation":25585,"Question_score":2,"Answer_content":"For simplicity, let's assume that we are talking about some really simple kernel, say triangular kernel: K(x) =\\begin{cases}1 - |x| \u0026amp; \\text{if } x \\in [-1, 1] \\\\0 \u0026amp; \\text{otherwise}\\end{cases}Recall that in kernel density estimation for estimating density  we combine  kernels parametrized by  centered at points :\\hat{f}_h(x) = \\frac{1}{n}\\sum_{i=1}^n K_h (x - x_i) = \\frac{1}{nh} \\sum_{i=1}^n K\\Big(\\frac{x-x_i}{h}\\Big)Notice that by  we mean that we want to re-scale the difference of some  with point  by factor . Most of the kernels (excluding Gaussian) are limited to the  range, so this means that they will return densities equal to zero for points out of  range. Saying it differently,  is scale parameter for kernel, that changes it's range from  to .This is illustrated on the plot below, where  points are used for estimating kernel densities with different bandwidthes  (colored points on top mark the individual values, colored lines are the kernels, gray line is overall kernel estimate). As you can see,  makes the kernels narrower, while  makes them wider. Changing  influences both the individual kernels and the final kernel density estimate, since it's a mixture distribution of individual kernels. Higher  makes the kernel density estimate smoother, while as  gets smaller it leads to kernels being closer to individual datapoints, and with  you would end up with just a bunch of Direc delta functions centered at  points.And the R code that produced the plots:set.seed(123)n \u0026lt;- 7x \u0026lt;- rnorm(n, sd = 3)K \u0026lt;- function(x) ifelse(x \u0026gt;= -1 \u0026amp; x \u0026lt;= 1, 1 - abs(x), 0)kde \u0026lt;- function(x, data, h, K) {  n \u0026lt;- length(data)  out \u0026lt;- outer(x, data, function(xi,yi) K((xi-yi)/h))  rowSums(out)/(n*h)} xx = seq(-8, 8, by = 0.001)for (h in c(0.5, 1, 1.5, 2)) {  plot(NA, xlim = c(-4, 8), ylim = c(0, 0.5), xlab = \"\", ylab = \"\",       main = paste0(\"h = \", h))  for (i in 1:n) {    lines(xx, K((xx-x[i])/h)/n, type = \"l\", col = rainbow(n)[i])    rug(x[i], lwd = 2, col = rainbow(n)[i], side = 3, ticksize = 0.075)  }  lines(xx, kde(xx, x, h, K), col = \"darkgray\")}For more details you can check the great introductory books by Silverman (1986) and Wand \u0026amp; Jones (1995).Silverman, B.W. (1986). Density estimation for statistics and data analysis. CRC/Chapman \u0026amp; Hall.Wand, M.P and Jones, M.C. (1995). Kernel Smoothing. London: Chapman \u0026amp; Hall/CRC.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-29 01:30:33","Question_id":226232}
{"_id":{"$oid":"5837a58aa05283111e4d71b9"},"Last_activity":"2011-10-18 21:51:37","Creator_reputation":1237,"Question_score":95,"Answer_content":"The LASSO (Least Absolute Shrinkage and Selection Operator) is a regression method that involves penalizing the absolute size of the regression coefficients. By penalizing (or equivalently constraining the sum of the absolute values of the estimates) you end up in a situation where some of the parameter estimates may be exactly zero. The larger the penalty applied, the further estimates are shrunk towards zero.This is convenient when we want some automatic feature/variable selection, or when dealing with highly correlated predictors, where standard regression will usually have regression coefficients that are 'too large'.http://www-stat.stanford.edu/~tibs/ElemStatLearn/ (Free download) has a good description of the LASSO and related methods.","Display_name":"dcl","Creater_id":845,"Start_date":"2011-10-18 21:42:21","Question_id":17251}
{"_id":{"$oid":"5837a58aa05283111e4d71ba"},"Last_activity":"2011-10-18 21:35:32","Creator_reputation":3221,"Question_score":22,"Answer_content":"From Robert Tibshirani's (the author of the original lasso paper) page:  A simple explanation of the Lasso and Least Angle Regression.","Display_name":"Mike Wierzbicki","Creater_id":5594,"Start_date":"2011-10-18 21:35:32","Question_id":17251}
{"_id":{"$oid":"5837a58aa05283111e4d71cb"},"Last_activity":"2016-07-29 04:49:16","Creator_reputation":12922,"Question_score":12,"Answer_content":"You're right, the solution surface is going to be a hyperplane in general.  It's just that the word hyperplane is a mouthful, plane is shorter, and line is even shorter.  As you continue on in math, the one dimensional case becomes discussed ever more rarely so the tradeoffBig words for high dimensional, Small words for small dimensionalstarts to look, well, backwards.For example, when I see an equation like , where  is a matrix and  are vectors, I call this a linear equation.  In an earlier part of my life, I would call this a system of linear equations, reserving linear equation for the one dimensional case.  But then I got to a point where the one dimensional case just did not come up very often, while the multi-dimensional case was everywhere.This also happens with notation.  Ever seen someone write \\frac{\\partial f}{\\partial x} = 2x That symbol on the left is a name of a function, so to be formal and pedantic, you should write \\frac{\\partial f}{\\partial x}(x) = 2x It get's worse in multi-dimenstions, when the deriviative takes two arguments, one is where you take the derivative, and the other is in which direction you evaluate the derivative, which looks like \\nabla_{x} f (v)but people get lazy very quickly, and begin to drop one or the other arguments, leaving them understood by context.  Professional mathematicians, tongues firmly in cheek, call this abuse of notation.  There are subjects in which it would be essentially impossible to express oneself without abusing notation, my beloved differential geometry being a case in point. The great Nicolas Bourbaki expressed the point very eloquently  As far as possible we have drawn attention in the text to abuses of language, without which any mathematical text runs the risk of pedantry, not to say unreadability.    — Bourbaki (1988)You even comment on an abuse of notation I fell into above without neven noticing it myself!  Technically since you wrote df/dx as a partial derivative, even though the other implied variables would be held as constant, wouldn't the partial derivative technically still be a function of all the variables of the original function, as in df/dx (x, y, ...)?You're perfectly correct, and this gives a good (unintentional) illustration of what I'm getting at here.I encounter the derivative in a true one-variable sense so rarely in my day-to-day work and studies, that I've essentially forgotten that  is the correct notation here.  I intended the above to be about a one variable function, but unconsciously signaled otherwise by my use of .  Guess i think of it as when we say \"infinite sum\" instead of \"the limit of a sum as the number of terms approaches infinity\". The way I think about it is that it's fine as long as the conceptual difference is clear. In this case (multiple regression), I wasn't really sure what we were talking about in the first place.Yah, that's a consistent way to think about it.  The only real difference is that there we have such a common situation that we invented additional(*) notation and terminology ( and \"infinite sum\") to express it.  In other cases we generalize a concept, and then that generalized concept becomes so ubiquitous that we reuse old notation or terminology for the generalized concept.As lazy people we want to economize words in the common cases.(*) Historically, this is not how infinite sums developed.  The limit of partial sums definition was developed a posteriori when mathematicians started encountering situations where it was necessary to reason very precisely.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-07-28 12:53:16","Question_id":226172}
{"_id":{"$oid":"5837a58aa05283111e4d71cc"},"Last_activity":"2016-07-28 18:43:18","Creator_reputation":152738,"Question_score":5,"Answer_content":"\"Linear\" doesn't quite mean what you think it does in this context - it's a bit more generalFirstly, it's not really a reference to linearity in the x's but to the parameters* (\"linear in the parameters\").Secondly a linear function in the linear algebra sense is essentially a linear map;  is a linear function in -space. So a plane (or more generally hyperplane) of best fit is still \"linear regression\".* though it will be linear in the supplied x's if you consider the constant column of 's as part of the coordinate-vectore (or alternatively think of it in homogenous coordinates with normalization of the additional coordinate). Or you could just say  is linear in both  and ","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-28 17:54:31","Question_id":226172}
{"_id":{"$oid":"5837a58aa05283111e4d71d8"},"Last_activity":"2016-07-29 04:47:35","Creator_reputation":25585,"Question_score":3,"Answer_content":"Your formula 1 - sum(fit\\hat y_i\\bar yyR^2\\bar y$ is basically the same as prediction from intercept-only regression, so the dividend is residuals from your model and divisor is residuals from the null model (the most basic one that we can imagine).","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-29 04:35:02","Question_id":226260}
{"_id":{"$oid":"5837a58aa05283111e4d71d9"},"Last_activity":"2016-07-29 04:34:56","Creator_reputation":12922,"Question_score":3,"Answer_content":"You're missing an important part of the  computation\u0026gt; library(data.table)\u0026gt; set.seed(154)\u0026gt; dt = data.table(x = rnorm(100))\u0026gt; dt[, y := 1+0.2*x + rnorm(100)]\u0026gt; fit = lm(y~x, data = dt)\u0026gt; summary(fit)Call:lm(formula = y ~ x, data = dt)Residuals:    Min      1Q  Median      3Q     Max -2.6419 -0.6176 -0.0164  0.6459  2.6444 Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)  1.09677    0.10214  10.738   \u0026lt;2e-16 ***x            0.21187    0.09974   2.124   0.0362 *  ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 1.021 on 98 degrees of freedomMultiple R-squared:  0.04402,   Adjusted R-squared:  0.03427 F-statistic: 4.513 on 1 and 98 DF,  p-value: 0.03616\u0026gt; dt[, 1 - sum(fit$residuals^2) / sum((y - mean(y))^2)][1] 0.04402207The difference is in the very last line.","Display_name":"Matthew Drury","Creater_id":74500,"Start_date":"2016-07-29 04:34:56","Question_id":226260}
{"_id":{"$oid":"5837a58aa05283111e4d71e6"},"Last_activity":"2014-01-24 01:32:12","Creator_reputation":608,"Question_score":1,"Answer_content":"If you have only positive samples, you will never be able to compute a false alarm rate because you don't have any negative samples that could be incorrectly labelled positive.This is because FAR = FA / N = 0 / 0with N = number of negative samples and FA the number of false alarm in your sample.In other words, you will need to collect a set of negative samples if you want to compute this statistic.","Display_name":"Calimo","Creater_id":36682,"Start_date":"2014-01-24 01:32:12","Question_id":83201}
{"_id":{"$oid":"5837a58aa05283111e4d71fd"},"Last_activity":"2016-07-29 04:20:42","Creator_reputation":21,"Question_score":1,"Answer_content":"Not sure how to answer this, maybe you could ask more explicit questions.For sure you first need to transform this data so that all data on a family is merged in one row. So you need to create some aggregation. You cannot keep a 'gender' attribute for example. Maybe you want attributes like: number adults, number children, marital status, sum education years, average adult age, ...Then you can create linear regression models to model the relation age -\u003e incomeOnce you have done this I suggest to create a plot of this relation over the years to answer your second question.","Display_name":"Tobi","Creater_id":124963,"Start_date":"2016-07-29 04:20:42","Question_id":226225}
{"_id":{"$oid":"5837a58aa05283111e4d7209"},"Last_activity":"2016-07-29 04:06:20","Creator_reputation":57772,"Question_score":2,"Answer_content":"If you are exposing the same mosquitoes to multiple stimuli then your data is not independent and you will have to account for that in the analysis. One way to do this is with a mixed model. If you are using different mosquitoes each time, then it seems like an ANOVA problem, but I don't know what you mean by \"control set to 0\" nor why it \"doesn't seem like a good way\".  Can you  elucidate? ","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2016-07-29 04:06:20","Question_id":226254}
{"_id":{"$oid":"5837a58aa05283111e4d7218"},"Last_activity":"2016-07-29 03:57:42","Creator_reputation":26386,"Question_score":0,"Answer_content":"If you look at the definition of a Bayes factor (as, e.g., in my book),  it is expressed as the ratio of two marginal likelihoodsm_0(x_1,\\ldots,x_N)=\\int_{\\Theta_0} \\ell_0(\\theta_0|x_1,\\ldots,x_N)\\pi_0(\\theta_0)\\text{d}\\theta_0andm_1(x_1,\\ldots,x_N)=\\int_{\\Theta_1} \\ell_1(\\theta_1|x_1,\\ldots,x_N)\\pi_1(\\theta_1)\\text{d}\\theta_1[with generic notations]. Hence you have to integrate one likelihood per model for the entire dataset or sample. In each model you thus end up with a single numerical value.","Display_name":"Xi\u0026#39;an","Creater_id":7224,"Start_date":"2016-07-29 03:57:42","Question_id":226088}
{"_id":{"$oid":"5837a58aa05283111e4d7225"},"Last_activity":"2016-07-29 03:21:55","Creator_reputation":26,"Question_score":1,"Answer_content":"Interesting question. In a practical sense, Bayesian and Likelihood-based approaches only differ in that the former requires the specification of a prior.  If you can come up with a Likelihood-based model:L(\\theta;Data) = f(Data\\vert\\theta),then you \"just\" need to specify a prior for , . The way you typically go through the process of checking the residuals against a set of assumptions, adjusting/removing variables, rerunning the model, checking the assumptions again, and again is probably based on the fitted model using maximum likelihood estimation. In a Bayesian framework, however, people take advantage of having access to the full posterior distribution. Then, you can either obtain a point estimator from this distribution (e.g. posterior mean, posterior median, ...) and conduct the same analysis using this estimate. Alternatively, and more formally, you can integrate the parameters out with respect to the posterior distribution and use the predictive posterior distributionf(y\\vert Data) = \\int f(y\\vert\\theta)\\pi(\\theta\\vert Data)d\\theta.For instance, in this paper the authors check the goodness of fit of a Bayesian linear regression model based on the predictive residuals, something like a Bayesian QQ plot.Of course, if your model is complicated, sampling from the posterior distribution may be a difficult task, but conceptually, it is possible to conduct the same type of analysis, with some care.","Display_name":"Big Ramy","Creater_id":124958,"Start_date":"2016-07-29 03:21:55","Question_id":225899}
{"_id":{"$oid":"5837a58aa05283111e4d7232"},"Last_activity":"2016-07-29 02:31:40","Creator_reputation":3832,"Question_score":3,"Answer_content":"To do gradient descent, you need continuous parameters, and the loss function has to be differentiable with respect to them. Random forests have discrete hyperparameters (e.g. tree depth, number of trees, number of features, etc.). So, unfortunately, gradient descent won't work in this context. Another point is that the loss function (presumably test set error) may have multiple local minima, and any local search procedure can get trapped.Grid search and random search can be used to explore a broad range of hyperparameter space, and you can hone in on good regions after your initial search. Grid search is popular but, as Simone mentioned, random search can be faster in some cases. This is because some hyperparameters may not strongly affect performance relative to the others. Grid search will waste time exploring different values of the unimportant hyperparameters while holding the more important hyperparameters fixed. Random search updates all hyperparameters on each step, so it has a better chance of hitting the important ones.  Bergstra and Bengio (2012). Random Search for Hyper-Parameter Optimization.","Display_name":"user20160","Creater_id":116440,"Start_date":"2016-07-29 02:31:40","Question_id":226230}
{"_id":{"$oid":"5837a58aa05283111e4d7233"},"Last_activity":"2016-07-29 00:57:55","Creator_reputation":2608,"Question_score":2,"Answer_content":"I remember I attended an amazing presentation on hyperparameter tuning https://speakerdeck.com/tmls/keras-by-keisuke-kamataki-tmls-number-2The speaker also pointed out how surprisingly random sampling of hyperparameters works very well.","Display_name":"Simone","Creater_id":2719,"Start_date":"2016-07-29 00:57:55","Question_id":226230}
{"_id":{"$oid":"5837a58aa05283111e4d7242"},"Last_activity":"2016-07-28 23:39:53","Creator_reputation":152738,"Question_score":2,"Answer_content":"Clearly you're dealing with continuous probabilities on (0,1). There's an infinite number of distributions that fit that. (Truncated Gaussian can work if it does what you need, but it's not as easy to work with as a number of other options.)Possibly the most common choice for such a distribution is the beta distribution.It has two parameters and can have a variety of shapes. It can look close to normal, or it can be right or left skew, it can have a \"U\" shape, and so on:We have: uniform (black)symmetric about 0.5, hill-shaped (green)reasonably right skew (blue)slightly left skew hill-shape (dark red / brownish)U shaped but asymmetric (magenta / pinkish) ... and I'm not even really sure what the best way to describe the red one is.The distribution may have a wide spread (as the above ones all do) or it can be very tightly concentrated about some point.You say you want to have something that can be concentrated near 0.To get a beta distribution to concentrate near zero, the second parameter (beta) should be much larger than the first parameter (alpha), and should generally exceed 1 (or you'll get some concentration near 1 as well). If you want the values near zero to have a peak somewhat above 0, alpha should exceed 1. If you want the peak at zero, alpha should be less than or equal to 1.It's easy to make the densities concentrate even more strongly near 0 if desired.However, you should not restrict yourself to this family if it doesn't match what you think you are dealing with. For example, if you wanted a distribution that had two peaks with at least one not at 0 or 1 then it would not be suitable (but a mixture of beta densities might do well).Without more information I hesitate to leap into suggesting more distributions - if this suggestion doesn't include what you want you'll need to give more details.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-27 19:15:26","Question_id":226015}
{"_id":{"$oid":"5837a58aa05283111e4d7254"},"Last_activity":"2016-07-28 20:44:46","Creator_reputation":338,"Question_score":0,"Answer_content":"Going to try for a less technical explanation.First, start with the dot product between two vectors. This tells you how \"similar\" the vectors are. If the vectors represent points in your data set, the dot product tells you if they are similar or not.But, in some (many) cases, the dot product is not the best metric of similarity. For example:Maybe points that have low dot products are similar for some other reasons.You may have data items that are not well represented as points.So, instead of using the dot product, you use a \"kernel\" which is just a function that takes two points and gives you a measure of their similarity. I'm not 100% sure of what technical conditions a function must meet to technically be a kernel, but this is the idea.One very nice thing is that the kernel can help you put your domain knowledge into the problem in the sense that you can say two points are the same because of xyz reason which comes form you knowing about the domain.","Display_name":"roundsquare","Creater_id":122754,"Start_date":"2016-07-28 20:44:46","Question_id":23386}
{"_id":{"$oid":"5837a58aa05283111e4d7255"},"Last_activity":"2016-07-28 16:47:33","Creator_reputation":8055,"Question_score":1,"Answer_content":"From Williams, Christopher KI, and Carl Edward Rasmussen. \"Gaussian processes for machine learning.\" the MIT Press 2, no. 3 (2006). Page 80.  kernel = a function of two arguments mapping a pair of inputs ,  into .Also, kernel = kernel function.Kernels used in machine learning algorithms typically satisfied more properties, such as being positive semidefinite.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-07-28 16:47:33","Question_id":23386}
{"_id":{"$oid":"5837a58aa05283111e4d7256"},"Last_activity":"2012-02-16 09:39:05","Creator_reputation":2224,"Question_score":9,"Answer_content":"For x,y on S, certain functions K(x,y) can be expressed as an inner product (in usually a different space). K is often referred to as a kernel or a kernel function. The word kernel is used in different ways throughout mathematics, but this is the most common usage in machine learning.The kernel trick is a way of mapping observations from a general set S into an inner product space V (equipped with its natural norm), without ever having to compute the mapping explicitly, in the hope that the observations will gain meaningful linear structure in V. This is important in terms of efficiency (computing dot products in a very high dimensional space very quicky) and practicality (we can convert linear ML algorithms to non-linear ML algorithms).For a function K to be considered a valid kernel it has to satisfy Mercer's conditions. This in practical terms means that we need to ensure the kernel matrix (computing the kernel product of every datapoint you have) will always positive semi-definite. This will ensure that the training objective function is convex, a very important property.","Display_name":"carlosdc","Creater_id":1540,"Start_date":"2012-02-16 09:39:05","Question_id":23386}
{"_id":{"$oid":"5837a58aa05283111e4d7265"},"Last_activity":"2016-07-28 19:59:10","Creator_reputation":8055,"Question_score":0,"Answer_content":"t-SNE has many open source implementations. One of the easiest to use is probably  sklearn.manifold.TSNE. sklearn.manifold contains other manifold learning methods to plot your data to 2D:","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-07-28 19:59:10","Question_id":196}
{"_id":{"$oid":"5837a58aa05283111e4d7266"},"Last_activity":"2014-12-25 22:27:03","Creator_reputation":21,"Question_score":0,"Answer_content":"Look also SCaVis data plotting library. It works on any platform since Java. It supports many data containers and plot styles (2D, 3D etc.)","Display_name":"IraS","Creater_id":64474,"Start_date":"2014-12-25 22:27:03","Question_id":196}
{"_id":{"$oid":"5837a58aa05283111e4d7267"},"Last_activity":"2012-11-20 23:38:02","Creator_reputation":27913,"Question_score":11,"Answer_content":"The lattice package in R.  Lattice is a powerful and elegant high-level data visualization  system, with an emphasis on multivariate data,that is sufﬁcient for  typical graphics needs, and is also ﬂexible enough to handle most  nonstandard requirements.Quick-R has a quick introduction.","Display_name":"Jeromy Anglim","Creater_id":183,"Start_date":"2010-07-19 20:35:58","Question_id":196}
{"_id":{"$oid":"5837a58aa05283111e4d7268"},"Last_activity":"2012-11-08 15:14:04","Creator_reputation":140,"Question_score":3,"Answer_content":"Viewpoints is useful for multi-variate data sets.","Display_name":"mankoff","Creater_id":957,"Start_date":"2010-08-15 10:26:03","Question_id":196}
{"_id":{"$oid":"5837a58aa05283111e4d7269"},"Last_activity":"2010-08-15 23:19:27","Creator_reputation":860,"Question_score":3,"Answer_content":"Python's matplotlib","Display_name":"Andre Holzner","Creater_id":961,"Start_date":"2010-08-15 23:19:27","Question_id":196}
{"_id":{"$oid":"5837a58aa05283111e4d726a"},"Last_activity":"2010-07-20 01:38:38","Creator_reputation":796,"Question_score":4,"Answer_content":"ggobi and the R links to Ggobi are really rather good for this.   There are simpler visualisations (iPlots is very nice, also interactive, as mentioned).But it depends whether you are doing something more specialised.   For example TreeView lets you visualise the kind of cluster dendrograms you get out of microarrays.","Display_name":"user211","Creater_id":211,"Start_date":"2010-07-20 01:38:38","Question_id":196}
{"_id":{"$oid":"5837a58aa05283111e4d726b"},"Last_activity":"2010-07-20 00:26:11","Creator_reputation":428,"Question_score":14,"Answer_content":"Mondrian: Exploratory data analysis with focus on large data and databases.iPlots: a package for the R statistical environment which provides high interaction statistical graphics, written in Java.","Display_name":"rcs","Creater_id":103,"Start_date":"2010-07-19 23:41:25","Question_id":196}
{"_id":{"$oid":"5837a58aa05283111e4d726c"},"Last_activity":"2010-07-19 19:42:01","Creator_reputation":7937,"Question_score":12,"Answer_content":"How about R with ggplot2?Other tools that I really like:ProcessingPrefuse  Protovis","Display_name":"Shane","Creater_id":5,"Start_date":"2010-07-19 19:24:38","Question_id":196}
{"_id":{"$oid":"5837a58aa05283111e4d727b"},"Last_activity":"2016-07-28 18:52:10","Creator_reputation":2865,"Question_score":4,"Answer_content":"Some of my thoughts, may not be correct though.  I understand the reason we have such design (for hinge and logistic loss) is we want the objective function to be convex.Convexity is surely a nice property, but I think the most important reason is we want the objective function to have non-zero derivatives, so that we can make use of the derivatives to solve it. The objective function can be non-convex, in which case we often just stop at some local optima or saddle points.  and interestingly, it also penalize correctly classified instances if  they are weakly classified. It is a really strange design.I think such design sort of advises the model to not only make the right predictions, but also be confident about the predictions. If we don't want correctly classified instances to get punished, we can for example, move the hinge loss (blue) to the left by 1, so that they no longer get any loss. But I believe this often leads to worse result in practice.  what are the prices we need to pay by using different \"proxy loss  functions\", such as hinge loss and logistic loss?IMO by choosing different loss functions we are bringing different assumptions to the model. For example, the logistic regression loss (red) assumes a Bernoulli distribution, the MSE loss (green) assumes a Gaussian noise.Following the least squares vs. logistic regression example in PRML, I added the hinge loss for comparison.As shown in the figure, hinge loss and logistic regression / cross entropy / log-likelihood / softplus have very close results, because their objective functions are close (figure below), while MSE is generally more sensitive to outliers.  Hinge loss does not always have a unique solution because it's not strictly convex.However one important property of hinge loss is, data points far away from the decision boundary contribute nothing to the loss, the solution will be the same with those points removed. The remaining points are called support vectors in the context of SVM. Whereas SVM uses a regularizer term to ensure the maximum margin property and a unique solution.","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-07-27 02:46:30","Question_id":222585}
{"_id":{"$oid":"5837a58aa05283111e4d727c"},"Last_activity":"2016-07-28 07:36:35","Creator_reputation":19151,"Question_score":-1,"Answer_content":"Ideally your loss function should reflect actual loss incurred by business. For instance, if you're classifying damaged goods, then the loss of misclassification could be like this:marking damaged goods that were not: lost profit on potential salenot marking damaged goods that were damaged: cost of return processing","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-07-28 07:36:35","Question_id":222585}
{"_id":{"$oid":"5837a58aa05283111e4d7289"},"Last_activity":"2016-07-28 18:20:27","Creator_reputation":343,"Question_score":1,"Answer_content":"In the paper authours proposed a mathematical procedure for finding informed trader activities. They used a Vector ARMA model and derived a presence criterion to detect an informed trader activitiy.","Display_name":"Nick","Creater_id":111637,"Start_date":"2016-07-28 18:20:27","Question_id":226020}
{"_id":{"$oid":"5837a58aa05283111e4d7294"},"Last_activity":"2016-07-28 17:21:48","Creator_reputation":106,"Question_score":0,"Answer_content":"You can pass weights to SGDRegressor's fit method, although it is suggested for big training samples, n \u003e 10000.I have not used/tested this myself and I do not know the reasoning behind this suggestion, perhaps would be worth looking into for you.One possible reason I can think of comes from using SGDClassifier  for implementing logistic regression with elasticnet penalty, as suggested in Scikit documentation. As discussed here SGDClassifier may have slower convergence compared to better optimized solvers. I have experienced a similar issue with a multinomial logistic model.","Display_name":"Oğuzhan \u0026#214;ğreden","Creater_id":94989,"Start_date":"2016-07-28 17:21:48","Question_id":222435}
{"_id":{"$oid":"5837a58aa05283111e4d72a1"},"Last_activity":"2016-07-28 08:55:35","Creator_reputation":4134,"Question_score":5,"Answer_content":"I think most articles on \"censored variables\" will be related to the response variable which is quite a different story.Being a censored regressor is not automatically a problem. If you are not fully trusting this regressor or if the corresponding \"residuals versus variable\"-plot shows troubles in the two extreme values 21 and 60, then you can still decide to add dummy variables like year60: 1 if 60 or above, 0 otherwiseyear21: 1 if 21 or below, 0 otherwiseto the regression to allow the model to be flexible enough to represent the relationship. Of course, because you don't have values outside the interval from 21 to 60, nothing can be made to recover the information loss. All you can do is trying to choose a flexibly enough regression equation.Let me demonstrate the idea on a simple example with just this one covariable in R# Step 1: Generate and visualize dataset.seed(29)age \u0026lt;- 15:90ageCensored \u0026lt;- pmin(60, pmax(21, age)) # censored at 21 an 60outcome \u0026lt;- 20 + 0.5 * age + 0.03 * (age - 40)^2 + rnorm(length(age))*10plot(outcome ~ ageCensored)# Simple linear regression, ignoring for potential misfit at the endpointsfit \u0026lt;- lm(outcome ~ ageCensored)summary(fit)abline(fit, col = \"red\") # to add the regression line to the scatter plot above# Output            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept) 17.30597    3.99649   4.330 4.61e-05 ***ageCensored  0.60062    0.08176   7.346 2.21e-10 ***[...]Residual standard error: 10.39 on 74 degrees of freedomMultiple R-squared:  0.4217,    Adjusted R-squared:  0.4139 F-statistic: 53.97 on 1 and 74 DF,  p-value: 2.213e-10# Residual versus fitted plot shows considerable misfit which is also directly visible from the scatter plot with the regression lineplot(fit, which = 1)        # Now we can either improve the fit by using a squared age effect (by knowing how the data way generated) or using the dummy \"trick\" mentioned above. Let's try with the dummy trick.fit2 \u0026lt;- lm(outcome ~ ageCensored + I(ageCensored == 21) + I(ageCensored == 60))summary(fit2)plot(fit2, which = 1)# Results                         Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)               31.0754    10.4810   2.965   0.0041 ** ageCensored                0.3242     0.2498   1.298   0.1984    I(ageCensored == 21)TRUE   4.3685     8.4830   0.515   0.6082    I(ageCensored == 60)TRUE  43.7598     6.3583   6.882 1.82e-09 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 16.89 on 72 degrees of freedomMultiple R-squared:  0.6965,    Adjusted R-squared:  0.6838 F-statistic: 55.07 on 3 and 72 DF,  p-value: \u0026lt; 2.2e-16# Residuals versus fitted plot looks better now (although heterogeneity can be spottet at the right endpoint, a problem which I do not account for simplicity)# Plot of the regression function against ageplot(outcome ~ ageCensored, xlim = range(age), xlab = \"age\")lines(age, predict(fit2, list(ageCensored)), col = \"red\")Note that since in your data, you cannot distinguish a 60 year old person with a person older than 60 (i.e. you don't know what value is really censored), you cannot do much more here. If you had this information, you could slighly redefine the dummy variables toyear\u0026gt;60: 1 if above 61, 0 otherwiseyear\u0026lt;21: 1 if below 21, 0 otherwiseto treat persons ages 60 or 21 separately from the censored ones.","Display_name":"Michael M","Creater_id":30351,"Start_date":"2016-07-28 08:55:35","Question_id":226098}
{"_id":{"$oid":"5837a58aa05283111e4d72b0"},"Last_activity":"2016-07-28 14:16:05","Creator_reputation":19151,"Question_score":1,"Answer_content":"Fit a model from seasonal ARIMA family. Estimate the residual for a month of interest, and compare it to the error variance. This will give you an idea of whether the \"dip\" was larger than \"normal\".","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-07-28 14:16:05","Question_id":226183}
{"_id":{"$oid":"5837a58aa05283111e4d72bd"},"Last_activity":"2016-07-28 15:09:14","Creator_reputation":11,"Question_score":0,"Answer_content":"You probably want to do a second partial derivative test. First, you need to find critical points Next, calculate the determinant of the Hessian matrix of . If the determinant is negative for a given critical point, this point is a saddle. Keep in mind though, that this only a sufficient condition for a saddle. When the determinant is 0 the test is inconclusive.","Display_name":"hubi86","Creater_id":88964,"Start_date":"2016-07-28 15:09:14","Question_id":226185}
{"_id":{"$oid":"5837a58aa05283111e4d72ce"},"Last_activity":"2016-07-28 13:42:30","Creator_reputation":5445,"Question_score":3,"Answer_content":"The results are not inconsistent. The models are answering different questions. lm fits a linear model, so the estimate for QM is the overall (linear) slope, taking no account of clustering - ie, that observations within each subject may be more alike one another. On the other hand, lme fits a linear mixed effects model, in particular a random intercepts model in your case, where the QM estimate is an \"average\" of the slopes for QM within each subject. So if the association at the group level is markedly different to the overall association, you will get different results, which is an example of Simpson's Paradox (the grouping variable confounds the association).A simple simulation shows this:We create 3 groups of data, all with a negative slope, and plot them, along with regression lines for each oneset.seed(123)n \u0026lt;- 20X1 \u0026lt;- rnorm(n ,10,2)Y1 \u0026lt;- 10-0.2*X1+rnorm(n ,0,1)X2 \u0026lt;- rnorm(n ,20,2)Y2 \u0026lt;- 20-0.2*X2+rnorm(n ,0,1)X3 \u0026lt;- rnorm(n ,30,2)Y3 \u0026lt;- 30-0.2*X3+rnorm(n ,0,1)xlimits \u0026lt;- c(min(X1,X2,X3),max(X1,X2,X3))ylimits \u0026lt;- c(min(Y1,Y2,Y3),max(Y1,Y2,Y3))plot(X1,Y1,xlim=xlimits, ylim=ylimits,col=\"red\",ylab=\"Y\",xlab=\"X\")points(X2,Y2,col=\"blue\")points(X3,Y3, col=\"green\")abline(m1,col=\"red\")abline(m2,col=\"blue\")abline(m3, col=\"green\")It is clear from the figure that if we aggregate the data, the overall regression line slope will be positive, as we can easily demonstrate:\u0026gt; dt \u0026lt;- data.frame(Y=c(Y1,Y2,Y3),X=c(X1,X2,X3))\u0026gt; dt$grp \u0026lt;- as.factor(c(rep(1,n),rep(2,n),rep(3,n)))=\u0026gt; summary(m0 \u0026lt;- lm(Y~X,data=dt))lm(formula = Y ~ X, data = dt)Residuals:    Min      1Q  Median      3Q     Max -5.0922 -1.1382  0.0353  1.5890  3.2975 Coefficients:            Estimate Std. Error t value Pr(\u0026gt;|t|)    (Intercept)  1.06688    0.64787   1.647    0.105    X            0.71873    0.02927  24.554   \u0026lt;2e-16 ***abline(m0,col=\"black\")But if we fit a mixed model, we get the average slope, which is obviously negative (along with a random intercepts estimate):Linear mixed-effects model fit by REML Data: dt        AIC      BIC    logLik  178.8397 187.0815 -85.41987Random effects: Formula: ~1 | grp        (Intercept)  ResidualStdDev:    9.906642 0.8493246Fixed effects: Y ~ X                 Value Std.Error DF   t-value p-value(Intercept) 19.908660  5.853642 56  3.401072  0.0012X           -0.204189  0.060771 56 -3.359980  0.0014","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-28 10:32:56","Question_id":226001}
{"_id":{"$oid":"5837a58aa05283111e4d72cf"},"Last_activity":"2016-07-28 09:38:13","Creator_reputation":11445,"Question_score":3,"Answer_content":"This is similar to the difference between unpaired and paired t-tests. The lm calls throw way all information about the individuals. Any systematic differences among individuals in terms of  values are just lumped together into the error term, leading to large residual errors in the models similar to using unpaired t-tests on paired observations. The lme calls keep the information about the individuals and take advantage of within-individual relations between responses; when you write random=~1|subject you allow each subject to have its own intercept (value of  when QM=0) beyond which the (common) slope of the ~QM relation acts. Your lme calls are thus similar in concept to paired t-tests. As the lme calls correct for systematic differences among subjects in terms of intercepts, they provide greater power than the lm calls for detecting a true non-zero slope of the ~QM relation, as your results demonstrate.","Display_name":"EdM","Creater_id":28500,"Start_date":"2016-07-27 18:17:17","Question_id":226001}
{"_id":{"$oid":"5837a58aa05283111e4d72dc"},"Last_activity":"2016-06-28 14:36:57","Creator_reputation":2186,"Question_score":2,"Answer_content":"I'd like to highlight two possible options for multiclass performance metrics under class imbalance: Kohen's Kappa (see here for details in scikit.learn), and Computing one ROC curve and area under the curve (AUC) per individual class (see here for details in scikit.learn).For the latter: as you have  classes, and ROC/AUC are conceptually designed for 2-class-problems, you will likely need to calculate one ROC curve and AUC value per individual class. This could be done e.g. in a \"1-vs-all\" manner, where you test for each class how much it is confused with other classes. The thereby obtained  metrics can be used to e.g. look at the distribution of AUC values (e.g. boxplots or similar) to compare and select a best suited model from multiple models. If this process needs to be done fully automated, consider computing the mean/median and sd/mad of AUC over all classes (the first indicates the \"average\" performance over classes, the latter the performance spread). By doing this for all models you obtain scalar values which you could use to select a model suited for your problem.","Display_name":"geekoverdose","Creater_id":112731,"Start_date":"2016-06-28 14:36:57","Question_id":221039}
{"_id":{"$oid":"5837a58aa05283111e4d72e8"},"Last_activity":"2016-07-28 13:23:50","Creator_reputation":133,"Question_score":1,"Answer_content":"I think the way I would approach this (given your specific example) is to describe my results as follows:When y is 1 and z is 0, the odds ratio (including interaction) is exp(-0.2231)When y is 0 and z is 1, the odds ratio (including interaction) is exp(-0.9136)When y = z = 1, the odds ratio (including interation) is exp(-0.2231-0.9136+17.0961)I guess the question is how do you want to interpret the model and this is how I would choose to interpret such a model.","Display_name":"godspeed","Creater_id":103245,"Start_date":"2016-07-28 13:23:50","Question_id":226167}
{"_id":{"$oid":"5837a58ca05283111e4d73e8"},"Last_activity":"2016-07-27 14:03:40","Creator_reputation":56,"Question_score":0,"Answer_content":"Nevermind, I think I figured it out:where L is the number of binomals there are.","Display_name":"K23","Creater_id":69505,"Start_date":"2016-07-26 16:04:16","Question_id":225785}
{"_id":{"$oid":"5837a58ca05283111e4d73f7"},"Last_activity":"2016-07-27 13:56:46","Creator_reputation":3734,"Question_score":1,"Answer_content":"I think the most powerful approach would be to do a linear regression using final score as the outcome, baseline score as a covariate and number of sessions as a categorical covariate. You could also then add in some of the other variables which you have as potential confounders. Having said that there would not be anything wrong with your suggestion of using KW on the gain scores, it would just not be quite so powerful.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-07-27 13:56:46","Question_id":225985}
{"_id":{"$oid":"5837a58ca05283111e4d7406"},"Last_activity":"2016-07-27 13:48:43","Creator_reputation":147868,"Question_score":1,"Answer_content":"I will illustrate with the example in the question, because a general answer is too complicated to write down.Let  be the common distribution function.  We will need the distributions of the order statistics .  Their distribution functions  are easy to express in terms of  and its distribution function  because, heuristically, the chance that  lies within an infinitesimal interval  is given by the trinomial distribution with probabilities , , and ,\\eqalign{f_{[k]}(x)dx \u0026amp;= \\Pr(x_{[k]} \\in (x, x+dx]) \\\\\u0026amp;= \\binom{n}{k-1,1,n-k} F(x)^{k-1} (1-F(x+dx))^{n-k} f(x)dx\\\\\u0026amp;= \\frac{n!}{(k-1)!(1)!(n-k)!} F(x)^{k-1} (1-F(x))^{n-k} f(x)dx.}Because the  are iid, they are exchangeable: every possible ordering  of the  indices has equal probability.  will correspond to some order statistic, but which order statistic depends on .  Therefore let  be the value of  for which\\eqalign{x_{[k]} = X = \\max\u0026amp;\\left(  \\min(x_{\\sigma(1)},x_{\\sigma(2)},x_{\\sigma(3)}),\\min(x_{\\sigma(1)},x_{\\sigma(4)},x_{\\sigma(5)}), \\right. \\\\\u0026amp; \\left. \\min(x_{\\sigma(5)},x_{\\sigma(6)},x_{\\sigma(7)}),\\min(x_{\\sigma(3)},x_{\\sigma(6)},x_{\\sigma(8)})\\right).}The distribution of  is a mixture over all the values of .  To write this down, let  be the number of reorderings  for which , whence  is the chance that .  Thus the density function of  is\\eqalign{g(x) \u0026amp;= \\frac{1}{n!} \\sum_{\\sigma \\in \\mathfrak{S}_n} f_{k(\\sigma)}(x) \\\\\u0026amp;= \\frac{1}{n!}\\sum_{k=1}^n p(k)\\binom{n}{k-1,1,n-k} F(x)^{k-1} (1-F(x))^{n-k} f(x) \\\\\u0026amp;=\\left(\\sum_{k=1}^n \\frac{p(k)}{(k-1)!(n-k)!}F(x)^{k-1} (1-F(x))^{n-k} \\right)f(x) .}I do not know of any general way to find the .  In this example, exhaustive enumeration gives\\begin{array}{l|rrrrrrrrr}k \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp; 7 \u0026amp; 8 \u0026amp; 9\\\\\\hlinep(k) \u0026amp; 0 \u0026amp; 20160 \u0026amp; 74880 \u0026amp; 106560 \u0026amp; 92160 \u0026amp; 51840 \u0026amp; 17280 \u0026amp; 0 \u0026amp; 0\\end{array}The figure shows a histogram of  simulated values of  where  is an Exponential distribution.  On it is superimposed in red the graph of .  It fits beautifully.The R code that produced this simulation follows.set.seed(17)n.sim \u0026lt;- 1e4n \u0026lt;- 9x \u0026lt;- matrix(rexp(n.sim*n), n)X \u0026lt;- pmax(pmin(x[1,], x[2,], x[3,]),          pmin(x[1,], x[4,], x[5,]),          pmin(x[5,], x[6,], x[7,]),          pmin(x[3,], x[6,], x[8,]))f \u0026lt;- function(x, p) {  n \u0026lt;- length(p)  y \u0026lt;- outer(1:n, x, function(k, x) {    pexp(x)^(k-1) * pexp(x, lower.tail=FALSE)^(n-k) * dexp(x) * p[k] /      (factorial(k-1) * factorial(n-k))  })  colSums(y)}hist(X, freq=FALSE)curve(f(x, p), add=TRUE, lwd=2, col=\"Red\")","Display_name":"whuber","Creater_id":919,"Start_date":"2016-07-27 13:48:43","Question_id":225867}
{"_id":{"$oid":"5837a58ca05283111e4d7417"},"Last_activity":"2016-07-27 13:30:40","Creator_reputation":133,"Question_score":1,"Answer_content":"As far as I can tell, significance testing for coefficients has not been introduced in most LASSO implementations. So could a difference not be that while we can determine statistically significant variables in OLS, we cannot do so with LASSO except making a weaker statement that the LASSO coefficients of corresponding variables selected are the \"important\" variables to consider?","Display_name":"godspeed","Creater_id":103245,"Start_date":"2016-07-27 13:30:40","Question_id":225970}
{"_id":{"$oid":"5837a58ca05283111e4d7418"},"Last_activity":"2016-07-27 12:28:47","Creator_reputation":13106,"Question_score":4,"Answer_content":"  Are the LASSO coefficients interpreted in the same method as logistic regression?Let me rephrase: Are the LASSO coefficients interpreted in the same way as, for example, OLS coefficients in a logistic regression?LASSO (a penalized estimation method) aims at estimating the same quantities (model coefficients) as, say, OLS (an unpenalized method). The model is the same, and the interpretation remains the same. The numerical values from LASSO will normally differ from those from OLS: some will be closer to zero, others will be exactly zero. If a sensible amount of penalization has been applied, the LASSO estimates will lie closer to the true values than the OLS estimates, which is a desirable result.  Would it be appropriate to use the features selected from LASSO in logistic regression?There is no inherent problem with that, but you could use LASSO not only for feature selection but also for coefficient estimation. As I mention above, LASSO estimates may be more accurate than, say, OLS estimates.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-27 12:28:47","Question_id":225970}
{"_id":{"$oid":"5837a58ca05283111e4d7429"},"Last_activity":"2016-07-27 12:52:45","Creator_reputation":1486,"Question_score":0,"Answer_content":"Whenever you add a new variable in a regression analysis, all coefficients may change. They will always change unless the added variable is uncorrelated to all the previous variable. The extreme case where two or more variables are highly correlated is multicollinearity.And about your question of switching signs of one of the variables, it will just change the signs of some of your coefficients. In fact, applying any linear transformation to any variable won't change anything; coefficients will just get transformed in the same way to keep yielding the same predictions for the same situation.","Display_name":"Pere","Creater_id":123561,"Start_date":"2016-07-27 12:52:45","Question_id":225965}
{"_id":{"$oid":"5837a58ca05283111e4d7436"},"Last_activity":"2016-07-27 08:39:40","Creator_reputation":8367,"Question_score":1,"Answer_content":"De Finetti's representation theorem, which is important in mathematical statistics, shows that something like this is true for exchangeable random variables. It says that for any sequence of exchangeable random variables, there exists another random variable conditional on which the former are IID.But now I'll attack your problem more directly. Let's formalize it like this:Conjecture: Let  be a probability space. If  are Bernoulli random variables on , then there exist independent Bernoulli random variables  on  and a function  such that for all , .This statement is false. For a counterexample, consider the case when:, , , , Notice that  and  are dependent, sinceP(X_1 = 0, X_2 = 0) = \\tfrac{1}{4} ≠ \\tfrac{3}{8} = \\tfrac{1}{2} \\cdot \\tfrac{3}{4} = P(X_1 = 0)P(X_2 = 0).Obviously,  will need to be greater than 1. However, on this probability space, there is no pair of independent Bernoulli random variables. There are only 8 distinct Bernoulli random variables (because ), and all  pairs of these are dependent. The following Python program proves this by checking every pair.import numpy as npfrom itertools import combinationsprobs = np.array([2, 1, 1])  # Multiplied by 4 so we can do integer arithmetic.ys = [np.array([a, b, c])    for a in (False, True) for b in (False, True) for c in (False, True)]for y1, y2 in combinations(ys, 2):    def f(a, b):        v1 = y1 if a else ~y1        v2 = y2 if b else ~y2        return np.sum(probs[v1 \u0026amp; v2]) == np.sum(probs[v1]) * np.sum(probs[v2])    if f(0, 0) and f(0, 1) and f(1, 0) and f(1, 1):        print \"Independent\"    else:        print \"Dependent\"","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-26 23:00:08","Question_id":225815}
{"_id":{"$oid":"5837a58ca05283111e4d7443"},"Last_activity":"2016-07-27 12:44:21","Creator_reputation":11,"Question_score":1,"Answer_content":"As a pragmatic approach to handle dual character of an ordinal variable,(categorical and continuous) is a calibration. Otherwise. it will be complex to work with a set independent standards due to levels from an ordinal variable.So the continuous dependent variable needs to be re-scaled to one chosen level from the ordinal variable. After calibrate it from all levels to one chosen level, there is a re-scaled set as if the dependent dataset is based at one level. The categorical character of the ordinal variable can be disregard. Then a regression is possible with the re-scaled set with  \"ordinal\" variable as continuous variable. Any conclusion from regression can be translated back by \"un-scaling\".","Display_name":"SRRussel","Creater_id":124766,"Start_date":"2016-07-27 12:44:21","Question_id":193257}
{"_id":{"$oid":"5837a58ca05283111e4d7452"},"Last_activity":"2016-07-27 12:37:56","Creator_reputation":12290,"Question_score":1,"Answer_content":"If you're using rstan or have access to R, I'd suggest loading the brms package. Then create your model something like:mod \u0026lt;- brm (tgt ~ foo + bar + baz, family=\"acat\")stancode (mod)And you'll see what brms does to implement such a model. Note that acat is adjacent categories, and you may want one of the other ordinal model types (cumulative, sratio, or cratio).brms-generated Stan is fairly straightforward and readable. Or maybe you just want a Stan-based model but don't actually want to learn Stan itself, in which case brms or rstanarm would be very helpful. Of course, this may be too far afield from your original question. Just a suggestion.","Display_name":"Wayne","Creater_id":1764,"Start_date":"2016-07-27 12:37:56","Question_id":225963}
{"_id":{"$oid":"5837a58ca05283111e4d7461"},"Last_activity":"2016-07-27 12:13:37","Creator_reputation":8065,"Question_score":4,"Answer_content":"  In the K-Fold method, do we still hold out a test set for the very end, and only use the remaining data for training and hyperparameter tuning (ie. we split the remaining data into k folds, and then use the average accuracy after training with each fold (or whatever performance metric we choose) to tune our hyperparameters)?Yes. As a rule, the test set should never be used to change your model (e.g., its hyperparameters).However, cross-validation can sometimes be used for purposes other than hyperparameter tuning, e.g. determining to what extent the train/test split impacts the results.","Display_name":"Franck Dernoncourt","Creater_id":12359,"Start_date":"2016-07-27 10:53:56","Question_id":225949}
{"_id":{"$oid":"5837a58ca05283111e4d746e"},"Last_activity":"2016-07-27 12:14:12","Creator_reputation":13106,"Question_score":1,"Answer_content":"  According to bias-variance tradeoff concept, does training on a larger set compared to a smaller sample add more variance and reduces bias from the model, and does this become a factor to consider in my original question?Citing Hastie et al. (2001) section 7.3 \"The Bias-Variance Decomposition\" (as of 2nd edition, 2009), if we assume  Y = f(X) + \\varepsilon  with  and , the expected prediction error of a regression fit  at an input point  using squared error loss will be\\begin{aligned}\\text{Err}(x_0) \u0026amp;= \\mathbb{E}[((Y-\\hat f(x_0))^2|X=x_0] \\\\                 \u0026amp;= [\\mathbb{E}\\hat f(x_0)-f(x_0)]^2 + \\mathbb{E}[\\hat f(x_0)-\\mathbb{E}\\hat f(x_0)]^2 + \\sigma_{\\varepsilon}^2 \\\\                \u0026amp;= \\text{Bias}^2(\\hat f(x_0)) + \\text{Var}(\\hat f(x_0)) + \\sigma_{\\varepsilon}^2 \\\\                \u0026amp;= \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}.\\end{aligned}For a given model (a functional form), changing the sample size will only affect ; namely, increasing the sample will diminish it. Meanwhile,  will stay the same as the functional form  is fixed. (Clearly, the irreducible error also stays the same.)So in your original question, you reduce the expected squared error by reestimating the chosen model on the full sample as compared to having estimated it on just the training sample.References:Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning. Vol. 1. Springer, Berlin: Springer series in statistics, 2001.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-27 12:14:12","Question_id":225820}
{"_id":{"$oid":"5837a58ca05283111e4d746f"},"Last_activity":"2016-07-26 22:37:08","Creator_reputation":8367,"Question_score":1,"Answer_content":"Yes, you should do that. It will be a lot faster than the cross-validation, anyway, since you're running the modeling procedure only once instead of k times.When you do cross-validation, recall, you aren't looking at the model with just one training set; you're trying out a bunch of slightly different training sets. Refitting the model to all your data is similar in this way, and the resulting fitted model differs only from the one you created for each fold in that it has a little more training data, and hence, in all likelihood, it is a little more accurate.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-26 22:37:08","Question_id":225820}
{"_id":{"$oid":"5837a58ca05283111e4d747c"},"Last_activity":"2016-07-27 11:59:22","Creator_reputation":31,"Question_score":3,"Answer_content":"Yes, see Theorem 6.21 of [LT13], Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes, volume 23. Springer Science \u0026amp; Business Media, 2013.For simplicity you may also look at section 8 of my paper. http://arxiv.org/pdf/1507.06370v2.pdf(I just summarized those theorems -- the purpose of the paper is completely different)","Display_name":"Alex Wenxin Xu","Creater_id":124757,"Start_date":"2016-07-27 11:59:22","Question_id":182549}
{"_id":{"$oid":"5837a58ca05283111e4d7489"},"Last_activity":"2016-07-27 11:51:25","Creator_reputation":66,"Question_score":1,"Answer_content":"If you are using software like SPSS or Minitab, then specify that you want to do a quadratic regression and leave the X values as they are. The software can give the X and X^2 terms negative coefficients to fit the curve through negative Y values.","Display_name":"Groovy_Worm","Creater_id":124314,"Start_date":"2016-07-27 11:51:25","Question_id":225966}
{"_id":{"$oid":"5837a58ca05283111e4d748a"},"Last_activity":"2016-07-27 11:49:27","Creator_reputation":1486,"Question_score":0,"Answer_content":"When you introduce a quadratic term in a regression, you are just adjusting a parabola to your data, and a parabola can take positive and negative values depending on the coeficients. Then, there is no need to change signs of your data or their squares.","Display_name":"Pere","Creater_id":123561,"Start_date":"2016-07-27 11:49:27","Question_id":225966}
{"_id":{"$oid":"5837a58ca05283111e4d7497"},"Last_activity":"2016-07-27 11:46:24","Creator_reputation":103,"Question_score":0,"Answer_content":"Variance is the degree by which a random vairable changes with respect to its expected value Owing to the stochastic nature of be underlying process the random variable represents.Covariance is the degree by which two different random variables change with respect to each other. This could happen when random variables are driven by the same underlying process, or derivatives thereof. Either processes represented by these random variables are affecting each other, or it's the same process but one of the random variables is derived from the other.","Display_name":"Kingz","Creater_id":25136,"Start_date":"2016-07-27 11:46:24","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d7498"},"Last_activity":"2016-01-25 07:32:46","Creator_reputation":147868,"Question_score":203,"Answer_content":"Sometimes we can \"augment knowledge\" with an unusual or different approach.  I would like this reply to be accessible to kindergartners and also have some fun, so everybody get out your crayons!Given paired  data, draw their scatterplot.  (The younger students may need a teacher to produce this for them. :-)  Each pair of points ,  in that plot determines a rectangle: it's the smallest rectangle, whose sides are parallel to the axes, containing those points.  Thus the points are either at the upper right and lower left corners (a \"positive\" relationship) or they are at the upper left and lower right corners (a \"negative\" relationship).Draw all possible such rectangles. Color them transparently, making the positive rectangles red (say) and the negative rectangles \"anti-red\" (blue).  In this fashion, wherever rectangles overlap, their colors are either enhanced when they are the same (blue and blue or red and red) or cancel out when they are different.(In this illustration of a positive (red) and negative (blue) rectangle, the overlap ought to be white; unfortunately, this software does not have a true \"anti-red\" color.   The overlap is gray, so it will darken the plot, but on the whole the net amount of red is correct.)Now we're ready for the explanation of covariance.The covariance is the net amount of red in the plot (treating blue as negative values).Here are some examples with 32 binormal points drawn from distributions with the given covariances, ordered from most negative (bluest) to most positive (reddest).Let's deduce some properties of covariance.  Understanding of these properties will be accessible to anyone who has actually drawn a few of the rectangles. :-)Bilinearity. Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.Correlation. Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.Relationship to linear associations. Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances.  Linear associations can be fully interpreted by means of the preceding two characterizations.Sensitivity to outliers. A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points.  It alone can create a net positive or negative amount of red in the overall picture.Incidentally, this definition of covariance differs from the usual one only by a universal constant of proportionality (independent of the data set size).  The mathematically inclined will have no trouble performing the algebraic demonstration of the equivalence.","Display_name":"whuber","Creater_id":919,"Start_date":"2011-11-10 10:14:03","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d7499"},"Last_activity":"2015-08-16 19:27:43","Creator_reputation":183,"Question_score":5,"Answer_content":"Here's another attempt to explain covariance with a picture.  Every panel in the picture below contains 50 points simulated from a bivariate distribution with correlation between x \u0026amp; y of 0.8 and variances as shown in the row and column labels.  The covariance is shown in the lower-right corner of each panel.Anyone interested in improving this...here's the R code:library(mvtnorm)rowvars \u0026lt;- colvars \u0026lt;- c(10,20,30,40,50)all \u0026lt;- NULLfor(i in 1:length(colvars)){  colvar \u0026lt;- colvars[i]  for(j in 1:length(rowvars)){    set.seed(303)  # Put seed here to show same data in each panel    rowvar \u0026lt;- rowvars[j]    # Simulate 50 points, corr=0.8    sig \u0026lt;- matrix(c(rowvar, .8*sqrt(rowvar)*sqrt(colvar), .8*sqrt(rowvar)*sqrt(colvar), colvar), nrow=2)    yy \u0026lt;- rmvnorm(50, mean=c(0,0), sig)    dati \u0026lt;- data.frame(i=i, j=j, colvar=colvar, rowvar=rowvar, covar=.8*sqrt(rowvar)*sqrt(colvar), yy)    all \u0026lt;- rbind(all, dati)  }}names(all) \u0026lt;- c('i','j','colvar','rowvar','covar','x','y')all \u0026lt;- transform(all, colvar=factor(colvar), rowvar=factor(rowvar))library(latticeExtra)useOuterStrips(xyplot(y~x|colvar*rowvar, all, cov=all$covar,                      panel=function(x,y,subscripts, cov,...){                        panel.xyplot(x,y,...)                        print(cor(x,y))                        ltext(14,-12, round(cov[subscripts][1],0))                      }))","Display_name":"Kevin Wright","Creater_id":29238,"Start_date":"2015-08-16 13:39:07","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d749a"},"Last_activity":"2014-09-15 10:53:39","Creator_reputation":109,"Question_score":8,"Answer_content":"I really like Whuber's answer, so I gathered some more resources. Covariance describes both how far the variables are spread out, and the nature of their relationship. Covariance uses rectangles to describe how far away an observation is from the mean on a scatter graph:If a rectangle has long sides and a high width or short sides and a short width, it provides evidence that the two variables move together.If a rectangle has two sides that are relatively long for that variables, and two sides that are relatively short for the other variable, this observation provides evidence the variables do not move together very well.If the rectangle is in the 2nd or 4th quadrant, then when one variable is greater than the mean, the other is less than the mean. An increase in one variable is associated with a decrease in the other.I found a cool visualization of this at http://sciguides.com/guides/covariance/, It explains what covariance is if you just know the mean.","Display_name":"arthur.00","Creater_id":41596,"Start_date":"2014-08-09 10:36:07","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d749b"},"Last_activity":"2012-05-06 16:56:40","Creator_reputation":25915,"Question_score":0,"Answer_content":"I would simply explain correlation which is pretty intuitive.  I would say \"Correlation measures the strength of relationship between two variables X and Y.  Correlation is between -1 and 1 and will be close to 1 in absolute value when the relationship is strong.  Covariance is just the correlation multiplied by the standard deviations of the two variables.  So while correlation is dimensionless, covariance is in the product of the units for variable X and variable Y.","Display_name":"Michael Chernick","Creater_id":11032,"Start_date":"2012-05-06 16:56:40","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d749c"},"Last_activity":"2011-11-10 03:50:01","Creator_reputation":37894,"Question_score":36,"Answer_content":"To elaborate on my comment, I used to teach the covariance as a measure of the (average) co-variation between two variables, say  and . It is useful to recall the basic formula (simple to explain, no need to talk about mathematical expectancies for an introductory course):\r\\text{cov}(x,y)=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)\rso that we clearly see that each observation, , might contribute positively or negatively to the covariance, depending on the product of their deviation from the mean of the two variables,  and . Note that I do not speak of magnitude here, but simply of the sign of the contribution of the ith observation.This is what I've depicted in the following diagrams. Artificial data were generated using a linear model (left, ; right, , where  were drawn from a gaussian distribution with zero mean and , and  from an uniform distribution on the interval ). The vertical and horizontal bars represent the mean of  and , respectively. That mean that instead of \"looking at individual observations\" from the origin , we can do it from . This just amounts to a translation on the x- and y-axis. In this new coordinate system, every observation that is located in the upper-right or lower-left quadrant contributes positively to the covariance, whereas observations located in the two other quadrants contribute negatively to it. In the first case (left), the covariance equals 30.11 and the distribution in the four quadrants is given below:   +  -+ 30  2-  0 28Clearly, when the 's are above their mean, so do the corresponding 's (wrt. ). Eye-balling the shape of the 2D cloud of points, when  values increase  values tend to increase too. (But remember we could also use the fact that there is a clear relationship between the covariance and the slope of the regression line, i.e. .)In the second case (right, same ), the covariance equals 3.54 and the distribution across quadrants is more \"homogeneous\" as shown below:   +  -+ 18 14- 12 16In other words, there is an increased number of case where the 's and 's do not covary in the same direction wrt. their means.Note that we could reduce the covariance by scaling either  or . In the left panel, the covariance of  (or ) is reduced by a ten fold amount (3.01). Since the units of measurement and the spread of  and  (relative to their means) make it difficult to interpret the value of the covariance in absolute terms, we generally scale both variables by their standard deviations and get the correlation coefficient. This means that in addition to re-centering our  scatterplot to  we also scale the x- and y-unit in terms of standard deviation, which leads to a more interpretable measure of the linear covariation between  and .","Display_name":"chl","Creater_id":930,"Start_date":"2011-11-10 03:50:01","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d749d"},"Last_activity":"2011-11-09 02:28:56","Creator_reputation":555,"Question_score":0,"Answer_content":"Two variables that would have a high positive covariance (correlation) would be the number of people in a room, and the number of fingers that are in the room.  (As the number of people increases, we expect the number of fingers to increase as well.)  Something that might have a negative covariance (correlation) would be a person's age, and the number of hair follicles on their head. Or, the number of zits on a person's face (in a certain age group), and how many dates they have in a week.  We expect people with more years to have less hair, and people with more acne to have less dates.. These are negatively correlated.","Display_name":"Adam","Creater_id":6369,"Start_date":"2011-11-09 02:28:56","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d749e"},"Last_activity":"2011-11-07 19:23:19","Creator_reputation":3705,"Question_score":6,"Answer_content":"I am answering my own question, but I thought It'd be great for the people coming across this post to check out some of the explanations on this page.I'm paraphrasing one of the very well articulated answers (by a user'Zhop'). I'm doing so in case if that site shuts down or the page gets taken down when someone eons from now accesses this post ;)  Covariance is a measure of how much two variables change together.  Compare this to Variance, which is just the range over which one  measure (or variable) varies.    In studying social patterns, you might hypothesize that wealthier  people are likely to be more educated, so you'd try to see how closely  measures of wealth and education stay together.  You would use a  measure of covariance to determine this.    ...    I'm not sure what you mean when you ask how does it apply to  statistics. It is one measure taught in many stats classes.  Did you  mean, when should you use it?    You use it when you want to see how much two or more variables change  in relation to each other.    Think of people on a team.  Look at how they vary in geographic  location compared to each other.  When the team is playing or  practicing, the distance between individual members is very small and  we would say they are in the same location.  And when their location  changes, it changes for all individuals together (say, travelling on a  bus to a game). In this situation, we would say they have a high level  of covariance.  But when they aren't playing, then the covariance rate  is likely to be pretty low, because they are all going to different  places at different rates of speed.    So you can predict one team member's location, based on another team  member's location when they are practicing or playing a game with a  high degree of accuracy.  The covariance measurement would be close to  1, I believe.  But when they are not practicing or playing, you would  have a much smaller chance of predicting one person's location, based  on a team member's location.   It would be close to zero, probably,  although not zero, since sometimes team members will be friends, and  might go places together on their own time.    However, if you randomly selected individuals in the United States,  and tried to use one of them to predict the other's locations, you'd  probably find the covariance was zero.  In other words, there is  absolutely no relation between one randomly selected person's location  in the US, and another's.Adding another one (by 'CatofGrey') that helps augment the intuition:  In probability theory and statistics, covariance is the measure of how  much two random variables vary together (as distinct from variance,  which measures how much a single variable varies).    If two variables tend to vary together (that is, when one of them is  above its expected value, then the other variable tends to be above  its expected value too), then the covariance between the two variables  will be positive. On the other hand, if one of them is above its  expected value and the other variable tends to be below its expected  value, then the covariance between the two variables will be negative.These two together have made me understand covariance as I've never understood it before! Simply amazing!!","Display_name":"PhD","Creater_id":4426,"Start_date":"2011-11-07 19:23:19","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d749f"},"Last_activity":"2011-11-07 16:33:15","Creator_reputation":57792,"Question_score":7,"Answer_content":"Covariance is a measure of how much one variable goes up when the other goes up.","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2011-11-07 16:33:15","Question_id":18058}
{"_id":{"$oid":"5837a58ca05283111e4d74ac"},"Last_activity":"2016-07-27 01:53:35","Creator_reputation":431,"Question_score":3,"Answer_content":"The R package pdc offers clustering for multivariate time series. Permutation Distribution Clustering is a complexity-based dissimilarity measure for time series. If you can assume that differences in time series are due to differences w.r.t. complexity and, specifically not due to differences in means, variances, or the moments in general, this may be a valid approach. The algorithmic time complexity of calculating the pdc representation of a multivariate time series is in O(DTN) with D being the number of dimensions, T being the length of the time series and N being the number of time series. This is probably as efficient as it gets since a single sweep over each dimension of each time series is enough to obtain the compressed complexity representation. This representation can be used to calculate dissimilarity between two time series at low cost (depending on the chosen representational complexity which can either be pre-specified or derived from the data). Here is a simple worked example with a hierarchical clustering of multivariate white-noise time series (the plot illustrates only the first dimension of each time series):require(\"pdc\")num.ts \u0026lt;- 20 # number of time seriesnum.dim \u0026lt;- 12 # number of dimensionslen.ts \u0026lt;- 600*10 # number of time series# generate Gaussian white noisedata \u0026lt;- array(dim = c(len.ts, num.ts, num.dim),data = rnorm(num.ts*num.dim*len.ts))# obtain clustering with embedding dimension of 5pdc \u0026lt;- pdclust(X = data, m=5,t=1)# plot hierarchical clusteringplot(pdc)The command pdcDist(data) generates a dissimilarity matrix:Since the data are all white noise, there is no apparent structure in the dissimilarity matrix.         1        2        3        4        5        6        72 4.832894                                                      3 4.810718 4.790286                                             4 4.812738 4.796530 4.809482                                    5 4.798458 4.772756 4.751079 4.786206                           6 4.812076 4.793027 4.798996 4.758193 4.751691                  7 4.786515 4.771505 4.754735 4.837236 4.775775 4.794706         8 4.808709 4.832403 4.722993 4.781267 4.784397 4.776600 4.787757For more information refer to:Brandmaier, A. M. (2015). pdc: An R package for complexity-based clustering of time series. Journal of Statistical Software, 67. doi:10.18637/jss.v067.i05(Full text)","Display_name":"Brandmaier","Creater_id":20352,"Start_date":"2016-07-27 01:53:35","Question_id":225822}
{"_id":{"$oid":"5837a58ca05283111e4d74b9"},"Last_activity":"2016-07-27 11:34:58","Creator_reputation":358,"Question_score":1,"Answer_content":"Firstly: My calculations yield a required sample size of  (see below for more details). Secondly: In this context, the concept of power is not needed at all. Power is only needed in the context of statistical testing, where you need to know the distribution of your test statistic under . In this context, you are simply interested in \"sharpening\" your inference made from a sample and don't actually test any hypothesis. The result that you will get can be interpreted as: \"If I gather a sample of size 239, I will be able to conclude with at least 95% probability that the true prevalence of HIV testing amoung homosexuals is within 19.2%  5%.\" (This holds only if your study also finds a prevalence of 19.2% - see below for more details)Without going too much into the details (I am sure this is a topic well covered online), we obtain a CI with 1- probability of covering the proportion  using: . If we now decide on a desired \"precision\" (as you called it) , which will reflect half of the width of the obtained CI, we can calculate the required sample size with respect to the chosen  and  as:. refers to the  quantile of the  distribution.In reality,  might deviate from the estimated proportion in previous studies (let me call that ). If this is the case, you might not be able to achieve your desired \"precision\" (In fact you will not be able to achieve your desired precision, if ) and you will be even more precise if . Now, using the above formula and ,  and , I obtain a required sample size of .","Display_name":"E L M","Creater_id":124036,"Start_date":"2016-07-26 02:31:36","Question_id":225357}
{"_id":{"$oid":"5837a58ca05283111e4d74c6"},"Last_activity":"2016-07-27 11:34:38","Creator_reputation":275,"Question_score":7,"Answer_content":"You're seeing the effect of R choosing different bandwidths for the grouped and ungrouped plots. A smaller bandwidth will result in greater resolution (along the horizontal axis) of peaks and valleys in the data, while a larger bandwidth will smear them out. You can see this if you set the bandwidths yourself:library(ggplot2)library(gridExtra)grid.arrange(arrangeGrob(ggplot(my_df) +                           geom_density(aes(Result, fill=Status), alpha=.3, bw=0.2)  +                           facet_grid(Group ~ .) + ggtitle(\"bw=0.2\"),                         ggplot(my_df) + geom_density(aes(Result, fill=Status), alpha=.3, bw=0.05)  +                           facet_grid(Group ~ .) + ggtitle(\"bw=0.05\"), ncol=2),             arrangeGrob(ggplot(my_df) +                           geom_density(aes(Result, fill=Status), alpha=.3, bw=0.2) +                           ggtitle(\"bw=0.2\"),                         ggplot(my_df) +                           geom_density(aes(Result, fill=Status), alpha=.3, bw=0.05) +                           ggtitle(\"bw=0.05\"), ncol=2),             ncol=1, heights=c(3,2))Notice that the grouped and ungrouped plots are the same (save for scaling) when the bandwidths (bw) are the same.I'm not sure if there's a way to extract the chosen bandwidth from within ggplot. However, ggplot is using the density function to generate the density plot, and density uses bw.nrd0 to choose the bandwidth. So let's use bw.nrd0 to see what bandwidths are being chosen. In the output below, notice how R chooses narrower bandwidths when considering each group separately, as compared with the case where both groups are combined.Each Status separately:sapply(split(my_df, my_dfResult)})After    Before 0.1224065 0.2983324Each Group and Status separately:sapply(split(my_df, interaction(my_dfGroup)), function(d) {bw.nrd0(d$Result)})After.1   Before.1    After.2   Before.2 0.09229781 0.07725860 0.07847190 0.06391614UPDATE: Regarding your comment, do you mean the \"spike\" in the uppermost plot in your question? I think this is a scaling effect due to the relative number of points in each group. my_df %\u0026gt;% group_by(Group, Status) %\u0026gt;% tally %\u0026gt;% ungroup %\u0026gt;%     mutate(Percent=round(n/sum(n)*100,1))  Group Status     n Percent  \u0026lt;dbl\u0026gt;  \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;   \u0026lt;dbl\u0026gt;1     1  After    13    10.82     1 Before    14    11.73     2  After    52    43.34     2 Before    41    34.2Note that Group==2 has many more observations than Group==1. If we equalize the number of observations in each group, then this scaling effect goes away:grid.arrange(my_df %\u0026gt;%                ggplot() +               geom_density(aes(Result, fill=Status), alpha=.3) +               ggtitle(\"Original Data\"),             my_df %\u0026gt;%                group_by(Status, Group) %\u0026gt;%               sample_n(100, replace=TRUE) %\u0026gt;%               ggplot() +                geom_density(aes(Result, fill=Status), alpha=.3) +               ggtitle(\"Resampled so each group has equal number of data points\"), ncol=1)You might find it easier to compare groups using another type of plot. Below are box and violin plots. The latter is a density plot, but each group's \"violin\" is scaled so that they all have the same area, which might make it easier to compare the shapes and locations of each distribution.grid.arrange(ggplot(my_df, aes(x=factor(Group), y=Result, colour=Status)) +               geom_boxplot(width=0.4, position=position_dodge(0.5))  + theme_bw(),             ggplot(my_df, aes(x=factor(Group), y=Result, colour=Status)) +               geom_violin(bw=0.07)  + theme_bw(), ncol=2)Or a histogram, which will show you where the data points are, without the model-based smoothing of the density estimate (though of course the binwidth will affect the shape of the plot):grid.arrange(ggplot(my_df) +               geom_histogram(aes(Result, fill=Status), alpha=.4, binwidth=0.1)  +               facet_grid(Group ~ .) + theme_bw(),             ggplot(my_df) +               geom_histogram(aes(Result, fill=Status), alpha=.4, , binwidth=0.1) + theme_bw(),              ncol=1, heights=c(3,2))","Display_name":"eipi10","Creater_id":3162,"Start_date":"2016-07-26 13:31:23","Question_id":225776}
{"_id":{"$oid":"5837a58ca05283111e4d74d3"},"Last_activity":"2016-07-27 11:30:37","Creator_reputation":11,"Question_score":1,"Answer_content":"By using Bayes' theorem we can writeFrom this we see that ROC curve obtained by thresholding  will be the same as ROC curve obtained by thresholding  only if  is constant.","Display_name":"hubi86","Creater_id":88964,"Start_date":"2016-07-27 11:30:37","Question_id":220383}
{"_id":{"$oid":"5837a58ca05283111e4d74e0"},"Last_activity":"2016-07-27 11:12:15","Creator_reputation":33236,"Question_score":0,"Answer_content":"This depends in part on what software you are using.  In R you can use the ellipse package to generate a confidence ellipse on the intercept and slope and see if the point 0, 1 is in the ellipse.For a general approach (not quite as good as the ellipse approach, but will work in any software that does regression):Compute a new column that is the difference between observed and predicted.  Use this column as the y-variable and your predicted column as the x-variable.  Now look at the individual tests of the intercept and slope, if either is significant then you should reject your null of 0,1.  You may also want to look at the correlation between the estimated intercept and slope, if that is high (in absolute value) then you may want to rerun with your predictions (x-variable) centered.","Display_name":"Greg Snow","Creater_id":4505,"Start_date":"2016-07-27 11:12:15","Question_id":225956}
{"_id":{"$oid":"5837a58ca05283111e4d74f1"},"Last_activity":"2015-08-12 17:40:20","Creator_reputation":8035,"Question_score":16,"Answer_content":"Although this isn't a common analysis, it really is one of interest.  The accepted answer fits the way you asked your question, but I'm going to provide another reasonably well accepted technique that may or may not be equivalent (I'll leave it to better minds to comment on that).This approach is to use the following Z test:Where  is the standard error of .This equation is provided by Clogg, C. C., Petkova, E., \u0026amp; Haritou, A. (1995). Statistical methods for comparing regression coefficients between models. American Journal of Sociology, 100(5), 1261-1293. and is cited by Paternoster, R., Brame, R., Mazerolle, P., \u0026amp; Piquero, A. (1998). Using the correct statistical test for equality of regression coefficients. Criminology, 36(4), 859-866. equation 4, which is available free of a paywall.  I've adapted Peternoster's formula to use  rather than  because it is possible that you might be interested in different DVs for some awful reason and my memory of Clogg et al. was that their formula used .  I also remember cross checking this formula against Cohen, Cohen, West, and Aiken, and the root of the same thinking can be found there in the confidence interval of differences between coefficients, equation 2.8.6, pg 46-47.","Display_name":"rpierce","Creater_id":196,"Start_date":"2014-05-21 06:28:29","Question_id":93540}
{"_id":{"$oid":"5837a58ca05283111e4d74f2"},"Last_activity":"2014-04-13 03:05:31","Creator_reputation":656,"Question_score":6,"Answer_content":"For people with a similar question, let me provide a simple outline of the answer.The trick is to set up the two equations as a system of seemingly unrelated equations and to estimate them jointly. That is, we stack  and  on top of each other, and doing more or less the same with the design matrix. That is, the system to be estimated is:  This will lead to a variance-covariance matrix that allows to test for equality of the two coefficients. ","Display_name":"coffeinjunky","Creater_id":31634,"Start_date":"2014-04-13 03:05:31","Question_id":93540}
{"_id":{"$oid":"5837a58ca05283111e4d74ff"},"Last_activity":"2016-07-27 11:01:17","Creator_reputation":11,"Question_score":0,"Answer_content":"It looks that the observed values exceeds the dominion of the distribution. When a random variable is defined as a function of another random variable, PyMC checks that no value of the parent distribution leads to an impossible value for the child distribution. Please look here for a more comprehensive explanation on a similar subject.","Display_name":"Francisco Grings","Creater_id":109235,"Start_date":"2016-07-27 11:01:17","Question_id":210933}
{"_id":{"$oid":"5837a58ca05283111e4d750c"},"Last_activity":"2016-07-27 10:47:55","Creator_reputation":4284,"Question_score":10,"Answer_content":"As Silverfish said, the problem in your reasoning is that to find the PDF of a squared random variable, or any other transformed random variable, you can't just perform that transformation on the PDF. If we want to know the actual PDF of a squared random variable we must calculate . One way to do this is to use the CDF method below, Since the derivative of the CDF is the PDF, we take the derivative of both sides with respect to  and get,  which is the PDF we expect for a Chi-square distribution with one degree of freedom.","Display_name":"TrynnaDoStat","Creater_id":23801,"Start_date":"2016-07-27 08:52:02","Question_id":225914}
{"_id":{"$oid":"5837a58ca05283111e4d750d"},"Last_activity":"2016-07-27 08:41:19","Creator_reputation":19151,"Question_score":1,"Answer_content":"Actually,  distribution \"looks like\" normal. The exact relation for  is How come? As you noted a variable from this distribution can be thought of as a sum of squared normals: . However, the squared normal random variables are themselves random variables, albeit not normal anymore. We also know from CLT, that a sum of random variables must converge to some kind of a normal variable.Also, you proposed to square the normal PDF to get the PDF of the square of normal variable. That doesn't work. For instance, if you take an integral of your new proposed PDF, you get  instead of 1. So, the squared PDF of normal distribution is not even a PDF, it doesn't normalize to 1.","Display_name":"Aksakal","Creater_id":36041,"Start_date":"2016-07-27 08:36:14","Question_id":225914}
{"_id":{"$oid":"5837a58ca05283111e4d750e"},"Last_activity":"2016-07-27 08:14:48","Creator_reputation":25650,"Question_score":13,"Answer_content":"Normally distributed random variable can take values ranging from  to . Squaring any real value would lead to positive values. So how can possibly  be normally distributed?Squaring probability density function does not make any sense since this would give you \"probabilities squared\" (more precisely: squared density) rather then squared random variable. Applying any function  to random variable  means applying it to 's values rather then to it's probability density function, or cumulative distribution function.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-27 08:08:14","Question_id":225914}
{"_id":{"$oid":"5837a58ca05283111e4d7521"},"Last_activity":"2016-07-27 10:01:15","Creator_reputation":41,"Question_score":2,"Answer_content":"survey computes the standard errors with consideration of the loss of precision introduced by sampling weights. Weights in glm simply adjust the weight given to the errors in the least squares estimation, so the standard errors aren't correct. Here's a selection from Lumley (2010):  In a model-based analysis it would be necessary to specify the random part of the model correctly to get correct standard errors, but all our standard error estimates are design-based and so are valid regardless of the model. It is worth noting that the “sandwich”, or “model-robust”, or “heteroskedasticity-consistent” standard errors sometimes used in model-based regression analysis are almost identical to the design-based standard errors we will use; the main difference being in the handling of stratification.So without strata in your design, you will likely find that using sandwich will get you identical or near-identical SE estimates.library(sandwich)coefs \u0026lt;- vcovHC(glm11, type=\"HC0\")coeftest(glm11,coefs)In my test, they didn't compute out exactly when using \"HC0\" or \"HC1\", but were very close. svyglm is now reporting a z-value instead of t-value as well.","Display_name":"commscho","Creater_id":81463,"Start_date":"2016-07-27 10:01:15","Question_id":57107}
{"_id":{"$oid":"5837a58ca05283111e4d7522"},"Last_activity":"2013-04-24 14:05:42","Creator_reputation":5340,"Question_score":5,"Answer_content":"There are lots of different sorts of weights and they get kind of confusing.  You have to be pretty careful when you're using different functions or software that you're using the kind of weights you think you're using.The svyglm function uses survey weights - these weight the importance of each case to make them representative (to each other, after twang).  I'm not sure what weight does in glm() - I think they represent the accuracy of the measures.  (If you're using the binomial family, they have different meaning).  The survey weights (in surveyglm) are the weights that you want, to give you the correct standard errors.(There are also frequency weights, analytic weights, and importance weights).","Display_name":"Jeremy Miles","Creater_id":17072,"Start_date":"2013-04-24 14:05:42","Question_id":57107}
{"_id":{"$oid":"5837a58ca05283111e4d752f"},"Last_activity":"2016-07-27 09:44:24","Creator_reputation":5445,"Question_score":3,"Answer_content":"I will answer your questions in turn, but first note that since quest1 and quest2 measure the same thing, you should include either one or the other. Also,  lag:I(lag^2) is exactly the same as I(lag^3), so usually this is not what you want, unless you specifically think that there is a cubic relationship and you want to include interactions between the quadratic term and the other variables - all of which makes interpretation much harder. So if you suspect there is a cubic relationship, it is better to just include +I(lag^3) in the model formula.So I would suggest the initial model to be:lmer(phy ~ lag * group * quest1 + I(lag^2) + (1|id))or possibly:lmer(phy ~ lag * group * quest1 + I(lag^2) + I(lag^3) + (1|id))No. Stepwise procedures are a poor method of variable selection, as discussed in depth here.The model should be informed by theory, so it's not a question of whether to believe in the model, it's a question of whether the model fits the data. Which interactions to include should be driven by what the underlying physical process suggests. To just throw all the variables into an all-way interaction model is folly.Yes, you have sufficient groups to estimate random intercepts, and enough observations to fit the interactions.The 5-way model you specified would be very hard to interpret but if you proceed with the model I suggested it would be perfectly possible.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-27 08:39:55","Question_id":225904}
{"_id":{"$oid":"5837a58ca05283111e4d7530"},"Last_activity":"2016-07-27 08:29:03","Creator_reputation":3734,"Question_score":1,"Answer_content":"To take q4 I think it is very unlikely that you can produce a sensible interpretation of anything above a three-way interaction and that can be a struggle.Q3 - are these all within subject variables? i assume not and so I would be a bit hesitant about having three covariates with only 15 peopleQ2 - see answer to Q4Q1 - if you mean are you interpreting the AIC, BIC correctly then yes I think you are.Why are you including an interaction between lag and lag^2? If you fitted lag * (the other covariates) and lag^2 * (the other covariates) you would have fewer terms and perhaps a better starting point.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-07-27 08:29:03","Question_id":225904}
{"_id":{"$oid":"5837a58ca05283111e4d753d"},"Last_activity":"2016-07-27 09:39:52","Creator_reputation":6097,"Question_score":19,"Answer_content":"I think you are overlooking the fact that it does not matter whether \"we\" can distinguish the dice or not, but rather it matters that the dice are unique and distinct, and act on their own accord.So if in the closed box scenario, you open the box and see a 1 and a 2, you don't know whether it is  or , because you cannot distinguish the dice. However, both  and  would lead to the same visual you see, that is, a 1 and a 2. So there are two outcomes favoring that visual. Similarly for every non-same pair, there are two outcomes favoring each visual, and thus there are 36 possible outcomes. Mathematically, the formula for the probability of an event is\\dfrac{\\text{Number of outcomes for the event}}{\\text{Number of total possible outcomes}}. However, this formula only holds for when each outcome is equally likely. In the first table, each of those pairs is equally likely, so the formula holds. In your second table, each outcome is not equally likely, so the formula does not work. The way you find the answer using your table isProbability of 1 and 2 = Probability of  + Probability of  = .Another way to to think about this is that this experiment is the exact same as rolling each die separately, where you can spot Die 1 and Die 2.  Thus the outcomes and their probabilities will match with the closed box experiment.","Display_name":"Greenparker","Creater_id":31978,"Start_date":"2016-07-25 10:01:03","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d753e"},"Last_activity":"2016-07-26 13:16:58","Creator_reputation":25650,"Question_score":9,"Answer_content":"  Imagine that you threw your fair six-sided die and you got \u0026#9856;. The  result was so fascinating that you called your friend Dave and told  him about it. Since he  was curious what he'd get when  throwing his fair six-sided die, he threw it and got \u0026#9857;.A standard die has six sides. If you are not cheating then it lands on each side with equal probability, i.e.  in  times. The probability that you throw \u0026#9856;, the same as with the other sides, is . The probability that you throw \u0026#9856;, and your friend throws \u0026#9857;, is  since the two events are independent and we multiply independent probabilities. Saying it differently, there are  arrangements of such pairs that can be easily listed (as you already did). The probability of the opposite event (you throw \u0026#9857; and your friend throws \u0026#9856;) is also . The probabilities that you throw \u0026#9856;, and your friend throws \u0026#9857;, or that you throw \u0026#9857;, and your friend throws \u0026#9856;, are exclusive, so we add them . Among all the possible arrangements, there are two meeting this condition.How do we know all of this? Well, on the grounds of probability, combinatorics and logic, but those three need some factual knowledge to rely on. We know on the basis of the experience of thousands of gamblers and some physics, that there is no reason to believe that a fair six-sided die has other than an equiprobable chance of landing on each side. Similarly, we have no reason to suspect that two independent throws are somehow related and influence each other.You can imagine a box with tickets labeled using all the -combinations (with repetition) of numbers from  to . That would limit the number of possible outcomes to  and change the probabilities. However if you think of such a definition in term of dice, then you would have to imagine two dice that are somehow glued together. This is something very different than two dice that can function independently and can be thrown alone landing on each side with equal probability without affecting each other.All that said, one needs to comment that such models are possible, but not for things like dice. For example, in particle physics based on empirical observations it appeared that Bose-Einstein statistic of non-distinguishable particles (see also the stars-and-bars problem) is more appropriate than the distinguishable-particles model. You can find some remarks about those models in Probability or Probability via Expectation by Peter Whittle, or in volume one of An introduction to probability theory and its applications by William Feller.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-25 13:22:44","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d753f"},"Last_activity":"2016-07-26 02:52:38","Creator_reputation":1486,"Question_score":2,"Answer_content":"If we just observe \"Somebody gives me a box. I open the box. There is a  and a \", without further information, we don't know anything about the probability.If we know that the two dice are fair and that they have been rolled, then the probability is 1/18 as all other answer have explained. The fact we don't know if the die with 1 o the die with 2 was rolled first doesn't matter, because we must account for both ways - and therefore the probability is 1/18 instead of 1/36.But if we don't know which process led to having the 1-2 combination, we can't know anything about the probability. Maybe the person who handed us the box just purposely chose this combination and stuck the dice to the box (probability=1), or maybe he shacked the box rolling the dice (probability=1/18) or he might have chosen at random one combination from the 21 combinations in the table you gave us in the question, and therefore probability=1/21.In summary, we know the probability because we know what process lead to the final situation, and we can compute probability for each stage (probability for each dice). The process matters, even if we haven't seen it taking place.To end the answer, I'll give a couple of examples where the process matters a lot:We flip ten coins. What's the probability getting heads all of ten times? You can see that the probability (1/1024) is a lot smaller than the probability of getting a 10 if we just choose a random number between 0 and 10 (1/11).If you have enjoyed this problem, you can try with the Monty Hall problem. It's a similar problem where the process matters much more than what our intuition would expect.","Display_name":"Pere","Creater_id":123561,"Start_date":"2016-07-26 02:52:38","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d7540"},"Last_activity":"2016-07-25 15:15:56","Creator_reputation":121,"Question_score":2,"Answer_content":"We can deduce that your second table does not represent the scenario accurately.You have eliminated all the cells below and left of the diagonal, on the supposed basis that (1, 2) and (2, 1) are congruent and therefore redundant outcomes.Instead suppose that you roll one die twice in a row.  Is it valid to count 1-then-2 as an identical outcome as 2-then-1?  Clearly not.  Even though the second roll outcome does not depend on the first, they are still distinct outcomes.  You cannot eliminate rearrangements as duplicates.  Now, rolling two dice at once is the same for this purpose as rolling one die twice in a row.  You therefore cannot eliminate rearrangements.(Still not convinced?  Here is an analogy of sorts.  You walk from your house to the top of the mountain.  Tomorrow you walk back.  Was there any point in time on both days when you were at the same place?  Maybe?  Now imagine you walk from your house to the top of the mountain, and on the same day another person walks from the top of the mountain to your house.  Is there any time that day when you meet?  Obviously yes.  They are the same question.  Transposition in time of untangled events does not change deductions that can be made from those events.)","Display_name":"wberry","Creater_id":124493,"Start_date":"2016-07-25 15:15:56","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d7541"},"Last_activity":"2016-07-25 14:16:34","Creator_reputation":31,"Question_score":1,"Answer_content":"The probability of event A and B is calculated by multiplying both probabilities.The probability of rolling a 1 when there are six possible options is 1/6. The probability of rolling a 2 when there are six possible options is 1/6.1/6 * 1/6 = 1/36.However, the event is not contingent on time (in other words, it is not required that we roll a 1 before a 2; only that we roll both a 1 and 2 in two rolls). Thus, I could roll a 1 and then 2 and satisfy the condition of rolling both 1 and 2, or I could roll a 2 and then 1 and satisfy the condition of rolling both 1 and 2.The probability of rolling 2 and then 1 has the same calculation:1/6 * 1/6 = 1/36.The probability of either A or B is the sum of the probabilities. So let's say event A is rolling 1 then 2, and event B is rolling 2 then 1.Probability of Event A: 1/36Probability of Event B: 1/361/36 + 1/36 = 2/36 which reduces to 1/18.","Display_name":"HappySnowman","Creater_id":102933,"Start_date":"2016-07-25 14:16:34","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d7542"},"Last_activity":"2016-07-25 11:59:03","Creator_reputation":51,"Question_score":5,"Answer_content":"The key idea is that if you list the 36 possible outcomes of two distinguishable dice, you are listing equally probable outcomes. This is not obvious, or axiomatic; it's true only if your dice are fair and not somehow connected. If you list the outcomes of indistinguishable dice, they are not equally probable, because why should they be, any more than the outcomes \"win the lottery\" and \"don't win the lottery\" are equally probable. To get to the conclusion, you need:We are working with fair dice, for which all six numbers are equally probable.The two dice are independent, so that the probability of die number two obtaining a particular number is always independent of what number die number one gave. (Imagine instead rolling the same die twice on a sticky surface of some kind that made the second roll come out different.)Given those two facts about the situation, the rules of probability tell you that the probability of achieving any pair  is the probability of achieving  on the first die times that of achieving  on the second. If you start lumping  and  together, then you don't have the simple independence of events to help you any more, so you can't just multiply probabilities. Instead, you have made a collection of mutually exclusive events (if ), so you can safely add the probabilities of getting  and  if they are different.The idea that you can get probabilities by just counting possibilities relies on assumptions of equal probability and independence. These assumptions are rarely verified in reality but almost always in classroom problems.","Display_name":"Anne","Creater_id":124473,"Start_date":"2016-07-25 11:42:53","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d7543"},"Last_activity":"2016-07-25 11:55:58","Creator_reputation":141,"Question_score":4,"Answer_content":"Let's start by stating the assumption: indistinguishable dice only roll 21 possible outcomes, while distinguishable dice roll 36 possible outcomes.To test the difference, get a pair of identical white dice. Coat one in a UV-absorbent material like sunscreen, which is invisible to the naked eye. The dice still appear indistinguishable until you look at them under a black light, when the coated die appears black while the clean die glows.Conceal the pair of dice in a box and shake it. What are the odds you'll get a 2 and a 1 when you open the box? Intuitively you might think \"rolling a 1 and a 2\" is just 1 of 21 possible outcomes because you can't tell the dice apart. But if you open the box under a black light, you can tell them apart. When you can tell the dice apart, \"rolling a 1 and a 2\" is 2 of 36 possible combinations. Does that mean a black light has the power to change the probability of obtaining a certain outcome, even if the dice are only exposed to the light and observed after they've been rolled? Of course not. Nothing changes the dice after you stop shaking the box. The probability of a given outcome can't change.Since the original assumption depends on a change that doesn't exist, it's reasonable to conclude that the original assumption was incorrect. But what about the original assumption is incorrect - that indistinguishable dice only roll 21 possible outcomes, or that distinguishable dice roll 36 possible outcomes?Clearly the black light experiment demonstrated that observation has no impact on probability (at least on this scale - quantum probability is a different matter) or the distinctness of objects. The term \"indistinguishable\" merely describes something which observation cannot differentiate from something else. In other words, the fact that the dice appear the same under some circumstances (i.e. that they aren't under a black light) and not others has no bear on the fact that they are truly two distinct objects. This would be true even if the circumstances under which you're able to distinguish between them are never discovered.In short: your ability to distinguish between the dice being rolled is irrelevant when analyzing the probability of a particular outcome. Each die is inherently distinct. All outcomes are based on this fact, not on an observer's point of view.","Display_name":"talrnu","Creater_id":68649,"Start_date":"2016-07-25 11:55:58","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d7544"},"Last_activity":"2016-07-25 10:29:19","Creator_reputation":10758,"Question_score":15,"Answer_content":"Lets imagine that the first scenario involves rolling one red die and one blue die, while the second involves you rolling a pair of white dice.In the first case, can write down every possible outcome as (red die, blue die), which gives you this table (reproduced from your question):\\begin{array} {|c|c|c|c|c|c|c|}\\hline \\frac{\\textrm{Blue}}{\\textrm{Red}}\u0026amp;1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \\\\\\hline1 \u0026amp; (1,1) \u0026amp; \\mathbf{(1,2)} \u0026amp; (1,3) \u0026amp; (1,4) \u0026amp; (1,5) \u0026amp; (1,6) \\\\\\hline2 \u0026amp; \\mathbf{(2,1)} \u0026amp; (2,2) \u0026amp; (2,3) \u0026amp; (2,4) \u0026amp; (2,5) \u0026amp; (2,6) \\\\\\hline3 \u0026amp; (3,1) \u0026amp; (3,2) \u0026amp; (3,3) \u0026amp; (3,4) \u0026amp; (3,5) \u0026amp; (3,6) \\\\\\hline4 \u0026amp; (4,1) \u0026amp; (4,2) \u0026amp; (4,3) \u0026amp; (4,4) \u0026amp; (4,5) \u0026amp; (4,6) \\\\\\hline5 \u0026amp; (5,1) \u0026amp; (5,2) \u0026amp; (5,3) \u0026amp; (5,4) \u0026amp; (5,5) \u0026amp; (5,6) \\\\\\hline6 \u0026amp; (6,1) \u0026amp; (6,2) \u0026amp; (6,3) \u0026amp; (6,4) \u0026amp; (6,5) \u0026amp; (6,6) \\\\\\hline\\end{array}Our idealized dice are fair (each outcome is equally likely) and you've listed every outcome. Based on this, you correctly conclude that a one and a two occurs with probability , or  So far, so good.Next, suppose you roll two identical dice instead. You've correctly listed all the possible outcomes, but you incorrectly assumed all of these outcomes are equally likely. In particular, the  outcomes are half as likely as the other outcomes. Because of this, you cannot just calculate the probability by adding up the # of desired outcomes over the total number of outcomes. Instead, you need to weight each outcome by the probability of it occurring. If you run through the math, you'll find that it comes out the same--one doubly-likely event in the numerator out of 15 double-likely events and 6 singleton events.The next question is \"how could I know that the events aren't all equally likely?\" One way to think about this is to imagine what would happen if you could distinguish the two dice. Perhaps you put a tiny mark on each die. This can't change the outcome, but it reduces the problem the previous one. Alternately, suppose you write the chart out so that instead of Blue/Red, it reads Left Die/Right Die.As a further exercise, think about the difference between seeing an ordered outcome (red=1, blue=2) vs. an unordered one (one die showing 1, one die showing 2).","Display_name":"Matt Krause","Creater_id":7250,"Start_date":"2016-07-25 10:29:19","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d7545"},"Last_activity":"2016-07-25 10:05:34","Creator_reputation":176,"Question_score":4,"Answer_content":"If you translate this into terms of coins - say, flipping two indistinguishable pennies - it becomes a question of only three outcomes: 2 heads, 2 tails, 1 of each, and the problem is easier to spot. The same logic applies, and we see that it's more likely to get 1 of each than to get 2 heads or 2 tails. That's the slipperiness of your second table - it represents all possible outcomes, even though they are not all equally weighted probabilities, as in the first table. It would be ill-defined to try to spell out what each row and column in the second table means - they're only meaningful in the combined table where each outcome has 1 box, regardless of likelihood, whereas the first table displays \"all the equally likely outcomes of die 1, each having its own row,\" and similarly for columns and die 2.","Display_name":"Non-Contradiction","Creater_id":123196,"Start_date":"2016-07-25 10:05:34","Question_id":225552}
{"_id":{"$oid":"5837a58ca05283111e4d7552"},"Last_activity":"2016-07-27 09:25:59","Creator_reputation":10758,"Question_score":2,"Answer_content":"That seems totally reasonable! If you do the math, you'll find that the underlying operations are exactly the same in both cases. In one case, you multiply each input () by the corresponding input (), then add a separate bias term (), so the input to your nonlinearity is: b + \\sum_{i=1}^{i=n} w_ix_iNow, suppose we append a one to  to create  and extend the weight vector to match. The nonlinearity's input becomes:\\sum_{i=1}^{i=n+1} w_ix_i The first  elements of  will be the same, and the last element will become . Anything times one is itself, so...the final answer is the same.Furthermore, this lets you do every stage as a single matrix multiplication (). These can be performed vastly more efficiently than a naive for-loop like:output = biasfor i=1 to i=N    output = output + (weight[i] * input[i])endIf you're interested in this, you may want to look into linear algebra packages like BLAS (or ATLAS, MKL, etc), or some of the GPGPU work like CUDA and OpenCL. ","Display_name":"Matt Krause","Creater_id":7250,"Start_date":"2016-07-27 09:25:59","Question_id":225925}
{"_id":{"$oid":"5837a58ca05283111e4d7561"},"Last_activity":"2016-04-28 10:28:47","Creator_reputation":76495,"Question_score":1,"Answer_content":"(Converting my comment into an answer so that this doesn't stay officially unanswered.)I don't know of such a thing, but it may exist.  However, it seems to me that this isn't really much of a problem.  PCA is more of a descriptive technique than an inferential technique.  We can contrast it with running a simple product moment correlation.  If you have two variables,  \u0026amp; , and you duplicated your data (such that you had two copies of every observation), the computed  wouldn't change relative to computing  on only the original  rows.  What would happen is that the computed confidence interval around  would be too narrow, and the -value would be too low.  These effects occur because Pearson's  can be seen as both a descriptive statistic and an inferential statistic.  PCA doesn't really have that latter inferential attribute.  As a result, there is no harm in duplicating your data and running PCA\u0026mdash;you should get the same eigenvectors and eigenvalues.  The implication, therefore, is that you can get a weighted PCA manually by duplicating the  rows and copying the  rows seven times over such that your final dataset is .  Then run PCA on the enlarged dataset.  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-04-28 10:28:47","Question_id":207368}
{"_id":{"$oid":"5837a58ca05283111e4d7570"},"Last_activity":"2015-10-05 23:57:27","Creator_reputation":693,"Question_score":1,"Answer_content":"You can do this by weighting each sample of  by weight . To do this simply multiply each data point coordinate by  as seen in this answer Weighted principal components analysis","Display_name":"j__","Creater_id":64136,"Start_date":"2015-10-05 23:57:27","Question_id":175640}
{"_id":{"$oid":"5837a58ca05283111e4d7587"},"Last_activity":"2016-07-27 08:22:43","Creator_reputation":36,"Question_score":1,"Answer_content":"mAP probably stands for mean Average Precision. The AP provides a measure of quality across all recall levels for single class classification, it can be seen as the area under the precision-recall curve. Then the mAP is the mean of APs in multi-class classification.","Display_name":"Jelle Sch\u0026#252;hmacher","Creater_id":115521,"Start_date":"2016-07-27 08:22:43","Question_id":225897}
{"_id":{"$oid":"5837a58ca05283111e4d7594"},"Last_activity":"2016-07-27 08:17:39","Creator_reputation":528,"Question_score":1,"Answer_content":"The posterior predictive distribution can be obtained from any software that produces draws from the posterior distribution. It sounds as if your likelihood is normal or log-normal, in which case you can multiply the predictor(s) by a draw of the coefficient(s) to form a linear predictor. And then for each new observation you are trying to predict, draw from a normal distribution with mean equal to the corresponding linear predictor and standard deviation equal to the corresponding posterior draw of the error standard deviation. (If your likelihood is log-normal or the outcome was logged, then antilog this draw from the normal distribution). Then just repeat that process for all the draws of the coefficient(s) and error standard deviation that you obtain from the posterior distribution.The above is not specific to Stan but you can perform the above steps in the generated quantities block of a Stan program utilizing the normal_rng function.","Display_name":"Ben Goodrich","Creater_id":16579,"Start_date":"2016-07-27 08:17:39","Question_id":225804}
{"_id":{"$oid":"5837a58ca05283111e4d75a1"},"Last_activity":"2016-07-27 08:15:28","Creator_reputation":3645,"Question_score":1,"Answer_content":"Heavy-tailed data, power laws, extreme value distributions and theory are a special class of models in statistics. One useful way to interpret or further analyze your data would be to estimate a tail index. While there are several sophisticated estimators such as Hill's or Pickand's methods, a straightforward heuristic is to use OLS regression on the log(rank) against log(size) with the resulting coefficient providing a proxy for the index. Once that is derived, then this value can be compared against a Tweedie distribution table to determine in which family your data's distribution belongs.There are many, many resources available that discuss heavy-tailed data, power laws, tail indexes and the like. Here are a few suggestions:https://en.wikipedia.org/wiki/Heavy-tailed_distributionFor tail index domains see the Tweedie table \"Examples\" here https://en.wikipedia.org/wiki/Tweedie_distributionTail index heuristicXavier Gabaix, RANK−1/2: A SIMPLE WAY TO IMPROVE THE OLS ESTIMATION OF TAIL EXPONENTS available here ... http://www.eco.uc3m.es/temp/jbes.2009.06157.pdfCosma Shalizi, the CMU machine learning guru has much to say about power laws here, So you think you have a power law? http://www.stat.cmu.edu/~cshalizi/2010-10-18-Meetup.pdf","Display_name":"DJohnson","Creater_id":82102,"Start_date":"2016-07-27 08:15:28","Question_id":225913}
{"_id":{"$oid":"5837a58ca05283111e4d75b2"},"Last_activity":"2016-06-22 08:31:26","Creator_reputation":6,"Question_score":0,"Answer_content":"You are applying Poisson model to the dataset which does not behave this way -- the assumption of independence in time does not hold.For the dataset in consideration. We look at the prevalence of hypertension for different cities and age/sex groups.Try the following R code:# As example read high blood pressure datalibrary(foreign)d \u0026lt;- read.dta(\"hbp.dta\")# Reformat datadcity)dage_group) # Remove misleading unused age groupsdhbp) - 1# Fit logistic regressionfm \u0026lt;- glm(hbp ~ city + age_group*sex, data=d)# Basic model selectionbm \u0026lt;- step(fm)summary(bm)You will find that the risk age group turns out to be (30-34) which are, indeed, the oldest subjects in the data. Being a woman is slight protective factor and the city3 has the highest prevalence. Therefore to test the Poisson behavior let us subset to, say, city1, men.ind1 \u0026lt;- which(dsex == \"Male\")d2 \u0026lt;- d[c(ind1,ind2),]ag \u0026lt;- aggregate(hbp ~ age_group, d2, sum)The result is:age_group hbp15 - 19   520 - 24  1225 - 29  1930 - 34  24Now fit Poisson:x \u0026lt;- (1:4)m1 \u0026lt;- glm(agR^2 = 0.96$ so it is good.","Display_name":"slakov","Creater_id":120897,"Start_date":"2016-06-22 08:31:26","Question_id":219215}
{"_id":{"$oid":"5837a58ca05283111e4d75c3"},"Last_activity":"2015-09-19 04:58:57","Creator_reputation":3511,"Question_score":2,"Answer_content":"It is fairly obvious that you cannot (naively) test for difference in distributions for groups that were defined using the same data. This is known as \"selective testing\", \"double dipping\", \"circular inference\", etc.An example would be performing a t-test on the heights of \"tall\" and \"short\" people in your data. The null will (almost) always be rejected. Having said that- one may indeed account for the clustering stage at the testing stage. I am unfamiliar, however, with a particular reference that does that, but I suspect this should have been done. ","Display_name":"JohnRos","Creater_id":6961,"Start_date":"2015-09-19 04:58:57","Question_id":18706}
{"_id":{"$oid":"5837a58ca05283111e4d75d4"},"Last_activity":"2016-07-27 06:01:32","Creator_reputation":2186,"Question_score":0,"Answer_content":"Building on the answer of @HarveyMotulsky, I'll give a simple example demonstrating that the information you seek is not present in the information you have. Consider those two example vectors a and b, which are the same, except for -1 replaced with -9 (and 1 with 9):a = c(-10,-1,-1,-1,1,1,1,10)print(a)# -10  -1  -1  -1   1   1   1  10b = c(-10,-9,-9,-9,9,9,9,10)print(b)# -10  -9  -9  -9   9   9   9  10Those vectors have the amount of elements, min, max, and mean...min(a); min(b)# [1] -10# [1] -10max(a); max(b)# [1] 10# [1] 10mean(a); mean(b)# [1] 0# [1] 0...but not the same standard deviation:sd(a); sd(b)# [1] 5.424811# [1] 9.899495So, in case you only have the nr. of elements, min, max, and mean, though they tell you something about your data, you still have an infinite amount of options for which data originated those, thereby an infinite amount of possible standard deviations.","Display_name":"geekoverdose","Creater_id":112731,"Start_date":"2016-07-27 06:01:32","Question_id":225889}
{"_id":{"$oid":"5837a58ca05283111e4d75d5"},"Last_activity":"2016-07-27 05:48:11","Creator_reputation":7473,"Question_score":1,"Answer_content":"The standard deviation quantifies variation among values. Without knowing all the individual values, you can't possibly calculate the standard deviation.The standard error of the mean is computed from the standard deviation and sample size. So you can't know it either.","Display_name":"Harvey Motulsky","Creater_id":25,"Start_date":"2016-07-27 05:48:11","Question_id":225889}
{"_id":{"$oid":"5837a58ca05283111e4d75e8"},"Last_activity":"2016-07-27 05:27:42","Creator_reputation":12,"Question_score":0,"Answer_content":"You can use package clickstream or clickclust in R language. It performs exactly what you are looking for. ","Display_name":"Sagar","Creater_id":110371,"Start_date":"2016-07-27 05:27:42","Question_id":31972}
{"_id":{"$oid":"5837a58ca05283111e4d75e9"},"Last_activity":"2013-06-11 11:23:29","Creator_reputation":1752,"Question_score":3,"Answer_content":"It is a good question with many practical applications.Your data are sequential so we need a similarity measure between any pair of sequences. I recommend Levensthein distance since it is very intuitive and very nicely defined. See also this nice bachelor thesis with an overview of more measures for sequential data.Finally, if one has the distances between all pairs of sequences, we can use any clustering algorithm that takes a distance matrix as input (for example any hierarchical algorithm).","Display_name":"Miroslav Sabo","Creater_id":14730,"Start_date":"2012-10-09 03:59:02","Question_id":31972}
{"_id":{"$oid":"5837a58ca05283111e4d75f6"},"Last_activity":"2016-07-27 05:21:32","Creator_reputation":5445,"Question_score":0,"Answer_content":"A mixed model would be sensible since you have repeated measures and enough subjects to estimate random intercepts for them. Alternatively a repeated measures ANOVA could suffice, if the outcome is continuous.The model specification looks OK, assuming that the outcome is continuous, but you should check the residuals and random intercepts to see if they are plausibly normally distributed. If the outcome is not continuous then you should use a generalized linear mixed model.You could use lsmeans to compare the fixed effects. The random effects (random intercept for Subject) is estimated as a standard deviation and variance in the output. You don't need a p-value for that.Edit: To clarify, the model you should use depends, in part, on type of outcome/response variable you have. I mentioned continuous, though in reality there are different definitions of variable types, for example see here and here. For linear mixed models (such as lmer), one the main requirements is that the residuals are normally distributed. If the outcome variable is, for example a count, or an ordered categorical variable, it is highly likely that the residuals won't be normally distributed, and you should use a generalised linear mixed model instead.","Display_name":"Robert Long","Creater_id":7486,"Start_date":"2016-07-25 10:06:50","Question_id":225543}
{"_id":{"$oid":"5837a58ca05283111e4d7603"},"Last_activity":"2016-07-27 04:14:29","Creator_reputation":9093,"Question_score":2,"Answer_content":"Yes. Instead of using a two-sided critical value from a t-distribution with  degrees of freedom (e.g.,  for  and , two-sided), you would use just the upper critical value (e.g.,  for  and , one-sided).","Display_name":"Wolfgang","Creater_id":1934,"Start_date":"2016-07-27 04:14:29","Question_id":225874}
{"_id":{"$oid":"5837a58ca05283111e4d7612"},"Last_activity":"2016-07-27 04:42:42","Creator_reputation":3734,"Question_score":1,"Answer_content":"You will need from each study an estimate of the prevalence and its standard error. You will probably want to transform them in some way. You do not say whether you have study level information on sex and age (just average age or proportion women) rather than individual level data. If you only have study level data you need to use software which does the meta-analysis appropriately. I do not know whether you can do this in SPSS but there are many packages available in R which do. My personal preference is for the metafor package. If you have individual level data then you can use a mixed effects regression.There is a wealth of material on the author's website http://www.metafor-project.org/doku.php which should help you get further with your analysis.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-07-27 04:17:26","Question_id":225865}
{"_id":{"$oid":"5837a58ca05283111e4d761f"},"Last_activity":"2016-07-27 04:05:53","Creator_reputation":201,"Question_score":3,"Answer_content":"In a CNN, each neuron produces one feature map. Since dropout works per-neuron, dropping a neuron means that the corresponding feature map is dropped - e.g. each position has the same value (usually 0). So each feature map is either fully dropped or not dropped at all. Pooling usually operates separately on each feature map, so it should not make any difference if you apply dropout before or after pooling. At least this is the case for pooling operations like maxpooling or averaging.","Display_name":"schreon","Creater_id":22540,"Start_date":"2016-07-27 04:05:53","Question_id":147850}
{"_id":{"$oid":"5837a58ca05283111e4d7620"},"Last_activity":"2015-04-26 10:08:41","Creator_reputation":161,"Question_score":6,"Answer_content":"This tutorial uses pooling before dropout and gets good results.That doesn't necessarily mean the other order doesn't work of course. My experience is limited, I've only used them on dense layers without pooling.","Display_name":"Mark","Creater_id":46948,"Start_date":"2015-04-26 10:08:41","Question_id":147850}
{"_id":{"$oid":"5837a58ca05283111e4d762d"},"Last_activity":"2012-01-17 07:19:53","Creator_reputation":147868,"Question_score":7,"Answer_content":"Making the change of variables  puts the problem into the same form with .  Rewrite the density in the form  and rewrite the equations as\\eqalign{\r\\int_0^1 x f(x,\\nu,\\tau) dx   \u0026amp;= \\mu \\int_0^1  f(x,\\nu,\\tau)\\\\\r\\int_0^1 x^2 f(x,\\nu,\\tau)dx  \u0026amp;= (\\mu^2+\\sigma^2) \\int_0^1 f(x,\\nu,\\tau).\r}All three integrals can be written explicitly in terms of the exponential exp and the cumulative standard normal distribution pnorm.  This suggests the following R code for the core calculations; straightforward algebraic manipulations will produce the desired values of , , and :f0 \u0026lt;- function(x){ # Normalization factor    mu\u0026lt;-x[1];sigma\u0026lt;-x[2]    pnorm((1-mu)/sigma)-pnorm(-mu/sigma)}f1 \u0026lt;- function(x){ # First moment    mu\u0026lt;-x[1];sigma\u0026lt;-x[2]    (-exp(-0.5*((1-mu)/sigma)^2)+exp(-0.5*(mu/sigma)^2))*sigma*0.39894228040143268 + mu * f0(x)}f2 \u0026lt;- function(x){ # Second moment    mu\u0026lt;-x[1];sigma\u0026lt;-x[2]    (-(1+mu)*exp(-0.5*((1-mu)/sigma)^2)+mu*exp(-0.5*(mu/sigma)^2))*sigma*0.39894228040143268 + (mu^2+sigma^2) * f0(x)}f \u0026lt;- function(x,y){ # Discrepancy between first two moments determined by parameters x and target values (y)    mu\u0026lt;-y[1];sigma\u0026lt;-y[2]    c\u0026lt;-f0(x)    (mu-f1(x)/c)^2 + (mu^2+sigma^2-f2(x)/c)^2}fit \u0026lt;- function(mu, sigma) {    # Computes a truncated normal distribution on [0,1] with mean mu and SD sigma    #     (the \"max ent\" solution--but see the comments).     # NB: naively takes (mu,sigma) as starting values.    optim(c(mu,sigma), function(x){f(x,c(mu,sigma))})}As an example of running and checking this code, consider the problem with , :\u0026gt; mu \u0026lt;- .2\u0026gt; sigma \u0026lt;- .1\u0026gt; z \u0026lt;- fit(mu, sigma)\u0026gt; zpar) / f0(zpar) / f0(zpar[1]\u0026gt; tau \u0026lt;- zpar)\u0026gt; curve(exp(-0.5*((x-nu)/tau)^2)/(tau*gamma) * 0.39894, 0, 1)As required, this is a truncated normal distribution with the desired mean and variance.  Shift and rescale it to arbitrary intervals  as needed.  This example was easy--the truncated distribution is close to normal anyway--but you will find that more extreme examples (such as ) work just fine.  (The speed is reasonable, too: on my system fit is taking 4 to 7 milliseconds.)","Display_name":"whuber","Creater_id":919,"Start_date":"2012-01-16 12:59:53","Question_id":21173}
{"_id":{"$oid":"5837a58ca05283111e4d7640"},"Last_activity":"2016-07-27 03:16:22","Creator_reputation":150,"Question_score":2,"Answer_content":"From the help page for fisher.test():  Note that the conditional Maximum Likelihood Estimate (MLE) rather  than the unconditional MLE (the sample odds ratio) is used.","Display_name":"zx8754","Creater_id":6454,"Start_date":"2016-07-27 03:16:22","Question_id":54530}
{"_id":{"$oid":"5837a58ca05283111e4d764d"},"Last_activity":"2016-07-27 03:14:37","Creator_reputation":11,"Question_score":1,"Answer_content":"You are making an error when taking the integral. Indeed,p(\\alpha|\\eta,k) \\neq p(\\alpha|k) = \\int p(\\alpha, \\eta|k) d\\eta.Instead of integrating, you ought to use Bayes' rule; p(\\alpha|\\eta, k) \\propto p(\\alpha, \\eta|k) where the proportionality constant  does not need to be computed since it is constant with respect to . Indeed, we can remove more factors which do not depend on  in your expression of , which yields an answer similar to the one the authors arrive at. (There is a small difference which leads me to suspect that there is a transcription error somewhere.)","Display_name":"Martin L","Creater_id":46058,"Start_date":"2016-07-27 03:14:37","Question_id":225834}
{"_id":{"$oid":"5837a58ca05283111e4d765c"},"Last_activity":"2016-07-27 02:30:20","Creator_reputation":146,"Question_score":0,"Answer_content":"Can PCA be appliedYes. You should only take care, that if your counts are low (say, about 5 plants in a lot) you have to take into account the statistical uncertainty. Since 80% as in 4 out of 5 is not the same as 80% as in 4000 out of 5000. See here for a thread that addresses this: PCA on count-based dataWhich R package can I useSee here: princomp","Display_name":"Ytsen de Boer","Creater_id":109818,"Start_date":"2016-07-27 02:30:20","Question_id":108173}
{"_id":{"$oid":"5837a58da05283111e4d7668"},"Last_activity":"2016-07-27 02:23:25","Creator_reputation":152803,"Question_score":1,"Answer_content":"If the batches differ in complexity then assigning complex batches to high performers is guaranteed to make them do worse. So unless you're accounting for the different difficulty of batches when comparing performance, the next time you go \"oh, they're doing badly\". People are terrible at appropriately adjusting for this via intuition (give them a measure and they'll use it).The point was to measure performance, but if it's used the way you suggest it no longer measures performance, it confounds it with task complexity. If there's some good measure of task complexity/difficulty, it would be possible (e.g. via regression methods or similar) to account for this and still have a measure of performance (by giving a way to have a complexity-adjusted performance). Response to the discussion of the boxplots (using your numbering):\\1. The notches aren't \"weird\"; the interval they give is just further out that at least one of the quartiles (/hinges). That might just by accident be too small a number for you to regard it as reliable, but if you apply some sensible criterion to decide your n is large enough to use, it probably won't really correspond to this -- maybe it would be half as many or twice as many assaying \"the plot looks funny\".\\3. On sample mean, sure. But not on all sensible measures. Note also that these estimates are so noisy that every median (and every mean, though that's not really relevant) is inside Ken's notch-interval, so saying \"Ken is fastest\" is like saying my coin is better than your because when we each tossed 10 times I got 6 heads and you got 4. It's probably just noise.\\4. Nothing about the plot makes 25% \"too many\", though the interpretation that there are at least 25% zeros is the case for everyone but Max.\\6. This is simply a result of the fact that the distribution is reasonably skew. \"In their zone\" doesn't mean anything I can discern but whatever it's intended to mean, this isn't evidence foranything than a count variable with a small mean (especially with a fair chance of a zero) is likely to be pretty right skew. There's no basis for assigning much meaning to that, it's almost certainly a consequence of the thing you're measuring, and the small sample sizes would suggest that anything above \"it's just the nature of the process\" is probably noise.\\8. -- 11. No, it's probably just noise. You don't have nearly enough data here for this level of detailed conclusion. \"In their groove\" doesn't mean anything to me, but whatever it is, this doesn't show it","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-27 02:23:25","Question_id":225137}
{"_id":{"$oid":"5837a58da05283111e4d766d"},"Last_activity":"2016-07-27 02:18:41","Creator_reputation":723,"Question_score":0,"Answer_content":"My guess is that Mahout uses some sort of simplified first-order method for optimisation (similar to the linear perceptron's delta rule) whereas R's classic GLM from statistics uses ML estimation. I'm suspecting this from the fact that they're using the word \"train\" instead of \"estimate\". It would be very uncommon (not to say weird) to use the word \"train\" for OLS or ML estimation. I've also noticed that, when you see terms from classical statistics such as \"linear regression\" and \"logistic regression\" used in a machine learning context, the difference is always in the optimisation process, i.e. estimation vs training (again, a good example is the linear perceptron). Training algorithms in machine learning are supposed to be more robust than parameter estimation methods in parametric statistics because they favour raw computational power over mathematical accuracy, though, they both have their pros and cons. Anyway, I think it's good to pose such questions so that researchers, employers, and employees alike, can finally start understanding the differences between a machine learning expert (specialised computer scientist) and a statistician (specialised mathematician). They definitely have overlapping ground, such as the topics discussed in this question, but they're certainly not overlapping fields. ","Display_name":"Digio","Creater_id":83065,"Start_date":"2016-07-27 02:18:41","Question_id":74295}
{"_id":{"$oid":"5837a58da05283111e4d766e"},"Last_activity":"2014-04-23 23:34:40","Creator_reputation":937,"Question_score":-1,"Answer_content":"My intuition is that your learning rate is too high, which could cause your model to constantly overshoot the minima.Try out your code with small values of rate, maybe 0.05, to begin with and check if it tallies with your R output. A detail that may be relevant is confirming if the model is predicting the probability of 1 or 0. If R is predicting for 1's and Mahout for 0, then you could end up with coefficients of opposite signs.","Display_name":"Arun Jose","Creater_id":30417,"Start_date":"2014-04-23 23:34:40","Question_id":74295}
{"_id":{"$oid":"5837a58da05283111e4d766f"},"Last_activity":"2014-04-23 23:03:19","Creator_reputation":114,"Question_score":2,"Answer_content":"Logistic regression refers to the same thing in both fields.  It seems like Mahout does some things by default that make its implementation of logistic a little more than just logistic.  First, Mahout seems to be regularizing the coefficients.  If its doing this by default, I would also expect it to be standardizing (scaling and centering) the inputs. Passing it a value of lambda=0 should prevent regularization, but you still have to make sure that the inputs are not being standardized.  If you want to do regularized GLM in R check out the glmnet package.  ","Display_name":"Bruno","Creater_id":43863,"Start_date":"2014-04-23 22:40:11","Question_id":74295}
{"_id":{"$oid":"5837a58da05283111e4d767c"},"Last_activity":"2016-07-27 01:50:34","Creator_reputation":20492,"Question_score":2,"Answer_content":"I recently wrote a (hopefully) accessible article to illustrate the problem to a non-technical audience in the context of time series forecasting: Kolassa (2016). \"Sometimes it's better to be simple than correct\", Foresight, 40:20-26.I start by simulating 10,000 monthly time series of length 12 with a known but weak seasonality. The seasonality is easily visible when aggregating data:However, it is invisible in disaggregate data, e.g., in the first five of the 10,000 series:Now, assume that we fit two models to each separate series, a misspecified simple one using only the intercept,y_t = \\beta_0+\\epsilon,and a correct regression model regressing simulated sales on the (known and common) seasonal effects,y_t = \\beta_0+\\beta_1 s_t+\\epsilon.Here are one year ahead forecasts for the first five series from the two models:We see that the seasonal shape is sometimes upside down, simply because of the noise in the simulated series. Here are mean squared errors for both models per holdout month:We see that the misspecified simpler model always has lower errors than the correctly specified more complex one.Finally, here is the connection to bias and variance - these are violin plots of the parameter estimates for both models across the 10,000 series (the dashed line indicates the true parameter values):We notice that the correct model (of course) yields estimates that are unbiased, i.e., they are distributed around the true value. Conversely, the simpler misspecified model has biased parameter estimates: the intercept is on average too high, and the seasonal coefficient is biased low (because it doesn't occur in the model, so it's implicitly always zero, when the true value is one).However, the key point is that the correct model's parameter estimates are far more variable, i.e., their violin plots are spread out a lot more than the corresponding violin plots in the misspecified simple model. And this variance in parameter estimates directly translates into higher forecasting MSEs (whereas the lower bias will reduce MSEs).The bottom line is that when we are interested in forecasting or prediction, we shouldn't only care about the bias of our parameter estimates, but also about their variance. A larger model will (usually) have lower bias, but higher variance, and especially when we fit weak signals with little data, the higher variance may lead to larger forecasting errors. Shrinkage may help, by increasing bias, but reducing variance, hopefully in a way that total error is reduced.","Display_name":"Stephan Kolassa","Creater_id":1352,"Start_date":"2016-07-27 01:50:34","Question_id":225845}
{"_id":{"$oid":"5837a58da05283111e4d768d"},"Last_activity":"2016-07-27 01:42:56","Creator_reputation":3038,"Question_score":0,"Answer_content":"If you use the following notation:  is the growth rate for fish  in tank ,  temperature in tank  and  the mass of fish  then you estimate a regression of the form: using the R-code lmeModel\u0026lt;-lme(sgr ~ mass + temp , random=~1|tank). Note that the intercept can be different for each tank ( has  as subscript). The  can be extracted from lmeModel using the extraction functions fixef and ranef. In fact,  has two components, i.e. , where  is a fixed effect and  a random effect.  The function fixef gived you the maximum likelihood estimator of the fixed effects, the function ranef gived the best linear unbiased predictors (BLUP) of the random effects, you will see that ranef yields one value for each tank. You may also try a model like , where the coefficient  also depends on the tank, using the code lmeModel\u0026lt;-lme(sgr ~ mass + temp , random=~1+temp|tank)Using the R-code lmeModel\u0026lt;-lme(sgr ~ mass + temp , random=~1|tank/temp) is similar to the first model, only that in the the first case you assume a different intercept for each tank while in the model with random=~1|tank/temp you assume a different intercept for each temperature within each tank. So it is something like , where T in  stands for temperature. However, if you treat temperature as a factor (i.e.  a categorical variable) then I think it does not make much difference. EDIT 27-07-2016:About the question in your comment;  if temperature is categorical, having four values, then this categorical variable is replaces by three (the number of categories minus 1) dummy variables, . In that case lmeModel\u0026lt;-lme(sgr ~ mass + temp , random=~1|tank) estimates  while lmeModel\u0026lt;-lme(sgr ~ mass + temp , random=~1+temp|tank) estimates ","Display_name":"fcop","Creater_id":83346,"Start_date":"2016-07-22 01:16:00","Question_id":225036}
{"_id":{"$oid":"5837a58da05283111e4d769a"},"Last_activity":"2015-10-28 09:49:38","Creator_reputation":1673,"Question_score":18,"Answer_content":"These are not very strict terms, and for sure - they are extremely correlated. However, there are slight differences:loss function is usually a function defined on a data point, your prediction and label, and measures the penalty. For examplesquare loss , used in linear regressionhinge loss , used in SVM0/1 loss , used in theoretical analysis and definition of accuracycost function is usually a big more general object, it might for example be a sum of loss functions over your training set plus some model complexity penalty (regularization). For exampleMean Squared Error SVM cost function  (there are additional constraints connecting  with  and with training set)objective function is the most general term, which might by any function that you optimize during training, for example a probability of generating training set in maximum likelihood approach is a well defined objective function, but it is not a loss function nor cost function (however you could define an equivalent cost function). For example:MLE is a type of objective function (which you minimize)Divergence between classes can be an objective function (while it is barely a cost function, unless you define something artificial, like 1-Divergence, and name it a cost)In short words, I would say that loss function is a part of a cost function which is a type of an objective function","Display_name":"lejlot","Creater_id":28903,"Start_date":"2015-10-25 16:01:04","Question_id":179026}
{"_id":{"$oid":"5837a58da05283111e4d769b"},"Last_activity":"2015-10-28 01:58:49","Creator_reputation":251,"Question_score":1,"Answer_content":"According to Prof. Andrew Ng (see slides on page 11),Function h(X) represents your hypothesis. For fixed fitting parameters theta, it is a function of features X. I'd say this can also be called the Objective Function.The Cost function J is a function of the fitting parameters theta. J = J(theta).According to the Hastie et al.'s textbook \"Elements of Statistical Learning\", by p.37:  \"We seek a function f (X) for predicting Y given values of the input  X.\" [...] the  loss function L(Y, f(X)) is \"a function for penalizing the  errors in prediction\",So it seems \"loss function\" is a slightly more general term than \"cost function\". If you seek for \"loss\" in that PDF, I think that they use \"cost function\" and \"loss function\" somewhat synonymously.Indeed, p. 502  \"The situation [in Clustering] is somewhat similar to the specification  of a loss or cost function in prediction problems (supervised  learning)\".Maybe these terms exist because they evolved  independently in different academic communities. \"Objective Function\" is an old term used in Operations Research, and Engineering Mathematics. \"Loss function\" might be more in use among statisticians. But I'm speculating here.","Display_name":"knb","Creater_id":20107,"Start_date":"2015-10-28 01:48:11","Question_id":179026}
{"_id":{"$oid":"5837a58da05283111e4d76a8"},"Last_activity":"2016-07-27 01:04:50","Creator_reputation":25650,"Question_score":2,"Answer_content":"You are right. In fact, since proper Poisson model would be incorrect in here because of dealing with continuous outcome, you'll be using quasi-Poisson model.It is called quasi-likelihood and was described for the first time by Wedderburn (1974):  To define a likelihood we have to specify the form of distribution of  the observations, but to define a quasi-likelihood function we need  only specify a relation between the mean and variance of the  observations and the quasi-likelihood can then be used for estimation.You can find some more description and examples in paper by McCullagh (1983) and handbooks on GLM's.In case of quasi-Poisson model, the quasi-likelihood is  y \\log \\mu - \\mu where  and  as described in McCullagh (1983).Wedderburn, R. W. (1974). Quasi-likelihood functions, generalized linear models, and the Gauss—Newton method. Biometrika, 61(3), 439-447.McCullagh, P. (1983). Quasi-likelihood functions. The Annals of Statistics, 59-67.","Display_name":"Tim","Creater_id":35989,"Start_date":"2016-07-27 00:27:40","Question_id":225825}
{"_id":{"$oid":"5837a58da05283111e4d76b5"},"Last_activity":"2016-07-27 00:44:45","Creator_reputation":431,"Question_score":0,"Answer_content":"Here is a thought how one could go about this: Structural Equation Model Trees are model-based trees with parametric multivariate normal distributions as outcomes in each leaf. There is an R-implementation called semtree based on OpenMx (http://openmx.psyc.virginia.edu) model specifications. The package is freely available here: http://brandmaier.de/semtree/.OpenMx has tutorials on how to implement mixture models. So, in principle, it should work to specify a mixture model in OpenMx and pass it to the semtree()-function for recursive partitioning.Here is another thought: When we assume that there is sample heterogeneity w.r.t. the original model, then both methods are addressing this issue differently. Trees recursively split the sample to find homogeneous subgroups based on observed predictors of heterogeneity whereas mixture models provide a probabilistic approach to estimate latent subgroups. Wouldn't it make sense to first run a tree to reduce sample heterogeneity and, in a second step, run mixture models in each leaf of the final tree to uncover the remaining heterogeneity that was not \"explained away\" in the first step? And here are some references to SEM trees and forests:Brandmaier, A. M., Prindle, J. J., McArdle, J. J., \u0026amp; Lindenberger, U. (in press). Theory-guided exploration with structural equation model forests. Psychological Methods.Brandmaier, A. M., von Oertzen, T., McArdle, J. J., \u0026amp; Lindenberger, U. (2014). Exploratory data mining with structural equation model trees. In J. J. McArdle \u0026amp; G. Ritschard (Eds.), Contemporary issues in exploratory data mining in the behavioral sciences (pp. 96-127). New York: Routledge.Brandmaier, A. M., von Oertzen, T., McArdle, J. J., \u0026amp; Lindenberger, U. (2013). Structural equation model trees. Psychological Methods, 18, 71-86. doi: 10.1037/a0030001","Display_name":"Brandmaier","Creater_id":20352,"Start_date":"2016-07-27 00:44:45","Question_id":221989}
{"_id":{"$oid":"5837a58da05283111e4d76c2"},"Last_activity":"2016-07-27 00:41:58","Creator_reputation":14835,"Question_score":4,"Answer_content":"(The answer below merely introduces and states the theorem proven in[0].  The beauty in that paper is that most of the arguments are made in terms of basic linear algebra. To answer this question it will be enough to state the main results but by all mean, go check the original source).In any situation where the multivariate pattern of the data can be described by a  variate elliptical distribution, statistical inference will, by definition, reduce to the problem of fitting   (and characterizing) a  variate location vector (say ) and a  by  symmetric semi-positive definite matrix (say ) to the data. For reasons I explain below (but which you already assume as premises) it will often be more meaningful to decompose  into a shape component (a SPSD matrix of the same size as ) accounting for the shape of the density contours of your multivariate distribution and a scalar  expressing the scale of these contours. In univariate data (), , the covariance matrix of your data is a scalar and, as will follow from the discussion below, the shape component of  is 1 so that  equals its scale component  always and no ambiguity is possible.In multivariate data, many choice of scaling functions  are possible. One in particular () stands out in having a key desirable propriety. This should make it the preferred choice of scaling factor in the context of elliptical families. Many problems in MV statistics involve estimation of a scatter matrix, defined as a function(al) symmetric semi positive definite in  and satisfying:(0)\\quad\\boldsymbol\\varSigma(\\boldsymbol A\\boldsymbol X+\\boldsymbol b)=\\boldsymbol A\\boldsymbol\\varSigma(\\boldsymbol X)\\boldsymbol A^\\top(for non singular matrices  and vectors ). For example the classical estimate of covariance satisfies (0) but it is by no means the only one.   In the presence of elliptical distributed data, where all the density contours are ellipses defined by the same shape matrix, up to multiplication by a scalar, it is natural to consider normalized versions of  of the form:\\boldsymbol V_S = \\boldsymbol\\varSigma / S(\\boldsymbol\\varSigma) where  is a 1-honogenous function satisfying:(1)\\quad S(\\lambda \\boldsymbol\\varSigma)=\\lambda S(\\boldsymbol\\varSigma) for all . Then,  is called the shape component of the scatter matrix (in short shape matrix) and  is called the scale component of the scatter matrix. Examples of multivariate estimation problems where the loss function only depends on  through its shape component  include tests of sphericity, PCA and CCA among others. Of course, there are many possible scaling functions so this still leaves the open the question of what (if any) of several choices of normalization function  is in some sense optimal. For example: (for example the one proposed by @amoeba in his comment below the OP's question. See also [1], [2], [3]) ([4], [5], [6], [7], [8]) (the first entry of the covariance matrix) (the first eigenvalue of )However,  is the only  scaling function for which the Fisher Information matrix for the corresponding estimates of scale and shape, in locally asymptotically normal families, are block diagonal (that is the scale and shape components of the estimation problem are asymptotically orthogonal) [0]. This means, among other things, that the scale functional  is the only choice of  for which the non specification of  does not cause any loss of efficiency when performing inference on .I do not know of any comparably strong optimality characterization for any of the many possible choices of  that satisfy (1). [0] Paindaveine, D., A canonical definition of shape, Statistics \u0026amp; Probability Letters, Volume 78, Issue 14, 1 October 2008, Pages 2240-2247. Ungated link[1] Dumbgen, L. (1998). On Tyler’s M-functional of scatterin high dimension, Ann. Inst. Statist. Math. 50, 471–491.[2] Ollila, E., T.P. Hettmansperger, and H. Oja (2004). Affine  equivariant multivariate sign methods. Preprint, University of Jyvaskyla.[3]  Tyler,  D.E. (1983).  Robustness and efficiency properties of scatter matrices, Biometrika 70, 411–420.[4] Dumbgen, L., and D.E. Tyler (2005). On the breakdown properties of some multivariate M-Functionals, Scand. J. Statist.32, 247–264.[5] Hallin, M. and D. Paindaveine (2008). Optimal rank-based tests for homogeneity of scatter, Ann. Statist., to appear.[6] Salibian-Barrera, M., S. Van Aelst, and G. Willems (2006). Principal components analysis based on multivariate MM-estimators with fast and robust bootstrap, J. Amer. Statist. Assoc. 101, 1198–1211.[7] Taskinen, S., C. Croux, A. Kankainen, E. Ollila, and H. Oja (2006). Influence functions and efficiencies of the canonical correlation and vector estimates based on scatter and shape matrices,J. Multivariate Anal. 97, 359–384.[8] Tatsuoka, K.S., and D.E. Tyler (2000). On the uniqueness of S-Functionals and M-functionals under nonelliptical distributions, Ann. Statist. 28, 1219–1243.","Display_name":"user603","Creater_id":603,"Start_date":"2016-07-25 13:25:42","Question_id":225434}
{"_id":{"$oid":"5837a58da05283111e4d76c3"},"Last_activity":"2016-07-25 19:17:05","Creator_reputation":2865,"Question_score":0,"Answer_content":"The entropy concept from information theory seems to suit the purpose, as a measure of unpredictability of information content, which is given byH(X)=-\\int p(x)\\log p(x) dx.If we assume a multivariate Gaussian distribution for  with mean  and covariance  derived from the data, according to wikipedia, the differential entropy is then,H(X)=\\frac{1}{2}\\log((2\\pi e)^n\\det(\\Sigma))where  is the number of dimensions. Since multivariate Gaussian is the distribution that maximizes the differential entropy for given covariance, this formula gives an entropy upper bound for an unknown distribution with a given variance.And it depends on the determinant of the covariance matrix, as @user603 suggests.","Display_name":"dontloo","Creater_id":95569,"Start_date":"2016-07-25 06:28:47","Question_id":225434}
{"_id":{"$oid":"5837a58da05283111e4d76d0"},"Last_activity":"2016-07-26 08:08:11","Creator_reputation":76,"Question_score":2,"Answer_content":"Use the likelihood ratio test. Twice the difference in log likelihood (larger minus smaller) will be approximately  distributed in this case. Generally, the degrees on freedom equals the number of additional predictors in the larger model. ","Display_name":"not_bonferroni","Creater_id":117710,"Start_date":"2016-07-26 08:08:11","Question_id":225713}
{"_id":{"$oid":"5837a58da05283111e4d76df"},"Last_activity":"2016-07-26 22:49:37","Creator_reputation":8367,"Question_score":1,"Answer_content":"The argument is effectively the butterfly effect: any one tiny historical detail might have indirectly somehow changed, e.g., the exact text of Oliver Twist. So nearly everything that has happened is effectively part of the data-generating process.While I think there is some merit to this argument, it applies just as well to traditional low-dimensional data like the approval ratings of US presidents, so I don't think it says anything special about high-dimensional data like images.","Display_name":"Kodiologist","Creater_id":14076,"Start_date":"2016-07-26 22:49:37","Question_id":225819}
{"_id":{"$oid":"5837a58da05283111e4d76f0"},"Last_activity":"2016-07-26 19:59:21","Creator_reputation":152803,"Question_score":1,"Answer_content":"There's no particular inherent difficulty with small samples other than the restriction of actual/attainable significance levels.For example, if your sample sizes are  and  and you had  then you could not get a two-tailed significance level below 10%The case for  is better -- you can attain a significance level of 1/35, or roughly 3%.For sample sizes of 4 and 5 you can get 1/63 and at sample sizes of 5 and 5 you can get a significance level as low as 1/126 (below 1%)There's a table in this answer -- but that's one tailed. For two tailed you need to double those fractions.The numbers I have given so far is assuming no ties -- if there are any ties, the attainable significance levels will be substantially worse.Of course the other issue is power -- not that low power invalidates a test per se -- it's still a valid test in that it does what it says on the box, it just may not be any help. Specifically power may be very poor at such small sample sizes. But if all you need is a test you can carry out at some given significance level and the low power isn't a primary issue, then it's perfectly valid to use two samples of size 3 if you are happy to work at  (and with samples that small, insisting on a 5% significance level wouldn't make much sense to me anyway).With sample sizes both at least 4 and no ties, you should be able to go about your business with no problems (though you are stuck with only a few choices for significance level).At samples of size (4,5) and higher, attainable significance levels in the absence of ties are better; there's even a convenient attainable level just below 5% if that's what you need.I see no good reason for the calculator to stop you going below 5. It may be that the calculator isn't using the exact distribution of the test statistic in which case that may impose a higher bound. I suggest using something better than an online calculator for your statistical analysis. (There are free stats programs available and some are very good.)","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-26 19:50:26","Question_id":225698}
{"_id":{"$oid":"5837a58da05283111e4d76ff"},"Last_activity":"2016-07-26 18:14:00","Creator_reputation":14,"Question_score":0,"Answer_content":"Apply a transformation function to the data. The same transformation must be applied to all the samples. For example taking the logarithm of both sample values can lower the skew to the right.Use the ladder of powers if log does not work.3 cube2 square1 no transformation1/2 square root1/3 cube root0 log-.5 reciprocal root-1 reciprocal-2 reciprocal squared If the data is right skewed try functions from the bottom.If the data is left skewed try function from the top.Good luck","Display_name":"MikeB0317","Creater_id":124508,"Start_date":"2016-07-26 18:14:00","Question_id":225812}
{"_id":{"$oid":"5837a58da05283111e4d770c"},"Last_activity":"2016-07-26 17:50:59","Creator_reputation":1,"Question_score":0,"Answer_content":"Typically you will have a regression model looks like this:Y = \\beta_{0} + \\beta_{1}X + \\epsilon where  is an error term independent of . If  and  are known, we still cannot perfectly predict Y using X due to . Therefore, we use RSE as an judgement value of Standard Deviation of . RSE is explained pretty much clearly in \"Introduction to Stat Learning\". ","Display_name":"newbiettn","Creater_id":124637,"Start_date":"2016-07-26 17:50:59","Question_id":57746}
{"_id":{"$oid":"5837a58da05283111e4d770d"},"Last_activity":"2015-10-13 14:45:51","Creator_reputation":834,"Question_score":9,"Answer_content":"Say we have the following ANOVA table (adapted from R's example(aov) command):          Df Sum Sq Mean Sq F value Pr(\u0026gt;F)Model      1   37.0   37.00   0.483  0.525Residuals  4  306.3   76.57               If you divide the sum of squares from any source of variation (model or residuals) by its respective degrees of freedom, you get the mean square. Particularly for the residuals:\\frac{306.3}{4} = 76.575 \\approx 76.57So 76.57 is the mean square of the residuals, i.e., the amount of residual (after applying the model) variation on your response variable.The residual standard error you've asked about is nothing more than the positive square root of the mean square error. In my example, the residual standard error would be equal to , or approximately 8.75. R would output this information as \"8.75 on 4 degrees of freedom\".","Display_name":"Waldir Leoncio","Creater_id":27433,"Start_date":"2015-10-13 08:12:24","Question_id":57746}
{"_id":{"$oid":"5837a58da05283111e4d770e"},"Last_activity":"2013-04-30 14:57:13","Creator_reputation":17499,"Question_score":12,"Answer_content":"A fitted regression model uses the parameters to generate point estimate predictions which are the means of observed responses if you were to replicate the study with the same  values an infinite number of times (and when the linear model is true). The difference between these predicted values and the ones used to fit the model are called \"residuals\" which, when replicating the data collection process, have properties of random variables with 0 means. The observed residuals are then used to subsequently estimate the variability in these values and to estimate the sampling distribution of the parameters. When the residual standard error is exactly 0 then the model fits the data perfectly (likely due to overfitting). If the residual standard error can not be shown to be significantly different from the variability in the unconditional response, then there is little evidence to suggest the linear model has any predictive ability.","Display_name":"AdamO","Creater_id":8013,"Start_date":"2013-04-30 14:57:13","Question_id":57746}
{"_id":{"$oid":"5837a58da05283111e4d771b"},"Last_activity":"2016-07-26 17:48:13","Creator_reputation":5218,"Question_score":1,"Answer_content":"It sounds like you want to treat the day-to-day variations as random noise and compare different sites against each other. For that, you can investigate one way analysis of variance. Common charts are box plots, bars with error bars and points with means diamonds shown here:","Display_name":"xan","Creater_id":1191,"Start_date":"2016-07-26 17:48:13","Question_id":225294}
{"_id":{"$oid":"5837a58da05283111e4d7728"},"Last_activity":"2016-07-26 17:08:13","Creator_reputation":14,"Question_score":0,"Answer_content":"R values range from -1 to 1. A value of -1 indicates a completely negative correlation, and 1 indicates a completely positive correlation. The distribution for the correlation coefficient is not normally distributed, and its variance is not constant. Fisher's transformation attempts to solve this problem, which allows us to compute the confidence interval. Determining correlation and no correlation would depend on your confidence interval. This interval defines a range that estimates that values of r that are likely to contain unknown. This is useful because it provides a range of potential values for r given an unknown. Otherwise you are simply left with a number from -1 to 1, and you have to make a personal decision if it correlated. Is .5 a correlation, what about .4? I would consider that something interesting when examining data, but its not enough to definitely say yes this is the golden relationship. Confidence interval gives us a mathematical way to say yes or no under given criteria which is what is lacking in the correlation equation. The CI is calculated using the mean and standard deviation. For variance uses size and sample size and for this, CI cannot be calculated directly. Fisher's transformation provides an indirect calculation that can be be made for CI. This is why most of the Fischer problems from your stats books solve for or give CI. ","Display_name":"MikeB0317","Creater_id":124508,"Start_date":"2016-07-26 17:08:13","Question_id":225760}
{"_id":{"$oid":"5837a58da05283111e4d7729"},"Last_activity":"2016-07-26 12:53:28","Creator_reputation":1219,"Question_score":1,"Answer_content":"\"No correlation\" means r=0.00000000000000000... that is almost never the case and never of practical interest. You could define, how small an r is \"as small as to be zero for practical purposes\". Computing correlation indices in statistics software will give you a confidence interval for the correlation coefficient. If you say, that abs(r)\u0026lt;.10 you can compute the confidence interval and see, if it includes only values that meet that requirement.","Display_name":"Bernhard","Creater_id":117812,"Start_date":"2016-07-26 12:53:28","Question_id":225760}
{"_id":{"$oid":"5837a58da05283111e4d7736"},"Last_activity":"2016-07-26 17:38:43","Creator_reputation":152803,"Question_score":1,"Answer_content":"The canonical link function is not chosen because it's logical (or \"successful\" whatever that means in this case) for any particular purpose (though in many cases it works out that way) -- it relates to a specific feature of the way the mean parameter appears in the density function.If a link function doesn't suit what you do, you shouldn't use it.In particular, if  is not linear in  ... you have the wrong link.  The reciprocal link function in exponential regression doesn't constrain to positive values, It does within the range of the data but can go outside it if you extrapolate.If it's important to avoid, you might consider whether a log-link is a better choice.  (a) Regarding the lack of constraint of results to non-negative predictions, my thinking is that, as long as we choose an initial trial parameter vector that results in a non-negative linear predictor, the chances of ending up with negative predictions is negligible since the sheer number of points in the neighbourhood of zero will make the predictions very accurate in that region.Yes, in practice, it usually isn't a problem.  (b) I am not clear as to how transforming values close to zero to be spread out over the range of the reciprocal function makes the regression more successful. I can see that it reduces leverage compared with an ordinary regression to fit the means, but is there something deeper connected with it being easier to fit a linear function to points that are reasonably far apart?    keen to get some more understanding around what makes a good link function over and above that it constrains predictions to appropriate values.Well, usually the main criterion is that the linear predictor is linear; if you have only factors for predictors that's obviously not a direct concern (I'd think more about things like interactions -- a good choice of link may make the model additive in the main effects which has some advantages).However, getting feasible predictions in some part of the range may be important. In some cases this indicates a problem with the model (you may in fact know that the  at some special value ofr the set of predictors,  for example, which suggests you should incorporate  that knowledge in the model. In other cases you want it to produce only positive predictions for any possible set of 's in which case you might look at different links (or indeed to some other way of imposing the constraint).","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-07-26 17:38:43","Question_id":225755}
{"_id":{"$oid":"5837a58da05283111e4d7743"},"Last_activity":"2016-07-26 16:20:27","Creator_reputation":41,"Question_score":4,"Answer_content":"Aaron Han (1987 in econometrics) proposed the Maximum Rank Correlation estimator that fits regression models by maximizing tau. Dougherty and Thomas (2012 in the psychology literature) recently proposed a very similar algorithm. There is an abundance of work on the MRC illustrating its properties.Aaron K. Han, Non-parametric analysis of a generalized regression model: The maximum rank correlation estimator, Journal of Econometrics, Volume 35, Issues 2–3, July 1987, Pages 303-316, ISSN 0304-4076, http://dx.doi.org/10.1016/0304-4076(87)90030-3.(http://www.sciencedirect.com/science/article/pii/0304407687900303)Dougherty, M. R., \u0026amp; Thomas, R. P. (2012). Robust decision making in a nonlinear world. Psychological review, 119 (2), 321. Retrieved from http://damlab.umd.edu/pdf%20articles/DoughertyThomas2012Rev.pdf.","Display_name":"rankman","Creater_id":53153,"Start_date":"2014-07-30 19:49:50","Question_id":64938}
{"_id":{"$oid":"5837a58da05283111e4d7744"},"Last_activity":"2015-07-20 09:12:15","Creator_reputation":152803,"Question_score":20,"Answer_content":"There's a very straightforward means by which to use almost any correlation measure to fit linear regressions, and which reproduces least squares when you use the Pearson correlation.Consider that if the slope of a relationship is , the correlation between  and  should be expected to be . Indeed, if it were anything other than , there'd be some uncaptured linear relationship - which is what the correlation measure would be picking up.We might therefore estimate the slope by finding the slope,  that makes the sample correlation between  and  be . In many cases -- e.g. when using rank-based measures -- the correlation will be a step-function of the value of the slope estimate, so there may be an interval where it's zero. In that case we normally define the sample estimate to be the center of the interval. Often the step function jumps from above zero to  below zero at some point, and in that case the estimate is at the jump point.This definition works, for example, with all manner of rank based and robust correlations. It can also be used to obtain an interval for the slope (in the usual manner - by finding the slopes that mark the border between just significant correlations and just insignificant correlations).This only defines the slope, of course; once the slope is estimated, the intercept can be based on a suitable location estimate computed on the residuals . With the rank-based correlations the median is a common choice, but there are many other suitable choices.Here's the correlation plotted against the slope for the car data in R:The Pearson correlation crosses 0 at the least squares slope, 3.932The Kendall correlation crosses 0 at the Theil-Sen slope, 3.667The Spearman correlation crosses 0 giving a \"Spearman-line\" slope of 3.714  Those are the three slope estimates for our example. Now we need intercepts. For simplicity I'll just use the mean residual for the first intercept and the median for the other two (it doesn't matter very much in this case):           intercept Pearson:  -17.573 *      Kendall:  -15.667 Spearman: -16.285*(the small difference from least squares is due to rounding error in the slope estimate; no doubt there's similar rounding error in the other estimates)The corresponding fitted lines (using the same color scheme as above) are:Edit: By comparison, the quadrant-correlation slope is 3.333Both the Kendall correlation and Spearman correlation slopes are substantially more robust to influential outliers than least squares. See here for a dramatic example in the case of the Kendall.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2014-07-31 03:20:50","Question_id":64938}
{"_id":{"$oid":"5837a58da05283111e4d7745"},"Last_activity":"2013-07-20 05:42:49","Creator_reputation":39376,"Question_score":19,"Answer_content":"The proportional odds (PO) model generalizes Wilcoxon and Kruskal-Wallis tests.  Spearman's correlation when  is binary is the Wilcoxon test statistic simply translated.  So you could say that the PO model is a unifying method.  Since the PO model can have as many intercepts as there are unique values of  (less one), it handles both ordinal and continuous .The numerator of the score  statistic in the PO model is exactly the Wilcoxon statistic.The PO model is a special case of a more general family of cumulative probability (some call cumulative link) models including the probit, proportional hazards, and complementary log-log models.  For a case study see Chapter 15 of my Handouts.","Display_name":"Frank Harrell","Creater_id":4253,"Start_date":"2013-07-20 05:42:49","Question_id":64938}
{"_id":{"$oid":"5837a58da05283111e4d7751"},"Last_activity":"2016-07-26 16:05:51","Creator_reputation":14,"Question_score":0,"Answer_content":"it is a bit difficult to answer your question here is my best try. Frequency in this definition is nothing other than the rate of occurrence. The frequency has an inverse relationship to the period. For example say the frequency of an event is 60. This means that the event will happen 60 times a minute, the period is then nothing other than 1/60 or that once every seconds an event will occur.In your case and since mentioned the use of senors I am assuming that these data sensors are connected to a micro controller. Similar to your computer's GHz micro-controllers also have a baud rate, which is simply how fast the internal clock is ticking. The baud rate is the rate that the information is transferred to the device. If my assumption is correct you need to determine the speed at which your device operates, and set the baud rate (or frequency) to mathematically be set to the once per minute requirement. 9600 is a typical baud rate. This means that 9600 bits are sent per second, or every 1/9600 one bit is sent across the communication channel.Set your frequency / baud rate to the proper value assuming my 9600 assumption, and size of your data reading.Google baud rate equations, determine the data bus speed, and solve for the correct value given the conditions.I hope this helped!","Display_name":"MikeB0317","Creater_id":124508,"Start_date":"2016-07-26 16:05:51","Question_id":225793}
{"_id":{"$oid":"5837a58da05283111e4d775e"},"Last_activity":"2016-07-25 06:37:07","Creator_reputation":147868,"Question_score":6,"Answer_content":"From the scatterplot it looks like the data are independent.  (There are many ways to test this: consult information on time series analysis and autocorrelations.)  Assuming this is the case, the chance of any value in a set of  values (not just the very last one) being smallest does not depend on when the value occurred.  Consequently all values have an equal chance of being smallest.  Assuming there are no ties for smallest, that chance must be  so that all chances sum to unity, as probabilities must.As an application, you have  values.  The next one will bring  up to .  Therefore the chance that it is the smallest is .Incidentally, this rule implies the expected number of new minima among such a sequence is approximately .  The illustrated data appear to have  new minima (plotted as orange points) and indeed .  A quick simulation (taking ten seconds or so) gives us a sense of the distribution of numbers of new minima, which might be of some interest (and perhaps help inform the intuition).This is the R code to produce such a simulation.N \u0026lt;- 1e4sim \u0026lt;- apply(matrix(runif(11475 * N), ncol=N), 2, function(x) {  y \u0026lt;- cummin(x)  n \u0026lt;- length(y)  sum(y[-1] != y[-n])})hist(sim, breaks=seq(min(sim)-1/2, max(sim)+1/2, by=1),     xlab=\"# new minima\", main=\"Histogram of Simulated Results\")","Display_name":"whuber","Creater_id":919,"Start_date":"2016-07-25 06:21:46","Question_id":225511}
{"_id":{"$oid":"5837a58da05283111e4d776b"},"Last_activity":"2016-07-26 15:14:03","Creator_reputation":1,"Question_score":0,"Answer_content":"A binomial tree has two branches each with a probably of 0.5. Actually, p=0.5, and q=1-0.5=0.5. This generates a normal distribution with an evenly distributed probability mass. Actually, we have to assume that each tier in the tree is complete. When we break data up into bins, we get a real number from the division, but we round up. Well, that's a tier that is incomplete, so we don't end up with a histogram approximating the normal. Change the branching probabilities to p=0.9999 and q=0.0001 and that gets us a skewed normal. The probability mass shifted. That accounts for skewness. Having incomplete tiers or bins less than 2^n generate binomial trees with areas that have no probability mass. This gives us kurtosis. Response to comment:When I was talking about determining the number of bins, round up to the next integer. Quincunx machines drop balls that come to eventually approximate the normal distribution via the binomial. Several assumptions are made by such a machine: 1) the number of bins is finite, 2) the underlying tree is binary, and 3) the probabilities are fixed. The Quincunx machine at the Museum of Mathematics in New York, lets the user dynamically change the probabilities. The probabilities can change at any time, even before the current layer is finished. Hence this idea about the bins not being filled. Unlike what I said in my original answer when you have a void in the tree, the distribution demonstrates kurtosis. I'm looking at this from the perspective of generative systems. I use a triangle to summarize decision trees. When a novel decision is made, more bins are added at the base of the triangle, and in terms of the distribution, in the tails. Trimming subtrees from the tree would leave voids in the distribution's probability mass. I only replied to give you an intuitive sense. Labels? I've used Excel and played with the probabilities in the binomial and generated the expected skews. I have not done so with kurtosis, it doesn't help that we are forced to think about probability mass as being static while using language suggesting movement. The underlying data or balls cause the kurtosis. Then, we analyze it variously and attribute it to shape descriptive terms like center, shoulder, and tail. The only things we have to work with are the bins. Bins live dynamic lives even if the data can't. ","Display_name":"David Locke","Creater_id":124439,"Start_date":"2016-07-25 07:29:14","Question_id":132914}
{"_id":{"$oid":"5837a58da05283111e4d776c"},"Last_activity":"2016-04-13 09:03:25","Creator_reputation":5797,"Question_score":4,"Answer_content":"This is a bit of an old thread, but i wish to correct a misstatement in the comment by Fg Nu who wrote \"Moments are parameterized by the natural numbers, and completely characterize a distribution\".Moments do NOT completely characterize a distribution. Specifically, knowledge of all infinite number of moments, even if they exist, does not necessarily uniquely determine the distribution.Per my favorite probability book, Feller \"An Introduction to Probability Theory and Its Applications Vol II\" (see my answer at Real-life examples of common distributions ), section VII.3 example on pp. 227-228, the Lognormal is not determined by its moments, meaning that there are other distributions having all infinite number of moments the same as the Lognormal, but different distribution functions.  As is widely known, the Moment Generating Function does not exist for the Lognormal, nor can it for these other distributions possessing the same moments.As stated on p. 228, an essentially nonzero random variable  is determined by its moments if they all exist and\\sum_{n=1}^{\\infty} (\\mathbb{E}[X^{2n}])^{-1/(2n)}diverges. Note that this is not an if and only if.  This condition does not hold for the Lognormal, and indeed it is not determined by its moments.On the other hand, distributions (random variables) which share all infinite number of moments, can only differ by so much, due to inequalities which can be derived from their moments.","Display_name":"Mark L. Stone","Creater_id":78964,"Start_date":"2015-06-15 09:53:32","Question_id":132914}
{"_id":{"$oid":"5837a58da05283111e4d776d"},"Last_activity":"2015-01-11 08:48:04","Creator_reputation":1285,"Question_score":12,"Answer_content":"It's been a long time since I took a physics class, so let me know if any of this is incorrect.General description of moments with physical analogsTake a random variable, . The -th moment of  around  is:m_n(c)=E[(X-c)^n]This corresponds exactly to the physical sense of a moment. Imagine  as a collection of points along the real line with density given by the pdf. Place a fulcrum under this line at  and start calculating moments relative to that fulcrum, and the calculations will correspond exactly to statistical moments.Most of the time, the -th moment of  refers to the moment around 0 (moments where the fulcrum is placed at 0):m_n=E[X^n]The -th central moment of  is:\\hat m_n=m_n(m_1) =E[(X-m_1)^n]This corresponds to moments where the fulcrum is placed at the center of mass, so the distribution is balanced. It allows moments to be more easily interpreted, as we'll see below. The first central moment will always be zero, because the distribution is balanced.The -th standardized moment of  is:\\tilde m_n = \\dfrac{\\hat m_n}{\\left(\\sqrt{\\hat m_2}\\right)^n}=\\dfrac{E[(X-m_1)^n]}{\\left(\\sqrt{E[(X-m_1)^2]}\\right)^n}Again, this scales moments by the spread of the distribution, allowing for easier interpretation specifically of Kurtosis. The first standardized moment will always be zero, the second will always be one. This corresponds to the moment of the standard score (z-score) of a variable. I don't have a great physical analog for this concept.Commonly used momentsFor any distribution there are potentially an infinite number of moments. Enough moments will almost always fully characterize and distribution (deriving the necessary conditions for this to be certain is a part of the moment problem). Four moments are commonly talked about a lot in statistics:Mean - the 1st moment (centered around zero). It is the center of mass of the distribution, or alternatively it's proportional to the moment of torque of the distribution relative to a fulcrum at 0.Variance - the 2nd central moment. Interpreted as representing the degree to which the distribution of  is spread out. It corresponds to the moment of inertia of a distribution balanced on its fulcrum.Skewness - the 3rd central moment (sometimes standardized). A measure of the skew of a distribution in one direction or another. Relative to a normal distribution (which has no skew), positively skewed distribution have a low probability of extremely high outcomes, negatively skewed distributions have a small probability of extremely low outcomes. Physical analogs are difficult, but loosely it measures the asymmetry of a distribution. As an example, the figure below is taken from Wikipedia.Kurtosis - the 4th standardized moment, usually excess Kurtosis, the 4th standardized moment minus three. Kurtosis measures the extent to which  places more probability on the center of the distribution relative to the tails. Higher Kurtosis means less frequent larger deviations from the mean and more frequent smaller deviations. It is often interpreted relative to the normal distribution, which has a 4th standardized moment of 3, hence an excess Kurtosis of 0. Here a physical analog is even more difficult, but in the figure below, taken from Wikipedia, the distributions with higher peaks have greater Kurtosis.We rarely talk about moments beyond Kurtosis, precisely because there is very little intuition to them. This is similar to physicists stopping after the second moment.","Display_name":"jayk","Creater_id":52022,"Start_date":"2015-01-10 21:08:23","Question_id":132914}
{"_id":{"$oid":"5837a58da05283111e4d776e"},"Last_activity":"2015-01-10 11:27:45","Creator_reputation":141,"Question_score":2,"Answer_content":"A corollary to Glen_b's remarks is that the first moment, the mean, corresponds to the center of gravity for a physical object, and the second moment around the mean, the variance, corresponds to its moment of inertia.  After that, you're on your own.","Display_name":"Mike Anderson","Creater_id":37819,"Start_date":"2015-01-10 11:27:45","Question_id":132914}
{"_id":{"$oid":"5837a58da05283111e4d777d"},"Last_activity":"2016-07-26 15:06:58","Creator_reputation":1486,"Question_score":0,"Answer_content":"I assume your cards are sampled without replacement, although if sampled with replacement the maths wouldn't be very different (but results may be very different).I'll go in two steps: First I will try to find the probability of getting 13 cards with different numbers or figures, since you stated that a repeated number or figure breaks an strictly increasing sequence. Second, I'l try the probability of a sample of 13 different cards get well ordered.For the first card, we can chose any of our 52 cards. For the second, we have to chose a different number, so we only have 48 out of 51 remaining cards to chose, for the third we have 44 out of 50, and so, until we have 4 out of 40 for the last one. That is:P(\\text{13 different numbers}) =  \\frac{52\\times48\\times44\\times40\\times36\\times32\\times28\\times24\\times20\\times16\\times12\\times8\\times4}{52\\times51\\times50\\times49\\times48\\times47\\times46\\times45\\times44\\times43\\times42\\times41\\times40} = \\frac{4\\times13!}{52\\times51\\times50\\times49\\times48\\times47\\times46\\times45\\times44\\times43\\times42\\times41\\times40}For the second part, there is only a way to order 13 different numbers (or cards) to have an increasing sequence, and therefore the probability of getting such a sequence will be of  on .Then:P(\\text{13 cards in strictly increasing sequence}) = \\frac{4\\times13!}{52\\times51\\times50\\times49\\times48\\times47\\times46\\times45\\times44\\times43\\times42\\times41\\times40}\\times\\frac{1}{13!}=\\frac{4}{52\\times51\\times50\\times49\\times48\\times47\\times46\\times45\\times44\\times43\\times42\\times41\\times40} And that is about .","Display_name":"Pere","Creater_id":123561,"Start_date":"2016-07-26 15:06:58","Question_id":223614}
{"_id":{"$oid":"5837a58da05283111e4d778d"},"Last_activity":"2016-07-26 14:35:42","Creator_reputation":611,"Question_score":2,"Answer_content":"The (conditional) marginal distribution can be computed from the (conditional) joint distribution: p(\\alpha|k) = \\int p(\\alpha,\\eta|k)\\,d\\eta.Therefore, ifp(\\alpha,\\eta|k) = c\\,p(\\alpha)\\,\\alpha^{k-1}\\,(\\alpha+n)\\,\\eta^{\\alpha}\\,(1-\\eta)^{n-1},where  is an appropriate constant, thenp(\\alpha|k) = c\\,p(\\alpha)\\,\\alpha^{k-1}\\,(\\alpha+n)\\,\\int\\eta^{\\alpha}\\,(1-\\eta)^{n-1}\\,d\\eta.Since  is a dummy variable, you can rename it .","Display_name":"mef","Creater_id":1047,"Start_date":"2016-07-26 14:35:42","Question_id":225745}
{"_id":{"$oid":"5837a58da05283111e4d779a"},"Last_activity":"2016-07-26 14:23:51","Creator_reputation":152803,"Question_score":2,"Answer_content":"Economists have some rules of thumb they use ... but they don't call it Savitzky-Golay (who wrote their paper in 1964), they call it Hodrick-Prescott (1997). [Actuaries call it Whittaker-Henderson graduation (the usual reference for Whittaker being his 1923 book, and for Henderson, papers in 1916 and 1924), so they do somewhat better at naming it for the right people. But it's basically just discrete smoothing splines, where derivatives of some order are replaced with differences of some order.]However, those rules that are used for Hodrick-Prescott filtering are based on some assumptions and approximations and don't necessarily compare well with smoothing parameters chosen in other ways.One thing you might do to choose a smoothing parameter is some form of crossvalidation (if you have an implementation that deals with missingness); if your primary aim is to forecast you could substitute mean square one-step ahead prediction error.However, it's not hard to estimate degrees of freedom consumed by the fit; taking the approach in Ye (1998) [1] of summing the partial derivatives of the fit with respect to the observations  to obtain model d.f. (a definition which is consistent with our usual notions when we apply that to cases where our usual notions work).In this particular case, this calculation is easily written as the trace of a matrix. If you can write  - which is easy in this case - then model df is the trace of .(You could argue for it from more basic considerations but this is a fairly intuitive way to look at it to my mind, and generalizes to a very wide array of methods for getting a some fit.)[1] Ye, J. (1998),\"On measuring and correcting the effects of data mining and model selection.\"J Am Statist Assoc., 93, pp120–31.","Display_name":"Glen_b","Creater_id":805,"Start_date":"2016-04-06 22:29:47","Question_id":205955}
{"_id":{"$oid":"5837a58da05283111e4d77ad"},"Last_activity":"2016-07-26 13:23:28","Creator_reputation":76495,"Question_score":3,"Answer_content":"I wonder if you really want McNemar's test (more specifically the McNemar-Bowker test).  These are tests to see if the marginal proportions are the same (see here).  Under the assumption that pairings can be recovered from the orders of the two vectors, here is a table of your data with the marginal proportions computed:    bef = c(4, 3, 4, 5, 4, 4, 4, 5, 3)aft = c(5, 4, 3, 5, 5, 4, 5, 5, 4)table(bef, aft)#    aft# bef 3 4 5#   3 0 2 0#   4 1 1 3#   5 0 0 2table(bef)/sum(table(bef))# bef#         3         4         5 # 0.2222222 0.5555556 0.2222222 table(aft)/sum(table(aft))# aft#         3         4         5 # 0.1111111 0.3333333 0.5555556 Since you state that your variables are ordered, it seems more appropriate to assess if one set has higher values than the other.  That would use the Wilcoxon signed rank test:  wilcox.test(bef, aft, paired=TRUE)#   Wilcoxon signed rank test with continuity correction# # data:  bef and aft# V = 3.5, p-value = 0.1294# alternative hypothesis: true location shift is not equal to 0Because you have ties, you cannot use the exact version of the test; you use the asymptotic approximation. If that bothers you due to the small sample size, you could simulate the null and compute the p-value that way.  There are different ways to do that, but I'm not sure how much difference they will make in practice.  Bootstrapping is generally not recommended with small samples.  You could try a permutation-based version, or you could do a Monte-Carlo simulation of the null based on the marginal distribution of the values (ignoring timepoint).  ","Display_name":"gung","Creater_id":7290,"Start_date":"2016-07-26 12:58:53","Question_id":225732}
{"_id":{"$oid":"5837a58da05283111e4d77ae"},"Last_activity":"2016-07-26 10:38:36","Creator_reputation":3734,"Question_score":1,"Answer_content":"It is almost certainly because you have a pair of cells which both have zeroes (3, 5) and (5, 3). If you look at the way the test is defined that will lead to problems.What to do is a problem. You could always merge categories but that may not make scientific sense. I think you may have to program your own solution. You could define a log-linear model of quasi-symmetry without including the offending cells (treating them as structural zeroes) but that may not be a sensible assumption either. More data would help but I suppose that may not be an option.","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-07-26 09:53:47","Question_id":225732}
{"_id":{"$oid":"5837a58da05283111e4d77bb"},"Last_activity":"2016-07-26 13:17:21","Creator_reputation":17914,"Question_score":5,"Answer_content":"Lasso is only useful if you're restricting yourself to consider models which are linear in the parameters to be estimated. Stated another way, the lasso does not evaluate whether you have chosen the correct form of the relationship between the independent and dependent variable(s).It is very plausible that there may be nonlinear, interactive or polynomial effects in an arbitrary data set. However, these alternative model specifications will only be evaluated if the user conducts that analysis; the lasso is not a substitute for doing so.For a simple example of how this can go wrong, consider a data set in which disjoint intervals of the independent variable will predict alternating high and low values of the dependent variable. This will be challenging to sort out using conventional linear models, since there is not a linear effect in the manifest variables present for analysis (but some transformation of the manifest variables may be helpful). Left in its manifest form, the lasso will incorrectly conclude that this feature is extraneous and zero out its coefficient because there is no linear relationship. On the other hand, because there are axis-aligned splits in the data, a tree-based model like a random forest will probably do pretty well. ","Display_name":"Sycorax","Creater_id":22311,"Start_date":"2016-07-26 07:55:43","Question_id":7935}
{"_id":{"$oid":"5837a58da05283111e4d77bc"},"Last_activity":"2013-12-03 13:27:41","Creator_reputation":14047,"Question_score":4,"Answer_content":"I am not a LASSO expert but I am an expert in time series. If you have time series data or spatial data then I would studiously avoid a solution that was predicated on independent observations. Furthermore if there are unknown deterministic effects that have played havoc with your data (level shifts / time trends etc) then LASSO would be even less a good hammer. In closing when you have time series data you often need to segment the data when faced with parameters or error variance that change over time. ","Display_name":"IrishStat","Creater_id":3382,"Start_date":"2013-11-27 05:14:31","Question_id":7935}
{"_id":{"$oid":"5837a58da05283111e4d77bd"},"Last_activity":"2013-12-03 13:01:49","Creator_reputation":15694,"Question_score":15,"Answer_content":"LASSO encourages shrinking of coefficients to 0, i.e. dropping those variates from your model. On contrast, other regularization techniques like a ridge tend to keep all variates. So I'd recommend to think about whether this dropping makes sense for your data. E.g. consider setting up a clinical diagnostic test either on gene microarray data or on vibrational spectroscopic data.You'd expect some genes to carry relevant information, but lots of other genes are just noise wrt. your application. Dropping those variates is a perfectly sensible idea.By contrast, vibrational spectroscopic data sets (while usually having similar dimensions compared to microarray data) tend to have the relevant information \"smeared\" over large parts of the spectrum (correlation). In this situation, asking the regularization to drop variates is not a particularly sensible approach. The more so, as other regularization techniques like PLS are more adapted to this type of data.The Elements of Statistical Learning gives a good discussion of the LASSO, and contrasts it to other regularization techniques.","Display_name":"cbeleites","Creater_id":4598,"Start_date":"2013-11-28 09:35:22","Question_id":7935}
{"_id":{"$oid":"5837a58da05283111e4d77be"},"Last_activity":"2013-11-28 08:07:11","Creator_reputation":18720,"Question_score":11,"Answer_content":"If two predictors are highly correlated LASSO can end up dropping one rather arbitrarily. That's not very good when you're wanting to make predictions for a population where those two predictors aren't highly correlated, \u0026amp; perhaps a reason for preferring ridge regression in those circumstances.You might also think standardization of predictors (to say when coefficients are \"big\" or \"small\") rather arbitrary \u0026amp; be puzzled (like me) about sensible ways to standardize categorical predictors.","Display_name":"Scortchi","Creater_id":17230,"Start_date":"2013-11-28 07:33:38","Question_id":7935}
{"_id":{"$oid":"5837a58da05283111e4d77bf"},"Last_activity":"2013-11-27 11:26:33","Creator_reputation":892,"Question_score":17,"Answer_content":"If you only care about prediction error and don't care about interpretability, casual-inference, model-simplicity, coefficients' tests, etc, why do you still want to use linear regression model? You can use something like boosting on decision trees or support vector regression and get better prediction quality and still avoid overfitting in both mentioned cases. That is Lasso may not be the best choice to get best prediction quality.If my understanding is correct, Lasso is intended for situations when you are still  interested in the model itself, not only predictions. That is - see selected variables and their coefficients, interpret in some way etc. And for this - Lasso may not be the best choice in certain situations as discussed in other questions here.","Display_name":"Kochede","Creater_id":31774,"Start_date":"2013-11-27 08:00:54","Question_id":7935}
{"_id":{"$oid":"5837a58da05283111e4d77c0"},"Last_activity":"2011-03-06 17:58:47","Creator_reputation":57792,"Question_score":18,"Answer_content":"There is NO reason to do stepwise selection.  It's just wrong.LASSO/LAR are the best automatic methods.  But they are automatic methods.  They let the analyst not think.In many analyses, some variables should be in the model REGARDLESS of any measure of significance.  Sometimes they are necessary control variables.  Other times, finding a small effect can be substantively important.","Display_name":"Peter Flom","Creater_id":686,"Start_date":"2011-03-06 17:58:47","Question_id":7935}
{"_id":{"$oid":"5837a58da05283111e4d77c1"},"Last_activity":"2011-03-06 17:38:38","Creator_reputation":3016,"Question_score":0,"Answer_content":"One big one is the difficulty of doing hypothesis testing.  You can't easily figure out which variables are statistically significant with Lasso.  With stepwise regression, you can do hypothesis testing to some degree, if you're careful about your treatment of multiple testing.","Display_name":"dsimcha","Creater_id":1347,"Start_date":"2011-03-06 17:38:38","Question_id":7935}
{"_id":{"$oid":"5837a58da05283111e4d77c2"},"Last_activity":"2011-03-06 17:23:32","Creator_reputation":520,"Question_score":6,"Answer_content":"One practical disadvantage of lasso and other regularization techniques is finding the optimal regularization coefficient, lambda. Using cross validation to find this value can be just as expensive as stepwise selection techniques. ","Display_name":"rm999","Creater_id":2965,"Start_date":"2011-03-06 17:23:32","Question_id":7935}
{"_id":{"$oid":"5837a58da05283111e4d77cf"},"Last_activity":"2016-07-26 13:02:13","Creator_reputation":30045,"Question_score":5,"Answer_content":"Here is what Hastie et al. have to say about it (in context of two-class LDA) in The Elements of Statistical Learning, section 4.3:  Since this derivation of the LDA direction via least squares does not use a  Gaussian assumption for the features, its applicability extends beyond the  realm of Gaussian data. However the derivation of the particular intercept  or cut-point given in (4.11) does require Gaussian data. Thus it makes  sense to instead choose the cut-point that empirically minimizes training  error for a given dataset. This is something we have found to work well in  practice, but have not seen it mentioned in the literature.I don't fully understand the derivation via least squares they refer to, but in general [Update: I am going to summarize it briefly at some point] I think that this paragraph makes sense: even if the data are very non Gaussian or class covariances are very different, the LDA axis will probably still yield some discriminability. However, the cut-point on this axis (separating two classes) given by LDA can be completely off. Optimizing it separately can substantially improve classification.Notice that this refers to the classification performance only. If all you are after is dimensionality reduction, then the LDA axis is all you need. So my guess is that for dimensionality reduction LDA will often do a decent job even if the assumptions are violated.Regarding rLDA and QDA: rLDA has to be used if there are not enough data points to reliably estimate within-class covariance (and is vital in this case). And QDA is a non-linear method, so I am not sure how to use it for dimensionality reduction.","Display_name":"amoeba","Creater_id":28666,"Start_date":"2014-08-06 15:46:25","Question_id":110908}
{"_id":{"$oid":"5837a58da05283111e4d77de"},"Last_activity":"2016-07-26 12:46:49","Creator_reputation":15862,"Question_score":6,"Answer_content":"This is fairly straight-foward.  The log ratio of the densities is equal to: \\log\\left (\\frac {f_1}{f_2}\\right)=x\\log\\left (\\frac {\\lambda_1}{\\lambda_2}\\right)+\\lambda_2-\\lambda_1Then you take expectation of this expression wrt , which simply replaces  with its expectation (in this case).   So you have: D_{KL} (f_1||f_2)=\\lambda_1\\log\\left (\\frac {\\lambda_1}{\\lambda_2}\\right)+\\lambda_2-\\lambda_1","Display_name":"probabilityislogic","Creater_id":2392,"Start_date":"2015-04-10 21:20:01","Question_id":145789}
{"_id":{"$oid":"5837a58da05283111e4d77eb"},"Last_activity":"2016-07-26 12:46:26","Creator_reputation":19763,"Question_score":3,"Answer_content":"Epsilon is the local radius for expanding clusters. Think of it as a step size - DBSCAN never takes a step larger than this, but by doing multiple steps DBSCAN clusters can become much larger than eps.If you want your \"clusters\" to have a maximum radius, that is a setucover type of problem, so you will probably want a greedy appeoximation. It's not a clustering problem, because you do not allow the clustering algorithm to discover structure larger than that. You want to approximate your data with a cover, ignoring structure.","Display_name":"Anony-Mousse","Creater_id":7828,"Start_date":"2016-07-26 12:46:26","Question_id":225655}
{"_id":{"$oid":"5837a58da05283111e4d77ec"},"Last_activity":"2016-07-26 02:43:40","Creator_reputation":8958,"Question_score":2,"Answer_content":"The meaning of  is that of the neighbourhood size. The neighbourhood of a point , denoted by , is defined as the . Here  is a database of  objects (points) and  a query point. is what would be constitute a reasonable radius for your particular problem. For example when looking to cluster cities tens of kilometres is probably reasonable. See also this post. Yes, I guess  seems like a reasonable first estimate. I would probably try something bigger first but this does not seems horribly misplaced. Let me point out that choosing your distance metric is probably more important than your  in a way. You can also re-run your analysis with a different  and see the influence of it but your insights will be tied directly to the distance metric used.","Display_name":"usεr11852","Creater_id":11852,"Start_date":"2016-07-26 02:43:40","Question_id":225655}
{"_id":{"$oid":"5837a58da05283111e4d77f9"},"Last_activity":"2016-07-26 12:39:23","Creator_reputation":538,"Question_score":0,"Answer_content":"If your data is normally distributed -- you can analyze a number of ways, including a QQ Plot -- then it is fine to run a t-test. But, in order to make the least number of assumptions about the data it is best to use the non-parametric Wilcoxon Signed Rank test.Due to the fact that you have very few samples (24) I would advise going the Wilcoxon Signed Rank path. I would thoroughly analyze this question because it appears to answer a lot on necessary questions. Be sure to understand exactly how the type I error and the power behaves in your test.","Display_name":"a.powell","Creater_id":119338,"Start_date":"2016-07-26 12:39:23","Question_id":225733}
{"_id":{"$oid":"5837a58da05283111e4d7806"},"Last_activity":"2016-07-26 12:34:05","Creator_reputation":13106,"Question_score":2,"Answer_content":"I will focus on ARMAX versus VAR. I am not quite sure what a dynamic regression is. (I have seen a few different interpretations. Funnily, there are textbooks and lecture notes with chapters called \"Dynamic regression\" that do not really delimit this class of models. Also, Rob J. Hyndman notes in his blog post \"The ARIMAX model muddle\" that different books use that term for different models).An ARMAX model has the form y_t = \\beta x_t + \\varphi_1 y_{t-1} + \\dotsc + \\varphi_p y_{t-p} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dotsc + \\theta_q \\varepsilon_{t-q} (one could also have more than one exogenous variable and/or lags of exogenous variables in the above equation.)The dependent variable is a univariate time series. The model cannot be used for forecasting  unless one has the future values of the independent variable  available, or has a separate model for predicting .The model is estimated using maximum likelihood (slow), often using a state space representation.Allowing for both AR and MA terms offers a parsimonious representation of the process.A VAR model has the form z_t = \\varphi_1 z_{t-1} + \\dotsc + \\varphi_p z_{t-p} + \\varepsilon_t where  is a vector; for example, .The dependent variable is a multivariate time series. The model can be used for forecasting all components of , e.g. for . Given data up to and including time , forecasts for time  are straightforward to obtain; forecasts for  where  can be obtained iteratively.The model can be estimated using OLS or GLS (fast).Lack of MA terms may (or may not) require large AR order to approximate the process well, and large AR order means a large number of parameters to be estimated and thus high estimation variance. Fortunately, regularization (shrinkage) applies pretty straightforwardly to VAR models (unlike ARMAX), so the variance can be tamed.  [H]ow do we decide when to use which[?]It depends on your intentions and the data at hand. If you need fast estimation and direct applicability to forecasting, try a VAR. If you need a parsimonious representation, try ARMAX. Also, ARMAX and VAR could be combined to obtain the VARIMAX model that has a multivariate dependent variable, does allow for forecasting of all of its components but also takes a long time to estimate, is prone to convergence problems and is difficult to regularize.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-26 10:08:14","Question_id":225597}
{"_id":{"$oid":"5837a58da05283111e4d7807"},"Last_activity":"2016-07-26 07:17:02","Creator_reputation":14047,"Question_score":-1,"Answer_content":"ARMAX models are more general that VAR and Dynamic Regressive . You can see ARMAX in action here https://twitter.com/tomdireill/status/717694028104474625 where the GASX/GASY example from the Box-Jenkins text is analyzed/","Display_name":"IrishStat","Creater_id":3382,"Start_date":"2016-07-26 07:17:02","Question_id":225597}
{"_id":{"$oid":"5837a58da05283111e4d7818"},"Last_activity":"2016-07-26 12:18:52","Creator_reputation":6097,"Question_score":1,"Answer_content":"If it is a known error in both  and , you can subtract errors from both  and  to get  and , and then fit the regression on  and .So, lets say there is a known constant error of  in your  and a known constant error of  in your . (These are signed errors). Then you define and , and fit the regression on  and . The model would then beY' = a + bX' + \\epsilon, where  is the intercept,  is the slope and  is the random error with zero mean. Just to see how this regression related back to the old one,Y - c_y = a + b(X - c_x) + \\epsilon \\Rightarrow Y = c_y + a - bc_x + bX + \\epsilon. Thus we get that if you fit the regression on the original  and , then the slope is unaffected by the constant errors and the -intercept changes according to the constants.","Display_name":"Greenparker","Creater_id":31978,"Start_date":"2016-07-26 08:01:18","Question_id":225706}
{"_id":{"$oid":"5837a58da05283111e4d7825"},"Last_activity":"2016-07-26 12:16:46","Creator_reputation":13106,"Question_score":0,"Answer_content":"You could predict the independent variables using separate models or expert forecasts (the latter should be available for variables of such broad interest as income, inflation and interest rates). Then you could use them in the fitted ARMAX model to predict sales.Alternatively, you could model the variables together using a vector autoregression (VAR) (or a vector error correction model -- a version of VAR suited for cointegrated variables). This would allow forecasting all of the variables within one model. One-step-ahead forecasts from a VAR are straightforward, while multiple-step-ahead forecasts can be constructed iteratively.See this answer for a comparison of ARMAX and VAR models.  Can I create a model using lagged values of the dependent and independent variables, used together in a regression model?Yes, you could also do that within an ARMAX or a VAR framework to forecast the dependent variable directly using lags of itself and the other variables. For -step-ahead forecasts you would need to use lags of at least  to enable direct forecasting.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-26 12:11:01","Question_id":225578}
{"_id":{"$oid":"5837a58da05283111e4d7832"},"Last_activity":"2016-07-26 12:11:37","Creator_reputation":176,"Question_score":2,"Answer_content":"Don't worry, this is a common misunderstanding. Let's consider this.   If I flip a fair coin 10 times, and all of them land on heads,  according to the Law of Large Numbers I am more likely to get tails on  the next flip. However, this is clearly the Gambler's Fallacy.     How am I misinterpreting the Law of Large Numbers?The part I bolded is where the issue is, since the follow-up - that this is clearly the Gambler's Fallacy - is the correct angle. The Law of Large Numbers states that   the average of the results obtained from a large number of trials  should be close to the expected value, and will tend to become closer  as more trials are performed.The first issue is kind of a footnote - that ten trials isn't near enough to qualify for the \"large number.\" (How large the large number has to be depends on the distribution - in the case of the binomial coin toss, it's roughly 30.) There is a deeper issue, though - even if you got the coin to land on heads one million times in a row, the odds of the next toss are still 50/50. How do we reconcile this? The Law of Large Numbers' stock-in-trade is in ratios. Over huge numbers, the ratio of \"interesting events::possible outcomes\" approximates the probability of an interesting event happening on any single occasion. For a concrete example - it's actually plausible that you flip a coin one million times as heads more than tails in the long run, where your ratio is something like 100.1:100 million (heads:tails). The \"Big Idea\" here is that, when you run a huge number of trials, that large number in the denominator will dilute the freak ten-heads streaks that appear and otherwise substantial numbers (say, 1,000,000) on either side of the scale matter a lot less to the ratio between them. (For further reading, Jordan Ellenberg does a fantastic job explaining this early in his book, How Not to be Wrong.)","Display_name":"Non-Contradiction","Creater_id":123196,"Start_date":"2016-07-26 12:11:37","Question_id":225765}
{"_id":{"$oid":"5837a58da05283111e4d7843"},"Last_activity":"2016-07-24 11:43:28","Creator_reputation":1900,"Question_score":0,"Answer_content":"Overfitting occurs when you have too few records relative to other parameters (e.g., predictors or features). I'm not familiar with your data, but it sounds like the subsetting is creating additional records. ","Display_name":"Ryan Zotti","Creater_id":8401,"Start_date":"2016-07-24 11:43:28","Question_id":225380}
{"_id":{"$oid":"5837a58da05283111e4d7850"},"Last_activity":"2016-07-26 10:43:39","Creator_reputation":13106,"Question_score":1,"Answer_content":"It depends on what you want to do with the model.If you want to do forecasting, AIC will asymptotically select the model with the smallest squared forecast error (AIC is said to be efficient). If AIC suggests lag 0, you would be best off just estimating the intercept and skipping any lags.If you want to find the true model (and if the true model is in the pool of your candidate models), BIC will asymptotically find it (BIC is said to be consistent). If BIC suggests lag 0, probably your data was not generated by a VAR model.If you want to test some hypothesis, build a model within which this is feasible, and do it (e.g. with the likelihood ratio test).If the selection by the information criteria or the test result are counterintuitive, probably you have reasons to blame the sample (too small, not really representative)? Then you could stick with what your intuition suggests until you collect more and better data. Or you could do Bayesian VAR modelling where you can incorporate your intuition directly into the modelling procedure via specifying a prior distribution for the model parameters.Or probably the pool of candidate models is poor? Maybe the true model is VAR(5) with all but the 5th lag coefficients being zeros, but you do not have such a model in your pool, while a full unrestricted VAR(5) has high estimation variance and thus poor AIC/BIC/LR statistic?Then you could reconsider the pool of models you are using.Or you could use regularized estimation (shrinkage) applied on a relatively large model to strike a good balance between flexibility and overfitting.","Display_name":"Richard Hardy","Creater_id":53690,"Start_date":"2016-07-26 10:38:37","Question_id":225624}
{"_id":{"$oid":"5837a58da05283111e4d7861"},"Last_activity":"2015-10-28 09:15:44","Creator_reputation":121,"Question_score":2,"Answer_content":"In one of my article on Perceptron, I tried to explain Supervised Learning. I have also explained Unsupervised Learning and Reinforcement Learning with example. The article is written for Novices. But you can check out and see if that helps.\"An Intuitive Example of Artificial Neural Network (Perceptron) Detecting Cars / Pedestrians from a Self-driven Car\"http://www.spicelogic.com/Journal/Perceptron-Artificial-Neural-Networks-10[]2","Display_name":"Emran Hussain","Creater_id":93383,"Start_date":"2015-10-28 00:42:43","Question_id":144154}
{"_id":{"$oid":"5837a58da05283111e4d786e"},"Last_activity":"2016-07-26 10:57:48","Creator_reputation":17499,"Question_score":2,"Answer_content":"The Venables Ripley book discusses periodic splines. Basically, by specifying (correctly) the periodicity, the data are aggregated into replications over a period and splines are fit to interpolate the trend. For instance, using the AirPassengers dataset from R to model flight trends, I might use a categorical fixed effect for annual effects and a spline to interpolate the residual monthly trends. My spline interpolation is arguably a bad one, but finding a good fitting spline is another topic altogether :) This example is perhaps a bit more useful because it deals with averaging out other auto-regressive trends.My from-scratch method fits the periodic spline with a discontinuity at the end-point, but one could easily address this by duplicating these data over two periods and fitting the spline to the central half. matplot(matrix(log(AirPassengers), ncol=12), type='l', axes=F, ylab='log(Passengers)', xlab='Month')axis(1, at=1:12, labels=month.abb)axis(2)box()title('Monthly air passenger data 1949:1960')ap \u0026lt;- data.frame('lflights'= log(c(AirPassengers)), month=month.abb, year=rep(1949:1960, each=12))apmonth, month.abb)apresidualsmatplot(matrix(apmonthly.pred \u0026lt;- lm(monthly.diff~bs(month.n, degree=2, knots = c(5)), data=ap)monthly.pred[1:12], lwd=2)","Display_name":"AdamO","Creater_id":8013,"Start_date":"2016-07-26 10:57:48","Question_id":225729}
{"_id":{"$oid":"5837a58da05283111e4d787b"},"Last_activity":"2016-07-26 10:34:10","Creator_reputation":646,"Question_score":3,"Answer_content":"Yes you can (as long as your weights are integers (fractional to be pedantic)), though it's obviously not very efficient.To see this, note that most loss functions can be written as \\text{loss}(y, p) = \\sum_{i=1}^n l(y_i, p_i)where  is the predicted value of  for a suitable function .We can easily transform this to a weighted loss function by introducing weights:\\text{weighted loss}(y, p) = \\sum_{i=1}^n w_i l(y_i, p_i)Now we see that if we duplicate each observation   times, and minimize the (unweighted) loss, that this is equivalent to minimizing the weighted loss with weights . Of course duplicating something  times is difficult so make sure your weights are integers.Note that adding a regularization penalty to the loss function does not have any effects on this reasoning.","Display_name":"Sven","Creater_id":67254,"Start_date":"2016-07-26 10:28:51","Question_id":222768}
{"_id":{"$oid":"5837a58da05283111e4d788c"},"Last_activity":"2016-07-26 10:24:19","Creator_reputation":1900,"Question_score":0,"Answer_content":"After the input layer, depth is simply the number of filters. From the offial Stanford course materials:   \"The depth of the output volume is a hyperparameter: it corresponds to  the number of filters we would like to use.\"Source: http://cs231n.github.io/convolutional-networks/","Display_name":"Ryan Zotti","Creater_id":8401,"Start_date":"2016-07-26 06:24:47","Question_id":225686}
{"_id":{"$oid":"5837a58da05283111e4d7899"},"Last_activity":"2016-07-26 10:05:51","Creator_reputation":33236,"Question_score":5,"Answer_content":"The short answer is \"You cannot prove causation using only a statistical test\".Statistical tests can provide illumination and support, but tests are only as good as their conditions and assumptions and that requires knowledge outside of the data an any specific tests.  Even if tests can rule out other possible causes, there could always be an additional unmeasured \"cause\" that would invalidate the tests, knowledge beyond the observed data and tests can speak to the reasonableness of these unmeasured causes.Some places to start learning more (these are starting places, look at the references and links, search on terms from these pages, etc.):https://en.wikipedia.org/wiki/Rubin_causal_modelhttps://en.wikipedia.org/wiki/Causality#Statistics_and_economics","Display_name":"Greg Snow","Creater_id":4505,"Start_date":"2016-07-26 10:05:51","Question_id":225731}
{"_id":{"$oid":"5837a58da05283111e4d78a8"},"Last_activity":"2016-07-26 09:47:30","Creator_reputation":1040,"Question_score":0,"Answer_content":"Kullback-Leibler (and other information theoretic divergences, the -divergences) is linked to the Fisher Information: KL is locally an approximation of the Fisher-Rao geodesic distance, which is a Riemannian metric parameterized by the Fisher Information matrix. It has the property to be the only Riemannian metric (up to a multiplicative factor) which is invariant to reparameterization of the parameter space (in case of a parametric distribution).Also, and this is important here, the divergence KL (and -divergences) can be seen as diverging by the 'right' amount with respect to 'information' / statistical uncertainty: if you have a parametric distribution, paramaterized by , and you estimate  with an unbiased estimator, then the Cramer-Rao lower bound tells you that , where  is the Fisher Information matrix parameterizing the KL divergence .Information Geometry is the field which investigates this kind of questions.If you require a symmetry, you either symmetrized KL, it is called the Jeffreys divergence, or use Hellinger (which is also a -divergence and a proper metric distance).","Display_name":"mic","Creater_id":67168,"Start_date":"2016-07-26 09:47:30","Question_id":225730}
{"_id":{"$oid":"5837a58da05283111e4d78b5"},"Last_activity":"2016-07-26 09:44:28","Creator_reputation":3734,"Question_score":2,"Answer_content":"1 - Yes they are the same model parameterised differently2 depends on which is easiest to interpret3 - in the second case each of the eight cells is compared against the corner cell AA:XX. In the first case AB and BB are compared with AA (and similarly for XY, YY with XX) and then four extra terms tell you whether any of those cells needs an extra, so AB:XY is extra over and above what you would have expected from it being an AB and an XY. So for instance the predicted value for cell AB:XY is the intercept plus AB plus XY plus AB:XY","Display_name":"mdewey","Creater_id":101426,"Start_date":"2016-07-26 07:51:39","Question_id":225697}
{"_id":{"$oid":"5837a58da05283111e4d78c6"},"Last_activity":"2016-07-26 09:05:37","Creator_reputation":147868,"Question_score":5,"Answer_content":"This question might be of interest to statisticians and data scientists because of the multidimensional array handling issues it presents.Loops aren't the problem.  Your goal likely is to perform the operation efficiently, regardless of how it might be done, and perhaps to do it in an obviously parallelizable way.Exploit R's ability to manage multidimensional arrays.  (It inherits this from FORTRAN, which even in its original design handled up to seven dimensions.)  Move groups of data en masse into an array and then reshape it as needed.  These native operations will be fast, even if some looping needs to be done.Although it uses short high-level loops (each executing  times), the following solution will create a \"block Toeplitz\" array with  and  blocks--an array with four million elements--within a half second and small use of overhead storage.  It is based on the observation that the Toeplitz pattern can be constructed from the stringU_n, U_{n-1}, \\ldots, U_1, U_0, U_1^\\prime, U_2^\\prime, \\ldots, U_n^\\primeby taking the last  elements to form the first column of output, shifting those elements to the left to form the second column of output, and so on, until the last column of output is formed from the first  elements of this string.The code uses n to represent  (because R indexing starts at 1 instead of 0).  The string of matrices itself, as a  array, is stored in a temporary array strip.At the end, the  array that is created by \"blasting\" parts of the strip down the columns of the output X is permuted to get its values stored in the right order (I was too lazy to work out the optimal storage configuration at the outset) and then recast as a  array (which involves no movement of data).  These column-wise operations are each highly vectorized and capable of massive parallelism.The initial value of U contains obvious patterns to check that the output is correct.## Create n square matrices of dimension k by k.#U \u0026lt;- list(matrix(1:4, 2), matrix(5:8, 2), matrix(9:12, 2))#U \u0026lt;- lapply(1:1000, function(i) matrix(-3:0 + 4*i, 2))system.time({  k \u0026lt;- min(unlist(lapply(U, dim)))  n \u0026lt;- length(U)  #  # Create the \"strip\".  #  strip \u0026lt;- array(NA, dim=c(k,k,2*n-1))  for (i in 1:n) strip[,,i] \u0026lt;- U[[n+1-i]]  if (n \u0026gt; 1) for (i in 2:n) strip[,,n+i-1] \u0026lt;- t(U[[i]])  #  # Assemble into \"block-Toeplitz\" form.  #  X \u0026lt;- array(NA, dim=c(k,k,n,n))  #  # Blast the strip across X.  #  for (i in 1:n) X[,,,i] \u0026lt;- strip[,,(n+1-i):(2*n-i)]  X \u0026lt;- matrix(aperm(X, c(1,3,2,4)), n*k)})","Display_name":"whuber","Creater_id":919,"Start_date":"2016-07-26 09:05:37","Question_id":171342}
{"_id":{"$oid":"5837a58da05283111e4d78c7"},"Last_activity":"2016-07-26 07:52:40","Creator_reputation":11,"Question_score":1,"Answer_content":"The following function that takes as argument a list of blocks. However, this may be not the best solution: there still one loop present. And it needs some more work, since it doesn't do the transpose of the blocks lower diagonal (in my case I have symmetric matrices). toeplitz.block \u0026lt;- function(blocks) {    l \u0026lt;- length(blocks)    m.str \u0026lt;- toeplitz(1:l)    res \u0026lt;- lapply(1:l,function(k) {        res \u0026lt;- matrix(0,ncol=ncol(m.str),nrow=nrow(m.str))        res[m.str == k] \u0026lt;- 1        res %x% blocks[[k]]    })    Reduce(\"+\",res)}  ","Display_name":"a.grochmal","Creater_id":124568,"Start_date":"2016-07-26 07:28:14","Question_id":171342}
{"_id":{"$oid":"5837a58da05283111e4d78d3"},"Last_activity":"2016-07-26 08:56:28","Creator_reputation":329,"Question_score":3,"Answer_content":"The last statement you have \\begin{equation}\\frac{(\\alpha+n)\\beta(\\alpha+1,n)}{\\alpha} = \\beta(\\alpha,n)\\end{equation}is true if you expand  as  then follow @ArtificialBreeze's suggestion. Here it is\\begin{split}\\frac{(\\alpha+n)\\beta(\\alpha+1,n)}{\\alpha} \u0026amp; = \\frac{(\\alpha+n)\\color{red}{\\Gamma(\\alpha+1)}\\Gamma(n)}{\\alpha\\color{blue}{\\Gamma(\\alpha+1+n)}} \\\\\u0026amp;= \\frac{(\\alpha+n)\\color{red}{[\\alpha\\times\\Gamma(\\alpha)]}\\Gamma(n)}{\\alpha\\color{blue}{[(\\alpha+n)\\Gamma(\\alpha+n)]}} \\\\\\end{split}Cancelling out terms will result in , which is what you have above.","Display_name":"Ye Tian","Creater_id":82624,"Start_date":"2016-07-26 08:46:21","Question_id":225721}
{"_id":{"$oid":"5837a58da05283111e4d78e2"},"Last_activity":"2016-07-26 02:49:24","Creator_reputation":15571,"Question_score":1,"Answer_content":"Your code/general approach is pretty hard to follow. Here's how I might do this, which may contain the ingredients for your own solution:clear allwebuse cattaneo2, clear/* Define Your Own BS Program */capture program drop mybsprogram define mybs, rclasspsmatch2 mbsmoke mmarried c.mage##c.mage fbaby medu, outcome(bweight) atereturn scalar att = r(att)return scalar ate = r(ate)endbootstrap  att = r(att) ate = r(ate), reps(500) saving(\"PSM_ATEs.dta\", replace): mybsestat bootstrap, all/* For Comparison */psmatch2 mbsmoke mmarried c.mage##c.mage fbaby medu, outcome(bweight) ateYou might also consider using Stata's own matching command, which accounts for the estimation of the PS in its standard errors (so no bootstrapping is needed):teffects psmatch (bweight) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), atetteffects psmatch (bweight) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), ate","Display_name":"Dimitriy V. Masterov","Creater_id":7071,"Start_date":"2016-07-26 02:49:24","Question_id":225629}
{"_id":{"$oid":"5837a58da05283111e4d78f1"},"Last_activity":"2016-07-26 08:06:39","Creator_reputation":404,"Question_score":1,"Answer_content":"If you want a coefficient of an interaction term that is common to species and years, your code is correct. I don't have much experience of principal component analysis, so I can't answer the latter part of your question, sorry.Here is my example (for rjags);# I supposed that your data was like this.#   yr2001 yr2002 yr2003 yr2004 yr2005 ... ind species ...       pc1         pc2 # 1 Y[1,1] Y[1,2] Y[1,3] Y[1,4] Y[1,5] ...   1       1 ... 0.2543029  0.32230799   # 2 Y[2,1] Y[2,2] Y[2,3] Y[2,4] Y[2,5] ...   2       2 ... 0.2219234 -0.38741117   :model{  for(ind in 1:nind) {                   ## nind = nrow(d$X)    for(yr in 1:nyear) {      Y[ind, yr] ~ dbin(phi[ind, yr], 12)      logit(phi[ind,yr]) \u0026lt;-        phi.sp[species[ind]] +           ## effect of species         phi.year[yr] +                   ## effect of year        phi.pc1[species[ind]]*pc1[ind] + ## Effect of morphology, but only the PCA scores        phi.pc2[species[ind]]*pc2[ind] + ##  of individuals captured)        PC1_2 * pc1[ind] * pc2[ind]    } ## (yr in 1:year)  } ## (ind in 1:nind)  for(sp in 1:nspecies){               ## Prior    phi.sp[sp] ~ dnorm(0, 0.01)    phi.pc1[sp] ~ dnorm(0, 0.01)    phi.pc2[sp] ~ dnorm(0, 0.01)  } ## (sp in 1:nspecies)  for(yr in 1:nyear){                  ## Prior    phi.year[yr] ~ dnorm(0, 0.01)  } ## (yr in 1:nyear)  PC1_2 ~ dnorm(0, 0.01)               ## Prior} ## (end)","Display_name":"cuttlefish44","Creater_id":119029,"Start_date":"2016-07-25 21:19:05","Question_id":225299}
{"_id":{"$oid":"5837a58da05283111e4d7904"},"Last_activity":"2012-12-21 02:19:11","Creator_reputation":474,"Question_score":1,"Answer_content":"This is some further advise/discussion I was given:  AIC RIW can only be calculated from a balanced candidate model set. If you have 3 variables (e.g. repro, time \u0026amp; WR) then the balanced set (without interactions) isreprotimeWRrepro + timerepro + WRtime + WRrepro + time + WRintercept onlythe number of models in the set is 2 to the power of the number of explanatory variables (in this case = 8)with 2-way interactions your candidate model set ALSO includes the following (i.e. in addition to those above)repro + time + repro*timerepro + WR + repro*WRtime + WR + time*WRrepro + time + WR + repro*timerepro + time + WR + repro*WRrepro + time + WR + time*WRIf you want the 3-way interaction, then you would ALSO add this to all of the models described above.Each variable relative importance weight is then the SUM of ALL AIC-weights from models that contain that variable. Because AIC-weights are standardized to sum to one within a candidate model set, then RIW for each variable can range from 0 to 1. Do not divide the result by the number of models it is contained in – it is the total sum. I would only use these for balanced candidate model sets; I wouldn’t use RIW for a smaller number of models. NOTE that if you include interactions, then you can only compare the RIWs of main effects with each other, and you can only compare the RIWs of interactions with each other. You cannot compare main effect RIWs with interaction RIWs (because main effects are present in more models than interactions).FYI: a strong explanatory variable will have a RIW of around 0.9, moderate effects of around 0.6-0.9, very weak effects of around 0.5-0.6 and below that, forget about it. For interactions, a strong effect could be \u003e0.7, moderate \u003e0.5.If you’re not using RIWs then simply look at  your model table and see if you get consistent improvements in AIC when you add specific variables, and by how much. Strong effects will often give you improvements in AIC of \u003e5, moderate 2-5 and weak 0-2. If you don’t get an improvement at all, then it isn’t explaining anything.if you don’t have a balanced candidate set, but DO have the AIC weights (which it appears you do), then you can simply use the ratios of these to determine the strength of support for one model over another. E.g. if you have model 1 with AIC-weight of 0.7 and model 2 with an AIC-weight of 0.15; then model 1 has 4.6 times more support from the data than model 2 (0.7/0.15). You can use this to assess the relative strength of variables as they go in and out of models. But you don’t NEED to do these calculations – and can simply refer the reader to the table. Especially if you have a dominant model; or a series of models at the top that all contain a particular variable. Then it is simply obvious to everyone that it is important.","Display_name":"Kerry","Creater_id":14125,"Start_date":"2012-12-21 02:19:11","Question_id":46289}
{"_id":{"$oid":"5837a58da05283111e4d7905"},"Last_activity":"2012-12-20 08:49:02","Creator_reputation":631,"Question_score":0,"Answer_content":"Two sentences down from the Page 169 quote, Brunham \u0026amp; Andersen (2002) explains why the ballancing is needed.  Burnham and Anderson (2002), Page 169: This ballancing puts each  variable on equal footing.Another words, if one variable is only in the model set once but another variable is in the model set many times, you have handicapped the variable that is under represented.For example, let's say you have 5 models and variable A was only in one model and variable B was in 4 models.Model       AIC     wA           12.0    0.579B           14.5    0.166B + C       15.0    0.129B + C + D   16.0    0.078B + D       17.0    0.048Notice that Model A is clearly the best model based on AIC alone but based on relative variable importance (RIV), variable A has a RIV of 0.579 but B has a RIV of 0.421.  This suggests on the face of if that variables A \u0026amp; B have similar relative importance but variable A was hadicapped because it was only included in one model.I have come across other cases where RIV values are adjusted by the number of models like Kittle et al. (2008) did.  Looks like MuMIn does not adjust RIV by the number of models.","Display_name":"RioRaider","Creater_id":12318,"Start_date":"2012-12-20 08:49:02","Question_id":46289}
